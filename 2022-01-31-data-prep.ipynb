{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MovieLens 100K Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from copy import deepcopy\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics.pairwise import pairwise_distances\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data():\n",
    "    \"\"\"Read dataset\"\"\"\n",
    "\n",
    "    dataset = pd.DataFrame()\n",
    "\n",
    "    # Load Movielens 100K Data\n",
    "    data_dir = './data/bronze/ml-100k/u.data'\n",
    "    dataset = pd.read_csv(data_dir, sep='\\t', header=None, names=['uid', 'mid', 'rating', 'timestamp'],\n",
    "                                engine='python')\n",
    "\n",
    "    # Reindex data\n",
    "    user_id = dataset[['uid']].drop_duplicates().reindex()\n",
    "    user_id['userId'] = np.arange(len(user_id))\n",
    "    dataset = pd.merge(dataset, user_id, on=['uid'], how='left')\n",
    "    item_id = dataset[['mid']].drop_duplicates()\n",
    "    item_id['itemId'] = np.arange(len(item_id))\n",
    "    dataset = pd.merge(dataset, item_id, on=['mid'], how='left')\n",
    "    if 'test' in dataset:\n",
    "        dataset = dataset[['userId', 'itemId', 'rating', 'timestamp', 'test']]\n",
    "    else:\n",
    "        dataset = dataset[['userId', 'itemId', 'rating', 'timestamp']]\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = read_data()\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class data_loader(Dataset):\n",
    "    \"\"\"Convert user, item, negative and target Tensors into Pytorch Dataset\"\"\"\n",
    "\n",
    "    def __init__(self, user_tensor, positive_item_tensor, negative_item_tensor, target_tensor):\n",
    "        self.user_tensor = user_tensor\n",
    "        self.positive_item_tensor = positive_item_tensor\n",
    "        self.negative_item_tensor = negative_item_tensor\n",
    "        self.target_tensor = target_tensor\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.user_tensor[index], self.positive_item_tensor[index], self.negative_item_tensor[index], self.target_tensor[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.user_tensor.size(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class data_loader_implicit(Dataset):\n",
    "    \"\"\"Convert user and item Tensors into Pytorch Dataset\"\"\"\n",
    "\n",
    "    def __init__(self, user_tensor, item_tensor):\n",
    "        self.user_tensor = user_tensor\n",
    "        self.item_tensor = item_tensor\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.user_tensor[index], self.item_tensor[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.user_tensor.size(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class data_loader_test_explicit(Dataset):\n",
    "    \"\"\"Convert user, item and target Tensors into Pytorch Dataset\"\"\"\n",
    "\n",
    "    def __init__(self, user_tensor, item_tensor, target_tensor):\n",
    "        self.user_tensor = user_tensor\n",
    "        self.item_tensor = item_tensor\n",
    "        self.target_tensor = target_tensor\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.user_tensor[index], self.item_tensor[index], self.target_tensor[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.user_tensor.size(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class data_loader_negatives(Dataset):\n",
    "    \"\"\"Convert user and item negative Tensors into Pytorch Dataset\"\"\"\n",
    "\n",
    "    def __init__(self, user_neg_tensor, item_neg_tensor):\n",
    "        self.user_neg_tensor = user_neg_tensor\n",
    "        self.item_neg_tensor = item_neg_tensor\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.user_neg_tensor[index], self.item_neg_tensor[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.user_neg_tensor.size(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data sampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SampleGenerator(object):\n",
    "    \"\"\"Construct dataset\"\"\"\n",
    "\n",
    "    def __init__(self, ratings, config, split_val):\n",
    "        \"\"\"\n",
    "        args:\n",
    "            ratings: pd.DataFrame containing 4 columns = ['userId', 'itemId', 'rating', 'timestamp']\n",
    "            config: dictionary containing the configuration hyperparameters\n",
    "            split_val: boolean that takes True if we are using a validation set\n",
    "        \"\"\"\n",
    "        assert 'userId' in ratings.columns\n",
    "        assert 'itemId' in ratings.columns\n",
    "        assert 'rating' in ratings.columns\n",
    "\n",
    "        self.config = config\n",
    "        self.ratings = ratings\n",
    "        self.split_val = split_val\n",
    "        self.preprocess_ratings = self._binarize(ratings)\n",
    "        self.user_pool = set(self.ratings['userId'].unique())\n",
    "        self.item_pool = set(self.ratings['itemId'].unique())\n",
    "        # create negative item samples\n",
    "        self.negatives = self._sample_negative(ratings, self.split_val)\n",
    "        if self.config['loo_eval']:\n",
    "            if self.split_val:\n",
    "                self.train_ratings, self.val_ratings = self._split_loo(self.preprocess_ratings, split_val=True)\n",
    "            else:\n",
    "                self.train_ratings, self.test_ratings = self._split_loo(self.preprocess_ratings, split_val=False)\n",
    "        else:\n",
    "            self.test_rate = self.config['test_rate']\n",
    "            if self.split_val:\n",
    "                self.train_ratings, self.val_ratings = self.train_test_split_random(self.ratings, split_val=True)\n",
    "            else:\n",
    "                self.train_ratings, self.test_ratings = self.train_test_split_random(self.ratings, split_val=False)\n",
    "\n",
    "    def _binarize(self, ratings):\n",
    "        \"\"\"binarize into 0 or 1 for imlicit feedback\"\"\"\n",
    "        ratings = deepcopy(ratings)\n",
    "        ratings['rating'] = 1.0\n",
    "        return ratings\n",
    "\n",
    "    def train_test_split_random(self, ratings, split_val):\n",
    "        \"\"\"Random train/test split\"\"\"\n",
    "        if 'test' in list(ratings):\n",
    "            test = ratings[ratings['test'] == 1]\n",
    "            train = ratings[ratings['test'] == 0]\n",
    "        else:\n",
    "            train, test = train_test_split(ratings, test_size=self.test_rate)\n",
    "        if split_val:\n",
    "            train, val = train_test_split(train, test_size=self.test_rate / (1 - self.test_rate))\n",
    "            return train[['userId', 'itemId', 'rating']], val[['userId', 'itemId', 'rating']]\n",
    "        else:\n",
    "            return train[['userId', 'itemId', 'rating']], test[['userId', 'itemId', 'rating']]\n",
    "\n",
    "    def _split_loo(self, ratings, split_val):\n",
    "        \"\"\"leave-one-out train/test split\"\"\"\n",
    "        if 'test' in list(ratings):\n",
    "            test = ratings[ratings['test'] == 1]\n",
    "            ratings = ratings[ratings['test'] == 0]\n",
    "            if split_val:\n",
    "                ratings['rank_latest'] = ratings.groupby(['userId'])['timestamp'].rank(method='first', ascending=False)\n",
    "                val = ratings[ratings['rank_latest'] == 1]\n",
    "                train = ratings[ratings['rank_latest'] > 1]\n",
    "                return train[['userId', 'itemId', 'rating']], val[['userId', 'itemId', 'rating']]\n",
    "            return ratings[['userId', 'itemId', 'rating']], test[['userId', 'itemId', 'rating']]\n",
    "        ratings['rank_latest'] = ratings.groupby(['userId'])['timestamp'].rank(method='first', ascending=False)\n",
    "        test = ratings[ratings['rank_latest'] == 1]\n",
    "        if split_val:\n",
    "            val = ratings[ratings['rank_latest'] == 2]\n",
    "            train = ratings[ratings['rank_latest'] > 2]\n",
    "            assert train['userId'].nunique() == test['userId'].nunique() == val['userId'].nunique()\n",
    "            return train[['userId', 'itemId', 'rating']], val[['userId', 'itemId', 'rating']]\n",
    "        train = ratings[ratings['rank_latest'] > 1]\n",
    "        assert train['userId'].nunique() == test['userId'].nunique()\n",
    "        return train[['userId', 'itemId', 'rating']], test[['userId', 'itemId', 'rating']]\n",
    "\n",
    "    def _sample_negative(self, ratings, split_val):\n",
    "        \"\"\"return all negative items & 100 sampled negative test items & 100 sampled negative val items\"\"\"\n",
    "        interact_status = ratings.groupby('userId')['itemId'].apply(set).reset_index().rename(\n",
    "            columns={'itemId': 'interacted_items'})\n",
    "        interact_status['negative_items'] = interact_status['interacted_items'].apply(lambda x: self.item_pool - x)\n",
    "        interact_status['test_negative_samples'] = interact_status['negative_items'].apply(lambda x: random.sample(x, 100))\n",
    "        interact_status['negative_items'] = interact_status.apply(lambda x: (x.negative_items - set(x.test_negative_samples)), axis=1)\n",
    "        if split_val:\n",
    "            interact_status['val_negative_samples'] = interact_status['negative_items'].apply(lambda x: random.sample(x, 100))\n",
    "            interact_status['negative_items'] = interact_status.apply(lambda x: (x.negative_items - set(x.val_negative_samples)), axis=1)\n",
    "            return interact_status[['userId', 'negative_items', 'test_negative_samples', 'val_negative_samples']]\n",
    "        else:\n",
    "            return interact_status[['userId', 'negative_items', 'test_negative_samples']]\n",
    "\n",
    "    def train_data_loader(self, batch_size):\n",
    "        \"\"\"instance train loader for one training epoch\"\"\"\n",
    "        train_ratings = pd.merge(self.train_ratings, self.negatives[['userId', 'negative_items']], on='userId')\n",
    "        users = [int(x) for x in train_ratings['userId']]\n",
    "        items = [int(x) for x in train_ratings['itemId']]\n",
    "        ratings = [float(x) for x in train_ratings['rating']]\n",
    "        neg_items = [random.choice(list(neg_list)) for neg_list in train_ratings['negative_items']]\n",
    "        dataset = data_loader(user_tensor=torch.LongTensor(users),\n",
    "                              positive_item_tensor=torch.LongTensor(items),\n",
    "                              negative_item_tensor=torch.LongTensor(neg_items),\n",
    "                              target_tensor=torch.FloatTensor(ratings))\n",
    "        return DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    def test_data_loader(self, batch_size):\n",
    "        \"\"\"create evaluation data\"\"\"\n",
    "        if self.config['loo_eval']:\n",
    "            test_ratings = pd.merge(self.test_ratings, self.negatives[['userId', 'test_negative_samples']], on='userId')\n",
    "            test_users, test_items, negative_users, negative_items = [], [], [], []\n",
    "            for row in test_ratings.itertuples():\n",
    "                test_users.append(int(row.userId))\n",
    "                test_items.append(int(row.itemId))\n",
    "                for i in range(len(row.test_negative_samples)):\n",
    "                    negative_users.append(int(row.userId))\n",
    "                    negative_items.append(int(row.test_negative_samples[i]))\n",
    "            dataset = data_loader_implicit(user_tensor=torch.LongTensor(test_users),\n",
    "                                           item_tensor=torch.LongTensor(test_items))\n",
    "            dataset_negatives = data_loader_negatives(user_neg_tensor=torch.LongTensor(negative_users),\n",
    "                                                      item_neg_tensor=torch.LongTensor(negative_items))\n",
    "            return [DataLoader(dataset, batch_size=batch_size, shuffle=False), DataLoader(dataset_negatives, batch_size=batch_size, shuffle=False)]\n",
    "        else:\n",
    "            test_ratings = self.test_ratings\n",
    "            test_users = [int(x) for x in test_ratings['userId']]\n",
    "            test_items = [int(x) for x in test_ratings['itemId']]\n",
    "            test_ratings = [float(x) for x in test_ratings['rating']]\n",
    "            dataset = data_loader_test_explicit(user_tensor=torch.LongTensor(test_users),\n",
    "                                                item_tensor=torch.LongTensor(test_items),\n",
    "                                                target_tensor=torch.FloatTensor(test_ratings))\n",
    "            return DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    def val_data_loader(self, batch_size):\n",
    "        \"\"\"create validation data\"\"\"\n",
    "        if self.config['loo_eval']:\n",
    "            val_ratings = pd.merge(self.val_ratings, self.negatives[['userId', 'val_negative_samples']], on='userId')\n",
    "            val_users, val_items, negative_users, negative_items = [], [], [], []\n",
    "            for row in val_ratings.itertuples():\n",
    "                val_users.append(int(row.userId))\n",
    "                val_items.append(int(row.itemId))\n",
    "                for i in range(len(row.val_negative_samples)):\n",
    "                    negative_users.append(int(row.userId))\n",
    "                    negative_items.append(int(row.val_negative_samples[i]))\n",
    "            dataset = data_loader_implicit(user_tensor=torch.LongTensor(val_users),\n",
    "                                           item_tensor=torch.LongTensor(val_items))\n",
    "            dataset_negatives = data_loader_negatives(user_neg_tensor=torch.LongTensor(negative_users),\n",
    "                                                      item_neg_tensor=torch.LongTensor(negative_items))\n",
    "            return [DataLoader(dataset, batch_size=batch_size, shuffle=False), DataLoader(dataset_negatives, batch_size=batch_size, shuffle=False)]\n",
    "        else:\n",
    "            val_ratings = self.val_ratings\n",
    "            val_users = [int(x) for x in val_ratings['userId']]\n",
    "            val_items = [int(x) for x in val_ratings['itemId']]\n",
    "            val_ratings = [float(x) for x in val_ratings['rating']]\n",
    "            dataset = data_loader_test_explicit(user_tensor=torch.LongTensor(val_users),\n",
    "                                                item_tensor=torch.LongTensor(val_items),\n",
    "                                                target_tensor=torch.FloatTensor(val_ratings))\n",
    "            return DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    def create_explainability_matrix(self, include_test=False):\n",
    "        \"\"\"create explainability matrix\"\"\"\n",
    "        if not include_test:\n",
    "            print('Creating explainability matrix...')\n",
    "            interaction_matrix = pd.crosstab(self.train_ratings.userId, self.train_ratings.itemId)\n",
    "            missing_columns = list(set(range(self.config['num_items'])) - set(list(interaction_matrix)))\n",
    "            missing_rows = list(set(range(self.config['num_users'])) - set(interaction_matrix.index))\n",
    "            for missing_column in missing_columns:\n",
    "                interaction_matrix[missing_column] = [0] * len(interaction_matrix)\n",
    "            for missing_row in missing_rows:\n",
    "                interaction_matrix.loc[missing_row] = [0] * self.config['num_items']\n",
    "            interaction_matrix = np.array(interaction_matrix[list(range(self.config['num_items']))].sort_index())\n",
    "        elif not self.split_val:\n",
    "            print('Creating test explainability matrix...')\n",
    "            interaction_matrix = np.array(pd.crosstab(self.preprocess_ratings.userId, self.preprocess_ratings.itemId)[\n",
    "                                              list(range(self.config['num_items']))].sort_index())\n",
    "        else:\n",
    "            print('Creating val explainability matrix...')\n",
    "            interaction_matrix = pd.crosstab(self.train_ratings.userId.append(self.val_ratings.userId), self.train_ratings.itemId.append(self.val_ratings.itemId))\n",
    "            missing_columns = list(set(range(self.config['num_items'])) - set(list(interaction_matrix)))\n",
    "            missing_rows = list(set(range(self.config['num_users'])) - set(interaction_matrix.index))\n",
    "            for missing_column in missing_columns:\n",
    "                interaction_matrix[missing_column] = [0] * len(interaction_matrix)\n",
    "            for missing_row in missing_rows:\n",
    "                interaction_matrix.loc[missing_row] = [0] * self.config['num_items']\n",
    "            interaction_matrix = np.array(interaction_matrix[list(range(self.config['num_items']))].sort_index())\n",
    "        #item_similarity_matrix = 1 - pairwise_distances(interaction_matrix.T, metric = \"hamming\")\n",
    "        item_similarity_matrix = cosine_similarity(interaction_matrix.T)\n",
    "        np.fill_diagonal(item_similarity_matrix, 0)\n",
    "        neighborhood = [np.argpartition(row, - self.config['neighborhood'])[- self.config['neighborhood']:]\n",
    "                        for row in item_similarity_matrix]\n",
    "        explainability_matrix = np.array([[sum([interaction_matrix[user, neighbor] for neighbor in neighborhood[item]])\n",
    "                                           for item in range(self.config['num_items'])] for user in\n",
    "                                          range(self.config['num_users'])]) / self.config['neighborhood']\n",
    "        #explainability_matrix[explainability_matrix < 0.1] = 0\n",
    "        #explainability_matrix = explainability_matrix + self.config['epsilon']\n",
    "        return explainability_matrix\n",
    "\n",
    "    def create_popularity_vector(self, include_test=False):\n",
    "        \"\"\"create popularity vector\"\"\"\n",
    "        if not include_test:\n",
    "            print('Creating popularity vector...')\n",
    "            interaction_matrix = pd.crosstab(self.train_ratings.userId, self.train_ratings.itemId)\n",
    "            missing_columns = list(set(range(self.config['num_items'])) - set(list(interaction_matrix)))\n",
    "            missing_rows = list(set(range(self.config['num_users'])) - set(interaction_matrix.index))\n",
    "            for missing_column in missing_columns:\n",
    "                interaction_matrix[missing_column] = [0] * len(interaction_matrix)\n",
    "            for missing_row in missing_rows:\n",
    "                interaction_matrix.loc[missing_row] = [0] * self.config['num_items']\n",
    "            interaction_matrix = np.array(interaction_matrix[list(range(self.config['num_items']))].sort_index())\n",
    "        elif not self.split_val:\n",
    "            print('Creating test popularity vector...')\n",
    "            interaction_matrix = np.array(pd.crosstab(self.preprocess_ratings.userId, self.preprocess_ratings.itemId)[\n",
    "                                              list(range(self.config['num_items']))].sort_index())\n",
    "        else:\n",
    "            print('Creating val popularity vector...')\n",
    "            interaction_matrix = pd.crosstab(self.train_ratings.userId.append(self.val_ratings.userId),\n",
    "                                             self.train_ratings.itemId.append(self.val_ratings.itemId))\n",
    "            missing_columns = list(set(range(self.config['num_items'])) - set(list(interaction_matrix)))\n",
    "            missing_rows = list(set(range(self.config['num_users'])) - set(interaction_matrix.index))\n",
    "            for missing_column in missing_columns:\n",
    "                interaction_matrix[missing_column] = [0] * len(interaction_matrix)\n",
    "            for missing_row in missing_rows:\n",
    "                interaction_matrix.loc[missing_row] = [0] * self.config['num_items']\n",
    "            interaction_matrix = np.array(interaction_matrix[list(range(self.config['num_items']))].sort_index())\n",
    "        popularity_vector = np.sum(interaction_matrix, axis=0)\n",
    "        popularity_vector = (popularity_vector / max(popularity_vector)) ** 0.5\n",
    "        return popularity_vector\n",
    "\n",
    "    def create_neighborhood(self, include_test=False):\n",
    "        \"\"\"Determine item neighbors\"\"\"\n",
    "        if not include_test:\n",
    "            print('Determining item neighborhoods...')\n",
    "            interaction_matrix = pd.crosstab(self.train_ratings.userId, self.train_ratings.itemId)\n",
    "            missing_columns = list(set(range(self.config['num_items'])) - set(list(interaction_matrix)))\n",
    "            missing_rows = list(set(range(self.config['num_users'])) - set(interaction_matrix.index))\n",
    "            for missing_column in missing_columns:\n",
    "                interaction_matrix[missing_column] = [0] * len(interaction_matrix)\n",
    "            for missing_row in missing_rows:\n",
    "                interaction_matrix.loc[missing_row] = [0] * self.config['num_items']\n",
    "            interaction_matrix = np.array(interaction_matrix[list(range(self.config['num_items']))].sort_index())\n",
    "        elif not self.split_val:\n",
    "            print('Determining test item neighborhoods...')\n",
    "            interaction_matrix = np.array(pd.crosstab(self.preprocess_ratings.userId, self.preprocess_ratings.itemId)[\n",
    "                                              list(range(self.config['num_items']))].sort_index())\n",
    "        else:\n",
    "            print('Determining val item neighborhoods...')\n",
    "            interaction_matrix = pd.crosstab(self.train_ratings.userId.append(self.val_ratings.userId),\n",
    "                                             self.train_ratings.itemId.append(self.val_ratings.itemId))\n",
    "            missing_columns = list(set(range(self.config['num_items'])) - set(list(interaction_matrix)))\n",
    "            missing_rows = list(set(range(self.config['num_users'])) - set(interaction_matrix.index))\n",
    "            for missing_column in missing_columns:\n",
    "                interaction_matrix[missing_column] = [0] * len(interaction_matrix)\n",
    "            for missing_row in missing_rows:\n",
    "                interaction_matrix.loc[missing_row] = [0] * self.config['num_items']\n",
    "            interaction_matrix = np.array(interaction_matrix[list(range(self.config['num_items']))].sort_index())\n",
    "        item_similarity_matrix = cosine_similarity(interaction_matrix.T)\n",
    "        np.fill_diagonal(item_similarity_matrix, 0)\n",
    "        neighborhood = np.array([np.argpartition(row, - self.config['neighborhood'])[- self.config['neighborhood']:]\n",
    "                        for row in item_similarity_matrix])\n",
    "        return neighborhood, item_similarity_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {'model': 'BPR',\n",
    "          'dataset': 'ml-100k',\n",
    "          'num_epoch': 50,\n",
    "          'batch_size': 100,\n",
    "          'num_users': len(dataset['userId'].unique()),\n",
    "          'num_items': len(dataset['itemId'].unique()),\n",
    "          'test_rate': 0.2,\n",
    "          'loo_eval': True,\n",
    "          'neighborhood': 20,\n",
    "          }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_generator = SampleGenerator(dataset, config, split_val=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataLoader\n",
    "test_data = sample_generator.test_data_loader(config['batch_size'])\n",
    "test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(test_data[0]), len(test_data[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for (idx, batch) in enumerate(test_data[0]):\n",
    "    print('idx: {}\\n{}\\n{}\\n{}\\n{}'.format(idx, '='*100,\n",
    "                                           batch[0], '='*100,\n",
    "                                           batch[1]))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for (idx, batch) in enumerate(test_data[1]):\n",
    "    print('idx: {}\\n{}\\n{}\\n{}\\n{}'.format(idx, '='*100,\n",
    "                                           batch[0], '='*100,\n",
    "                                           batch[1]))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create explainability matrix\n",
    "explainability_matrix = sample_generator.create_explainability_matrix()\n",
    "explainability_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explainability_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_explainability_matrix = sample_generator.create_explainability_matrix(include_test=True)\n",
    "test_explainability_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_explainability_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create popularity vector\n",
    "popularity_vector = sample_generator.create_popularity_vector()\n",
    "popularity_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "popularity_vector.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_popularity_vector = sample_generator.create_popularity_vector(include_test=True)\n",
    "test_popularity_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_popularity_vector.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create item neighborhood\n",
    "neighborhood, item_similarity_matrix = sample_generator.create_neighborhood()\n",
    "neighborhood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neighborhood.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_similarity_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_similarity_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, test_item_similarity_matrix = sample_generator.create_neighborhood(include_test=True)\n",
    "test_item_similarity_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_item_similarity_matrix.shape"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyOVV2s6yOaiQAhKf6hEktPm",
   "collapsed_sections": [],
   "mount_file_id": "1oECZ1-YDDxo-5cuTGuiO7t-d4V6qvi_2",
   "name": "reco-tut-elf-ml100k-01-data-preparation.ipynb",
   "provenance": [
    {
     "file_id": "1vRFN2OMXJN6phWLVd7xItGEuzBoxzsq0",
     "timestamp": 1628348811833
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
