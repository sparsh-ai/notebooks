{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"2022-01-15-stosa.ipynb","provenance":[{"file_id":"https://github.com/recohut/nbs/blob/main/raw/STOSA.ipynb","timestamp":1644615007320},{"file_id":"1KMhtCewKYqHid2baez7-KujQkUniFuR1","timestamp":1642777015441}],"collapsed_sections":[],"authorship_tag":"ABX9TyPDJ6gbQiIBTBj7FVf2XqjF"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# STOSA"],"metadata":{"id":"dH_BYUrlsTer"}},{"cell_type":"markdown","source":["## Utils"],"metadata":{"id":"iCpEESnM1fQK"}},{"cell_type":"code","source":["import numpy as np\n","import math\n","import random\n","import os\n","import json\n","import pickle\n","from scipy.sparse import csr_matrix\n","from tqdm import tqdm\n","import multiprocessing\n","\n","import torch\n","import torch.nn.functional as F"],"metadata":{"id":"HLEs4ym_1mh1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def set_seed(seed):\n","    random.seed(seed)\n","    os.environ['PYTHONHASHSEED'] = str(seed)\n","    np.random.seed(seed)\n","    torch.manual_seed(seed)\n","    torch.cuda.manual_seed(seed)\n","    torch.cuda.manual_seed_all(seed)\n","    # some cudnn methods can be random even after fixing the seed\n","    # unless you tell it to be deterministic\n","    torch.backends.cudnn.deterministic = True\n","\n","# REPLACE WITH from recohut.utils.common_utils import seed_everything\n","\n","def random_neg_sample(item_set, item_size):\n","    item = random.randint(1, item_size - 1)\n","    while item in item_set:\n","        item = random.randint(1, item_size - 1)\n","    return item\n","\n","# REPLACE WITH from recohut.utils.negative_sampling import random_neg_sample\n","\n","class EarlyStopping:\n","    \"\"\"Early stops the training if validation loss doesn't improve after a given patience.\"\"\"\n","    def __init__(self, checkpoint_path, patience=7, verbose=False, delta=0):\n","        \"\"\"\n","        Args:\n","            patience (int): How long to wait after last time validation loss improved.\n","                            Default: 7\n","            verbose (bool): If True, prints a message for each validation loss improvement.\n","                            Default: False\n","            delta (float): Minimum change in the monitored quantity to qualify as an improvement.\n","                            Default: 0\n","        \"\"\"\n","        self.checkpoint_path = checkpoint_path\n","        self.patience = patience\n","        self.verbose = verbose\n","        self.counter = 0\n","        self.best_score = None\n","        self.early_stop = False\n","        self.delta = delta\n","\n","    def compare(self, score):\n","        for i in range(len(score)):\n","            if score[i] > self.best_score[i]+self.delta:\n","                return False\n","        return True\n","\n","    def __call__(self, score, model):\n","        # score HIT@10 NDCG@10\n","\n","        if self.best_score is None:\n","            self.best_score = score\n","            self.score_min = np.array([0]*len(score))\n","            self.save_checkpoint(score, model)\n","        elif self.compare(score):\n","            self.counter += 1\n","            print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n","            if self.counter >= self.patience:\n","                self.early_stop = True\n","        else:\n","            self.best_score = score\n","            self.save_checkpoint(score, model)\n","            self.counter = 0\n","\n","    def save_checkpoint(self, score, model):\n","        '''Saves model when validation loss decrease.'''\n","        if self.verbose:\n","            print(f'Validation score increased.  Saving model ...')\n","        torch.save(model.state_dict(), self.checkpoint_path)\n","        self.score_min = score\n","\n","# CHECK IF CAN BE REPLACE WITH from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n","# https://pytorch-lightning.readthedocs.io/en/latest/common/early_stopping.html\n","\n","def kmax_pooling(x, dim, k):\n","    index = x.topk(k, dim=dim)[1].sort(dim=dim)[0]\n","    return x.gather(dim, index).squeeze(dim)\n","\n","# REPLACE WITH from recohut.utils.pooling import kmax_pooling\n","\n","def avg_pooling(x, dim):\n","    return x.sum(dim=dim)/x.size(dim)\n","\n","# REPLACE WITH from recohut.utils.pooling import avg_pooling\n","\n","# def generate_rating_matrix_valid(user_seq, num_users, num_items):\n","#     # three lists are used to construct sparse matrix\n","#     row = []\n","#     col = []\n","#     data = []\n","#     for user_id, item_list in enumerate(user_seq):\n","#         for item in item_list[:-2]: #\n","#             row.append(user_id)\n","#             col.append(item)\n","#             data.append(1)\n","\n","#     row = np.array(row)\n","#     col = np.array(col)\n","#     data = np.array(data)\n","#     rating_matrix = csr_matrix((data, (row, col)), shape=(num_users, num_items))\n","\n","#     return rating_matrix\n","\n","# def generate_rating_matrix_test(user_seq, num_users, num_items):\n","#     # three lists are used to construct sparse matrix\n","#     row = []\n","#     col = []\n","#     data = []\n","#     for user_id, item_list in enumerate(user_seq):\n","#         for item in item_list[:-1]: #\n","#             row.append(user_id)\n","#             col.append(item)\n","#             data.append(1)\n","\n","#     row = np.array(row)\n","#     col = np.array(col)\n","#     data = np.array(data)\n","#     rating_matrix = csr_matrix((data, (row, col)), shape=(num_users, num_items))\n","\n","#     return rating_matrix\n","\n","def generate_rating_matrix(user_seq, num_users, num_items, n):\n","    row = []\n","    col = []\n","    data = []\n","    for user_id, item_list in enumerate(user_seq):\n","        for item in item_list[:-n]: #\n","            row.append(user_id)\n","            col.append(item)\n","            data.append(1)\n","\n","    row = np.array(row)\n","    col = np.array(col)\n","    data = np.array(data)\n","    rating_matrix = csr_matrix((data, (row, col)), shape=(num_users, num_items))\n","\n","    return rating_matrix\n","\n","# REPLACE WITH from recohut.transforms.matrix import generate_rating_matrix\n","\n","def get_user_seqs(data_file):\n","    lines = open(data_file).readlines()\n","    user_seq = []\n","    item_set = set()\n","    for line in lines:\n","        user, items = line.strip().split(' ', 1)\n","        items = items.split(' ')\n","        items = [int(item) for item in items]\n","        user_seq.append(items)\n","        item_set = item_set | set(items)\n","    max_item = max(item_set)\n","\n","    num_users = len(lines)\n","    num_items = max_item + 2\n","\n","    valid_rating_matrix = generate_rating_matrix(user_seq, num_users, num_items, n=2)\n","    test_rating_matrix = generate_rating_matrix(user_seq, num_users, num_items, n=1)\n","    return user_seq, max_item, valid_rating_matrix, test_rating_matrix, num_users\n","\n","def get_user_seqs_long(data_file):\n","    lines = open(data_file).readlines()\n","    user_seq = []\n","    long_sequence = []\n","    item_set = set()\n","    for line in lines:\n","        user, items = line.strip().split(' ', 1)\n","        items = items.split(' ')\n","        items = [int(item) for item in items]\n","        long_sequence.extend(items) #\n","        user_seq.append(items)\n","        item_set = item_set | set(items)\n","    max_item = max(item_set)\n","\n","    return user_seq, max_item, long_sequence\n","\n","def get_user_seqs_and_sample(data_file, sample_file):\n","    lines = open(data_file).readlines()\n","    user_seq = []\n","    item_set = set()\n","    for line in lines:\n","        user, items = line.strip().split(' ', 1)\n","        items = items.split(' ')\n","        items = [int(item) for item in items]\n","        user_seq.append(items)\n","        item_set = item_set | set(items)\n","    max_item = max(item_set)\n","\n","    lines = open(sample_file).readlines()\n","    sample_seq = []\n","    for line in lines:\n","        user, items = line.strip().split(' ', 1)\n","        items = items.split(' ')\n","        items = [int(item) for item in items]\n","        sample_seq.append(items)\n","\n","    assert len(user_seq) == len(sample_seq)\n","\n","    return user_seq, max_item, sample_seq\n","\n","def get_item2attribute_json(data_file):\n","    item2attribute = json.loads(open(data_file).readline())\n","    attribute_set = set()\n","    for item, attributes in item2attribute.items():\n","        attribute_set = attribute_set | set(attributes)\n","    attribute_size = max(attribute_set) # 331\n","    return item2attribute, attribute_size\n","\n","def get_eval_metrics_v2(pred_list, topk=10):\n","    NDCG = 0.0\n","    HIT = 0.0\n","    MRR = 0.0\n","    # [batch] the answer's rank\n","    for rank in pred_list:\n","        MRR += 1.0 / (rank + 1.0)\n","        if rank < topk:\n","            NDCG += 1.0 / np.log2(rank + 2.0)\n","            HIT += 1.0\n","    return HIT /len(pred_list), NDCG /len(pred_list), MRR /len(pred_list)\n","\n","# REPLACE WITH from recohut.evaluation.metrics import get_eval_metrics_v2\n","\n","def precision_at_k_per_sample(actual, predicted, topk):\n","    num_hits = 0\n","    for place in predicted:\n","        if place in actual:\n","            num_hits += 1\n","    return num_hits / (topk + 0.0)\n","\n","# REPLACE WITH from recohut.evaluation.metrics import precision_at_k_per_sample\n","\n","def precision_at_k(actual, predicted, topk):\n","    sum_precision = 0.0\n","    num_users = len(predicted)\n","    for i in range(num_users):\n","        act_set = set(actual[i])\n","        pred_set = set(predicted[i][:topk])\n","        sum_precision += len(act_set & pred_set) / float(topk)\n","\n","    return sum_precision / num_users\n","\n","# REPLACE WITH from recohut.evaluation.metrics import precision_at_k\n","\n","def recall_at_k(actual, predicted, topk):\n","    sum_recall = 0.0\n","    num_users = len(predicted)\n","    true_users = 0\n","    recall_dict = {}\n","    for i in range(num_users):\n","        act_set = set(actual[i])\n","        pred_set = set(predicted[i][:topk])\n","        if len(act_set) != 0:\n","            #sum_recall += len(act_set & pred_set) / float(len(act_set))\n","            one_user_recall = len(act_set & pred_set) / float(len(act_set))\n","            recall_dict[i] = one_user_recall\n","            sum_recall += one_user_recall\n","            true_users += 1\n","    return sum_recall / true_users, recall_dict\n","\n","# REPLACE WITH from recohut.evaluation.metrics import recall_at_k\n","\n","def cal_mrr(actual, predicted):\n","    sum_mrr = 0.\n","    true_users = 0\n","    num_users = len(predicted)\n","    mrr_dict = {}\n","    for i in range(num_users):\n","        r = []\n","        act_set = set(actual[i])\n","        pred_list = predicted[i]\n","        for item in pred_list:\n","            if item in act_set:\n","                r.append(1)\n","            else:\n","                r.append(0)\n","        r = np.array(r)\n","        if np.sum(r) > 0:\n","            #sum_mrr += np.reciprocal(np.where(r==1)[0]+1, dtype=np.float)[0]\n","            one_user_mrr = np.reciprocal(np.where(r==1)[0]+1, dtype=np.float)[0]\n","            sum_mrr += one_user_mrr\n","            true_users += 1\n","            mrr_dict[i] = one_user_mrr\n","        else:\n","            mrr_dict[i] = 0.\n","    return sum_mrr / len(predicted), mrr_dict\n","\n","# REPLACE WITH from recohut.evaluation.metrics import cal_mrr\n","\n","def ap_at_k(actual, predicted, k=10):\n","    \"\"\"\n","    Computes the average precision at k.\n","    This function computes the average precision at k between two lists of\n","    items.\n","    Parameters\n","    ----------\n","    actual : list\n","             A list of elements that are to be predicted (order doesn't matter)\n","    predicted : list\n","                A list of predicted elements (order does matter)\n","    k : int, optional\n","        The maximum number of predicted elements\n","    Returns\n","    -------\n","    score : double\n","            The average precision at k over the input lists\n","    \"\"\"\n","    if len(predicted)>k:\n","        predicted = predicted[:k]\n","\n","    score = 0.0\n","    num_hits = 0.0\n","\n","    for i,p in enumerate(predicted):\n","        if p in actual and p not in predicted[:i]:\n","            num_hits += 1.0\n","            score += num_hits / (i+1.0)\n","\n","    if not actual:\n","        return 0.0\n","\n","    return score / min(len(actual), k)\n","\n","# REPLACE WITH from recohut.evaluation.metrics import ap_at_k\n","\n","def map_at_k(actual, predicted, k=10):\n","    \"\"\"\n","    Computes the mean average precision at k.\n","    This function computes the mean average prescision at k between two lists\n","    of lists of items.\n","    Parameters\n","    ----------\n","    actual : list\n","             A list of lists of elements that are to be predicted\n","             (order doesn't matter in the lists)\n","    predicted : list\n","                A list of lists of predicted elements\n","                (order matters in the lists)\n","    k : int, optional\n","        The maximum number of predicted elements\n","    Returns\n","    -------\n","    score : double\n","            The mean average precision at k over the input lists\n","    \"\"\"\n","    return np.mean([ap_at_k(a, p, k) for a, p in zip(actual, predicted)])\n","\n","# REPLACE WITH from recohut.evaluation.metrics import map_at_k\n","\n","# def idcg_at_k(k):\n","#     \"\"\"Calculates the ideal discounted cumulative gain at k.\"\"\"\n","#     res = sum([1.0/math.log(i+2, 2) for i in range(k)])\n","#     if not res:\n","#         return 1.0\n","#     else:\n","#         return res\n","\n","def ndcg_at_k(actual, predicted, topk):\n","    res = 0\n","    ndcg_dict = {}\n","    for user_id in range(len(actual)):\n","        k = min(topk, len(actual[user_id]))\n","        # idcg = idcg_at_k(k)\n","        res = sum([1.0/math.log(i+2, 2) for i in range(k)])\n","        idcg = res if res else 1.0\n","        dcg_k = sum([int(predicted[user_id][j] in\n","                         set(actual[user_id])) / math.log(j+2, 2) for j in range(topk)])\n","        res += dcg_k / idcg\n","        ndcg_dict[user_id] = dcg_k / idcg\n","    return res / float(len(actual)), ndcg_dict\n","\n","# REPLACE WITH from recohut.evaluation.metrics import ndcg_at_k"],"metadata":{"id":"3V5zpUxd1ojh"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Datasets"],"metadata":{"id":"iMQIl8Od1sqt"}},{"cell_type":"code","source":["import random\n","\n","import torch\n","from torch.utils.data import Dataset"],"metadata":{"id":"Hp5K_4OS1vt7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class PretrainDataset(Dataset):\n","\n","    def __init__(self, args, user_seq, long_sequence):\n","        self.args = args\n","        self.user_seq = user_seq\n","        self.long_sequence = long_sequence\n","        self.max_len = args.max_seq_length\n","        self.part_sequence = []\n","        self.split_sequence()\n","\n","    def split_sequence(self):\n","        for seq in self.user_seq:\n","            input_ids = seq[-(self.max_len+2):-2] # keeping same as train set\n","            for i in range(len(input_ids)):\n","                self.part_sequence.append(input_ids[:i+1])\n","\n","    def __len__(self):\n","        return len(self.part_sequence)\n","\n","    def __getitem__(self, index):\n","\n","        sequence = self.part_sequence[index] # pos_items\n","        # sample neg item for every masked item\n","        masked_item_sequence = []\n","        neg_items = []\n","        # Masked Item Prediction\n","        item_set = set(sequence)\n","        for item in sequence[:-1]:\n","            prob = random.random()\n","            if prob < self.args.mask_p:\n","                masked_item_sequence.append(self.args.mask_id)\n","                neg_items.append(random_neg_sample(item_set, self.args.item_size))\n","            else:\n","                masked_item_sequence.append(item)\n","                neg_items.append(item)\n","\n","        # add mask at the last position\n","        masked_item_sequence.append(self.args.mask_id)\n","        neg_items.append(random_neg_sample(item_set, self.args.item_size))\n","\n","        # Segment Prediction\n","        if len(sequence) < 2:\n","            masked_segment_sequence = sequence\n","            pos_segment = sequence\n","            neg_segment = sequence\n","        else:\n","            sample_length = random.randint(1, len(sequence) // 2)\n","            start_id = random.randint(0, len(sequence) - sample_length)\n","            neg_start_id = random.randint(0, len(self.long_sequence) - sample_length)\n","            pos_segment = sequence[start_id: start_id + sample_length]\n","            neg_segment = self.long_sequence[neg_start_id:neg_start_id + sample_length]\n","            masked_segment_sequence = sequence[:start_id] + [self.args.mask_id] * sample_length + sequence[\n","                                                                                      start_id + sample_length:]\n","            pos_segment = [self.args.mask_id] * start_id + pos_segment + [self.args.mask_id] * (\n","                        len(sequence) - (start_id + sample_length))\n","            neg_segment = [self.args.mask_id] * start_id + neg_segment + [self.args.mask_id] * (\n","                        len(sequence) - (start_id + sample_length))\n","\n","        assert len(masked_segment_sequence) == len(sequence)\n","        assert len(pos_segment) == len(sequence)\n","        assert len(neg_segment) == len(sequence)\n","\n","        # padding sequence\n","        pad_len = self.max_len - len(sequence)\n","        masked_item_sequence = [0] * pad_len + masked_item_sequence\n","        pos_items = [0] * pad_len + sequence\n","        neg_items = [0] * pad_len + neg_items\n","        masked_segment_sequence = [0]*pad_len + masked_segment_sequence\n","        pos_segment = [0]*pad_len + pos_segment\n","        neg_segment = [0]*pad_len + neg_segment\n","\n","        masked_item_sequence = masked_item_sequence[-self.max_len:]\n","        pos_items = pos_items[-self.max_len:]\n","        neg_items = neg_items[-self.max_len:]\n","\n","        masked_segment_sequence = masked_segment_sequence[-self.max_len:]\n","        pos_segment = pos_segment[-self.max_len:]\n","        neg_segment = neg_segment[-self.max_len:]\n","\n","        # Associated Attribute Prediction\n","        # Masked Attribute Prediction\n","        attributes = []\n","        for item in pos_items:\n","            attribute = [0] * self.args.attribute_size\n","            try:\n","                now_attribute = self.args.item2attribute[str(item)]\n","                for a in now_attribute:\n","                    attribute[a] = 1\n","            except:\n","                pass\n","            attributes.append(attribute)\n","\n","\n","        assert len(attributes) == self.max_len\n","        assert len(masked_item_sequence) == self.max_len\n","        assert len(pos_items) == self.max_len\n","        assert len(neg_items) == self.max_len\n","        assert len(masked_segment_sequence) == self.max_len\n","        assert len(pos_segment) == self.max_len\n","        assert len(neg_segment) == self.max_len\n","\n","\n","        cur_tensors = (torch.tensor(attributes, dtype=torch.long),\n","                       torch.tensor(masked_item_sequence, dtype=torch.long),\n","                       torch.tensor(pos_items, dtype=torch.long),\n","                       torch.tensor(neg_items, dtype=torch.long),\n","                       torch.tensor(masked_segment_sequence, dtype=torch.long),\n","                       torch.tensor(pos_segment, dtype=torch.long),\n","                       torch.tensor(neg_segment, dtype=torch.long),)\n","        return cur_tensors\n","\n","class SASRecDataset(Dataset):\n","\n","    def __init__(self, args, user_seq, test_neg_items=None, data_type='train'):\n","        self.args = args\n","        self.user_seq = user_seq\n","        self.test_neg_items = test_neg_items\n","        self.data_type = data_type\n","        self.max_len = args.max_seq_length\n","\n","    def __getitem__(self, index):\n","\n","        user_id = index\n","        items = self.user_seq[index]\n","\n","        assert self.data_type in {\"train\", \"valid\", \"test\"}\n","\n","        # [0, 1, 2, 3, 4, 5, 6]\n","        # train [0, 1, 2, 3]\n","        # target [1, 2, 3, 4]\n","\n","        # valid [0, 1, 2, 3, 4]\n","        # answer [5]\n","\n","        # test [0, 1, 2, 3, 4, 5]\n","        # answer [6]\n","        if self.data_type == \"train\":\n","            input_ids = items[:-3]\n","            target_pos = items[1:-2]\n","            answer = [0] # no use\n","\n","        elif self.data_type == 'valid':\n","            input_ids = items[:-2]\n","            target_pos = items[1:-1]\n","            answer = [items[-2]]\n","\n","        else:\n","            input_ids = items[:-1]\n","            target_pos = items[1:]\n","            answer = [items[-1]]\n","\n","        target_neg = []\n","        seq_set = set(items)\n","        for _ in input_ids:\n","            target_neg.append(random_neg_sample(seq_set, self.args.item_size))\n","\n","        pad_len = self.max_len - len(input_ids)\n","        input_ids = [0] * pad_len + input_ids\n","        target_pos = [0] * pad_len + target_pos\n","        target_neg = [0] * pad_len + target_neg\n","\n","        input_ids = input_ids[-self.max_len:]\n","        target_pos = target_pos[-self.max_len:]\n","        target_neg = target_neg[-self.max_len:]\n","\n","        assert len(input_ids) == self.max_len\n","        assert len(target_pos) == self.max_len\n","        assert len(target_neg) == self.max_len\n","\n","        if self.test_neg_items is not None:\n","            test_samples = self.test_neg_items[index]\n","\n","            cur_tensors = (\n","                torch.tensor(user_id, dtype=torch.long), # user_id for testing\n","                torch.tensor(input_ids, dtype=torch.long),\n","                torch.tensor(target_pos, dtype=torch.long),\n","                torch.tensor(target_neg, dtype=torch.long),\n","                torch.tensor(answer, dtype=torch.long),\n","                torch.tensor(test_samples, dtype=torch.long),\n","            )\n","        else:\n","            cur_tensors = (\n","                torch.tensor(user_id, dtype=torch.long),  # user_id for testing\n","                torch.tensor(input_ids, dtype=torch.long),\n","                torch.tensor(target_pos, dtype=torch.long),\n","                torch.tensor(target_neg, dtype=torch.long),\n","                torch.tensor(answer, dtype=torch.long),\n","            )\n","\n","        return cur_tensors\n","\n","    def __len__(self):\n","        return len(self.user_seq)\n","\n","# REPLACE WITH from recohut.datasets.bases.sequential import SASRecDataset, SASRecDataModule"],"metadata":{"id":"xFzPCZMG1xvD"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Modules"],"metadata":{"id":"lS3yAbXw112p"}},{"cell_type":"code","source":["import numpy as np\n","\n","import copy\n","import math\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F"],"metadata":{"id":"QytHYa1y12sf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def GELU(x):\n","    \"\"\"Implementation of the GELU activation function.\n","        For information: OpenAI GPT's GELU is slightly different\n","        (and gives slightly different results):\n","        0.5 * x * (1 + torch.tanh(math.sqrt(2 / math.pi) *\n","        (x + 0.044715 * torch.pow(x, 3))))\n","    \"\"\"\n","    return x * 0.5 * (1.0 + torch.erf(x / math.sqrt(2.0)))\n","\n","# REPLACE WITH from recohut.models.layers.activation import GELU\n","\n","def swish(x):\n","    return x * torch.sigmoid(x)\n","\n","# REPLACE WITH from recohut.models.layers.activation import swish\n","\n","def wasserstein_distance(mean1, cov1, mean2, cov2):\n","    ret = torch.sum((mean1 - mean2) * (mean1 - mean2), -1)\n","    cov1_sqrt = torch.sqrt(torch.clamp(cov1, min=1e-24)) \n","    cov2_sqrt = torch.sqrt(torch.clamp(cov2, min=1e-24))\n","    ret = ret + torch.sum((cov1_sqrt - cov2_sqrt) * (cov1_sqrt - cov2_sqrt), -1)\n","\n","    return ret\n","\n","# REPLACE WITH from recohut.utils.stats import wasserstein_distance\n","\n","def wasserstein_distance_matmul(mean1, cov1, mean2, cov2):\n","    mean1_2 = torch.sum(mean1**2, -1, keepdim=True)\n","    mean2_2 = torch.sum(mean2**2, -1, keepdim=True)\n","    ret = -2 * torch.matmul(mean1, mean2.transpose(-1, -2)) + mean1_2 + mean2_2.transpose(-1, -2)\n","    #ret = torch.clamp(-2 * torch.matmul(mean1, mean2.transpose(-1, -2)) + mean1_2 + mean2_2.transpose(-1, -2), min=1e-24)\n","    #ret = torch.sqrt(ret)\n","\n","    cov1_2 = torch.sum(cov1, -1, keepdim=True)\n","    cov2_2 = torch.sum(cov2, -1, keepdim=True)\n","    #cov_ret = torch.clamp(-2 * torch.matmul(torch.sqrt(torch.clamp(cov1, min=1e-24)), torch.sqrt(torch.clamp(cov2, min=1e-24)).transpose(-1, -2)) + cov1_2 + cov2_2.transpose(-1, -2), min=1e-24)\n","    #cov_ret = torch.sqrt(cov_ret)\n","    cov_ret = -2 * torch.matmul(torch.sqrt(torch.clamp(cov1, min=1e-24)), torch.sqrt(torch.clamp(cov2, min=1e-24)).transpose(-1, -2)) + cov1_2 + cov2_2.transpose(-1, -2)\n","\n","    return ret + cov_ret\n","\n","# REPLACE WITH from recohut.utils.stats import wasserstein_distance_matmul\n","\n","def kl_distance(mean1, cov1, mean2, cov2):\n","    trace_part = torch.sum(cov1 / cov2, -1)\n","    mean_cov_part = torch.sum((mean2 - mean1) / cov2 * (mean2 - mean1), -1)\n","    determinant_part = torch.log(torch.prod(cov2, -1) / torch.prod(cov1, -1))\n","\n","    return (trace_part + mean_cov_part - mean1.shape[1] + determinant_part) / 2\n","\n","# REPLACE WITH from recohut.utils.stats import kl_distance\n","\n","def kl_distance_matmul(mean1, cov1, mean2, cov2):\n","    cov1_det = 1 / torch.prod(cov1, -1, keepdim=True)\n","    cov2_det = torch.prod(cov2, -1, keepdim=True)\n","    log_det = torch.log(torch.matmul(cov1_det, cov2_det.transpose(-1, -2)))\n","\n","    trace_sum = torch.matmul(1 / cov2, cov1.transpose(-1, -2))\n","\n","    #mean_cov_part1 = torch.matmul(mean1 / cov2, mean1.transpose(-1, -2))\n","    #mean_cov_part1 = torch.matmul(mean1 * mean1, (1 / cov2).transpose(-1, -2))\n","    #mean_cov_part2 = -torch.matmul(mean1 / cov2, mean2.transpose(-1, -2))\n","    #mean_cov_part2 = -torch.matmul(mean1 * mean2, (1 / cov2).transpose(-1, -2))\n","    #mean_cov_part3 = -torch.matmul(mean2 / cov2, mean1.transpose(-1, -2))\n","    #mean_cov_part4 = torch.matmul(mean2 / cov2, mean2.transpose(-1, -2))\n","    #mean_cov_part4 = torch.matmul(mean2 * mean2, (1 / cov2).transpose(-1, -2))\n","\n","    #mean_cov_part = mean_cov_part1 + mean_cov_part2 + mean_cov_part3 + mean_cov_part4\n","    mean_cov_part = torch.matmul((mean1 - mean2) ** 2, (1/cov2).transpose(-1, -2))\n","\n","    return (log_det + mean_cov_part + trace_sum - mean1.shape[-1]) / 2\n","\n","# REPLACE WITH from recohut.utils.stats import kl_distance_matmul\n","\n","def d2s_gaussiannormal(distance, gamma):\n","\n","    return torch.exp(-gamma*distance)\n","\n","def d2s_1overx(distance):\n","\n","    return 1/(1+distance)\n","\n","\n","ACT2FN = {\"GELU\": GELU, \"relu\": F.relu, \"swish\": swish}\n","\n","\n","class LayerNorm(nn.Module):\n","    def __init__(self, hidden_size, eps=1e-12):\n","        \"\"\"Construct a layernorm module in the TF style (epsilon inside the square root).\n","        \"\"\"\n","        super(LayerNorm, self).__init__()\n","        self.weight = nn.Parameter(torch.ones(hidden_size))\n","        self.bias = nn.Parameter(torch.zeros(hidden_size))\n","        self.variance_epsilon = eps\n","\n","    def forward(self, x):\n","        u = x.mean(-1, keepdim=True)\n","        s = (x - u).pow(2).mean(-1, keepdim=True)\n","        x = (x - u) / torch.sqrt(s + self.variance_epsilon)\n","        return self.weight * x + self.bias\n","\n","# REPLACE WITH from recohut.models.layers.common import LayerNorm\n","\n","class ItemPosEmbedding(nn.Module):\n","    \"\"\"Construct the embeddings from item, position.\n","    \"\"\"\n","    def __init__(self, args):\n","        super().__init__()\n","\n","        self.item_embeddings = nn.Embedding(args.item_size, args.hidden_size, padding_idx=0)\n","        self.position_embeddings = nn.Embedding(args.max_seq_length, args.hidden_size)\n","\n","        self.LayerNorm = LayerNorm(args.hidden_size, eps=1e-12)\n","        self.dropout = nn.Dropout(args.hidden_dropout_prob)\n","\n","        self.args = args\n","\n","    def forward(self, input_ids):\n","        seq_length = input_ids.size(1)\n","        position_ids = torch.arange(seq_length, dtype=torch.long, device=input_ids.device)\n","        position_ids = position_ids.unsqueeze(0).expand_as(input_ids)\n","        items_embeddings = self.item_embeddings(input_ids)\n","        position_embeddings = self.position_embeddings(position_ids)\n","        embeddings = items_embeddings + position_embeddings\n","        embeddings = self.LayerNorm(embeddings)\n","        embeddings = self.dropout(embeddings)\n","        return embeddings\n","\n","# REPLACE WITH from recohut.models.layers.common import ItemPosEmbedding\n","\n","class SelfAttention(nn.Module):\n","    def __init__(self, args):\n","        super().__init__()\n","        if args.hidden_size % args.num_attention_heads != 0:\n","            raise ValueError(\n","                \"The hidden size (%d) is not a multiple of the number of attention \"\n","                \"heads (%d)\" % (args.hidden_size, args.num_attention_heads))\n","        self.num_attention_heads = args.num_attention_heads\n","        self.attention_head_size = int(args.hidden_size / args.num_attention_heads)\n","        self.all_head_size = self.num_attention_heads * self.attention_head_size\n","\n","        self.query = nn.Linear(args.hidden_size, self.all_head_size)\n","        self.key = nn.Linear(args.hidden_size, self.all_head_size)\n","        self.value = nn.Linear(args.hidden_size, self.all_head_size)\n","\n","        self.attn_dropout = nn.Dropout(args.attention_probs_dropout_prob)\n","\n","        self.dense = nn.Linear(args.hidden_size, args.hidden_size)\n","        self.LayerNorm = LayerNorm(args.hidden_size, eps=1e-12)\n","        self.out_dropout = nn.Dropout(args.hidden_dropout_prob)\n","\n","    def transpose_for_scores(self, x):\n","        new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)\n","        x = x.view(*new_x_shape)\n","        return x.permute(0, 2, 1, 3)\n","\n","    def forward(self, input_tensor, attention_mask):\n","        mixed_query_layer = self.query(input_tensor)\n","        mixed_key_layer = self.key(input_tensor)\n","        mixed_value_layer = self.value(input_tensor)\n","\n","        query_layer = self.transpose_for_scores(mixed_query_layer)\n","        key_layer = self.transpose_for_scores(mixed_key_layer)\n","        value_layer = self.transpose_for_scores(mixed_value_layer)\n","\n","        # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n","        attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n","\n","        attention_scores = attention_scores / math.sqrt(self.attention_head_size)\n","        # Apply the attention mask is (precomputed for all layers in BertModel forward() function)\n","        # [batch_size heads seq_len seq_len] scores\n","        # [batch_size 1 1 seq_len]\n","        attention_scores = attention_scores + attention_mask\n","\n","        # Normalize the attention scores to probabilities.\n","        attention_probs = nn.Softmax(dim=-1)(attention_scores)\n","        # This is actually dropping out entire tokens to attend to, which might\n","        # seem a bit unusual, but is taken from the original Transformer paper.\n","        # Fixme\n","        attention_probs = self.attn_dropout(attention_probs)\n","        context_layer = torch.matmul(attention_probs, value_layer)\n","        context_layer = context_layer.permute(0, 2, 1, 3).contiguous()\n","        new_context_layer_shape = context_layer.size()[:-2] + (self.all_head_size,)\n","        context_layer = context_layer.view(*new_context_layer_shape)\n","        hidden_states = self.dense(context_layer)\n","        hidden_states = self.out_dropout(hidden_states)\n","        hidden_states = self.LayerNorm(hidden_states + input_tensor)\n","\n","        return hidden_states, attention_probs\n","\n","# REPLACE WITH from recohut.models.layers.common import SelfAttention\n","\n","class DistSelfAttention(nn.Module):\n","    def __init__(self, args):\n","        super(DistSelfAttention, self).__init__()\n","        if args.hidden_size % args.num_attention_heads != 0:\n","            raise ValueError(\n","                \"The hidden size (%d) is not a multiple of the number of attention \"\n","                \"heads (%d)\" % (args.hidden_size, args.num_attention_heads))\n","        self.num_attention_heads = args.num_attention_heads\n","        self.attention_head_size = int(args.hidden_size / args.num_attention_heads)\n","        self.all_head_size = self.num_attention_heads * self.attention_head_size\n","\n","        self.mean_query = nn.Linear(args.hidden_size, self.all_head_size)\n","        self.cov_query = nn.Linear(args.hidden_size, self.all_head_size)\n","        self.mean_key = nn.Linear(args.hidden_size, self.all_head_size)\n","        self.cov_key = nn.Linear(args.hidden_size, self.all_head_size)\n","        self.mean_value = nn.Linear(args.hidden_size, self.all_head_size)\n","        self.cov_value = nn.Linear(args.hidden_size, self.all_head_size)\n","\n","        self.activation = nn.ELU()\n","\n","        self.attn_dropout = nn.Dropout(args.attention_probs_dropout_prob)\n","        self.mean_dense = nn.Linear(args.hidden_size, args.hidden_size)\n","        self.cov_dense = nn.Linear(args.hidden_size, args.hidden_size)\n","        self.out_dropout = nn.Dropout(args.hidden_dropout_prob)\n","\n","        self.distance_metric = args.distance_metric\n","        self.LayerNorm = LayerNorm(args.hidden_size, eps=1e-12)\n","        self.gamma = args.kernel_param\n","\n","\n","    def transpose_for_scores(self, x):\n","        new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)\n","        x = x.view(*new_x_shape)\n","        return x.permute(0, 2, 1, 3)\n","\n","    def forward(self, input_mean_tensor, input_cov_tensor, attention_mask):\n","        mixed_mean_query_layer = self.mean_query(input_mean_tensor)\n","        mixed_mean_key_layer = self.mean_key(input_mean_tensor)\n","        mixed_mean_value_layer = self.mean_value(input_mean_tensor)\n","\n","        mean_query_layer = self.transpose_for_scores(mixed_mean_query_layer)\n","        mean_key_layer = self.transpose_for_scores(mixed_mean_key_layer)\n","        mean_value_layer = self.transpose_for_scores(mixed_mean_value_layer)\n","\n","        mixed_cov_query_layer = self.activation(self.cov_query(input_cov_tensor)) + 1\n","        mixed_cov_key_layer = self.activation(self.cov_key(input_cov_tensor)) + 1\n","        mixed_cov_value_layer = self.activation(self.cov_value(input_cov_tensor)) + 1\n","\n","        cov_query_layer = self.transpose_for_scores(mixed_cov_query_layer)\n","        cov_key_layer = self.transpose_for_scores(mixed_cov_key_layer)\n","        cov_value_layer = self.transpose_for_scores(mixed_cov_value_layer)\n","\n","        #if self.distance_metric == 'wasserstein':\n","        #    attention_scores = d2s_gaussiannormal(wasserstein_distance(mean_query_layer, cov_query_layer, mean_key_layer, cov_key_layer))\n","        #else:\n","        #    attention_scores = d2s_gaussiannormal(kl_distance(mean_query_layer, cov_query_layer, mean_key_layer, cov_key_layer))\n","        #attention_scores = d2s_gaussiannormal(wasserstein_distance_matmul(mean_query_layer, cov_query_layer, mean_key_layer, cov_key_layer))\n","        if self.distance_metric == 'wasserstein':\n","            #attention_scores = d2s_gaussiannormal(wasserstein_distance_matmul(mean_query_layer, cov_query_layer, mean_key_layer, cov_key_layer), self.gamma)\n","            attention_scores = -wasserstein_distance_matmul(mean_query_layer, cov_query_layer, mean_key_layer, cov_key_layer)\n","        else:\n","            attention_scores = -kl_distance_matmul(mean_query_layer, cov_query_layer, mean_key_layer, cov_key_layer)\n","\n","        attention_scores = attention_scores / math.sqrt(self.attention_head_size)\n","        attention_scores = attention_scores + attention_mask\n","        attention_probs = nn.Softmax(dim=-1)(attention_scores)\n","\n","        attention_probs = self.attn_dropout(attention_probs)\n","        mean_context_layer = torch.matmul(attention_probs, mean_value_layer)\n","        cov_context_layer = torch.matmul(attention_probs ** 2, cov_value_layer)\n","        mean_context_layer = mean_context_layer.permute(0, 2, 1, 3).contiguous()\n","        cov_context_layer = cov_context_layer.permute(0, 2, 1, 3).contiguous()\n","        new_context_layer_shape = mean_context_layer.size()[:-2] + (self.all_head_size,)\n","\n","        mean_context_layer = mean_context_layer.view(*new_context_layer_shape)\n","        cov_context_layer = cov_context_layer.view(*new_context_layer_shape)\n","\n","        mean_hidden_states = self.mean_dense(mean_context_layer)\n","        mean_hidden_states = self.out_dropout(mean_hidden_states)\n","        mean_hidden_states = self.LayerNorm(mean_hidden_states + input_mean_tensor)\n","\n","        cov_hidden_states = self.cov_dense(cov_context_layer)\n","        cov_hidden_states = self.out_dropout(cov_hidden_states)\n","        cov_hidden_states = self.LayerNorm(cov_hidden_states + input_cov_tensor)\n","\n","        return mean_hidden_states, cov_hidden_states, attention_probs\n","\n","# REPLACE WITH from recohut.models.layers.common import DistSelfAttention\n","\n","class DistMeanSelfAttention(nn.Module):\n","    def __init__(self, args):\n","        super(DistMeanSelfAttention, self).__init__()\n","        if args.hidden_size % args.num_attention_heads != 0:\n","            raise ValueError(\n","                \"The hidden size (%d) is not a multiple of the number of attention \"\n","                \"heads (%d)\" % (args.hidden_size, args.num_attention_heads))\n","        self.num_attention_heads = args.num_attention_heads\n","        self.attention_head_size = int(args.hidden_size / args.num_attention_heads)\n","        self.all_head_size = self.num_attention_heads * self.attention_head_size\n","\n","        self.mean_query = nn.Linear(args.hidden_size, self.all_head_size)\n","        self.mean_key = nn.Linear(args.hidden_size, self.all_head_size)\n","        self.mean_value = nn.Linear(args.hidden_size, self.all_head_size)\n","        self.cov_key = nn.Linear(args.hidden_size, self.all_head_size)\n","        self.cov_query = nn.Linear(args.hidden_size, self.all_head_size)\n","        self.cov_value = nn.Linear(args.hidden_size, self.all_head_size)\n","\n","        self.activation = nn.ELU()\n","\n","        self.attn_dropout = nn.Dropout(args.attention_probs_dropout_prob)\n","        self.mean_dense = nn.Linear(args.hidden_size, args.hidden_size)\n","        self.cov_dense = nn.Linear(args.hidden_size, args.hidden_size)\n","        self.out_dropout = nn.Dropout(args.hidden_dropout_prob)\n","\n","        self.LayerNorm = LayerNorm(args.hidden_size, eps=1e-12)\n","\n","\n","    def transpose_for_scores(self, x):\n","        new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)\n","        x = x.view(*new_x_shape)\n","        return x.permute(0, 2, 1, 3)\n","\n","    def forward(self, input_mean_tensor, input_cov_tensor, attention_mask):\n","        mixed_mean_query_layer = self.mean_query(input_mean_tensor)\n","        mixed_mean_key_layer = self.mean_key(input_mean_tensor)\n","        mixed_mean_value_layer = self.mean_value(input_mean_tensor)\n","\n","        mean_query_layer = self.transpose_for_scores(mixed_mean_query_layer)\n","        mean_key_layer = self.transpose_for_scores(mixed_mean_key_layer)\n","        mean_value_layer = self.transpose_for_scores(mixed_mean_value_layer)\n","\n","        mixed_cov_query_layer = self.activation(self.cov_query(input_cov_tensor)) + 1\n","        mixed_cov_key_layer = self.activation(self.cov_key(input_cov_tensor)) + 1\n","        mixed_cov_value_layer = self.activation(self.cov_value(input_cov_tensor)) + 1\n","\n","        cov_query_layer = self.transpose_for_scores(mixed_cov_query_layer)\n","        cov_key_layer = self.transpose_for_scores(mixed_cov_key_layer)\n","        cov_value_layer = self.transpose_for_scores(mixed_cov_value_layer)\n","\n","        mean_attention_scores = torch.matmul(mean_query_layer, mean_key_layer.transpose(-1, -2))\n","        cov_attention_scores = torch.matmul(cov_query_layer, cov_key_layer.transpose(-1, -2))\n","\n","        mean_attention_scores = mean_attention_scores / math.sqrt(self.attention_head_size)\n","        mean_attention_scores = mean_attention_scores + attention_mask\n","        mean_attention_probs = nn.Softmax(dim=-1)(mean_attention_scores)\n","\n","        cov_attention_scores = cov_attention_scores / math.sqrt(self.attention_head_size)\n","        cov_attention_scores = cov_attention_scores + attention_mask\n","        cov_attention_probs = nn.Softmax(dim=-1)(cov_attention_scores)\n","\n","        mean_attention_probs = self.attn_dropout(mean_attention_probs)\n","        cov_attention_probs = self.attn_dropout(cov_attention_probs)\n","        mean_context_layer = torch.matmul(mean_attention_probs, mean_value_layer)\n","        cov_context_layer = torch.matmul(cov_attention_probs, cov_value_layer)\n","        mean_context_layer = mean_context_layer.permute(0, 2, 1, 3).contiguous()\n","        cov_context_layer = cov_context_layer.permute(0, 2, 1, 3).contiguous()\n","        new_context_layer_shape = mean_context_layer.size()[:-2] + (self.all_head_size,)\n","\n","        mean_context_layer = mean_context_layer.view(*new_context_layer_shape)\n","        cov_context_layer = cov_context_layer.view(*new_context_layer_shape)\n","\n","        mean_hidden_states = self.mean_dense(mean_context_layer)\n","        mean_hidden_states = self.out_dropout(mean_hidden_states)\n","        mean_hidden_states = self.LayerNorm(mean_hidden_states + input_mean_tensor)\n","\n","        cov_hidden_states = self.cov_dense(cov_context_layer)\n","        cov_hidden_states = self.out_dropout(cov_hidden_states)\n","        cov_hidden_states = self.LayerNorm(cov_hidden_states + input_cov_tensor)\n","\n","        return mean_hidden_states, cov_hidden_states, mean_attention_probs\n","\n","# REPLACE WITH from recohut.models.layers.common import DistMeanSelfAttention\n","\n","class Intermediate(nn.Module):\n","    def __init__(self, args):\n","        super(Intermediate, self).__init__()\n","        self.dense_1 = nn.Linear(args.hidden_size, args.hidden_size * 4)\n","        if isinstance(args.hidden_act, str):\n","            self.intermediate_act_fn = ACT2FN[args.hidden_act]\n","        else:\n","            self.intermediate_act_fn = args.hidden_act\n","\n","        self.dense_2 = nn.Linear(args.hidden_size * 4, args.hidden_size)\n","        self.LayerNorm = LayerNorm(args.hidden_size, eps=1e-12)\n","        self.dropout = nn.Dropout(args.hidden_dropout_prob)\n","\n","\n","    def forward(self, input_tensor):\n","\n","        hidden_states = self.dense_1(input_tensor)\n","        hidden_states = self.intermediate_act_fn(hidden_states)\n","\n","        hidden_states = self.dense_2(hidden_states)\n","        hidden_states = self.dropout(hidden_states)\n","        hidden_states = self.LayerNorm(hidden_states + input_tensor)\n","\n","        return hidden_states\n","\n","\n","class DistIntermediate(nn.Module):\n","    def __init__(self, args):\n","        super(DistIntermediate, self).__init__()\n","        self.dense_1 = nn.Linear(args.hidden_size, args.hidden_size * 4)\n","        self.intermediate_act_fn = nn.ELU()\n","\n","        self.dense_2 = nn.Linear(args.hidden_size * 4, args.hidden_size)\n","        self.LayerNorm = LayerNorm(args.hidden_size, eps=1e-12)\n","        self.dropout = nn.Dropout(args.hidden_dropout_prob)\n","\n","\n","    def forward(self, input_tensor):\n","\n","        hidden_states = self.dense_1(input_tensor)\n","        hidden_states = self.intermediate_act_fn(hidden_states)\n","\n","        hidden_states = self.dense_2(hidden_states)\n","        hidden_states = self.dropout(hidden_states)\n","        hidden_states = self.LayerNorm(hidden_states + input_tensor)\n","\n","        return hidden_states\n","\n","\n","class Layer(nn.Module):\n","    def __init__(self, args):\n","        super(Layer, self).__init__()\n","        self.attention = SelfAttention(args)\n","        self.intermediate = Intermediate(args)\n","\n","    def forward(self, hidden_states, attention_mask):\n","        attention_output, attention_scores = self.attention(hidden_states, attention_mask)\n","        intermediate_output = self.intermediate(attention_output)\n","        return intermediate_output, attention_scores\n","\n","\n","class DistLayer(nn.Module):\n","    def __init__(self, args):\n","        super(DistLayer, self).__init__()\n","        self.attention = DistSelfAttention(args)\n","        self.mean_intermediate = DistIntermediate(args)\n","        self.cov_intermediate = DistIntermediate(args)\n","        self.activation_func = nn.ELU()\n","\n","    def forward(self, mean_hidden_states, cov_hidden_states, attention_mask):\n","        mean_attention_output, cov_attention_output, attention_scores = self.attention(mean_hidden_states, cov_hidden_states, attention_mask)\n","        mean_intermediate_output = self.mean_intermediate(mean_attention_output)\n","        cov_intermediate_output = self.activation_func(self.cov_intermediate(cov_attention_output)) + 1\n","        return mean_intermediate_output, cov_intermediate_output, attention_scores\n","\n","\n","class DistMeanSALayer(nn.Module):\n","    def __init__(self, args):\n","        super(DistMeanSALayer, self).__init__()\n","        self.attention = DistMeanSelfAttention(args)\n","        self.mean_intermediate = DistIntermediate(args)\n","        self.cov_intermediate = DistIntermediate(args)\n","        self.activation_func = nn.ELU()\n","\n","    def forward(self, mean_hidden_states, cov_hidden_states, attention_mask):\n","        mean_attention_output, cov_attention_output, attention_scores = self.attention(mean_hidden_states, cov_hidden_states, attention_mask)\n","        mean_intermediate_output = self.mean_intermediate(mean_attention_output)\n","        cov_intermediate_output = self.activation_func(self.cov_intermediate(cov_attention_output)) + 1\n","        return mean_intermediate_output, cov_intermediate_output, attention_scores\n","\n","\n","class DistSAEncoder(nn.Module):               \n","    def __init__(self, args):\n","        super(DistSAEncoder, self).__init__()\n","        layer = DistLayer(args)\n","        self.layer = nn.ModuleList([copy.deepcopy(layer)\n","                                    for _ in range(args.num_hidden_layers)])\n","\n","    def forward(self, mean_hidden_states, cov_hidden_states, attention_mask, output_all_encoded_layers=True):\n","        all_encoder_layers = []\n","        for layer_module in self.layer:\n","            maen_hidden_states, cov_hidden_states, att_scores = layer_module(mean_hidden_states, cov_hidden_states, attention_mask)\n","            if output_all_encoded_layers:\n","                all_encoder_layers.append([mean_hidden_states, cov_hidden_states, att_scores])\n","        if not output_all_encoded_layers:\n","            all_encoder_layers.append([mean_hidden_states, cov_hidden_states, att_scores])\n","        return all_encoder_layers\n","\n","\n","class DistMeanSAEncoder(nn.Module):\n","    def __init__(self, args):\n","        super(DistMeanSAEncoder, self).__init__()\n","        layer = DistMeanSALayer(args)\n","        self.layer = nn.ModuleList([copy.deepcopy(layer)\n","                                    for _ in range(args.num_hidden_layers)])\n","\n","    def forward(self, mean_hidden_states, cov_hidden_states, attention_mask, output_all_encoded_layers=True):\n","        all_encoder_layers = []\n","        for layer_module in self.layer:\n","            maen_hidden_states, cov_hidden_states, att_scores = layer_module(mean_hidden_states, cov_hidden_states, attention_mask)\n","            if output_all_encoded_layers:\n","                all_encoder_layers.append([mean_hidden_states, cov_hidden_states, att_scores])\n","        if not output_all_encoded_layers:\n","            all_encoder_layers.append([mean_hidden_states, cov_hidden_states, att_scores])\n","        return all_encoder_layers\n","\n","class Encoder(nn.Module):\n","    def __init__(self, args):\n","        super(Encoder, self).__init__()\n","        layer = Layer(args)\n","        self.layer = nn.ModuleList([copy.deepcopy(layer)\n","                                    for _ in range(args.num_hidden_layers)])\n","\n","    def forward(self, hidden_states, attention_mask, output_all_encoded_layers=True):\n","        all_encoder_layers = []\n","        for layer_module in self.layer:\n","            hidden_states, attention_scores = layer_module(hidden_states, attention_mask)\n","            if output_all_encoded_layers:\n","                all_encoder_layers.append([hidden_states, attention_scores])\n","        if not output_all_encoded_layers:\n","            all_encoder_layers.append([hidden_states, attention_scores])\n","        return all_encoder_layers"],"metadata":{"id":"tSNsav7z14xi"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Models"],"metadata":{"id":"-utPUH001zjb"}},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","# from modules import Encoder, LayerNorm"],"metadata":{"id":"GhNYlWIo1-Hc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class S3RecModel(nn.Module):\n","    def __init__(self, args):\n","        super(S3RecModel, self).__init__()\n","        self.item_embeddings = nn.Embedding(args.item_size, args.hidden_size, padding_idx=0)\n","        self.attribute_embeddings = nn.Embedding(args.attribute_size, args.hidden_size, padding_idx=0)\n","        self.position_embeddings = nn.Embedding(args.max_seq_length, args.hidden_size)\n","        self.item_encoder = Encoder(args)\n","        self.LayerNorm = LayerNorm(args.hidden_size, eps=1e-12)\n","        self.dropout = nn.Dropout(args.hidden_dropout_prob)\n","        self.args = args\n","\n","        # add unique dense layer for 4 losses respectively\n","        self.aap_norm = nn.Linear(args.hidden_size, args.hidden_size)\n","        self.mip_norm = nn.Linear(args.hidden_size, args.hidden_size)\n","        self.map_norm = nn.Linear(args.hidden_size, args.hidden_size)\n","        self.sp_norm = nn.Linear(args.hidden_size, args.hidden_size)\n","        self.criterion = nn.BCELoss(reduction='none')\n","        self.apply(self.init_weights)\n","\n","    # AAP\n","    def associated_attribute_prediction(self, sequence_output, attribute_embedding):\n","        '''\n","        :param sequence_output: [B L H]\n","        :param attribute_embedding: [arribute_num H]\n","        :return: scores [B*L tag_num]\n","        '''\n","        sequence_output = self.aap_norm(sequence_output) # [B L H]\n","        sequence_output = sequence_output.view([-1, self.args.hidden_size, 1]) # [B*L H 1]\n","        # [tag_num H] [B*L H 1] -> [B*L tag_num 1]\n","        score = torch.matmul(attribute_embedding, sequence_output)\n","        return torch.sigmoid(score.squeeze(-1)) # [B*L tag_num]\n","\n","    # MIP sample neg items\n","    def masked_item_prediction(self, sequence_output, target_item):\n","        '''\n","        :param sequence_output: [B L H]\n","        :param target_item: [B L H]\n","        :return: scores [B*L]\n","        '''\n","        sequence_output = self.mip_norm(sequence_output.view([-1,self.args.hidden_size])) # [B*L H]\n","        target_item = target_item.view([-1,self.args.hidden_size]) # [B*L H]\n","        score = torch.mul(sequence_output, target_item) # [B*L H]\n","        return torch.sigmoid(torch.sum(score, -1)) # [B*L]\n","\n","    # MAP\n","    def masked_attribute_prediction(self, sequence_output, attribute_embedding):\n","        sequence_output = self.map_norm(sequence_output)  # [B L H]\n","        sequence_output = sequence_output.view([-1, self.args.hidden_size, 1])  # [B*L H 1]\n","        # [tag_num H] [B*L H 1] -> [B*L tag_num 1]\n","        score = torch.matmul(attribute_embedding, sequence_output)\n","        return torch.sigmoid(score.squeeze(-1)) # [B*L tag_num]\n","\n","    # SP sample neg segment\n","    def segment_prediction(self, context, segment):\n","        '''\n","        :param context: [B H]\n","        :param segment: [B H]\n","        :return:\n","        '''\n","        context = self.sp_norm(context)\n","        score = torch.mul(context, segment) # [B H]\n","        return torch.sigmoid(torch.sum(score, dim=-1)) # [B]\n","\n","    #\n","    def add_position_embedding(self, sequence):\n","\n","        seq_length = sequence.size(1)\n","        position_ids = torch.arange(seq_length, dtype=torch.long, device=sequence.device)\n","        position_ids = position_ids.unsqueeze(0).expand_as(sequence)\n","        item_embeddings = self.item_embeddings(sequence)\n","        position_embeddings = self.position_embeddings(position_ids)\n","        sequence_emb = item_embeddings + position_embeddings\n","        sequence_emb = self.LayerNorm(sequence_emb)\n","        sequence_emb = self.dropout(sequence_emb)\n","\n","        return sequence_emb\n","\n","    def pretrain(self, attributes, masked_item_sequence, pos_items,  neg_items,\n","                  masked_segment_sequence, pos_segment, neg_segment):\n","\n","        # Encode masked sequence\n","        sequence_emb = self.add_position_embedding(masked_item_sequence)\n","        sequence_mask = (masked_item_sequence == 0).float() * -1e8\n","        sequence_mask = torch.unsqueeze(torch.unsqueeze(sequence_mask, 1), 1)\n","\n","        encoded_layers = self.item_encoder(sequence_emb,\n","                                          sequence_mask,\n","                                          output_all_encoded_layers=True)\n","        # [B L H]\n","        sequence_output = encoded_layers[-1]\n","\n","        attribute_embeddings = self.attribute_embeddings.weight\n","        # AAP\n","        aap_score = self.associated_attribute_prediction(sequence_output, attribute_embeddings)\n","        aap_loss = self.criterion(aap_score, attributes.view(-1, self.args.attribute_size).float())\n","        # only compute loss at non-masked position\n","        aap_mask = (masked_item_sequence != self.args.mask_id).float() * \\\n","                         (masked_item_sequence != 0).float()\n","        aap_loss = torch.sum(aap_loss * aap_mask.flatten().unsqueeze(-1))\n","\n","        # MIP\n","        pos_item_embs = self.item_embeddings(pos_items)\n","        neg_item_embs = self.item_embeddings(neg_items)\n","        pos_score = self.masked_item_prediction(sequence_output, pos_item_embs)\n","        neg_score = self.masked_item_prediction(sequence_output, neg_item_embs)\n","        mip_distance = torch.sigmoid(pos_score - neg_score)\n","        mip_loss = self.criterion(mip_distance, torch.ones_like(mip_distance, dtype=torch.float32))\n","        mip_mask = (masked_item_sequence == self.args.mask_id).float()\n","        mip_loss = torch.sum(mip_loss * mip_mask.flatten())\n","\n","        # MAP\n","        map_score = self.masked_attribute_prediction(sequence_output, attribute_embeddings)\n","        map_loss = self.criterion(map_score, attributes.view(-1, self.args.attribute_size).float())\n","        map_mask = (masked_item_sequence == self.args.mask_id).float()\n","        map_loss = torch.sum(map_loss * map_mask.flatten().unsqueeze(-1))\n","\n","        # SP\n","        # segment context\n","        segment_context = self.add_position_embedding(masked_segment_sequence)\n","        segment_mask = (masked_segment_sequence == 0).float() * -1e8\n","        segment_mask = torch.unsqueeze(torch.unsqueeze(segment_mask, 1), 1)\n","        segment_encoded_layers = self.item_encoder(segment_context,\n","                                               segment_mask,\n","                                               output_all_encoded_layers=True)\n","\n","        # take the last position hidden as the context\n","        segment_context = segment_encoded_layers[-1][:, -1, :]# [B H]\n","        # pos_segment\n","        pos_segment_emb = self.add_position_embedding(pos_segment)\n","        pos_segment_mask = (pos_segment == 0).float() * -1e8\n","        pos_segment_mask = torch.unsqueeze(torch.unsqueeze(pos_segment_mask, 1), 1)\n","        pos_segment_encoded_layers = self.item_encoder(pos_segment_emb,\n","                                                   pos_segment_mask,\n","                                                   output_all_encoded_layers=True)\n","        pos_segment_emb = pos_segment_encoded_layers[-1][:, -1, :]\n","\n","        # neg_segment\n","        neg_segment_emb = self.add_position_embedding(neg_segment)\n","        neg_segment_mask = (neg_segment == 0).float() * -1e8\n","        neg_segment_mask = torch.unsqueeze(torch.unsqueeze(neg_segment_mask, 1), 1)\n","        neg_segment_encoded_layers = self.item_encoder(neg_segment_emb,\n","                                                       neg_segment_mask,\n","                                                       output_all_encoded_layers=True)\n","        neg_segment_emb = neg_segment_encoded_layers[-1][:, -1, :] # [B H]\n","\n","        pos_segment_score = self.segment_prediction(segment_context, pos_segment_emb)\n","        neg_segment_score = self.segment_prediction(segment_context, neg_segment_emb)\n","\n","        sp_distance = torch.sigmoid(pos_segment_score - neg_segment_score)\n","\n","        sp_loss = torch.sum(self.criterion(sp_distance,\n","                                           torch.ones_like(sp_distance, dtype=torch.float32)))\n","\n","        return aap_loss, mip_loss, map_loss, sp_loss\n","\n","    # Fine tune\n","    # same as SASRec\n","    def finetune(self, input_ids):\n","\n","        attention_mask = (input_ids > 0).long()\n","        extended_attention_mask = attention_mask.unsqueeze(1).unsqueeze(2) # torch.int64\n","        max_len = attention_mask.size(-1)\n","        attn_shape = (1, max_len, max_len)\n","        subsequent_mask = torch.triu(torch.ones(attn_shape), diagonal=1) # torch.uint8\n","        subsequent_mask = (subsequent_mask == 0).unsqueeze(1)\n","        subsequent_mask = subsequent_mask.long()\n","\n","        if self.args.cuda_condition:\n","            subsequent_mask = subsequent_mask.cuda()\n","\n","        extended_attention_mask = extended_attention_mask * subsequent_mask\n","        extended_attention_mask = extended_attention_mask.to(dtype=next(self.parameters()).dtype) # fp16 compatibility\n","        extended_attention_mask = (1.0 - extended_attention_mask) * -10000.0\n","\n","        sequence_emb = self.add_position_embedding(input_ids)\n","\n","        item_encoded_layers = self.item_encoder(sequence_emb,\n","                                                extended_attention_mask,\n","                                                output_all_encoded_layers=True)\n","\n","        sequence_output = item_encoded_layers[-1]\n","        return sequence_output\n","\n","    def init_weights(self, module):\n","        \"\"\" Initialize the weights.\n","        \"\"\"\n","        if isinstance(module, (nn.Linear, nn.Embedding)):\n","            module.weight.data.normal_(mean=0.0, std=self.args.initializer_range)\n","        elif isinstance(module, LayerNorm):\n","            module.bias.data.zero_()\n","            module.weight.data.fill_(1.0)\n","        if isinstance(module, nn.Linear) and module.bias is not None:\n","            module.bias.data.zero_()"],"metadata":{"id":"HUiyJiHt2Aqg"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Seq Models"],"metadata":{"id":"T2pw42sq2Er3"}},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","# from modules import Encoder, LayerNorm, DistSAEncoder, DistMeanSAEncoder"],"metadata":{"id":"jK4hH0YB2Jn1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class SASRecModel(nn.Module):\n","    def __init__(self, args):\n","        super(SASRecModel, self).__init__()\n","        self.item_embeddings = nn.Embedding(args.item_size, args.hidden_size, padding_idx=0)\n","        self.position_embeddings = nn.Embedding(args.max_seq_length, args.hidden_size)\n","        self.item_encoder = Encoder(args)\n","        self.LayerNorm = LayerNorm(args.hidden_size, eps=1e-12)\n","        self.dropout = nn.Dropout(args.hidden_dropout_prob)\n","        self.args = args\n","\n","        self.criterion = nn.BCELoss(reduction='none')\n","        self.apply(self.init_weights)\n","\n","    def add_position_embedding(self, sequence):\n","\n","        seq_length = sequence.size(1)\n","        position_ids = torch.arange(seq_length, dtype=torch.long, device=sequence.device)\n","        position_ids = position_ids.unsqueeze(0).expand_as(sequence)\n","        item_embeddings = self.item_embeddings(sequence)\n","        position_embeddings = self.position_embeddings(position_ids)\n","        sequence_emb = item_embeddings + position_embeddings\n","        sequence_emb = self.LayerNorm(sequence_emb)\n","        sequence_emb = self.dropout(sequence_emb)\n","\n","        return sequence_emb\n","\n","\n","    def finetune(self, input_ids):\n","\n","        attention_mask = (input_ids > 0).long()\n","        extended_attention_mask = attention_mask.unsqueeze(1).unsqueeze(2) # torch.int64\n","        max_len = attention_mask.size(-1)\n","        attn_shape = (1, max_len, max_len)\n","        subsequent_mask = torch.triu(torch.ones(attn_shape), diagonal=1) # torch.uint8\n","        subsequent_mask = (subsequent_mask == 0).unsqueeze(1)\n","        subsequent_mask = subsequent_mask.long()\n","\n","        if self.args.cuda_condition:\n","            subsequent_mask = subsequent_mask.cuda()\n","\n","        extended_attention_mask = extended_attention_mask * subsequent_mask\n","        extended_attention_mask = extended_attention_mask.to(dtype=next(self.parameters()).dtype) # fp16 compatibility\n","        extended_attention_mask = (1.0 - extended_attention_mask) * -10000.0\n","\n","        sequence_emb = self.add_position_embedding(input_ids)\n","\n","        item_encoded_layers = self.item_encoder(sequence_emb,\n","                                                extended_attention_mask,\n","                                                output_all_encoded_layers=True)\n","\n","        sequence_output, attention_scores = item_encoded_layers[-1]\n","        return sequence_output, attention_scores\n","\n","    def init_weights(self, module):\n","        \"\"\" Initialize the weights.\n","        \"\"\"\n","        if isinstance(module, (nn.Linear, nn.Embedding)):\n","            # Slightly different from the TF version which uses truncated_normal for initialization\n","            # cf https://github.com/pytorch/pytorch/pull/5617\n","            module.weight.data.normal_(mean=0.0, std=self.args.initializer_range)\n","        elif isinstance(module, LayerNorm):\n","            module.bias.data.zero_()\n","            module.weight.data.fill_(1.0)\n","        if isinstance(module, nn.Linear) and module.bias is not None:\n","            module.bias.data.zero_()\n","\n","\n","class DistSAModel(nn.Module):\n","    def __init__(self, args):\n","        super(DistSAModel, self).__init__()\n","        self.item_mean_embeddings = nn.Embedding(args.item_size, args.hidden_size, padding_idx=0)\n","        self.item_cov_embeddings = nn.Embedding(args.item_size, args.hidden_size, padding_idx=0)\n","        self.position_mean_embeddings = nn.Embedding(args.max_seq_length, args.hidden_size)\n","        self.position_cov_embeddings = nn.Embedding(args.max_seq_length, args.hidden_size)\n","        self.user_margins = nn.Embedding(args.num_users, 1)\n","        self.item_encoder = DistSAEncoder(args)\n","        self.LayerNorm = LayerNorm(args.hidden_size, eps=1e-12)\n","        self.dropout = nn.Dropout(args.hidden_dropout_prob)\n","        self.args = args\n","\n","        self.apply(self.init_weights)\n","\n","    def add_position_mean_embedding(self, sequence):\n","\n","        seq_length = sequence.size(1)\n","        position_ids = torch.arange(seq_length, dtype=torch.long, device=sequence.device)\n","        position_ids = position_ids.unsqueeze(0).expand_as(sequence)\n","        item_embeddings = self.item_mean_embeddings(sequence)\n","        position_embeddings = self.position_mean_embeddings(position_ids)\n","        sequence_emb = item_embeddings + position_embeddings\n","        sequence_emb = self.LayerNorm(sequence_emb)\n","        sequence_emb = self.dropout(sequence_emb)\n","        elu_act = torch.nn.ELU()\n","        sequence_emb = elu_act(sequence_emb)\n","\n","        return sequence_emb\n","\n","    def add_position_cov_embedding(self, sequence):\n","\n","        seq_length = sequence.size(1)\n","        position_ids = torch.arange(seq_length, dtype=torch.long, device=sequence.device)\n","        position_ids = position_ids.unsqueeze(0).expand_as(sequence)\n","        item_embeddings = self.item_cov_embeddings(sequence)\n","        position_embeddings = self.position_cov_embeddings(position_ids)\n","        sequence_emb = item_embeddings + position_embeddings\n","        sequence_emb = self.LayerNorm(sequence_emb)\n","        elu_act = torch.nn.ELU()\n","        sequence_emb = elu_act(self.dropout(sequence_emb)) + 1\n","\n","        return sequence_emb\n","\n","    def finetune(self, input_ids, user_ids):\n","\n","        attention_mask = (input_ids > 0).long()\n","        extended_attention_mask = attention_mask.unsqueeze(1).unsqueeze(2) # torch.int64\n","        max_len = attention_mask.size(-1)\n","        attn_shape = (1, max_len, max_len)\n","        subsequent_mask = torch.triu(torch.ones(attn_shape), diagonal=1) # torch.uint8\n","        subsequent_mask = (subsequent_mask == 0).unsqueeze(1)\n","        subsequent_mask = subsequent_mask.long()\n","\n","        if self.args.cuda_condition:\n","            subsequent_mask = subsequent_mask.cuda()\n","\n","        extended_attention_mask = extended_attention_mask * subsequent_mask\n","        extended_attention_mask = extended_attention_mask.to(dtype=next(self.parameters()).dtype) # fp16 compatibility\n","        extended_attention_mask = (1.0 - extended_attention_mask) * (-2 ** 32 + 1)\n","\n","        mean_sequence_emb = self.add_position_mean_embedding(input_ids)\n","        cov_sequence_emb = self.add_position_cov_embedding(input_ids)\n","\n","        item_encoded_layers = self.item_encoder(mean_sequence_emb,\n","                                                cov_sequence_emb,\n","                                                extended_attention_mask,\n","                                                output_all_encoded_layers=True)\n","\n","        mean_sequence_output, cov_sequence_output, att_scores = item_encoded_layers[-1]\n","\n","        margins = self.user_margins(user_ids)\n","        return mean_sequence_output, cov_sequence_output, att_scores, margins\n","\n","    def init_weights(self, module):\n","        \"\"\" Initialize the weights.\n","        \"\"\"\n","        if isinstance(module, (nn.Linear, nn.Embedding)):\n","            # Slightly different from the TF version which uses truncated_normal for initialization\n","            module.weight.data.normal_(mean=0.01, std=self.args.initializer_range)\n","        elif isinstance(module, LayerNorm):\n","            module.bias.data.zero_()\n","            module.weight.data.fill_(1.0)\n","        if isinstance(module, nn.Linear) and module.bias is not None:\n","            module.bias.data.zero_()\n","\n","\n","class DistMeanSAModel(DistSAModel):\n","    def __init__(self, args):\n","        super(DistMeanSAModel, self).__init__(args)\n","        self.item_encoder = DistMeanSAEncoder(args)"],"metadata":{"id":"eZ-mnUL52LaJ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Trainers"],"metadata":{"id":"O1WFA-iN2NNC"}},{"cell_type":"code","source":["import numpy as np\n","import tqdm\n","import random\n","from collections import defaultdict\n","\n","import torch\n","import torch.nn as nn\n","from torch.optim import Adam\n","\n","# from utils import recall_at_k, ndcg_at_k, get_eval_metrics_v2, cal_mrr, get_user_performance_perpopularity, get_item_performance_perpopularity\n","# from modules import wasserstein_distance, kl_distance, wasserstein_distance_matmul, d2s_gaussiannormal, d2s_1overx, kl_distance_matmul"],"metadata":{"id":"QFxchqfk2Qob"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class Trainer:\n","    def __init__(self, model, train_dataloader,\n","                 eval_dataloader,\n","                 test_dataloader, args):\n","\n","        self.args = args\n","        self.cuda_condition = torch.cuda.is_available() and not self.args.no_cuda\n","        self.device = torch.device(\"cuda\" if self.cuda_condition else \"cpu\")\n","\n","        self.model = model\n","        if self.cuda_condition:\n","            self.model.cuda()\n","\n","        # Setting the train and test data loader\n","        self.train_dataloader = train_dataloader\n","        self.eval_dataloader = eval_dataloader\n","        self.test_dataloader = test_dataloader\n","\n","        # self.data_name = self.args.data_name\n","        betas = (self.args.adam_beta1, self.args.adam_beta2)\n","        self.optim = Adam(self.model.parameters(), lr=self.args.lr, betas=betas, weight_decay=self.args.weight_decay)\n","\n","        print(\"Total Parameters:\", sum([p.nelement() for p in self.model.parameters()]), flush=True)\n","        self.criterion = nn.BCELoss()\n","\n","    def train(self, epoch):\n","        self.iteration(epoch, self.train_dataloader)\n","\n","    def valid(self, epoch, full_sort=False):\n","        return self.iteration(epoch, self.eval_dataloader, full_sort, train=False)\n","\n","    def test(self, epoch, full_sort=False):\n","        return self.iteration(epoch, self.test_dataloader, full_sort, train=False)\n","\n","    def iteration(self, epoch, dataloader, full_sort=False, train=True):\n","        raise NotImplementedError\n","\n","    def get_sample_scores(self, epoch, pred_list):\n","        pred_list = (-pred_list).argsort().argsort()[:, 0]\n","        HIT_1, NDCG_1, MRR = get_eval_metrics_v2(pred_list, 1)\n","        HIT_5, NDCG_5, MRR = get_eval_metrics_v2(pred_list, 5)\n","        HIT_10, NDCG_10, MRR = get_eval_metrics_v2(pred_list, 10)\n","        post_fix = {\n","            \"Epoch\": epoch,\n","            \"HIT@1\": '{:.4f}'.format(HIT_1), \"NDCG@1\": '{:.4f}'.format(NDCG_1),\n","            \"HIT@5\": '{:.4f}'.format(HIT_5), \"NDCG@5\": '{:.4f}'.format(NDCG_5),\n","            \"HIT@10\": '{:.4f}'.format(HIT_10), \"NDCG@10\": '{:.4f}'.format(NDCG_10),\n","            \"MRR\": '{:.4f}'.format(MRR),\n","        }\n","        print(post_fix, flush=True)\n","        with open(self.args.log_file, 'a') as f:\n","            f.write(str(post_fix) + '\\n')\n","        return [HIT_1, NDCG_1, HIT_5, NDCG_5, HIT_10, NDCG_10, MRR], str(post_fix), None\n","\n","    def get_full_sort_score(self, epoch, answers, pred_list):\n","        recall, ndcg, mrr = [], [], 0\n","        recall_dict_list = []\n","        ndcg_dict_list = []\n","        for k in [1, 5, 10, 15, 20, 40]:\n","            recall_result, recall_dict_k = recall_at_k(answers, pred_list, k)\n","            recall.append(recall_result)\n","            recall_dict_list.append(recall_dict_k)\n","            ndcg_result, ndcg_dict_k = ndcg_at_k(answers, pred_list, k)\n","            ndcg.append(ndcg_result)\n","            ndcg_dict_list.append(ndcg_dict_k)\n","        mrr, mrr_dict = cal_mrr(answers, pred_list)\n","        post_fix = {\n","            \"Epoch\": epoch,\n","            \"HIT@1\": '{:.8f}'.format(recall[0]), \"NDCG@1\": '{:.8f}'.format(ndcg[0]),\n","            \"HIT@5\": '{:.8f}'.format(recall[1]), \"NDCG@5\": '{:.8f}'.format(ndcg[1]),\n","            \"HIT@10\": '{:.8f}'.format(recall[2]), \"NDCG@10\": '{:.8f}'.format(ndcg[2]),\n","            \"HIT@15\": '{:.8f}'.format(recall[3]), \"NDCG@15\": '{:.8f}'.format(ndcg[3]),\n","            \"HIT@20\": '{:.8f}'.format(recall[4]), \"NDCG@20\": '{:.8f}'.format(ndcg[4]),\n","            \"HIT@40\": '{:.8f}'.format(recall[5]), \"NDCG@40\": '{:.8f}'.format(ndcg[5]),\n","            \"MRR\": '{:.8f}'.format(mrr)\n","        }\n","        print(post_fix, flush=True)\n","        with open(self.args.log_file, 'a') as f:\n","            f.write(str(post_fix) + '\\n')\n","        return [recall[0], ndcg[0], recall[1], ndcg[1], recall[2], ndcg[2], recall[3], ndcg[3], recall[4], ndcg[4], recall[5], ndcg[5], mrr], str(post_fix), [recall_dict_list, ndcg_dict_list, mrr_dict]\n","\n","    def get_pos_items_ranks(self, batch_pred_lists, answers):\n","        num_users = len(batch_pred_lists)\n","        batch_pos_ranks = defaultdict(list)\n","        for i in range(num_users):\n","            pred_list = batch_pred_lists[i]\n","            true_set = set(answers[i])\n","            for ind, pred_item in enumerate(pred_list):\n","                if pred_item in true_set:\n","                    batch_pos_ranks[pred_item].append(ind+1)\n","        return batch_pos_ranks\n","\n","    def save(self, file_name):\n","        torch.save(self.model.cpu().state_dict(), file_name)\n","        self.model.to(self.device)\n","\n","    def load(self, file_name):\n","        self.model.load_state_dict(torch.load(file_name, map_location='cuda:0'))\n","\n","    def cross_entropy(self, seq_out, pos_ids, neg_ids):\n","        # [batch seq_len hidden_size]\n","        pos_emb = self.model.item_embeddings(pos_ids)\n","        neg_emb = self.model.item_embeddings(neg_ids)\n","        # [batch*seq_len hidden_size]\n","        pos = pos_emb.view(-1, pos_emb.size(2))\n","        neg = neg_emb.view(-1, neg_emb.size(2))\n","        seq_emb = seq_out.view(-1, self.args.hidden_size) # [batch*seq_len hidden_size]\n","        pos_logits = torch.sum(pos * seq_emb, -1) # [batch*seq_len]\n","        neg_logits = torch.sum(neg * seq_emb, -1)\n","        istarget = (pos_ids > 0).view(pos_ids.size(0) * self.model.args.max_seq_length).float() # [batch*seq_len]\n","        loss = torch.sum(\n","            - torch.log(torch.sigmoid(pos_logits) + 1e-24) * istarget -\n","            torch.log(1 - torch.sigmoid(neg_logits) + 1e-24) * istarget\n","        ) / torch.sum(istarget)\n","\n","        auc = torch.sum(\n","            ((torch.sign(pos_logits - neg_logits) + 1) / 2) * istarget\n","        ) / torch.sum(istarget)\n","\n","        return loss, auc\n","\n","    def predict_sample(self, seq_out, test_neg_sample):\n","        # [batch 100 hidden_size]\n","        test_item_emb = self.model.item_embeddings(test_neg_sample)\n","        # [batch hidden_size]\n","        test_logits = torch.bmm(test_item_emb, seq_out.unsqueeze(-1)).squeeze(-1)  # [B 100]\n","        return test_logits\n","\n","    def predict_full(self, seq_out):\n","        # [item_num hidden_size]\n","        test_item_emb = self.model.item_embeddings.weight\n","        # [batch hidden_size ]\n","        rating_pred = torch.matmul(seq_out, test_item_emb.transpose(0, 1))\n","        return rating_pred\n","\n","class PretrainTrainer(Trainer):\n","\n","    def __init__(self, model,\n","                 train_dataloader,\n","                 eval_dataloader,\n","                 test_dataloader, args):\n","        super(PretrainTrainer, self).__init__(\n","            model,\n","            train_dataloader,\n","            eval_dataloader,\n","            test_dataloader, args\n","        )\n","\n","    def pretrain(self, epoch, pretrain_dataloader):\n","\n","        desc = f'AAP-{self.args.aap_weight}-' \\\n","               f'MIP-{self.args.mip_weight}-' \\\n","               f'MAP-{self.args.map_weight}-' \\\n","               f'SP-{self.args.sp_weight}'\n","\n","        pretrain_data_iter = tqdm.tqdm(enumerate(pretrain_dataloader),\n","                                       desc=f\"{self.args.model_name}-{self.args.data_name} Epoch:{epoch}\",\n","                                       total=len(pretrain_dataloader),\n","                                       bar_format=\"{l_bar}{r_bar}\")\n","\n","        self.model.train()\n","        aap_loss_avg = 0.0\n","        mip_loss_avg = 0.0\n","        map_loss_avg = 0.0\n","        sp_loss_avg = 0.0\n","\n","        for i, batch in pretrain_data_iter:\n","            # 0. batch_data will be sent into the device(GPU or CPU)\n","            batch = tuple(t.to(self.device) for t in batch)\n","            attributes, masked_item_sequence, pos_items, neg_items, \\\n","            masked_segment_sequence, pos_segment, neg_segment = batch\n","\n","            aap_loss, mip_loss, map_loss, sp_loss = self.model.pretrain(attributes,\n","                                            masked_item_sequence, pos_items, neg_items,\n","                                            masked_segment_sequence, pos_segment, neg_segment)\n","\n","            joint_loss = self.args.aap_weight * aap_loss + \\\n","                         self.args.mip_weight * mip_loss + \\\n","                         self.args.map_weight * map_loss + \\\n","                         self.args.sp_weight * sp_loss\n","\n","            self.optim.zero_grad()\n","            joint_loss.backward()\n","            self.optim.step()\n","\n","            aap_loss_avg += aap_loss.item()\n","            mip_loss_avg += mip_loss.item()\n","            map_loss_avg += map_loss.item()\n","            sp_loss_avg += sp_loss.item()\n","\n","        num = len(pretrain_data_iter) * self.args.pre_batch_size\n","        post_fix = {\n","            \"epoch\": epoch,\n","            \"aap_loss_avg\": '{:.4f}'.format(aap_loss_avg /num),\n","            \"mip_loss_avg\": '{:.4f}'.format(mip_loss_avg /num),\n","            \"map_loss_avg\": '{:.4f}'.format(map_loss_avg / num),\n","            \"sp_loss_avg\": '{:.4f}'.format(sp_loss_avg / num),\n","        }\n","        print(desc)\n","        print(str(post_fix))\n","        with open(self.args.log_file, 'a') as f:\n","            f.write(str(desc) + '\\n')\n","            f.write(str(post_fix) + '\\n')\n","\n","class FinetuneTrainer(Trainer):\n","\n","    def __init__(self, model,\n","                 train_dataloader,\n","                 eval_dataloader,\n","                 test_dataloader, args):\n","        super(FinetuneTrainer, self).__init__(\n","            model,\n","            train_dataloader,\n","            eval_dataloader,\n","            test_dataloader, args\n","        )\n","    \n","    def iteration(self, epoch, dataloader, full_sort=False, train=True):\n","\n","        str_code = \"train\" if train else \"test\"\n","\n","        # Setting the tqdm progress bar\n","\n","        #rec_data_iter = tqdm.tqdm(enumerate(dataloader),\n","        #                          desc=\"Recommendation EP_%s:%d\" % (str_code, epoch),\n","        #                          total=len(dataloader),\n","        #                          bar_format=\"{l_bar}{r_bar}\")\n","        rec_data_iter = dataloader\n","        if train:\n","            self.model.train()\n","            rec_avg_loss = 0.0\n","            rec_cur_loss = 0.0\n","            rec_avg_auc = 0.0\n","\n","            #for i, batch in rec_data_iter:\n","            for batch in rec_data_iter:\n","                # 0. batch_data will be sent into the device(GPU or CPU)\n","                batch = tuple(t.to(self.device) for t in batch)\n","                _, input_ids, target_pos, target_neg, _ = batch\n","                # Binary cross_entropy\n","                sequence_output, _ = self.model.finetune(input_ids)\n","                loss, batch_auc = self.cross_entropy(sequence_output, target_pos, target_neg)\n","                self.optim.zero_grad()\n","                loss.backward()\n","                self.optim.step()\n","\n","                rec_avg_loss += loss.item()\n","                rec_cur_loss = loss.item()\n","                rec_avg_auc += batch_auc.item()\n","\n","            post_fix = {\n","                \"epoch\": epoch,\n","                \"rec_avg_loss\": '{:.4f}'.format(rec_avg_loss / len(rec_data_iter)),\n","                \"rec_cur_loss\": '{:.4f}'.format(rec_cur_loss),\n","                \"rec_avg_auc\": '{:.4f}'.format(rec_avg_auc / len(rec_data_iter)),\n","            }\n","\n","            if (epoch + 1) % self.args.log_freq == 0:\n","                print(str(post_fix), flush=True)\n","\n","            with open(self.args.log_file, 'a') as f:\n","                f.write(str(post_fix) + '\\n')\n","\n","        else:\n","            self.model.eval()\n","\n","            pred_list = None\n","\n","            if full_sort:\n","                answer_list = None\n","                #for i, batch in rec_data_iter:\n","                i = 0\n","                for batch in rec_data_iter:\n","                    # 0. batch_data will be sent into the device(GPU or cpu)\n","                    batch = tuple(t.to(self.device) for t in batch)\n","                    user_ids, input_ids, target_pos, target_neg, answers = batch\n","                    recommend_output, _ = self.model.finetune(input_ids)\n","\n","                    recommend_output = recommend_output[:, -1, :]\n","\n","                    rating_pred = self.predict_full(recommend_output)\n","\n","                    rating_pred = rating_pred.cpu().data.numpy().copy()\n","                    batch_user_index = user_ids.cpu().numpy()\n","                    rating_pred[self.args.train_matrix[batch_user_index].toarray() > 0] = 0\n","                    ind = np.argpartition(rating_pred, -40)[:, -40:]\n","                    arr_ind = rating_pred[np.arange(len(rating_pred))[:, None], ind]\n","                    arr_ind_argsort = np.argsort(arr_ind)[np.arange(len(rating_pred)), ::-1]\n","                    batch_pred_list = ind[np.arange(len(rating_pred))[:, None], arr_ind_argsort]\n","\n","                    if i == 0:\n","                        pred_list = batch_pred_list\n","                        answer_list = answers.cpu().data.numpy()\n","                    else:\n","                        pred_list = np.append(pred_list, batch_pred_list, axis=0)\n","                        answer_list = np.append(answer_list, answers.cpu().data.numpy(), axis=0)\n","                    i += 1\n","                return self.get_full_sort_score(epoch, answer_list, pred_list)\n","\n","            else:\n","                #for i, batch in rec_data_iter:\n","                i = 0\n","                for batch in rec_data_iter:\n","                    # 0. batch_data will be sent into the device(GPU or cpu)\n","                    batch = tuple(t.to(self.device) for t in batch)\n","                    user_ids, input_ids, target_pos, target_neg, answers, sample_negs = batch\n","                    recommend_output = self.model.finetune(input_ids)\n","                    test_neg_items = torch.cat((answers, sample_negs), -1)\n","                    recommend_output = recommend_output[:, -1, :]\n","\n","                    test_logits = self.predict_sample(recommend_output, test_neg_items)\n","                    test_logits = test_logits.cpu().detach().numpy().copy()\n","                    if i == 0:\n","                        pred_list = test_logits\n","                    else:\n","                        pred_list = np.append(pred_list, test_logits, axis=0)\n","                    i += 1\n","\n","                return self.get_sample_scores(epoch, pred_list)\n","\n","\n","class DistSAModelTrainer(Trainer):\n","\n","    def __init__(self, model,\n","                 train_dataloader,\n","                 eval_dataloader,\n","                 test_dataloader, args):\n","        super(DistSAModelTrainer, self).__init__(\n","            model,\n","            train_dataloader,\n","            eval_dataloader,\n","            test_dataloader, args\n","        )\n","\n","    def bpr_optimization(self, seq_mean_out, seq_cov_out, pos_ids, neg_ids):\n","        # [batch seq_len hidden_size]\n","        activation = nn.ELU()\n","        pos_mean_emb = self.model.item_mean_embeddings(pos_ids)\n","        pos_cov_emb = activation(self.model.item_cov_embeddings(pos_ids)) + 1\n","        neg_mean_emb = self.model.item_mean_embeddings(neg_ids)\n","        neg_cov_emb = activation(self.model.item_cov_embeddings(neg_ids)) + 1\n","\n","        # [batch*seq_len hidden_size]\n","        pos_mean = pos_mean_emb.view(-1, pos_mean_emb.size(2))\n","        pos_cov = pos_cov_emb.view(-1, pos_cov_emb.size(2))\n","        neg_mean = neg_mean_emb.view(-1, neg_mean_emb.size(2))\n","        neg_cov = neg_cov_emb.view(-1, neg_cov_emb.size(2))\n","        seq_mean_emb = seq_mean_out.view(-1, self.args.hidden_size) # [batch*seq_len hidden_size]\n","        seq_cov_emb = seq_cov_out.view(-1, self.args.hidden_size) # [batch*seq_len hidden_size]\n","\n","        if self.args.distance_metric == 'wasserstein':\n","            pos_logits = wasserstein_distance(seq_mean_emb, seq_cov_emb, pos_mean, pos_cov)\n","            neg_logits = wasserstein_distance(seq_mean_emb, seq_cov_emb, neg_mean, neg_cov)\n","            pos_vs_neg = wasserstein_distance(pos_mean, pos_cov, neg_mean, neg_cov)\n","        else:\n","            pos_logits = kl_distance(seq_mean_emb, seq_cov_emb, pos_mean, pos_cov)\n","            neg_logits = kl_distance(seq_mean_emb, seq_cov_emb, neg_mean, neg_cov)\n","            pos_vs_neg = kl_distance(pos_mean, pos_cov, neg_mean, neg_cov)\n","\n","        istarget = (pos_ids > 0).view(pos_ids.size(0) * self.model.args.max_seq_length).float() # [batch*seq_len]\n","        loss = torch.sum(-torch.log(torch.sigmoid(neg_logits - pos_logits + 1e-24)) * istarget) / torch.sum(istarget)\n","\n","        pvn_loss = self.args.pvn_weight * torch.sum(torch.clamp(pos_logits - pos_vs_neg, 0) * istarget) / torch.sum(istarget)\n","        auc = torch.sum(\n","            ((torch.sign(neg_logits - pos_logits) + 1) / 2) * istarget\n","        ) / torch.sum(istarget)\n","\n","        return loss, auc, pvn_loss\n","\n","    def ce_optimization(self, seq_mean_out, seq_cov_out, pos_ids, neg_ids):\n","        # [batch seq_len hidden_size]\n","        activation = nn.ELU()\n","        pos_mean_emb = self.model.item_mean_embeddings(pos_ids)\n","        pos_cov_emb = activation(self.model.item_cov_embeddings(pos_ids)) + 1\n","        neg_mean_emb = self.model.item_mean_embeddings(neg_ids)\n","        neg_cov_emb = activation(self.model.item_cov_embeddings(neg_ids)) + 1\n","\n","        # [batch*seq_len hidden_size]\n","        pos_mean = pos_mean_emb.view(-1, pos_mean_emb.size(2))\n","        pos_cov = pos_cov_emb.view(-1, pos_cov_emb.size(2))\n","        neg_mean = neg_mean_emb.view(-1, neg_mean_emb.size(2))\n","        neg_cov = neg_cov_emb.view(-1, neg_cov_emb.size(2))\n","        seq_mean_emb = seq_mean_out.view(-1, self.args.hidden_size) # [batch*seq_len hidden_size]\n","        seq_cov_emb = seq_cov_out.view(-1, self.args.hidden_size) # [batch*seq_len hidden_size]\n","\n","\n","        #pos_logits = d2s_gaussiannormal(wasserstein_distance(seq_mean_emb, seq_cov_emb, pos_mean, pos_cov), self.args.kernel_param)\n","        pos_logits = -wasserstein_distance(seq_mean_emb, seq_cov_emb, pos_mean, pos_cov)\n","        #neg_logits = d2s_gaussiannormal(wasserstein_distance(seq_mean_emb, seq_cov_emb, neg_mean, neg_cov), self.args.kernel_param)\n","        neg_logits = -wasserstein_distance(seq_mean_emb, seq_cov_emb, neg_mean, neg_cov)\n","\n","        istarget = (pos_ids > 0).view(pos_ids.size(0) * self.model.args.max_seq_length).float() # [batch*seq_len]\n","\n","        loss = torch.sum(\n","            - torch.log(torch.sigmoid(neg_logits) + 1e-24) * istarget -\n","            torch.log(1 - torch.sigmoid(pos_logits) + 1e-24) * istarget\n","        ) / torch.sum(istarget)\n","\n","        auc = torch.sum(\n","            ((torch.sign(neg_logits - pos_logits) + 1) / 2) * istarget\n","        ) / torch.sum(istarget)\n","\n","\n","        return loss, auc\n","\n","    def margin_optimization(self, seq_mean_out, seq_cov_out, pos_ids, neg_ids, margins):\n","        # [batch seq_len hidden_size]\n","        activation = nn.ELU()\n","        pos_mean_emb = self.model.item_mean_embeddings(pos_ids)\n","        pos_cov_emb = activation(self.model.item_cov_embeddings(pos_ids)) + 1\n","        neg_mean_emb = self.model.item_mean_embeddings(neg_ids)\n","        neg_cov_emb = activation(self.model.item_cov_embeddings(neg_ids)) + 1\n","\n","        # [batch*seq_len hidden_size]\n","        pos_mean = pos_mean_emb.view(-1, pos_mean_emb.size(2))\n","        pos_cov = pos_cov_emb.view(-1, pos_cov_emb.size(2))\n","        neg_mean = neg_mean_emb.view(-1, neg_mean_emb.size(2))\n","        neg_cov = neg_cov_emb.view(-1, neg_cov_emb.size(2))\n","        seq_mean_emb = seq_mean_out.view(-1, self.args.hidden_size) # [batch*seq_len hidden_size]\n","        seq_cov_emb = seq_cov_out.view(-1, self.args.hidden_size) # [batch*seq_len hidden_size]\n","\n","        if self.args.distance_metric == 'wasserstein':\n","            pos_logits = wasserstein_distance(seq_mean_emb, seq_cov_emb, pos_mean, pos_cov)\n","            neg_logits = wasserstein_distance(seq_mean_emb, seq_cov_emb, neg_mean, neg_cov)\n","            pos_vs_neg = wasserstein_distance(pos_mean, pos_cov, neg_mean, neg_cov)\n","        else:\n","            pos_logits = kl_distance(seq_mean_emb, seq_cov_emb, pos_mean, pos_cov)\n","            neg_logits = kl_distance(seq_mean_emb, seq_cov_emb, neg_mean, neg_cov)\n","            pos_vs_neg = kl_distance(pos_mean, pos_cov, neg_mean, neg_cov)\n","\n","        istarget = (pos_ids > 0).view(pos_ids.size(0) * self.model.args.max_seq_length).float() # [batch*seq_len]\n","        loss = torch.sum(torch.clamp(pos_logits - neg_logits, min=0) * istarget) / torch.sum(istarget)\n","        pvn_loss = self.args.pvn_weight * torch.sum(torch.clamp(pos_logits - pos_vs_neg, 0) * istarget) / torch.sum(istarget)\n","        auc = torch.sum(\n","            ((torch.sign(neg_logits - pos_logits) + 1) / 2) * istarget\n","        ) / torch.sum(istarget)\n","\n","        return loss, auc, pvn_loss\n","\n","    \n","    def dist_predict_full(self, seq_mean_out, seq_cov_out):\n","        elu_activation = torch.nn.ELU()\n","        test_item_mean_emb = self.model.item_mean_embeddings.weight\n","        test_item_cov_emb = elu_activation(self.model.item_cov_embeddings.weight) + 1\n","        #num_items, emb_size = test_item_cov_emb.shape\n","\n","        #seq_mean_out = seq_mean_out.unsqueeze(1).expand(-1, num_items, -1).reshape(-1, emb_size)\n","        #seq_cov_out = seq_cov_out.unsqueeze(1).expand(-1, num_items, -1).reshape(-1, emb_size)\n","\n","        #if args.distance_metric == 'wasserstein':\n","        #    return wasserstein_distance(seq_mean_out, seq_cov_out, test_item_mean_emb, test_item_cov_emb)\n","        #else:\n","        #    return kl_distance(seq_mean_out, seq_cov_out, test_item_mean_emb, test_item_cov_emb)\n","        #return d2s_1overx(wasserstein_distance_matmul(seq_mean_out, seq_cov_out, test_item_mean_emb, test_item_cov_emb))\n","        return wasserstein_distance_matmul(seq_mean_out, seq_cov_out, test_item_mean_emb, test_item_cov_emb)\n","        #return d2s_gaussiannormal(wasserstein_distance_matmul(seq_mean_out, seq_cov_out, test_item_mean_emb, test_item_cov_emb))\n","\n","    def kl_predict_full(self, seq_mean_out, seq_cov_out):\n","        elu_activation = torch.nn.ELU()\n","        test_item_mean_emb = self.model.item_mean_embeddings.weight\n","        test_item_cov_emb = elu_activation(self.model.item_cov_embeddings.weight) + 1\n","\n","        num_items = test_item_mean_emb.shape[0]\n","        eval_batch_size = seq_mean_out.shape[0]\n","        moded_num_items = eval_batch_size - num_items % eval_batch_size\n","        fake_mean_emb = torch.zeros(moded_num_items, test_item_mean_emb.shape[1], dtype=torch.float32).to(self.device)\n","        fake_cov_emb = torch.ones(moded_num_items, test_item_mean_emb.shape[1], dtype=torch.float32).to(self.device)\n","\n","        concated_mean_emb = torch.cat((test_item_mean_emb, fake_mean_emb), 0)\n","        concated_cov_emb = torch.cat((test_item_cov_emb, fake_cov_emb), 0)\n","\n","        assert concated_mean_emb.shape[0] == test_item_mean_emb.shape[0] + moded_num_items\n","\n","        num_batches = int(num_items / eval_batch_size)\n","        if moded_num_items > 0:\n","            num_batches += 1\n","\n","        results = torch.zeros(seq_mean_out.shape[0], concated_mean_emb.shape[0], dtype=torch.float32)\n","        start_i = 0\n","        for i_batch in range(num_batches):\n","            end_i = start_i + eval_batch_size\n","\n","            results[:, start_i:end_i] = kl_distance_matmul(seq_mean_out, seq_cov_out, concated_mean_emb[start_i:end_i, :], concated_cov_emb[start_i:end_i, :])\n","            #results[:, start_i:end_i] = d2s_gaussiannormal(kl_distance_matmul(seq_mean_out, seq_cov_out, concated_mean_emb[start_i:end_i, :], concated_cov_emb[start_i:end_i, :]))\n","            start_i += eval_batch_size\n","\n","        #print(results[:, :5])\n","        return results[:, :num_items]\n","\n","\n","    def iteration(self, epoch, dataloader, full_sort=False, train=True):\n","\n","        str_code = \"train\" if train else \"test\"\n","\n","        #rec_data_iter = tqdm.tqdm(enumerate(dataloader),\n","        #                          desc=f\"Recommendation EP_{str_code}:{epoch}\",\n","        #                          total=len(dataloader),\n","        #                          bar_format=\"{l_bar}{r_bar}\")\n","        rec_data_iter = dataloader\n","\n","        if train:\n","            self.model.train()\n","            rec_avg_loss = 0.0\n","            rec_cur_loss = 0.0\n","            rec_avg_pvn_loss = 0.0\n","            rec_avg_auc = 0.0\n","\n","            #for i, batch in rec_data_iter:\n","            for batch in rec_data_iter:\n","                # 0. batch_data will be sent into the device(GPU or CPU)\n","                batch = tuple(t.to(self.device) for t in batch)\n","                user_ids, input_ids, target_pos, target_neg, _ = batch\n","                # bpr optimization\n","                sequence_mean_output, sequence_cov_output, att_scores, margins = self.model.finetune(input_ids, user_ids)\n","                #print(att_scores[0, 0, :, :])\n","                loss, batch_auc, pvn_loss = self.bpr_optimization(sequence_mean_output, sequence_cov_output, target_pos, target_neg)\n","                #loss, batch_auc, pvn_loss = self.margin_optimization(sequence_mean_output, sequence_cov_output, target_pos, target_neg, margins)\n","                #loss, batch_auc = self.ce_optimization(sequence_mean_output, sequence_cov_output, target_pos, target_neg)\n","\n","                loss += pvn_loss\n","                self.optim.zero_grad()\n","                loss.backward()\n","                self.optim.step()\n","\n","                rec_avg_loss += loss.item()\n","                rec_cur_loss = loss.item()\n","                rec_avg_auc += batch_auc.item()\n","                rec_avg_pvn_loss += pvn_loss.item()\n","\n","            post_fix = {\n","                \"epoch\": epoch,\n","                \"rec_avg_loss\": '{:.4f}'.format(rec_avg_loss / len(rec_data_iter)),\n","                \"rec_cur_loss\": '{:.4f}'.format(rec_cur_loss),\n","                \"rec_avg_auc\": '{:.6f}'.format(rec_avg_auc / len(rec_data_iter)),\n","                \"rec_avg_pvn_loss\": '{:.6f}'.format(rec_avg_pvn_loss / len(rec_data_iter)),\n","            }\n","\n","            if (epoch + 1) % self.args.log_freq == 0:\n","                print(str(post_fix), flush=True)\n","\n","            with open(self.args.log_file, 'a') as f:\n","                f.write(str(post_fix) + '\\n')\n","        else:\n","            self.model.eval()\n","\n","            pred_list = None\n","\n","            if full_sort:\n","                answer_list = None\n","                with torch.no_grad():\n","                    #for i, batch in rec_data_iter:\n","                    i = 0\n","                    for batch in rec_data_iter:\n","                        # 0. batch_data will be sent into the device(GPU or cpu)\n","                        batch = tuple(t.to(self.device) for t in batch)\n","                        user_ids, input_ids, target_pos, target_neg, answers = batch\n","                        recommend_mean_output, recommend_cov_output, _, _ = self.model.finetune(input_ids, user_ids)\n","\n","                        recommend_mean_output = recommend_mean_output[:, -1, :]\n","                        recommend_cov_output = recommend_cov_output[:, -1, :]\n","\n","                        if self.args.distance_metric == 'kl':\n","                            rating_pred = self.kl_predict_full(recommend_mean_output, recommend_cov_output)\n","                        else:\n","                            rating_pred = self.dist_predict_full(recommend_mean_output, recommend_cov_output)\n","train_matrix\n","                        rating_pred = rating_pred.cpu().data.numpy().copy()\n","                        batch_user_index = user_ids.cpu().numpy()\n","                        rating_pred[self.args.train_matrix[batch_user_index].toarray() > 0] = 1e+24\n","                        # reference: https://stackoverflow.com/a/23734295, https://stackoverflow.com/a/20104162\n","                        ind = np.argpartition(rating_pred, 40)[:, :40]\n","                        #ind = np.argpartition(rating_pred, -40)[:, -40:]\n","                        arr_ind = rating_pred[np.arange(len(rating_pred))[:, None], ind]\n","                        # ascending order\n","                        arr_ind_argsort = np.argsort(arr_ind)[np.arange(len(rating_pred)), ::]\n","                        #arr_ind_argsort = np.argsort(arr_ind)[np.arange(len(rating_pred)), ::-1]\n","                        batch_pred_list = ind[np.arange(len(rating_pred))[:, None], arr_ind_argsort]\n","\n","                        if i == 0:\n","                            pred_list = batch_pred_list\n","                            answer_list = answers.cpu().data.numpy()\n","                        else:\n","                            pred_list = np.append(pred_list, batch_pred_list, axis=0)\n","                            answer_list = np.append(answer_list, answers.cpu().data.numpy(), axis=0)\n","                        i += 1\n","                    return self.get_full_sort_score(epoch, answer_list, pred_list)"],"metadata":{"id":"rGFv-bI22Riw"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Main"],"metadata":{"id":"8uM7b6uH2Vmx"}},{"cell_type":"code","source":["import os\n","import numpy as np\n","import random\n","import torch\n","import pickle\n","import argparse\n","\n","from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n","\n","# from datasets import SASRecDataset\n","# from trainers import FinetuneTrainer, DistSAModelTrainer\n","# from models import S3RecModel\n","# from seqmodels import SASRecModel, DistSAModel, DistMeanSAModel\n","# from utils import EarlyStopping, get_user_seqs, get_item2attribute_json, check_path, set_seed"],"metadata":{"id":"3Uu8d-nV2XFR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["parser = argparse.ArgumentParser()\n","\n","parser.add_argument('--data_dir', default='../data/', type=str)\n","parser.add_argument('--output_dir', default='output/', type=str)\n","parser.add_argument('--data_name', default='Beauty', type=str)\n","parser.add_argument('--do_eval', action='store_true')\n","parser.add_argument('--ckp', default=10, type=int, help=\"pretrain epochs 10, 20, 30...\")\n","\n","# model args\n","parser.add_argument(\"--model_name\", default='Finetune_full', type=str)\n","parser.add_argument(\"--hidden_size\", type=int, default=64, help=\"hidden size of transformer model\")\n","parser.add_argument(\"--num_hidden_layers\", type=int, default=2, help=\"number of layers\")\n","parser.add_argument('--num_attention_heads', default=2, type=int)\n","parser.add_argument('--hidden_act', default=\"GELU\", type=str) # GELU relu\n","parser.add_argument(\"--attention_probs_dropout_prob\", type=float, default=0.5, help=\"attention dropout p\")\n","parser.add_argument(\"--hidden_dropout_prob\", type=float, default=0.5, help=\"hidden dropout p\")\n","parser.add_argument(\"--initializer_range\", type=float, default=0.02)\n","parser.add_argument('--max_seq_length', default=50, type=int)\n","parser.add_argument('--distance_metric', default='wasserstein', type=str)\n","parser.add_argument('--pvn_weight', default=0.1, type=float)\n","parser.add_argument('--kernel_param', default=1.0, type=float)\n","\n","# train args\n","parser.add_argument(\"--lr\", type=float, default=0.001, help=\"learning rate of adam\")\n","parser.add_argument(\"--batch_size\", type=int, default=256, help=\"number of batch_size\")\n","parser.add_argument(\"--epochs\", type=int, default=400, help=\"number of epochs\")\n","parser.add_argument(\"--no_cuda\", action=\"store_true\")\n","parser.add_argument(\"--log_freq\", type=int, default=1, help=\"per epoch print res\")\n","parser.add_argument(\"--seed\", default=42, type=int)\n","\n","parser.add_argument(\"--weight_decay\", type=float, default=0.0, help=\"weight_decay of adam\")\n","parser.add_argument(\"--adam_beta1\", type=float, default=0.9, help=\"adam first beta value\")\n","parser.add_argument(\"--adam_beta2\", type=float, default=0.999, help=\"adam second beta value\")\n","parser.add_argument(\"--gpu_id\", type=str, default=\"0\", help=\"gpu_id\")\n","\n","args = parser.parse_args([])"],"metadata":{"id":"MYi3KZYe2iN6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["seed_everything(args.seed)\n","os.makedirs(args.output_dir, exist_ok=True)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lBIaUDPL2yDn","executionInfo":{"status":"ok","timestamp":1642777062667,"user_tz":-330,"elapsed":23,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}},"outputId":"6dd50254-3baa-4508-85bf-94c9bd9d21dd"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["output/ created\n"]}]},{"cell_type":"code","source":["os.environ[\"CUDA_VISIBLE_DEVICES\"] = args.gpu_id\n","args.cuda_condition = torch.cuda.is_available() and not args.no_cuda\n","args.cuda_condition"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"R8TYSOrB22LL","executionInfo":{"status":"ok","timestamp":1642777063095,"user_tz":-330,"elapsed":436,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}},"outputId":"1a076ab9-ac1b-4807-b483-b58c5e2eadc3"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["False"]},"metadata":{},"execution_count":16}]},{"cell_type":"code","source":["!pip install -qq git+https://github.com/RecoHut-Projects/recohut.git@US632593\n","!wget -q https://github.com/RecoHut-Datasets/amazon_beauty/raw/v1/amazon-ratings.zip\n","!unzip -qq amazon-ratings.zip"],"metadata":{"id":"cAzUea_327p7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","from recohut.transforms.user_grouping import create_user_sequences"],"metadata":{"id":"Vw_SkhvQYdnQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df = pd.read_csv('ratings_Beauty.csv', sep=',').sample(frac=0.2)\n","df.columns = ['USERID','ITEMID','RATING','TIMESTAMP']\n","seq_len = df.groupby('USERID').size()\n","df = df[np.in1d(df.USERID, seq_len[seq_len >= 10].index)]\n","create_user_sequences(df, save_path='./beautydata.txt')\n","\n","!wc -l beautydata.txt"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tN-wCepOJMiw","executionInfo":{"status":"ok","timestamp":1642782320068,"user_tz":-330,"elapsed":56617,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}},"outputId":"d7ffe414-eac3-409c-fe86-d516970cf0d4"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["467 beautydata.txt\n"]}]},{"cell_type":"code","source":["args.data_file = './beautydata.txt'\n","# args.data_file = args.data_dir + args.data_name + '.txt'\n","#item2attribute_file = args.data_dir + args.data_name + '_item2attributes.json'"],"metadata":{"id":"EBwxsa0gJIQP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["user_seq, max_item, valid_rating_matrix, test_rating_matrix, num_users = \\\n","    get_user_seqs(args.data_file)\n","\n","#item2attribute, attribute_size = get_item2attribute_json(item2attribute_file)"],"metadata":{"id":"pKBtY6Ln5mV1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["args.item_size = max_item + 2\n","args.num_users = num_users\n","args.mask_id = max_item + 1\n","#args.attribute_size = attribute_size + 1"],"metadata":{"id":"8JyC-NlT5xrU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# save model args\n","args_str = f'{args.model_name}-{args.data_name}-{args.hidden_size}-{args.num_hidden_layers}-{args.num_attention_heads}-{args.hidden_act}-{args.attention_probs_dropout_prob}-{args.hidden_dropout_prob}-{args.max_seq_length}-{args.lr}-{args.weight_decay}-{args.ckp}-{args.kernel_param}-{args.pvn_weight}'\n","args.log_file = os.path.join(args.output_dir, args_str + '.txt')\n","print(str(args))\n","with open(args.log_file, 'a') as f:\n","    f.write(str(args) + '\\n')\n","\n","#args.item2attribute = item2attribute\n","# set item score in train set to `0` in validation\n","args.train_matrix = valid_rating_matrix\n","\n","# save model\n","checkpoint = args_str + '.pt'\n","args.checkpoint_path = os.path.join(args.output_dir, checkpoint)"],"metadata":{"id":"DVFuTM-1549r","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1642782461988,"user_tz":-330,"elapsed":7,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}},"outputId":"c077a116-c3ae-4aef-f688-dcbbc8d54e3b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Namespace(adam_beta1=0.9, adam_beta2=0.999, attention_probs_dropout_prob=0.5, batch_size=256, checkpoint_path='output/Finetune_full-Beauty-64-2-2-gelu-0.5-0.5-50-0.001-0.0-10-1.0-0.1.pt', ckp=10, cuda_condition=False, data_dir='../data/', data_file='./beautydata.txt', data_name='Beauty', distance_metric='wasserstein', do_eval=False, epochs=400, gpu_id='0', hidden_act='gelu', hidden_dropout_prob=0.5, hidden_size=64, initializer_range=0.02, item_size=5416, kernel_param=1.0, log_file='output/Finetune_full-Beauty-64-2-2-gelu-0.5-0.5-50-0.001-0.0-10-1.0-0.1.txt', log_freq=1, lr=0.001, mask_id=5415, max_seq_length=50, model_name='Finetune_full', no_cuda=False, num_attention_heads=2, num_hidden_layers=2, num_users=467, output_dir='output/', pvn_weight=0.1, seed=42, train_matrix=<19898x14724 sparse matrix of type '<class 'numpy.longlong'>'\n","\twith 40 stored elements in Compressed Sparse Row format>, weight_decay=0.0)\n"]}]},{"cell_type":"code","source":["train_dataset = SASRecDataset(args, user_seq, data_type='train')\n","train_sampler = RandomSampler(train_dataset)\n","train_dataloader = DataLoader(train_dataset, sampler=train_sampler, batch_size=args.batch_size)\n","\n","eval_dataset = SASRecDataset(args, user_seq, data_type='valid')\n","eval_sampler = SequentialSampler(eval_dataset)\n","#eval_dataloader = DataLoader(eval_dataset, sampler=eval_sampler, batch_size=200)\n","\n","test_dataset = SASRecDataset(args, user_seq, data_type='test')\n","test_sampler = SequentialSampler(test_dataset)\n","#test_dataloader = DataLoader(test_dataset, sampler=test_sampler, batch_size=200)"],"metadata":{"id":"Ga9dX-1r580Z"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["if args.model_name == 'DistSAModel':\n","    model = DistSAModel(args=args)\n","    eval_dataloader = DataLoader(eval_dataset, sampler=eval_sampler, batch_size=100)\n","    test_dataloader = DataLoader(test_dataset, sampler=test_sampler, batch_size=100)\n","    trainer = DistSAModelTrainer(model, train_dataloader, eval_dataloader,\n","                                test_dataloader, args)\n","elif args.model_name == 'DistMeanSAModel':\n","    model = DistMeanSAModel(args=args)\n","    eval_dataloader = DataLoader(eval_dataset, sampler=eval_sampler, batch_size=100)\n","    test_dataloader = DataLoader(test_dataset, sampler=test_sampler, batch_size=100)\n","    trainer = DistSAModelTrainer(model, train_dataloader, eval_dataloader,\n","                                test_dataloader, args)\n","else:\n","    model = SASRecModel(args=args)\n","    eval_dataloader = DataLoader(eval_dataset, sampler=eval_sampler, batch_size=args.batch_size)\n","    test_dataloader = DataLoader(test_dataset, sampler=test_sampler, batch_size=args.batch_size)\n","\n","    trainer = FinetuneTrainer(model, train_dataloader, eval_dataloader,\n","                            test_dataloader, args)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zIYj2xL4WFOQ","executionInfo":{"status":"ok","timestamp":1642782463775,"user_tz":-330,"elapsed":12,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}},"outputId":"d444ede7-e569-48c6-dd63-fb6c5d5e0f96"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Total Parameters: 449920\n"]}]},{"cell_type":"code","source":["if args.do_eval:\n","    trainer.load(args.checkpoint_path)\n","    print(f'Load model from {args.checkpoint_path} for test!')\n","    scores, result_info, _ = trainer.test(0, full_sort=True)\n","\n","else:\n","    #pretrained_path = os.path.join(args.output_dir, f'{args.data_name}-epochs-{args.ckp}.pt')\n","    #try:\n","    #    trainer.load(pretrained_path)\n","    #    print(f'Load Checkpoint From {pretrained_path}!')\n","\n","    #except FileNotFoundError:\n","    #    print(f'{pretrained_path} Not Found! The Model is same as SASRec')\n","    \n","    if args.model_name == 'DistSAModel':\n","        early_stopping = EarlyStopping(args.checkpoint_path, patience=100, verbose=True)\n","    else:\n","        early_stopping = EarlyStopping(args.checkpoint_path, patience=50, verbose=True)\n","    for epoch in range(args.epochs):\n","        trainer.train(epoch)\n","        # evaluate on MRR\n","        scores, _, _ = trainer.valid(epoch, full_sort=True)\n","        early_stopping(np.array(scores[-1:]), trainer.model)\n","        if early_stopping.early_stop:\n","            print(\"Early stopping\")\n","            break\n","\n","    print('---------------Change to test_rating_matrix!-------------------')\n","    # load the best model\n","    trainer.model.load_state_dict(torch.load(args.checkpoint_path))\n","    valid_scores, _, _ = trainer.valid('best', full_sort=True)\n","    trainer.args.train_matrix = test_rating_matrix\n","    scores, result_info, _ = trainer.test('best', full_sort=True)"],"metadata":{"id":"2_dcgbrs2bK1","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1642782635203,"user_tz":-330,"elapsed":171435,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}},"outputId":"ab5be8bb-2c29-4795-c86d-e1743f2bddc5"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["{'epoch': 0, 'rec_avg_loss': '1.3923', 'rec_cur_loss': '1.3926', 'rec_avg_auc': '0.4950'}\n","{'Epoch': 0, 'HIT@1': '0.00214133', 'NDCG@1': '0.00214133', 'HIT@5': '0.00428266', 'NDCG@5': '0.00306355', 'HIT@10': '0.00642398', 'NDCG@10': '0.00370815', 'HIT@15': '0.01070664', 'NDCG@15': '0.00483491', 'HIT@20': '0.01498929', 'NDCG@20': '0.00583594', 'HIT@40': '0.02355460', 'NDCG@40': '0.00763461', 'MRR': '0.00381115'}\n","Validation score increased.  Saving model ...\n","{'epoch': 1, 'rec_avg_loss': '1.3777', 'rec_cur_loss': '1.3777', 'rec_avg_auc': '0.5489'}\n","{'Epoch': 1, 'HIT@1': '0.00214133', 'NDCG@1': '0.00214133', 'HIT@5': '0.00428266', 'NDCG@5': '0.00296971', 'HIT@10': '0.00428266', 'NDCG@10': '0.00296971', 'HIT@15': '0.01070664', 'NDCG@15': '0.00465888', 'HIT@20': '0.01498929', 'NDCG@20': '0.00570664', 'HIT@40': '0.02141328', 'NDCG@40': '0.00707709', 'MRR': '0.00359341'}\n","EarlyStopping counter: 1 out of 50\n","{'epoch': 2, 'rec_avg_loss': '1.3624', 'rec_cur_loss': '1.3576', 'rec_avg_auc': '0.6090'}\n","{'Epoch': 2, 'HIT@1': '0.00214133', 'NDCG@1': '0.00214133', 'HIT@5': '0.00214133', 'NDCG@5': '0.00214133', 'HIT@10': '0.00642398', 'NDCG@10': '0.00343582', 'HIT@15': '0.01284797', 'NDCG@15': '0.00509599', 'HIT@20': '0.01498929', 'NDCG@20': '0.00558351', 'HIT@40': '0.02141328', 'NDCG@40': '0.00691520', 'MRR': '0.00343840'}\n","EarlyStopping counter: 2 out of 50\n","{'epoch': 3, 'rec_avg_loss': '1.3463', 'rec_cur_loss': '1.3406', 'rec_avg_auc': '0.6669'}\n","{'Epoch': 3, 'HIT@1': '0.00214133', 'NDCG@1': '0.00214133', 'HIT@5': '0.00214133', 'NDCG@5': '0.00214133', 'HIT@10': '0.00642398', 'NDCG@10': '0.00346145', 'HIT@15': '0.01284797', 'NDCG@15': '0.00519984', 'HIT@20': '0.01284797', 'NDCG@20': '0.00519984', 'HIT@40': '0.02141328', 'NDCG@40': '0.00699116', 'MRR': '0.00351183'}\n","EarlyStopping counter: 3 out of 50\n","{'epoch': 4, 'rec_avg_loss': '1.3318', 'rec_cur_loss': '1.3292', 'rec_avg_auc': '0.7117'}\n","{'Epoch': 4, 'HIT@1': '0.00214133', 'NDCG@1': '0.00214133', 'HIT@5': '0.00214133', 'NDCG@5': '0.00214133', 'HIT@10': '0.00856531', 'NDCG@10': '0.00408043', 'HIT@15': '0.01284797', 'NDCG@15': '0.00519443', 'HIT@20': '0.01284797', 'NDCG@20': '0.00519443', 'HIT@40': '0.01927195', 'NDCG@40': '0.00656656', 'MRR': '0.00344371'}\n","EarlyStopping counter: 4 out of 50\n","{'epoch': 5, 'rec_avg_loss': '1.3175', 'rec_cur_loss': '1.3121', 'rec_avg_auc': '0.7544'}\n","{'Epoch': 5, 'HIT@1': '0.00214133', 'NDCG@1': '0.00214133', 'HIT@5': '0.00214133', 'NDCG@5': '0.00214133', 'HIT@10': '0.00856531', 'NDCG@10': '0.00422420', 'HIT@15': '0.01070664', 'NDCG@15': '0.00480287', 'HIT@20': '0.01284797', 'NDCG@20': '0.00529833', 'HIT@40': '0.01927195', 'NDCG@40': '0.00658064', 'MRR': '0.00350162'}\n","EarlyStopping counter: 5 out of 50\n","{'epoch': 6, 'rec_avg_loss': '1.3034', 'rec_cur_loss': '1.2993', 'rec_avg_auc': '0.7899'}\n","{'Epoch': 6, 'HIT@1': '0.00214133', 'NDCG@1': '0.00214133', 'HIT@5': '0.00214133', 'NDCG@5': '0.00214133', 'HIT@10': '0.00856531', 'NDCG@10': '0.00414205', 'HIT@15': '0.00856531', 'NDCG@15': '0.00414205', 'HIT@20': '0.01070664', 'NDCG@20': '0.00466593', 'HIT@40': '0.01498929', 'NDCG@40': '0.00554166', 'MRR': '0.00321255'}\n","EarlyStopping counter: 6 out of 50\n","{'epoch': 7, 'rec_avg_loss': '1.2908', 'rec_cur_loss': '1.2872', 'rec_avg_auc': '0.8248'}\n","{'Epoch': 7, 'HIT@1': '0.00214133', 'NDCG@1': '0.00214133', 'HIT@5': '0.00428266', 'NDCG@5': '0.00296971', 'HIT@10': '0.00642398', 'NDCG@10': '0.00358869', 'HIT@15': '0.00642398', 'NDCG@15': '0.00358869', 'HIT@20': '0.01070664', 'NDCG@20': '0.00458029', 'HIT@40': '0.01284797', 'NDCG@40': '0.00500120', 'MRR': '0.00307464'}\n","EarlyStopping counter: 7 out of 50\n","{'epoch': 8, 'rec_avg_loss': '1.2765', 'rec_cur_loss': '1.2767', 'rec_avg_auc': '0.8554'}\n","{'Epoch': 8, 'HIT@1': '0.00214133', 'NDCG@1': '0.00214133', 'HIT@5': '0.00214133', 'NDCG@5': '0.00214133', 'HIT@10': '0.00428266', 'NDCG@10': '0.00290408', 'HIT@15': '0.00642398', 'NDCG@15': '0.00346650', 'HIT@20': '0.00856531', 'NDCG@20': '0.00399038', 'HIT@40': '0.01284797', 'NDCG@40': '0.00489628', 'MRR': '0.00296478'}\n","EarlyStopping counter: 8 out of 50\n","{'epoch': 9, 'rec_avg_loss': '1.2647', 'rec_cur_loss': '1.2628', 'rec_avg_auc': '0.8749'}\n","{'Epoch': 9, 'HIT@1': '0.00214133', 'NDCG@1': '0.00214133', 'HIT@5': '0.00214133', 'NDCG@5': '0.00214133', 'HIT@10': '0.00642398', 'NDCG@10': '0.00347409', 'HIT@15': '0.00642398', 'NDCG@15': '0.00347409', 'HIT@20': '0.00856531', 'NDCG@20': '0.00399796', 'HIT@40': '0.01284797', 'NDCG@40': '0.00486789', 'MRR': '0.00294142'}\n","EarlyStopping counter: 9 out of 50\n","{'epoch': 10, 'rec_avg_loss': '1.2502', 'rec_cur_loss': '1.2483', 'rec_avg_auc': '0.8932'}\n","{'Epoch': 10, 'HIT@1': '0.00214133', 'NDCG@1': '0.00214133', 'HIT@5': '0.00428266', 'NDCG@5': '0.00296971', 'HIT@10': '0.00642398', 'NDCG@10': '0.00364522', 'HIT@15': '0.00642398', 'NDCG@15': '0.00364522', 'HIT@20': '0.00856531', 'NDCG@20': '0.00414068', 'HIT@40': '0.01498929', 'NDCG@40': '0.00539349', 'MRR': '0.00313976'}\n","EarlyStopping counter: 10 out of 50\n","{'epoch': 11, 'rec_avg_loss': '1.2365', 'rec_cur_loss': '1.2320', 'rec_avg_auc': '0.9196'}\n","{'Epoch': 11, 'HIT@1': '0.00214133', 'NDCG@1': '0.00214133', 'HIT@5': '0.00428266', 'NDCG@5': '0.00306355', 'HIT@10': '0.00428266', 'NDCG@10': '0.00306355', 'HIT@15': '0.00642398', 'NDCG@15': '0.00359888', 'HIT@20': '0.00856531', 'NDCG@20': '0.00409434', 'HIT@40': '0.01498929', 'NDCG@40': '0.00545153', 'MRR': '0.00318418'}\n","EarlyStopping counter: 11 out of 50\n","{'epoch': 12, 'rec_avg_loss': '1.2240', 'rec_cur_loss': '1.2209', 'rec_avg_auc': '0.9329'}\n","{'Epoch': 12, 'HIT@1': '0.00214133', 'NDCG@1': '0.00214133', 'HIT@5': '0.00428266', 'NDCG@5': '0.00349235', 'HIT@10': '0.00428266', 'NDCG@10': '0.00349235', 'HIT@15': '0.00642398', 'NDCG@15': '0.00405477', 'HIT@20': '0.00856531', 'NDCG@20': '0.00456829', 'HIT@40': '0.01498929', 'NDCG@40': '0.00591884', 'MRR': '0.00375032'}\n","EarlyStopping counter: 12 out of 50\n","{'epoch': 13, 'rec_avg_loss': '1.2132', 'rec_cur_loss': '1.2094', 'rec_avg_auc': '0.9416'}\n","{'Epoch': 13, 'HIT@1': '0.00214133', 'NDCG@1': '0.00214133', 'HIT@5': '0.00428266', 'NDCG@5': '0.00321199', 'HIT@10': '0.00642398', 'NDCG@10': '0.00392577', 'HIT@15': '0.00856531', 'NDCG@15': '0.00447386', 'HIT@20': '0.00856531', 'NDCG@20': '0.00447386', 'HIT@40': '0.02355460', 'NDCG@40': '0.00736749', 'MRR': '0.00374039'}\n","EarlyStopping counter: 13 out of 50\n","{'epoch': 14, 'rec_avg_loss': '1.2000', 'rec_cur_loss': '1.1962', 'rec_avg_auc': '0.9547'}\n","{'Epoch': 14, 'HIT@1': '0.00214133', 'NDCG@1': '0.00214133', 'HIT@5': '0.00642398', 'NDCG@5': '0.00428266', 'HIT@10': '0.00642398', 'NDCG@10': '0.00428266', 'HIT@15': '0.00856531', 'NDCG@15': '0.00487996', 'HIT@20': '0.00856531', 'NDCG@20': '0.00487996', 'HIT@40': '0.02141328', 'NDCG@40': '0.00746445', 'MRR': '0.00419008'}\n","Validation score increased.  Saving model ...\n","{'epoch': 15, 'rec_avg_loss': '1.1861', 'rec_cur_loss': '1.1829', 'rec_avg_auc': '0.9641'}\n","{'Epoch': 15, 'HIT@1': '0.00214133', 'NDCG@1': '0.00214133', 'HIT@5': '0.00642398', 'NDCG@5': '0.00441457', 'HIT@10': '0.00856531', 'NDCG@10': '0.00505918', 'HIT@15': '0.00856531', 'NDCG@15': '0.00505918', 'HIT@20': '0.01284797', 'NDCG@20': '0.00609657', 'HIT@40': '0.02141328', 'NDCG@40': '0.00781558', 'MRR': '0.00452814'}\n","Validation score increased.  Saving model ...\n","{'epoch': 16, 'rec_avg_loss': '1.1739', 'rec_cur_loss': '1.1714', 'rec_avg_auc': '0.9698'}\n","{'Epoch': 16, 'HIT@1': '0.00214133', 'NDCG@1': '0.00214133', 'HIT@5': '0.00642398', 'NDCG@5': '0.00432073', 'HIT@10': '0.00856531', 'NDCG@10': '0.00496534', 'HIT@15': '0.01284797', 'NDCG@15': '0.00612267', 'HIT@20': '0.01498929', 'NDCG@20': '0.00662676', 'HIT@40': '0.02783726', 'NDCG@40': '0.00913927', 'MRR': '0.00474053'}\n","Validation score increased.  Saving model ...\n","{'epoch': 17, 'rec_avg_loss': '1.1627', 'rec_cur_loss': '1.1610', 'rec_avg_auc': '0.9753'}\n","{'Epoch': 17, 'HIT@1': '0.00214133', 'NDCG@1': '0.00214133', 'HIT@5': '0.00642398', 'NDCG@5': '0.00432073', 'HIT@10': '0.01070664', 'NDCG@10': '0.00567911', 'HIT@15': '0.01713062', 'NDCG@15': '0.00735984', 'HIT@20': '0.01713062', 'NDCG@20': '0.00735984', 'HIT@40': '0.03211991', 'NDCG@40': '0.01030250', 'MRR': '0.00512781'}\n","Validation score increased.  Saving model ...\n","{'epoch': 18, 'rec_avg_loss': '1.1506', 'rec_cur_loss': '1.1476', 'rec_avg_auc': '0.9792'}\n","{'Epoch': 18, 'HIT@1': '0.00214133', 'NDCG@1': '0.00214133', 'HIT@5': '0.00642398', 'NDCG@5': '0.00432073', 'HIT@10': '0.01498929', 'NDCG@10': '0.00694270', 'HIT@15': '0.01713062', 'NDCG@15': '0.00752137', 'HIT@20': '0.01927195', 'NDCG@20': '0.00800888', 'HIT@40': '0.02997859', 'NDCG@40': '0.01020593', 'MRR': '0.00530025'}\n","Validation score increased.  Saving model ...\n","{'epoch': 19, 'rec_avg_loss': '1.1418', 'rec_cur_loss': '1.1391', 'rec_avg_auc': '0.9795'}\n","{'Epoch': 19, 'HIT@1': '0.00214133', 'NDCG@1': '0.00214133', 'HIT@5': '0.00428266', 'NDCG@5': '0.00349235', 'HIT@10': '0.01498929', 'NDCG@10': '0.00696452', 'HIT@15': '0.01713062', 'NDCG@15': '0.00754319', 'HIT@20': '0.01927195', 'NDCG@20': '0.00804727', 'HIT@40': '0.03640257', 'NDCG@40': '0.01154242', 'MRR': '0.00554160'}\n","Validation score increased.  Saving model ...\n","{'epoch': 20, 'rec_avg_loss': '1.1276', 'rec_cur_loss': '1.1216', 'rec_avg_auc': '0.9857'}\n","{'Epoch': 20, 'HIT@1': '0.00214133', 'NDCG@1': '0.00214133', 'HIT@5': '0.00642398', 'NDCG@5': '0.00432073', 'HIT@10': '0.01713062', 'NDCG@10': '0.00774901', 'HIT@15': '0.01713062', 'NDCG@15': '0.00774901', 'HIT@20': '0.02141328', 'NDCG@20': '0.00874061', 'HIT@40': '0.03854390', 'NDCG@40': '0.01224029', 'MRR': '0.00586285'}\n","Validation score increased.  Saving model ...\n","{'epoch': 21, 'rec_avg_loss': '1.1171', 'rec_cur_loss': '1.1186', 'rec_avg_auc': '0.9856'}\n","{'Epoch': 21, 'HIT@1': '0.00214133', 'NDCG@1': '0.00214133', 'HIT@5': '0.00642398', 'NDCG@5': '0.00432073', 'HIT@10': '0.01284797', 'NDCG@10': '0.00644187', 'HIT@15': '0.01713062', 'NDCG@15': '0.00760160', 'HIT@20': '0.02141328', 'NDCG@20': '0.00861057', 'HIT@40': '0.04068522', 'NDCG@40': '0.01256960', 'MRR': '0.00582511'}\n","EarlyStopping counter: 1 out of 50\n","{'epoch': 22, 'rec_avg_loss': '1.1057', 'rec_cur_loss': '1.1089', 'rec_avg_auc': '0.9885'}\n","{'Epoch': 22, 'HIT@1': '0.00428266', 'NDCG@1': '0.00428266', 'HIT@5': '0.00856531', 'NDCG@5': '0.00593941', 'HIT@10': '0.01284797', 'NDCG@10': '0.00727217', 'HIT@15': '0.01713062', 'NDCG@15': '0.00843190', 'HIT@20': '0.02141328', 'NDCG@20': '0.00943144', 'HIT@40': '0.04068522', 'NDCG@40': '0.01346229', 'MRR': '0.00697899'}\n","Validation score increased.  Saving model ...\n","{'epoch': 23, 'rec_avg_loss': '1.0948', 'rec_cur_loss': '1.0907', 'rec_avg_auc': '0.9892'}\n","{'Epoch': 23, 'HIT@1': '0.00428266', 'NDCG@1': '0.00428266', 'HIT@5': '0.00856531', 'NDCG@5': '0.00603325', 'HIT@10': '0.01070664', 'NDCG@10': '0.00674703', 'HIT@15': '0.01713062', 'NDCG@15': '0.00844209', 'HIT@20': '0.01927195', 'NDCG@20': '0.00894618', 'HIT@40': '0.04282655', 'NDCG@40': '0.01384973', 'MRR': '0.00705293'}\n","Validation score increased.  Saving model ...\n","{'epoch': 24, 'rec_avg_loss': '1.0859', 'rec_cur_loss': '1.0875', 'rec_avg_auc': '0.9904'}\n","{'Epoch': 24, 'HIT@1': '0.00428266', 'NDCG@1': '0.00428266', 'HIT@5': '0.00642398', 'NDCG@5': '0.00511103', 'HIT@10': '0.01070664', 'NDCG@10': '0.00658757', 'HIT@15': '0.01498929', 'NDCG@15': '0.00774729', 'HIT@20': '0.01927195', 'NDCG@20': '0.00878469', 'HIT@40': '0.04496788', 'NDCG@40': '0.01409598', 'MRR': '0.00693024'}\n","EarlyStopping counter: 1 out of 50\n","{'epoch': 25, 'rec_avg_loss': '1.0767', 'rec_cur_loss': '1.0788', 'rec_avg_auc': '0.9895'}\n","{'Epoch': 25, 'HIT@1': '0.00428266', 'NDCG@1': '0.00428266', 'HIT@5': '0.00642398', 'NDCG@5': '0.00511103', 'HIT@10': '0.01070664', 'NDCG@10': '0.00653859', 'HIT@15': '0.01498929', 'NDCG@15': '0.00768398', 'HIT@20': '0.02355460', 'NDCG@20': '0.00967513', 'HIT@40': '0.04925054', 'NDCG@40': '0.01488293', 'MRR': '0.00700291'}\n","EarlyStopping counter: 2 out of 50\n","{'epoch': 26, 'rec_avg_loss': '1.0655', 'rec_cur_loss': '1.0641', 'rec_avg_auc': '0.9896'}\n","{'Epoch': 26, 'HIT@1': '0.00428266', 'NDCG@1': '0.00428266', 'HIT@5': '0.00642398', 'NDCG@5': '0.00535332', 'HIT@10': '0.01070664', 'NDCG@10': '0.00668608', 'HIT@15': '0.01498929', 'NDCG@15': '0.00783148', 'HIT@20': '0.02355460', 'NDCG@20': '0.00984862', 'HIT@40': '0.04925054', 'NDCG@40': '0.01510945', 'MRR': '0.00724777'}\n","Validation score increased.  Saving model ...\n","{'epoch': 27, 'rec_avg_loss': '1.0598', 'rec_cur_loss': '1.0582', 'rec_avg_auc': '0.9918'}\n","{'Epoch': 27, 'HIT@1': '0.00428266', 'NDCG@1': '0.00428266', 'HIT@5': '0.00642398', 'NDCG@5': '0.00535332', 'HIT@10': '0.01070664', 'NDCG@10': '0.00668608', 'HIT@15': '0.01498929', 'NDCG@15': '0.00782716', 'HIT@20': '0.02355460', 'NDCG@20': '0.00985467', 'HIT@40': '0.04925054', 'NDCG@40': '0.01513960', 'MRR': '0.00726614'}\n","Validation score increased.  Saving model ...\n","{'epoch': 28, 'rec_avg_loss': '1.0470', 'rec_cur_loss': '1.0463', 'rec_avg_auc': '0.9913'}\n","{'Epoch': 28, 'HIT@1': '0.00428266', 'NDCG@1': '0.00428266', 'HIT@5': '0.00642398', 'NDCG@5': '0.00535332', 'HIT@10': '0.01070664', 'NDCG@10': '0.00674261', 'HIT@15': '0.01498929', 'NDCG@15': '0.00788370', 'HIT@20': '0.02569593', 'NDCG@20': '0.01036236', 'HIT@40': '0.04925054', 'NDCG@40': '0.01518074', 'MRR': '0.00730709'}\n","Validation score increased.  Saving model ...\n","{'epoch': 29, 'rec_avg_loss': '1.0382', 'rec_cur_loss': '1.0399', 'rec_avg_auc': '0.9905'}\n","{'Epoch': 29, 'HIT@1': '0.00214133', 'NDCG@1': '0.00214133', 'HIT@5': '0.00642398', 'NDCG@5': '0.00428266', 'HIT@10': '0.01070664', 'NDCG@10': '0.00567194', 'HIT@15': '0.01498929', 'NDCG@15': '0.00674261', 'HIT@20': '0.02141328', 'NDCG@20': '0.00827373', 'HIT@40': '0.04925054', 'NDCG@40': '0.01401761', 'MRR': '0.00581048'}\n","EarlyStopping counter: 1 out of 50\n","{'epoch': 30, 'rec_avg_loss': '1.0275', 'rec_cur_loss': '1.0242', 'rec_avg_auc': '0.9922'}\n","{'Epoch': 30, 'HIT@1': '0.00214133', 'NDCG@1': '0.00214133', 'HIT@5': '0.00642398', 'NDCG@5': '0.00413421', 'HIT@10': '0.01070664', 'NDCG@10': '0.00546697', 'HIT@15': '0.01070664', 'NDCG@15': '0.00546697', 'HIT@20': '0.02141328', 'NDCG@20': '0.00799764', 'HIT@40': '0.04710921', 'NDCG@40': '0.01337040', 'MRR': '0.00549288'}\n","EarlyStopping counter: 2 out of 50\n","{'epoch': 31, 'rec_avg_loss': '1.0234', 'rec_cur_loss': '1.0224', 'rec_avg_auc': '0.9939'}\n","{'Epoch': 31, 'HIT@1': '0.00214133', 'NDCG@1': '0.00214133', 'HIT@5': '0.00428266', 'NDCG@5': '0.00306355', 'HIT@10': '0.00856531', 'NDCG@10': '0.00454008', 'HIT@15': '0.01284797', 'NDCG@15': '0.00567272', 'HIT@20': '0.02141328', 'NDCG@20': '0.00764879', 'HIT@40': '0.04282655', 'NDCG@40': '0.01215482', 'MRR': '0.00495561'}\n","EarlyStopping counter: 3 out of 50\n","{'epoch': 32, 'rec_avg_loss': '1.0113', 'rec_cur_loss': '1.0111', 'rec_avg_auc': '0.9920'}\n","{'Epoch': 32, 'HIT@1': '0.00214133', 'NDCG@1': '0.00214133', 'HIT@5': '0.00428266', 'NDCG@5': '0.00306355', 'HIT@10': '0.00856531', 'NDCG@10': '0.00449110', 'HIT@15': '0.01284797', 'NDCG@15': '0.00563650', 'HIT@20': '0.01927195', 'NDCG@20': '0.00713299', 'HIT@40': '0.04282655', 'NDCG@40': '0.01203953', 'MRR': '0.00486666'}\n","EarlyStopping counter: 4 out of 50\n","{'epoch': 33, 'rec_avg_loss': '1.0051', 'rec_cur_loss': '1.0012', 'rec_avg_auc': '0.9909'}\n","{'Epoch': 33, 'HIT@1': '0.00214133', 'NDCG@1': '0.00214133', 'HIT@5': '0.00428266', 'NDCG@5': '0.00306355', 'HIT@10': '0.00856531', 'NDCG@10': '0.00449110', 'HIT@15': '0.01284797', 'NDCG@15': '0.00560161', 'HIT@20': '0.02355460', 'NDCG@20': '0.00809119', 'HIT@40': '0.04282655', 'NDCG@40': '0.01204195', 'MRR': '0.00486397'}\n","EarlyStopping counter: 5 out of 50\n","{'epoch': 34, 'rec_avg_loss': '0.9948', 'rec_cur_loss': '0.9933', 'rec_avg_auc': '0.9905'}\n","{'Epoch': 34, 'HIT@1': '0.00214133', 'NDCG@1': '0.00214133', 'HIT@5': '0.00428266', 'NDCG@5': '0.00296971', 'HIT@10': '0.00856531', 'NDCG@10': '0.00439726', 'HIT@15': '0.01284797', 'NDCG@15': '0.00550777', 'HIT@20': '0.02141328', 'NDCG@20': '0.00753676', 'HIT@40': '0.04282655', 'NDCG@40': '0.01197258', 'MRR': '0.00477461'}\n","EarlyStopping counter: 6 out of 50\n","{'epoch': 35, 'rec_avg_loss': '0.9872', 'rec_cur_loss': '0.9848', 'rec_avg_auc': '0.9927'}\n","{'Epoch': 35, 'HIT@1': '0.00214133', 'NDCG@1': '0.00214133', 'HIT@5': '0.00428266', 'NDCG@5': '0.00306355', 'HIT@10': '0.00856531', 'NDCG@10': '0.00449110', 'HIT@15': '0.01284797', 'NDCG@15': '0.00560161', 'HIT@20': '0.02569593', 'NDCG@20': '0.00862922', 'HIT@40': '0.04496788', 'NDCG@40': '0.01254311', 'MRR': '0.00498567'}\n","EarlyStopping counter: 7 out of 50\n","{'epoch': 36, 'rec_avg_loss': '0.9805', 'rec_cur_loss': '0.9748', 'rec_avg_auc': '0.9941'}\n","{'Epoch': 36, 'HIT@1': '0.00214133', 'NDCG@1': '0.00214133', 'HIT@5': '0.00428266', 'NDCG@5': '0.00306355', 'HIT@10': '0.00856531', 'NDCG@10': '0.00449110', 'HIT@15': '0.01284797', 'NDCG@15': '0.00560161', 'HIT@20': '0.02783726', 'NDCG@20': '0.00917289', 'HIT@40': '0.04496788', 'NDCG@40': '0.01263526', 'MRR': '0.00504968'}\n","EarlyStopping counter: 8 out of 50\n","{'epoch': 37, 'rec_avg_loss': '0.9688', 'rec_cur_loss': '0.9660', 'rec_avg_auc': '0.9944'}\n","{'Epoch': 37, 'HIT@1': '0.00214133', 'NDCG@1': '0.00214133', 'HIT@5': '0.00428266', 'NDCG@5': '0.00306355', 'HIT@10': '0.00856531', 'NDCG@10': '0.00445284', 'HIT@15': '0.01713062', 'NDCG@15': '0.00663401', 'HIT@20': '0.02783726', 'NDCG@20': '0.00915777', 'HIT@40': '0.04282655', 'NDCG@40': '0.01222348', 'MRR': '0.00497816'}\n","EarlyStopping counter: 9 out of 50\n","{'epoch': 38, 'rec_avg_loss': '0.9684', 'rec_cur_loss': '0.9722', 'rec_avg_auc': '0.9917'}\n","{'Epoch': 38, 'HIT@1': '0.00214133', 'NDCG@1': '0.00214133', 'HIT@5': '0.00428266', 'NDCG@5': '0.00306355', 'HIT@10': '0.00642398', 'NDCG@10': '0.00377732', 'HIT@15': '0.01713062', 'NDCG@15': '0.00655580', 'HIT@20': '0.02569593', 'NDCG@20': '0.00858411', 'HIT@40': '0.04282655', 'NDCG@40': '0.01214591', 'MRR': '0.00490377'}\n","EarlyStopping counter: 10 out of 50\n","{'epoch': 39, 'rec_avg_loss': '0.9601', 'rec_cur_loss': '0.9646', 'rec_avg_auc': '0.9916'}\n","{'Epoch': 39, 'HIT@1': '0.00214133', 'NDCG@1': '0.00214133', 'HIT@5': '0.00428266', 'NDCG@5': '0.00306355', 'HIT@10': '0.00642398', 'NDCG@10': '0.00377732', 'HIT@15': '0.01927195', 'NDCG@15': '0.00701483', 'HIT@20': '0.02569593', 'NDCG@20': '0.00852789', 'HIT@40': '0.04282655', 'NDCG@40': '0.01209009', 'MRR': '0.00485634'}\n","EarlyStopping counter: 11 out of 50\n","{'epoch': 40, 'rec_avg_loss': '0.9572', 'rec_cur_loss': '0.9615', 'rec_avg_auc': '0.9916'}\n","{'Epoch': 40, 'HIT@1': '0.00214133', 'NDCG@1': '0.00214133', 'HIT@5': '0.00428266', 'NDCG@5': '0.00306355', 'HIT@10': '0.00642398', 'NDCG@10': '0.00377732', 'HIT@15': '0.01713062', 'NDCG@15': '0.00646674', 'HIT@20': '0.02569593', 'NDCG@20': '0.00849332', 'HIT@40': '0.04282655', 'NDCG@40': '0.01202577', 'MRR': '0.00481054'}\n","EarlyStopping counter: 12 out of 50\n","{'epoch': 41, 'rec_avg_loss': '0.9441', 'rec_cur_loss': '0.9385', 'rec_avg_auc': '0.9917'}\n","{'Epoch': 41, 'HIT@1': '0.00214133', 'NDCG@1': '0.00214133', 'HIT@5': '0.00428266', 'NDCG@5': '0.00306355', 'HIT@10': '0.00642398', 'NDCG@10': '0.00377732', 'HIT@15': '0.01284797', 'NDCG@15': '0.00538332', 'HIT@20': '0.02355460', 'NDCG@20': '0.00791720', 'HIT@40': '0.04282655', 'NDCG@40': '0.01189013', 'MRR': '0.00471382'}\n","EarlyStopping counter: 13 out of 50\n","{'epoch': 42, 'rec_avg_loss': '0.9431', 'rec_cur_loss': '0.9434', 'rec_avg_auc': '0.9908'}\n","{'Epoch': 42, 'HIT@1': '0.00214133', 'NDCG@1': '0.00214133', 'HIT@5': '0.00428266', 'NDCG@5': '0.00306355', 'HIT@10': '0.00642398', 'NDCG@10': '0.00373906', 'HIT@15': '0.00856531', 'NDCG@15': '0.00427439', 'HIT@20': '0.02141328', 'NDCG@20': '0.00732179', 'HIT@40': '0.04068522', 'NDCG@40': '0.01131117', 'MRR': '0.00452524'}\n","EarlyStopping counter: 14 out of 50\n","{'epoch': 43, 'rec_avg_loss': '0.9339', 'rec_cur_loss': '0.9311', 'rec_avg_auc': '0.9917'}\n","{'Epoch': 43, 'HIT@1': '0.00214133', 'NDCG@1': '0.00214133', 'HIT@5': '0.00428266', 'NDCG@5': '0.00306355', 'HIT@10': '0.00642398', 'NDCG@10': '0.00373906', 'HIT@15': '0.01070664', 'NDCG@15': '0.00480972', 'HIT@20': '0.01713062', 'NDCG@20': '0.00631415', 'HIT@40': '0.04068522', 'NDCG@40': '0.01121695', 'MRR': '0.00446290'}\n","EarlyStopping counter: 15 out of 50\n","{'epoch': 44, 'rec_avg_loss': '0.9274', 'rec_cur_loss': '0.9290', 'rec_avg_auc': '0.9906'}\n","{'Epoch': 44, 'HIT@1': '0.00214133', 'NDCG@1': '0.00214133', 'HIT@5': '0.00428266', 'NDCG@5': '0.00306355', 'HIT@10': '0.00642398', 'NDCG@10': '0.00373906', 'HIT@15': '0.01070664', 'NDCG@15': '0.00484957', 'HIT@20': '0.01498929', 'NDCG@20': '0.00585854', 'HIT@40': '0.04068522', 'NDCG@40': '0.01119913', 'MRR': '0.00445866'}\n","EarlyStopping counter: 16 out of 50\n","{'epoch': 45, 'rec_avg_loss': '0.9199', 'rec_cur_loss': '0.9328', 'rec_avg_auc': '0.9915'}\n","{'Epoch': 45, 'HIT@1': '0.00214133', 'NDCG@1': '0.00214133', 'HIT@5': '0.00428266', 'NDCG@5': '0.00321199', 'HIT@10': '0.00642398', 'NDCG@10': '0.00388751', 'HIT@15': '0.01070664', 'NDCG@15': '0.00499801', 'HIT@20': '0.01284797', 'NDCG@20': '0.00552189', 'HIT@40': '0.03854390', 'NDCG@40': '0.01088243', 'MRR': '0.00454266'}\n","EarlyStopping counter: 17 out of 50\n","{'epoch': 46, 'rec_avg_loss': '0.9082', 'rec_cur_loss': '0.9063', 'rec_avg_auc': '0.9908'}\n","{'Epoch': 46, 'HIT@1': '0.00214133', 'NDCG@1': '0.00214133', 'HIT@5': '0.00428266', 'NDCG@5': '0.00321199', 'HIT@10': '0.00642398', 'NDCG@10': '0.00388751', 'HIT@15': '0.00856531', 'NDCG@15': '0.00443560', 'HIT@20': '0.01284797', 'NDCG@20': '0.00547299', 'HIT@40': '0.03854390', 'NDCG@40': '0.01076889', 'MRR': '0.00446460'}\n","EarlyStopping counter: 18 out of 50\n","{'epoch': 47, 'rec_avg_loss': '0.9040', 'rec_cur_loss': '0.9010', 'rec_avg_auc': '0.9901'}\n","{'Epoch': 47, 'HIT@1': '0.00214133', 'NDCG@1': '0.00214133', 'HIT@5': '0.00428266', 'NDCG@5': '0.00321199', 'HIT@10': '0.00642398', 'NDCG@10': '0.00385660', 'HIT@15': '0.00856531', 'NDCG@15': '0.00440469', 'HIT@20': '0.01284797', 'NDCG@20': '0.00543172', 'HIT@40': '0.03854390', 'NDCG@40': '0.01068726', 'MRR': '0.00440277'}\n","EarlyStopping counter: 19 out of 50\n","{'epoch': 48, 'rec_avg_loss': '0.9040', 'rec_cur_loss': '0.8955', 'rec_avg_auc': '0.9915'}\n","{'Epoch': 48, 'HIT@1': '0.00214133', 'NDCG@1': '0.00214133', 'HIT@5': '0.00428266', 'NDCG@5': '0.00321199', 'HIT@10': '0.00428266', 'NDCG@10': '0.00321199', 'HIT@15': '0.00856531', 'NDCG@15': '0.00434463', 'HIT@20': '0.01284797', 'NDCG@20': '0.00537260', 'HIT@40': '0.03426124', 'NDCG@40': '0.00982588', 'MRR': '0.00424239'}\n","EarlyStopping counter: 20 out of 50\n","{'epoch': 49, 'rec_avg_loss': '0.8965', 'rec_cur_loss': '0.8800', 'rec_avg_auc': '0.9906'}\n","{'Epoch': 49, 'HIT@1': '0.00214133', 'NDCG@1': '0.00214133', 'HIT@5': '0.00428266', 'NDCG@5': '0.00321199', 'HIT@10': '0.00428266', 'NDCG@10': '0.00321199', 'HIT@15': '0.00856531', 'NDCG@15': '0.00434463', 'HIT@20': '0.01284797', 'NDCG@20': '0.00538203', 'HIT@40': '0.03426124', 'NDCG@40': '0.00982265', 'MRR': '0.00424275'}\n","EarlyStopping counter: 21 out of 50\n","{'epoch': 50, 'rec_avg_loss': '0.8907', 'rec_cur_loss': '0.8916', 'rec_avg_auc': '0.9884'}\n","{'Epoch': 50, 'HIT@1': '0.00214133', 'NDCG@1': '0.00214133', 'HIT@5': '0.00428266', 'NDCG@5': '0.00349235', 'HIT@10': '0.00642398', 'NDCG@10': '0.00411134', 'HIT@15': '0.00642398', 'NDCG@15': '0.00411134', 'HIT@20': '0.01284797', 'NDCG@20': '0.00565282', 'HIT@40': '0.03211991', 'NDCG@40': '0.00966820', 'MRR': '0.00452724'}\n","EarlyStopping counter: 22 out of 50\n","{'epoch': 51, 'rec_avg_loss': '0.8830', 'rec_cur_loss': '0.8801', 'rec_avg_auc': '0.9912'}\n","{'Epoch': 51, 'HIT@1': '0.00214133', 'NDCG@1': '0.00214133', 'HIT@5': '0.00428266', 'NDCG@5': '0.00321199', 'HIT@10': '0.00642398', 'NDCG@10': '0.00383097', 'HIT@15': '0.00642398', 'NDCG@15': '0.00383097', 'HIT@20': '0.01498929', 'NDCG@20': '0.00586618', 'HIT@40': '0.03426124', 'NDCG@40': '0.00979756', 'MRR': '0.00423040'}\n","EarlyStopping counter: 23 out of 50\n","{'epoch': 52, 'rec_avg_loss': '0.8772', 'rec_cur_loss': '0.8804', 'rec_avg_auc': '0.9895'}\n","{'Epoch': 52, 'HIT@1': '0.00214133', 'NDCG@1': '0.00214133', 'HIT@5': '0.00428266', 'NDCG@5': '0.00306355', 'HIT@10': '0.00642398', 'NDCG@10': '0.00368253', 'HIT@15': '0.00856531', 'NDCG@15': '0.00421786', 'HIT@20': '0.01498929', 'NDCG@20': '0.00571435', 'HIT@40': '0.03640257', 'NDCG@40': '0.01002081', 'MRR': '0.00408977'}\n","EarlyStopping counter: 24 out of 50\n","{'epoch': 53, 'rec_avg_loss': '0.8758', 'rec_cur_loss': '0.8774', 'rec_avg_auc': '0.9913'}\n","{'Epoch': 53, 'HIT@1': '0.00214133', 'NDCG@1': '0.00214133', 'HIT@5': '0.00428266', 'NDCG@5': '0.00306355', 'HIT@10': '0.00642398', 'NDCG@10': '0.00368253', 'HIT@15': '0.01070664', 'NDCG@15': '0.00476595', 'HIT@20': '0.01284797', 'NDCG@20': '0.00525347', 'HIT@40': '0.03640257', 'NDCG@40': '0.01000768', 'MRR': '0.00408682'}\n","EarlyStopping counter: 25 out of 50\n","{'epoch': 54, 'rec_avg_loss': '0.8675', 'rec_cur_loss': '0.8562', 'rec_avg_auc': '0.9886'}\n","{'Epoch': 54, 'HIT@1': '0.00214133', 'NDCG@1': '0.00214133', 'HIT@5': '0.00428266', 'NDCG@5': '0.00306355', 'HIT@10': '0.00642398', 'NDCG@10': '0.00368253', 'HIT@15': '0.01070664', 'NDCG@15': '0.00477871', 'HIT@20': '0.01498929', 'NDCG@20': '0.00576962', 'HIT@40': '0.03854390', 'NDCG@40': '0.01043911', 'MRR': '0.00416361'}\n","EarlyStopping counter: 26 out of 50\n","{'epoch': 55, 'rec_avg_loss': '0.8623', 'rec_cur_loss': '0.8703', 'rec_avg_auc': '0.9875'}\n","{'Epoch': 55, 'HIT@1': '0.00214133', 'NDCG@1': '0.00214133', 'HIT@5': '0.00428266', 'NDCG@5': '0.00306355', 'HIT@10': '0.00642398', 'NDCG@10': '0.00368253', 'HIT@15': '0.01070664', 'NDCG@15': '0.00482362', 'HIT@20': '0.01498929', 'NDCG@20': '0.00583259', 'HIT@40': '0.03426124', 'NDCG@40': '0.00966026', 'MRR': '0.00408289'}\n","EarlyStopping counter: 27 out of 50\n","{'epoch': 56, 'rec_avg_loss': '0.8557', 'rec_cur_loss': '0.8603', 'rec_avg_auc': '0.9902'}\n","{'Epoch': 56, 'HIT@1': '0.00214133', 'NDCG@1': '0.00214133', 'HIT@5': '0.00428266', 'NDCG@5': '0.00321199', 'HIT@10': '0.00642398', 'NDCG@10': '0.00383097', 'HIT@15': '0.01284797', 'NDCG@15': '0.00552603', 'HIT@20': '0.01498929', 'NDCG@20': '0.00601355', 'HIT@40': '0.03211991', 'NDCG@40': '0.00943728', 'MRR': '0.00423419'}\n","EarlyStopping counter: 28 out of 50\n","{'epoch': 57, 'rec_avg_loss': '0.8496', 'rec_cur_loss': '0.8480', 'rec_avg_auc': '0.9895'}\n","{'Epoch': 57, 'HIT@1': '0.00214133', 'NDCG@1': '0.00214133', 'HIT@5': '0.00428266', 'NDCG@5': '0.00349235', 'HIT@10': '0.00642398', 'NDCG@10': '0.00413696', 'HIT@15': '0.01284797', 'NDCG@15': '0.00584478', 'HIT@20': '0.01498929', 'NDCG@20': '0.00633229', 'HIT@40': '0.03426124', 'NDCG@40': '0.01017521', 'MRR': '0.00468948'}\n","EarlyStopping counter: 29 out of 50\n","{'epoch': 58, 'rec_avg_loss': '0.8461', 'rec_cur_loss': '0.8400', 'rec_avg_auc': '0.9856'}\n","{'Epoch': 58, 'HIT@1': '0.00214133', 'NDCG@1': '0.00214133', 'HIT@5': '0.00428266', 'NDCG@5': '0.00349235', 'HIT@10': '0.00642398', 'NDCG@10': '0.00416787', 'HIT@15': '0.01284797', 'NDCG@15': '0.00585705', 'HIT@20': '0.01498929', 'NDCG@20': '0.00634456', 'HIT@40': '0.03426124', 'NDCG@40': '0.01022758', 'MRR': '0.00472589'}\n","EarlyStopping counter: 30 out of 50\n","{'epoch': 59, 'rec_avg_loss': '0.8464', 'rec_cur_loss': '0.8527', 'rec_avg_auc': '0.9890'}\n","{'Epoch': 59, 'HIT@1': '0.00214133', 'NDCG@1': '0.00214133', 'HIT@5': '0.00428266', 'NDCG@5': '0.00349235', 'HIT@10': '0.00642398', 'NDCG@10': '0.00416787', 'HIT@15': '0.01284797', 'NDCG@15': '0.00588762', 'HIT@20': '0.01498929', 'NDCG@20': '0.00638308', 'HIT@40': '0.03426124', 'NDCG@40': '0.01025735', 'MRR': '0.00475239'}\n","EarlyStopping counter: 31 out of 50\n","{'epoch': 60, 'rec_avg_loss': '0.8399', 'rec_cur_loss': '0.8357', 'rec_avg_auc': '0.9879'}\n","{'Epoch': 60, 'HIT@1': '0.00214133', 'NDCG@1': '0.00214133', 'HIT@5': '0.00428266', 'NDCG@5': '0.00349235', 'HIT@10': '0.00856531', 'NDCG@10': '0.00478685', 'HIT@15': '0.01284797', 'NDCG@15': '0.00592794', 'HIT@20': '0.01498929', 'NDCG@20': '0.00643203', 'HIT@40': '0.03211991', 'NDCG@40': '0.00987578', 'MRR': '0.00472419'}\n","EarlyStopping counter: 32 out of 50\n","{'epoch': 61, 'rec_avg_loss': '0.8280', 'rec_cur_loss': '0.8313', 'rec_avg_auc': '0.9875'}\n","{'Epoch': 61, 'HIT@1': '0.00214133', 'NDCG@1': '0.00214133', 'HIT@5': '0.00428266', 'NDCG@5': '0.00321199', 'HIT@10': '0.00856531', 'NDCG@10': '0.00453211', 'HIT@15': '0.01070664', 'NDCG@15': '0.00509453', 'HIT@20': '0.01498929', 'NDCG@20': '0.00612249', 'HIT@40': '0.02997859', 'NDCG@40': '0.00912791', 'MRR': '0.00427104'}\n","EarlyStopping counter: 33 out of 50\n","{'epoch': 62, 'rec_avg_loss': '0.8244', 'rec_cur_loss': '0.8345', 'rec_avg_auc': '0.9904'}\n","{'Epoch': 62, 'HIT@1': '0.00214133', 'NDCG@1': '0.00214133', 'HIT@5': '0.00428266', 'NDCG@5': '0.00321199', 'HIT@10': '0.00856531', 'NDCG@10': '0.00453211', 'HIT@15': '0.01070664', 'NDCG@15': '0.00509453', 'HIT@20': '0.01498929', 'NDCG@20': '0.00609407', 'HIT@40': '0.02783726', 'NDCG@40': '0.00871227', 'MRR': '0.00420309'}\n","EarlyStopping counter: 34 out of 50\n","{'epoch': 63, 'rec_avg_loss': '0.8205', 'rec_cur_loss': '0.8220', 'rec_avg_auc': '0.9903'}\n","{'Epoch': 63, 'HIT@1': '0.00214133', 'NDCG@1': '0.00214133', 'HIT@5': '0.00428266', 'NDCG@5': '0.00349235', 'HIT@10': '0.00856531', 'NDCG@10': '0.00488164', 'HIT@15': '0.01070664', 'NDCG@15': '0.00542973', 'HIT@20': '0.01498929', 'NDCG@20': '0.00642928', 'HIT@40': '0.02783726', 'NDCG@40': '0.00905101', 'MRR': '0.00461821'}\n","EarlyStopping counter: 35 out of 50\n","{'epoch': 64, 'rec_avg_loss': '0.8046', 'rec_cur_loss': '0.8043', 'rec_avg_auc': '0.9908'}\n","{'Epoch': 64, 'HIT@1': '0.00214133', 'NDCG@1': '0.00214133', 'HIT@5': '0.00428266', 'NDCG@5': '0.00349235', 'HIT@10': '0.00856531', 'NDCG@10': '0.00488164', 'HIT@15': '0.01070664', 'NDCG@15': '0.00541698', 'HIT@20': '0.01498929', 'NDCG@20': '0.00640789', 'HIT@40': '0.03211991', 'NDCG@40': '0.00984720', 'MRR': '0.00471927'}\n","EarlyStopping counter: 36 out of 50\n","{'epoch': 65, 'rec_avg_loss': '0.8057', 'rec_cur_loss': '0.8077', 'rec_avg_auc': '0.9889'}\n","{'Epoch': 65, 'HIT@1': '0.00214133', 'NDCG@1': '0.00214133', 'HIT@5': '0.00428266', 'NDCG@5': '0.00349235', 'HIT@10': '0.00856531', 'NDCG@10': '0.00488164', 'HIT@15': '0.00856531', 'NDCG@15': '0.00488164', 'HIT@20': '0.01498929', 'NDCG@20': '0.00638677', 'HIT@40': '0.02997859', 'NDCG@40': '0.00944274', 'MRR': '0.00465860'}\n","EarlyStopping counter: 37 out of 50\n","{'epoch': 66, 'rec_avg_loss': '0.8014', 'rec_cur_loss': '0.8002', 'rec_avg_auc': '0.9898'}\n","{'Epoch': 66, 'HIT@1': '0.00214133', 'NDCG@1': '0.00214133', 'HIT@5': '0.00428266', 'NDCG@5': '0.00349235', 'HIT@10': '0.00856531', 'NDCG@10': '0.00488164', 'HIT@15': '0.00856531', 'NDCG@15': '0.00488164', 'HIT@20': '0.01498929', 'NDCG@20': '0.00638849', 'HIT@40': '0.02997859', 'NDCG@40': '0.00945799', 'MRR': '0.00466848'}\n","EarlyStopping counter: 38 out of 50\n","{'epoch': 67, 'rec_avg_loss': '0.7962', 'rec_cur_loss': '0.8004', 'rec_avg_auc': '0.9882'}\n","{'Epoch': 67, 'HIT@1': '0.00214133', 'NDCG@1': '0.00214133', 'HIT@5': '0.00428266', 'NDCG@5': '0.00349235', 'HIT@10': '0.00856531', 'NDCG@10': '0.00496889', 'HIT@15': '0.00856531', 'NDCG@15': '0.00496889', 'HIT@20': '0.01498929', 'NDCG@20': '0.00647401', 'HIT@40': '0.02997859', 'NDCG@40': '0.00953063', 'MRR': '0.00474832'}\n","EarlyStopping counter: 39 out of 50\n","{'epoch': 68, 'rec_avg_loss': '0.7874', 'rec_cur_loss': '0.7798', 'rec_avg_auc': '0.9897'}\n","{'Epoch': 68, 'HIT@1': '0.00214133', 'NDCG@1': '0.00214133', 'HIT@5': '0.00642398', 'NDCG@5': '0.00441457', 'HIT@10': '0.00856531', 'NDCG@10': '0.00512835', 'HIT@15': '0.00856531', 'NDCG@15': '0.00512835', 'HIT@20': '0.01498929', 'NDCG@20': '0.00666120', 'HIT@40': '0.03211991', 'NDCG@40': '0.01015361', 'MRR': '0.00502377'}\n","EarlyStopping counter: 40 out of 50\n","{'epoch': 69, 'rec_avg_loss': '0.7875', 'rec_cur_loss': '0.7919', 'rec_avg_auc': '0.9867'}\n","{'Epoch': 69, 'HIT@1': '0.00214133', 'NDCG@1': '0.00214133', 'HIT@5': '0.00642398', 'NDCG@5': '0.00441457', 'HIT@10': '0.00856531', 'NDCG@10': '0.00512835', 'HIT@15': '0.01070664', 'NDCG@15': '0.00567644', 'HIT@20': '0.01498929', 'NDCG@20': '0.00667747', 'HIT@40': '0.03211991', 'NDCG@40': '0.01020426', 'MRR': '0.00505877'}\n","EarlyStopping counter: 41 out of 50\n","{'epoch': 70, 'rec_avg_loss': '0.7810', 'rec_cur_loss': '0.7834', 'rec_avg_auc': '0.9880'}\n","{'Epoch': 70, 'HIT@1': '0.00214133', 'NDCG@1': '0.00214133', 'HIT@5': '0.00642398', 'NDCG@5': '0.00456302', 'HIT@10': '0.00856531', 'NDCG@10': '0.00527679', 'HIT@15': '0.01284797', 'NDCG@15': '0.00636022', 'HIT@20': '0.01498929', 'NDCG@20': '0.00686430', 'HIT@40': '0.03211991', 'NDCG@40': '0.01042089', 'MRR': '0.00528440'}\n","EarlyStopping counter: 42 out of 50\n","{'epoch': 71, 'rec_avg_loss': '0.7687', 'rec_cur_loss': '0.7627', 'rec_avg_auc': '0.9890'}\n","{'Epoch': 71, 'HIT@1': '0.00214133', 'NDCG@1': '0.00214133', 'HIT@5': '0.00642398', 'NDCG@5': '0.00456302', 'HIT@10': '0.00856531', 'NDCG@10': '0.00527679', 'HIT@15': '0.01284797', 'NDCG@15': '0.00636022', 'HIT@20': '0.01713062', 'NDCG@20': '0.00736125', 'HIT@40': '0.03211991', 'NDCG@40': '0.01042765', 'MRR': '0.00528939'}\n","EarlyStopping counter: 43 out of 50\n","{'epoch': 72, 'rec_avg_loss': '0.7714', 'rec_cur_loss': '0.7629', 'rec_avg_auc': '0.9901'}\n","{'Epoch': 72, 'HIT@1': '0.00000000', 'NDCG@1': '0.00000000', 'HIT@5': '0.00642398', 'NDCG@5': '0.00377272', 'HIT@10': '0.00856531', 'NDCG@10': '0.00448649', 'HIT@15': '0.01070664', 'NDCG@15': '0.00503458', 'HIT@20': '0.01713062', 'NDCG@20': '0.00656743', 'HIT@40': '0.03426124', 'NDCG@40': '0.01000736', 'MRR': '0.00425234'}\n","EarlyStopping counter: 44 out of 50\n","{'epoch': 73, 'rec_avg_loss': '0.7614', 'rec_cur_loss': '0.7654', 'rec_avg_auc': '0.9904'}\n","{'Epoch': 73, 'HIT@1': '0.00000000', 'NDCG@1': '0.00000000', 'HIT@5': '0.00642398', 'NDCG@5': '0.00377272', 'HIT@10': '0.00856531', 'NDCG@10': '0.00448649', 'HIT@15': '0.01070664', 'NDCG@15': '0.00503458', 'HIT@20': '0.01713062', 'NDCG@20': '0.00654765', 'HIT@40': '0.02997859', 'NDCG@40': '0.00916082', 'MRR': '0.00411467'}\n","EarlyStopping counter: 45 out of 50\n","{'epoch': 74, 'rec_avg_loss': '0.7580', 'rec_cur_loss': '0.7559', 'rec_avg_auc': '0.9859'}\n","{'Epoch': 74, 'HIT@1': '0.00000000', 'NDCG@1': '0.00000000', 'HIT@5': '0.00642398', 'NDCG@5': '0.00349235', 'HIT@10': '0.00856531', 'NDCG@10': '0.00420613', 'HIT@15': '0.01070664', 'NDCG@15': '0.00476855', 'HIT@20': '0.01713062', 'NDCG@20': '0.00627367', 'HIT@40': '0.02997859', 'NDCG@40': '0.00886919', 'MRR': '0.00375398'}\n","EarlyStopping counter: 46 out of 50\n","{'epoch': 75, 'rec_avg_loss': '0.7578', 'rec_cur_loss': '0.7552', 'rec_avg_auc': '0.9878'}\n","{'Epoch': 75, 'HIT@1': '0.00214133', 'NDCG@1': '0.00214133', 'HIT@5': '0.00642398', 'NDCG@5': '0.00428266', 'HIT@10': '0.00856531', 'NDCG@10': '0.00499643', 'HIT@15': '0.01070664', 'NDCG@15': '0.00557510', 'HIT@20': '0.01713062', 'NDCG@20': '0.00708022', 'HIT@40': '0.02997859', 'NDCG@40': '0.00965289', 'MRR': '0.00482581'}\n","EarlyStopping counter: 47 out of 50\n","{'epoch': 76, 'rec_avg_loss': '0.7519', 'rec_cur_loss': '0.7546', 'rec_avg_auc': '0.9906'}\n","{'Epoch': 76, 'HIT@1': '0.00214133', 'NDCG@1': '0.00214133', 'HIT@5': '0.00642398', 'NDCG@5': '0.00428266', 'HIT@10': '0.00856531', 'NDCG@10': '0.00499643', 'HIT@15': '0.01284797', 'NDCG@15': '0.00611043', 'HIT@20': '0.01927195', 'NDCG@20': '0.00758955', 'HIT@40': '0.03211991', 'NDCG@40': '0.01008512', 'MRR': '0.00490256'}\n","EarlyStopping counter: 48 out of 50\n","{'epoch': 77, 'rec_avg_loss': '0.7374', 'rec_cur_loss': '0.7209', 'rec_avg_auc': '0.9889'}\n","{'Epoch': 77, 'HIT@1': '0.00214133', 'NDCG@1': '0.00214133', 'HIT@5': '0.00642398', 'NDCG@5': '0.00441457', 'HIT@10': '0.01070664', 'NDCG@10': '0.00574733', 'HIT@15': '0.01284797', 'NDCG@15': '0.00629542', 'HIT@20': '0.01927195', 'NDCG@20': '0.00781791', 'HIT@40': '0.03211991', 'NDCG@40': '0.01031141', 'MRR': '0.00515750'}\n","EarlyStopping counter: 49 out of 50\n","{'epoch': 78, 'rec_avg_loss': '0.7378', 'rec_cur_loss': '0.7239', 'rec_avg_auc': '0.9886'}\n","{'Epoch': 78, 'HIT@1': '0.00214133', 'NDCG@1': '0.00214133', 'HIT@5': '0.00642398', 'NDCG@5': '0.00441457', 'HIT@10': '0.00856531', 'NDCG@10': '0.00512835', 'HIT@15': '0.01498929', 'NDCG@15': '0.00682341', 'HIT@20': '0.01927195', 'NDCG@20': '0.00783238', 'HIT@40': '0.03211991', 'NDCG@40': '0.01035837', 'MRR': '0.00518570'}\n","EarlyStopping counter: 50 out of 50\n","Early stopping\n","---------------Change to test_rating_matrix!-------------------\n","{'Epoch': 'best', 'HIT@1': '0.00428266', 'NDCG@1': '0.00428266', 'HIT@5': '0.00642398', 'NDCG@5': '0.00535332', 'HIT@10': '0.01070664', 'NDCG@10': '0.00674261', 'HIT@15': '0.01498929', 'NDCG@15': '0.00788370', 'HIT@20': '0.02569593', 'NDCG@20': '0.01036236', 'HIT@40': '0.04925054', 'NDCG@40': '0.01518074', 'MRR': '0.00730709'}\n","{'Epoch': 'best', 'HIT@1': '0.00000000', 'NDCG@1': '0.00000000', 'HIT@5': '0.00642398', 'NDCG@5': '0.00353043', 'HIT@10': '0.00856531', 'NDCG@10': '0.00414942', 'HIT@15': '0.01713062', 'NDCG@15': '0.00643590', 'HIT@20': '0.01927195', 'NDCG@20': '0.00693999', 'HIT@40': '0.02569593', 'NDCG@40': '0.00828610', 'MRR': '0.00383749'}\n"]}]},{"cell_type":"code","source":["print(args_str)\n","#print(result_info)\n","with open(args.log_file, 'a') as f:\n","    f.write(args_str + '\\n')\n","    f.write(result_info + '\\n')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BrS2DYuHV_8l","executionInfo":{"status":"ok","timestamp":1642782806789,"user_tz":-330,"elapsed":519,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}},"outputId":"5fe533f1-ade7-4870-b3ca-b2e38912f37e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Finetune_full-Beauty-64-2-2-gelu-0.5-0.5-50-0.001-0.0-10-1.0-0.1\n"]}]},{"cell_type":"code","source":[""],"metadata":{"id":"qlXzHibVfE4d"},"execution_count":null,"outputs":[]}]}