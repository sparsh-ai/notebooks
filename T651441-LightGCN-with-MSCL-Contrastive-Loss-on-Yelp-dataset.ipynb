{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"T651441 | LightGCN with MSCL Contrastive Loss on Yelp dataset","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyN5rXZ/bhIoPBeapuiVEnV8"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"id":"2mKh3mM6wllC"},"source":["import argparse\n","\n","\n","def parse_args():\n","    parser = argparse.ArgumentParser(description=\"Go lightGCN\")\n","    parser.add_argument('--bpr_batch', type=int,default=2048,\n","                        help=\"the batch size for bpr loss training procedure\")\n","    parser.add_argument('--recdim', type=int,default=64,\n","                        help=\"the embedding size of lightGCN\")\n","    parser.add_argument('--layer', type=int,default=2,\n","                        help=\"the layer num of lightGCN\")\n","    parser.add_argument('--lr', type=float,default=0.001,\n","                        help=\"the learning rate\")\n","    parser.add_argument('--decay', type=float,default=1e-4,\n","                        help=\"the weight decay for l2 normalizaton\")\n","    parser.add_argument('--dropout', type=int,default=0,\n","                        help=\"using the dropout or not\")\n","    parser.add_argument('--keep_prob', type=float,default=0.9,\n","                        help=\"the batch size for bpr loss training procedure\")\n","    # parser.add_argument('--maskfea', type=float,default=0.0,\n","    #                     help=\"the batch size for bpr loss training procedure\")\n","    parser.add_argument('--a_fold', type=int,default=100,\n","                        help=\"the fold num used to split large adj matrix, like gowalla\")\n","    parser.add_argument('--testbatch', type=int,default=100,\n","                        help=\"the batch size of users for testing\")\n","    parser.add_argument('--dataset', type=str,default='yelp2018',\n","                        help=\"available datasets: [gowalla, yelp2018, amazon-book]\")\n","    parser.add_argument('--path', type=str,default=\"./checkpoints\",\n","                        help=\"path to save weights\")\n","    parser.add_argument('--topks', nargs='?',default=\"[20]\",\n","                        help=\"@k test list\")\n","    parser.add_argument('--tensorboard', type=int,default=1,\n","                        help=\"enable tensorboard\")\n","    parser.add_argument('--comment', type=str,default=\"lgn\")\n","    parser.add_argument('--load', type=int,default=0)\n","    parser.add_argument('--epochs', type=int,default=100)\n","    parser.add_argument('--multicore', type=int, default=0, help='whether we use multiprocessing or not in test')\n","    parser.add_argument('--pretrain', type=int, default=0, help='whether we use pretrained weight or not')\n","    parser.add_argument('--seed', type=int, default=2020, help='random seed')\n","    parser.add_argument('--model', type=str, default='lgn', help='rec-model, support [mf, lgn]')\n","    parser.add_argument('--info', type=str, default='' )\n","    parser.add_argument('--temperature', type=float, default=0.1, help='temperature')\n","    parser.add_argument('--methods', type=str,default='LightGCN', )\n","    return parser.parse_args(args={})"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"sZsUudt8v9SX"},"source":["import os\n","from os.path import join\n","import torch\n","from enum import Enum\n","import multiprocessing\n","\n","os.environ['KMP_DUPLICATE_LIB_OK'] = 'True'\n","args = parse_args()\n","\n","ROOT_PATH = \"./LightGCNMultiCL/\"\n","CODE_PATH = join(ROOT_PATH, 'code')\n","DATA_PATH = join(ROOT_PATH, 'data')\n","BOARD_PATH = join(CODE_PATH, 'runs3')\n","FILE_PATH = join(CODE_PATH, 'checkpoints')\n","import sys\n","sys.path.append(join(CODE_PATH, 'sources'))\n","\n","\n","if not os.path.exists(FILE_PATH):\n","    os.makedirs(FILE_PATH, exist_ok=True)\n","\n","\n","config = {}\n","all_dataset = ['gowalla', 'yelp2018', 'amazon-book', 'pinterest', 'steam','ifashion']\n","all_models  = ['mf', 'lgn']\n","# config['batch_size'] = 4096\n","config['bpr_batch_size'] = args.bpr_batch\n","config['latent_dim_rec'] = args.recdim\n","config['lightGCN_n_layers']= args.layer\n","config['dropout'] = args.dropout  \n","config['keep_prob']=args.keep_prob\n","config['A_n_fold'] = args.a_fold\n","config['test_u_batch_size'] = args.testbatch\n","config['multicore'] = args.multicore\n","config['lr'] = args.lr\n","config['decay'] = args.decay\n","config['pretrain'] = args.pretrain\n","config['A_split'] = False\n","config['bigdata'] = False\n","config['dataset'] = args.dataset\n","config['info'] = args.info\n","config['temperature'] = args.temperature\n","config['methods'] = args.methods\n","\n","GPU = torch.cuda.is_available()\n","device = torch.device('cuda' if GPU else \"cpu\")\n","CORES = multiprocessing.cpu_count() // 2\n","seed = args.seed\n","\n","dataset = args.dataset\n","model_name = args.model\n","if dataset not in all_dataset:\n","    raise NotImplementedError(f\"Haven't supported {dataset} yet!, try {all_dataset}\")\n","if model_name not in all_models:\n","    raise NotImplementedError(f\"Haven't supported {model_name} yet!, try {all_models}\")\n","\n","\n","TRAIN_epochs = args.epochs\n","LOAD = args.load\n","PATH = args.path\n","topks = eval(args.topks)\n","tensorboard = args.tensorboard\n","comment = args.comment\n","# let pandas shut up\n","from warnings import simplefilter\n","simplefilter(action=\"ignore\", category=FutureWarning)\n","\n","def cprint(words : str):\n","    print(f\"\\033[0;30;43m{words}\\033[0m\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"rbInFFI9w6u-"},"source":["import os\n","from os.path import join\n","import sys\n","import torch\n","import numpy as np\n","import pandas as pd\n","from torch.utils.data import Dataset, DataLoader\n","from scipy.sparse import csr_matrix\n","import scipy.sparse as sp\n","from time import time"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"aXem_nB9xM-S"},"source":["class BasicDataset(Dataset):\n","    def __init__(self):\n","        print(\"init dataset\")\n","    \n","    @property\n","    def n_users(self):\n","        raise NotImplementedError\n","    \n","    @property\n","    def m_items(self):\n","        raise NotImplementedError\n","    \n","    @property\n","    def trainDataSize(self):\n","        raise NotImplementedError\n","    \n","    @property\n","    def testDict(self):\n","        raise NotImplementedError\n","    \n","    @property\n","    def allPos(self):\n","        raise NotImplementedError\n","    \n","    def getUserItemFeedback(self, users, items):\n","        raise NotImplementedError\n","    \n","    def getUserPosItems(self, users):\n","        raise NotImplementedError\n","    \n","    def getUserNegItems(self, users):\n","        \"\"\"\n","        not necessary for large dataset\n","        it's stupid to return all neg items in super large dataset\n","        \"\"\"\n","        raise NotImplementedError\n","    \n","    def getSparseGraph(self):\n","        \"\"\"\n","        build a graph in torch.sparse.IntTensor.\n","        Details in NGCF's matrix form\n","        A = \n","            |I,   R|\n","            |R^T, I|\n","        \"\"\"\n","        raise NotImplementedError\n","\n","class Loader(BasicDataset):\n","    \"\"\"\n","    Dataset type for pytorch \\n\n","    Incldue graph information\n","    gowalla dataset\n","    \"\"\"\n","\n","    def __init__(self,config = config,path=\".\"):\n","        # train or test\n","        cprint(f'loading [{path}]')\n","        self.split = config['A_split']\n","        self.folds = config['A_n_fold']\n","        self.mode_dict = {'train': 0, \"test\": 1}\n","        self.mode = self.mode_dict['train']\n","        self.n_user = 0\n","        self.m_item = 0\n","        train_file = path + '/train.txt'\n","        test_file = path + '/test.txt'\n","        self.path = path\n","        trainUniqueUsers, trainItem, trainUser = [], [], []\n","        testUniqueUsers, testItem, testUser = [], [], []\n","        self.traindataSize = 0\n","        self.testDataSize = 0\n","\n","        with open(train_file) as f:\n","            for l in f.readlines():\n","                if len(l) > 0:\n","                    l = l.strip('\\n').split(' ')\n","                    items = [int(i) for i in l[1:]]\n","                    uid = int(l[0])\n","                    trainUniqueUsers.append(uid)\n","                    trainUser.extend([uid] * len(items))\n","                    trainItem.extend(items)\n","                    self.m_item = max(self.m_item, max(items))\n","                    self.n_user = max(self.n_user, uid)\n","                    self.traindataSize += len(items)\n","        self.trainUniqueUsers = np.array(trainUniqueUsers)\n","        self.trainUser = np.array(trainUser)\n","        self.trainItem = np.array(trainItem)\n","\n","        with open(test_file) as f:\n","            for l in f.readlines():\n","                l=l.rstrip() \n","                l = l.strip('\\n').split(' ')\n","                if len(l) > 1:\n","                    \n","                    items = [int(i) for i in l[1:]]\n","                    uid = int(l[0])\n","                    testUniqueUsers.append(uid)\n","                    testUser.extend([uid] * len(items))\n","                    testItem.extend(items)\n","                    self.m_item = max(self.m_item, max(items))\n","                    self.n_user = max(self.n_user, uid)\n","                    self.testDataSize += len(items)\n","        self.m_item += 1\n","        self.n_user += 1\n","        self.testUniqueUsers = np.array(testUniqueUsers)\n","        self.testUser = np.array(testUser)\n","        self.testItem = np.array(testItem)\n","        \n","        self.Graph = None\n","        print(f\"{self.trainDataSize} interactions for training\")\n","        print(f\"{self.testDataSize} interactions for testing\")\n","        print(f\"{dataset} Sparsity : {(self.trainDataSize + self.testDataSize) / self.n_users / self.m_items}\")\n","\n","        # (users,items), bipartite graph\n","        self.UserItemNet = csr_matrix((np.ones(len(self.trainUser)), (self.trainUser, self.trainItem)),\n","                                      shape=(self.n_user, self.m_item))\n","        self.users_D = np.array(self.UserItemNet.sum(axis=1)).squeeze()\n","        self.users_D[self.users_D == 0.] = 1\n","        self.items_D = np.array(self.UserItemNet.sum(axis=0)).squeeze()\n","        self.items_D[self.items_D == 0.] = 1.\n","        # pre-calculate\n","        self._allPos = self.getUserPosItems(list(range(self.n_user)))\n","        self.__testDict = self.__build_test()\n","        self.Gui=None\n","        self.uu=True\n","        self.ii=True\n","        print(f\"{dataset} is ready to go\")\n","\n","    @property\n","    def n_users(self):\n","        return self.n_user\n","    \n","    @property\n","    def m_items(self):\n","        return self.m_item\n","    \n","    @property\n","    def trainDataSize(self):\n","        return self.traindataSize\n","    \n","    @property\n","    def testDict(self):\n","        return self.__testDict\n","\n","    @property\n","    def allPos(self):\n","        return self._allPos\n","\n","    def _split_A_hat(self,A):\n","        A_fold = []\n","        fold_len = (self.n_users + self.m_items) // self.folds\n","        for i_fold in range(self.folds):\n","            start = i_fold*fold_len\n","            if i_fold == self.folds - 1:\n","                end = self.n_users + self.m_items\n","            else:\n","                end = (i_fold + 1) * fold_len\n","            A_fold.append(self._convert_sp_mat_to_sp_tensor(A[start:end]).coalesce().to(device))\n","        return A_fold\n","\n","    def _convert_sp_mat_to_sp_tensor(self, X):\n","        coo = X.tocoo().astype(np.float32)\n","        row = torch.Tensor(coo.row).long()\n","        col = torch.Tensor(coo.col).long()\n","        index = torch.stack([row, col])\n","        data = torch.FloatTensor(coo.data)\n","        return torch.sparse.FloatTensor(index, data, torch.Size(coo.shape))\n","        \n","    def getSparseGraph(self):\n","        print(\"loading adjacency matrix\")\n","        if self.Graph is None:\n","            try:\n","                pre_adj_mat = sp.load_npz(self.path + '/s_pre_adj_mat.npz')\n","                print(\"successfully loaded...\")\n","                norm_adj = pre_adj_mat\n","            except :\n","                print(\"generating adjacency matrix\")\n","                s = time()\n","                adj_mat = sp.dok_matrix((self.n_users + self.m_items, self.n_users + self.m_items), dtype=np.float32)\n","                adj_mat = adj_mat.tolil()\n","                R = self.UserItemNet.tolil()\n","                adj_mat[:self.n_users, self.n_users:] = R\n","                adj_mat[self.n_users:, :self.n_users] = R.T\n","                adj_mat = adj_mat.todok()\n","\n","                #adj_mat = adj_mat + sp.eye(adj_mat.shape[0])\n","                \n","                rowsum = np.array(adj_mat.sum(axis=1))\n","                d_inv = np.power(rowsum, -0.5).flatten()\n","                d_inv[np.isinf(d_inv)] = 0.\n","                d_mat = sp.diags(d_inv)\n","                \n","                norm_adj = d_mat.dot(adj_mat)\n","                norm_adj = norm_adj.dot(d_mat)\n","                norm_adj = norm_adj.tocsr()\n","                end = time()\n","                print(f\"costing {end-s}s, saved norm_mat...\")\n","                sp.save_npz(self.path + '/s_pre_adj_mat.npz', norm_adj)\n","\n","            if self.split == True:\n","                self.Graph = self._split_A_hat(norm_adj)\n","                print(\"done split matrix\")\n","            else:\n","                self.Graph = self._convert_sp_mat_to_sp_tensor(norm_adj)\n","                self.Graph = self.Graph.coalesce().to(device)\n","                print(\"don't split the matrix\")\n","        return self.Graph\n","\n","    def __build_test(self):\n","        \"\"\"\n","        return:\n","            dict: {user: [items]}\n","        \"\"\"\n","        test_data = {}\n","        for i, item in enumerate(self.testItem):\n","            user = self.testUser[i]\n","            if test_data.get(user):\n","                test_data[user].append(item)\n","            else:\n","                test_data[user] = [item]\n","        return test_data\n","\n","    def getUserItemFeedback(self, users, items):\n","        \"\"\"\n","        users:\n","            shape [-1]\n","        items:\n","            shape [-1]\n","        return:\n","            feedback [-1]\n","        \"\"\"\n","        # print(self.UserItemNet[users, items])\n","        return np.array(self.UserItemNet[users, items]).astype('uint8').reshape((-1,))\n","\n","    def getUserPosItems(self, users):\n","        posItems = []\n","        for user in users:\n","            posItems.append(self.UserItemNet[user].nonzero()[1])\n","        return posItems"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"XYFBBLgoxd3M"},"source":["import torch\n","from torch import nn\n","import numpy as np\n","\n","from torch.nn import Module\n","import torch.nn.functional as F\n","\n","class BasicModel(nn.Module):    \n","    def __init__(self):\n","        super(BasicModel, self).__init__()\n","    \n","    def getUsersRating(self, users):\n","        raise NotImplementedError\n","    \n","class PairWiseModel(BasicModel):\n","    def __init__(self):\n","        super(PairWiseModel, self).__init__()\n","    def bpr_loss(self, users, pos, neg):\n","        \"\"\"\n","        Parameters:\n","            users: users list \n","            pos: positive items for corresponding users\n","            neg: negative items for corresponding users\n","        Return:\n","            (log-loss, l2-loss)\n","        \"\"\"\n","        raise NotImplementedError\n","    \n","class PureMF(BasicModel):\n","    def __init__(self, \n","                 config:dict, \n","                 dataset:BasicDataset):\n","        super(PureMF, self).__init__()\n","        self.num_users  = dataset.n_users\n","        self.num_items  = dataset.m_items\n","        self.latent_dim = config['latent_dim_rec']\n","        self.f = nn.Sigmoid()\n","        self.__init_weight()\n","        \n","    def __init_weight(self):\n","        self.embedding_user = torch.nn.Embedding(\n","            num_embeddings=self.num_users, embedding_dim=self.latent_dim)\n","        self.embedding_item = torch.nn.Embedding(\n","            num_embeddings=self.num_items, embedding_dim=self.latent_dim)\n","        print(\"using Normal distribution N(0,1) initialization for PureMF\")\n","        \n","    def getUsersRating(self, users):\n","        users = users.long()\n","        users_emb = self.embedding_user(users)\n","        items_emb = self.embedding_item.weight\n","        scores = torch.matmul(users_emb, items_emb.t())\n","        return self.f(scores)\n","    \n","    def bpr_loss(self, users, pos, neg):\n","        users_emb = self.embedding_user(users.long())\n","        pos_emb   = self.embedding_item(pos.long())\n","        neg_emb   = self.embedding_item(neg.long())\n","        pos_scores= torch.sum(users_emb*pos_emb, dim=1)\n","        neg_scores= torch.sum(users_emb*neg_emb, dim=1)\n","        loss = torch.mean(nn.functional.softplus(neg_scores - pos_scores))\n","        reg_loss = (1/2)*(users_emb.norm(2).pow(2) + \n","                          pos_emb.norm(2).pow(2) + \n","                          neg_emb.norm(2).pow(2))/float(len(users))\n","        return loss, reg_loss\n","        \n","\n","    def bpr_loss_gcl_Kpos(self,users_emb0,pos_emb0,users,pos_items,alpha):\n","       # users_emb0,pos_emb0 =  self.computer(nd,fd)   #几次？\n","\n","        users_emb=   users_emb0[users] \n","        loss=self.bpr_loss_gcl_unit(users_emb,pos_emb0[pos_items[:,0]],alpha)\n","        for i in range(pos_items.size()[1])[1:]:\n","            pos_emb=   pos_emb0[pos_items[:,i]]  \n","            loss=loss+self.bpr_loss_gcl_unit(users_emb,pos_emb,alpha)\n","        \n","       # losii=self.iiloss(pos_emb0,pos_items)\n","               \n","        return  loss.mean() ,0\n","  \n","    def bpr_loss_gcl_unit(self,users_emb,pos_emb,alpha): \n","        T=self.T \n","        sim_batch=torch.exp(torch.mm(users_emb,pos_emb.t() ) /T )  \n","        posself=sim_batch.diag() \n","        neg=  sim_batch.sum(dim=1)  \n","       # lossRS=-torch.log(( posself+0.00001) /(neg+0.00001) ).mean()\n","\n","        lossRS= -alpha*torch.log( posself+0.00001)+(1-alpha)*torch.log(neg+0.00001) \n","        lossRS=lossRS.mean()\n"," \n","        return  lossRS \n","\n","    def forward(self, users, items):\n","        users = users.long()\n","        items = items.long()\n","        users_emb = self.embedding_user(users)\n","        items_emb = self.embedding_item(items)\n","        scores = torch.sum(users_emb*items_emb, dim=1)\n","        return self.f(scores)\n"," \n","\n","\n","class LightGCN(BasicModel):\n","    def __init__(self, \n","                 config:dict, \n","                 dataset:BasicDataset):\n","        super(LightGCN, self).__init__()\n","        self.config = config\n","        self.dataset : dataloader.BasicDataset = dataset\n","        self.__init_weight()\n","\n","    # def getSparseEye(self,num):\n","    #     i = torch.LongTensor([[k for k in range(0,num)],[j for j in range(0,num)]])\n","    #     val = torch.FloatTensor([1]*num)\n","    #     return torch.sparse.FloatTensor(i,val)\n","    \n","\n","\n","    def __init_weight(self):\n","        self.num_users  = self.dataset.n_users\n","        self.num_items  = self.dataset.m_items\n","        self.latent_dim = self.config['latent_dim_rec']\n","        self.n_layers = self.config['lightGCN_n_layers']\n","        self.keep_prob = self.config['keep_prob']\n","        self.A_split = self.config['A_split']\n","        self.T = self.config['temperature']\n","        self.embedding_user = torch.nn.Embedding(\n","            num_embeddings = self.num_users, embedding_dim=self.latent_dim)\n","        self.embedding_item = torch.nn.Embedding(\n","            num_embeddings = self.num_items, embedding_dim=self.latent_dim)\n","        if self.config['pretrain'] == 0:\n","#             nn.init.xavier_uniform_(self.embedding_user.weight, gain=1)\n","#             nn.init.xavier_uniform_(self.embedding_item.weight, gain=1)\n","#             print('use xavier initilizer')\n","# random normal init seems to be a better choice when lightGCN actually don't use any non-linear activation function\n","            nn.init.normal_(self.embedding_user.weight, std=0.1)\n","            nn.init.normal_(self.embedding_item.weight, std=0.1)\n","            cprint('use NORMAL distribution initilizer')\n","        else:\n","            self.embedding_user.weight.data.copy_(torch.from_numpy(self.config['user_emb']))\n","            self.embedding_item.weight.data.copy_(torch.from_numpy(self.config['item_emb']))\n","            print('use pretarined data')\n","        self.f = nn.Sigmoid()\n","        self.Graph = self.dataset.getSparseGraph()\n","        self.alphapara = torch.nn.Parameter(torch.FloatTensor(1), requires_grad=True)\n","        self.alphapara.data.fill_(0.5)\n","       \n","         # print(\"save_txt\")\n","    def __dropout_x(self, x, keep_prob):\n","        size = x.size()\n","        index = x.indices().t()\n","        values = x.values()\n","        random_index = torch.rand(len(values)) + keep_prob\n","        random_index = random_index.int().bool()\n","        index = index[random_index]\n","        values = values[random_index]#/keep_prob\n","        g = torch.sparse.FloatTensor(index.t(), values, size)\n","        return g\n","        \n","\n","    def drop_feature(self,x, drop_prob):\n","        drop_mask = torch.empty(\n","            (x.size(1), ),\n","            dtype=torch.float32,\n","            device=x.device).uniform_(0, 1) < drop_prob\n","        x = x.clone()\n","        x[:, drop_mask] = 0 \n","        return x\n","\n","\n","    def __dropout(self, keep_prob):\n","        if self.A_split:\n","            graph = []\n","            for g in self.Graph:\n","                graph.append(self.__dropout_x(g, keep_prob))\n","        else:\n","            graph = self.__dropout_x(self.Graph, keep_prob)\n","        return graph\n","     \n","\n","    def computer(self):\n","        \"\"\"\n","        propagate methods for lightGCN\n","        \"\"\"       \n","        users_emb = self.embedding_user.weight\n","        items_emb = self.embedding_item.weight\n"," \n","        all_emb = torch.cat([users_emb, items_emb])\n","        embs = [all_emb]\n","\n","        if self.config['dropout']:\n","            if self.training:\n","                g_droped = self.__dropout(self.keep_prob)\n","            else:\n","                g_droped = self.Graph     \n","        else:\n","            g_droped = self.Graph \n","\n","        for layer in range(self.n_layers):\n","            if self.A_split:\n","                temp_emb = []\n","                for f in range(len(g_droped)):\n","                    temp_emb.append(torch.sparse.mm(g_droped[f], all_emb))\n","                side_emb = torch.cat(temp_emb, dim=0)\n","                all_emb = side_emb\n","            else:\n","                all_emb = torch.sparse.mm(g_droped, all_emb)\n","            embs.append(all_emb)\n","\n","        light_out=all_emb\n","\n","                # #mean\n","                # embs = torch.stack(embs, dim=1) \n","                # light_out = torch.mean(embs, dim=1)\n","                #sg\n","        light_out= embs[-1] \n","        users, items = torch.split(light_out, [self.num_users, self.num_items])\n","\n","        return users, items \n","\n","\n","    def getUsersRating(self, users):\n","        all_users, all_items = self.computer()\n","        users_emb = all_users[users.long()]\n","        items_emb = all_items\n","   \n","        rating = self.f(torch.matmul(users_emb, items_emb.t()))\n","        return rating\n","    \n","    def getEmbedding(self, users, pos_items, neg_items):\n","        all_users, all_items = self.computer()\n","        users_emb = all_users[users]\n","        pos_emb = all_items[pos_items]\n","        neg_emb = all_items[neg_items]\n","        users_emb_ego = self.embedding_user(users)\n","        pos_emb_ego = self.embedding_item(pos_items)\n","        neg_emb_ego = self.embedding_item(neg_items)\n","        return users_emb, pos_emb, neg_emb, users_emb_ego, pos_emb_ego, neg_emb_ego\n","    \n","    \n","    def bpr_loss(self, users, pos, neg):\n","        (users_emb, pos_emb, neg_emb, \n","        userEmb0,  posEmb0, negEmb0) = self.getEmbedding(users.long(), pos.long(), neg.long())\n","        reg_loss = (1/2)*(userEmb0.norm(2).pow(2) + \n","                         posEmb0.norm(2).pow(2)  +\n","                         negEmb0.norm(2).pow(2))/float(len(users))\n","        pos_scores = torch.mul(users_emb, pos_emb)\n","        pos_scores = torch.sum(pos_scores, dim=1)\n","        neg_scores = torch.mul(users_emb, neg_emb)\n","        neg_scores = torch.sum(neg_scores, dim=1)\n","        \n","        loss = torch.mean(torch.nn.functional.softplus(neg_scores - pos_scores))\n","        \n","        return loss, reg_loss\n","\n","#fei qita zhegnli\n","############################################################################################################################################\n","    def bpr_loss_gcl_Kpos(self,users_emb0,pos_emb0,users,pos_items,alpha):\n","\n","        alpha=self.alphapara\n","        users_emb=   users_emb0[users] \n","        loss=self.bpr_loss_gcl_unit(users_emb,pos_emb0[pos_items[:,0]],alpha)\n","        for i in range(pos_items.size()[1])[1:]:\n","            pos_emb=   pos_emb0[pos_items[:,i]]  \n","            loss=loss+self.bpr_loss_gcl_unit(users_emb,pos_emb,alpha)\n","\n","        return  loss.mean() \n","    \n"," \n"," \n"," \n","    def bpr_loss_gcl_unit(self,users_emb,pos_emb,alpha): \n","        T=self.T \n"," \n","        ##ori\n","        sim_batch=torch.exp(torch.mm(users_emb,pos_emb.t() ) /T )  \n"," \n","        posself=sim_batch.diag() \n","        neg=  sim_batch.sum(dim=1)    ########################################\n","        \n","        lossRS= -alpha*torch.log( posself+0.00001)+(1-alpha)*torch.log(neg+0.00001) \n","        lossRS=lossRS.mean()\n"," \n","        return  lossRS \n"," \n","####################################################################################\n","#\n","    def forward(self, users, items):\n","        # compute embedding\n","        all_users, all_items = self.computer()\n","        # print('forward')\n","        #all_users, all_items = self.computer()\n","        users_emb = all_users[users]\n","        items_emb = all_items[items]\n","        inner_pro = torch.mul(users_emb, items_emb)\n","        gamma     = torch.sum(inner_pro, dim=1)\n","        return gamma"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"t0jJwvYJyQx7","executionInfo":{"status":"ok","timestamp":1639464776731,"user_tz":-330,"elapsed":10,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}},"outputId":"0e8aeaed-3400-453c-ae3a-ebdace0fa9a0"},"source":["import torch\n","from torch import nn, optim\n","import numpy as np\n","from torch import log\n","from time import time\n","from sklearn.metrics import roc_auc_score\n","import random\n","import os\n","\n","import torch.nn.functional as F\n","\n","try:\n","    from cppimport import imp_from_filepath\n","    from os.path import join, dirname\n","    path = join(dirname(__file__), \"sources/sampling.cpp\")\n","    sampling = imp_from_filepath(path)\n","    sampling.seed(seed)\n","    sample_ext = True\n","    cprint(\"Cpp extension loaded\")\n","except:\n","    cprint(\"Cpp extension not loaded !\")\n","    sample_ext = False\n","\n","\n","class BPRLoss:\n","    def __init__(self,\n","                 recmodel : PairWiseModel,\n","                 config : dict):\n","        self.model = recmodel\n","        self.weight_decay = config['decay']\n","        self.lr = config['lr']\n","        self.opt = optim.Adam(recmodel.parameters(), lr=self.lr, weight_decay=self.weight_decay)\n","        self.T=config['temperature']\n","        # self.dropedge = config['dropedge']        \n","        # self.maskfea = config['maskfea']\n","\n","    def stageOne(self, users, pos,info=\"\",epoch=5 ): \n","        \n","        if True:\n","            users_emb0,pos_emb0 =  self.model.computer() \n","            users_emb0=F.normalize(users_emb0)\n","            pos_emb0=F.normalize(pos_emb0)\n","            \n","\n","\n","            if \"alpha\" in info:\n","                alpha=float(info.split(\"alpha\")[1])#.split(\"_\")[0])\n","            else:\n","                alpha=0.5  \n","         \n","            loss = self.model.bpr_loss_gcl_Kpos(users_emb0,pos_emb0,users, pos,alpha)# bpr_loss_gcl_Kpos\n","                \n","            \n","            \n","        self.opt.zero_grad()\n","        loss.backward()\n","        self.opt.step() \n","        return loss.cpu().item() \n"," \n","\n","def UniformSample_original(dataset, neg_ratio = 1):\n","    dataset : BasicDataset\n","    allPos = dataset.allPos\n","    start = time()\n","    #S = UniformSample_original_python(dataset,neg_ratio)\n","\n","    if sample_ext:\n","        S = sampling.sample_negative(dataset.n_users, dataset.m_items,\n","                                     dataset.trainDataSize, allPos, neg_ratio)\n","    else:\n","        S = UniformSample_original_python(dataset)\n","    return S\n","\n","def UniformSample_original_python(dataset,neg_ratio=10):\n","    \"\"\"\n","    the original impliment of BPR Sampling in LightGCN\n","    :return:\n","        np.array\n","    \"\"\"\n","    total_start = time()\n","    dataset : BasicDataset\n","    user_num = dataset.trainDataSize\n","    users = np.random.randint(0, dataset.n_users, user_num)\n","    allPos = dataset.allPos\n","    S = []\n","    sample_time1 = 0.\n","    sample_time2 = 0.\n","    for i, user in enumerate(users):\n","        start = time()\n","        posForUser = allPos[user]\n","        if len(posForUser) == 0:\n","            continue\n","        sample_time2 += time() - start\n","        posindex = np.random.randint(0, len(posForUser), size=neg_ratio)\n","        positem = posForUser[posindex]\n","        # while True:\n","        #     negitem = np.random.randint(0, dataset.m_items)\n","        #     if negitem in posForUser:\n","        #         continue\n","        #     else:\n","        #         break\n","        S.append([user, positem])\n","        end = time()\n","        sample_time1 += end - start\n","    total = time() - total_start\n","\n","    return np.array(S)\n","\n","# ===================end samplers==========================\n","# =====================utils====================================\n","\n","def set_seed(seed):\n","    np.random.seed(seed)\n","    if torch.cuda.is_available():\n","        torch.cuda.manual_seed(seed)\n","        torch.cuda.manual_seed_all(seed)\n","    torch.manual_seed(seed)\n","\n","def getFileName():\n","    if model_name == 'mf':\n","        file = f\"mf-{dataset}-{config['latent_dim_rec']}.pth.tar\"\n","    elif model_name == 'lgn':\n","        file = f\"lgn-{dataset}-{config['lightGCN_n_layers']}-{config['latent_dim_rec']}.pth.tar\"\n","    return os.path.join(FILE_PATH,file)\n","\n","def minibatch(*tensors, **kwargs):\n","\n","    batch_size = kwargs.get('batch_size', config['bpr_batch_size'])\n","\n","    if len(tensors) == 1:\n","        tensor = tensors[0]\n","        for i in range(0, len(tensor), batch_size):\n","            yield tensor[i:i + batch_size]\n","    else:\n","        for i in range(0, len(tensors[0]), batch_size):\n","            yield tuple(x[i:i + batch_size] for x in tensors)\n","\n","\n","def shuffle(*arrays, **kwargs):\n","\n","    require_indices = kwargs.get('indices', False)\n","\n","    if len(set(len(x) for x in arrays)) != 1:\n","        raise ValueError('All inputs to shuffle must have '\n","                         'the same length.')\n","\n","    shuffle_indices = np.arange(len(arrays[0]))\n","    np.random.shuffle(shuffle_indices)\n","\n","    if len(arrays) == 1:\n","        result = arrays[0][shuffle_indices]\n","    else:\n","        result = tuple(x[shuffle_indices] for x in arrays)\n","\n","    if require_indices:\n","        return result, shuffle_indices\n","    else:\n","        return result\n","\n","\n","class timer:\n","    \"\"\"\n","    Time context manager for code block\n","        with timer():\n","            do something\n","        timer.get()\n","    \"\"\"\n","    from time import time\n","    TAPE = [-1]  # global time record\n","    NAMED_TAPE = {}\n","\n","    @staticmethod\n","    def get():\n","        if len(timer.TAPE) > 1:\n","            return timer.TAPE.pop()\n","        else:\n","            return -1\n","\n","    @staticmethod\n","    def dict(select_keys=None):\n","        hint = \"|\"\n","        if select_keys is None:\n","            for key, value in timer.NAMED_TAPE.items():\n","                hint = hint + f\"{key}:{value:.2f}|\"\n","        else:\n","            for key in select_keys:\n","                value = timer.NAMED_TAPE[key]\n","                hint = hint + f\"{key}:{value:.2f}|\"\n","        return hint\n","\n","    @staticmethod\n","    def zero(select_keys=None):\n","        if select_keys is None:\n","            for key, value in timer.NAMED_TAPE.items():\n","                timer.NAMED_TAPE[key] = 0\n","        else:\n","            for key in select_keys:\n","                timer.NAMED_TAPE[key] = 0\n","\n","    def __init__(self, tape=None, **kwargs):\n","        if kwargs.get('name'):\n","            timer.NAMED_TAPE[kwargs['name']] = timer.NAMED_TAPE[\n","                kwargs['name']] if timer.NAMED_TAPE.get(kwargs['name']) else 0.\n","            self.named = kwargs['name']\n","            if kwargs.get(\"group\"):\n","                #TODO: add group function\n","                pass\n","        else:\n","            self.named = False\n","            self.tape = tape or timer.TAPE\n","\n","    def __enter__(self):\n","        self.start = timer.time()\n","        return self\n","\n","    def __exit__(self, exc_type, exc_val, exc_tb):\n","        if self.named:\n","            timer.NAMED_TAPE[self.named] += timer.time() - self.start\n","        else:\n","            self.tape.append(timer.time() - self.start)\n","\n","\n","# ====================Metrics==============================\n","# =========================================================\n","def RecallPrecision_ATk(test_data, r, k):\n","    \"\"\"\n","    test_data should be a list? cause users may have different amount of pos items. shape (test_batch, k)\n","    pred_data : shape (test_batch, k) NOTE: pred_data should be pre-sorted\n","    k : top-k\n","    \"\"\"\n","    right_pred = r[:, :k].sum(1)\n","    precis_n = k\n","    recall_n = np.array([len(test_data[i]) for i in range(len(test_data))])\n","    recall = np.sum(right_pred/recall_n)\n","    precis = np.sum(right_pred)/precis_n\n","    return {'recall': recall, 'precision': precis}\n","\n","\n","def MRRatK_r(r, k):\n","    \"\"\"\n","    Mean Reciprocal Rank\n","    \"\"\"\n","    pred_data = r[:, :k]\n","    scores = np.log2(1./np.arange(1, k+1))\n","    pred_data = pred_data/scores\n","    pred_data = pred_data.sum(1)\n","    return np.sum(pred_data)\n","\n","def NDCGatK_r(test_data,r,k):\n","    \"\"\"\n","    Normalized Discounted Cumulative Gain\n","    rel_i = 1 or 0, so 2^{rel_i} - 1 = 1 or 0\n","    \"\"\"\n","    assert len(r) == len(test_data)\n","    pred_data = r[:, :k]\n","\n","    test_matrix = np.zeros((len(pred_data), k))\n","    for i, items in enumerate(test_data):\n","        length = k if k <= len(items) else len(items)\n","        test_matrix[i, :length] = 1\n","    max_r = test_matrix\n","    idcg = np.sum(max_r * 1./np.log2(np.arange(2, k + 2)), axis=1)\n","    dcg = pred_data*(1./np.log2(np.arange(2, k + 2)))\n","    dcg = np.sum(dcg, axis=1)\n","    idcg[idcg == 0.] = 1.\n","    ndcg = dcg/idcg\n","    ndcg[np.isnan(ndcg)] = 0.\n","    return np.sum(ndcg)\n","\n","def AUC(all_item_scores, dataset, test_data):\n","    \"\"\"\n","        design for a single user\n","    \"\"\"\n","    dataset : BasicDataset\n","    r_all = np.zeros((dataset.m_items, ))\n","    r_all[test_data] = 1\n","    r = r_all[all_item_scores >= 0]\n","    test_item_scores = all_item_scores[all_item_scores >= 0]\n","    return roc_auc_score(r, test_item_scores)\n","\n","def getLabel(test_data, pred_data):\n","    r = []\n","    for i in range(len(test_data)):\n","        groundTrue = test_data[i]\n","        predictTopK = pred_data[i]\n","        pred = list(map(lambda x: x in groundTrue, predictTopK))\n","        pred = np.array(pred).astype(\"float\")\n","        r.append(pred)\n","    return np.array(r).astype('float')"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[0;30;43mCpp extension not loaded !\u001b[0m\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"H7GlVt5ByzMv","executionInfo":{"status":"ok","timestamp":1639464779617,"user_tz":-330,"elapsed":2892,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}},"outputId":"72b276cd-8b0c-4f64-b746-1738ed95f0df"},"source":["!wget -q --show-progress https://github.com/RecoHut-Datasets/yelp/raw/v2/s_pre_adj_mat.npz\n","!wget -q --show-progress https://github.com/RecoHut-Datasets/yelp/raw/v2/train.txt\n","!wget -q --show-progress https://github.com/RecoHut-Datasets/yelp/raw/v2/test.txt"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["s_pre_adj_mat.npz   100%[===================>]  14.87M  --.-KB/s    in 0.1s    \n","train.txt           100%[===================>]   6.58M  --.-KB/s    in 0.09s   \n","test.txt            100%[===================>]   1.89M  --.-KB/s    in 0.07s   \n"]}]},{"cell_type":"code","source":["!pip install -r ../requirements.txt"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"IJhhlXtYulph","executionInfo":{"status":"ok","timestamp":1639465031494,"user_tz":-330,"elapsed":26992,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}},"outputId":"e519cfa6-f826-41b5-d6ef-dad3fef2d7ee"},"execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting pandas==0.24.2\n","  Downloading pandas-0.24.2-cp37-cp37m-manylinux1_x86_64.whl (10.1 MB)\n","\u001b[K     |████████████████████████████████| 10.1 MB 5.1 MB/s \n","\u001b[?25hCollecting scipy==1.3.0\n","  Downloading scipy-1.3.0-cp37-cp37m-manylinux1_x86_64.whl (25.2 MB)\n","\u001b[K     |████████████████████████████████| 25.2 MB 447 kB/s \n","\u001b[?25hCollecting numpy==1.16.4\n","  Downloading numpy-1.16.4-cp37-cp37m-manylinux1_x86_64.whl (17.3 MB)\n","\u001b[K     |████████████████████████████████| 17.3 MB 12.2 MB/s \n","\u001b[?25hCollecting tensorboardX==1.8\n","  Downloading tensorboardX-1.8-py2.py3-none-any.whl (216 kB)\n","\u001b[K     |████████████████████████████████| 216 kB 69.5 MB/s \n","\u001b[?25hCollecting scikit-learn==0.23.2\n","  Downloading scikit_learn-0.23.2-cp37-cp37m-manylinux1_x86_64.whl (6.8 MB)\n","\u001b[K     |████████████████████████████████| 6.8 MB 42.1 MB/s \n","\u001b[?25hCollecting tqdm==4.48.2\n","  Downloading tqdm-4.48.2-py2.py3-none-any.whl (68 kB)\n","\u001b[K     |████████████████████████████████| 68 kB 5.3 MB/s \n","\u001b[?25hRequirement already satisfied: pytz>=2011k in /usr/local/lib/python3.7/dist-packages (from pandas==0.24.2->-r ../requirements.txt (line 1)) (2018.9)\n","Requirement already satisfied: python-dateutil>=2.5.0 in /usr/local/lib/python3.7/dist-packages (from pandas==0.24.2->-r ../requirements.txt (line 1)) (2.8.2)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from tensorboardX==1.8->-r ../requirements.txt (line 4)) (1.15.0)\n","Requirement already satisfied: protobuf>=3.2.0 in /usr/local/lib/python3.7/dist-packages (from tensorboardX==1.8->-r ../requirements.txt (line 4)) (3.17.3)\n","Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn==0.23.2->-r ../requirements.txt (line 5)) (1.1.0)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn==0.23.2->-r ../requirements.txt (line 5)) (3.0.0)\n","Installing collected packages: numpy, scipy, tqdm, tensorboardX, scikit-learn, pandas\n","  Attempting uninstall: numpy\n","    Found existing installation: numpy 1.19.5\n","    Uninstalling numpy-1.19.5:\n","      Successfully uninstalled numpy-1.19.5\n","  Attempting uninstall: scipy\n","    Found existing installation: scipy 1.4.1\n","    Uninstalling scipy-1.4.1:\n","      Successfully uninstalled scipy-1.4.1\n","  Attempting uninstall: tqdm\n","    Found existing installation: tqdm 4.62.3\n","    Uninstalling tqdm-4.62.3:\n","      Successfully uninstalled tqdm-4.62.3\n","  Attempting uninstall: scikit-learn\n","    Found existing installation: scikit-learn 1.0.1\n","    Uninstalling scikit-learn-1.0.1:\n","      Successfully uninstalled scikit-learn-1.0.1\n","  Attempting uninstall: pandas\n","    Found existing installation: pandas 1.1.5\n","    Uninstalling pandas-1.1.5:\n","      Successfully uninstalled pandas-1.1.5\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","xarray 0.18.2 requires numpy>=1.17, but you have numpy 1.16.4 which is incompatible.\n","xarray 0.18.2 requires pandas>=1.0, but you have pandas 0.24.2 which is incompatible.\n","scikit-image 0.18.3 requires numpy>=1.16.5, but you have numpy 1.16.4 which is incompatible.\n","pywavelets 1.2.0 requires numpy>=1.17.3, but you have numpy 1.16.4 which is incompatible.\n","pyerfa 2.0.0.1 requires numpy>=1.17, but you have numpy 1.16.4 which is incompatible.\n","pyarrow 3.0.0 requires numpy>=1.16.6, but you have numpy 1.16.4 which is incompatible.\n","plotnine 0.6.0 requires pandas>=0.25.0, but you have pandas 0.24.2 which is incompatible.\n","mizani 0.6.0 requires pandas>=0.25.0, but you have pandas 0.24.2 which is incompatible.\n","kapre 0.3.6 requires numpy>=1.18.5, but you have numpy 1.16.4 which is incompatible.\n","jaxlib 0.1.71+cuda111 requires numpy>=1.18, but you have numpy 1.16.4 which is incompatible.\n","jax 0.2.25 requires numpy>=1.18, but you have numpy 1.16.4 which is incompatible.\n","imbalanced-learn 0.8.1 requires scikit-learn>=0.24, but you have scikit-learn 0.23.2 which is incompatible.\n","google-colab 1.0.0 requires pandas~=1.1.0; python_version >= \"3.0\", but you have pandas 0.24.2 which is incompatible.\n","fbprophet 0.7.1 requires pandas>=1.0.4, but you have pandas 0.24.2 which is incompatible.\n","datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\n","astropy 4.3.1 requires numpy>=1.17, but you have numpy 1.16.4 which is incompatible.\n","albumentations 0.1.12 requires imgaug<0.2.7,>=0.2.5, but you have imgaug 0.2.9 which is incompatible.\u001b[0m\n","Successfully installed numpy-1.16.4 pandas-0.24.2 scikit-learn-0.23.2 scipy-1.3.0 tensorboardX-1.8 tqdm-4.48.2\n"]},{"output_type":"display_data","data":{"application/vnd.colab-display-data+json":{"pip_warning":{"packages":["numpy","pandas","scipy","sklearn","tqdm"]}}},"metadata":{}}]},{"cell_type":"code","source":["# !git clone https://github.com/haotangxjtu/MSCL.git\n","%cd MSCL/code\n","!python main.py --layer 2 --dataset=\"yelp2018\" --temperature 0.2 --info sgk15_alpha0.45"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CkjipO8Ot5DN","executionInfo":{"status":"ok","timestamp":1639465138769,"user_tz":-330,"elapsed":19151,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}},"outputId":"cf055f74-246f-4a6d-9b14-1643738ab610"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/MSCL/code\n","\u001b[0;30;43mCpp extension not loaded !\u001b[0m\n",">>SEED: 2020\n","\u001b[0;30;43mloading [../data/yelp2018]\u001b[0m\n","1237259 interactions for training\n","324147 interactions for testing\n","yelp2018 Sparsity : 0.0012958757851778647\n","yelp2018 is ready to go\n","===========config================\n","{'A_n_fold': 100,\n"," 'A_split': False,\n"," 'bigdata': False,\n"," 'bpr_batch_size': 2048,\n"," 'dataset': 'yelp2018',\n"," 'decay': 0.0001,\n"," 'dropout': 0,\n"," 'info': 'sgk15_alpha0.45',\n"," 'keep_prob': 0.9,\n"," 'latent_dim_rec': 64,\n"," 'lightGCN_n_layers': 2,\n"," 'lr': 0.001,\n"," 'methods': 'LightGCN',\n"," 'multicore': 0,\n"," 'pretrain': 0,\n"," 'temperature': 0.2,\n"," 'test_u_batch_size': 100}\n","cores for test: 1\n","comment: lgn\n","tensorboard: 1\n","LOAD: 0\n","Weight path: ./checkpoints\n","Test Topks: [20]\n","using bpr loss\n","===========end===================\n","\u001b[0;30;43muse NORMAL distribution initilizer\u001b[0m\n","loading adjacency matrix\n","successfully loaded...\n","don't split the matrix\n","load and save to //th/LightGCNMultiCL/code/checkpoints/lgn-yelp2018-2-64.pth.tar\n","start\n","Traceback (most recent call last):\n","  File \"main.py\", line 84, in <module>\n","    output_information = Procedure.BPR_train_original(dataset, Recmodel, bpr, epoch, neg_k=Neg_k,w=w)\n","  File \"/content/MSCL/code/Procedure.py\", line 41, in BPR_train_original\n","    S=S.astype('int32') \n","ValueError: setting an array element with a sequence.\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HwieAgTUxsoy","executionInfo":{"status":"ok","timestamp":1639464787230,"user_tz":-330,"elapsed":7623,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}},"outputId":"aa876d6b-c535-4735-87eb-ce91d17a1e29"},"source":["from pprint import pprint\n","\n","dataset = Loader(path='.')\n","\n","print('===========config================')\n","pprint(config)\n","print(\"cores for test:\", CORES)\n","print(\"comment:\", comment)\n","print(\"tensorboard:\", tensorboard)\n","print(\"LOAD:\", LOAD)\n","print(\"Weight path:\", PATH)\n","print(\"Test Topks:\", topks)\n","print(\"using bpr loss\")\n","print('===========end===================')\n","\n","MODELS = {\n","    'mf': PureMF,\n","    'lgn': LightGCN\n","}"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[0;30;43mloading [.]\u001b[0m\n","1237259 interactions for training\n","324147 interactions for testing\n","yelp2018 Sparsity : 0.0012958757851778647\n","yelp2018 is ready to go\n","===========config================\n","{'A_n_fold': 100,\n"," 'A_split': False,\n"," 'bigdata': False,\n"," 'bpr_batch_size': 2048,\n"," 'dataset': 'yelp2018',\n"," 'decay': 0.0001,\n"," 'dropout': 0,\n"," 'info': '',\n"," 'keep_prob': 0.9,\n"," 'latent_dim_rec': 64,\n"," 'lightGCN_n_layers': 2,\n"," 'lr': 0.001,\n"," 'methods': 'LightGCN',\n"," 'multicore': 0,\n"," 'pretrain': 0,\n"," 'temperature': 0.1,\n"," 'test_u_batch_size': 100}\n","cores for test: 1\n","comment: lgn\n","tensorboard: 1\n","LOAD: 0\n","Weight path: ./checkpoints\n","Test Topks: [20]\n","using bpr loss\n","===========end===================\n"]}]},{"cell_type":"code","metadata":{"id":"dFJns77Qyo77"},"source":["import numpy as np\n","import torch\n","from pprint import pprint\n","from time import time\n","from tqdm import tqdm\n","import multiprocessing\n","from sklearn.metrics import roc_auc_score\n"," \n","\n","CORES = multiprocessing.cpu_count() // 2\n","\n","\n","def BPR_train_original(dataset, recommend_model, loss_class, epoch, neg_k=1, w=None):\n","    Recmodel = recommend_model\n","    Recmodel.train()\n","    bpr: BPRLoss = loss_class\n","    \n","    # u i+ i+  \n","    with timer(name=\"Sample\"):\n","        kk=config[\"info\"].split(\"k\")[1]\n","        if \"_\" in kk:\n","            kk=int(kk.split(\"_\")[0] )\n","        else:\n","            kk=int(kk)\n","\n","        S = UniformSample_original(dataset,kk)\n","     \n","    S=S.astype('int32') \n","    users = torch.Tensor(S[:, 0]).long()\n","    posItems = torch.Tensor(S[:, 1:]).long() \n","  \n","    users = users.to(device)\n","    posItems = posItems.to(device)\n","   \n","  \n","    users, posItems = shuffle(users, posItems)\n","    total_batch = len(users) // config['bpr_batch_size'] + 1\n","    aver_loss = 0.\n","    for (batch_i,\n","         (batch_users,\n","          batch_pos, )) in enumerate(minibatch(users,\n","                                                   posItems,\n","                                                   batch_size=config['bpr_batch_size'])):\n","        cri = bpr.stageOne(batch_users, batch_pos,config[\"info\"],epoch)\n","        aver_loss += cri\n","        if tensorboard:\n","            w.add_scalar(f'BPRLoss/BPR', cri, epoch * int(len(users) / config['bpr_batch_size']) + batch_i)\n","    aver_loss = aver_loss / total_batch\n","    time_info = timer.dict()\n","    timer.zero()\n","    return f\"loss{aver_loss:.3f}-{time_info}\"\n","    \n","    \n","def test_one_batch(X):\n","    sorted_items = X[0].numpy()\n","    groundTrue = X[1]\n","    r = getLabel(groundTrue, sorted_items)\n","    pre, recall, ndcg = [], [], []\n","    for k in topks:\n","        ret = RecallPrecision_ATk(groundTrue, r, k)\n","        pre.append(ret['precision'])\n","        recall.append(ret['recall'])\n","        ndcg.append(NDCGatK_r(groundTrue,r,k))\n","    return {'recall':np.array(recall), \n","            'precision':np.array(pre), \n","            'ndcg':np.array(ndcg)}\n","        \n","            \n","def Test(dataset, Recmodel, epoch, w=None, multicore=0):\n","    u_batch_size = config['test_u_batch_size']\n","    dataset: BasicDataset\n","    testDict: dict = dataset.testDict\n","    Recmodel: LightGCN\n","    # eval mode with no dropout\n","    Recmodel = Recmodel.eval()\n","    max_K = max(topks)\n","    if multicore == 1:\n","        pool = multiprocessing.Pool(CORES)\n","    results = {'precision': np.zeros(len(topks)),\n","               'recall': np.zeros(len(topks)),\n","               'ndcg': np.zeros(len(topks))}\n","    with torch.no_grad():\n","        users = list(testDict.keys())\n","        try:\n","            assert u_batch_size <= len(users) / 10\n","        except AssertionError:\n","            print(f\"test_u_batch_size is too big for this dataset, try a small one {len(users) // 10}\")\n","        users_list = []\n","        rating_list = []\n","        groundTrue_list = []\n","        # auc_record = []\n","        # ratings = []\n","        total_batch = len(users) // u_batch_size + 1\n","        for batch_users in minibatch(users, batch_size=u_batch_size):\n","            allPos = dataset.getUserPosItems(batch_users)\n","            groundTrue = [testDict[u] for u in batch_users]\n","            batch_users_gpu = torch.Tensor(batch_users).long()\n","            batch_users_gpu = batch_users_gpu.to(device)\n","\n","            rating = Recmodel.getUsersRating(batch_users_gpu)\n","            #rating = rating.cpu()\n","            exclude_index = []\n","            exclude_items = []\n","            for range_i, items in enumerate(allPos):\n","                exclude_index.extend([range_i] * len(items))\n","                exclude_items.extend(items)\n","            rating[exclude_index, exclude_items] = -(1<<10)\n","            _, rating_K = torch.topk(rating, k=max_K)\n","            rating = rating.cpu().numpy()\n","            # aucs = [ \n","            #         AUC(rating[i],\n","            #                   dataset, \n","            #                   test_data) for i, test_data in enumerate(groundTrue)\n","            #     ]\n","            # auc_record.extend(aucs)\n","            del rating\n","            users_list.append(batch_users)\n","            rating_list.append(rating_K.cpu())\n","            groundTrue_list.append(groundTrue)\n","        assert total_batch == len(users_list)\n","        X = zip(rating_list, groundTrue_list)\n","        if multicore == 1:\n","            pre_results = pool.map(test_one_batch, X)\n","        else:\n","            pre_results = []\n","            for x in X:\n","                pre_results.append(test_one_batch(x))\n","        scale = float(u_batch_size/len(users))\n","        for result in pre_results:\n","            results['recall'] += result['recall']\n","            results['precision'] += result['precision']\n","            results['ndcg'] += result['ndcg']\n","        results['recall'] /= float(len(users))\n","        results['precision'] /= float(len(users))\n","        results['ndcg'] /= float(len(users))\n","        # results['auc'] = np.mean(auc_record)\n","        if tensorboard:\n","            w.add_scalars(f'Test/Recall@{topks}',\n","                          {str(topks[i]): results['recall'][i] for i in range(len(topks))}, epoch)\n","            w.add_scalars(f'Test/Precision@{topks}',\n","                          {str(topks[i]): results['precision'][i] for i in range(len(topks))}, epoch)\n","            w.add_scalars(f'Test/NDCG@{topks}',\n","                          {str(topks[i]): results['ndcg'][i] for i in range(len(topks))}, epoch)\n","        if multicore == 1:\n","            pool.close()\n","        print(results)\n","        return results"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":495},"id":"lghPWvaczUm1","executionInfo":{"status":"error","timestamp":1639464792311,"user_tz":-330,"elapsed":5089,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}},"outputId":"22a97172-cbe2-4244-f46c-3ac5be9f213f"},"source":["import torch\n","import numpy as np\n","from torch.utils.tensorboard import SummaryWriter\n","import time\n","from os.path import join\n","import os\n","# ==============================\n","set_seed(seed)\n","print(\">>SEED:\", seed)\n","# ==============================\n","\n","Recmodel = MODELS[model_name](config, dataset)\n","Recmodel = Recmodel.to(device)\n","bpr = BPRLoss(Recmodel, config)\n","\n","weight_file = getFileName()\n","print(f\"load and save to {weight_file}\")\n","if LOAD:\n","    try:\n","        Recmodel.load_state_dict(torch.load(weight_file,map_location=torch.device('cpu')))\n","        cprint(f\"loaded model weights from {weight_file}\")\n","    except FileNotFoundError:\n","        print(f\"{weight_file} not exists, start from beginning\")\n","Neg_k = 1\n","\n","def early_stopping(log_value, best_value, stopping_step, expected_order='recall', flag_step=100):\n","    # early stopping strategy:\n","    #assert expected_order in ['recall', 'ndcg']\n","\n","    if (expected_order == 'recall' and log_value >= best_value)   :\n","        stopping_step = 0\n","        best_value = log_value\n","    else:\n","        stopping_step += 1\n","\n","    if stopping_step >= flag_step:\n","        print(\"Early stopping is trigger at step: {} log:{}\".format(flag_step, log_value))\n","        should_stop = True\n","    else:\n","        should_stop = False\n","    return best_value, stopping_step, should_stop\n","\n"," \n","#####\n","stopping_step = 0\n","should_stop = False\n","cur_best_pre_0=0.0\n","# init tensorboard\n","if tensorboard:\n","    w : SummaryWriter = SummaryWriter(\n","                                    join(BOARD_PATH, time.strftime(\"%m-%d-%Hh%Mm%Ss-\") + \"-\" + comment)\n","                                    )\n","else:\n","    w = None\n","    cprint(\"not enable tensorflowboard\")\n","\n","save_path = join(BOARD_PATH,config['dataset'] +config['methods'] + config['info']+\"-T\" + str(config['temperature'])+\"L\"+str(config['lightGCN_n_layers'])+time.strftime(\"%m-%d-%Hh%Mm%Ss-\") +\".txt\") \n","\n","d = os.path.dirname(save_path)\n","if not os.path.exists(d):\n","    os.makedirs(d) \n","lossinfo=[]\n","epochtime=[]\n","try:\n","    for epoch in range(TRAIN_epochs):\n","        f = open(save_path, 'a') \n","        start = time.time()\n","\n","        if epoch ==0  : \n","            f.write(\"Info :   \")\n","            for k,v in config.items():\n","                f.write( str(k)+\",\"+str(v)+\"\\n\")\n","            print(\"start\")  \n","\n","\n"," \n","            \n","        output_information = BPR_train_original(dataset, Recmodel, bpr, epoch, neg_k=Neg_k,w=w)\n","        epochtime.append(time.time()-start)\n","       # print(f'EPOCH[{epoch+1}/{TRAIN_epochs}] {output_information}',config['dataset'] + config['info']+str(np.mean(epochtime)))\n","        \n","        if epoch > 0:\n","            cprint(\"[TEST]\")\n","            outs=Test(dataset, Recmodel, epoch, w, config['multicore'])\n","\n","            f.write(\"\\n\"+str(epoch)+\":   \")\n","            for k,v in outs.items():\n","                f.write( str(k)+\",\"+str(v))  \n","            cur_best_pre_0, stopping_step, should_stop = early_stopping(outs['recall'][0], cur_best_pre_0,\n","                                                                                stopping_step, expected_order='recall', flag_step=20)\n","\n","\n","        lossinfo.append(output_information)\n","        torch.save(Recmodel.state_dict(), weight_file)\n","        if epoch == range(TRAIN_epochs)[-1]:\n","            f.write( 'finish ,best recall '+str(cur_best_pre_0) )\n","            f.close()  \n","\n","        if should_stop == True:\n","            f.write( 'early stop ,best recall '+str(cur_best_pre_0)+str(epoch) )\n","            f.close()\n","            break        \n"," \n","    f = open(save_path, 'a')\n","    for i, val in enumerate(lossinfo):\n","        f.write(\"\\n\"+ str(i+1)+\",\"+str(val)) \n","    f.close()\n","\n","    print(f'last-time',config['dataset'] + config['info']+str(np.mean(epochtime)))\n","\n","finally: \n","    if tensorboard:\n","        w.close()"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":[">>SEED: 2020\n","\u001b[0;30;43muse NORMAL distribution initilizer\u001b[0m\n","loading adjacency matrix\n","successfully loaded...\n","don't split the matrix\n","load and save to ./LightGCNMultiCL/code/checkpoints/lgn-<__main__.Loader object at 0x7f62285e9790>-2-64.pth.tar\n","start\n"]},{"output_type":"error","ename":"IndexError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-10-4ede06d3fc8f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m         \u001b[0moutput_information\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBPR_train_original\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRecmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbpr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mneg_k\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNeg_k\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m         \u001b[0mepochtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m        \u001b[0;31m# print(f'EPOCH[{epoch+1}/{TRAIN_epochs}] {output_information}',config['dataset'] + config['info']+str(np.mean(epochtime)))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-9-0d9fb70c8215>\u001b[0m in \u001b[0;36mBPR_train_original\u001b[0;34m(dataset, recommend_model, loss_class, epoch, neg_k, w)\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;31m# u i+ i+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtimer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Sample\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0mkk\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"info\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"k\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m\"_\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkk\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m             \u001b[0mkk\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"_\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mIndexError\u001b[0m: list index out of range"]}]},{"cell_type":"code","metadata":{"id":"Hp6-GM--zrOF"},"source":[""],"execution_count":null,"outputs":[]}]}