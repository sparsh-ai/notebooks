{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"T934743 | Augmented Shilling Attack (AUSH)","provenance":[],"collapsed_sections":[],"mount_file_id":"1s1m55RJS3xAstlUM0F9RaRj38b_kMgtj","authorship_tag":"ABX9TyPzuMLRxm8YoTHC9sbBLMMn"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"v2XUbj7yX7v2","executionInfo":{"status":"ok","timestamp":1633100494236,"user_tz":-330,"elapsed":825,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}},"outputId":"8b4095ca-70a2-4384-964f-90291abf667c"},"source":["%tensorflow_version 1.x"],"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["TensorFlow 1.x selected.\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pdpN0eIdYyT5","executionInfo":{"status":"ok","timestamp":1633100740620,"user_tz":-330,"elapsed":27620,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}},"outputId":"d2a75b8d-6abd-4711-bfd4-e3b39fc4a86d"},"source":["!pip install -q surprise"],"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[K     |████████████████████████████████| 11.8 MB 10.0 MB/s \n","\u001b[?25h  Building wheel for scikit-surprise (setup.py) ... \u001b[?25l\u001b[?25hdone\n"]}]},{"cell_type":"code","metadata":{"id":"f0PJLz1SXem7"},"source":["%%writefile example.sh\n","for target_id in 5 395 181 565 254 601 623 619 64 558\n","do\n","\tfor rec_model_name in IAUtoRec UAUtoRec NNMF NMF_25\n","\tdo\n","\t\tpython main_eval_attack.py --dataset filmTrust --rec_model_name $rec_model_name --attack_method G0 --target_id $target_id --attack_num 50 --filler_num 36 >> filmTrust_result_G0\n","\t\t#nohup python main_gan_attack_baseline.py --dataset filmTrust --target_id 5 --attack_num 50 --filler_num 36 --loss 0 >> G0_log 2>&1 &\n","\tdone\n","done"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Dt2Y8carX2NZ","executionInfo":{"status":"ok","timestamp":1633101111117,"user_tz":-330,"elapsed":678,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}}},"source":["import time\n","import numpy as np\n","import scipy\n","import math\n","import os\n","import shutil\n","import pandas as pd\n","from scipy.sparse import csr_matrix\n","from six.moves import xrange\n","import random\n","import copy\n","import itertools\n","import gzip\n","import sys, argparse\n","from sklearn.model_selection import train_test_split\n","\n","import tensorflow as tf\n","from tensorflow.python.framework import ops\n","\n","if \"concat_v2\" in dir(tf):\n","    def concat(tensors, axis, *args, **kwargs):\n","        return tf.concat_v2(tensors, axis, *args, **kwargs)\n","else:\n","    def concat(tensors, axis, *args, **kwargs):\n","        return tf.concat(tensors, axis, *args, **kwargs)"],"execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"id":"_txSkOEiaKYc","executionInfo":{"status":"ok","timestamp":1633101083278,"user_tz":-330,"elapsed":746,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}}},"source":["image_summary = tf.summary.image\n","scalar_summary = tf.summary.scalar\n","histogram_summary = tf.summary.histogram\n","merge_summary = tf.summary.merge\n","SummaryWriter = tf.summary.FileWriter"],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"id":"vGifuYTUZu_r"},"source":["class load_data():\n","\n","    def __init__(self, path_train, path_test,\n","                 header=None, sep='\\t', threshold=4, print_log=True):\n","        self.path_train = path_train\n","        self.path_test = path_test\n","        self.header = header if header is not None else ['user_id', 'item_id', 'rating']\n","        self.sep = sep\n","        self.threshold = threshold\n","        self.print_log = print_log\n","\n","        self._main_load()\n","\n","    def _main_load(self):\n","        # load data 得到用户总数，item总数，dataframe格式的train,test,train_without vali,validate\n","        self._load_file()\n","        #\n","        # dataframe to matrix\n","        self.train_matrix, self.train_matrix_implicit = self._data_to_matrix(self.train_data)\n","        self.test_matrix, self.test_matrix_implicit = self._data_to_matrix(self.test_data)\n","\n","    def _load_file(self):\n","        if self.print_log:\n","            print(\"load train/test data\\t:\\n\", self.path_train)\n","        self.train_data = pd.read_csv(self.path_train, sep=self.sep, names=self.header, engine='python').loc[:,\n","                          ['user_id', 'item_id', 'rating']]\n","        self.test_data = pd.read_csv(self.path_test, sep=self.sep, names=self.header, engine='python').loc[:,\n","                         ['user_id', 'item_id', 'rating']]\n","        # 不能保证每个item都有在训练集里出现\n","        self.n_users = len(set(self.test_data.user_id.unique()) | set(self.train_data.user_id.unique()))\n","        self.n_items = len(set(self.test_data.item_id.unique()) | set(self.train_data.item_id.unique()))\n","\n","        if self.print_log:\n","            print(\"Number of users:\", self.n_users, \",Number of items:\", self.n_items, flush=True)\n","            print(\"Train size:\", self.train_data.shape[0], \",Test size:\", self.test_data.shape[0], flush=True)\n","\n","    def _data_to_matrix(self, data_frame):\n","        row, col, rating, implicit_rating = [], [], [], []\n","        for line in data_frame.itertuples():\n","            uid, iid, r = list(line)[1:]\n","            implicit_r = 1 if r >= self.threshold else 0\n","\n","            row.append(uid)\n","            col.append(iid)\n","            rating.append(r)\n","            implicit_rating.append(implicit_r)\n","\n","        matrix = csr_matrix((rating, (row, col)), shape=(self.n_users, self.n_items))\n","        matrix_implicit = csr_matrix((implicit_rating, (row, col)), shape=(self.n_users, self.n_items))\n","        return matrix, matrix_implicit\n","\n","    def get_global_mean_std(self):\n","        return self.train_matrix.data.mean(), self.train_matrix.data.std()\n","\n","    def get_all_mean_std(self):\n","        flag = 1\n","        for v in ['global_mean', 'global_std', 'item_means', 'item_stds']:\n","            if not hasattr(self, v):\n","                flag = 0\n","                break\n","        if flag == 0:\n","            global_mean, global_std = self.get_global_mean_std()\n","            item_means, item_stds = [global_mean] * self.n_items, [global_std] * self.n_items\n","            train_matrix_t = self.train_matrix.transpose()\n","            for iid in range(self.n_items):\n","                item_vec = train_matrix_t.getrow(iid).toarray()[0]\n","                ratings = item_vec[np.nonzero(item_vec)]\n","                if len(ratings) > 0:\n","                    item_means[iid], item_stds[iid] = ratings.mean(), ratings.std()\n","            self.global_mean, self.global_std, self.item_means, self.item_stds \\\n","                = global_mean, global_std, item_means, item_stds\n","        return self.global_mean, self.global_std, self.item_means, self.item_stds\n","\n","    def get_item_pop(self):\n","        # item_pops = [0] * self.n_items\n","        # train_matrix_t = self.train_matrix.transpose()\n","        # for iid in range(self.n_items):\n","        #     item_vec = train_matrix_t.getrow(iid).toarray()[0]\n","        #     item_pops[iid] = len(np.nonzero(item_vec)[0])\n","        item_pops_dict = dict(self.train_data.groupby('item_id').size())\n","        item_pops = [0] * self.n_items\n","        for iid in item_pops_dict.keys():\n","            item_pops[iid] = item_pops_dict[iid]\n","        return item_pops\n","\n","    def get_user_nonrated_items(self):\n","        non_rated_indicator = self.train_matrix.toarray()\n","        non_rated_indicator[non_rated_indicator > 0] = 1\n","        non_rated_indicator = 1 - non_rated_indicator\n","        user_norated_items = {}\n","        for uid in range(self.n_users):\n","            user_norated_items[uid] = list(non_rated_indicator[uid].nonzero()[0])\n","        return user_norated_items\n","\n","    def get_item_nonrated_users(self, item_id):\n","        item_vec = np.squeeze(self.train_matrix[:, item_id].toarray())\n","        # item_vec = self.train_matrix.toarray().transpose()[item_id]\n","        item_vec[item_vec > 0] = 1\n","        non_rated_indicator = 1 - item_vec\n","        return list(non_rated_indicator.nonzero()[0])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"V7I7rQQOZvC5"},"source":["def load_attack_info(seletced_item_path, target_user_path):\n","    attack_info = {}\n","    with open(seletced_item_path, \"r\") as fin:\n","        for line in fin:\n","            line = line.strip(\"\\n\").split(\"\\t\")\n","            target_item, selected_items = int(line[0]), list(map(int, line[1].split(\",\")))\n","            attack_info[target_item] = [selected_items]\n","    with open(target_user_path, \"r\") as fin:\n","        for line in fin:\n","            line = line.strip(\"\\n\").split(\"\\t\")\n","            target_item, target_users = int(line[0]), list(map(int, line[1].split(\",\")))\n","            attack_info[target_item].append(target_users)\n","    return attack_info\n","\n","\n","def attacked_file_writer(clean_path, attacked_path, fake_profiles, n_users_ori):\n","    data_to_write = \"\"\n","    i = 0\n","    for fake_profile in fake_profiles:\n","        injected_iid = fake_profile.nonzero()[0]\n","        injected_rating = fake_profile[injected_iid]\n","        data_to_write += ('\\n'.join(\n","            map(lambda x: '\\t'.join(map(str, [n_users_ori + i] + list(x))), zip(injected_iid, injected_rating))) + '\\n')\n","        i += 1\n","    if os.path.exists(attacked_path): os.remove(attacked_path)\n","    shutil.copyfile(clean_path, attacked_path)\n","    with open(attacked_path, 'a+')as fout:\n","        fout.write(data_to_write)\n","\n","\n","def target_prediction_writer(predictions, hit_ratios, dst_path):\n","    # uid - rating - HR\n","    data_to_write = []\n","    for uid in range(len(predictions)):\n","        data_to_write.append('\\t'.join(map(str, [uid, predictions[uid]] + hit_ratios[uid])))\n","    with open(dst_path, 'w')as fout:\n","        fout.write('\\n'.join(data_to_write))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YTNUTKooikPi"},"source":["def parse(path):\n","    g = gzip.open(path, 'rb')\n","    for l in g:\n","        yield eval(l)\n","\n","\n","def getDF(path):\n","    i = 0\n","    df = {}\n","    for d in parse(path):\n","        df[i] = d\n","        i += 1\n","    return pd.DataFrame.from_dict(df, orient='index')\n","\n","\n","def data_preprocess(data_set, gz_path):\n","    data = getDF(gz_path)[['reviewerID', 'asin', 'overall']]\n","    data.columns = ['uid', 'iid', 'rating']\n","    # 数据统计\n","    uids, iids = data.uid.unique(), data.iid.unique()\n","    n_uids, n_iids, n_ratings = len(uids), len(iids), data.shape[0]\n","    print('用户数:', n_uids, '\\t物品数:', n_iids, '\\t评分数:', n_ratings, '\\t Sparsity :', n_ratings / (n_iids * n_uids))\n","    print('用户平均评分数:', n_ratings / n_uids)\n","    # id转换\n","    uid_update = dict(zip(uids, range(n_uids)))\n","    iid_update = dict(zip(iids, range(n_iids)))\n","\n","    data.uid = data.uid.apply(lambda x: uid_update[x])\n","    data.iid = data.iid.apply(lambda x: iid_update[x])\n","    # 数据集划分\n","    train_idxs, test_idxs = train_test_split(list(range(n_ratings)), test_size=0.1)\n","    # 结果保存\n","    train_data = data.iloc[train_idxs]\n","    test_data = data.iloc[test_idxs]\n","    path_train = \"../data/data/\" + data_set + \"_train.dat\"\n","    path_test = \"../data/data/\" + data_set + \"_test.dat\"\n","    train_data.to_csv(path_train, index=False, header=None, sep='\\t')\n","    test_data.to_csv(path_test, index=False, header=None, sep='\\t')\n","    np.save(\"../data/data/\" + data_set + \"_id_update\", [uid_update, iid_update])\n","\n","\n","def exp_select(data_set, target_items, selected_num, target_user_num):\n","    path_test = \"../data/data/\" + data_set + \"_test.dat\"\n","    path_train = \"../data/data/\" + data_set + \"_train.dat\"\n","    dataset_class = load_data(path_train=path_train, path_test=path_test,\n","                              header=['user_id', 'item_id', 'rating'],\n","                              sep='\\t', print_log=True)\n","    # 物品流行度\n","    item_pops = dataset_class.get_item_pop()\n","    # 物品id按照流行度降序排列\n","    # TODO 可以参考按照这个选择selected item，比如[item_pops[items_sorted[i*len(items_sorted)//20]] for i in range(5)]\n","    items_sorted = np.array(item_pops).argsort()[::-1]\n","    \"\"\"1.bandwagon攻击方法，每个目标的selcted都是全局热门top3\"\"\"\n","    bandwagon_selected = items_sorted[:selected_num]\n","    print('bandwagon_selected:', bandwagon_selected)\n","\n","    \"\"\"2.segment攻击方法，每个目标的selcted都是全局热门topN中随机组合\"\"\"\n","    threshold = dataset_class.test_data.rating.mean()\n","    threshold = threshold if threshold < 3 else 3.0\n","    print('高分阈值:', threshold)\n","    selected_candidates = items_sorted[:20]\n","    # 排列组合\n","    selected_candidates = list(itertools.combinations(selected_candidates, selected_num))\n","\n","    result = {}\n","    target_items = [j for i in range(2, 10) for j in\n","                    items_sorted[i * len(items_sorted) // 10:(i * len(items_sorted) // 10) + 2]][::-1]\n","    target_items = list(\n","        np.random.choice([i for i in range(len(item_pops)) if item_pops[i] == 3], 4, replace=False)) + target_items\n","    print('target_items:', target_items)\n","    print('评分数:', [item_pops[i] for i in target_items])\n","    for target in target_items:\n","        target_rated = set(dataset_class.train_data[dataset_class.train_data.item_id == target].user_id.values)\n","        data_tmp = dataset_class.train_data[~dataset_class.train_data.user_id.isin(target_rated)].copy()\n","        data_tmp = data_tmp[data_tmp.rating >= threshold]\n","        np.random.shuffle(selected_candidates)\n","        # 目标用户硬约束，要求对每个selected都有评分\n","        for selected_items in selected_candidates:\n","            target_users = data_tmp[data_tmp.item_id.isin(selected_items)].groupby(\n","                'user_id').size()\n","            # 对selected_items都有评分\n","            if target_users[(target_users == selected_num)].shape[0] >= target_user_num:\n","                target_users = sorted(target_users[(target_users == selected_num)].index)\n","                result[target] = [sorted(selected_items), target_users]\n","                print('target:', target, '硬约束')\n","                break\n","        # 硬约束找不到则执行，软约束，部分有评分\n","        if target not in result:\n","            for selected_items in selected_candidates:\n","                # 对selected_items有评分\n","                target_users = data_tmp[data_tmp.item_id.isin(selected_items)].groupby(\n","                    'user_id').size()\n","                target_users = sorted(dict(target_users).items(), key=lambda x: x[1], reverse=True)\n","                min = target_users[target_user_num][1]\n","                target_users = [i[0] for i in target_users[:target_user_num] if i[1] > selected_num // 2]\n","                if len(target_users) >= target_user_num:\n","                    result[target] = [sorted(selected_items), sorted(target_users)]\n","                    print('target:', target, '软约束,最少评selected数目：', min)\n","                    break\n","        # 无目标用户\n","        if target not in result:\n","            print('target:', target, '无目标用户')\n","            a = 1\n","    \"\"\"3.存result\"\"\"\n","    key = list(result.keys())\n","    selected_items = [','.join(map(str, result[k][0])) for k in key]\n","    target_users = [','.join(map(str, result[k][1])) for k in key]\n","    selected_items = pd.DataFrame(dict(zip(['id', 'selected_items'], [key, selected_items])))\n","    target_users = pd.DataFrame(dict(zip(['id', 'target_users'], [key, target_users])))\n","    selected_items.to_csv(\"../data/data/\" + data_set + '_selected_items', index=False, header=None, sep='\\t')\n","    target_users.to_csv(\"../data/data/\" + data_set + '_target_users', index=False, header=None, sep='\\t')\n","\n","\n","if __name__ == '__main__':\n","    data_set = 'office'\n","    gz_path = 'C:\\\\Users\\\\ariaschen\\\\Downloads\\\\reviews_Office_Products_5.json.gz'\n","\n","    \"\"\"step1:数据统计+格式转换\"\"\"\n","    data_preprocess(data_set, gz_path)\n","    \"\"\"# step2:选攻击目标,以及每个目标的selected items和目标用户\"\"\"\n","    target_items = None\n","    # selselected_num和target_user_num是为每个攻击目标选择多少个selected_items和多少个目标用户，默认为3和50\n","    # 但可能会遇到宣布不够个数的情况，处理办法（1）换攻击目标（2）参数调小,比如这个我改为selected_num=2, target_user_num=30\n","    exp_select(data_set, target_items, selected_num=2, target_user_num=30)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"cellView":"form","id":"x2OBX_scYRsg"},"source":["#@markdown class NNMF()\n","class NNMF():\n","    def __init__(self, sess, dataset_class, num_factor_1=100, num_factor_2=10, hidden_dimension=50,\n","                 learning_rate=0.001, reg_rate=0.01, epoch=500, batch_size=256,\n","                 show_time=False, T=5, display_step=1000):\n","        self.learning_rate = learning_rate\n","        self.epochs = epoch\n","        self.batch_size = batch_size\n","        self.reg_rate = reg_rate\n","        self.sess = sess\n","        self.dataset_class = dataset_class\n","        self.num_user = dataset_class.n_users\n","        self.num_item = dataset_class.n_items\n","        self.dataset_class.test_matrix_dok = self.dataset_class.test_matrix.todok()\n","\n","        self.num_factor_1 = num_factor_1\n","        self.num_factor_2 = num_factor_2\n","        self.hidden_dimension = hidden_dimension\n","        self.show_time = show_time\n","        self.T = T\n","        self.display_step = display_step\n","        print(\"NNMF.\")\n","\n","        self.dataset_class_train_matrix_coo = self.dataset_class.train_matrix.tocoo()\n","        self.user = self.dataset_class_train_matrix_coo.row.reshape(-1)\n","        self.item = self.dataset_class_train_matrix_coo.col.reshape(-1)\n","        self.rating = self.dataset_class_train_matrix_coo.data\n","\n","        self._build_network()\n","        init = tf.global_variables_initializer()\n","        self.sess.run(init)\n","\n","    def _build_network(self):\n","        print(\"num_factor_1=%d, num_factor_2=%d, hidden_dimension=%d\" % (\n","            self.num_factor_1, self.num_factor_2, self.hidden_dimension))\n","\n","        # model dependent arguments\n","        self.user_id = tf.placeholder(dtype=tf.int32, shape=[None], name='user_id')\n","        self.item_id = tf.placeholder(dtype=tf.int32, shape=[None], name='item_id')\n","        self.y = tf.placeholder(\"float\", [None], 'rating')\n","        # latent feature vectors\n","        P = tf.Variable(tf.random_normal([self.num_user, self.num_factor_1], stddev=0.01))\n","        Q = tf.Variable(tf.random_normal([self.num_item, self.num_factor_1], stddev=0.01))\n","        # latent feature matrix(K=1?)\n","        U = tf.Variable(tf.random_normal([self.num_user, self.num_factor_2], stddev=0.01))\n","        V = tf.Variable(tf.random_normal([self.num_item, self.num_factor_2], stddev=0.01))\n","\n","        input = tf.concat(values=[tf.nn.embedding_lookup(P, self.user_id),\n","                                  tf.nn.embedding_lookup(Q, self.item_id),\n","                                  tf.multiply(tf.nn.embedding_lookup(U, self.user_id),\n","                                              tf.nn.embedding_lookup(V, self.item_id))\n","                                  ], axis=1)\n","        #\n","        # tf1->tf2\n","        # regularizer = tf.contrib.layers.l2_regularizer(scale=self.reg_rate)\n","        regularizer = tf.keras.regularizers.l2(self.reg_rate)\n","        layer_1 = tf.layers.dense(inputs=input, units=2 * self.num_factor_1 + self.num_factor_2,\n","                                  bias_initializer=tf.random_normal_initializer,\n","                                  kernel_initializer=tf.random_normal_initializer, activation=tf.sigmoid,\n","                                  kernel_regularizer=regularizer)\n","        layer_2 = tf.layers.dense(inputs=layer_1, units=self.hidden_dimension, activation=tf.sigmoid,\n","                                  bias_initializer=tf.random_normal_initializer,\n","                                  kernel_initializer=tf.random_normal_initializer,\n","                                  kernel_regularizer=regularizer)\n","        layer_3 = tf.layers.dense(inputs=layer_2, units=self.hidden_dimension, activation=tf.sigmoid,\n","                                  bias_initializer=tf.random_normal_initializer,\n","                                  kernel_initializer=tf.random_normal_initializer,\n","                                  kernel_regularizer=regularizer)\n","        layer_4 = tf.layers.dense(inputs=layer_3, units=self.hidden_dimension, activation=tf.sigmoid,\n","                                  bias_initializer=tf.random_normal_initializer,\n","                                  kernel_initializer=tf.random_normal_initializer,\n","                                  kernel_regularizer=regularizer)\n","        output = tf.layers.dense(inputs=layer_4, units=1, activation=None,\n","                                 bias_initializer=tf.random_normal_initializer,\n","                                 kernel_initializer=tf.random_normal_initializer,\n","                                 kernel_regularizer=regularizer)\n","        self.pred_rating = tf.reshape(output, [-1])\n","        self.loss = tf.reduce_sum(tf.square(self.y - self.pred_rating)) \\\n","                    + tf.losses.get_regularization_loss() + self.reg_rate * (\n","                            tf.norm(U) + tf.norm(V) + tf.norm(P) + tf.norm(Q))\n","        self.optimizer = tf.train.RMSPropOptimizer(learning_rate=self.learning_rate).minimize(self.loss)\n","\n","    def train(self):\n","        self.num_training = len(self.rating)\n","        total_batch = int(self.num_training / self.batch_size)\n","        idxs = np.random.permutation(self.num_training)  # shuffled ordering\n","        user_random = list(self.user[idxs])\n","        item_random = list(self.item[idxs])\n","        rating_random = list(self.rating[idxs])\n","        # train\n","        for i in range(total_batch):\n","            batch_user = user_random[i * self.batch_size:(i + 1) * self.batch_size]\n","            batch_item = item_random[i * self.batch_size:(i + 1) * self.batch_size]\n","            batch_rating = rating_random[i * self.batch_size:(i + 1) * self.batch_size]\n","\n","            _, loss = self.sess.run([self.optimizer, self.loss], feed_dict={self.user_id: batch_user,\n","                                                                            self.item_id: batch_item,\n","                                                                            self.y: batch_rating\n","                                                                            })\n","        return loss\n","\n","    def test(self, test_data):\n","        error = 0\n","        error_mae = 0\n","        test_set = list(test_data.keys())\n","        for (u, i) in test_set:\n","            pred_rating_test = self.predict([u], [i])[0]\n","            error += (float(test_data.get((u, i))) - pred_rating_test) ** 2\n","            error_mae += (np.abs(float(test_data.get((u, i))) - pred_rating_test))\n","        rmse = np.sqrt(error / len(test_set))\n","        mae = error_mae / len(test_set)\n","        return rmse, mae\n","\n","    def execute(self):\n","        loss_prev = float(\"inf\")\n","        for epoch in range(self.epochs):\n","            loss_cur = self.train()\n","            if epoch % self.T == 0:\n","                print(\"epoch:\\t\", epoch, \"\\tloss:\\t\", loss_cur)\n","            if abs(loss_cur - loss_prev) < math.exp(-5):\n","                break\n","            loss_prev = loss_cur\n","        rmse, mae = self.test(self.dataset_class.test_matrix_dok)\n","        print(\"training done\\tRMSE : \", rmse, \"\\tMAE : \", mae)\n","\n","    def save(self, path):\n","        saver = tf.train.Saver()\n","        saver.save(self.sess, path)\n","\n","    def restore(self, path):\n","        init = tf.global_variables_initializer()\n","        self.sess.run(init)\n","        saver = tf.train.Saver()\n","        saver.restore(self.sess, path)\n","\n","    def predict(self, user_id, item_id):\n","        if type(item_id) != list:\n","            item_id = [item_id]\n","        if type(user_id) != list:\n","            user_id = [user_id] * len(item_id)\n","        return self.sess.run([self.pred_rating], feed_dict={self.user_id: user_id, self.item_id: item_id})[0]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"cellView":"form","id":"s8wYiC7MYHG8"},"source":["#@markdown class IAutoRec()\n","class IAutoRec():\n","    def __init__(self, sess, dataset_class, learning_rate=0.001, reg_rate=0.1, epoch=500, batch_size=500,\n","                 hidden_neuron=500, verbose=False, T=5, display_step=1000):\n","        self.learning_rate = learning_rate\n","        self.epochs = epoch\n","        self.batch_size = batch_size\n","        self.reg_rate = reg_rate\n","        self.hidden_neuron = hidden_neuron\n","        self.sess = sess\n","        self.dataset_class = dataset_class\n","        self.num_user = dataset_class.n_users\n","        self.num_item = dataset_class.n_items\n","        self.dataset_class.test_matrix_dok = self.dataset_class.test_matrix.todok()\n","        self.verbose = verbose\n","        self.T = T\n","        self.display_step = display_step\n","\n","        self.train_data = self.dataset_class.train_matrix.toarray()\n","        self.train_data_mask = scipy.sign(self.train_data)\n","\n","        print(\"IAutoRec.\",end=' ')\n","        self._build_network()\n","        init = tf.global_variables_initializer()\n","        self.sess.run(init)\n","\n","    def _build_network(self):\n","        # placeholder\n","        self.rating_matrix = tf.placeholder(dtype=tf.float32, shape=[self.num_user, None])\n","        self.rating_matrix_mask = tf.placeholder(dtype=tf.float32, shape=[self.num_user, None])\n","        self.keep_rate_net = tf.placeholder(tf.float32)\n","        self.keep_rate_input = tf.placeholder(tf.float32)\n","        # Variable\n","        V = tf.Variable(tf.random_normal([self.hidden_neuron, self.num_user], stddev=0.01))\n","        W = tf.Variable(tf.random_normal([self.num_user, self.hidden_neuron], stddev=0.01))\n","        mu = tf.Variable(tf.random_normal([self.hidden_neuron], stddev=0.01))\n","        b = tf.Variable(tf.random_normal([self.num_user], stddev=0.01))\n","        layer_1 = tf.nn.dropout(tf.sigmoid(tf.expand_dims(mu, 1) + tf.matmul(V, self.rating_matrix)),\n","                                self.keep_rate_net)\n","        self.layer_2 = tf.matmul(W, layer_1) + tf.expand_dims(b, 1)\n","        self.loss = tf.reduce_mean(tf.square(\n","            tf.norm(tf.multiply((self.rating_matrix - self.layer_2), self.rating_matrix_mask)))) + self.reg_rate * (\n","                            tf.square(tf.norm(W)) + tf.square(tf.norm(V)))\n","        self.optimizer = tf.train.AdamOptimizer(learning_rate=self.learning_rate).minimize(self.loss)\n","\n","    def train(self):\n","        self.num_training = self.num_item\n","        total_batch = int(self.num_training / self.batch_size)\n","        idxs = np.random.permutation(self.num_training)  # shuffled ordering\n","        loss = float('inf')\n","        for i in range(total_batch):\n","            if i == total_batch - 1:\n","                batch_set_idx = idxs[i * self.batch_size:]\n","            elif i < total_batch - 1:\n","                batch_set_idx = idxs[i * self.batch_size: (i + 1) * self.batch_size]\n","\n","            _, loss = self.sess.run([self.optimizer, self.loss],\n","                                    feed_dict={\n","                                        self.rating_matrix: self.dataset_class.train_matrix[:, batch_set_idx].toarray(),\n","                                        self.rating_matrix_mask: scipy.sign(\n","                                            self.dataset_class.train_matrix[:, batch_set_idx].toarray()),\n","                                        self.keep_rate_net: 1\n","                                    })  # 0.95\n","        return loss\n","\n","    def test(self, test_data):\n","        self.reconstruction = self.sess.run(self.layer_2, feed_dict={self.rating_matrix: self.train_data,\n","                                                                     self.rating_matrix_mask: self.train_data_mask,\n","                                                                     self.keep_rate_net: 1})\n","        error = 0\n","        error_mae = 0\n","        test_set = list(test_data.keys())\n","        for (u, i) in test_set:\n","            pred_rating_test = self.reconstruction[u, i]  # self.predict(u, i)\n","            error += (float(test_data.get((u, i))) - pred_rating_test) ** 2\n","            error_mae += (np.abs(float(test_data.get((u, i))) - pred_rating_test))\n","        rmse = np.sqrt(error / len(test_set))\n","        mae = error_mae / len(test_set)\n","        return rmse, mae\n","\n","    def execute(self):\n","        loss_prev = float(\"inf\")\n","        for epoch in range(self.epochs):\n","            loss_cur = self.train()\n","            # if epoch % self.T == 0:\n","                # print(\"epoch:\\t\", epoch, \"\\tloss:\\t\", loss_cur)\n","            if abs(loss_cur - loss_prev) < math.exp(-5):\n","                break\n","            loss_prev = loss_cur\n","        rmse, mae = self.test(self.dataset_class.test_matrix_dok)\n","        print(\"training done\\tRMSE : \", rmse, \"\\tMAE : \", mae)\n","\n","    def save(self, path):\n","        saver = tf.train.Saver()\n","        saver.save(self.sess, path)\n","\n","    def restore(self, path):\n","        init = tf.global_variables_initializer()\n","        self.sess.run(init)\n","        saver = tf.train.Saver()\n","        saver.restore(self.sess, path)\n","\n","    def predict(self, user_id, item_id):\n","        self.reconstruction = self.sess.run(self.layer_2, feed_dict={self.rating_matrix: self.train_data,\n","                                                                     self.rating_matrix_mask: self.train_data_mask,\n","                                                                     self.keep_rate_net: 1})\n","        return self.reconstruction[user_id, item_id]\n","        # if not hasattr(self, 'reconstruction_all'):\n","        #     self.reconstruction_all = self.sess.run(self.layer_2,\n","        #                                             feed_dict={self.rating_matrix: self.train_data,\n","        #                                                        self.rating_matrix_mask: self.train_data_mask,\n","        #                                                        self.keep_rate_net: 1})\n","        # return self.reconstruction_all[user_id, item_id]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"cellView":"form","id":"d4JUb7ctYeFn"},"source":["#@markdown class UAutoRec()\n","class UAutoRec():\n","    def __init__(self, sess, dataset_class, learning_rate=0.001, reg_rate=0.1, epoch=500, batch_size=200,\n","                 hidden_neuron=500, verbose=False, T=5, display_step=1000, layer=1):\n","        self.learning_rate = learning_rate\n","        self.epochs = epoch\n","        self.batch_size = batch_size\n","        self.reg_rate = reg_rate\n","        self.hidden_neuron = hidden_neuron\n","        self.sess = sess\n","        self.dataset_class = dataset_class\n","        self.num_user = dataset_class.n_users\n","        self.num_item = dataset_class.n_items\n","        self.dataset_class.test_matrix_dok = self.dataset_class.test_matrix.todok()\n","        self.verbose = verbose\n","        self.T = T\n","        self.display_step = display_step\n","        print(\"UAutoRec.\")\n","        # 评分矩阵是IAutoRec的转置\n","        self.train_data = self.dataset_class.train_matrix.toarray().transpose()\n","        self.train_data_mask = scipy.sign(self.train_data)\n","\n","        self.layer = layer\n","\n","        self._build_network()\n","        init = tf.global_variables_initializer()\n","        self.sess.run(init)\n","\n","    def _build_network(self):\n","        # placeholder\n","        self.rating_matrix = tf.placeholder(dtype=tf.float32, shape=[self.num_item, None])\n","        self.rating_matrix_mask = tf.placeholder(dtype=tf.float32, shape=[self.num_item, None])\n","        if self.layer == 1:\n","            # Variable\n","            V = tf.Variable(tf.random_normal([self.hidden_neuron, self.num_item], stddev=0.01))\n","            W = tf.Variable(tf.random_normal([self.num_item, self.hidden_neuron], stddev=0.01))\n","\n","            mu = tf.Variable(tf.random_normal([self.hidden_neuron], stddev=0.01))\n","            b = tf.Variable(tf.random_normal([self.num_item], stddev=0.01))\n","            layer_1 = tf.sigmoid(tf.expand_dims(mu, 1) + tf.matmul(V, self.rating_matrix))\n","            self.layer_2 = tf.matmul(W, layer_1) + tf.expand_dims(b, 1)\n","            Loss_norm = tf.square(tf.norm(W)) + tf.square(tf.norm(V))\n","        elif self.layer == 3:\n","            V_1 = tf.Variable(tf.random_normal([self.hidden_neuron, self.num_item], stddev=0.01))\n","            V_2 = tf.Variable(tf.random_normal([self.hidden_neuron // 2, self.hidden_neuron], stddev=0.01))\n","            V_3 = tf.Variable(tf.random_normal([self.hidden_neuron, self.hidden_neuron // 2], stddev=0.01))\n","            W = tf.Variable(tf.random_normal([self.num_item, self.hidden_neuron], stddev=0.01))\n","            mu_1 = tf.Variable(tf.random_normal([self.hidden_neuron], stddev=0.01))\n","            mu_2 = tf.Variable(tf.random_normal([self.hidden_neuron // 2], stddev=0.01))\n","            mu_3 = tf.Variable(tf.random_normal([self.hidden_neuron], stddev=0.01))\n","            b = tf.Variable(tf.random_normal([self.num_item], stddev=0.01))\n","            #\n","            layer_1 = tf.sigmoid(tf.matmul(V_1, self.rating_matrix) + tf.expand_dims(mu_1, 1))\n","            layer_2 = tf.sigmoid(tf.matmul(V_2, layer_1) + tf.expand_dims(mu_2, 1))\n","            layer_3 = tf.sigmoid(tf.matmul(V_3, layer_2) + tf.expand_dims(mu_3, 1))\n","            self.layer_2 = tf.matmul(W, layer_3) + tf.expand_dims(b, 1)\n","            Loss_norm = tf.square(tf.norm(W)) + tf.square(tf.norm(V_1)) + tf.square(tf.norm(V_3)) + tf.square(\n","                tf.norm(V_3))\n","        self.loss = tf.reduce_mean(tf.square(\n","            tf.norm(tf.multiply((self.rating_matrix - self.layer_2),\n","                                self.rating_matrix_mask)))) + self.reg_rate + Loss_norm\n","\n","        self.optimizer = tf.train.AdamOptimizer(learning_rate=self.learning_rate).minimize(self.loss)\n","\n","    def train(self):\n","        self.num_training = self.num_user\n","        total_batch = int(self.num_training / self.batch_size)\n","        idxs = np.random.permutation(self.num_training)  # shuffled ordering\n","        for i in range(total_batch):\n","            if i == total_batch - 1:\n","                batch_set_idx = idxs[i * self.batch_size:]\n","            elif i < total_batch - 1:\n","                batch_set_idx = idxs[i * self.batch_size: (i + 1) * self.batch_size]\n","\n","            _, loss = self.sess.run([self.optimizer, self.loss],\n","                                    feed_dict={self.rating_matrix: self.train_data[:, batch_set_idx],\n","                                               self.rating_matrix_mask: self.train_data_mask[:, batch_set_idx]\n","                                               })\n","        return loss\n","\n","    def test(self, test_data):\n","        self.reconstruction = self.sess.run(self.layer_2, feed_dict={self.rating_matrix: self.train_data,\n","                                                                     self.rating_matrix_mask:\n","                                                                         self.train_data_mask})\n","        error = 0\n","        error_mae = 0\n","        test_set = list(test_data.keys())\n","        for (u, i) in test_set:\n","            pred_rating_test = self.predict(u, i)\n","            error += (float(test_data.get((u, i))) - pred_rating_test) ** 2\n","            error_mae += (np.abs(float(test_data.get((u, i))) - pred_rating_test))\n","        rmse = np.sqrt(error / len(test_set))\n","        mae = error_mae / len(test_set)\n","        return rmse, mae\n","\n","    def execute(self):\n","        loss_prev = float(\"inf\")\n","        for epoch in range(self.epochs):\n","            loss_cur = self.train()\n","            if epoch % self.T == 0:\n","                print(\"epoch:\\t\", epoch, \"\\tloss:\\t\", loss_cur)\n","            if abs(loss_cur - loss_prev) < math.exp(-5):\n","                break\n","            loss_prev = loss_cur\n","        rmse, mae = self.test(self.dataset_class.test_matrix_dok)\n","        print(\"training done\\tRMSE : \", rmse, \"\\tMAE : \", mae)\n","\n","    def save(self, path):\n","        saver = tf.train.Saver()\n","        saver.save(self.sess, path)\n","\n","    def restore(self, path):\n","        init = tf.global_variables_initializer()\n","        self.sess.run(init)\n","        saver = tf.train.Saver()\n","        saver.restore(self.sess, path)\n","\n","    def predict(self, user_id, item_id):\n","        self.reconstruction = self.sess.run(self.layer_2, feed_dict={self.rating_matrix: self.train_data,\n","                                                                     self.rating_matrix_mask:\n","                                                                         self.train_data_mask})\n","        return self.reconstruction[item_id, user_id]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"cellView":"form","id":"RlSrs12GaeTS"},"source":["#@markdown DCGAN\n","class batch_norm(object):\n","    def __init__(self, epsilon=1e-5, momentum=0.9, name=\"batch_norm\"):\n","        with tf.variable_scope(name):\n","            self.epsilon = epsilon\n","            self.momentum = momentum\n","            self.name = name\n","\n","    def __call__(self, x, train=True):\n","        return tf.contrib.layers.batch_norm(x,\n","                                            decay=self.momentum,\n","                                            updates_collections=None,\n","                                            epsilon=self.epsilon,\n","                                            scale=True,\n","                                            is_training=train,\n","                                            scope=self.name)\n","\n","\n","def conv_cond_concat(x, y):\n","    \"\"\"Concatenate conditioning vector on feature map axis.\"\"\"\n","    x_shapes = x.get_shape()\n","    y_shapes = y.get_shape()\n","    return concat([\n","        x, y * tf.ones([x_shapes[0], x_shapes[1], x_shapes[2], y_shapes[3]])], 3)\n","\n","\n","def conv2d(input_, output_dim,\n","           k_h=5, k_w=5, d_h=2, d_w=2, stddev=0.02,\n","           name=\"conv2d\"):\n","    with tf.variable_scope(name):\n","        w = tf.get_variable('w', [k_h, k_w, input_.get_shape()[-1], output_dim],\n","                            initializer=tf.truncated_normal_initializer(stddev=stddev))\n","        conv = tf.nn.conv2d(input_, w, strides=[1, d_h, d_w, 1], padding='SAME')\n","\n","        biases = tf.get_variable('biases', [output_dim], initializer=tf.constant_initializer(0.0))\n","        conv = tf.reshape(tf.nn.bias_add(conv, biases), conv.get_shape())\n","\n","        return conv\n","\n","\n","# kernel_size = 5 * 5\n","def deconv2d(input_, output_shape,\n","             k_h=5, k_w=5, d_h=2, d_w=2, stddev=0.02,\n","             name=\"deconv2d\", with_w=False):\n","    with tf.variable_scope(name):\n","        # filter : [height, width, output_channels, in_channels]\n","        w = tf.get_variable('w', [k_h, k_w, output_shape[-1], input_.get_shape()[-1]],\n","                            initializer=tf.random_normal_initializer(stddev=stddev))\n","\n","        try:\n","            deconv = tf.nn.conv2d_transpose(input_, w, output_shape=output_shape,\n","                                            strides=[1, d_h, d_w, 1])\n","\n","        # Support for verisons of TensorFlow before 0.7.0\n","        except AttributeError:\n","            deconv = tf.nn.deconv2d(input_, w, output_shape=output_shape,\n","                                    strides=[1, d_h, d_w, 1])\n","\n","        biases = tf.get_variable('biases', [output_shape[-1]], initializer=tf.constant_initializer(0.0))\n","        deconv = tf.reshape(tf.nn.bias_add(deconv, biases), deconv.get_shape())\n","\n","        if with_w:\n","            return deconv, w, biases\n","        else:\n","            return deconv\n","\n","\n","def lrelu(x, leak=0.2, name=\"lrelu\"):\n","    return tf.maximum(x, leak * x)\n","\n","\n","def linear(input_, output_size, scope=None, stddev=0.02, bias_start=0.0, with_w=False):\n","    shape = input_.get_shape().as_list()\n","\n","    with tf.variable_scope(scope or \"Linear\"):\n","        try:\n","            matrix = tf.get_variable(\"Matrix\", [shape[1], output_size], tf.float32,\n","                                     tf.random_normal_initializer(stddev=stddev))\n","        except ValueError as err:\n","            msg = \"NOTE: Usually, this is due to an issue with the image dimensions.  Did you correctly set '--crop' or '--input_height' or '--output_height'?\"\n","            err.args = err.args + (msg,)\n","            raise\n","        bias = tf.get_variable(\"bias\", [output_size],\n","                               initializer=tf.constant_initializer(bias_start))\n","        if with_w:\n","            return tf.matmul(input_, matrix) + bias, matrix, bias\n","        else:\n","            return tf.matmul(input_, matrix) + bias\n","\n","\n","def conv_out_size_same(size, stride):\n","    return int(math.ceil(float(size) / float(stride)))\n","\n","\n","def gen_random(size):\n","    # z - N(0,100)\n","    return np.random.normal(0, 100, size=size)\n","\n","\n","class DCGAN(object):\n","    def __init__(self, sess, dataset_class,batch_size=64, height=29, width=58, z_dim=100, gf_dim=64, df_dim=64,\n","                 gfc_dim=1024, dfc_dim=1024, max_to_keep=1):\n","        self.sess = sess\n","        self.dataset_class = dataset_class\n","        self.batch_size = batch_size\n","\n","        self.height = height\n","        self.width = width\n","        self.z_dim = z_dim\n","        self.gf_dim = gf_dim\n","        self.df_dim = df_dim\n","        self.gfc_dim = gfc_dim\n","        self.dfc_dim = dfc_dim\n","        # batch normalization : deals with poor initialization helps gradient flow\n","        self.d_bn1 = batch_norm(name='d_bn1')\n","        self.d_bn2 = batch_norm(name='d_bn2')\n","        self.d_bn3 = batch_norm(name='d_bn3')\n","        self.g_bn0 = batch_norm(name='g_bn0')\n","        self.g_bn1 = batch_norm(name='g_bn1')\n","        self.g_bn2 = batch_norm(name='g_bn2')\n","        self.g_bn3 = batch_norm(name='g_bn3')\n","\n","        self.max_to_keep = max_to_keep\n","\n","        self.build_model()\n","\n","    def build_model(self):\n","        self.inputs = tf.placeholder(tf.float32,\n","                                     [self.batch_size, self.height, self.width, 1],\n","                                     name='real_images')\n","        inputs = self.inputs\n","        # 生成器\n","        self.z = tf.placeholder(tf.float32, [None, self.z_dim], name='z')\n","        self.G = self.generator(self.z)\n","        # 判别器 - real&fake\n","        self.D, self.D_logits = self.discriminator(inputs, reuse=False)\n","        self.D_, self.D_logits_ = self.discriminator(self.G, reuse=True)\n","\n","        # 损失函数\n","        def sigmoid_cross_entropy_with_logits(x, y):\n","            try:\n","                return tf.nn.sigmoid_cross_entropy_with_logits(logits=x, labels=y)\n","            except:\n","                return tf.nn.sigmoid_cross_entropy_with_logits(logits=x, targets=y)\n","\n","        self.d_loss_real = tf.reduce_mean(\n","            sigmoid_cross_entropy_with_logits(self.D_logits, tf.ones_like(self.D)))\n","        self.d_loss_fake = tf.reduce_mean(\n","            sigmoid_cross_entropy_with_logits(self.D_logits_, tf.zeros_like(self.D_)))\n","\n","        self.g_loss = tf.reduce_mean(\n","            sigmoid_cross_entropy_with_logits(self.D_logits_, tf.ones_like(self.D_)))\n","        self.d_loss = self.d_loss_real + self.d_loss_fake\n","        #\n","        t_vars = tf.trainable_variables()\n","        self.d_vars = [var for var in t_vars if 'd_' in var.name]\n","        self.g_vars = [var for var in t_vars if 'g_' in var.name]\n","\n","        self.saver = tf.train.Saver(max_to_keep=self.max_to_keep)\n","\n","    def train(self, config):\n","        d_optim = tf.train.AdamOptimizer(config.learning_rate, beta1=config.beta1) \\\n","            .minimize(self.d_loss, var_list=self.d_vars)\n","        g_optim = tf.train.AdamOptimizer(config.learning_rate, beta1=config.beta1) \\\n","            .minimize(self.g_loss, var_list=self.g_vars)\n","        try:\n","            tf.global_variables_initializer().run()\n","        except:\n","            tf.initialize_all_variables().run()\n","        train_idxs = list(range(self.dataset_class.train_matrix.shape[0]))\n","        for epoch in xrange(config.epoch):\n","            np.random.shuffle(train_idxs)\n","            for i in range(len(train_idxs) // self.batch_size):\n","                cur_idxs = train_idxs[i * self.batch_size:(i + 1) * self.batch_size]\n","                batch_inputs = self.dataset_class.train_matrix[cur_idxs].toarray()\n","                # transform range&shape\n","                batch_inputs = (batch_inputs - 2.5) / 2.5\n","                batch_inputs = np.reshape(batch_inputs, [self.batch_size, self.height, self.width, 1])\n","                # batch_inputs = np.random.random_sample([self.batch_size, self.height, self.width, 1])\n","                batch_z = gen_random(size=[config.batch_size, self.z_dim]).astype(np.float32)\n","\n","                # Update D network\n","                _ = self.sess.run(d_optim, feed_dict={self.inputs: batch_inputs, self.z: batch_z})\n","\n","                # Update G network\n","                _ = self.sess.run(g_optim, feed_dict={self.z: batch_z})\n","\n","                # Run g_optim twice to make sure that d_loss does not go to zero (different from paper)\n","\n","                errD_fake = self.d_loss_fake.eval({self.z: batch_z})\n","                errD_real = self.d_loss_real.eval({self.inputs: batch_inputs})\n","                errG = self.g_loss.eval({self.z: batch_z})\n","\n","                print(\"Epoch:[%2d/%2d]d_loss: %.8f, g_loss: %.8f\" \\\n","                      % (epoch, config.epoch, errD_fake + errD_real, errG))\n","\n","    def discriminator(self, image, reuse=False):\n","        with tf.variable_scope(\"discriminator\") as scope:\n","            if reuse:\n","                scope.reuse_variables()\n","            # 论文中给的判别器结构:[conv+BN+LeakyRelu[64,128,256,512]]+[FC]+[sigmoid]\n","            h0 = lrelu(conv2d(image, self.df_dim, name='d_h0_conv'))\n","            h1 = lrelu(self.d_bn1(conv2d(h0, self.df_dim * 2, name='d_h1_conv')))\n","            h2 = lrelu(self.d_bn2(conv2d(h1, self.df_dim * 4, name='d_h2_conv')))\n","            h3 = lrelu(self.d_bn3(conv2d(h2, self.df_dim * 8, name='d_h3_conv')))\n","            h4 = linear(tf.reshape(h3, [self.batch_size, -1]), 1, 'd_h4_lin')\n","\n","            return tf.nn.sigmoid(h4), h4\n","\n","    def generator(self, z):\n","        with tf.variable_scope(\"generator\") as scope:\n","            s_h, s_w = self.height, self.width\n","            # CONV stride=2\n","            s_h2, s_w2 = conv_out_size_same(s_h, 2), conv_out_size_same(s_w, 2)\n","            s_h4, s_w4 = conv_out_size_same(s_h2, 2), conv_out_size_same(s_w2, 2)\n","            s_h8, s_w8 = conv_out_size_same(s_h4, 2), conv_out_size_same(s_w4, 2)\n","            s_h16, s_w16 = conv_out_size_same(s_h8, 2), conv_out_size_same(s_w8, 2)\n","\n","            # FC of 2*4*512&ReLU&BN\n","            self.z_, self.h0_w, self.h0_b = linear(\n","                z, self.gf_dim * 8 * s_h16 * s_w16, 'g_h0_lin', with_w=True)\n","            self.h0 = tf.reshape(\n","                self.z_, [-1, s_h16, s_w16, self.gf_dim * 8])\n","            h0 = tf.nn.relu(self.g_bn0(self.h0))\n","\n","            # four transposed CONV of [256,128,64] &ReLU&BN&kernel_size = 5 * 5\n","            self.h1, self.h1_w, self.h1_b = deconv2d(\n","                h0, [self.batch_size, s_h8, s_w8, self.gf_dim * 4], name='g_h1', with_w=True)\n","            h1 = tf.nn.relu(self.g_bn1(self.h1))\n","            h2, self.h2_w, self.h2_b = deconv2d(\n","                h1, [self.batch_size, s_h4, s_w4, self.gf_dim * 2], name='g_h2', with_w=True)\n","            h2 = tf.nn.relu(self.g_bn2(h2))\n","            h3, self.h3_w, self.h3_b = deconv2d(\n","                h2, [self.batch_size, s_h2, s_w2, self.gf_dim * 1], name='g_h3', with_w=True)\n","            h3 = tf.nn.relu(self.g_bn3(h3))\n","\n","            # transposed CONV of [1] &tanh\n","            h4, self.h4_w, self.h4_b = deconv2d(\n","                h3, [self.batch_size, s_h, s_w, 1], name='g_h4', with_w=True)\n","\n","            return tf.nn.tanh(h4)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ozxoV-hYYkNu"},"source":["def get_model_network(sess, model_name, dataset_class):\n","    model = None\n","    if model_name == \"IAutoRec\":\n","        model = IAutoRec(sess, dataset_class)\n","    elif model_name == \"UAutoRec\":\n","        model = UAutoRec(sess, dataset_class)\n","    elif model_name == \"NNMF\":\n","        model = NNMF(sess, dataset_class)\n","    return model\n","\n","\n","def get_top_n(model, n):\n","    top_n = {}\n","    user_nonrated_items = model.dataset_class.get_user_nonrated_items()\n","    for uid in range(model.num_user):\n","        items = user_nonrated_items[uid]\n","        ratings = model.predict([uid] * len(items), items)\n","        item_rating = list(zip(items, ratings))\n","        item_rating.sort(key=lambda x: x[1], reverse=True)\n","        top_n[uid] = [x[0] for x in item_rating[:n]]\n","    return top_n\n","\n","\n","def pred_for_target(model, target_id):\n","    target_predictions = model.predict(list(range(model.num_user)), [target_id] * model.num_user)\n","\n","    top_n = get_top_n(model, n=50)\n","    hit_ratios = {}\n","    for uid in top_n:\n","        hit_ratios[uid] = [1 if target_id in top_n[uid][:i] else 0 for i in [1, 3, 5, 10, 20, 50]]\n","    return target_predictions, hit_ratios\n","\n","\n","def rec_trainer(model_name, dataset_class, target_id, is_train, model_path):\n","    tf.reset_default_graph()\n","    tf_config = tf.ConfigProto()\n","    tf_config.gpu_options.allow_growth = True\n","    with tf.Session(config=tf_config) as sess:\n","\n","        rec_model = get_model_network(sess, model_name, dataset_class)\n","        if is_train:\n","            print('--> start train recommendation model...')\n","            rec_model.execute()\n","            rec_model.save(model_path)\n","        else:\n","            rec_model.restore(model_path)\n","        print('--> start pred for each user...')\n","        predictions, hit_ratios = pred_for_target(rec_model, target_id)\n","    return predictions, hit_ratios"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"cellView":"form","id":"3iPMFbT_YkKI"},"source":["#@markdown surprise trainer\n","# import os\n","# from surprise import Dataset, Reader, accuracy\n","# from surprise import SVD, SVDpp, NMF, KNNBasic, KNNWithMeans, KNNWithZScore\n","# from surprise.model_selection import PredefinedKFold\n","# from collections import defaultdict\n","\n","\n","# def get_top_n(predictions, n=50):\n","#     # First map the predictions to each user.\n","#     top_n = defaultdict(list)\n","#     for uid, iid, true_r, est, _ in predictions:\n","#         top_n[uid].append((iid, est))\n","#     # Then sort the predictions for each user and retrieve the k highest ones.\n","#     for uid, user_ratings in top_n.items():\n","#         user_ratings.sort(key=lambda x: x[1], reverse=True)\n","#         top_n[uid] = user_ratings[:n]\n","#     return top_n\n","\n","\n","# def get_model(model_name):\n","#     algo = None\n","#     if 'KNN' in model_name:\n","#         model_name = model_name.split('_')\n","#         knn_model_name = model_name[0]\n","#         user_based = False if len(model_name) > 1 and model_name[1] == 'I' else True\n","#         dis_method = 'msd' if len(model_name) < 3 else model_name[2]\n","#         k = 20 if len(model_name) < 4 else int(model_name[3])\n","#         sim_options = {'user_based': user_based, 'name': dis_method}\n","#         if knn_model_name == 'KNNBasic':\n","#             algo = KNNBasic(sim_options=sim_options, k=k)\n","#         elif knn_model_name == 'KNNWithMeans':\n","#             algo = KNNWithMeans(sim_options=sim_options, k=k)\n","#         elif knn_model_name == 'KNNWithZScore':\n","#             algo = KNNWithZScore(sim_options=sim_options, k=k)\n","#     elif 'SVDpp' in model_name or 'SVD' in model_name or 'NMF' in model_name:\n","#         model_name = model_name.split('_')\n","#         n_factors = 25 if len(model_name) == 1 else int(model_name[1])\n","#         if model_name[0] == 'SVDpp':\n","#             algo = SVDpp(n_factors=n_factors)\n","#         elif model_name[0] == 'SVD':\n","#             algo = SVD(n_factors=n_factors)\n","#         elif model_name[0] == 'NMF':\n","#             algo = NMF(n_factors=n_factors)\n","#     return algo\n","\n","\n","# def get_model_old(model_name):\n","#     algo = None\n","#     if model_name == 'KNNBasic_U':\n","#         sim_options = {'user_based': True}\n","#         algo = KNNBasic(sim_options=sim_options, k=20)\n","#     elif model_name == 'KNNBasic_I':\n","#         sim_options = {'user_based': False}\n","#         algo = KNNBasic(sim_options=sim_options, k=20)\n","#         # algo = KNNBasic()\n","#     elif model_name == 'KNNWithMeans_I':\n","#         algo = KNNWithMeans(sim_options={'user_based': False}, k=20)\n","#     elif model_name == 'KNNWithMeans_U':\n","#         algo = KNNWithMeans(sim_options={'user_based': True}, k=20)\n","#     elif model_name == 'KNNWithZScore_I':\n","#         algo = KNNWithZScore(sim_options={'user_based': False}, k=20)\n","#     elif model_name == 'KNNWithZScore_U':\n","#         algo = KNNWithZScore(sim_options={'user_based': True}, k=20)\n","#     elif model_name == 'SVDpp':\n","#         algo = SVDpp()\n","#     elif model_name == 'SVD':\n","#         algo = SVD()\n","#     elif model_name == 'NMF':\n","#         algo = NMF()\n","#     elif 'NMF_' in model_name:\n","#         n_factors = int(model_name.split(\"_\")[1])\n","#         algo = NMF(n_factors=n_factors)\n","#     elif 'SVDpp_' in model_name:\n","#         n_factors = int(model_name.split(\"_\")[1])\n","#         algo = SVDpp(n_factors=n_factors)\n","#     elif 'SVD_' in model_name:\n","#         n_factors = int(model_name.split(\"_\")[1])\n","#         algo = SVD(n_factors=n_factors)\n","#     elif 'KNNBasic_U_' in model_name:\n","#         k = int(model_name.split(\"_\")[-1])\n","#         sim_options = {'user_based': True}\n","#         algo = KNNBasic(sim_options=sim_options, k=k)\n","#     elif 'KNNBasic_I_' in model_name:\n","#         k = int(model_name.split(\"_\")[-1])\n","#         sim_options = {'user_based': False}\n","#         algo = KNNBasic(sim_options=sim_options, k=k)\n","#     return algo\n","\n","\n","# def basic_rec(model_name, train_path, test_path, target_id):\n","#     # build data\n","#     # TODO check float and min_r\n","#     reader = Reader(line_format='user item rating', sep='\\t', rating_scale=(1, 5))\n","#     data = Dataset.load_from_folds([(train_path, test_path)], reader=reader)\n","#     trainset, testset = None, None\n","#     pkf = PredefinedKFold()\n","#     for trainset_, testset_ in pkf.split(data):\n","#         trainset, testset = trainset_, testset_\n","\n","#     # train model\n","#     rec_algo = get_model(model_name)\n","#     rec_algo.fit(trainset)\n","#     # eval\n","#     preds = rec_algo.test(testset)\n","#     rmse = accuracy.rmse(preds, verbose=True)\n","\n","#     # predor target\n","#     fn_pred = lambda uid: rec_algo.predict(str(uid), str(target_id), r_ui=0).est\n","#     target_predictions = list(map(fn_pred, range(trainset.n_users)))\n","\n","#     # topn\n","#     testset = trainset.build_anti_testset()\n","#     predictions = rec_algo.test(testset)\n","#     top_n = get_top_n(predictions, n=50)\n","\n","#     hit_ratios = {}\n","#     for uid, user_ratings in top_n.items():\n","#         topN = [int(iid) for (iid, _) in user_ratings]\n","#         hits = [1 if target_id in topN[:i] else 0 for i in [1, 3, 5, 10, 20, 50]]\n","#         hit_ratios[int(uid)] = hits\n","#     return target_predictions, hit_ratios"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"cellView":"form","id":"ovapnDmQYkIK"},"source":["#@markdown class BaselineAttack\n","class BaselineAttack:\n","\n","    def __init__(self, attack_num, filler_num, n_items, target_id,\n","                 global_mean, global_std, item_means, item_stds, r_max, r_min, fixed_filler_indicator=None):\n","        #\n","        self.attack_num = attack_num\n","        self.filler_num = filler_num\n","        self.n_items = n_items\n","        self.target_id = target_id\n","        self.global_mean = global_mean\n","        self.global_std = global_std\n","        self.item_means = item_means\n","        self.item_stds = item_stds\n","        self.r_max = r_max\n","        self.r_min = r_min\n","        # 固定sample的filler\n","        self.fixed_filler_indicator = fixed_filler_indicator\n","\n","    def RandomAttack(self):\n","        filler_candis = list(set(range(self.n_items)) - {self.target_id})\n","        fake_profiles = np.zeros(shape=[self.attack_num, self.n_items], dtype=float)\n","        # target\n","        fake_profiles[:, self.target_id] = self.r_max\n","        # fillers\n","        for i in range(self.attack_num):\n","            if self.fixed_filler_indicator is None:\n","                fillers = np.random.choice(filler_candis, size=self.filler_num, replace=False)\n","            else:\n","                # 读已有的sample结果\n","                fillers = np.where(np.array(self.fixed_filler_indicator[i])== 1)[0]\n","            ratings = np.random.normal(loc=self.global_mean, scale=self.global_std, size=self.filler_num)\n","            for f_id, r in zip(fillers, ratings):\n","                fake_profiles[i][f_id] = max(math.exp(-5), min(self.r_max, r))\n","        return fake_profiles\n","\n","    def BandwagonAttack(self, selected_ids):\n","        filler_candis = list(set(range(self.n_items)) - set([self.target_id] + selected_ids))\n","        fake_profiles = np.zeros(shape=[self.attack_num, self.n_items], dtype=float)\n","        # target & selected patch\n","        fake_profiles[:, [self.target_id] + selected_ids] = self.r_max\n","        # fillers\n","        for i in range(self.attack_num):\n","            if self.fixed_filler_indicator is None:\n","                fillers = np.random.choice(filler_candis, size=self.filler_num, replace=False)\n","            else:\n","                # 读已有的sample结果\n","                fillers = np.where(np.array(self.fixed_filler_indicator[i])== 1)[0]\n","            ratings = np.random.normal(loc=self.global_mean, scale=self.global_std, size=self.filler_num)\n","            for f_id, r in zip(fillers, ratings):\n","                fake_profiles[i][f_id] = max(math.exp(-5), min(self.r_max, r))\n","        return fake_profiles\n","\n","    def AverageAttack(self):\n","        filler_candis = list(set(range(self.n_items)) - {self.target_id})\n","        fake_profiles = np.zeros(shape=[self.attack_num, self.n_items], dtype=float)\n","        # target\n","        fake_profiles[:, self.target_id] = self.r_max\n","        # fillers\n","        fn_normal = lambda iid: np.random.normal(loc=self.item_means[iid], scale=self.item_stds[iid], size=1)[0]\n","        for i in range(self.attack_num):\n","            if self.fixed_filler_indicator is None:\n","                fillers = np.random.choice(filler_candis, size=self.filler_num, replace=False)\n","            else:\n","                # 读已有的sample结果\n","                fillers = np.where(np.array(self.fixed_filler_indicator[i])== 1)[0]\n","            ratings = map(fn_normal, fillers)\n","            for f_id, r in zip(fillers, ratings):\n","                fake_profiles[i][f_id] = max(math.exp(-5), min(self.r_max, r))\n","        return fake_profiles\n","\n","    def SegmentAttack(self, selected_ids):\n","        filler_candis = list(set(range(self.n_items)) - set([self.target_id] + selected_ids))\n","        fake_profiles = np.zeros(shape=[self.attack_num, self.n_items], dtype=float)\n","        # target & selected patch\n","        fake_profiles[:, [self.target_id] + selected_ids] = self.r_max\n","        # fillers\n","        for i in range(self.attack_num):\n","            if self.fixed_filler_indicator is None:\n","                fillers = np.random.choice(filler_candis, size=self.filler_num, replace=False)\n","            else:\n","                # 读已有的sample结果\n","                fillers = np.where(np.array(self.fixed_filler_indicator[i])== 1)[0]\n","            fake_profiles[i][fillers] = self.r_min\n","        return fake_profiles"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"cellView":"form","id":"pvBb5S53Zi4s"},"source":["#@markdown class GAN_Attacker\n","class GAN_Attacker:\n","    def __init__(self):\n","        print(\"GAN Attack model\")\n","\n","    def DIS(self, input, inputDim, h, activation, hiddenLayers, _reuse=False):\n","        # input->hidden\n","        y, _, W, b = self.FullyConnectedLayer(input, inputDim, h, activation, \"dis\", 0, reuse=_reuse)\n","\n","        # stacked hidden layers\n","        for layer in range(hiddenLayers - 1):\n","            y, _, W, b = self.FullyConnectedLayer(y, h, h, activation, \"dis\", layer + 1, reuse=_reuse)\n","\n","        # hidden -> output\n","        y, _, W, b = self.FullyConnectedLayer(y, h, 1, \"none\", \"dis\", hiddenLayers + 1, reuse=_reuse)\n","\n","        return y\n","\n","    def GEN(self, input, num_item, h, outputDim, activation, decay, name=\"gen\", _reuse=False):\n","        \"\"\"\n","        input   :   sparse filler vectors\n","        output  :   reconstructed selected vector\n","        \"\"\"\n","        # input+thnh\n","        # input_tanh = tf.nn.tanh(input)\n","\n","        # input->hidden\n","\n","        y, L2norm, W, b = self.FullyConnectedLayer(input, num_item, h // decay, activation, name, 0, reuse=_reuse)\n","\n","        # stacked hidden layers\n","        h = h // decay\n","        layer = 0\n","        # for layer in range(hiddenLayers - 1):\n","        while True:\n","            y, this_L2, W, b = self.FullyConnectedLayer(y, h, h // decay, activation, name, layer + 1, reuse=_reuse)\n","            L2norm = L2norm + this_L2\n","            layer += 1\n","            if h // decay > outputDim:\n","                h = h // decay\n","            else:\n","                break\n","        # hidden -> output\n","        y, this_L2, W, b = self.FullyConnectedLayer(y, h // decay, outputDim, \"none\", name, layer + 1, reuse=_reuse)\n","        L2norm = L2norm + this_L2\n","        y = tf.nn.sigmoid(y) * 5\n","        return y, L2norm\n","\n","    def FullyConnectedLayer(self, input, inputDim, outputDim, activation, model, layer, reuse=False):\n","        scale1 = math.sqrt(6 / (inputDim + outputDim))\n","\n","        wName = model + \"_W\" + str(layer)\n","        bName = model + \"_B\" + str(layer)\n","\n","        with tf.variable_scope(model) as scope:\n","\n","            if reuse == True:\n","                scope.reuse_variables()\n","\n","            W = tf.get_variable(wName, [inputDim, outputDim],\n","                                initializer=tf.random_uniform_initializer(-scale1, scale1))\n","            b = tf.get_variable(bName, [outputDim], initializer=tf.random_uniform_initializer(-0.01, 0.01))\n","\n","            y = tf.matmul(input, W) + b\n","\n","            L2norm = tf.nn.l2_loss(W) + tf.nn.l2_loss(b)\n","\n","            if activation == \"none\":\n","                y = tf.identity(y, name=\"output\")\n","                return y, L2norm, W, b\n","\n","            elif activation == \"sigmoid\":\n","                return tf.nn.sigmoid(y), L2norm, W, b\n","\n","            elif activation == \"tanh\":\n","                return tf.nn.tanh(y), L2norm, W, b\n","            elif activation == \"relu\":\n","                return tf.nn.relu(y), L2norm, W, b"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"cellView":"form","id":"LKVytFWgYkGc"},"source":["#@markdown class CopyGanAttacker\n","class CopyGanAttacker:\n","    def __init__(self, dataset_class, target_id, filler_num, attack_num, filler_method):\n","        # data set info\n","        self.dataset_class = dataset_class\n","        self.num_user = dataset_class.n_users\n","        self.num_item = dataset_class.n_items\n","        self.rating_matrix = dataset_class.train_matrix.toarray()  # tf.constant()\n","\n","        # attack info\n","        self.target_id = target_id\n","        self.filler_num = filler_num\n","        self.attack_num = attack_num\n","        self.filler_method = filler_method\n","\n","    def build_model(self):\n","        # define place_holder\n","        # self.user_vector = tf.placeholder(tf.int32, [None, self.num_item])\n","        # self.item_vector = tf.placeholder(tf.int32, [None, self.num_item])\n","        self.sampled_template = tf.placeholder(tf.int32, [self.args.batch_size, self.num_item])\n","        self.batch_filler_index = tf.placeholder(tf.int32, [None, self.args.batch_size])\n","        # user/item embedding\n","        # c = tf.constant(c)\n","        user_embedding = self.towerMlp(self.rating_matrix, self.num_item, self.args.embedding_dim)\n","        item_embedding = self.towerMlp(self.rating_matrix.transpose(), self.num_user, self.args.embedding_dim)\n","\n","        \"\"\"\n","        copy net  \n","        p_copy(j)=sigmoid (w x j’s item embedding + w x u’s user embedding + b)\"\"\"\n","        with tf.name_scope(\"copyNet\"):\n","            w1 = tf.get_variable('w1', [self.args.embedding_dim, self.num_item])\n","            p1 = tf.matmul(tf.nn.embedding_lookup(user_embedding, self.batch_filler_index), w1)  # batch*item_num\n","            w2 = tf.get_variable('w2', [self.args.embedding_dim, 1])\n","            p2 = tf.matmul(item_embedding, w2)  # item_num*1\n","            b = tf.get_variable('b', [self.item_num])\n","            copy_prob = tf.nn.sigmoid(p1 + p2 + b)  # batch*item_num\n","        \"\"\"\n","        generate net\n","        p_gen(j=r)\n","        \"\"\"\n","        with tf.name_scope(\"genNet\"):\n","            gen_probabilitiy_list = []\n","            for i in range(5):\n","                with tf.name_scope(\"s_%d\" % i):\n","                    w1 = tf.get_variable('w1', [self.args.embedding_dim, self.num_item])\n","                    p1 = tf.matmul(tf.nn.embedding_lookup(user_embedding, self.batch_filler_index),\n","                                   w1)  # batch*item_num\n","                    w2 = tf.get_variable('w2', [self.args.embedding_dim, 1])\n","                    p2 = tf.matmul(item_embedding, w2)  # item_num*1\n","                    b = tf.get_variable('b', [self.item_num])\n","                    gen_probability = p1 + p2 + b\n","                    gen_probabilitiy_list.append(tf.expand_dims(gen_probability, 2))  # batch*item_num*1\n","            gen_rating_distri = tf.nn.softmax(tf.concat(gen_probabilitiy_list, axis=2))  # batch*item_num*5\n","        \"\"\"\n","        Rating\n","        rating p(r) = p_copy(j) x p_copy(j=r) + (1-p_copy(j)) x p_gen(j=r)\n","        \"\"\"\n","        copy_rating_distri = tf.reshape(tf.expand_dims(tf.one_hot(self.sampled_template, 5), 3),\n","                                        [self.args.batch_size, -1, 5])\n","        rating_distri = copy_prob * copy_rating_distri + (1 - copy_prob) * gen_rating_distri  # batch*item_num*5\n","        rating_value = tf.tile(tf.constant([[[1., 2., 3., 4., 5.]]]), [self.args.batch_size, self.num_item, 1])\n","        fake_profiles = tf.reduce_sum(rating_distri * rating_value, 2)\n","\n","        \"\"\"\n","        loss function\n","        \"\"\"\n","        with tf.name_scope(\"Discriminator\"):\n","            D_real = self.towerMlp(self.sampled_template, self.num_item, 1)\n","            D_fake = self.towerMlp(fake_profiles, self.num_item, 1)\n","\n","        \"\"\"\n","        loss function\n","        \"\"\"\n","        with tf.name_scope(\"loss_D\"):\n","            d_loss_real = tf.reduce_mean(\n","                tf.nn.sigmoid_cross_entropy_with_logits(logits=D_real, labels=tf.ones_like(D_real)),\n","                name=\"loss_real\")\n","            d_loss_fake = tf.reduce_mean(\n","                tf.nn.sigmoid_cross_entropy_with_logits(logits=D_fake, labels=tf.zeros_like(D_fake)),\n","                name=\"loss_fake\")\n","            loss_D = d_loss_real + d_loss_fake\n","        with tf.name_scope(\"loss_G\"):\n","            # reconstruction loss\n","            loss_rec = tf.reduce_mean(tf.square(fake_profiles - self.sampled_template))\n","            # adversial loss\n","            loss_adv = tf.reduce_mean(\n","                tf.nn.sigmoid_cross_entropy_with_logits(logits=D_fake, labels=tf.ones_like(D_fake)))\n","            loss_G = loss_rec + loss_adv\n","\n","    def towerMlp(self, input, inputDim, outputDim):\n","        dim, x = inputDim // 2, input\n","        while dim > outputDim:\n","            layer = tf.layers.dense(\n","                inputs=x,\n","                units=dim,\n","                kernel_initializer=tf.random_normal_initializer,\n","                activation=tf.nn.relu,\n","                kernel_regularizer=tf.contrib.layers.l2_regularizer(scale=self.reg_rate))\n","            dim, x = dim // 2, layer\n","        output = tf.layers.dense(\n","            inputs=x,\n","            units=outputDim,\n","            kernel_initializer=tf.random_normal_initializer,\n","            activation=tf.nn.sigmoid,\n","            kernel_regularizer=tf.contrib.layers.l2_regularizer(scale=self.reg_rate))\n","        return output"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"cellView":"form","id":"IlNqogtTYkEi"},"source":["#@markdown class Train_G_Attacker\n","class Train_G_Attacker:\n","    def __init__(self, dataset_class, params_D, params_G, target_id, selected_id_list,\n","                 filler_num, attack_num, filler_method, loss_setting):\n","        # TODO:init refine\n","        # data set info\n","        self.dataset_class = dataset_class\n","        self.num_user = dataset_class.n_users\n","        self.num_item = dataset_class.n_items\n","\n","        # attack info\n","        self.target_id = target_id\n","        self.selected_id_list = selected_id_list\n","        self.selected_num = len(self.selected_id_list)\n","        self.filler_num = filler_num\n","        self.attack_num = attack_num\n","        self.filler_method = filler_method\n","        self.loss_setting = loss_setting\n","\n","        # model params\n","        self.totalEpochs = 150\n","        self.ZR_ratio = 0.5\n","        # G\n","        if params_G is None:\n","            # MLP structure\n","            self.hiddenDim_G = 400\n","            # optimize params\n","            self.reg_G = 0.0001\n","            self.lr_G = 0.01\n","            self.opt_G = 'adam'\n","            self.step_G = 1\n","            self.batchSize_G = 128 * 2\n","            self.batchNum_G = 10\n","            # self.G_loss_weights = [1, 1, 1, 1]\n","            self.G_loss_weights = [1, 1, 1]\n","            self.decay_g = 3\n","        else:\n","            self.hiddenDim_G, self.hiddenLayer_G, self.scale, \\\n","            self.reg_G, self.lr_G, self.opt_G, self.step_G, self.batchSize_G, self.batchNum_G, self.G_loss_weights = params_G\n","\n","        # if params_D is None:\n","        #     # MLP structure\n","        #     self.hiddenDim_D = 150\n","        #     self.hiddenLayer_D = 3\n","        #     # optimize params\n","        #     self.reg_D = 1e-05\n","        #     self.lr_D = 0.0001\n","        #     self.opt_D = 'adam'\n","        #     self.step_D = 1\n","        #     self.batchSize_D = 64\n","        # else:\n","        #     self.hiddenDim_D, self.hiddenLayer_D, \\\n","        #     self.reg_D, self.lr_D, self.opt_D, self.step_D, self.batchSize_D = params_D\n","        #\n","        self.log_dir = '_'.join(\n","            list(map(str, [self.loss_setting] + self.G_loss_weights + [self.step_G, self.ZR_ratio, str(target_id)])))\n","\n","    def train_gan(self):\n","        for epoch in range(self.totalEpochs):\n","            self.epoch = epoch\n","            with open(self.log_path, \"a+\") as fout:\n","                fout.write(\"epoch:\" + str(epoch) + \"\\n\")\n","                fout.flush()\n","\n","            # for epoch_D in range(self.step_D):\n","            #     self.epoch_D = epoch_D\n","            #     loss_D, a, b = self.train_D()\n","            #     print('D', epoch_D, ':', round(loss_D, 5), a, end=\"\")\n","            #     print(b[0])\n","            #     with open(self.log_path, \"a+\") as fout:\n","            #         log_tmp = 'D' + str(epoch_D) + ':' + str(round(loss_D, 5)) + str(a) + str(b[0])\n","            #         fout.write(log_tmp + \"\\n\")\n","            #         fout.flush()\n","\n","            for epoch_G in range(self.step_G):\n","                self.epoch_G = epoch_G\n","                loss_G, loss_G_array, g_out_seed, log_info = self.train_G()\n","                with open(self.log_path, \"a+\") as fout:\n","                    log_tmp = 'G' + str(epoch_G) + ':' + str(round(loss_G, 5)) \\\n","                              + str(loss_G_array) + str(g_out_seed) + str(log_info)\n","                    fout.write(log_tmp + \"\\n\")\n","                    fout.flush()\n","                print('G', epoch_G, ':', round(loss_G, 5), loss_G_array, g_out_seed, log_info)\n","\n","    def execute(self, is_train, model_path, final_attack_setting):\n","        self.log_path = 'logs/' + self.log_dir + '/' + \"training_log.log\"\n","        # if os.path.exists('logs/' + self.log_dir):\n","        #     print(\"\\n\\n\\nexist!!\\n\\n\\n\")\n","        #     return\n","\n","        with tf.Graph().as_default():\n","            self._data_preparation()\n","            self._build_graph()\n","            self.sess = tf.Session()\n","            self.sess.run(tf.global_variables_initializer())\n","            if is_train == 0:\n","                self.restore(model_path)\n","            else:\n","                self.wirter = tf.summary.FileWriter('logs/' + self.log_dir + '/', self.sess.graph)\n","                self.train_gan()\n","                self.save(model_path)\n","            # 生成攻击文件\n","            fake_profiles, real_profiles_, filler_indicator_ \\\n","                = self.fake_profiles_generator(final_attack_setting)\n","            return fake_profiles, real_profiles_, filler_indicator_\n","\n","    def fake_profiles_generator(self, final_attack_setting):\n","        fake_num, real_vector, filler_indicator = final_attack_setting\n","\n","        # input filler\n","        if real_vector is None or filler_indicator is None:\n","            batchList = self.batchList.copy()\n","            while fake_num > len(batchList):\n","                batchList += batchList\n","            random.shuffle(batchList)\n","            sampled_index = batchList[:fake_num]\n","            real_vector = self.dataset_class.train_matrix[sampled_index].toarray()\n","            filler_indicator = self.filler_sampler(sampled_index)\n","\n","        # output fake profiles\n","        fake_profiles = self.sess.run(self.fakeData, feed_dict={self.G_input: real_vector,\n","                                                                self.filler_dims: filler_indicator})\n","        return fake_profiles, real_vector, filler_indicator\n","\n","    def _build_graph(self):\n","        self.filler_dims = tf.placeholder(tf.float32, [None, self.num_item])  # filler = 1, otherwise 0\n","        self.selected_dims = tf.squeeze(\n","            tf.reduce_sum(tf.one_hot([self.selected_id_list], self.num_item, dtype=tf.float32), 1))\n","\n","        self.models = GAN_Attacker()\n","        # G\n","        with tf.name_scope(\"Generator\"):\n","            self.G_input = tf.placeholder(tf.float32, [None, self.num_item], name=\"G_input\")\n","            self.rating_matrix_mask = tf.placeholder(tf.float32, [None, self.num_item])  # rated = 1, otherwise 0\n","            self.G_output, self.G_L2norm = self.models.GEN(self.G_input * self.filler_dims, self.num_item,\n","                                                           self.hiddenDim_G, self.selected_num, 'sigmoid',\n","                                                           decay=self.decay_g, name=\"gen\")\n","\n","        with tf.name_scope(\"Fake_Data\"):\n","            selected_patch = None\n","            for i in range(self.selected_num):\n","                one_hot = tf.one_hot(self.selected_id_list[i], self.num_item, dtype=tf.float32)\n","                mask = tf.boolean_mask(self.G_output, tf.one_hot(i, self.selected_num, dtype=tf.int32), axis=1)\n","                if i == 0:\n","                    selected_patch = one_hot * mask\n","                else:\n","                    selected_patch += one_hot * mask\n","            self.fakeData = selected_patch + self.target_patch + self.G_input * self.filler_dims\n","        # # D\n","        # with tf.name_scope(\"Discriminator\"):\n","        #     self.realData_ = tf.placeholder(tf.float32, shape=[None, self.num_item], name=\"real_data\")\n","        #     self.filler_dims_D = tf.placeholder(tf.float32, [None, self.num_item])  # filler = 1, otherwise 0\n","        #     self.realData = self.realData_ * (self.filler_dims_D + self.selected_dims)\n","        #\n","        #     self.D_real = self.models.DIS(self.realData * self.target_mask, self.num_item * 1, self.hiddenDim_D,\n","        #                                   'sigmoid', self.hiddenLayer_D)\n","        #\n","        #     self.D_fake = self.models.DIS(self.fakeData * self.target_mask, self.num_item * 1, self.hiddenDim_D,\n","        #                                   'sigmoid', self.hiddenLayer_D, _reuse=True)\n","\n","        self.g_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope='gen')\n","        # self.d_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope='dis')\n","\n","        # define loss & optimizer for G\n","        with tf.name_scope(\"loss_G\"):\n","            # self.g_loss_gan = tf.reduce_mean(\n","            #     tf.nn.sigmoid_cross_entropy_with_logits(logits=self.D_fake, labels=tf.ones_like(self.D_fake)))\n","            self.g_loss_reconstruct_seed = tf.reduce_mean(\n","                tf.reduce_sum(tf.square(self.fakeData - self.G_input) * self.rating_matrix_mask * self.selected_dims,\n","                              1, keepdims=True))\n","            self.g_loss_list = [self.g_loss_reconstruct_seed]\n","\n","            if self.loss_setting == 1:\n","                self.g_loss_seed = tf.reduce_mean(\n","                    tf.reduce_mean(tf.square(self.G_output - 5.0), 1, keepdims=True))\n","                self.g_loss_list.append(self.g_loss_seed)\n","            self.g_loss_l2 = self.reg_G * self.G_L2norm\n","            self.g_loss_list.append(self.g_loss_l2)\n","            # self.g_loss_list = [self.g_loss_gan, self.g_loss_seed,\n","            #                     self.g_loss_reconstruct_seed, self.g_loss_l2]\n","            # self.g_loss_list = [self.g_loss_seed, self.g_loss_reconstruct_seed, self.g_loss_l2]\n","            self.g_loss = sum(self.g_loss_list[i] * self.G_loss_weights[i] for i in range(len(self.g_loss_list)))\n","\n","        # tensorboard summary\n","        self.add_loss_summary(type='G')\n","\n","        with tf.name_scope(\"optimizer_G\"):\n","            if self.opt_G == 'sgd':\n","                self.trainer_G = tf.train.GradientDescentOptimizer(self.lr_G).minimize(self.g_loss,\n","                                                                                       var_list=self.g_vars,\n","                                                                                       name=\"GradientDescent_G\")\n","            elif self.opt_G == 'adam':\n","                self.trainer_G = tf.train.AdamOptimizer(self.lr_G).minimize(self.g_loss, var_list=self.g_vars,\n","                                                                            name=\"Adam_G\")\n","\n","        # define loss & optimizer for D\n","\n","        # with tf.name_scope(\"loss_D\"):\n","        #     d_loss_real = tf.reduce_mean(\n","        #         tf.nn.sigmoid_cross_entropy_with_logits(logits=self.D_real, labels=tf.ones_like(self.D_real)),\n","        #         name=\"loss_real\")\n","        #     d_loss_fake = tf.reduce_mean(\n","        #         tf.nn.sigmoid_cross_entropy_with_logits(logits=self.D_fake, labels=tf.zeros_like(self.D_fake)),\n","        #         name=\"loss_fake\")\n","        #     D_L2norm = 0\n","        #     for pr in self.d_vars:\n","        #         D_L2norm += tf.nn.l2_loss(pr)\n","        #     self.d_loss = d_loss_real + d_loss_fake + self.reg_D * D_L2norm\n","        #     self.d_loss_real, self.d_loss_fake, self.D_L2norm = d_loss_real, d_loss_fake, D_L2norm\n","        # with tf.name_scope(\"optimizer_D\"):\n","        #     if self.opt_D == 'sgd':\n","        #         self.trainer_D = tf.train.GradientDescentOptimizer(self.lr_D).minimize(self.d_loss,\n","        #                                                                                var_list=self.d_vars,\n","        #                                                                                name=\"GradientDescent_D\")\n","        #     elif self.opt_D == 'adam':\n","        #         self.trainer_D = tf.train.AdamOptimizer(self.lr_D).minimize(self.d_loss, var_list=self.d_vars,\n","        #                                                                     name=\"Adam_D\")\n","\n","    def _data_preparation(self):\n","        self.target_patch = tf.one_hot(self.target_id, self.num_item, dtype=tf.float32) * 5\n","        self.target_mask = 1 - tf.one_hot(self.target_id, self.num_item, dtype=tf.float32)\n","        # prepare train data\n","        self.train_matrix = self.dataset_class.train_matrix.toarray().astype(np.float32)\n","        self.train_mask = self.train_matrix.copy()\n","        self.train_mask[self.train_mask > 0] = 1\n","        self.filler_candi_set = set(range(self.num_item)) - set(self.selected_id_list + [self.target_id])\n","        self.filler_candi_list = list(self.filler_candi_set)\n","\n","        # sample filler>filler num\n","        self.batchList = []\n","        for i in range(self.num_user):\n","            set_rated = set(self.train_mask[i].nonzero()[0])\n","            if len(self.filler_candi_set & set_rated) < self.filler_num: continue\n","            self.batchList.append(i)\n","\n","        # 没有在train set对target item评分的用户，用来算all user的pred shift\n","        self.non_rated_users = self.dataset_class.get_item_nonrated_users(self.target_id)\n","        # item pop/avg\n","        self.item_pop = np.array(self.dataset_class.get_item_pop())\n","        _, _, self.item_avg, _ = self.dataset_class.get_all_mean_std()\n","        self.item_avg = np.array(self.item_avg)\n","\n","        # big cap\n","        if self.filler_method == 3:\n","            print(\"\\n==\\n==\\n修改路径！！\\n==\\n\")\n","            attack_info_path = [\"../data/data/filmTrust_selected_items\", \"../data/data/filmTrust_selected_items\"]\n","            attack_info = load_attack_info(*attack_info_path)\n","            target_users = attack_info[self.target_id][1]\n","            uid_values = self.dataset_class.train_data.user_id.values\n","            idxs = [idx for idx in range(len(uid_values)) if uid_values[idx] in target_users]\n","            iid_values = self.dataset_class.train_data.loc[idxs, 'item_id']\n","            iid_values = iid_values.tolist()\n","            from collections import Counter\n","            iid_values = Counter(iid_values)\n","            self.item_big_cap = np.array([iid_values.get(iid, 0.5) for iid in range(self.num_item)])\n","\n","    def train_G(self):\n","        t1 = time.time()\n","        random.seed(int(t1))\n","        random.shuffle(self.batchList)\n","        #\n","        batch_real_vector = None\n","        batch_run_res = None\n","        #\n","        total_loss_g = 0\n","        # total_loss_array = np.array([0., 0., 0., 0.])\n","        total_loss_array = np.array([0.] * len(self.g_loss_list))\n","        total_batch = int(len(self.batchList) / self.batchSize_G) + 1\n","        for batch_id in range(total_batch):\n","            if batch_id == total_batch - 1:\n","                batch_index = self.batchList[batch_id * self.batchSize_G:]\n","            else:\n","                batch_index = self.batchList[batch_id * self.batchSize_G: (batch_id + 1) * self.batchSize_G]\n","\n","            batch_size = len(batch_index)\n","            batch_real_vector = self.train_matrix[batch_index]\n","            batch_mask = self.train_mask[batch_index]\n","\n","            # sample zero for zero reconstruction\n","            batch_mask_ZR = batch_mask.copy()\n","            if self.ZR_ratio > 0:\n","                for idx in range(batch_size):\n","                    batch_mask_ZR[idx][self.selected_id_list] = \\\n","                        [1 if i == 1 or random.random() < self.ZR_ratio else 0 for i in\n","                         batch_mask_ZR[idx][self.selected_id_list]]\n","\n","            # sample fillers randomly\n","            batch_filler_indicator = self.filler_sampler(batch_index)\n","\n","            batch_run_res = self.sess.run(\n","                [self.trainer_G, self.g_loss] + self.g_loss_list + [self.G_output, self.G_loss_merged],\n","                feed_dict={self.G_input: batch_real_vector,\n","                           self.filler_dims: batch_filler_indicator,\n","                           self.rating_matrix_mask: batch_mask_ZR})  # Update G\n","\n","            total_loss_g += batch_run_res[1]\n","            total_loss_array += np.array(batch_run_res[2:2 + len(total_loss_array)])\n","\n","        self.wirter.add_summary(batch_run_res[-1], self.step_G * self.epoch + self.epoch_G + 1)\n","        total_loss_array = [round(i, 2) for i in total_loss_array]\n","        g_out_seed = [round(i, 2) for i in np.mean(batch_run_res[-2], 0)]\n","        #\n","        fn_float_to_str = lambda x: str(round(x, 2))\n","        r = batch_real_vector.transpose()[self.selected_id_list].transpose()\n","        g = batch_run_res[-2]\n","        rmse = list(map(fn_float_to_str, np.sum(np.square(np.abs(r - g)), 0)))\n","        var_col = list(map(fn_float_to_str, np.var(g, 0)))\n","        self.add_loss_summary(type=\"var\", info=np.var(g, 0))\n","        var_row = round(np.mean(np.var(g, 1)), 2)\n","        # var_col_ori = list(map(fn_float_to_str, np.var(r, 0)))\n","        # var_row_ori = round(np.mean(np.var(r, 1)), 2)\n","        log_info = \"rmse : \" + ','.join(rmse)\n","        log_info += \"\\tvar_col : \" + ','.join(var_col) + \"\\tvar_row : \" + str(var_row)\n","        # log_info += \"\\tvar_col_ori : \" + ','.join(var_col_ori) + \"\\tvar_row_ori : \" + str(var_row_ori)\n","        return total_loss_g, total_loss_array, g_out_seed, log_info  # [g_out_seed, mae, [var_col, var_row]]\n","\n","    # def train_D(self):\n","    #     \"\"\"\n","    #     每个epoch各产生self.batchSize_D个realData和fakeData\n","    #     \"\"\"\n","    #     t1 = time.time()\n","    #     random.seed(int(t1))\n","    #     random.shuffle(self.batchList)\n","    #\n","    #     total_loss_d, total_loss_d_real, total_loss_d_fake = 0, 0, 0\n","    #     #\n","    #     batch_filler_indicator = None\n","    #\n","    #     total_batch = int(len(self.batchList) / self.batchSize_D) + 1\n","    #     for batch_id in range(total_batch):\n","    #         # prepare data\n","    #         if batch_id == total_batch - 1:\n","    #             batch_index = self.batchList[batch_id * self.batchSize_D:]\n","    #         else:\n","    #             batch_index = self.batchList[batch_id * self.batchSize_D: (batch_id + 1) * self.batchSize_D]\n","    #         batch_size = len(batch_index)\n","    #         batch_real_vector = self.train_matrix[batch_index]\n","    #         batch_filler_indicator = self.filler_sampler(batch_index)\n","    #\n","    #         # optimize\n","    #         _, total_loss_d_, total_loss_d_real_, total_loss_d_fake_ \\\n","    #             = self.sess.run([self.trainer_D, self.d_loss, self.d_loss_real, self.d_loss_fake],\n","    #                             feed_dict={self.realData_: batch_real_vector,\n","    #                                        self.G_input: batch_real_vector,\n","    #                                        self.filler_dims: batch_filler_indicator,\n","    #                                        self.filler_dims_D: batch_filler_indicator})  # Update D\n","    #         total_loss_d += total_loss_d_\n","    #         total_loss_d_real += total_loss_d_real_\n","    #         total_loss_d_fake += total_loss_d_fake_\n","    #     self.add_loss_summary(type=\"D\", info=[total_loss_d, total_loss_d_real, total_loss_d_fake])\n","    #     debug_info = [self.G_output, self.fakeData,\n","    #                   tf.squeeze(tf.nn.sigmoid(self.D_real)), tf.squeeze(tf.nn.sigmoid(self.D_fake))]\n","    #     info = self.sess.run(debug_info, feed_dict={self.realData_: batch_real_vector,\n","    #                                                 self.G_input: batch_real_vector,\n","    #                                                 self.filler_dims: batch_filler_indicator,\n","    #                                                 self.filler_dims_D: batch_filler_indicator})\n","    #\n","    #     D_real, D_fake = info[2:4]\n","    #     fake_data = info[1]\n","    #     # lower bound\n","    #     lower_bound = []\n","    #     for v in fake_data:\n","    #         t = v.copy()\n","    #         t[[self.target_id]] = 0.0  # 对判别器mask掉了target信息\n","    #         t[self.selected_id_list] = 5.0\n","    #         lower_bound.append(t)\n","    #     # upper bound\n","    #     upper_bound = []\n","    #     i = 0\n","    #     for v in fake_data:\n","    #         t = v.copy()\n","    #         t[self.selected_id_list] = batch_real_vector[i][self.selected_id_list]\n","    #         t[[self.target_id]] = 0.0  # 对判别器mask掉了target信息\n","    #         upper_bound.append(t)\n","    #         i += 1\n","    #     zero_data = []  # fake_data.copy()\n","    #     for v in fake_data:\n","    #         t = v.copy()\n","    #         t[[self.target_id]] = 0.0  # 对判别器mask掉了target信息\n","    #         t[self.selected_id_list] = 0.0\n","    #         zero_data.append(t)\n","    #     random_data = []\n","    #     for v in fake_data:\n","    #         t = v.copy()\n","    #         t[self.selected_id_list] = np.random.choice(list([1., 2., 3., 4., 5.]), size=self.selected_num,\n","    #                                                     replace=True)\n","    #         t[[self.target_id]] = 0.0  # 对判别器mask掉了target信息\n","    #         random_data.append(t)\n","    #\n","    #     D_lower_bound = self.sess.run(tf.squeeze(tf.nn.sigmoid(self.D_real)),\n","    #                                   feed_dict={self.realData_: lower_bound,\n","    #                                              self.filler_dims_D: batch_filler_indicator})\n","    #     D_upper_bound = self.sess.run(tf.squeeze(tf.nn.sigmoid(self.D_real)),\n","    #                                   feed_dict={self.realData_: upper_bound,\n","    #                                              self.filler_dims_D: batch_filler_indicator})\n","    #\n","    #     D_zero = self.sess.run(tf.squeeze(tf.nn.sigmoid(self.D_real)),\n","    #                            feed_dict={self.realData_: zero_data, self.filler_dims_D: batch_filler_indicator})\n","    #     D_random = self.sess.run(tf.squeeze(tf.nn.sigmoid(self.D_real)),\n","    #                              feed_dict={self.realData_: random_data, self.filler_dims_D: batch_filler_indicator})\n","    #     # filler=1通常会更假\n","    #\n","    #     d_info = [round(np.mean(D_real), 2), round(np.mean(D_fake), 2),\n","    #               [round(np.mean(D_lower_bound), 2), round(np.mean(D_upper_bound), 2)],\n","    #               round(np.mean(D_zero), 2), round(np.mean(D_random), 2)]\n","    #     # s = [\"T:\", \"G:\", \"s=5:\", \"s=0:\", \"s=random:\", \"s=5,f=1:\"]\n","    #     # s = [\"real:\", \"fake:\", \"seed=5:\", \"seed=0:\", \"seed=random:\", \"seed=5,filler=1:\"]\n","    #     # d_info = ' '.join([str(d_info[i]) for i in range(len(d_info))])  # s[i]+ str(d_info[i])\n","    #\n","    #     #\n","    #     fn_float_to_str = lambda x: str(round(x, 2))\n","    #     g_out_seed = list(map(fn_float_to_str, np.mean(info[0], 0)))  # [round(i, 2) for i in np.mean(info[0], 0)]\n","    #\n","    #     #\n","    #\n","    #     g = info[0]\n","    #     var_col = list(map(fn_float_to_str, np.var(g, 0)))\n","    #     var_row = round(np.mean(np.var(g, 1)), 2)\n","    #     log_info = \"\\tg_out_seed:\" + ','.join(g_out_seed), \"\\tvar_col : \" + ','.join(var_col) + \"\\tvar_row : \" + str(\n","    #         var_row)\n","    #\n","    #     return total_loss_d, d_info, log_info\n","\n","    def filler_sampler(self, uid_list):\n","        if self.filler_method == 0:\n","            batch_filler_indicator = []\n","            for uid in uid_list:\n","                filler_candi = np.array(\n","                    list(set(self.filler_candi_list) & set(self.train_mask[uid].nonzero()[0].tolist())))\n","                if len(filler_candi) > self.filler_num:\n","                    filler_candi = np.random.choice(filler_candi, size=self.filler_num, replace=False)\n","                filler_indicator = [1 if iid in filler_candi else 0 for iid in range(self.num_item)]\n","                batch_filler_indicator.append(filler_indicator)\n","            return batch_filler_indicator\n","        else:\n","            return self.filler_sampler_method(uid_list)\n","\n","    def filler_sampler_method(self, uid_list):\n","        batch_filler_indicator = []\n","        for uid in uid_list:\n","            filler_candi = np.array(\n","                list(set(self.filler_candi_list) & set(self.train_mask[uid].nonzero()[0].tolist())))\n","            if len(filler_candi) > self.filler_num:\n","                # sample using a specific method\n","                # -------------------------\n","                prob = self.item_avg[filler_candi] if self.filler_method == 1 \\\n","                    else self.item_pop[filler_candi] if self.filler_method == 2 \\\n","                    else self.item_big_cap[filler_candi] if self.filler_method == 3 \\\n","                    else None\n","                prob = None if prob is None else prob / sum(prob)\n","                # -------------------------\n","\n","                filler_candi = np.random.choice(filler_candi, size=self.filler_num, replace=False, p=prob)\n","            filler_indicator = [1 if iid in filler_candi else 0 for iid in range(self.num_item)]\n","            batch_filler_indicator.append(filler_indicator)\n","        return batch_filler_indicator\n","\n","    def save(self, path):\n","        saver = tf.train.Saver()\n","        saver.save(self.sess, path)\n","\n","    def restore(self, path):\n","        saver = tf.train.Saver()\n","        saver.restore(self.sess, path)\n","\n","    def add_loss_summary(self, type=\"G\", info=None):\n","        # , total_loss_g, total_g_loss_gan, total_g_loss_seed, total_g_loss_reconstruct,total_g_loss_l2):\n","        if type == \"G\":\n","            # tf.summary.scalar('Generator/adversarial', self.g_loss_gan)\n","            if hasattr(self, 'g_loss_seed'):\n","                tf.summary.scalar('Generator/seed', self.g_loss_seed)\n","            tf.summary.scalar('Generator/selected_reconstruct', self.g_loss_reconstruct_seed)\n","            tf.summary.scalar('Generator/l2_normal', self.g_loss_l2)\n","            tf.summary.scalar('Generator/Sum', self.g_loss)\n","            self.G_loss_merged = tf.summary.merge_all()\n","\n","        # elif type == 'D':\n","        #     total_loss_d, total_loss_d_real, total_loss_d_fake = info\n","        #     loss_summary = []\n","        #     tag_list = ['Discriminator/Sum', 'Discriminator/real', 'Discriminator/fake']\n","        #     simple_value_list = [total_loss_d, total_loss_d_real, total_loss_d_fake]\n","        #     for i in range(3):\n","        #         loss_summary.append(tf.Summary.Value(tag=tag_list[i], simple_value=simple_value_list[i]))\n","        #     self.wirter.add_summary(tf.Summary(value=loss_summary), self.epoch * self.step_D + self.epoch_D + 1)\n","        elif type == 'var':\n","            var_summary = []\n","            for i in range(self.selected_num):\n","                var_summary.append(tf.Summary.Value(tag='Var/' + str(i), simple_value=info[i]))\n","            self.wirter.add_summary(tf.Summary(value=var_summary), self.step_G * self.epoch + self.epoch_G + 1)\n","        else:\n","            print(\"summary type error\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"cellView":"form","id":"Sbe-0hLiYkBu"},"source":["#@markdown class Train_GAN_Attacker\n","class Train_GAN_Attacker:\n","    def __init__(self, dataset_class, params_D, params_G, target_id, selected_id_list,\n","                 filler_num, attack_num, filler_method):\n","        # TODO:init refine\n","        # data set info\n","        self.dataset_class = dataset_class\n","        self.num_user = dataset_class.n_users\n","        self.num_item = dataset_class.n_items\n","\n","        # attack info\n","        self.target_id = target_id\n","        self.selected_id_list = selected_id_list\n","        self.selected_num = len(self.selected_id_list)\n","        self.filler_num = filler_num\n","        self.attack_num = attack_num\n","        self.filler_method = filler_method\n","\n","        # model params\n","        self.totalEpochs = 150\n","        self.ZR_ratio = 0.5\n","        # G\n","        if params_G is None:\n","            # MLP structure\n","            self.hiddenDim_G = 400\n","            # optimize params\n","            self.reg_G = 0.0001\n","            self.lr_G = 0.01\n","            self.opt_G = 'adam'\n","            self.step_G = 1\n","            self.batchSize_G = 128 * 2\n","            self.batchNum_G = 10\n","            self.G_loss_weights = [1, 1, 1, 1]\n","            self.decay_g = 3\n","        else:\n","            self.hiddenDim_G, self.hiddenLayer_G, self.scale, \\\n","            self.reg_G, self.lr_G, self.opt_G, self.step_G, self.batchSize_G, self.batchNum_G, self.G_loss_weights = params_G\n","\n","        if params_D is None:\n","            # MLP structure\n","            self.hiddenDim_D = 150\n","            self.hiddenLayer_D = 3\n","            # optimize params\n","            self.reg_D = 1e-05\n","            self.lr_D = 0.0001\n","            self.opt_D = 'adam'\n","            self.step_D = 1\n","            self.batchSize_D = 64\n","        else:\n","            self.hiddenDim_D, self.hiddenLayer_D, \\\n","            self.reg_D, self.lr_D, self.opt_D, self.step_D, self.batchSize_D = params_D\n","        #\n","        self.log_dir = '_'.join(\n","            list(map(str, self.G_loss_weights + [self.step_G, self.step_D, self.ZR_ratio, str(target_id)])))\n","\n","    def train_gan(self):\n","        for epoch in range(self.totalEpochs):\n","            self.epoch = epoch\n","            with open(self.log_path, \"a+\") as fout:\n","                fout.write(\"epoch:\" + str(epoch) + \"\\n\")\n","                fout.flush()\n","\n","            for epoch_D in range(self.step_D):\n","                self.epoch_D = epoch_D\n","                loss_D, a, b = self.train_D()\n","                print('D', epoch_D, ':', round(loss_D, 5), a, end=\"\")\n","                print(b[0])\n","                with open(self.log_path, \"a+\") as fout:\n","                    log_tmp = 'D' + str(epoch_D) + ':' + str(round(loss_D, 5)) + str(a) + str(b[0])\n","                    fout.write(log_tmp + \"\\n\")\n","                    fout.flush()\n","\n","            for epoch_G in range(self.step_G):\n","                self.epoch_G = epoch_G\n","                loss_G, loss_G_array, g_out_seed, log_info = self.train_G()\n","                with open(self.log_path, \"a+\") as fout:\n","                    log_tmp = 'G' + str(epoch_G) + ':' + str(round(loss_G, 5)) \\\n","                              + str(loss_G_array) + str(g_out_seed) + str(log_info)\n","                    fout.write(log_tmp + \"\\n\")\n","                    fout.flush()\n","                print('G', epoch_G, ':', round(loss_G, 5), loss_G_array, g_out_seed, log_info)\n","\n","    def execute(self, is_train, model_path, final_attack_setting):\n","        self.log_path = 'logs/' + self.log_dir + '/' + \"training_log.log\"\n","\n","        with tf.Graph().as_default():\n","            self._data_preparation()\n","            self._build_graph()\n","            self.sess = tf.Session()\n","            self.sess.run(tf.global_variables_initializer())\n","            # 训练或恢复模型\n","            if is_train == 0:\n","                if model_path != 'no':\n","                    self.restore(model_path)\n","            else:\n","                self.wirter = tf.summary.FileWriter('logs/' + self.log_dir + '/', self.sess.graph)\n","                self.train_gan()\n","                self.save(model_path)\n","            # 生成攻击文件\n","            fake_profiles, real_profiles_, filler_indicator_ \\\n","                = self.fake_profiles_generator(final_attack_setting)\n","            return fake_profiles, real_profiles_, filler_indicator_\n","\n","    def fake_profiles_generator(self, final_attack_setting):\n","        fake_num, real_vector, filler_indicator = final_attack_setting\n","\n","        # input filler\n","        if real_vector is None or filler_indicator is None:\n","            batchList = self.batchList.copy()\n","            while fake_num > len(batchList):\n","                batchList += batchList\n","            random.shuffle(batchList)\n","            sampled_index = batchList[:fake_num]\n","            real_vector = self.dataset_class.train_matrix[sampled_index].toarray()\n","            filler_indicator = self.filler_sampler(sampled_index)\n","\n","        # output fake profiles\n","        fake_profiles = self.sess.run(self.fakeData, feed_dict={self.G_input: real_vector,\n","                                                                self.filler_dims: filler_indicator})\n","        return fake_profiles, real_vector, filler_indicator\n","        # if return_real_filler == 0:\n","        #     return fake_profiles\n","        # else:\n","        #     # batchList = self.batchList.copy()\n","        #     # while fake_num > len(batchList):\n","        #     #     batchList += batchList\n","        #     # random.shuffle(batchList)\n","        #     # sampled_index = batchList[:fake_num]\n","        #     # # real_profiles = self.train_matrix[sampled_index]\n","        #     # real_profiles = self.dataset_class.train_matrix[sampled_index].toarray()\n","        #     # filler_indicator = np.array(self.filler_sampler(sampled_index))\n","        #     # for idx in range(filler_indicator.shape[0]):\n","        #     #     filler_indicator[idx][self.selected_id_list + [self.target_id]] = 1\n","        #     # return fake_profiles, real_profiles * filler_indicator\n","        #     return fake_profiles, real_vector, filler_indicator\n","\n","    def _build_graph(self):\n","        self.filler_dims = tf.placeholder(tf.float32, [None, self.num_item])  # filler = 1, otherwise 0\n","        self.selected_dims = tf.squeeze(\n","            tf.reduce_sum(tf.one_hot([self.selected_id_list], self.num_item, dtype=tf.float32), 1))\n","\n","        self.models = GAN_Attacker()\n","        # G\n","        with tf.name_scope(\"Generator\"):\n","            self.G_input = tf.placeholder(tf.float32, [None, self.num_item], name=\"G_input\")\n","            self.rating_matrix_mask = tf.placeholder(tf.float32, [None, self.num_item])  # rated = 1, otherwise 0\n","            self.G_output, self.G_L2norm = self.models.GEN(self.G_input * self.filler_dims, self.num_item,\n","                                                           self.hiddenDim_G, self.selected_num, 'sigmoid',\n","                                                           decay=self.decay_g, name=\"gen\")\n","\n","        with tf.name_scope(\"Fake_Data\"):\n","            selected_patch = None\n","            for i in range(self.selected_num):\n","                one_hot = tf.one_hot(self.selected_id_list[i], self.num_item, dtype=tf.float32)\n","                mask = tf.boolean_mask(self.G_output, tf.one_hot(i, self.selected_num, dtype=tf.int32), axis=1)\n","                if i == 0:\n","                    selected_patch = one_hot * mask\n","                else:\n","                    selected_patch += one_hot * mask\n","            self.fakeData = selected_patch + self.target_patch + self.G_input * self.filler_dims\n","        # D\n","        with tf.name_scope(\"Discriminator\"):\n","            self.realData_ = tf.placeholder(tf.float32, shape=[None, self.num_item], name=\"real_data\")\n","            self.filler_dims_D = tf.placeholder(tf.float32, [None, self.num_item])  # filler = 1, otherwise 0\n","            self.realData = self.realData_ * (self.filler_dims_D + self.selected_dims)\n","\n","            self.D_real = self.models.DIS(self.realData * self.target_mask, self.num_item * 1, self.hiddenDim_D,\n","                                          'sigmoid', self.hiddenLayer_D)\n","\n","            self.D_fake = self.models.DIS(self.fakeData * self.target_mask, self.num_item * 1, self.hiddenDim_D,\n","                                          'sigmoid', self.hiddenLayer_D, _reuse=True)\n","\n","        self.g_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope='gen')\n","        self.d_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope='dis')\n","\n","        # define loss & optimizer for G\n","        with tf.name_scope(\"loss_G\"):\n","            self.g_loss_gan = tf.reduce_mean(\n","                tf.nn.sigmoid_cross_entropy_with_logits(logits=self.D_fake, labels=tf.ones_like(self.D_fake)))\n","            self.g_loss_seed = tf.reduce_mean(\n","                tf.reduce_mean(tf.square(self.G_output - 5.0), 1, keepdims=True))\n","            self.g_loss_reconstruct_seed = tf.reduce_mean(\n","                tf.reduce_sum(tf.square(self.fakeData - self.G_input) * self.rating_matrix_mask * self.selected_dims,\n","                              1, keepdims=True))\n","            self.g_loss_l2 = self.reg_G * self.G_L2norm\n","\n","            self.g_loss_list = [self.g_loss_gan, self.g_loss_seed,\n","                                self.g_loss_reconstruct_seed, self.g_loss_l2]\n","            self.g_loss = sum(self.g_loss_list[i] * self.G_loss_weights[i] for i in range(len(self.G_loss_weights)))\n","\n","        # tensorboard summary\n","        self.add_loss_summary(type='G')\n","\n","        with tf.name_scope(\"optimizer_G\"):\n","            if self.opt_G == 'sgd':\n","                self.trainer_G = tf.train.GradientDescentOptimizer(self.lr_G).minimize(self.g_loss,\n","                                                                                       var_list=self.g_vars,\n","                                                                                       name=\"GradientDescent_G\")\n","            elif self.opt_G == 'adam':\n","                self.trainer_G = tf.train.AdamOptimizer(self.lr_G).minimize(self.g_loss, var_list=self.g_vars,\n","                                                                            name=\"Adam_G\")\n","\n","        # define loss & optimizer for D\n","\n","        with tf.name_scope(\"loss_D\"):\n","            d_loss_real = tf.reduce_mean(\n","                tf.nn.sigmoid_cross_entropy_with_logits(logits=self.D_real, labels=tf.ones_like(self.D_real)),\n","                name=\"loss_real\")\n","            d_loss_fake = tf.reduce_mean(\n","                tf.nn.sigmoid_cross_entropy_with_logits(logits=self.D_fake, labels=tf.zeros_like(self.D_fake)),\n","                name=\"loss_fake\")\n","            D_L2norm = 0\n","            for pr in self.d_vars:\n","                D_L2norm += tf.nn.l2_loss(pr)\n","            self.d_loss = d_loss_real + d_loss_fake + self.reg_D * D_L2norm\n","            self.d_loss_real, self.d_loss_fake, self.D_L2norm = d_loss_real, d_loss_fake, D_L2norm\n","        with tf.name_scope(\"optimizer_D\"):\n","            if self.opt_D == 'sgd':\n","                self.trainer_D = tf.train.GradientDescentOptimizer(self.lr_D).minimize(self.d_loss,\n","                                                                                       var_list=self.d_vars,\n","                                                                                       name=\"GradientDescent_D\")\n","            elif self.opt_D == 'adam':\n","                self.trainer_D = tf.train.AdamOptimizer(self.lr_D).minimize(self.d_loss, var_list=self.d_vars,\n","                                                                            name=\"Adam_D\")\n","\n","    def _data_preparation(self):\n","        self.target_patch = tf.one_hot(self.target_id, self.num_item, dtype=tf.float32) * 5\n","        self.target_mask = 1 - tf.one_hot(self.target_id, self.num_item, dtype=tf.float32)\n","        # prepare train data\n","        # self.train_matrix = self.dataset_class.train_matrix.toarray().astype(np.float32)\n","        # self.train_mask = self.train_matrix.copy()\n","        # self.train_mask[self.train_mask > 0] = 1\n","        self.filler_candi_set = set(range(self.num_item)) - set(self.selected_id_list + [self.target_id])\n","        self.filler_candi_list = list(self.filler_candi_set)\n","\n","        # sample filler>filler num\n","        self.batchList = []\n","        for i in range(self.num_user):\n","            set_rated = set(self.dataset_class.train_matrix[i].toarray()[0].nonzero()[0])\n","            # set_rated = set(self.train_mask[i].nonzero()[0])\n","            if len(self.filler_candi_set & set_rated) < self.filler_num: continue\n","            self.batchList.append(i)\n","\n","        # 没有在train set对target item评分的用户，用来算all user的pred shift\n","        self.non_rated_users = self.dataset_class.get_item_nonrated_users(self.target_id)\n","        # item pop/avg\n","        self.item_pop = np.array(self.dataset_class.get_item_pop())\n","        _, _, self.item_avg, _ = self.dataset_class.get_all_mean_std()\n","        self.item_avg = np.array(self.item_avg)\n","\n","        # big cap\n","        if self.filler_method == 3:\n","            print(\"\\n==\\n==\\n修改路径！！\\n==\\n\")\n","            attack_info_path = [\"../data/data/filmTrust_selected_items\", \"../data/data/filmTrust_selected_items\"]\n","            attack_info = load_attack_info(*attack_info_path)\n","            target_users = attack_info[self.target_id][1]\n","            uid_values = self.dataset_class.train_data.user_id.values\n","            idxs = [idx for idx in range(len(uid_values)) if uid_values[idx] in target_users]\n","            iid_values = self.dataset_class.train_data.loc[idxs, 'item_id']\n","            iid_values = iid_values.tolist()\n","            from collections import Counter\n","            iid_values = Counter(iid_values)\n","            self.item_big_cap = np.array([iid_values.get(iid, 0.5) for iid in range(self.num_item)])\n","\n","    def train_G(self):\n","        t1 = time.time()\n","        random.seed(int(t1))\n","        random.shuffle(self.batchList)\n","        #\n","        batch_real_vector = None\n","        batch_run_res = None\n","        #\n","        total_loss_g = 0\n","        total_loss_array = np.array([0., 0., 0., 0.])\n","        total_batch = int(len(self.batchList) / self.batchSize_G) + 1\n","        for batch_id in range(total_batch):\n","            if batch_id == total_batch - 1:\n","                batch_index = self.batchList[batch_id * self.batchSize_G:]\n","            else:\n","                batch_index = self.batchList[batch_id * self.batchSize_G: (batch_id + 1) * self.batchSize_G]\n","\n","            batch_size = len(batch_index)\n","            # batch_real_vector = self.train_matrix[batch_index]\n","            batch_real_vector = self.dataset_class.train_matrix[batch_index].toarray()\n","            # batch_mask = self.train_mask[batch_index]\n","            batch_mask = batch_real_vector.copy()\n","            batch_mask[batch_mask > 0] = 1\n","\n","            # sample zero for zero reconstruction\n","            batch_mask_ZR = batch_mask.copy()\n","            if self.ZR_ratio > 0:\n","                for idx in range(batch_size):\n","                    batch_mask_ZR[idx][self.selected_id_list] = \\\n","                        [1 if i == 1 or random.random() < self.ZR_ratio else 0 for i in\n","                         batch_mask_ZR[idx][self.selected_id_list]]\n","\n","            # sample fillers randomly\n","            batch_filler_indicator = self.filler_sampler(batch_index)\n","\n","            batch_run_res = self.sess.run(\n","                [self.trainer_G, self.g_loss] + self.g_loss_list + [self.G_output, self.G_loss_merged],\n","                feed_dict={self.G_input: batch_real_vector,\n","                           self.filler_dims: batch_filler_indicator,\n","                           self.rating_matrix_mask: batch_mask_ZR})  # Update G\n","\n","            total_loss_g += batch_run_res[1]\n","            total_loss_array += np.array(batch_run_res[2:2 + len(total_loss_array)])\n","\n","        self.wirter.add_summary(batch_run_res[-1], self.step_G * self.epoch + self.epoch_G + 1)\n","        total_loss_array = [round(i, 2) for i in total_loss_array]\n","        g_out_seed = [round(i, 2) for i in np.mean(batch_run_res[-2], 0)]\n","        #\n","        fn_float_to_str = lambda x: str(round(x, 2))\n","        r = batch_real_vector.transpose()[self.selected_id_list].transpose()\n","        g = batch_run_res[-2]\n","        rmse = list(map(fn_float_to_str, np.sum(np.square(np.abs(r - g)), 0)))\n","        var_col = list(map(fn_float_to_str, np.var(g, 0)))\n","        self.add_loss_summary(type=\"var\", info=np.var(g, 0))\n","        var_row = round(np.mean(np.var(g, 1)), 2)\n","        # var_col_ori = list(map(fn_float_to_str, np.var(r, 0)))\n","        # var_row_ori = round(np.mean(np.var(r, 1)), 2)\n","        log_info = \"rmse : \" + ','.join(rmse)\n","        log_info += \"\\tvar_col : \" + ','.join(var_col) + \"\\tvar_row : \" + str(var_row)\n","        # log_info += \"\\tvar_col_ori : \" + ','.join(var_col_ori) + \"\\tvar_row_ori : \" + str(var_row_ori)\n","        return total_loss_g, total_loss_array, g_out_seed, log_info  # [g_out_seed, mae, [var_col, var_row]]\n","\n","    def train_D(self):\n","        \"\"\"\n","        每个epoch各产生self.batchSize_D个realData和fakeData\n","        \"\"\"\n","        t1 = time.time()\n","        random.seed(int(t1))\n","        random.shuffle(self.batchList)\n","\n","        total_loss_d, total_loss_d_real, total_loss_d_fake = 0, 0, 0\n","        #\n","        batch_filler_indicator = None\n","\n","        total_batch = int(len(self.batchList) / self.batchSize_D) + 1\n","        for batch_id in range(total_batch):\n","            # prepare data\n","            if batch_id == total_batch - 1:\n","                batch_index = self.batchList[batch_id * self.batchSize_D:]\n","            else:\n","                batch_index = self.batchList[batch_id * self.batchSize_D: (batch_id + 1) * self.batchSize_D]\n","            batch_size = len(batch_index)\n","            batch_real_vector = self.dataset_class.train_matrix[batch_index].toarray()\n","            # batch_real_vector = self.train_matrix[batch_index]\n","            batch_filler_indicator = self.filler_sampler(batch_index)\n","\n","            # optimize\n","            _, total_loss_d_, total_loss_d_real_, total_loss_d_fake_ \\\n","                = self.sess.run([self.trainer_D, self.d_loss, self.d_loss_real, self.d_loss_fake],\n","                                feed_dict={self.realData_: batch_real_vector,\n","                                           self.G_input: batch_real_vector,\n","                                           self.filler_dims: batch_filler_indicator,\n","                                           self.filler_dims_D: batch_filler_indicator})  # Update D\n","            total_loss_d += total_loss_d_\n","            total_loss_d_real += total_loss_d_real_\n","            total_loss_d_fake += total_loss_d_fake_\n","        self.add_loss_summary(type=\"D\", info=[total_loss_d, total_loss_d_real, total_loss_d_fake])\n","        debug_info = [self.G_output, self.fakeData,\n","                      tf.squeeze(tf.nn.sigmoid(self.D_real)), tf.squeeze(tf.nn.sigmoid(self.D_fake))]\n","        info = self.sess.run(debug_info, feed_dict={self.realData_: batch_real_vector,\n","                                                    self.G_input: batch_real_vector,\n","                                                    self.filler_dims: batch_filler_indicator,\n","                                                    self.filler_dims_D: batch_filler_indicator})\n","\n","        D_real, D_fake = info[2:4]\n","        fake_data = info[1]\n","        # lower bound\n","        lower_bound = []\n","        for v in fake_data:\n","            t = v.copy()\n","            t[[self.target_id]] = 0.0  # 对判别器mask掉了target信息\n","            t[self.selected_id_list] = 5.0\n","            lower_bound.append(t)\n","        # upper bound\n","        upper_bound = []\n","        i = 0\n","        for v in fake_data:\n","            t = v.copy()\n","            t[self.selected_id_list] = batch_real_vector[i][self.selected_id_list]\n","            t[[self.target_id]] = 0.0  # 对判别器mask掉了target信息\n","            upper_bound.append(t)\n","            i += 1\n","        zero_data = []  # fake_data.copy()\n","        for v in fake_data:\n","            t = v.copy()\n","            t[[self.target_id]] = 0.0  # 对判别器mask掉了target信息\n","            t[self.selected_id_list] = 0.0\n","            zero_data.append(t)\n","        random_data = []\n","        for v in fake_data:\n","            t = v.copy()\n","            t[self.selected_id_list] = np.random.choice(list([1., 2., 3., 4., 5.]), size=self.selected_num,\n","                                                        replace=True)\n","            t[[self.target_id]] = 0.0  # 对判别器mask掉了target信息\n","            random_data.append(t)\n","\n","        D_lower_bound = self.sess.run(tf.squeeze(tf.nn.sigmoid(self.D_real)),\n","                                      feed_dict={self.realData_: lower_bound,\n","                                                 self.filler_dims_D: batch_filler_indicator})\n","        D_upper_bound = self.sess.run(tf.squeeze(tf.nn.sigmoid(self.D_real)),\n","                                      feed_dict={self.realData_: upper_bound,\n","                                                 self.filler_dims_D: batch_filler_indicator})\n","\n","        D_zero = self.sess.run(tf.squeeze(tf.nn.sigmoid(self.D_real)),\n","                               feed_dict={self.realData_: zero_data, self.filler_dims_D: batch_filler_indicator})\n","        D_random = self.sess.run(tf.squeeze(tf.nn.sigmoid(self.D_real)),\n","                                 feed_dict={self.realData_: random_data, self.filler_dims_D: batch_filler_indicator})\n","        # filler=1通常会更假\n","\n","        d_info = [round(np.mean(D_real), 2), round(np.mean(D_fake), 2),\n","                  [round(np.mean(D_lower_bound), 2), round(np.mean(D_upper_bound), 2)],\n","                  round(np.mean(D_zero), 2), round(np.mean(D_random), 2)]\n","        # s = [\"T:\", \"G:\", \"s=5:\", \"s=0:\", \"s=random:\", \"s=5,f=1:\"]\n","        # s = [\"real:\", \"fake:\", \"seed=5:\", \"seed=0:\", \"seed=random:\", \"seed=5,filler=1:\"]\n","        # d_info = ' '.join([str(d_info[i]) for i in range(len(d_info))])  # s[i]+ str(d_info[i])\n","\n","        #\n","        fn_float_to_str = lambda x: str(round(x, 2))\n","        g_out_seed = list(map(fn_float_to_str, np.mean(info[0], 0)))  # [round(i, 2) for i in np.mean(info[0], 0)]\n","\n","        #\n","\n","        g = info[0]\n","        var_col = list(map(fn_float_to_str, np.var(g, 0)))\n","        var_row = round(np.mean(np.var(g, 1)), 2)\n","        log_info = \"\\tg_out_seed:\" + ','.join(g_out_seed), \"\\tvar_col : \" + ','.join(var_col) + \"\\tvar_row : \" + str(\n","            var_row)\n","\n","        return total_loss_d, d_info, log_info\n","\n","    def filler_sampler(self, uid_list):\n","        if self.filler_method == 0:\n","            batch_filler_indicator = []\n","            for uid in uid_list:\n","                # filler_candi = np.array(\n","                #     list(set(self.filler_candi_list) & set(self.train_mask[uid].nonzero()[0].tolist())))\n","                filler_candi = np.array(list(set(self.filler_candi_list)\n","                                             & set(self.dataset_class.train_matrix[uid].toarray()[0].nonzero()[0])))\n","                #\n","                if len(filler_candi) > self.filler_num:\n","                    filler_candi = np.random.choice(filler_candi, size=self.filler_num, replace=False)\n","                filler_indicator = [1 if iid in filler_candi else 0 for iid in range(self.num_item)]\n","                batch_filler_indicator.append(filler_indicator)\n","            return batch_filler_indicator\n","        else:\n","            return self.filler_sampler_method(uid_list)\n","\n","    def filler_sampler_method(self, uid_list):\n","        batch_filler_indicator = []\n","        for uid in uid_list:\n","            # filler_candi = np.array(\n","            #     list(set(self.filler_candi_list) & set(self.train_mask[uid].nonzero()[0].tolist())))\n","            filler_candi = np.array(list(set(self.filler_candi_list)\n","                                         & set(self.dataset_class.train_matrix[uid].toarray()[0].nonzero()[0])))\n","\n","            if len(filler_candi) > self.filler_num:\n","                # sample using a specific method\n","                # -------------------------\n","                prob = self.item_avg[filler_candi] if self.filler_method == 1 \\\n","                    else self.item_pop[filler_candi] if self.filler_method == 2 \\\n","                    else self.item_big_cap[filler_candi] if self.filler_method == 3 \\\n","                    else None\n","                prob = None if prob is None else prob / sum(prob)\n","                # -------------------------\n","\n","                filler_candi = np.random.choice(filler_candi, size=self.filler_num, replace=False, p=prob)\n","            filler_indicator = [1 if iid in filler_candi else 0 for iid in range(self.num_item)]\n","            batch_filler_indicator.append(filler_indicator)\n","        return batch_filler_indicator\n","\n","    def save(self, path):\n","        saver = tf.train.Saver()\n","        saver.save(self.sess, path)\n","\n","    def restore(self, path):\n","        saver = tf.train.Saver()\n","        saver.restore(self.sess, path)\n","\n","    def add_loss_summary(self, type=\"G\", info=None):\n","        # , total_loss_g, total_g_loss_gan, total_g_loss_seed, total_g_loss_reconstruct,total_g_loss_l2):\n","        if type == \"G\":\n","            tf.summary.scalar('Generator/adversarial', self.g_loss_gan)\n","            tf.summary.scalar('Generator/seed', self.g_loss_seed)\n","            tf.summary.scalar('Generator/selected_reconstruct', self.g_loss_reconstruct_seed)\n","            tf.summary.scalar('Generator/l2_normal', self.g_loss_l2)\n","            tf.summary.scalar('Generator/Sum', self.g_loss)\n","            self.G_loss_merged = tf.summary.merge_all()\n","\n","        elif type == 'D':\n","            total_loss_d, total_loss_d_real, total_loss_d_fake = info\n","            loss_summary = []\n","            tag_list = ['Discriminator/Sum', 'Discriminator/real', 'Discriminator/fake']\n","            simple_value_list = [total_loss_d, total_loss_d_real, total_loss_d_fake]\n","            for i in range(3):\n","                loss_summary.append(tf.Summary.Value(tag=tag_list[i], simple_value=simple_value_list[i]))\n","            self.wirter.add_summary(tf.Summary(value=loss_summary), self.epoch * self.step_D + self.epoch_D + 1)\n","        elif type == 'var':\n","            var_summary = []\n","            for i in range(self.selected_num):\n","                var_summary.append(tf.Summary.Value(tag='Var/' + str(i), simple_value=info[i]))\n","            self.wirter.add_summary(tf.Summary(value=var_summary), self.step_G * self.epoch + self.epoch_G + 1)\n","        else:\n","            print(\"summary type error\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"cellView":"form","id":"hqVzuSKciHPR"},"source":["#@markdown WGAN\n","class batch_norm(object):\n","    def __init__(self, epsilon=1e-5, momentum=0.9, name=\"batch_norm\"):\n","        with tf.variable_scope(name):\n","            self.epsilon = epsilon\n","            self.momentum = momentum\n","            self.name = name\n","\n","    def __call__(self, x, train=True):\n","        return tf.contrib.layers.batch_norm(x,\n","                                            decay=self.momentum,\n","                                            updates_collections=None,\n","                                            epsilon=self.epsilon,\n","                                            scale=True,\n","                                            is_training=train,\n","                                            scope=self.name)\n","\n","\n","def conv_cond_concat(x, y):\n","    \"\"\"Concatenate conditioning vector on feature map axis.\"\"\"\n","    x_shapes = x.get_shape()\n","    y_shapes = y.get_shape()\n","    return concat([\n","        x, y * tf.ones([x_shapes[0], x_shapes[1], x_shapes[2], y_shapes[3]])], 3)\n","\n","\n","def conv2d(input_, output_dim,\n","           k_h=5, k_w=5, d_h=2, d_w=2, stddev=0.02,\n","           name=\"conv2d\"):\n","    with tf.variable_scope(name):\n","        w = tf.get_variable('w', [k_h, k_w, input_.get_shape()[-1], output_dim],\n","                            initializer=tf.truncated_normal_initializer(stddev=stddev))\n","        conv = tf.nn.conv2d(input_, w, strides=[1, d_h, d_w, 1], padding='SAME')\n","\n","        biases = tf.get_variable('biases', [output_dim], initializer=tf.constant_initializer(0.0))\n","        conv = tf.reshape(tf.nn.bias_add(conv, biases), conv.get_shape())\n","\n","        return conv\n","\n","\n","# kernel_size = 5 * 5\n","def deconv2d(input_, output_shape,\n","             k_h=5, k_w=5, d_h=2, d_w=2, stddev=0.02,\n","             name=\"deconv2d\", with_w=False):\n","    with tf.variable_scope(name):\n","        # filter : [height, width, output_channels, in_channels]\n","        w = tf.get_variable('w', [k_h, k_w, output_shape[-1], input_.get_shape()[-1]],\n","                            initializer=tf.random_normal_initializer(stddev=stddev))\n","\n","        try:\n","            deconv = tf.nn.conv2d_transpose(input_, w, output_shape=output_shape,\n","                                            strides=[1, d_h, d_w, 1])\n","\n","        # Support for verisons of TensorFlow before 0.7.0\n","        except AttributeError:\n","            deconv = tf.nn.deconv2d(input_, w, output_shape=output_shape,\n","                                    strides=[1, d_h, d_w, 1])\n","\n","        biases = tf.get_variable('biases', [output_shape[-1]], initializer=tf.constant_initializer(0.0))\n","        deconv = tf.reshape(tf.nn.bias_add(deconv, biases), deconv.get_shape())\n","\n","        if with_w:\n","            return deconv, w, biases\n","        else:\n","            return deconv\n","\n","\n","def lrelu(x, leak=0.2, name=\"lrelu\"):\n","    return tf.maximum(x, leak * x)\n","\n","\n","def linear(input_, output_size, scope=None, stddev=0.02, bias_start=0.0, with_w=False):\n","    shape = input_.get_shape().as_list()\n","\n","    with tf.variable_scope(scope or \"Linear\"):\n","        try:\n","            matrix = tf.get_variable(\"Matrix\", [shape[1], output_size], tf.float32,\n","                                     tf.random_normal_initializer(stddev=stddev))\n","        except ValueError as err:\n","            msg = \"NOTE: Usually, this is due to an issue with the image dimensions.  Did you correctly set '--crop' or '--input_height' or '--output_height'?\"\n","            err.args = err.args + (msg,)\n","            raise\n","        bias = tf.get_variable(\"bias\", [output_size],\n","                               initializer=tf.constant_initializer(bias_start))\n","        if with_w:\n","            return tf.matmul(input_, matrix) + bias, matrix, bias\n","        else:\n","            return tf.matmul(input_, matrix) + bias\n","\n","\n","def conv_out_size_same(size, stride):\n","    return int(math.ceil(float(size) / float(stride)))\n","\n","\n","def gen_random(size):\n","    # z - N(0,100)\n","    return np.random.normal(0, 100, size=size)\n","\n","\n","class WGAN(object):\n","    def __init__(self, sess, dataset_class,batch_size=64, height=29, width=58, z_dim=100, gf_dim=64, df_dim=64,\n","                 gfc_dim=1024, dfc_dim=1024, max_to_keep=1):\n","        self.sess = sess\n","        self.dataset_class = dataset_class\n","        self.batch_size = batch_size\n","\n","        self.height = height\n","        self.width = width\n","        self.z_dim = z_dim\n","        self.gf_dim = gf_dim\n","        self.df_dim = df_dim\n","        self.gfc_dim = gfc_dim\n","        self.dfc_dim = dfc_dim\n","        # batch normalization : deals with poor initialization helps gradient flow\n","        self.d_bn1 = batch_norm(name='d_bn1')\n","        self.d_bn2 = batch_norm(name='d_bn2')\n","        self.d_bn3 = batch_norm(name='d_bn3')\n","        self.g_bn0 = batch_norm(name='g_bn0')\n","        self.g_bn1 = batch_norm(name='g_bn1')\n","        self.g_bn2 = batch_norm(name='g_bn2')\n","        self.g_bn3 = batch_norm(name='g_bn3')\n","\n","        self.max_to_keep = max_to_keep\n","\n","        self.build_model()\n","\n","    def build_model(self):\n","        self.inputs = tf.placeholder(tf.float32,\n","                                     [self.batch_size, self.height, self.width, 1],\n","                                     name='real_images')\n","        inputs = self.inputs\n","        # 生成器\n","        self.z = tf.placeholder(tf.float32, [None, self.z_dim], name='z')\n","        self.G = self.generator(self.z)\n","        # 判别器 - real&fake\n","        self.D, self.D_logits = self.discriminator(inputs, reuse=False)\n","        self.D_, self.D_logits_ = self.discriminator(self.G, reuse=True)\n","\n","        # def _cross_entropy_loss(self, logits, labels):\n","        #     xentropy = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits, labels))\n","        #     return xentropy\n","        self.d_loss = tf.reduce_mean(tf.square(self.D_logits - self.D_logits_))\n","        self.g_loss = tf.reduce_mean(tf.square(self.D_logits_))\n","        # self.d_loss_real = tf.reduce_mean(\n","        #     _cross_entropy_loss(self.D_logits, tf.ones_like(self.D)))\n","        # self.d_loss_fake = tf.reduce_mean(\n","        #     _cross_entropy_loss(self.D_logits_, tf.zeros_like(self.D_)))\n","        #\n","        # self.g_loss = tf.reduce_mean(\n","        #     _cross_entropy_loss(self.D_logits_, tf.ones_like(self.D_)))\n","        # self.d_loss = self.d_loss_real + self.d_loss_fake\n","        #\n","        t_vars = tf.trainable_variables()\n","        self.d_vars = [var for var in t_vars if 'd_' in var.name]\n","        self.g_vars = [var for var in t_vars if 'g_' in var.name]\n","\n","        self.saver = tf.train.Saver(max_to_keep=self.max_to_keep)\n","\n","    def train(self, config):\n","        d_optim = tf.train.RMSPropOptimizer(config.learning_rate, decay=config.beta1) \\\n","            .minimize(self.d_loss, var_list=self.d_vars)\n","        g_optim =tf.train.RMSPropOptimizer(config.learning_rate, decay=config.beta1) \\\n","            .minimize(self.g_loss, var_list=self.g_vars)\n","        try:\n","            tf.global_variables_initializer().run()\n","        except:\n","            tf.initialize_all_variables().run()\n","        train_idxs = list(range(self.dataset_class.train_matrix.shape[0]))\n","        for epoch in xrange(config.epoch):\n","            np.random.shuffle(train_idxs)\n","            for i in range(len(train_idxs) // self.batch_size):\n","                cur_idxs = train_idxs[i * self.batch_size:(i + 1) * self.batch_size]\n","                batch_inputs = self.dataset_class.train_matrix[cur_idxs].toarray()\n","                # transform range&shape\n","                batch_inputs = (batch_inputs - 2.5) / 2.5\n","                batch_inputs = np.reshape(batch_inputs, [self.batch_size, self.height, self.width, 1])\n","                # batch_inputs = np.random.random_sample([self.batch_size, self.height, self.width, 1])\n","                batch_z = gen_random(size=[config.batch_size, self.z_dim]).astype(np.float32)\n","\n","                # Update D network\n","                _ = self.sess.run(d_optim, feed_dict={self.inputs: batch_inputs, self.z: batch_z})\n","\n","                # Update G network\n","                _ = self.sess.run(g_optim, feed_dict={self.z: batch_z})\n","\n","                # Run g_optim twice to make sure that d_loss does not go to zero (different from paper)\n","\n","                errD= self.d_loss.eval({self.inputs: batch_inputs,self.z: batch_z})\n","                # errD_real = self.d_loss_real.eval({self.inputs: batch_inputs})\n","                errG = self.g_loss.eval({self.z: batch_z})\n","\n","                print(\"Epoch:[%2d/%2d]d_loss: %.8f, g_loss: %.8f\" \\\n","                      % (epoch, config.epoch, errD, errG))\n","\n","    def discriminator(self, image, reuse=False):\n","        with tf.variable_scope(\"discriminator\") as scope:\n","            if reuse:\n","                scope.reuse_variables()\n","            # 论文中给的判别器结构:[conv+BN+LeakyRelu[64,128,256,512]]+[FC]+[sigmoid]\n","            h0 = lrelu(conv2d(image, self.df_dim, name='d_h0_conv'))\n","            h1 = lrelu(self.d_bn1(conv2d(h0, self.df_dim * 2, name='d_h1_conv')))\n","            h2 = lrelu(self.d_bn2(conv2d(h1, self.df_dim * 4, name='d_h2_conv')))\n","            h3 = lrelu(self.d_bn3(conv2d(h2, self.df_dim * 8, name='d_h3_conv')))\n","            h4 = linear(tf.reshape(h3, [self.batch_size, -1]), 1, 'd_h4_lin')\n","\n","            return tf.nn.sigmoid(h4), h4\n","\n","    def generator(self, z):\n","        with tf.variable_scope(\"generator\") as scope:\n","            s_h, s_w = self.height, self.width\n","            # CONV stride=2\n","            s_h2, s_w2 = conv_out_size_same(s_h, 2), conv_out_size_same(s_w, 2)\n","            s_h4, s_w4 = conv_out_size_same(s_h2, 2), conv_out_size_same(s_w2, 2)\n","            s_h8, s_w8 = conv_out_size_same(s_h4, 2), conv_out_size_same(s_w4, 2)\n","            s_h16, s_w16 = conv_out_size_same(s_h8, 2), conv_out_size_same(s_w8, 2)\n","\n","            # FC of 2*4*512&ReLU&BN\n","            self.z_, self.h0_w, self.h0_b = linear(\n","                z, self.gf_dim * 8 * s_h16 * s_w16, 'g_h0_lin', with_w=True)\n","            self.h0 = tf.reshape(\n","                self.z_, [-1, s_h16, s_w16, self.gf_dim * 8])\n","            h0 = tf.nn.relu(self.g_bn0(self.h0))\n","\n","            # four transposed CONV of [256,128,64] &ReLU&BN&kernel_size = 5 * 5\n","            self.h1, self.h1_w, self.h1_b = deconv2d(\n","                h0, [self.batch_size, s_h8, s_w8, self.gf_dim * 4], name='g_h1', with_w=True)\n","            h1 = tf.nn.relu(self.g_bn1(self.h1))\n","            h2, self.h2_w, self.h2_b = deconv2d(\n","                h1, [self.batch_size, s_h4, s_w4, self.gf_dim * 2], name='g_h2', with_w=True)\n","            h2 = tf.nn.relu(self.g_bn2(h2))\n","            h3, self.h3_w, self.h3_b = deconv2d(\n","                h2, [self.batch_size, s_h2, s_w2, self.gf_dim * 1], name='g_h3', with_w=True)\n","            h3 = tf.nn.relu(self.g_bn3(h3))\n","\n","            # transposed CONV of [1] &tanh\n","            h4, self.h4_w, self.h4_b = deconv2d(\n","                h3, [self.batch_size, s_h, s_w, 1], name='g_h4', with_w=True)\n","\n","            return tf.nn.tanh(h4)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"CsioaIWhjGKr"},"source":["def train_rec(data_set_name, model_name, attack_method, target_id, is_train):\n","    if attack_method == \"no\":\n","        attack_method = \"\"\n","        model_path = \"../result/model_ckpt/\" + '_'.join([model_name, data_set_name]) + \".ckpt\"\n","    else:\n","        model_path = \"../result/model_ckpt/\" + '_'.join([model_name, data_set_name, attack_method]) + \".ckpt\"\n","    path_train = \"../data/data_attacked/\" + '_'.join([data_set_name, str(target_id), attack_method]) + \".dat\"\n","    path_test = \"../data/data/\" + data_set_name + \"_test.dat\"\n","    if attack_method == \"\": path_train = \"../data/data/\" + data_set_name + \"_train.dat\"\n","\n","    # load_data\n","    dataset_class = load_data(path_train=path_train, path_test=path_test,\n","                              header=['user_id', 'item_id', 'rating'],\n","                              sep='\\t', print_log=True)\n","    # train rec\n","    if model_name in [\"IAutoRec\", \"UAutoRec\", \"NNMF\"]:\n","        predictions, hit_ratios = rec_trainer(model_name, dataset_class, target_id, is_train, model_path)\n","    else:\n","        predictions, hit_ratios = basic_rec(model_name, path_train, path_test, target_id)\n","\n","    # write to file\n","    dst_path = \"../result/pred_result/\" + '_'.join([model_name, data_set_name, str(target_id), attack_method])\n","    dst_path = dst_path.strip('_')\n","    target_prediction_writer(predictions, hit_ratios, dst_path)\n","\n","\n","def parse_arg():\n","    parser = argparse.ArgumentParser()\n","    # 数据集名称，用来选择训练数据路径\n","    parser.add_argument('--dataset', type=str, default='automotive', help='input data_set_name,filmTrust or ml100k')\n","    # 要攻击的推荐模型的名称,其中NMF_25里的25指item/user的embedding size\n","    parser.add_argument('--model_name', type=str, default='NMF_25', help='NNMF,IAutoRec,UAutoRec,NMF_25')\n","    # 攻击方法\n","    parser.add_argument('--attack_method', type=str, default='G1',\n","                        help='no,gan,segment,average,random,bandwagon')\n","    # 目标item id\n","    # filmTrust:random = [5, 395, 181, 565, 254]    tail = [601, 623, 619, 64, 558]\n","    # ml100k:random = [62, 1077, 785, 1419, 1257]   tail = [1319, 1612, 1509, 1545, 1373]\n","    # 5,395,181,565,254,601,623,619,64,558\n","    # 62,1077,785,1419,1257,1319,1612,1509,1545,1373\n","    # 1166,1574,759,494,549,1272,1728,1662,450,1456,595,566,764,1187,1816,1478,1721,2294,2413,1148\n","    parser.add_argument('--target_ids', type=str, default='866',\n","                        help='attack target')\n","    # 参数 - 攻击数量，即往数据集里插入多少假用户\n","    parser.add_argument('--attack_num', type=int, default=50,\n","                        help='num of attack fake user,50 for ml100k and filmTrust')\n","    # 参数 - filler数量，可理解为是每个假用户有多少评分\n","    parser.add_argument('--filler_num', type=int, default=4,\n","                        help='num of filler items each fake user,90 for ml100k,36 for filmTrust')\n","\n","    args = parser.parse_args()\n","    args.target_ids = list(map(int, args.target_ids.split(',')))\n","    return args\n","\n","\n","if __name__ == '__main__':\n","    \"\"\"parse args\"\"\"\n","    args = parse_arg()\n","\n","    \"\"\"train\"\"\"\n","    if args.attack_method == 'no':\n","        attack_method_ = args.attack_method\n","    else:\n","        attack_method_ = '_'.join([args.attack_method, str(args.attack_num), str(args.filler_num)])\n","    is_train = 1\n","    train_rec(args.dataset, args.model_name, attack_method_, args.target_ids[0], is_train=is_train)\n","    for target in args.target_ids[1:]:\n","        if args.attack_method == 'no':\n","            is_train = 0\n","        train_rec(args.dataset, args.model_name, attack_method_, target, is_train=is_train)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fKcCIkdejGHB"},"source":["def gan_attack(data_set_name, attack_method, target_id, is_train, write_to_file=1, final_attack_setting=None):\n","    path_train = '../data/data/' + data_set_name + '_train.dat'\n","    path_test = '../data/data/' + data_set_name + '_test.dat'\n","    attack_info_path = [\"../data/data/\" + data_set_name + \"_selected_items\",\n","                        \"../data/data/\" + data_set_name + \"_target_users\"]\n","    # 读取seletced items和target users\n","    attack_info = load_attack_info(*attack_info_path)\n","    dataset_class = load_data(path_train=path_train, path_test=path_test, header=['user_id', 'item_id', 'rating'],\n","                              sep='\\t', print_log=True)\n","    # 攻击设置\n","    if len(attack_method.split('_')[1:]) == 2:\n","        attack_num, filler_num = map(int, attack_method.split('_')[1:])\n","        filler_method = 0\n","    else:\n","        attack_num, filler_num, filler_method = map(int, attack_method.split('_')[1:])\n","    # 0:重构 1:重构+seed\n","    loss_setting = int(attack_method.split('_')[0][-1])\n","    selected_items = attack_info[target_id][0]\n","    model_path = \"../result/model_ckpt/\" + '_'.join([data_set_name, attack_method, str(target_id)]) + \".ckpt\"\n","\n","    #\n","    gan_attacker = Train_G_Attacker(dataset_class, params_D=None, params_G=None, target_id=target_id,\n","                                    selected_id_list=selected_items,\n","                                    filler_num=filler_num, attack_num=attack_num, filler_method=filler_method,\n","                                    loss_setting=loss_setting)\n","    # if is_train:\n","    #     fake_profiles = gan_attacker.execute(is_train=True, model_path=model_path)\n","    # else:\n","    #     fake_profiles, real_profiles = gan_attacker.execute(is_train=False, model_path=model_path)\n","    #     if write_to_file == 0:\n","    #         return fake_profiles, real_profiles\n","    fake_profiles, real_profiles, filler_indicator = gan_attacker.execute(is_train=is_train, model_path=model_path,\n","                                                                          final_attack_setting=final_attack_setting)\n","    gan_attacker.sess.close()\n","    # \"\"\"inject and write to file\"\"\"\n","    if write_to_file == 1:\n","        dst_path = \"../data/data_attacked/\" + '_'.join([data_set_name, str(target_id), attack_method]) + \".dat\"\n","        attacked_file_writer(path_train, dst_path, fake_profiles, dataset_class.n_users)\n","    return fake_profiles, real_profiles, filler_indicator\n","\n","\n","def parse_arg():\n","    parser = argparse.ArgumentParser()\n","    # 数据集名称，用来选择训练数据路径\n","    parser.add_argument('--dataset', type=str, default='automotive', help='filmTrust/ml100k/grocery')\n","    # 目标item\n","    # filmTrust:random = [5, 395, 181, 565, 254]    tail = [601, 623, 619, 64, 558]\n","    # ml100k:random = [62, 1077, 785, 1419, 1257]   tail = [1319, 1612, 1509, 1545, 1373]\n","    # 5,395,181,565,254,601,623,619,64,558\n","    # 62,1077,785,1419,1257,1319,1612,1509,1545,1373\n","    # 1166,1574,759,494,549,1272,1728,1662,450,1456,595,566,764,1187,1816,1478,1721,2294,2413,1148\n","    # 88,22,122,339,1431,1141,1656,477,1089,866\n","    parser.add_argument('--target_ids', type=str, default='88,22,122,339,1431,1141,1656,477,1089,866',\n","                        help='attack target list')\n","    # 参数 - 攻击数量，即往数据集里插入多少假用户\n","    parser.add_argument('--attack_num', type=int, default=50,\n","                        help='num of attack fake user,50 for ml100k and filmTrust')\n","    # 参数 - filler数量，可理解为是每个假用户有多少评分\n","    parser.add_argument('--filler_num', type=int, default=4,\n","                        help='num of filler items each fake user,90 for ml100k,36 for filmTrust')\n","    # 参数 - 选择filler item的方法，0是随机\n","    parser.add_argument('--filler_method', type=str, default='', help='0/1/2/3')\n","    # 生成的攻击结果写入文件还是返回numpy矩阵，这里设置为1就好\n","    parser.add_argument('--write_to_file', type=int, default=1, help='write to fake profile to file or return array')\n","    # 0：损失函数只用重构损失,1：损失函数用重构损失+seed损失\n","    parser.add_argument('--loss', type=int, default=1, help='0:reconstruction,1:reconstruction+seed')\n","    #\n","    args = parser.parse_args()\n","    #\n","    args.target_ids = list(map(int, args.target_ids.split(',')))\n","    return args\n","\n","\n","if __name__ == '__main__':\n","    \"\"\"parse args\"\"\"\n","    args = parse_arg()\n","    \"\"\"train\"\"\"\n","    is_train = 1\n","    attack_method = '_'.join(\n","        ['G' + str(args.loss), str(args.attack_num), str(args.filler_num), str(args.filler_method)]).strip('_')\n","    #\n","    for target_id in args.target_ids:\n","        \"\"\"读取生成攻击时的sample的filler\"\"\"\n","        attackSetting_path = '_'.join(map(str, [args.dataset, args.attack_num, args.filler_num, target_id]))\n","        attackSetting_path = \"../data/data_attacked/\" + attackSetting_path + '_attackSetting'\n","        real_profiles, filler_indicator = np.load(attackSetting_path + '.npy')\n","        final_attack_setting = [args.attack_num, real_profiles, filler_indicator]\n","\n","        \"\"\"训练模型并注入攻击\"\"\"\n","        _ = gan_attack(args.dataset, attack_method, target_id, is_train,\n","                       write_to_file=args.write_to_file,\n","                       final_attack_setting=final_attack_setting)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_Esdgb3_jGER"},"source":["def gan_attack(data_set_name, attack_method, target_id, is_train, write_to_file=1, final_attack_setting=None):\n","    # 路径设置\n","    path_train = '../data/data/' + data_set_name + '_train.dat'\n","    path_test = '../data/data/' + data_set_name + '_test.dat'\n","    attack_info_path = [\"../data/data/\" + data_set_name + \"_selected_items\",\n","                        \"../data/data/\" + data_set_name + \"_target_users\"]\n","    model_path = \"../result/model_ckpt/\" + '_'.join([data_set_name, attack_method, str(target_id)]) + \".ckpt\"\n","\n","    # 读取seletced items和target users\n","    attack_info = load_attack_info(*attack_info_path)\n","    dataset_class = load_data(path_train=path_train, path_test=path_test, header=['user_id', 'item_id', 'rating'],\n","                              sep='\\t', print_log=True)\n","    # 攻击设置\n","    if len(attack_method.split('_')[1:]) == 2:\n","        attack_num, filler_num = map(int, attack_method.split('_')[1:])\n","        filler_method = 0\n","    else:\n","        attack_num, filler_num, filler_method = map(int, attack_method.split('_')[1:])\n","    selected_items = attack_info[target_id][0]\n","\n","    #\n","    gan_attacker = Train_GAN_Attacker(dataset_class, params_D=None, params_G=None, target_id=target_id,\n","                                      selected_id_list=selected_items,\n","                                      filler_num=filler_num, attack_num=attack_num, filler_method=filler_method)\n","    #\n","    # if is_train:\n","    #     # 训练->模型保存->生成fake_profiles\n","    #     fake_profiles = gan_attacker.execute(is_train=True, model_path=model_path,\n","    #                                          final_attack_setting=final_attack_setting)\n","    # else:\n","    #     # restore>模型保存->生成fake_profiles\n","    #     fake_profiles, real_profiles = gan_attacker.execute(is_train=False, model_path=model_path,\n","    #                                                         final_attack_setting=final_attack_setting)\n","    fake_profiles, real_profiles, filler_indicator = gan_attacker.execute(is_train=is_train, model_path=model_path,\n","                                                                          final_attack_setting=final_attack_setting)\n","    gan_attacker.sess.close()\n","\n","    # \"\"\"inject and write to file\"\"\"\n","    if write_to_file == 1:\n","        dst_path = \"../data/data_attacked/\" + '_'.join([data_set_name, str(target_id), attack_method]) + \".dat\"\n","        attacked_file_writer(path_train, dst_path, fake_profiles, dataset_class.n_users)\n","    return fake_profiles, real_profiles, filler_indicator\n","\n","\n","def parse_arg():\n","    parser = argparse.ArgumentParser()\n","    # 数据集名称，用来选择训练数据路径\n","    parser.add_argument('--dataset', type=str, default='ml100k', help='filmTrust/ml100k/grocery')\n","    # 目标item\n","    # filmTrust:random = [5, 395, 181, 565, 254]    tail = [601, 623, 619, 64, 558]\n","    # ml100k:random = [62, 1077, 785, 1419, 1257]   tail = [1319, 1612, 1509, 1545, 1373]\n","    # 5,395,181,565,254,601,623,619,64,558\n","    # 62,1077,785,1419,1257,1319,1612,1509,1545,1373\n","    parser.add_argument('--target_ids', type=str, default='62,1077,785,1419,1257,1319,1612,1509,1545,1373',\n","                        help='attack target list')\n","    # 参数 - 攻击数量，即往数据集里插入多少假用户\n","    parser.add_argument('--attack_num', type=int, default=50,\n","                        help='num of attack fake user,50 for ml100k and filmTrust')\n","    # 参数 - filler数量，可理解为是每个假用户有多少评分\n","    parser.add_argument('--filler_num', type=int, default=90,\n","                        help='num of filler items each fake user,90 for ml100k,36 for filmTrust')\n","    # 参数 - 选择filler item的方法，0是随机\n","    parser.add_argument('--filler_method', type=str, default='', help='0/1/2/3')\n","    # 生成的攻击结果写入文件还是返回numpy矩阵，这里设置为1就好\n","    parser.add_argument('--write_to_file', type=int, default=1, help='write to fake profile to file or return array')\n","    #\n","    args = parser.parse_args()\n","    #\n","    args.target_ids = list(map(int, args.target_ids.split(',')))\n","    return args\n","\n","\n","if __name__ == '__main__':\n","    \"\"\"parse args\"\"\"\n","    args = parse_arg()\n","    \"\"\"train\"\"\"\n","    is_train = 1\n","    attack_method = '_'.join(['gan', str(args.attack_num), str(args.filler_num), str(args.filler_method)]).strip('_')\n","\n","    #\n","    for target_id in args.target_ids:\n","        \"\"\"读取生成攻击时的sample的filler\"\"\"\n","        attackSetting_path = '_'.join(map(str, [args.dataset, args.attack_num, args.filler_num, target_id]))\n","        attackSetting_path = \"../data/data_attacked/\" + attackSetting_path + '_attackSetting'\n","        real_profiles, filler_indicator = np.load(attackSetting_path + '.npy')\n","        final_attack_setting = [args.attack_num, real_profiles, filler_indicator]\n","\n","        \"\"\"训练模型并注入攻击\"\"\"\n","        _ = gan_attack(args.dataset, attack_method, target_id, is_train,\n","                       write_to_file=args.write_to_file,\n","                       final_attack_setting=final_attack_setting)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"sE32fhaujGBd"},"source":["def get_data(data_set_name):\n","    path_train = '../data/data/' + data_set_name + '_train.dat'\n","    path_test = '../data/data/' + data_set_name + '_test.dat'\n","    dataset_class = load_data(path_train=path_train, path_test=path_test,\n","                              header=['user_id', 'item_id', 'rating'],\n","                              sep='\\t', print_log=False)\n","    attack_info_path = [\"../data/data/\" + data_set_name + \"_selected_items\",\n","                        \"../data/data/\" + data_set_name + \"_target_users\"]\n","    attack_info = load_attack_info(*attack_info_path)\n","    return dataset_class, attack_info\n","\n","\n","def baseline_attack(dataset_class, attack_info, attack_method, target_id, bandwagon_selected,\n","                    fixed_filler_indicator=None):\n","    \"\"\"load data\"\"\"\n","    selected_ids, target_users = attack_info[target_id]\n","    attack_model, attack_num, filler_num = attack_method.split('_')\n","    attack_num, filler_num = int(attack_num), int(filler_num)\n","\n","    \"\"\"attack class\"\"\"\n","    global_mean, global_std, item_means, item_stds = dataset_class.get_all_mean_std()\n","    baseline_attacker = BaselineAttack(attack_num, filler_num, dataset_class.n_items, target_id,\n","                                       global_mean, global_std, item_means, item_stds, 5.0, 1.0,\n","                                       fixed_filler_indicator=fixed_filler_indicator)\n","    # fake profile array\n","    fake_profiles = None\n","    if attack_model == \"random\":\n","        fake_profiles = baseline_attacker.RandomAttack()\n","    elif attack_model == \"bandwagon\":\n","        fake_profiles = baseline_attacker.BandwagonAttack(bandwagon_selected)\n","    elif attack_model == \"average\":\n","        fake_profiles = baseline_attacker.AverageAttack()\n","    elif attack_model == \"segment\":\n","        fake_profiles = baseline_attacker.SegmentAttack(selected_ids)\n","    else:\n","        print('attack_method error')\n","        exit()\n","    return fake_profiles\n","\n","\n","def parse_arg():\n","    parser = argparse.ArgumentParser()\n","    # 数据集名称，用来选择训练数据路径\n","    parser.add_argument('--dataset', type=str, default='automotive', help='filmTrust/ml100k/grocery')\n","    # 攻击方法，逗号隔开\n","    parser.add_argument('--attack_methods', type=str, default='average',\n","                        help='average,segment,random,bandwagon')\n","    # 目标item，逗号隔开，这里前五个是随机target后五个是长尾target\n","    # filmTrust:random = [5, 395, 181, 565, 254]    tail = [601, 623, 619, 64, 558]\n","    # ml100k:random = [62, 1077, 785, 1419, 1257]   tail = [1319, 1612, 1509, 1545, 1373]\n","    # 1166,1574,759,494,549,1272,1728,1662,450,1456,595,566,764,1187,1816,1478,1721,2294,2413,1148\n","    # 62,1077,785,1419,1257,1319,1612,1509,1545,1373\n","    # 88,22,122,339,1431,1141,1656,477,1089,866\n","    parser.add_argument('--targets', type=str, default='88,22,122,339,1431,1141,1656,477,1089,866',\n","                        help='attack_targets')\n","    # 参数 - 攻击数量，即往数据集里插入多少假用户\n","    parser.add_argument('--attack_num', type=int, default=50, help='fixed 50')\n","    # 参数 - filler数量，可理解为是每个假用户有多少评分\n","    parser.add_argument('--filler_num', type=int, default=4, help='90 for ml100k,36 for filmTrust')\n","    parser.add_argument('--bandwagon_selected', type=str, default='180,99,49',\n","                        help='180,99,49 for ml100k,103,98,115 for filmTrust')\n","    #\n","    parser.add_argument('--sample_filler', type=int, default=1, help='sample filler')\n","    #\n","\n","    args = parser.parse_args()\n","    #\n","    args.attack_methods = args.attack_methods.split(',')\n","    args.targets = list(map(int, args.targets.split(',')))\n","    args.bandwagon_selected = list(map(int, args.bandwagon_selected.split(',')))\n","    return args\n","\n","\n","if __name__ == '__main__':\n","    \"\"\"parse args\"\"\"\n","    args = parse_arg()\n","\n","    \"\"\"attack\"\"\"\n","    dataset_class, attack_info = get_data(args.dataset)\n","    # 对每种攻击方法&攻击目标，生成fake profile并写入目标路径\n","    for target_id in args.targets:\n","        # 固定filler\n","        attackSetting_path = '_'.join(map(str, [args.dataset, args.attack_num, args.filler_num, target_id]))\n","        attackSetting_path = \"../data/data_attacked/\" + attackSetting_path + '_attackSetting'\n","        if args.sample_filler:\n","            gan_attacker = Train_GAN_Attacker(dataset_class, params_D=None, params_G=None, target_id=target_id,\n","                                              selected_id_list=attack_info[target_id][0],\n","                                              filler_num=args.filler_num, attack_num=args.attack_num, filler_method=0)\n","            _, real_profiles, filler_indicator = gan_attacker.execute(is_train=0, model_path='no',\n","                                                                      final_attack_setting=[args.attack_num,\n","                                                                                            None, None])\n","\n","            np.save(attackSetting_path, [real_profiles, filler_indicator])\n","        else:\n","            real_profiles, filler_indicator = np.load(attackSetting_path + '.npy')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fsfaOuosjF-Y"},"source":["def attack_evaluate(real_preds_path, attacked_preds_file, non_rated_users, target_users):\n","    #\n","    names = ['uid', 'rating', 'HR_1', 'HR_3', 'HR_5', 'HR_10', 'HR_20', 'HR_50']\n","    real_preds = pd.read_csv(real_preds_path, sep='\\t', names=names, engine='python')\n","    attacked_preds = pd.read_csv(attacked_preds_file, sep='\\t', names=names, engine='python')\n","    # pred\n","    shift_target = np.mean(attacked_preds.iloc[target_users, 1].values - real_preds.iloc[target_users, 1].values)\n","    shift_all = np.mean(attacked_preds.iloc[non_rated_users, 1].values - real_preds.iloc[non_rated_users, 1].values)\n","    #\n","    HR_real_target = real_preds.iloc[target_users, range(2, 8)].mean().values\n","    HR_real_all = real_preds.iloc[non_rated_users, range(2, 8)].mean().values\n","\n","    HR_attacked_target = attacked_preds.iloc[target_users, range(2, 8)].mean().values\n","    HR_attacked_all = attacked_preds.iloc[non_rated_users, range(2, 8)].mean().values\n","    return shift_target, HR_real_target, HR_attacked_target, shift_all, HR_real_all, HR_attacked_all\n","\n","\n","def eval_attack(data_set_name, rec_model_name, attack_method, target_id):\n","    dir = \"../result/pred_result/\"\n","    real_preds_path = dir + '_'.join([rec_model_name, data_set_name, str(target_id)])\n","    attacked_preds_file = real_preds_path + \"_\" + attack_method\n","    \"\"\"\n","    ml100k\n","    \"\"\"\n","    if data_set_name == 'ml100k':\n","        path_train = \"../data/data/ml100k_train.dat\"\n","        path_test = \"../data/data/ml100k_test.dat\"\n","        attack_info_path = [\"../data/data/ml100k_selected_items\", \"../data/data/ml100k_target_users\"]\n","    elif data_set_name == 'filmTrust':\n","        path_train = \"../data/data/filmTrust_train.dat\"\n","        path_test = \"../data/data/filmTrust_test.dat\"\n","        attack_info_path = [\"../data/data/filmTrust_selected_items\", \"../data/data/filmTrust_target_users\"]\n","\n","    else:\n","        path_train = \"../data/data/\" + data_set_name + \"_train.dat\"\n","        path_test = \"../data/data/\" + data_set_name + \"_test.dat\"\n","        attack_info_path = [\"../data/data/\" + data_set_name + \"_selected_items\",\n","                            \"../data/data/\" + data_set_name + \"_target_users\"]\n","\n","    attack_info = load_attack_info(*attack_info_path)\n","    dataset_class = load_data(path_train=path_train, path_test=path_test, header=['user_id', 'item_id', 'rating'],\n","                              sep='\\t', print_log=False)\n","\n","    #\n","    target_users = attack_info[target_id][1]\n","    non_rated_users = dataset_class.get_item_nonrated_users(target_id)\n","    #\n","    res = attack_evaluate(real_preds_path, attacked_preds_file, non_rated_users, target_users)\n","    #\n","    target, all = res[:3], res[3:]\n","    target_str = '\\t'.join([str(target[0]), '\\t'.join(map(str, target[1])), '\\t'.join(map(str, target[2]))])\n","    all_str = '\\t'.join([str(all[0]), '\\t'.join(map(str, all[1])), '\\t'.join(map(str, all[2]))])\n","\n","    # info\n","    info = '\\t'.join([rec_model_name, attack_method, str(target_id)])\n","    # print(info + '\\t' + target_str + '\\t' + all_str)\n","    return info + '\\t' + target_str + '\\t' + all_str\n","\n","\n","def parse_arg():\n","    parser = argparse.ArgumentParser()\n","    # 数据集名称，用来选择训练数据路径\n","    parser.add_argument('--dataset', type=str, default='automotive', help='filmTrust/ml100k/office')\n","    # 参数 - 攻击数量，即往数据集里插入多少假用户\n","    parser.add_argument('--attack_num', type=int, default=50, help='50 for ml100k and filmTrust')\n","    # 参数 - filler数量，可理解为是每个假用户有多少评分\n","    parser.add_argument('--filler_num', type=int, default=4, help='90 for ml100k,36 for filmTrust')\n","    # 攻击方法\n","    parser.add_argument('--attack_methods', type=str, default='G0,G1',\n","                        help='gan,G0,G1,segment,average,random,bandwagon')\n","    # 目标模型\n","    parser.add_argument('--rec_model_names', type=str, default='NNMF,IAutoRec,UAutoRec,NMF_25',\n","                        help='NNMF,IAutoRec,UAutoRec,NMF_25')\n","    # 目标item，逗号隔开，这里前五个是随机target后五个是长尾target\n","    # filmTrust:5,395,181,565,254,601,623,619,64,558 - random*5+tail*5\n","    # ml100k:62,1077,785,1419,1257,1319,1612,1509,1545,1373 - random*5+tail*5\n","    # 1166,1574,759,494,549,1272,1728,1662,450,1456,595,566,764,1187,1816,1478,1721,2294,2413,1148\n","    # 88,22,122,339,1431,1141,1656,477,1089,866\n","    parser.add_argument('--target_ids', type=str, default='88,22,122,339,1431,1141,1656,477,1089,866',\n","                        help='target_id')\n","\n","    #\n","    args = parser.parse_args()\n","    #\n","    args.attack_methods = args.attack_methods.split(',')\n","    args.rec_model_names = args.rec_model_names.split(',')\n","    args.target_ids = list(map(int, args.target_ids.split(',')))\n","    return args\n","\n","\n","if __name__ == '__main__':\n","    \"\"\"parse args\"\"\"\n","    args = parse_arg()\n","    \"\"\"eval\"\"\"\n","    result = []\n","\n","    for attack_method in args.attack_methods:\n","        for rec_model_name in args.rec_model_names:\n","            for target_id in args.target_ids:\n","                attack_method_ = '_'.join([attack_method, str(args.attack_num), str(args.filler_num)])\n","                try:\n","                    result_ = eval_attack(args.dataset, rec_model_name, attack_method_, target_id)\n","                    result.append(result_.split('\\t'))\n","                except:\n","                    print(attack_method, rec_model_name, target_id)\n","\n","    result = np.array(result).transpose()\n","    result = pd.DataFrame(dict(zip(range(result.shape[0]), result)))\n","    result.to_excel(args.dataset + '_performance_all.xls', index=False)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MusordfrjF6r"},"source":["def eval_eigen_value(profiles):\n","    U_T_U = np.dot(profiles.transpose(), profiles)\n","    eig_val, _ = eig(U_T_U)\n","    top_10 = [i.real for i in eig_val[:10]]\n","    return top_10\n","\n","\n","def get_item_distribution(profiles):\n","    # [min(max(0, round(i)), 5) for i in a]\n","    profiles_T = profiles.transpose()\n","    fn_count = lambda item_vec: np.array(\n","        [sum([1 if (min(max(0, round(j)), 5) == i) else 0 for j in item_vec]) for i in range(6)])\n","    fn_norm = lambda item_vec: item_vec / sum(item_vec)\n","    item_distribution = np.array(list(map(fn_count, profiles_T)))\n","    item_distribution = np.array(list(map(fn_norm, item_distribution)))\n","    return item_distribution\n","\n","\n","def eval_TVD_JS(P, Q):\n","    # TVD\n","    dis_TVD = np.mean(np.sum(np.abs(P - Q) / 2, 1))\n","    # JS\n","    fn_KL = lambda p, q: scipy.stats.entropy(p, q)\n","    M = (P + Q) / 2\n","    js_vec = []\n","    for iid in range(P.shape[0]):\n","        p, q, m = P[iid], Q[iid], M[iid]\n","        js_vec.append((fn_KL(p, m) + fn_KL(q, m)) / 2)\n","    dis_JS = np.mean(np.array(js_vec))\n","    return dis_TVD, dis_JS\n","\n","\n","def print_eigen_result(real_profiles, fake_profiles_gan, baseline_fake_profiles, baseline_methods):\n","    top_10_res = []\n","    top_10_real = eval_eigen_value(real_profiles)\n","    top_10_res.append(\"real\\t\" + '\\t'.join(map(str, top_10_real)))\n","    top_10_baseline = []\n","    for idx in range(len(baseline_fake_profiles)):\n","        top_10_baseline.append(eval_eigen_value(baseline_fake_profiles[idx]))\n","        top_10_res.append(baseline_methods[idx] + \"\\t\" + '\\t'.join(map(str, top_10_baseline[-1])))\n","    top_10_gan = eval_eigen_value(fake_profiles_gan)\n","    # top_10_sample_5 = eval_eigen_value(fake_profiles_sample_5)\n","    # top_10_real_sample = eval_eigen_value(real_profiles_gan)\n","    top_10_res.append(\"gan\\t\" + '\\t'.join(map(str, top_10_gan)))\n","    # top_10_res.append(\"sample_5\\t\" + '\\t'.join(map(str, top_10_sample_5)))\n","    # top_10_res.append(\"real_sample\\t\" + '\\t'.join(map(str, top_10_real_sample)))\n","    print(\"\\n\".join(top_10_res))\n","\n","\n","def get_distance_result(target_id, real_profiles, fake_profiles_gan, baseline_fake_profiles, baseline_methods):\n","    k = ['target_id', 'attack_method', 'dis_TVD', 'dis_JS']\n","    v = [[], [], [], []]\n","    res_dis = []\n","    real_item_distribution = get_item_distribution(real_profiles)\n","    # real_gan_item_distribution = get_item_distribution(real_profiles_gan)\n","    fake_gan_distribution = get_item_distribution(fake_profiles_gan)\n","    # fake_sample_5_distribution = get_item_distribution(fake_profiles_sample_5)\n","    # dis_TVD, dis_JS = eval_TVD_JS(real_item_distribution, real_gan_item_distribution)\n","    # res_dis.append('\\t'.join(map(str, [\"real\", \"real_gan\", dis_TVD, dis_JS])))\n","    # dis_TVD, dis_JS = eval_TVD_JS(real_gan_item_distribution, fake_gan_distribution)\n","    # res_dis.append('\\t'.join(map(str, [\"real_gan\", \"gan\", dis_TVD, dis_JS])))\n","    # dis_TVD, dis_JS = eval_TVD_JS(real_item_distribution, fake_sample_5_distribution)\n","    # res_dis.append('\\t'.join(map(str, [\"real\", \"sample_5\", dis_TVD, dis_JS])))\n","    # dis_TVD, dis_JS = eval_TVD_JS(real_gan_item_distribution, fake_sample_5_distribution)\n","    # res_dis.append('\\t'.join(map(str, [\"real_gan\", \"sample_5\", dis_TVD, dis_JS])))\n","    dis_TVD, dis_JS = eval_TVD_JS(real_item_distribution, fake_gan_distribution)\n","    v[1] += ['gan']\n","    v[2] += [dis_TVD]\n","    v[3] += [dis_JS]\n","    # res_dis.append('\\t'.join(map(str, [target_id, \"gan\", dis_TVD, dis_JS])))\n","    for idx in range(len(baseline_fake_profiles)):\n","        dis_TVD, dis_JS = eval_TVD_JS(real_item_distribution, get_item_distribution(baseline_fake_profiles[idx]))\n","        v[1] += [baseline_methods[idx]]\n","        v[2] += [dis_TVD]\n","        v[3] += [dis_JS]\n","        # res_dis.append('\\t'.join(map(str, [target_id, baseline_methods[idx], dis_TVD, dis_JS])))\n","    v[0] = [target_id] * len(v[1])\n","    result = pd.DataFrame(dict(zip(k, v)))\n","    # print('\\n'.join(res_dis))\n","    return result\n","\n","\n","def profiles_generator(target_id, dataset_class, attack_info, bandwagon_selected, sample_num, args, real_profiles,\n","                       filler_indicator, pre_fix, has_G=False):\n","    # baseline fake profiles\n","    baseline_methods = [\"segment\", \"average\", \"random\", \"bandwagon\"]\n","    baseline_fake_profiles = []\n","    for attack_method in baseline_methods:\n","        attack_model = '_'.join([attack_method, str(sample_num), str(args.filler_num)])\n","        fake_profiles = baseline_attack(dataset_class, attack_info, attack_model, target_id,\n","                                        bandwagon_selected, filler_indicator)\n","        baseline_fake_profiles.append(fake_profiles)\n","\n","    for attack_method in baseline_methods:\n","        attack_model = '_'.join([attack_method, str(sample_num), str(args.filler_num)])\n","        fake_profiles = baseline_attack(dataset_class, attack_info, attack_model, target_id,\n","                                        bandwagon_selected, None)\n","        baseline_fake_profiles.append(fake_profiles)\n","    baseline_methods = baseline_methods + [i + '_rand' for i in baseline_methods]\n","\n","    final_attack_setting = [sample_num, real_profiles, filler_indicator]\n","    # new_baseline\n","    if has_G:\n","        for attack_method in ['G0' + pre_fix, 'G1' + pre_fix]:\n","            baseline_methods.append(attack_method)\n","            fake_profiles_G, _, _ = gan_attack_baseline(args.dataset, attack_method, target_id, False, 0,\n","                                                        final_attack_setting=final_attack_setting)\n","            baseline_fake_profiles.append(fake_profiles_G)\n","\n","    # gan profiles\n","    attack_method = \"gan\" + pre_fix\n","    fake_profiles_gan, _, _ = gan_attack(args.dataset, attack_method, target_id, False, write_to_file=0,\n","                                         final_attack_setting=final_attack_setting)\n","    return fake_profiles_gan, baseline_fake_profiles, baseline_methods\n","\n","\n","def parse_arg():\n","    parser = argparse.ArgumentParser()\n","    # 数据集名称，用来选择训练数据路径\n","    parser.add_argument('--dataset', type=str, default='ml100k',\n","                        help='input data_set_name,filmTrust or ml100k grocery')\n","    # 参数 - 攻击数量，即往数据集里插入多少假用户\n","    parser.add_argument('--attack_num', type=int, default=50,\n","                        help='num of attack fake user,50 for ml100k and filmTrust')\n","    # 参数 - filler数量，可理解为是每个假用户有多少评分\n","    parser.add_argument('--filler_num', type=int, default=90,\n","                        help='num of filler items each fake user,90 for ml100k,36 for filmTrust')\n","    # filmTrust:5,395,181,565,254,601,623,619,64,558 - random*5+tail*5\n","    # ml100k:62,1077,785,1419,1257,1319,1612,1509,1545,1373 - random*5+tail*5\n","    parser.add_argument('--targets', type=str, default='62,1077,785,1419,1257,1319,1612,1509,1545,1373', help='attack_targets')\n","    parser.add_argument('--bandwagon_selected', type=str, default='180,99,49',\n","                        help='180,99,49 for ml100k,103,98,115 for filmTrust')\n","    #\n","    args = parser.parse_args()\n","    #\n","    args.targets = list(map(int, args.targets.split(',')))\n","    args.bandwagon_selected = list(map(int, args.bandwagon_selected.split(',')))\n","    return args\n","\n","\n","if __name__ == '__main__':\n","    \"\"\"\n","    step1 - load data\n","    step2 - 共所有攻击方法生成评分矩阵\n","    step3 - 真假评分矩阵的距离度量\n","    \"\"\"\n","\n","    #\n","    \"\"\"parse args\"\"\"\n","    args = parse_arg()\n","    pre_fix = '_' + str(args.attack_num) + '_' + str(args.filler_num)\n","\n","    \"\"\"step1 - load data\"\"\"\n","    path_train = \"../data/data/\" + args.dataset + \"_train.dat\"\n","    path_test = \"../data/data/\" + args.dataset + \"_test.dat\"\n","    attack_info_path = [\"../data/data/\" + args.dataset + \"_selected_items\",\n","                        \"../data/data/\" + args.dataset + \"_target_users\"]\n","    dataset_class = load_data(path_train=path_train, path_test=path_test, header=['user_id', 'item_id', 'rating'],\n","                              sep='\\t', print_log=False)\n","    attack_info = load_attack_info(*attack_info_path)\n","\n","    sample_num = dataset_class.n_users\n","    result = None\n","    for target_id in args.targets:\n","        selected = attack_info[target_id][0]\n","        \"\"\"step2.1 - 生成固定的filler\"\"\"\n","        attackSetting_path = '_'.join(map(str, [args.dataset, sample_num, args.filler_num, target_id]))\n","        attackSetting_path = \"../data/data_attacked/\" + attackSetting_path + '_attackSetting'\n","        gan_attacker = Train_GAN_Attacker(dataset_class, params_D=None, params_G=None, target_id=target_id,\n","                                          selected_id_list=selected, filler_num=args.filler_num,\n","                                          attack_num=args.attack_num, filler_method=0)\n","        _, real_profiles, filler_indicator = gan_attacker.execute(is_train=0, model_path='no',\n","                                                                  final_attack_setting=[sample_num, None, None])\n","        np.save(attackSetting_path, [real_profiles, filler_indicator])\n","        \"\"\"step2.2 - 为所有攻击方法生成评分矩阵\"\"\"\n","        fake_profiles_gan, baseline_fake_profiles, baseline_methods \\\n","            = profiles_generator(target_id, dataset_class, attack_info, args.bandwagon_selected, sample_num, args,\n","                                 real_profiles, filler_indicator, pre_fix, has_G=True)\n","\n","        \"\"\"step3 - 真假评分矩阵的距离度量\"\"\"\n","        # result_ = get_distance_result(target_id, real_profiles, fake_profiles_gan, baseline_fake_profiles,\n","        #                               baseline_methods)\n","        result_ = get_distance_result(target_id, dataset_class.train_matrix.toarray(), fake_profiles_gan,\n","                                      baseline_fake_profiles,\n","                                      baseline_methods)\n","\n","        result = result_ if result is None else pd.concat([result, result_])\n","    print(result)\n","    result.to_excel(args.dataset + '_distance_lianyun.xls', index=False)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"oHPOQjPjjl1P"},"source":["def eval_eigen_value(profiles):\n","    U_T_U = np.dot(profiles.transpose(), profiles)\n","    eig_val, _ = eig(U_T_U)\n","    top_10 = [i.real for i in eig_val[:10]]\n","    return top_10\n","\n","\n","def get_item_distribution(profiles):\n","    # [min(max(0, round(i)), 5) for i in a]\n","    profiles_T = profiles.transpose()\n","    fn_count = lambda item_vec: np.array(\n","        [sum([1 if (min(max(0, round(j)), 5) == i) else 0 for j in item_vec]) for i in range(6)])\n","    fn_norm = lambda item_vec: item_vec / sum(item_vec)\n","    item_distribution = np.array(list(map(fn_count, profiles_T)))\n","    item_distribution = np.array(list(map(fn_norm, item_distribution)))\n","    return item_distribution\n","\n","\n","def eval_TVD_JS(P, Q):\n","    # TVD\n","    dis_TVD = np.mean(np.sum(np.abs(P - Q) / 2, 1))\n","    # JS\n","    fn_KL = lambda p, q: scipy.stats.entropy(p, q)\n","    M = (P + Q) / 2\n","    js_vec = []\n","    for iid in range(P.shape[0]):\n","        p, q, m = P[iid], Q[iid], M[iid]\n","        js_vec.append((fn_KL(p, m) + fn_KL(q, m)) / 2)\n","    dis_JS = np.mean(np.array(js_vec))\n","    return dis_TVD, dis_JS\n","\n","\n","def print_eigen_result(real_profiles, fake_profiles_gan, baseline_fake_profiles, baseline_methods):\n","    top_10_res = []\n","    top_10_real = eval_eigen_value(real_profiles)\n","    top_10_res.append(\"real\\t\" + '\\t'.join(map(str, top_10_real)))\n","    top_10_baseline = []\n","    for idx in range(len(baseline_fake_profiles)):\n","        top_10_baseline.append(eval_eigen_value(baseline_fake_profiles[idx]))\n","        top_10_res.append(baseline_methods[idx] + \"\\t\" + '\\t'.join(map(str, top_10_baseline[-1])))\n","    top_10_gan = eval_eigen_value(fake_profiles_gan)\n","    # top_10_sample_5 = eval_eigen_value(fake_profiles_sample_5)\n","    # top_10_real_sample = eval_eigen_value(real_profiles_gan)\n","    top_10_res.append(\"gan\\t\" + '\\t'.join(map(str, top_10_gan)))\n","    # top_10_res.append(\"sample_5\\t\" + '\\t'.join(map(str, top_10_sample_5)))\n","    # top_10_res.append(\"real_sample\\t\" + '\\t'.join(map(str, top_10_real_sample)))\n","    print(\"\\n\".join(top_10_res))\n","\n","\n","def get_distance_result(target_id, real_profiles, fake_profiles_list, method_name):\n","    k = ['target_id', 'attack_method', 'dis_TVD', 'dis_JS']\n","    v = [[], [], [], []]\n","    res_dis = []\n","    real_item_distribution = get_item_distribution(real_profiles)\n","    for idx in range(len(fake_profiles_list)):\n","        dis_TVD, dis_JS = eval_TVD_JS(real_item_distribution, get_item_distribution(fake_profiles_list[idx]))\n","        v[1] += [method_name[idx]]\n","        v[2] += [dis_TVD]\n","        v[3] += [dis_JS]\n","    v[0] = [target_id] * len(v[1])\n","    result = pd.DataFrame(dict(zip(k, v)))\n","    return result\n","\n","\n","def profiles_generator(target_id, dataset_class, attack_info, bandwagon_selected, sample_num, args, real_profiles,\n","                       filler_indicator, pre_fix, has_G=False):\n","    # baseline fake profiles\n","    baseline_methods = [\"segment\", \"average\", \"random\", \"bandwagon\"]\n","    baseline_fake_profiles = []\n","    for attack_method in baseline_methods:\n","        attack_model = '_'.join([attack_method, str(sample_num), str(args.filler_num)])\n","        fake_profiles = baseline_attack(dataset_class, attack_info, attack_model, target_id,\n","                                        bandwagon_selected, filler_indicator)\n","        baseline_fake_profiles.append(fake_profiles)\n","\n","    for attack_method in baseline_methods:\n","        attack_model = '_'.join([attack_method, str(sample_num), str(args.filler_num)])\n","        fake_profiles = baseline_attack(dataset_class, attack_info, attack_model, target_id,\n","                                        bandwagon_selected, None)\n","        baseline_fake_profiles.append(fake_profiles)\n","    baseline_methods = baseline_methods + [i + '_rand' for i in baseline_methods]\n","\n","    final_attack_setting = [sample_num, real_profiles, filler_indicator]\n","    # new_baseline\n","    if has_G:\n","        for attack_method in ['G0' + pre_fix, 'G1' + pre_fix]:\n","            baseline_methods.append(attack_method)\n","            fake_profiles_G, _, _ = gan_attack_baseline(args.dataset, attack_method, target_id, False, 0,\n","                                                        final_attack_setting=final_attack_setting)\n","            baseline_fake_profiles.append(fake_profiles_G)\n","\n","    # gan profiles\n","    attack_method = \"gan\" + pre_fix\n","    fake_profiles_gan, _, _ = gan_attack(args.dataset, attack_method, target_id, False, write_to_file=0,\n","                                         final_attack_setting=final_attack_setting)\n","    return fake_profiles_gan, baseline_fake_profiles, baseline_methods\n","\n","\n","def parse_arg():\n","    parser = argparse.ArgumentParser()\n","    # 数据集名称，用来选择训练数据路径\n","    parser.add_argument('--dataset', type=str, default='ml100k',\n","                        help='input data_set_name,filmTrust or ml100k grocery')\n","    # 参数 - 攻击数量，即往数据集里插入多少假用户\n","    parser.add_argument('--attack_num', type=int, default=50,\n","                        help='num of attack fake user,50 for ml100k and filmTrust')\n","    # 参数 - filler数量，可理解为是每个假用户有多少评分\n","    parser.add_argument('--filler_num', type=int, default=90,\n","                        help='num of filler items each fake user,90 for ml100k,36 for filmTrust')\n","    # filmTrust:5,395,181,565,254,601,623,619,64,558 - random*5+tail*5\n","    # ml100k:62,1077,785,1419,1257,1319,1612,1509,1545,1373 - random*5+tail*5\n","    parser.add_argument('--targets', type=str, default='62,1077,785,1419,1257,1319,1612,1509,1545,1373',\n","                        help='attack_targets')\n","    parser.add_argument('--bandwagon_selected', type=str, default='180,99,49',\n","                        help='180,99,49 for ml100k,103,98,115 for filmTrust')\n","    #\n","    args = parser.parse_args()\n","    #\n","    args.targets = list(map(int, args.targets.split(',')))\n","    args.bandwagon_selected = list(map(int, args.bandwagon_selected.split(',')))\n","    return args\n","\n","\n","if __name__ == '__main__':\n","    \"\"\"\n","    step1 - load data\n","    step2 - 共所有攻击方法生成评分矩阵\n","    step3 - 真假评分矩阵的距离度量\n","    \"\"\"\n","\n","    #\n","    \"\"\"parse args\"\"\"\n","    args = parse_arg()\n","    pre_fix = '_' + str(args.attack_num) + '_' + str(args.filler_num)\n","\n","    \"\"\"step1 - load data\"\"\"\n","    path_train = \"../data/data/\" + args.dataset + \"_train.dat\"\n","    path_test = \"../data/data/\" + args.dataset + \"_test.dat\"\n","    attack_info_path = [\"../data/data/\" + args.dataset + \"_selected_items\",\n","                        \"../data/data/\" + args.dataset + \"_target_users\"]\n","    dataset_class = load_data(path_train=path_train, path_test=path_test, header=['user_id', 'item_id', 'rating'],\n","                              sep='\\t', print_log=False)\n","    attack_info = load_attack_info(*attack_info_path)\n","\n","    sample_num = dataset_class.n_users\n","    result = None\n","    for target_id in args.targets:\n","        selected = attack_info[target_id][0]\n","        \"\"\"step2.1 - real_profiles\"\"\"\n","        gan_attacker = Train_GAN_Attacker(dataset_class, params_D=None, params_G=None, target_id=target_id,\n","                                          selected_id_list=selected, filler_num=args.filler_num,\n","                                          attack_num=args.attack_num, filler_method=0)\n","        _, real_profiles, filler_indicator = gan_attacker.execute(is_train=0, model_path='no',\n","                                                                  final_attack_setting=[sample_num, None, None])\n","        \"\"\"step2.2 - 为所有攻击方法生成评分矩阵\"\"\"\n","        # dcgan数据\n","        dir = None\n","        fake_profiles_list = []\n","        method_list = []\n","        for attack_method in ['IAutoRec', 'UAutoRec', 'NNMF', 'NMF_25']:\n","            path_dcgan = dir + 'D-%s-ml100k\\\\ml100k_%d_dcgan_50_90.dat' % (attack_method, target_id)\n","            dataset_class_dcgan = load_data(path_train=path_dcgan, path_test=path_test,\n","                                            header=['user_id', 'item_id', 'rating'],\n","                                            sep='\\t', print_log=False)\n","            fake_profiles_ = dataset_class_dcgan.train_matrix.toarray()[dataset_class.n_users:]\n","            while fake_profiles_.shape[0] < dataset_class.n_users:\n","                fake_profiles_ = np.concatenate([fake_profiles_, fake_profiles_])\n","            fake_profiles_ = fake_profiles_[:dataset_class.n_users]\n","            # 同样的方法读入wgan数据\n","            path_wgan = dir + 'W-%s-ml100k\\\\ml100k_%d_wgan_50_90.dat' % (attack_method, target_id)\n","            dataset_class_dcgan = load_data(path_train=path_dcgan, path_test=path_test,\n","                                            header=['user_id', 'item_id', 'rating'],\n","                                            sep='\\t', print_log=False)\n","            fake_profiles_w = dataset_class_dcgan.train_matrix.toarray()[dataset_class.n_users:]\n","            while fake_profiles_w.shape[0] < dataset_class.n_users:\n","                fake_profiles_w = np.concatenate([fake_profiles_w, fake_profiles_w])\n","            fake_profiles_w = fake_profiles_w[:dataset_class.n_users]\n","            #\n","            fake_profiles_list += [fake_profiles_, fake_profiles_w]\n","            method_list += ['dcgan', 'wgan']\n","        \"\"\"step3 - 真假评分矩阵的距离度量\"\"\"\n","        result_ = get_distance_result(target_id, real_profiles, fake_profiles_list, method_list)\n","        result = result_ if result is None else pd.concat([result, result_])\n","    print(result)\n","    result.groupby('attack_method').mean().to_excel(args.dataset + '_distance_new.xls', index=False)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"nch-c8tAjltW"},"source":["columns = ['Rec_model', 'attack_method', 'target_id']\n","# 后面是攻击效果\n","hr = ['HR_1', 'HR_3', 'HR_5', 'HR_10', 'HR_20', 'HR_50']\n","hr_ori = [i + '_ori' for i in hr]\n","# 段内用户的 预估分偏移量+攻击前的HR+攻击后的HR\n","columns += [i + '_inseg' for i in ['shift'] + hr_ori + hr]\n","# 全部用户的 预估分偏移量+攻击前的HR+攻击后的HR\n","columns += [i + '_all' for i in ['shift'] + hr_ori + hr]\n","# 需要统计的数值指标：\n","columns_r = [i + '_inseg' for i in ['shift'] + hr] + [i + '_all' for i in ['shift'] + hr]\n","\"\"\"\"\"\"\n","# data = pd.read_excel('filmTrust_distance.xls')\n","# data.groupby('attack_method').mean()[['dis_TVD','dis_JS']].to_excel('filmTrust_distance_avg.xls')\n","\n","# data = pd.read_excel('ml100k_performance_all.xls')\n","# data = pd.read_excel('../result_ijcai/filmTrust_performance_all.xls')\n","# data = pd.read_excel('../result_ijcai/ml100k_performance_all.xls')\n","# data = pd.read_excel('office_performance_all.xls')\n","data = pd.read_excel('automotive_performance_all.xls')\n","data.columns = columns\n","data = data[['Rec_model', 'attack_method', 'target_id', 'shift_inseg', 'HR_10_inseg', 'shift_all', 'HR_10_all']]\n","# target_type_dict = dict(\n","#     zip([62, 1077, 785, 1419, 1257] + [1319, 1612, 1509, 1545, 1373], ['random'] * 5 + ['tail'] * 5))\n","# target_type_dict = dict(zip([5, 395, 181, 565, 254] + [601, 623, 619, 64, 558], ['random'] * 5 + ['tail'] * 5))\n","target_type_dict = dict(zip([1141, 1656, 477, 1089, 866] + [88, 22, 122, 339, 1431], ['random'] * 5 + ['tail'] * 5))\n","data['target_type'] = data.target_id.apply(lambda x: target_type_dict[x])\n","data['attack_method'] = data.attack_method.apply(lambda x: x.split('_')[0])\n","result = data.groupby(['Rec_model','attack_method', 'target_type']).mean()[['shift_all', 'HR_10_all']]\n","result.to_excel('ml100k_performance_0119_sample_strategy.xlsx')\n","exit()"],"execution_count":null,"outputs":[]}]}