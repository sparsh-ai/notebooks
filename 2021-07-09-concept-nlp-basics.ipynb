{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    },
    "colab": {
      "name": "03-NLP-Basics.ipynb",
      "provenance": [],
      "toc_visible": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v8RREnf5Ms_I"
      },
      "source": [
        "# Concept - Natural Language Processing 101\n",
        "> Understand the basic process of pattern recognition that is needed to work with text data. This approach can be used for a large number of basic operations on text data like Document Classification (e.g. topic of an article),Sequence to Sequence Learning (e.g. translations), and Sentiment Analysis\n",
        "\n",
        "- toc: true\n",
        "- badges: true\n",
        "- comments: true\n",
        "- categories: [Concept, TFIDF, NLP, Visualization, Altair]\n",
        "- image:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aX0HRp2SM6c2"
      },
      "source": [
        "Lets understand the basic process of pattern recognition that is needed to work with text data. This approach can be used for a large number of basic operations on text data like \n",
        "- Document Classification (e.g. topic of an article)\n",
        "- Sequence to Sequence Learning (e.g. translations)\n",
        "- Sentiment Analysis\n",
        "\n",
        "Lets start with seeing how a text sequence can be encoded into numbers and processed to prepare for these tasks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KrAAWGSAMs_a"
      },
      "source": [
        "## Tokenisation\n",
        "\n",
        "Lets take the sentence - **The quick brown fox jumped over the lazy dog**\n",
        "\n",
        "We need to first break this sentence in to smaller constituents - called **tokens**. Now there are three ways of creating the tokens can happen:\n",
        "\n",
        "- **Individual character** - create tokens for each\n",
        "- **Individual word** - Create tokens for each word in the sentence\n",
        "- **N-gram** - Create tokens by taking n-grams words in the sentence"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_247UN-8Ms_c"
      },
      "source": [
        "### Create word tokens"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ra093bFgMs_d"
      },
      "source": [
        "**Pre-processing - split, punctuation & case**\n",
        "\n",
        "There is some basic **pre-processing** that has been done in the process of creating word tokens\n",
        "- Split the sentence on **whitespace**\n",
        "- Filter **punctuations**\n",
        "- Change to **lower case** text\n",
        "\n",
        "After this pre-processing, we can get each token."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CkczbwgEMs_f"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import altair as alt\n",
        "\n",
        "import spacy\n",
        "from keras.preprocessing.text import text_to_word_sequence\n",
        "from keras.preprocessing.text import one_hot, hashing_trick"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MLUmP61sMs_v"
      },
      "source": [
        "! python -m spacy download en_core_web_sm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CGFKq3l_Ms_k"
      },
      "source": [
        "sentence = 'The quick brown fox jumped over the lazy dog.'"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f-MWZVh8Ms_l",
        "outputId": "7351306b-e5fb-4fd8-9492-4def61dad5a6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "text_to_word_sequence(sentence)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['the', 'quick', 'brown', 'fox', 'jumped', 'over', 'the', 'lazy', 'dog']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "itGv8WC2Ms_x"
      },
      "source": [
        "nlp = spacy.load('en_core_web_sm')"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JPvnhJ_-Ms_z"
      },
      "source": [
        "sentence = 'The quick brown fox jumped over the lazy dog'\n",
        "doc = nlp(sentence)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IRo0m0kiMs_0",
        "outputId": "0a13b9b4-6dfd-4e3c-b699-1ab89bacb49d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "for token in doc:\n",
        "    print(token)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The\n",
            "quick\n",
            "brown\n",
            "fox\n",
            "jumped\n",
            "over\n",
            "the\n",
            "lazy\n",
            "dog\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6dKO21nPMs_3"
      },
      "source": [
        "## Vectorisation\n",
        "\n",
        "Once you have tokens, we need to find a way to represent them as vectors. Let's look at two traditional way of representing them as vectors\n",
        "\n",
        "- Frequency Based\n",
        "    - Binary\n",
        "    - Count \n",
        "    - tfidf\n",
        "    - Co-occurence (Skipgram)\n",
        "- Prediction Based\n",
        "    - Pre-trained Vectors\n",
        "    - Learning Vectors\n",
        "    - Learning vectors with the task"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fBLXilXtMs_5"
      },
      "source": [
        "### One-Hot Encoding "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jPf81n4XMs_7",
        "outputId": "170eacc7-62ff-4d6c-f653-016d2bc8b238",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Given a size of vocabulary, do one-hot encoding\n",
        "one_hot(sentence, n=10)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[5, 3, 3, 7, 8, 8, 5, 9, 9]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NdxhaclNMs_9",
        "outputId": "8ccf6bf4-064a-464d-afdd-3c53f1c6dc19",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Given a size of vocabulary, do hash encoding (to save space)\n",
        "hashing_trick(sentence, n=100, hash_function=\"md5\")"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[51, 13, 19, 11, 7, 95, 51, 74, 33]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yhzRPwUxMs_-"
      },
      "source": [
        "> Tip: Using the Tokenizer API"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tP4D3xYbMtAA"
      },
      "source": [
        "from keras.preprocessing.text import Tokenizer"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9xSSfmK0MtAB"
      },
      "source": [
        "# Instantiate the Tokenizer\n",
        "simple_tokenizer = Tokenizer()"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nDjdGU48MtAD"
      },
      "source": [
        "# Fit the Tokenizer\n",
        "simple_tokenizer.fit_on_texts([sentence])"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4rs3IUXyMtAU"
      },
      "source": [
        "def get_sentence_vectors(sentences, tokenizer, mode=\"binary\"):\n",
        "    matrix = tokenizer.texts_to_matrix(sentences, mode=mode)\n",
        "    df = pd.DataFrame(matrix)\n",
        "    df.drop(columns=0, inplace=True)\n",
        "    df.columns = tokenizer.word_index\n",
        "    return df"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oCzgQYTYMtAE",
        "outputId": "bad34f45-bf44-482b-bd54-edfb368db5ab",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# See the word vectors\n",
        "simple_tokenizer.word_index"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'brown': 3,\n",
              " 'dog': 8,\n",
              " 'fox': 4,\n",
              " 'jumped': 5,\n",
              " 'lazy': 7,\n",
              " 'over': 6,\n",
              " 'quick': 2,\n",
              " 'the': 1}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kiJjAcjiMtAI"
      },
      "source": [
        "Normally we will be working with a set of text (like sentences), so it is better to use the tokenizer API"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zaJAHfcTMtAK"
      },
      "source": [
        "sentences = ['The quick brown fox jumped over the lazy dog', \n",
        "             'The dog woke up lazily and barked at the fox',\n",
        "             'The fox looked back and just ignored the dog']"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LZ_dESr1MtAL"
      },
      "source": [
        "# Instantiate and Fit\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(sentences)"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "95uSs146RREx",
        "outputId": "d93f0b4f-48ff-4bf2-b73b-4c70d4d78c06",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "tokenizer.word_index"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'and': 4,\n",
              " 'at': 14,\n",
              " 'back': 16,\n",
              " 'barked': 13,\n",
              " 'brown': 6,\n",
              " 'dog': 3,\n",
              " 'fox': 2,\n",
              " 'ignored': 18,\n",
              " 'jumped': 7,\n",
              " 'just': 17,\n",
              " 'lazily': 12,\n",
              " 'lazy': 9,\n",
              " 'looked': 15,\n",
              " 'over': 8,\n",
              " 'quick': 5,\n",
              " 'the': 1,\n",
              " 'up': 11,\n",
              " 'woke': 10}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-NOA9LArMtAN",
        "outputId": "9e43af8f-9583-4130-ed0d-d83314c17526",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "tokenizer.texts_to_sequences(sentences)"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[1, 5, 6, 2, 7, 8, 1, 9, 3],\n",
              " [1, 3, 10, 11, 12, 4, 13, 14, 1, 2],\n",
              " [1, 2, 15, 16, 4, 17, 18, 1, 3]]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d5j3-XLnMtAP",
        "outputId": "f0a57191-e564-4d98-b44e-6d1a401647af",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "tokenizer.texts_to_matrix(sentences, mode=\"binary\")"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0.],\n",
              "       [0., 1., 1., 1., 1., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 0.,\n",
              "        0., 0., 0.],\n",
              "       [0., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
              "        1., 1., 1.]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jgHR2qghMtAV",
        "outputId": "676773cf-e5f4-4c34-ada8-a46b496bf36f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 162
        }
      },
      "source": [
        "_x = get_sentence_vectors(sentences, tokenizer, mode=\"tfidf\")\n",
        "_x"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>the</th>\n",
              "      <th>fox</th>\n",
              "      <th>dog</th>\n",
              "      <th>and</th>\n",
              "      <th>quick</th>\n",
              "      <th>brown</th>\n",
              "      <th>jumped</th>\n",
              "      <th>over</th>\n",
              "      <th>lazy</th>\n",
              "      <th>woke</th>\n",
              "      <th>up</th>\n",
              "      <th>lazily</th>\n",
              "      <th>barked</th>\n",
              "      <th>at</th>\n",
              "      <th>looked</th>\n",
              "      <th>back</th>\n",
              "      <th>just</th>\n",
              "      <th>ignored</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.947512</td>\n",
              "      <td>0.559616</td>\n",
              "      <td>0.559616</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.916291</td>\n",
              "      <td>0.916291</td>\n",
              "      <td>0.916291</td>\n",
              "      <td>0.916291</td>\n",
              "      <td>0.916291</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.947512</td>\n",
              "      <td>0.559616</td>\n",
              "      <td>0.559616</td>\n",
              "      <td>0.693147</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.916291</td>\n",
              "      <td>0.916291</td>\n",
              "      <td>0.916291</td>\n",
              "      <td>0.916291</td>\n",
              "      <td>0.916291</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.947512</td>\n",
              "      <td>0.559616</td>\n",
              "      <td>0.559616</td>\n",
              "      <td>0.693147</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.916291</td>\n",
              "      <td>0.916291</td>\n",
              "      <td>0.916291</td>\n",
              "      <td>0.916291</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        the       fox       dog  ...      back      just   ignored\n",
              "0  0.947512  0.559616  0.559616  ...  0.000000  0.000000  0.000000\n",
              "1  0.947512  0.559616  0.559616  ...  0.000000  0.000000  0.000000\n",
              "2  0.947512  0.559616  0.559616  ...  0.916291  0.916291  0.916291\n",
              "\n",
              "[3 rows x 18 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a0xngigzVyod",
        "outputId": "0c6e2134-0e84-4f41-e248-ad978d2580d0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 359
        }
      },
      "source": [
        "_x = _x.rename_axis('sentence').reset_index().melt(id_vars=['sentence'])\n",
        "_x.head(10)"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentence</th>\n",
              "      <th>variable</th>\n",
              "      <th>value</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>the</td>\n",
              "      <td>0.947512</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>the</td>\n",
              "      <td>0.947512</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>the</td>\n",
              "      <td>0.947512</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>fox</td>\n",
              "      <td>0.559616</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>fox</td>\n",
              "      <td>0.559616</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>2</td>\n",
              "      <td>fox</td>\n",
              "      <td>0.559616</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>0</td>\n",
              "      <td>dog</td>\n",
              "      <td>0.559616</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>1</td>\n",
              "      <td>dog</td>\n",
              "      <td>0.559616</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>2</td>\n",
              "      <td>dog</td>\n",
              "      <td>0.559616</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>0</td>\n",
              "      <td>and</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   sentence variable     value\n",
              "0         0      the  0.947512\n",
              "1         1      the  0.947512\n",
              "2         2      the  0.947512\n",
              "3         0      fox  0.559616\n",
              "4         1      fox  0.559616\n",
              "5         2      fox  0.559616\n",
              "6         0      dog  0.559616\n",
              "7         1      dog  0.559616\n",
              "8         2      dog  0.559616\n",
              "9         0      and  0.000000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RpAjw1BJMtAY",
        "outputId": "322fbf82-3560-4434-97c4-a572ad8713ad",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 147
        }
      },
      "source": [
        "alt.Chart(_x).mark_rect().encode(\n",
        "    x=alt.X('variable:N', title=\"word\"),\n",
        "    y=alt.Y('sentence:N', title=\"sentence\"),\n",
        "    color=alt.Color('value:Q', title=\"tfidf\")\n",
        ").properties(\n",
        "    width=700\n",
        ").interactive()"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "alt.Chart(...)"
            ],
            "text/html": [
              "\n",
              "<div id=\"altair-viz-3caaa2dc73df4dad9aa4021cbcaffc17\"></div>\n",
              "<script type=\"text/javascript\">\n",
              "  (function(spec, embedOpt){\n",
              "    let outputDiv = document.currentScript.previousElementSibling;\n",
              "    if (outputDiv.id !== \"altair-viz-3caaa2dc73df4dad9aa4021cbcaffc17\") {\n",
              "      outputDiv = document.getElementById(\"altair-viz-3caaa2dc73df4dad9aa4021cbcaffc17\");\n",
              "    }\n",
              "    const paths = {\n",
              "      \"vega\": \"https://cdn.jsdelivr.net/npm//vega@5?noext\",\n",
              "      \"vega-lib\": \"https://cdn.jsdelivr.net/npm//vega-lib?noext\",\n",
              "      \"vega-lite\": \"https://cdn.jsdelivr.net/npm//vega-lite@4.8.1?noext\",\n",
              "      \"vega-embed\": \"https://cdn.jsdelivr.net/npm//vega-embed@6?noext\",\n",
              "    };\n",
              "\n",
              "    function loadScript(lib) {\n",
              "      return new Promise(function(resolve, reject) {\n",
              "        var s = document.createElement('script');\n",
              "        s.src = paths[lib];\n",
              "        s.async = true;\n",
              "        s.onload = () => resolve(paths[lib]);\n",
              "        s.onerror = () => reject(`Error loading script: ${paths[lib]}`);\n",
              "        document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
              "      });\n",
              "    }\n",
              "\n",
              "    function showError(err) {\n",
              "      outputDiv.innerHTML = `<div class=\"error\" style=\"color:red;\">${err}</div>`;\n",
              "      throw err;\n",
              "    }\n",
              "\n",
              "    function displayChart(vegaEmbed) {\n",
              "      vegaEmbed(outputDiv, spec, embedOpt)\n",
              "        .catch(err => showError(`Javascript Error: ${err.message}<br>This usually means there's a typo in your chart specification. See the javascript console for the full traceback.`));\n",
              "    }\n",
              "\n",
              "    if(typeof define === \"function\" && define.amd) {\n",
              "      requirejs.config({paths});\n",
              "      require([\"vega-embed\"], displayChart, err => showError(`Error loading script: ${err.message}`));\n",
              "    } else if (typeof vegaEmbed === \"function\") {\n",
              "      displayChart(vegaEmbed);\n",
              "    } else {\n",
              "      loadScript(\"vega\")\n",
              "        .then(() => loadScript(\"vega-lite\"))\n",
              "        .then(() => loadScript(\"vega-embed\"))\n",
              "        .catch(showError)\n",
              "        .then(() => displayChart(vegaEmbed));\n",
              "    }\n",
              "  })({\"config\": {\"view\": {\"continuousWidth\": 400, \"continuousHeight\": 300}}, \"data\": {\"name\": \"data-b81da020d8d16f30b951e3d78ec7f69f\"}, \"mark\": \"rect\", \"encoding\": {\"color\": {\"type\": \"quantitative\", \"field\": \"value\", \"title\": \"tfidf\"}, \"x\": {\"type\": \"nominal\", \"field\": \"variable\", \"title\": \"word\"}, \"y\": {\"type\": \"nominal\", \"field\": \"sentence\", \"title\": \"sentence\"}}, \"selection\": {\"selector007\": {\"type\": \"interval\", \"bind\": \"scales\", \"encodings\": [\"x\", \"y\"]}}, \"width\": 700, \"$schema\": \"https://vega.github.io/schema/vega-lite/v4.8.1.json\", \"datasets\": {\"data-b81da020d8d16f30b951e3d78ec7f69f\": [{\"sentence\": 0, \"variable\": \"the\", \"value\": 0.9475118935396932}, {\"sentence\": 1, \"variable\": \"the\", \"value\": 0.9475118935396932}, {\"sentence\": 2, \"variable\": \"the\", \"value\": 0.9475118935396932}, {\"sentence\": 0, \"variable\": \"fox\", \"value\": 0.5596157879354227}, {\"sentence\": 1, \"variable\": \"fox\", \"value\": 0.5596157879354227}, {\"sentence\": 2, \"variable\": \"fox\", \"value\": 0.5596157879354227}, {\"sentence\": 0, \"variable\": \"dog\", \"value\": 0.5596157879354227}, {\"sentence\": 1, \"variable\": \"dog\", \"value\": 0.5596157879354227}, {\"sentence\": 2, \"variable\": \"dog\", \"value\": 0.5596157879354227}, {\"sentence\": 0, \"variable\": \"and\", \"value\": 0.0}, {\"sentence\": 1, \"variable\": \"and\", \"value\": 0.6931471805599453}, {\"sentence\": 2, \"variable\": \"and\", \"value\": 0.6931471805599453}, {\"sentence\": 0, \"variable\": \"quick\", \"value\": 0.9162907318741551}, {\"sentence\": 1, \"variable\": \"quick\", \"value\": 0.0}, {\"sentence\": 2, \"variable\": \"quick\", \"value\": 0.0}, {\"sentence\": 0, \"variable\": \"brown\", \"value\": 0.9162907318741551}, {\"sentence\": 1, \"variable\": \"brown\", \"value\": 0.0}, {\"sentence\": 2, \"variable\": \"brown\", \"value\": 0.0}, {\"sentence\": 0, \"variable\": \"jumped\", \"value\": 0.9162907318741551}, {\"sentence\": 1, \"variable\": \"jumped\", \"value\": 0.0}, {\"sentence\": 2, \"variable\": \"jumped\", \"value\": 0.0}, {\"sentence\": 0, \"variable\": \"over\", \"value\": 0.9162907318741551}, {\"sentence\": 1, \"variable\": \"over\", \"value\": 0.0}, {\"sentence\": 2, \"variable\": \"over\", \"value\": 0.0}, {\"sentence\": 0, \"variable\": \"lazy\", \"value\": 0.9162907318741551}, {\"sentence\": 1, \"variable\": \"lazy\", \"value\": 0.0}, {\"sentence\": 2, \"variable\": \"lazy\", \"value\": 0.0}, {\"sentence\": 0, \"variable\": \"woke\", \"value\": 0.0}, {\"sentence\": 1, \"variable\": \"woke\", \"value\": 0.9162907318741551}, {\"sentence\": 2, \"variable\": \"woke\", \"value\": 0.0}, {\"sentence\": 0, \"variable\": \"up\", \"value\": 0.0}, {\"sentence\": 1, \"variable\": \"up\", \"value\": 0.9162907318741551}, {\"sentence\": 2, \"variable\": \"up\", \"value\": 0.0}, {\"sentence\": 0, \"variable\": \"lazily\", \"value\": 0.0}, {\"sentence\": 1, \"variable\": \"lazily\", \"value\": 0.9162907318741551}, {\"sentence\": 2, \"variable\": \"lazily\", \"value\": 0.0}, {\"sentence\": 0, \"variable\": \"barked\", \"value\": 0.0}, {\"sentence\": 1, \"variable\": \"barked\", \"value\": 0.9162907318741551}, {\"sentence\": 2, \"variable\": \"barked\", \"value\": 0.0}, {\"sentence\": 0, \"variable\": \"at\", \"value\": 0.0}, {\"sentence\": 1, \"variable\": \"at\", \"value\": 0.9162907318741551}, {\"sentence\": 2, \"variable\": \"at\", \"value\": 0.0}, {\"sentence\": 0, \"variable\": \"looked\", \"value\": 0.0}, {\"sentence\": 1, \"variable\": \"looked\", \"value\": 0.0}, {\"sentence\": 2, \"variable\": \"looked\", \"value\": 0.9162907318741551}, {\"sentence\": 0, \"variable\": \"back\", \"value\": 0.0}, {\"sentence\": 1, \"variable\": \"back\", \"value\": 0.0}, {\"sentence\": 2, \"variable\": \"back\", \"value\": 0.9162907318741551}, {\"sentence\": 0, \"variable\": \"just\", \"value\": 0.0}, {\"sentence\": 1, \"variable\": \"just\", \"value\": 0.0}, {\"sentence\": 2, \"variable\": \"just\", \"value\": 0.9162907318741551}, {\"sentence\": 0, \"variable\": \"ignored\", \"value\": 0.0}, {\"sentence\": 1, \"variable\": \"ignored\", \"value\": 0.0}, {\"sentence\": 2, \"variable\": \"ignored\", \"value\": 0.9162907318741551}]}}, {\"mode\": \"vega-lite\"});\n",
              "</script>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FxSiN_LsMtAb",
        "outputId": "0186d431-6704-4d64-93f6-a7514db1f1a9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "one_hot_results = tokenizer.texts_to_matrix(sentence, mode='binary')\n",
        "\n",
        "word_index = tokenizer.word_index\n",
        "print('Found %s unique tokens.' % len(word_index))"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 18 unique tokens.\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}