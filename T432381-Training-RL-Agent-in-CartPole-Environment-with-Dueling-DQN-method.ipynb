{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"T432381 | Training RL Agent in CartPole Environment with Dueling DQN method","provenance":[{"file_id":"11_5lxJoi19PUPuTzS5RBlSitYErAUY3h","timestamp":1638447606203}],"collapsed_sections":[],"authorship_tag":"ABX9TyP29ArM7Qz2Un3YlEu/0oCC"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"tyMytwWZFUhp"},"source":["# Training RL Agent in CartPole Environment with Dueling DQN method"]},{"cell_type":"code","metadata":{"id":"ysNR-nNZEHYs","executionInfo":{"status":"ok","timestamp":1638447818229,"user_tz":-330,"elapsed":464,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}}},"source":["import argparse\n","from datetime import datetime\n","import os\n","import random\n","from collections import deque\n","\n","import gym\n","import numpy as np\n","import tensorflow as tf\n","from tensorflow.keras.layers import Add, Dense, Input\n","from tensorflow.keras.optimizers import Adam"],"execution_count":10,"outputs":[]},{"cell_type":"code","metadata":{"id":"doMRsX9OEImC","executionInfo":{"status":"ok","timestamp":1638447736367,"user_tz":-330,"elapsed":13,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}}},"source":["tf.keras.backend.set_floatx(\"float64\")"],"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dCLXisl6ELQ0","executionInfo":{"status":"ok","timestamp":1638447749958,"user_tz":-330,"elapsed":447,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}},"outputId":"17957169-0377-456f-fdc7-b81bbb44a0cc"},"source":["parser = argparse.ArgumentParser()\n","parser.add_argument(\"--env\", default=\"CartPole-v0\")\n","parser.add_argument(\"--lr\", type=float, default=0.005)\n","parser.add_argument(\"--batch_size\", type=int, default=64)\n","parser.add_argument(\"--gamma\", type=float, default=0.95)\n","parser.add_argument(\"--eps\", type=float, default=1.0)\n","parser.add_argument(\"--eps_decay\", type=float, default=0.995)\n","parser.add_argument(\"--eps_min\", type=float, default=0.01)\n","parser.add_argument(\"--logdir\", default=\"logs\")\n","\n","args = parser.parse_args([])\n","logdir = os.path.join(\n","    args.logdir, parser.prog, args.env, datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",")\n","print(f\"Saving training logs to:{logdir}\")\n","writer = tf.summary.create_file_writer(logdir)"],"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Saving training logs to:logs/ipykernel_launcher.py/CartPole-v0/20211202-122232\n"]}]},{"cell_type":"code","metadata":{"id":"xFoB1mz4EZHC","executionInfo":{"status":"ok","timestamp":1638447763919,"user_tz":-330,"elapsed":415,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}}},"source":["class ReplayBuffer:\n","    def __init__(self, capacity=10000):\n","        self.buffer = deque(maxlen=capacity)\n","\n","    def put(self, state, action, reward, next_state, done):\n","        self.buffer.append([state, action, reward, next_state, done])\n","\n","    def sample(self):\n","        sample = random.sample(self.buffer, args.batch_size)\n","        states, actions, rewards, next_states, done = map(np.asarray, zip(*sample))\n","        states = np.array(states).reshape(args.batch_size, -1)\n","        next_states = np.array(next_states).reshape(args.batch_size, -1)\n","        return states, actions, rewards, next_states, done\n","\n","    def size(self):\n","        return len(self.buffer)"],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"id":"c3BmqKjDEatV","executionInfo":{"status":"ok","timestamp":1638447780914,"user_tz":-330,"elapsed":457,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}}},"source":["class DuelingDQN:\n","    def __init__(self, state_dim, aciton_dim):\n","        self.state_dim = state_dim\n","        self.action_dim = aciton_dim\n","        self.epsilon = args.eps\n","\n","        self.model = self.nn_model()\n","\n","    def nn_model(self):\n","        backbone = tf.keras.Sequential(\n","            [\n","                Input((self.state_dim,)),\n","                Dense(32, activation=\"relu\"),\n","                Dense(16, activation=\"relu\"),\n","            ]\n","        )\n","        state_input = Input((self.state_dim,))\n","        backbone_1 = Dense(32, activation=\"relu\")(state_input)\n","        backbone_2 = Dense(16, activation=\"relu\")(backbone_1)\n","        value_output = Dense(1)(backbone_2)\n","        advantage_output = Dense(self.action_dim)(backbone_2)\n","        output = Add()([value_output, advantage_output])\n","        model = tf.keras.Model(state_input, output)\n","        model.compile(loss=\"mse\", optimizer=Adam(args.lr))\n","        return model\n","\n","    def predict(self, state):\n","        return self.model.predict(state)\n","\n","    def get_action(self, state):\n","        state = np.reshape(state, [1, self.state_dim])\n","        self.epsilon *= args.eps_decay\n","        self.epsilon = max(self.epsilon, args.eps_min)\n","        q_value = self.predict(state)[0]\n","        if np.random.random() < self.epsilon:\n","            return random.randint(0, self.action_dim - 1)\n","        return np.argmax(q_value)\n","\n","    def train(self, states, targets):\n","        self.model.fit(states, targets, epochs=1, verbose=0)"],"execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"id":"PQZ6EpazGSub","executionInfo":{"status":"ok","timestamp":1638447789684,"user_tz":-330,"elapsed":437,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}}},"source":["class Agent:\n","    def __init__(self, env):\n","        self.env = env\n","        self.state_dim = self.env.observation_space.shape[0]\n","        self.action_dim = self.env.action_space.n\n","\n","        self.model = DuelingDQN(self.state_dim, self.action_dim)\n","        self.target_model = DuelingDQN(self.state_dim, self.action_dim)\n","        self.update_target()\n","\n","        self.buffer = ReplayBuffer()\n","\n","    def update_target(self):\n","        weights = self.model.model.get_weights()\n","        self.target_model.model.set_weights(weights)\n","\n","    def replay_experience(self):\n","        for _ in range(10):\n","            states, actions, rewards, next_states, done = self.buffer.sample()\n","            targets = self.model.predict(states)\n","            next_q_values = self.target_model.predict(next_states).max(axis=1)\n","            targets[range(args.batch_size), actions] = (\n","                rewards + (1 - done) * next_q_values * args.gamma\n","            )\n","            self.model.train(states, targets)\n","\n","    def train(self, max_episodes=1000):\n","        with writer.as_default():\n","            for ep in range(max_episodes):\n","                done, episode_reward = False, 0\n","                state = self.env.reset()\n","                while not done:\n","                    action = self.model.get_action(state)\n","                    next_state, reward, done, _ = self.env.step(action)\n","                    self.buffer.put(state, action, reward * 0.01, next_state, done)\n","                    episode_reward += reward\n","                    state = next_state\n","\n","                if self.buffer.size() >= args.batch_size:\n","                    self.replay_experience()\n","                self.update_target()\n","                print(f\"Episode#{ep} Reward:{episode_reward}\")\n","                tf.summary.scalar(\"episode_reward\", episode_reward, step=ep)"],"execution_count":8,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sw6rePMLEcxu","executionInfo":{"status":"ok","timestamp":1638447847354,"user_tz":-330,"elapsed":25638,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}},"outputId":"078e32e1-e3f9-473e-b910-ba1c6e77db79"},"source":["if __name__ == \"__main__\":\n","    env = gym.make(\"CartPole-v0\")\n","    agent = Agent(env)\n","    agent.train(max_episodes=10)  # Increase max_episodes value"],"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["Episode#0 Reward:16.0\n","Episode#1 Reward:19.0\n","Episode#2 Reward:28.0\n","Episode#3 Reward:62.0\n","Episode#4 Reward:60.0\n","Episode#5 Reward:23.0\n","Episode#6 Reward:15.0\n","Episode#7 Reward:60.0\n","Episode#8 Reward:20.0\n","Episode#9 Reward:25.0\n"]}]},{"cell_type":"markdown","metadata":{"id":"ixk34zYIEgB_"},"source":["---"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VDwjGZWIE5il","executionInfo":{"status":"ok","timestamp":1638447857111,"user_tz":-330,"elapsed":3429,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}},"outputId":"debec7e8-fac4-41b9-b9dd-b661673ac3cc"},"source":["!pip install -q watermark\n","%reload_ext watermark\n","%watermark -a \"Sparsh A.\" -m -iv -u -t -d"],"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["Author: Sparsh A.\n","\n","Last updated: 2021-12-02 12:24:19\n","\n","Compiler    : GCC 7.5.0\n","OS          : Linux\n","Release     : 5.4.104+\n","Machine     : x86_64\n","Processor   : x86_64\n","CPU cores   : 2\n","Architecture: 64bit\n","\n","tensorflow: 2.7.0\n","numpy     : 1.19.5\n","argparse  : 1.1\n","IPython   : 5.5.0\n","gym       : 0.17.3\n","\n"]}]},{"cell_type":"markdown","metadata":{"id":"haKsOAX2E1XI"},"source":["---"]},{"cell_type":"markdown","metadata":{"id":"4s5DJf8WE2as"},"source":["**END**"]}]}