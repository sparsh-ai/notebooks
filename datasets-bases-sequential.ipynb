{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp datasets.bases.sequential"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequential Base Dataset\n",
    "> Implementation of sequential base dataset modules in Pytorch Lightning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev.showdoc import *\n",
    "from fastcore.nb_imports import *\n",
    "from fastcore.test import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from typing import Any, Iterable, List, Optional, Tuple, Union, Callable\n",
    "\n",
    "import random\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.data import RandomSampler, SequentialSampler\n",
    "\n",
    "from pytorch_lightning import LightningDataModule\n",
    "\n",
    "from recohut.utils.common_utils import *\n",
    "from recohut.datasets.bases.common import Dataset as BaseDataset\n",
    "\n",
    "from recohut.utils.grouping import create_user_sequences\n",
    "from recohut.utils.matrix import generate_rating_matrix\n",
    "from recohut.utils.negative_sampling import random_neg_sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SequentialDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class SequentialDataset(Dataset, BaseDataset):\n",
    "    def __init__(self,\n",
    "                 data_dir,\n",
    "                 data_type='train',\n",
    "                 history_size=8,\n",
    "                 step_size=1,\n",
    "                 seed=42,\n",
    "                 mask=1,\n",
    "                 *args,\n",
    "                 **kwargs):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            data_dir: Where to save/load the data\n",
    "            data_type: train/valid/test\n",
    "        \"\"\"\n",
    "        self.data_type = data_type\n",
    "        self.history_size = history_size\n",
    "        self.step_size = step_size\n",
    "        self.seed = seed\n",
    "        self.mask = mask\n",
    "\n",
    "        super().__init__(data_dir)\n",
    "\n",
    "        self._process()\n",
    "\n",
    "    @property\n",
    "    def raw_file_names(self):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    @property\n",
    "    def processed_file_names(self):\n",
    "        return ['data.pt']\n",
    "\n",
    "    def download(self):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def load_ratings_df(self):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def map_column(self, df: pd.DataFrame, col_name: str):\n",
    "        \"\"\"Maps column values to integers.\n",
    "        \"\"\"\n",
    "        values = sorted(list(df[col_name].unique()))\n",
    "        mapping = {k: i + 2 for i, k in enumerate(values)}\n",
    "        inverse_mapping = {v: k for k, v in mapping.items()}\n",
    "        df[col_name + \"_mapped\"] = df[col_name].map(mapping)\n",
    "        return df, mapping, inverse_mapping\n",
    "\n",
    "    def get_context(self, df: pd.DataFrame, split: str, context_size: int = 120, val_context_size: int = 5, seed: int = 42):\n",
    "        \"\"\"Create a training / validation samples.\n",
    "        \"\"\"\n",
    "        random.seed(seed)\n",
    "        if split == \"train\":\n",
    "            end_index = random.randint(10, df.shape[0] - val_context_size)\n",
    "        elif split in [\"valid\", \"test\"]:\n",
    "            end_index = df.shape[0]\n",
    "        else:\n",
    "            raise ValueError\n",
    "        start_index = max(0, end_index - context_size)\n",
    "        context = df[start_index:end_index]\n",
    "        return context\n",
    "\n",
    "    def pad_list(self, list_integers, history_size: int, pad_val: int = 0, mode=\"left\"):\n",
    "        \"\"\"Pad list from left or right\n",
    "        \"\"\"\n",
    "        if len(list_integers) < history_size:\n",
    "            if mode == \"left\":\n",
    "                list_integers = [pad_val] * (history_size - len(list_integers)) + list_integers\n",
    "            else:\n",
    "                list_integers = list_integers + [pad_val] * (history_size - len(list_integers))\n",
    "        return list_integers\n",
    "\n",
    "    def mask_list(self, l1, p=0.8):\n",
    "        random.seed(self.seed)\n",
    "        l1 = [a if random.random() < p else self.mask for a in l1]\n",
    "        return l1\n",
    "\n",
    "    def mask_last_elements_list(self, l1, val_context_size: int = 5):\n",
    "        l1 = l1[:-val_context_size] + self.mask_list(l1[-val_context_size:], p=0.5)\n",
    "        return l1\n",
    "\n",
    "    def make_user_history(self, data):\n",
    "        user_history = [ [] for _ in range(self.num_users) ]\n",
    "        for u, i, r in data: user_history[u].append(i)\n",
    "        return user_history\n",
    "\n",
    "    # def pad(self, arr, max_len = None, pad_with = -1, side = 'right'):\n",
    "    #     seq_len = max_len if max_len is not None else max(map(len, arr))\n",
    "    #     seq_len = min(seq_len, 200) # You don't need more than this\n",
    "\n",
    "    #     for i in range(len(arr)):\n",
    "    #         while len(arr[i]) < seq_len: \n",
    "    #             pad_elem = arr[i][-1] if len(arr[i]) > 0 else 0\n",
    "    #             pad_elem = pad_elem if pad_with == -1 else pad_with\n",
    "    #             if side == 'right': arr[i].append(pad_elem)\n",
    "    #             else: arr[i] = [ pad_elem ] + arr[i]\n",
    "    #         arr[i] = arr[i][-seq_len:] # Keep last `seq_len` items\n",
    "    #     return arr\n",
    "\n",
    "    # def sequential_pad(self, arr, max_seq_len, total_items):\n",
    "    #     # Padding left side so that we can simply take out [:, -1, :] in the output\n",
    "    #     return self.pad(\n",
    "    #         arr, max_len = max_seq_len, \n",
    "    #         pad_with = total_items, side = 'left'\n",
    "    #     )\n",
    "\n",
    "    # def scatter(self, batch, tensor_kind, last_dimension):\n",
    "    #     ret = tensor_kind(len(batch), last_dimension).zero_()\n",
    "\n",
    "    #     if not torch.is_tensor(batch):\n",
    "    #         if ret.is_cuda: batch = torch.cuda.LongTensor(batch)\n",
    "    #         else: batch = torch.LongTensor(batch)\n",
    "\n",
    "    #     return ret.scatter_(1, batch, 1)\n",
    "\n",
    "    # def get_item_count_map(self, data):\n",
    "    #     item_count = defaultdict(int)\n",
    "    #     for u, i, r in data: item_count[i] += 1\n",
    "    #     return item_count\n",
    "\n",
    "    # def get_item_propensity(self, data, num_items, A = 0.55, B = 1.5):\n",
    "    #     item_freq_map = self.get_item_count_map()\n",
    "    #     item_freq = [ item_freq_map[i] for i in range(num_items) ]\n",
    "    #     num_instances = len(data)\n",
    "\n",
    "    #     C = (np.log(num_instances)-1)*np.power(B+1, A)\n",
    "    #     wts = 1.0 + C*np.power(np.array(item_freq)+B, -A)\n",
    "    #     return np.ravel(wts)\n",
    "\n",
    "    def create_sequences(self, values, window_size, step_size):\n",
    "        sequences = []\n",
    "        start_index = 0\n",
    "        while True:\n",
    "            end_index = start_index + window_size\n",
    "            seq = values[start_index:end_index]\n",
    "            if len(seq) < window_size:\n",
    "                seq = values[-window_size:]\n",
    "                if len(seq) == window_size:\n",
    "                    sequences.append(seq)\n",
    "                break\n",
    "            sequences.append(seq)\n",
    "            start_index += step_size\n",
    "        return sequences\n",
    "\n",
    "    def process(self):\n",
    "        df = self.load_ratings_df()\n",
    "        df.sort_values(by=\"timestamp\", inplace=True)\n",
    "        df, self.mapping, self.inverse_mapping = self.map_column(df, col_name=\"sid\")\n",
    "        self.grp_by = df.groupby(by=\"uid\")\n",
    "        self.groups = list(self.grp_by.groups)\n",
    "\n",
    "    def __len__(self):\n",
    "            return len(self.groups)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        group = self.groups[index]\n",
    "        df = self.grp_by.get_group(group)\n",
    "        context = self.get_context(df, split=self.data_type, context_size=self.history_size)\n",
    "        trg_items = context[\"sid_mapped\"].tolist()\n",
    "        if self.data_type == \"train\":\n",
    "            src_items = self.mask_list(trg_items)\n",
    "        else:\n",
    "            src_items = self.mask_last_elements_list(trg_items)\n",
    "        pad_mode = \"left\" if random.random() < 0.5 else \"right\"\n",
    "        trg_items = self.pad_list(trg_items, history_size=self.history_size, mode=pad_mode)\n",
    "        src_items = self.pad_list(src_items, history_size=self.history_size, mode=pad_mode)\n",
    "        src_items = torch.tensor(src_items, dtype=torch.long)\n",
    "        trg_items = torch.tensor(trg_items, dtype=torch.long)\n",
    "        return src_items, trg_items"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SequentialDataModule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class SequentialDataModule(LightningDataModule):\n",
    "\n",
    "    dataset_cls: str = \"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 data_dir: Optional[str] = None,\n",
    "                 num_workers: int = 0,\n",
    "                 normalize: bool = False,\n",
    "                 batch_size: int = 32,\n",
    "                 shuffle: bool = True,\n",
    "                 pin_memory: bool = True,\n",
    "                 drop_last: bool = False,\n",
    "                 *args, \n",
    "                 **kwargs) -> None:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            data_dir: Where to save/load the data\n",
    "            num_workers: How many workers to use for loading data\n",
    "            normalize: If true applies rating normalize\n",
    "            batch_size: How many samples per batch to load\n",
    "            shuffle: If true shuffles the train data every epoch\n",
    "            pin_memory: If true, the data loader will copy Tensors into CUDA pinned memory before\n",
    "                        returning them\n",
    "            drop_last: If true drops the last incomplete batch\n",
    "        \"\"\"\n",
    "        super().__init__(data_dir)\n",
    "\n",
    "        self.data_dir = data_dir if data_dir is not None else os.getcwd()\n",
    "        self.num_workers = num_workers\n",
    "        self.normalize = normalize\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.pin_memory = pin_memory\n",
    "        self.drop_last = drop_last\n",
    "        self.kwargs = kwargs\n",
    "\n",
    "    def prepare_data(self, *args: Any, **kwargs: Any) -> None:\n",
    "        \"\"\"Saves files to data_dir.\"\"\"\n",
    "        self.data = self.dataset_cls(self.data_dir, **self.kwargs)\n",
    "\n",
    "    def setup(self, stage: Optional[str] = None) -> None:\n",
    "        \"\"\"Creates train, val, and test dataset.\"\"\"\n",
    "        if stage == \"fit\" or stage is None:\n",
    "            self.dataset_train = self.dataset_cls(self.data_dir, data_type='train', **self.kwargs)\n",
    "            self.dataset_val = self.dataset_cls(self.data_dir, data_type='valid', **self.kwargs)\n",
    "        if stage == \"test\" or stage is None:\n",
    "            self.dataset_test = self.dataset_cls(self.data_dir, data_type='test', **self.kwargs)\n",
    "\n",
    "    def train_dataloader(self, *args: Any, **kwargs: Any) -> DataLoader:\n",
    "        \"\"\"The train dataloader.\"\"\"\n",
    "        return self._data_loader(self.dataset_train, shuffle=self.shuffle)\n",
    "\n",
    "    def val_dataloader(self, *args: Any, **kwargs: Any) -> Union[DataLoader, List[DataLoader]]:\n",
    "        \"\"\"The val dataloader.\"\"\"\n",
    "        return self._data_loader(self.dataset_val)\n",
    "\n",
    "    def test_dataloader(self, *args: Any, **kwargs: Any) -> Union[DataLoader, List[DataLoader]]:\n",
    "        \"\"\"The test dataloader.\"\"\"\n",
    "        return self._data_loader(self.dataset_test)\n",
    "\n",
    "    def _data_loader(self, dataset: Dataset, shuffle: bool = False) -> DataLoader:\n",
    "        return DataLoader(\n",
    "            dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=shuffle,\n",
    "            num_workers=self.num_workers,\n",
    "            drop_last=self.drop_last,\n",
    "            pin_memory=self.pin_memory,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ML1mDataset(SequentialDataset):\n",
    "    url = \"http://files.grouplens.org/datasets/movielens/ml-1m.zip\"\n",
    "\n",
    "    @property\n",
    "    def raw_file_names(self):\n",
    "        return 'ratings.dat'\n",
    "\n",
    "    def download(self):\n",
    "        path = download_url(self.url, self.raw_dir)\n",
    "        extract_zip(path, self.raw_dir)\n",
    "        from shutil import move, rmtree\n",
    "        move(os.path.join(self.raw_dir, 'ml-1m', self.raw_file_names), self.raw_dir)\n",
    "        rmtree(os.path.join(self.raw_dir, 'ml-1m'))\n",
    "        os.unlink(path)\n",
    "\n",
    "    def load_ratings_df(self):\n",
    "        df = pd.read_csv(self.raw_paths[0], sep='::', header=None, engine='python')\n",
    "        df.columns = ['uid', 'sid', 'rating', 'timestamp']\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ML1mDataModule(SequentialDataModule):\n",
    "    dataset_cls = ML1mDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args:\n",
    "    def __init__(self):\n",
    "        self.pad = 0\n",
    "        self.mask = 1\n",
    "        self.cap = 0\n",
    "        self.seed = 42\n",
    "        self.vocab_size = 10000\n",
    "        self.channels = 128\n",
    "        self.dropout = 0.4\n",
    "        self.learning_rate = 1e-4\n",
    "        self.history_size = 30\n",
    "        self.data_dir = '/content/data'\n",
    "        self.log_dir = '/content/recommender_logs'\n",
    "        self.model_dir = '/content/recommender_models'\n",
    "        self.batch_size = 32\n",
    "        self.shuffle = True\n",
    "        self.max_epochs = 2\n",
    "        self.val_epoch = 1\n",
    "        self.gpus = None\n",
    "        self.monitor = 'valid_loss'\n",
    "        self.mode = 'min'\n",
    "\n",
    "args = Args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = ML1mDataModule(data_sir=args.data_dir, **args.__dict__)\n",
    "ds.prepare_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SASRecDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class SASRecDataset(Dataset, BaseDataset):\n",
    "    \n",
    "    def __init__(self,\n",
    "                 data_dir,\n",
    "                 data_type='train',\n",
    "                 test_neg_items=None,\n",
    "                 min_seq_length=10,\n",
    "                 max_seq_length=50,\n",
    "                 sample_frac=None,\n",
    "                 *args,\n",
    "                 **kwargs):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            data_dir: Where to save/load the data\n",
    "            data_type: train/valid/test\n",
    "        \"\"\"\n",
    "        self.data_type = data_type\n",
    "        self.test_neg_items = test_neg_items\n",
    "        self.min_len = min_seq_length\n",
    "        self.max_len = max_seq_length\n",
    "        self.sample_frac = sample_frac\n",
    "\n",
    "        assert self.data_type in {\"train\", \"valid\", \"test\"}\n",
    "\n",
    "        super().__init__(data_dir)\n",
    "\n",
    "        self._process()\n",
    "        self.load_processed()\n",
    "\n",
    "        self.item_size = self.max_item + 2\n",
    "\n",
    "    @property\n",
    "    def raw_file_names(self):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    @property\n",
    "    def processed_file_names(self):\n",
    "        return ['user_seqs.txt', 'user_seqs.pkl']\n",
    "\n",
    "    def download(self):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def load_ratings_df(self):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def get_user_seqs(self):\n",
    "        lines = open(self.processed_paths[0]).readlines()\n",
    "        user_seq = []\n",
    "        item_set = set()\n",
    "        for line in lines:\n",
    "            user, items = line.strip().split(' ', 1)\n",
    "            items = items.split(' ')\n",
    "            items = [int(item) for item in items]\n",
    "            user_seq.append(items)\n",
    "            item_set = item_set | set(items)\n",
    "        max_item = max(item_set)\n",
    "\n",
    "        num_users = len(lines)\n",
    "        num_items = max_item + 2\n",
    "\n",
    "        valid_rating_matrix = generate_rating_matrix(user_seq, num_users, num_items, n=2)\n",
    "        test_rating_matrix = generate_rating_matrix(user_seq, num_users, num_items, n=1)\n",
    "        output = {\n",
    "            'user_seq': user_seq,\n",
    "            'max_item': max_item,\n",
    "            'valid_rating_matrix': valid_rating_matrix,\n",
    "            'test_rating_matrix': test_rating_matrix,\n",
    "            'num_users': num_users,\n",
    "        }\n",
    "        return output\n",
    "\n",
    "    def process(self):\n",
    "        if not os.path.exists(self.processed_paths[0]):\n",
    "            df = self.load_ratings_df()\n",
    "            # random sample\n",
    "            if self.sample_frac is not None:\n",
    "                df = df.sample(frac=self.sample_frac)\n",
    "            # filter based on item sequence length for user\n",
    "            seq_len = df.groupby('uid').size()\n",
    "            df = df[np.in1d(df.uid, seq_len[seq_len >= self.min_len].index)]\n",
    "            create_user_sequences(df, save_path=self.processed_paths[0],\n",
    "                                  user_col='uid', item_col='sid', ts_col='timestamp')\n",
    "        user_seqs = self.get_user_seqs()\n",
    "        with open(self.processed_paths[1], 'wb') as f:\n",
    "            pickle.dump(user_seqs, f)\n",
    "\n",
    "    def load_processed(self):\n",
    "        processed_data = pickle.load(open(self.processed_paths[1], 'rb'))\n",
    "        self.user_seq = processed_data['user_seq']\n",
    "        self.max_item = processed_data['max_item']\n",
    "        self.valid_rating_matrix = processed_data['valid_rating_matrix']\n",
    "        self.test_rating_matrix = processed_data['test_rating_matrix']\n",
    "        self.num_users = processed_data['num_users']\n",
    "        del processed_data\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        user_id = index\n",
    "        items = self.user_seq[index]\n",
    "\n",
    "        # [0, 1, 2, 3, 4, 5, 6]\n",
    "        # train [0, 1, 2, 3]\n",
    "        # target [1, 2, 3, 4]\n",
    "\n",
    "        # valid [0, 1, 2, 3, 4]\n",
    "        # answer [5]\n",
    "\n",
    "        # test [0, 1, 2, 3, 4, 5]\n",
    "        # answer [6]\n",
    "        if self.data_type == \"train\":\n",
    "            input_ids = items[:-3]\n",
    "            target_pos = items[1:-2]\n",
    "            answer = [0] # no use\n",
    "\n",
    "        elif self.data_type == 'valid':\n",
    "            input_ids = items[:-2]\n",
    "            target_pos = items[1:-1]\n",
    "            answer = [items[-2]]\n",
    "\n",
    "        else:\n",
    "            input_ids = items[:-1]\n",
    "            target_pos = items[1:]\n",
    "            answer = [items[-1]]\n",
    "\n",
    "        target_neg = []\n",
    "        seq_set = set(items)\n",
    "        for _ in input_ids:\n",
    "            target_neg.append(random_neg_sample(seq_set, self.item_size))\n",
    "\n",
    "        pad_len = self.max_len - len(input_ids)\n",
    "        input_ids = [0] * pad_len + input_ids\n",
    "        target_pos = [0] * pad_len + target_pos\n",
    "        target_neg = [0] * pad_len + target_neg\n",
    "\n",
    "        input_ids = input_ids[-self.max_len:]\n",
    "        target_pos = target_pos[-self.max_len:]\n",
    "        target_neg = target_neg[-self.max_len:]\n",
    "\n",
    "        assert len(input_ids) == self.max_len\n",
    "        assert len(target_pos) == self.max_len\n",
    "        assert len(target_neg) == self.max_len\n",
    "\n",
    "        if self.test_neg_items is not None:\n",
    "            test_samples = self.test_neg_items[index]\n",
    "\n",
    "            cur_tensors = (\n",
    "                torch.tensor(user_id, dtype=torch.long), # user_id for testing\n",
    "                torch.tensor(input_ids, dtype=torch.long),\n",
    "                torch.tensor(target_pos, dtype=torch.long),\n",
    "                torch.tensor(target_neg, dtype=torch.long),\n",
    "                torch.tensor(answer, dtype=torch.long),\n",
    "                torch.tensor(test_samples, dtype=torch.long),\n",
    "            )\n",
    "        else:\n",
    "            cur_tensors = (\n",
    "                torch.tensor(user_id, dtype=torch.long),  # user_id for testing\n",
    "                torch.tensor(input_ids, dtype=torch.long),\n",
    "                torch.tensor(target_pos, dtype=torch.long),\n",
    "                torch.tensor(target_neg, dtype=torch.long),\n",
    "                torch.tensor(answer, dtype=torch.long),\n",
    "            )\n",
    "\n",
    "        return cur_tensors\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.user_seq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SASRecDataModule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class SASRecDataModule(LightningDataModule):\n",
    "\n",
    "    dataset_cls = None\n",
    "\n",
    "    def __init__(self,\n",
    "                 data_dir: Optional[str] = None,\n",
    "                 num_workers: int = 0,\n",
    "                 batch_size: int = 32,\n",
    "                 pin_memory: bool = True,\n",
    "                 drop_last: bool = False,\n",
    "                 *args, \n",
    "                 **kwargs) -> None:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            data_dir: Where to save/load the data\n",
    "            num_workers: How many workers to use for loading data\n",
    "            batch_size: How many samples per batch to load\n",
    "            pin_memory: If true, the data loader will copy Tensors into CUDA pinned memory before\n",
    "                        returning them\n",
    "            drop_last: If true drops the last incomplete batch\n",
    "        \"\"\"\n",
    "        super().__init__(data_dir)\n",
    "\n",
    "        self.data_dir = data_dir if data_dir is not None else os.getcwd()\n",
    "        self.num_workers = num_workers\n",
    "        self.batch_size = batch_size\n",
    "        self.pin_memory = pin_memory\n",
    "        self.drop_last = drop_last\n",
    "        self.kwargs = kwargs\n",
    "\n",
    "    def prepare_data(self, *args: Any, **kwargs: Any) -> None:\n",
    "        \"\"\"Saves files to data_dir.\"\"\"\n",
    "        self.data = self.dataset_cls(self.data_dir, **self.kwargs)\n",
    "\n",
    "    def setup(self, stage: Optional[str] = None) -> None:\n",
    "        \"\"\"Creates train, val, and test dataset.\"\"\"\n",
    "        if stage == \"fit\" or stage is None:\n",
    "            self.dataset_train = self.dataset_cls(self.data_dir, data_type='train', **self.kwargs)\n",
    "            self.dataset_val = self.dataset_cls(self.data_dir, data_type='valid', **self.kwargs)\n",
    "        if stage == \"test\" or stage is None:\n",
    "            self.dataset_test = self.dataset_cls(self.data_dir, data_type='test', **self.kwargs)\n",
    "\n",
    "    def train_dataloader(self, *args: Any, **kwargs: Any) -> DataLoader:\n",
    "        \"\"\"The train dataloader.\"\"\"\n",
    "        train_sampler = RandomSampler(self.dataset_train)\n",
    "        return self._data_loader(self.dataset_train, sampler=train_sampler)\n",
    "\n",
    "    def val_dataloader(self, *args: Any, **kwargs: Any) -> Union[DataLoader, List[DataLoader]]:\n",
    "        \"\"\"The val dataloader.\"\"\"\n",
    "        val_sampler = SequentialSampler(self.dataset_val)\n",
    "        return self._data_loader(self.dataset_val, sampler=val_sampler)\n",
    "\n",
    "    def test_dataloader(self, *args: Any, **kwargs: Any) -> Union[DataLoader, List[DataLoader]]:\n",
    "        \"\"\"The test dataloader.\"\"\"\n",
    "        test_sampler = SequentialSampler(self.dataset_test)\n",
    "        return self._data_loader(self.dataset_test, sampler=test_sampler)\n",
    "\n",
    "    def _data_loader(self, dataset: Dataset, sampler) -> DataLoader:\n",
    "        return DataLoader(\n",
    "            dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=False,\n",
    "            sampler=sampler,\n",
    "            num_workers=self.num_workers,\n",
    "            drop_last=self.drop_last,\n",
    "            pin_memory=self.pin_memory,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AmazonBeautyDataset(SASRecDataset):\n",
    "\n",
    "    url = 'https://github.com/RecoHut-Datasets/amazon_beauty/raw/v1/amazon-ratings.zip'\n",
    "\n",
    "    @property\n",
    "    def raw_file_names(self):\n",
    "        return 'ratings_Beauty.csv'\n",
    "\n",
    "    def download(self):\n",
    "        path = download_url(self.url, self.raw_dir)\n",
    "        extract_zip(path, self.raw_dir)\n",
    "        os.unlink(path)\n",
    "\n",
    "    def load_ratings_df(self):\n",
    "        df = pd.read_csv(self.raw_paths[0])\n",
    "        df.columns = ['uid', 'sid', 'rating', 'timestamp']\n",
    "        # drop duplicate user-item pair records, keeping latest rating only\n",
    "        df.drop_duplicates(subset=['uid', 'sid'], keep='last', inplace=True)\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AmazonBeautyDataModule(SASRecDataModule):\n",
    "\n",
    "    dataset_cls = AmazonBeautyDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args:\n",
    "    def __init__(self):\n",
    "        self.data_dir = '/content/data'\n",
    "        self.min_len = 10\n",
    "        self.max_len = 50\n",
    "        self.sample_frac = 0.2\n",
    "        self.num_workers = 2\n",
    "        self.batch_size = 32\n",
    "\n",
    "args = Args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dm = AmazonBeautyDataModule(**args.__dict__)\n",
    "dm.prepare_data()\n",
    "dm.setup(stage='fit')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([322, 157,  54, 274, 489, 415, 258, 246, 155, 116, 119, 408,  22, 287,\n",
      "        421, 280, 292, 146,  14, 432,  62, 396, 302, 495, 331, 271, 370, 303,\n",
      "        201, 266, 241,   1]), tensor([[   0,    0,    0,  ...,  169, 4647, 3192],\n",
      "        [   0,    0,    0,  ...,   23, 4221, 1286],\n",
      "        [   0,    0,    0,  ..., 4683, 4629, 4307],\n",
      "        ...,\n",
      "        [   0,    0,    0,  ..., 3640, 1356, 1572],\n",
      "        [   0,    0,    0,  ..., 2436, 3059, 1251],\n",
      "        [   0,    0,    0,  ..., 2057, 3723, 4220]]), tensor([[   0,    0,    0,  ..., 4647, 3192, 2818],\n",
      "        [   0,    0,    0,  ..., 4221, 1286, 3835],\n",
      "        [   0,    0,    0,  ..., 4629, 4307,  445],\n",
      "        ...,\n",
      "        [   0,    0,    0,  ..., 1356, 1572, 2992],\n",
      "        [   0,    0,    0,  ..., 3059, 1251, 2169],\n",
      "        [   0,    0,    0,  ..., 3723, 4220, 1569]]), tensor([[   0,    0,    0,  ...,  282, 5128,  622],\n",
      "        [   0,    0,    0,  ..., 4801,  725,  381],\n",
      "        [   0,    0,    0,  ...,  682, 2373,  345],\n",
      "        ...,\n",
      "        [   0,    0,    0,  ..., 4851, 5359, 3163],\n",
      "        [   0,    0,    0,  ...,  130, 2670, 4336],\n",
      "        [   0,    0,    0,  ..., 5128, 2185, 1584]]), tensor([[0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0]])]\n"
     ]
    }
   ],
   "source": [
    "for batch in dm.train_dataloader():\n",
    "    print(batch)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Author: Sparsh A.\n",
      "\n",
      "Last updated: 2022-01-22 12:13:29\n",
      "\n",
      "recohut: 0.0.11\n",
      "\n",
      "Compiler    : GCC 7.5.0\n",
      "OS          : Linux\n",
      "Release     : 5.4.144+\n",
      "Machine     : x86_64\n",
      "Processor   : x86_64\n",
      "CPU cores   : 2\n",
      "Architecture: 64bit\n",
      "\n",
      "torch  : 1.10.0+cu111\n",
      "numpy  : 1.19.5\n",
      "pandas : 1.1.5\n",
      "IPython: 5.5.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "%reload_ext watermark\n",
    "%watermark -a \"Sparsh A.\" -m -iv -u -t -d -p recohut"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
