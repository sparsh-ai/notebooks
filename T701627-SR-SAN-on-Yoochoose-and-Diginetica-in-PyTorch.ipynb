{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"T701627 | SR-SAN on Yoochoose and Diginetica in PyTorch","provenance":[],"collapsed_sections":[],"mount_file_id":"1lF2rLiSd2Y2mFp6zC-xyK2Inf9vQxo2h","authorship_tag":"ABX9TyNR5mwv9Xs7gDHfv2E0gCGh"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"P1vXdMVJy82x"},"source":["# SR-SAN on Yoochoose and Diginetica in PyTorch"]},{"cell_type":"markdown","metadata":{"id":"4O2XCIFPx9hu"},"source":["Session-based recommendation aims to predict user's next behavior from current session and previous anonymous sessions. Capturing long-range dependencies between items is a vital challenge in session-based recommendation. A novel approach is proposed for session-based recommendation with self-attention networks (SR-SAN) as a remedy. The self-attention networks (SAN) allow SR-SAN capture the global dependencies among all items of a session regardless of their distance. In SR-SAN, a single item latent vector is used to capture both current interest and global interest instead of session embedding which is composed of current interest embedding and global interest embedding. Some experiments have been performed on some open benchmark datasets. Experimental results show that the proposed method outperforms some state-of-the-arts by comparisons.\n","\n","Firstly, a self-attention based model which captures and reserves the full dependencies among all items regardless of their distance is proposed without using RNNs or GNNs. Secondly, to generate session-based recommendations, the proposed method use a single item latent vector which jointly represents current interest and global interest instead of session embedding which is composed of current interest embedding and global interest embedding. In RNNs or GNNs based methods, the global interest embedding usually obtained by aggregating all items in the session with attention mechanism which is based on current interest embedding. However, this is redundant in SR-SAN which last item embedding is aggregating all items in the session with self-attention mechanism. In this way, the last item embedding in session can jointly represent current interest and global interest.\n","\n","It utilizes the self-attention to learn global item dependencies. The multi-head attention mechanism is adopted to allow SR-SAN focus on different important part of the session. The latent vector of the last item in the session is used to jointly represents current interest and global interest with prediction layer.\n","\n","<img src='_images/T701627_i1.png'>\n","\n","Session-based recommender system makes prediction based upon current user sessions data without accessing to the long-term preference profile. Let $V = \\{v_1, v_2, . . ., v_{|V|}\\}$ denote the set consisting of all unique items involved in all the sessions. An anonymous session sequence $S$ can be represented by a list $S = [s_1, s_2, . . ., s_n]$, where $s_i ∈ V$ represents a clicked item of the user within the session $S$. The task of session-based recommendation is to predict the next click $s_{n+1}$ for session $S$. Our models are constructed and trained as a classifier that learns to generate a score for each of the candidates in $V$. Let $\\hat{y} = \\{\\hat{y}_1, \\hat{y}_2, . . ., \\hat{y}_{|V|}\\}$ denote the output score vector, where $\\hat{y}_i$ corresponds to the score of item $v_i$. The items with top-K values in $\\hat{y}$ will be the candidate items for recommendation.\n","\n","The proposed model is made up of two parts. The first part is obtaining item latent vectors with self-attention networks, the second part of the proposed model is making recommendation with prediction layer.\n","\n","**Comparisons with Some Baseline Methods**\n","\n","<img src='_images/T701627_i2.png' width=50%>"]},{"cell_type":"markdown","metadata":{"id":"-HT9tL9qOxOA"},"source":["## Setup"]},{"cell_type":"markdown","metadata":{"id":"M6aJ98EmOyMm"},"source":["### Installations"]},{"cell_type":"code","metadata":{"id":"pVaRENuTLoKT"},"source":["!pip install torch==1.2"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tYx9mLxs1z3v"},"source":["### Imports"]},{"cell_type":"code","metadata":{"id":"F2D3QNhe5XYn"},"source":["import argparse\n","import time\n","import csv\n","import pickle\n","import operator\n","import datetime\n","import os\n","import numpy as np\n","import math\n","\n","import torch\n","from torch import nn\n","from torch.nn import Module, Parameter\n","import torch.nn.functional as F\n","from torch.nn import TransformerEncoder\n","from torch.nn import TransformerEncoderLayer"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"aGz_V3mX12rJ"},"source":["## Datasets"]},{"cell_type":"markdown","metadata":{"id":"FIEoKj3k16q1"},"source":["### Yoochoose"]},{"cell_type":"code","metadata":{"id":"6avuKGrH3m72"},"source":["!wget https://s3-eu-west-1.amazonaws.com/yc-rdata/yoochoose-data.7z\n","!7z x yoochoose-data.7z -o./yoochoose-data"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jHlbN_a-19Bv"},"source":["yoochoose_data_path = '/content/yoochoose-data'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"JiCBCRl84aZF"},"source":["class YoochooseDataset:\n","    def __init__(self, path='./'):\n","        self.path = path\n","\n","    def preprocess(self):\n","        dataset = os.path.join(self.path, 'yoochoose-clicks.dat')\n","        !sed -i '1i session_id,timestamp,item_id,category' $dataset\n","        print(\"-- Starting @ %ss\" % datetime.datetime.now())\n","        with open(dataset, \"r\") as f:\n","            reader = csv.DictReader(f, delimiter=',')\n","            sess_clicks = {}\n","            sess_date = {}\n","            ctr = 0\n","            curid = -1\n","            curdate = None\n","            for data in reader:\n","                sessid = data['session_id']\n","                if curdate and not curid == sessid:\n","                    date = ''\n","                    date = time.mktime(time.strptime(curdate[:19], '%Y-%m-%dT%H:%M:%S'))\n","                    sess_date[curid] = date\n","                curid = sessid\n","                item = data['item_id']\n","                curdate = ''\n","                curdate = data['timestamp']\n","                if sessid in sess_clicks:\n","                    sess_clicks[sessid] += [item]\n","                else:\n","                    sess_clicks[sessid] = [item]\n","                ctr += 1\n","            date = ''\n","            date = time.mktime(time.strptime(curdate[:19], '%Y-%m-%dT%H:%M:%S'))\n","            sess_date[curid] = date\n","\n","        print(\"-- Reading data @ %ss\" % datetime.datetime.now())\n","\n","        # Filter out length 1 sessions\n","        for s in list(sess_clicks):\n","            if len(sess_clicks[s]) == 1:\n","                del sess_clicks[s]\n","                del sess_date[s]\n","\n","        # Count number of times each item appears\n","        iid_counts = {}\n","        for s in sess_clicks:\n","            seq = sess_clicks[s]\n","            for iid in seq:\n","                if iid in iid_counts:\n","                    iid_counts[iid] += 1\n","                else:\n","                    iid_counts[iid] = 1\n","\n","        sorted_counts = sorted(iid_counts.items(), key=operator.itemgetter(1))\n","\n","        length = len(sess_clicks)\n","        for s in list(sess_clicks):\n","            curseq = sess_clicks[s]\n","            filseq = list(filter(lambda i: iid_counts[i] >= 5, curseq))\n","            if len(filseq) < 2:\n","                del sess_clicks[s]\n","                del sess_date[s]\n","            else:\n","                sess_clicks[s] = filseq\n","\n","        # Split out test set based on dates\n","        dates = list(sess_date.items())\n","        maxdate = dates[0][1]\n","\n","        for _, date in dates:\n","            if maxdate < date:\n","                maxdate = date\n","\n","        # 7 days for test\n","        splitdate = 0\n","        splitdate = maxdate - 86400 * 1  # the number of seconds for a day：86400\n","\n","        print('Splitting date', splitdate)      # Yoochoose: ('Split date', 1411930799.0)\n","        tra_sess = filter(lambda x: x[1] < splitdate, dates)\n","        tes_sess = filter(lambda x: x[1] > splitdate, dates)\n","\n","        # Sort sessions by date\n","        tra_sess = sorted(tra_sess, key=operator.itemgetter(1))     # [(session_id, timestamp), (), ]\n","        tes_sess = sorted(tes_sess, key=operator.itemgetter(1))     # [(session_id, timestamp), (), ]\n","        print(len(tra_sess))    # 186670    # 7966257\n","        print(len(tes_sess))    # 15979     # 15324\n","        print(tra_sess[:3])\n","        print(tes_sess[:3])\n","        \n","        print(\"-- Splitting train set and test set @ %ss\" % datetime.datetime.now())\n","\n","        # Choosing item count >=5 gives approximately the same number of items as reported in paper\n","        item_dict = {}\n","        # Convert training sessions to sequences and renumber items to start from 1\n","        def obtian_tra():\n","            train_ids = []\n","            train_seqs = []\n","            train_dates = []\n","            item_ctr = 1\n","            for s, date in tra_sess:\n","                seq = sess_clicks[s]\n","                outseq = []\n","                for i in seq:\n","                    if i in item_dict:\n","                        outseq += [item_dict[i]]\n","                    else:\n","                        outseq += [item_ctr]\n","                        item_dict[i] = item_ctr\n","                        item_ctr += 1\n","                if len(outseq) < 2:  # Doesn't occur\n","                    continue\n","                train_ids += [s]\n","                train_dates += [date]\n","                train_seqs += [outseq]\n","            print(item_ctr)     # 43098, 37484\n","            return train_ids, train_dates, train_seqs\n","\n","\n","        # Convert test sessions to sequences, ignoring items that do not appear in training set\n","        def obtian_tes():\n","            test_ids = []\n","            test_seqs = []\n","            test_dates = []\n","            for s, date in tes_sess:\n","                seq = sess_clicks[s]\n","                outseq = []\n","                for i in seq:\n","                    if i in item_dict:\n","                        outseq += [item_dict[i]]\n","                if len(outseq) < 2:\n","                    continue\n","                test_ids += [s]\n","                test_dates += [date]\n","                test_seqs += [outseq]\n","            return test_ids, test_dates, test_seqs\n","\n","        tra_ids, tra_dates, tra_seqs = obtian_tra()\n","        tes_ids, tes_dates, tes_seqs = obtian_tes()\n","\n","        def process_seqs(iseqs, idates):\n","            out_seqs = []\n","            out_dates = []\n","            labs = []\n","            ids = []\n","            for id, seq, date in zip(range(len(iseqs)), iseqs, idates):\n","                for i in range(1, len(seq)):\n","                    tar = seq[-i]\n","                    labs += [tar]\n","                    out_seqs += [seq[:-i]]\n","                    out_dates += [date]\n","                    ids += [id]\n","            return out_seqs, out_dates, labs, ids\n","\n","        tr_seqs, tr_dates, tr_labs, tr_ids = process_seqs(tra_seqs, tra_dates)\n","        te_seqs, te_dates, te_labs, te_ids = process_seqs(tes_seqs, tes_dates)\n","        tra = (tr_seqs, tr_labs)\n","        tes = (te_seqs, te_labs)\n","        print(len(tr_seqs))\n","        print(len(te_seqs))\n","        print(tr_seqs[:3], tr_dates[:3], tr_labs[:3])\n","        print(te_seqs[:3], te_dates[:3], te_labs[:3])\n","        all = 0\n","\n","        for seq in tra_seqs:\n","            all += len(seq)\n","        for seq in tes_seqs:\n","            all += len(seq)\n","        print('avg length: ', all/(len(tra_seqs) + len(tes_seqs) * 1.0))\n","\n","        if not os.path.exists('yoochoose1_64'):\n","            os.makedirs('yoochoose1_64')\n","        pickle.dump(tes, open('yoochoose1_64/test.txt', 'wb'))\n","        split64 = int(len(tr_seqs) / 64)\n","        print(len(tr_seqs[-split64:]))\n","\n","        tra64 = (tr_seqs[-split64:], tr_labs[-split64:])\n","        seq64 = tra_seqs[tr_ids[-split64]:]\n","\n","        pickle.dump(tra64, open('yoochoose1_64/train.txt', 'wb'))\n","        pickle.dump(seq64, open('yoochoose1_64/all_train_seq.txt', 'wb'))\n","\n","        print('Done.')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hQ0FWjgt7gP4","executionInfo":{"status":"ok","timestamp":1633848858187,"user_tz":-330,"elapsed":577936,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}},"outputId":"ad0618db-e1a2-47c4-e0f9-9c15262fa48e"},"source":["yc_data = YoochooseDataset(path=yoochoose_data_path)\n","yc_data.preprocess()"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["-- Starting @ 2021-10-10 06:45:53.102228s\n","-- Reading data @ 2021-10-10 06:51:22.149279s\n","Splitting date 1411959599.0\n","7966257\n","15324\n","[('171168', 1396321232.0), ('345618', 1396321275.0), ('263073', 1396321302.0)]\n","[('11532683', 1411959653.0), ('11464959', 1411959671.0), ('11296119', 1411959695.0)]\n","-- Splitting train set and test set @ 2021-10-10 06:52:17.837169s\n","37484\n","23670982\n","55898\n","[[1], [3], [5, 5]] [1396321232.0, 1396321275.0, 1396321302.0] [2, 4, 5]\n","[[33611, 37169, 6409], [33611, 37169], [33611]] [1411959653.0, 1411959653.0, 1411959653.0] [33128, 6409, 37169]\n","avg length:  3.9727042800167034\n","369859\n","Done.\n"]}]},{"cell_type":"markdown","metadata":{"id":"XDyp3JBgCr-0"},"source":["### Diginetica"]},{"cell_type":"code","metadata":{"id":"umn5D6v-CtOC"},"source":["!pip install -U -q PyDrive dvc dvc[gdrive]\n","!dvc get https://github.com/recohut/recodata diginetica/train-item-views.parquet.snappy"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":204},"id":"Ugi7XgwtC2TN","executionInfo":{"status":"ok","timestamp":1633850030417,"user_tz":-330,"elapsed":1985,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}},"outputId":"82ff6f13-b153-4994-82cb-4f4c1c4cd7b1"},"source":["import pandas as pd\n","\n","df = pd.read_parquet('/content/train-item-views.parquet.snappy')\n","df.head()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>sessionId</th>\n","      <th>userId</th>\n","      <th>itemId</th>\n","      <th>timeframe</th>\n","      <th>eventdate</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1</td>\n","      <td>NaN</td>\n","      <td>81766</td>\n","      <td>526309</td>\n","      <td>2016-05-09</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1</td>\n","      <td>NaN</td>\n","      <td>31331</td>\n","      <td>1031018</td>\n","      <td>2016-05-09</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>1</td>\n","      <td>NaN</td>\n","      <td>32118</td>\n","      <td>243569</td>\n","      <td>2016-05-09</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>1</td>\n","      <td>NaN</td>\n","      <td>9654</td>\n","      <td>75848</td>\n","      <td>2016-05-09</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>1</td>\n","      <td>NaN</td>\n","      <td>32627</td>\n","      <td>1112408</td>\n","      <td>2016-05-09</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   sessionId  userId  itemId  timeframe   eventdate\n","0          1     NaN   81766     526309  2016-05-09\n","1          1     NaN   31331    1031018  2016-05-09\n","2          1     NaN   32118     243569  2016-05-09\n","3          1     NaN    9654      75848  2016-05-09\n","4          1     NaN   32627    1112408  2016-05-09"]},"metadata":{},"execution_count":3}]},{"cell_type":"code","metadata":{"id":"ue4HY2SIDZLU"},"source":["df.columns = ['session_id', 'user_id', 'item_id', 'timeframe', 'eventdate']\n","df.to_csv('/content/train-item-views.csv', index=False)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"uhtw0alvDPcD"},"source":["class DigineticaDataset:\n","    def __init__(self, path='./'):\n","        self.path = path\n","\n","    def preprocess(self):\n","        dataset = os.path.join(self.path, 'train-item-views.csv')\n","        print(\"-- Starting @ %ss\" % datetime.datetime.now())\n","        with open(dataset, \"r\") as f:\n","            reader = csv.DictReader(f, delimiter=',')\n","            sess_clicks = {}\n","            sess_date = {}\n","            ctr = 0\n","            curid = -1\n","            curdate = None\n","            for data in reader:\n","                sessid = data['session_id']\n","                if curdate and not curid == sessid:\n","                    date = ''\n","                    date = time.mktime(time.strptime(curdate, '%Y-%m-%d'))\n","                    sess_date[curid] = date\n","                curid = sessid\n","                item = data['item_id'], int(data['timeframe'])\n","                curdate = ''\n","                curdate = data['eventdate']\n","                if sessid in sess_clicks:\n","                    sess_clicks[sessid] += [item]\n","                else:\n","                    sess_clicks[sessid] = [item]\n","                ctr += 1\n","            date = ''\n","            date = time.mktime(time.strptime(curdate, '%Y-%m-%d'))\n","            for i in list(sess_clicks):\n","                sorted_clicks = sorted(sess_clicks[i], key=operator.itemgetter(1))\n","                sess_clicks[i] = [c[0] for c in sorted_clicks]\n","            sess_date[curid] = date\n","\n","        print(\"-- Reading data @ %ss\" % datetime.datetime.now())\n","\n","        # Filter out length 1 sessions\n","        for s in list(sess_clicks):\n","            if len(sess_clicks[s]) == 1:\n","                del sess_clicks[s]\n","                del sess_date[s]\n","\n","        # Count number of times each item appears\n","        iid_counts = {}\n","        for s in sess_clicks:\n","            seq = sess_clicks[s]\n","            for iid in seq:\n","                if iid in iid_counts:\n","                    iid_counts[iid] += 1\n","                else:\n","                    iid_counts[iid] = 1\n","\n","        sorted_counts = sorted(iid_counts.items(), key=operator.itemgetter(1))\n","\n","        length = len(sess_clicks)\n","        for s in list(sess_clicks):\n","            curseq = sess_clicks[s]\n","            filseq = list(filter(lambda i: iid_counts[i] >= 5, curseq))\n","            if len(filseq) < 2:\n","                del sess_clicks[s]\n","                del sess_date[s]\n","            else:\n","                sess_clicks[s] = filseq\n","\n","        # Split out test set based on dates\n","        dates = list(sess_date.items())\n","        maxdate = dates[0][1]\n","\n","        for _, date in dates:\n","            if maxdate < date:\n","                maxdate = date\n","\n","        # 7 days for test\n","        splitdate = 0\n","        splitdate = maxdate - 86400 * 7\n","\n","        print('Splitting date', splitdate)      # Yoochoose: ('Split date', 1411930799.0)\n","        tra_sess = filter(lambda x: x[1] < splitdate, dates)\n","        tes_sess = filter(lambda x: x[1] > splitdate, dates)\n","\n","        # Sort sessions by date\n","        tra_sess = sorted(tra_sess, key=operator.itemgetter(1))     # [(session_id, timestamp), (), ]\n","        tes_sess = sorted(tes_sess, key=operator.itemgetter(1))     # [(session_id, timestamp), (), ]\n","        print(len(tra_sess))    # 186670    # 7966257\n","        print(len(tes_sess))    # 15979     # 15324\n","        print(tra_sess[:3])\n","        print(tes_sess[:3])\n","        \n","        print(\"-- Splitting train set and test set @ %ss\" % datetime.datetime.now())\n","\n","        # Choosing item count >=5 gives approximately the same number of items as reported in paper\n","        item_dict = {}\n","        # Convert training sessions to sequences and renumber items to start from 1\n","        def obtian_tra():\n","            train_ids = []\n","            train_seqs = []\n","            train_dates = []\n","            item_ctr = 1\n","            for s, date in tra_sess:\n","                seq = sess_clicks[s]\n","                outseq = []\n","                for i in seq:\n","                    if i in item_dict:\n","                        outseq += [item_dict[i]]\n","                    else:\n","                        outseq += [item_ctr]\n","                        item_dict[i] = item_ctr\n","                        item_ctr += 1\n","                if len(outseq) < 2:  # Doesn't occur\n","                    continue\n","                train_ids += [s]\n","                train_dates += [date]\n","                train_seqs += [outseq]\n","            print(item_ctr)     # 43098, 37484\n","            return train_ids, train_dates, train_seqs\n","\n","\n","        # Convert test sessions to sequences, ignoring items that do not appear in training set\n","        def obtian_tes():\n","            test_ids = []\n","            test_seqs = []\n","            test_dates = []\n","            for s, date in tes_sess:\n","                seq = sess_clicks[s]\n","                outseq = []\n","                for i in seq:\n","                    if i in item_dict:\n","                        outseq += [item_dict[i]]\n","                if len(outseq) < 2:\n","                    continue\n","                test_ids += [s]\n","                test_dates += [date]\n","                test_seqs += [outseq]\n","            return test_ids, test_dates, test_seqs\n","\n","        tra_ids, tra_dates, tra_seqs = obtian_tra()\n","        tes_ids, tes_dates, tes_seqs = obtian_tes()\n","\n","        def process_seqs(iseqs, idates):\n","            out_seqs = []\n","            out_dates = []\n","            labs = []\n","            ids = []\n","            for id, seq, date in zip(range(len(iseqs)), iseqs, idates):\n","                for i in range(1, len(seq)):\n","                    tar = seq[-i]\n","                    labs += [tar]\n","                    out_seqs += [seq[:-i]]\n","                    out_dates += [date]\n","                    ids += [id]\n","            return out_seqs, out_dates, labs, ids\n","\n","        tr_seqs, tr_dates, tr_labs, tr_ids = process_seqs(tra_seqs, tra_dates)\n","        te_seqs, te_dates, te_labs, te_ids = process_seqs(tes_seqs, tes_dates)\n","        tra = (tr_seqs, tr_labs)\n","        tes = (te_seqs, te_labs)\n","        print(len(tr_seqs))\n","        print(len(te_seqs))\n","        print(tr_seqs[:3], tr_dates[:3], tr_labs[:3])\n","        print(te_seqs[:3], te_dates[:3], te_labs[:3])\n","        all = 0\n","\n","        for seq in tra_seqs:\n","            all += len(seq)\n","        for seq in tes_seqs:\n","            all += len(seq)\n","        print('avg length: ', all/(len(tra_seqs) + len(tes_seqs) * 1.0))\n","\n","        if not os.path.exists('diginetica'):\n","            os.makedirs('diginetica')\n","        pickle.dump(tra, open('diginetica/train.txt', 'wb'))\n","        pickle.dump(tes, open('diginetica/test.txt', 'wb'))\n","        pickle.dump(tra_seqs, open('diginetica/all_train_seq.txt', 'wb'))\n","\n","        print('Done.')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SHhyliplElJk","executionInfo":{"status":"ok","timestamp":1633851227433,"user_tz":-330,"elapsed":21388,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}},"outputId":"952303f5-4b15-4a8b-b24b-87c33eb6f3ae"},"source":["yc_data = DigineticaDataset(path='/content')\n","yc_data.preprocess()"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["-- Starting @ 2021-10-10 07:33:25.005299s\n","-- Reading data @ 2021-10-10 07:33:39.738651s\n","Splitting date 1464134400.0\n","186670\n","15979\n","[('4737', 1451606400.0), ('4741', 1451606400.0), ('4742', 1451606400.0)]\n","[('289', 1464220800.0), ('290', 1464220800.0), ('302', 1464220800.0)]\n","-- Splitting train set and test set @ 2021-10-10 07:33:42.234211s\n","43098\n","719470\n","60858\n","[[1], [3, 4], [3]] [1451606400.0, 1451606400.0, 1451606400.0] [2, 5, 4]\n","[[21553, 20071, 8762, 21566, 6381], [21553, 20071, 8762, 21566], [21553, 20071, 8762]] [1464220800.0, 1464220800.0, 1464220800.0] [21566, 6381, 21566]\n","avg length:  4.850942344040704\n","Done.\n"]}]},{"cell_type":"markdown","metadata":{"id":"LMTOnf-H81gC"},"source":["## Utils"]},{"cell_type":"code","metadata":{"id":"vO2GDZgJApAK"},"source":["def data_masks(all_usr_pois, item_tail):\n","    us_lens = [len(upois) for upois in all_usr_pois]\n","    len_max = max(us_lens)\n","    us_pois = [upois + item_tail * (len_max - le) for upois, le in zip(all_usr_pois, us_lens)]\n","    us_msks = [[1] * le + [0] * (len_max - le) for le in us_lens]\n","    return us_pois, us_msks, len_max\n","\n","\n","def split_validation(train_set, valid_portion):\n","    train_set_x, train_set_y = train_set\n","    n_samples = len(train_set_x)\n","    sidx = np.arange(n_samples, dtype='int32')\n","    np.random.shuffle(sidx)\n","    n_train = int(np.round(n_samples * (1. - valid_portion)))\n","    valid_set_x = [train_set_x[s] for s in sidx[n_train:]]\n","    valid_set_y = [train_set_y[s] for s in sidx[n_train:]]\n","    train_set_x = [train_set_x[s] for s in sidx[:n_train]]\n","    train_set_y = [train_set_y[s] for s in sidx[:n_train]]\n","\n","    return (train_set_x, train_set_y), (valid_set_x, valid_set_y)\n","\n","\n","class Data():\n","    def __init__(self, data, shuffle=False, graph=None):\n","        inputs = data[0]\n","        inputs, mask, len_max = data_masks(inputs, [0])\n","        self.inputs = np.asarray(inputs)\n","        self.mask = np.asarray(mask)\n","        self.len_max = len_max\n","        self.targets = np.asarray(data[1])\n","        self.length = len(inputs)\n","        self.shuffle = shuffle\n","        self.graph = graph\n","\n","    def generate_batch(self, batch_size):\n","        if self.shuffle:\n","            shuffled_arg = np.arange(self.length)\n","            np.random.shuffle(shuffled_arg)\n","            self.inputs = self.inputs[shuffled_arg]\n","            self.mask = self.mask[shuffled_arg]\n","            self.targets = self.targets[shuffled_arg]\n","        n_batch = int(self.length / batch_size)\n","        if self.length % batch_size != 0:\n","            n_batch += 1\n","        slices = np.split(np.arange(n_batch * batch_size), n_batch)\n","        slices[-1] = slices[-1][:(self.length - batch_size * (n_batch - 1))]\n","        return slices\n","\n","    def get_slice(self, i):\n","        inputs, mask, targets = self.inputs[i], self.mask[i], self.targets[i]\n","        items, n_node, A, alias_inputs = [], [], [], []\n","        for u_input in inputs:\n","            n_node.append(len(np.unique(u_input)))\n","        max_n_node = np.max(n_node)\n","        for u_input in inputs:\n","            node = np.unique(u_input)\n","            items.append(node.tolist() + (max_n_node - len(node)) * [0])\n","            u_A = np.zeros((max_n_node, max_n_node))\n","            for i in np.arange(len(u_input) - 1):\n","                if u_input[i + 1] == 0:\n","                    break\n","                u = np.where(node == u_input[i])[0][0]\n","                v = np.where(node == u_input[i + 1])[0][0]\n","                u_A[u][v] = 1\n","            u_sum_in = np.sum(u_A, 0)\n","            u_sum_in[np.where(u_sum_in == 0)] = 1\n","            u_A_in = np.divide(u_A, u_sum_in)\n","            u_sum_out = np.sum(u_A, 1)\n","            u_sum_out[np.where(u_sum_out == 0)] = 1\n","            u_A_out = np.divide(u_A.transpose(), u_sum_out)\n","            u_A = np.concatenate([u_A_in, u_A_out]).transpose()\n","            A.append(u_A)\n","            alias_inputs.append([np.where(node == i)[0][0] for i in u_input])\n","        return alias_inputs, A, items, mask, targets"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9yRZQEHDAwQC"},"source":["## Model"]},{"cell_type":"code","metadata":{"id":"3SlFIw0VA6g_"},"source":["class SelfAttentionNetwork(Module):\n","    def __init__(self, opt, n_node):\n","        super(SelfAttentionNetwork, self).__init__()\n","        self.hidden_size = opt.hiddenSize\n","        self.n_node = n_node\n","        self.batch_size = opt.batchSize\n","        self.embedding = nn.Embedding(self.n_node, self.hidden_size)\n","        self.transformerEncoderLayer = TransformerEncoderLayer(d_model=self.hidden_size, nhead=opt.nhead,dim_feedforward=self.hidden_size * opt.feedforward)\n","        self.transformerEncoder = TransformerEncoder(self.transformerEncoderLayer, opt.layer)\n","        self.loss_function = nn.CrossEntropyLoss()\n","        self.optimizer = torch.optim.Adam(self.parameters(), lr=opt.lr, weight_decay=opt.l2)\n","        self.scheduler = torch.optim.lr_scheduler.StepLR(self.optimizer, step_size=opt.lr_dc_step, gamma=opt.lr_dc)\n","        self.reset_parameters()\n","\n","    def reset_parameters(self):\n","        stdv = 1.0 / math.sqrt(self.hidden_size)\n","        for weight in self.parameters():\n","            weight.data.uniform_(-stdv, stdv)\n","\n","    def compute_scores(self, hidden, mask):\n","        ht = hidden[torch.arange(mask.shape[0]).long(), torch.sum(mask, 1) - 1]  # batch_size x latent_size\n","        b = self.embedding.weight[1:]  # n_nodes x latent_size\n","        scores = torch.matmul(ht, b.transpose(1, 0))\n","        return scores\n","\n","    def forward(self, inputs, A):\n","        hidden = self.embedding(inputs)\n","        hidden = hidden.transpose(0,1).contiguous()\n","        hidden = self.transformerEncoder(hidden)\n","        hidden = hidden.transpose(0,1).contiguous()\n","        return hidden\n","\n","\n","def trans_to_cuda(variable):\n","    if torch.cuda.is_available():\n","        return variable.cuda()\n","    else:\n","        return variable\n","\n","\n","def trans_to_cpu(variable):\n","    if torch.cuda.is_available():\n","        return variable.cpu()\n","    else:\n","        return variable\n","\n","\n","def forward(model, i, data):\n","    alias_inputs, A, items, mask, targets = data.get_slice(i)\n","    alias_inputs = trans_to_cuda(torch.Tensor(alias_inputs).long())\n","    items = trans_to_cuda(torch.Tensor(items).long())\n","    A = trans_to_cuda(torch.Tensor(A).float())\n","    mask = trans_to_cuda(torch.Tensor(mask).long())\n","    hidden = model(items, A)\n","    get = lambda i: hidden[i][alias_inputs[i]]\n","    seq_hidden = torch.stack([get(i) for i in torch.arange(len(alias_inputs)).long()])\n","    return targets, model.compute_scores(seq_hidden, mask)\n","\n","\n","def train_test(model, train_data, test_data):    \n","    print('start training: ', datetime.datetime.now())\n","    model.train()\n","    total_loss = 0.0\n","    slices = train_data.generate_batch(model.batch_size)\n","    for i, j in zip(slices, np.arange(len(slices))):\n","        model.optimizer.zero_grad()\n","        targets, scores = forward(model, i, train_data)\n","        targets = trans_to_cuda(torch.Tensor(targets).long())\n","        loss = model.loss_function(scores, targets - 1)\n","        loss.backward()\n","        model.optimizer.step()\n","        total_loss += loss\n","        if j % int(len(slices) / 5 + 1) == 0:\n","            print('[%d/%d] Loss: %.4f' % (j, len(slices), loss.item()))\n","    print('\\tLoss:\\t%.3f' % total_loss)\n","\n","    print('start predicting: ', datetime.datetime.now())\n","    model.eval()\n","    hit, mrr = [], []\n","    slices = test_data.generate_batch(model.batch_size)\n","    for i in slices:\n","        targets, scores = forward(model, i, test_data)\n","        sub_scores = scores.topk(20)[1]\n","        sub_scores = trans_to_cpu(sub_scores).detach().numpy()\n","        for score, target, mask in zip(sub_scores, targets, test_data.mask):\n","            hit.append(np.isin(target - 1, score))\n","            if len(np.where(score == target - 1)[0]) == 0:\n","                mrr.append(0)\n","            else:\n","                mrr.append(1 / (np.where(score == target - 1)[0][0] + 1))\n","    hit = np.mean(hit) * 100\n","    mrr = np.mean(mrr) * 100\n","    model.scheduler.step()\n","    return hit, mrr"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KyDKrrXQBMNi"},"source":["## Run"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0uiAZTj_A7C4","executionInfo":{"status":"ok","timestamp":1633852518310,"user_tz":-330,"elapsed":11,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}},"outputId":"65607ce6-ef44-4c48-911c-03a44fa3bc01"},"source":["parser = argparse.ArgumentParser()\n","parser.add_argument('--dataset', default='diginetica', help='dataset name: diginetica/yoochoose1_64')\n","parser.add_argument('--batchSize', type=int, default=100, help='input batch size')\n","parser.add_argument('--hiddenSize', type=int, default=96, help='hidden state size')\n","parser.add_argument('--nhead', type=int, default=2, help='the number of heads of multi-head attention')\n","parser.add_argument('--layer', type=int, default=1, help='number of SAN layers')\n","parser.add_argument('--feedforward', type=int, default=4, help='the multipler of hidden state size')\n","parser.add_argument('--epoch', type=int, default=12, help='the number of epochs to train for')\n","parser.add_argument('--lr', type=float, default=0.001, help='learning rate')  # [0.001, 0.0005, 0.0001]\n","parser.add_argument('--lr_dc', type=float, default=0.1, help='learning rate decay rate')\n","parser.add_argument('--lr_dc_step', type=int, default=3, help='the number of steps after which the learning rate decay')\n","parser.add_argument('--l2', type=float, default=1e-5, help='l2 penalty')  # [0.001, 0.0005, 0.0001, 0.00005, 0.00001]\n","parser.add_argument('--patience', type=int, default=10, help='the number of epoch to wait before early stop ')\n","parser.add_argument('--validation', action='store_true', help='validation')\n","parser.add_argument('--valid_portion', type=float, default=0.1, help='split the portion of training set as validation set')\n","opt = parser.parse_args(args={})\n","print(opt)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Namespace(batchSize=100, dataset='diginetica', epoch=12, feedforward=4, hiddenSize=96, l2=1e-05, layer=1, lr=0.001, lr_dc=0.1, lr_dc_step=3, nhead=2, patience=10, valid_portion=0.1, validation=False)\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DZJqCsxOBLc1","executionInfo":{"status":"ok","timestamp":1633860699299,"user_tz":-330,"elapsed":5984747,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}},"outputId":"e4ab275e-b67a-4508-cb30-572211f2d0e9"},"source":["def main():\n","    train_data = pickle.load(open(opt.dataset + '/train.txt', 'rb'))\n","    if opt.validation:\n","        train_data, valid_data = split_validation(train_data, opt.valid_portion)\n","        test_data = valid_data\n","    else:\n","        test_data = pickle.load(open(opt.dataset + '/test.txt', 'rb'))\n","\n","    train_data = Data(train_data, shuffle=True)\n","    test_data = Data(test_data, shuffle=False)\n","    \n","    # n_node = 37484\n","    n_node = 43098\n","    # if opt.dataset == 'diginetica':\n","    #     n_node = 43098        \n","\n","    model = trans_to_cuda(SelfAttentionNetwork(opt, n_node))\n","\n","    start = time.time()\n","    best_result = [0, 0]\n","    best_epoch = [0, 0]\n","    bad_counter = 0\n","\n","    for epoch in range(opt.epoch):\n","        print('-------------------------------------------------------')\n","        print('epoch: ', epoch)\n","        hit, mrr = train_test(model, train_data, test_data)\n","        flag = 0\n","        if hit >= best_result[0]:\n","            best_result[0] = hit\n","            best_epoch[0] = epoch\n","            flag = 1\n","        if mrr >= best_result[1]:\n","            best_result[1] = mrr\n","            best_epoch[1] = epoch\n","            flag = 1\n","        print('Best Result:')\n","        print('\\tRecall@20:\\t%.4f\\tMMR@20:\\t%.4f\\tEpoch:\\t%d,\\t%d'% (best_result[0], best_result[1], best_epoch[0], best_epoch[1]))\n","        bad_counter += 1 - flag\n","        if bad_counter >= opt.patience:\n","            break\n","    print('-------------------------------------------------------')\n","    end = time.time()\n","    print(\"Run time: %f s\" % (end - start))\n","\n","\n","if __name__ == '__main__':\n","    main()"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["-------------------------------------------------------\n","epoch:  0\n","start training:  2021-10-10 07:55:45.273836\n","[0/7195] Loss: 10.6677\n","[1440/7195] Loss: 8.5698\n","[2880/7195] Loss: 7.1619\n","[4320/7195] Loss: 5.8865\n","[5760/7195] Loss: 5.8675\n","\tLoss:\t49098.215\n","start predicting:  2021-10-10 08:06:39.781629\n","Best Result:\n","\tRecall@20:\t45.9151\tMMR@20:\t14.8896\tEpoch:\t0,\t0\n","-------------------------------------------------------\n","epoch:  1\n","start training:  2021-10-10 08:07:08.585240\n","[0/7195] Loss: 5.3129\n","[1440/7195] Loss: 5.2753\n","[2880/7195] Loss: 4.5847\n","[4320/7195] Loss: 5.3179\n","[5760/7195] Loss: 5.1656\n","\tLoss:\t37207.652\n","start predicting:  2021-10-10 08:18:04.376789\n","Best Result:\n","\tRecall@20:\t47.9033\tMMR@20:\t15.5292\tEpoch:\t1,\t1\n","-------------------------------------------------------\n","epoch:  2\n","start training:  2021-10-10 08:18:33.538617\n","[0/7195] Loss: 4.8722\n","[1440/7195] Loss: 5.0588\n","[2880/7195] Loss: 5.4700\n","[4320/7195] Loss: 4.4588\n","[5760/7195] Loss: 5.0475\n","\tLoss:\t35667.793\n","start predicting:  2021-10-10 08:29:30.268410\n","Best Result:\n","\tRecall@20:\t48.6115\tMMR@20:\t15.7555\tEpoch:\t2,\t2\n","-------------------------------------------------------\n","epoch:  3\n","start training:  2021-10-10 08:29:59.568936\n","[0/7195] Loss: 4.9349\n","[1440/7195] Loss: 4.6859\n","[2880/7195] Loss: 4.5667\n","[4320/7195] Loss: 4.1434\n","[5760/7195] Loss: 3.9767\n","\tLoss:\t30751.988\n","start predicting:  2021-10-10 08:40:54.180745\n","Best Result:\n","\tRecall@20:\t51.3638\tMMR@20:\t17.1727\tEpoch:\t3,\t3\n","-------------------------------------------------------\n","epoch:  4\n","start training:  2021-10-10 08:41:22.553395\n","[0/7195] Loss: 4.2543\n","[1440/7195] Loss: 3.8915\n","[2880/7195] Loss: 3.8105\n","[4320/7195] Loss: 4.2378\n","[5760/7195] Loss: 4.0140\n","\tLoss:\t29448.672\n","start predicting:  2021-10-10 08:52:09.892111\n","Best Result:\n","\tRecall@20:\t51.3638\tMMR@20:\t17.1727\tEpoch:\t3,\t3\n","-------------------------------------------------------\n","epoch:  5\n","start training:  2021-10-10 08:52:38.087261\n","[0/7195] Loss: 3.8150\n","[1440/7195] Loss: 3.8234\n","[2880/7195] Loss: 3.5714\n","[4320/7195] Loss: 4.0254\n","[5760/7195] Loss: 4.2898\n","\tLoss:\t28780.891\n","start predicting:  2021-10-10 09:03:25.912218\n","Best Result:\n","\tRecall@20:\t51.3638\tMMR@20:\t17.1727\tEpoch:\t3,\t3\n","-------------------------------------------------------\n","epoch:  6\n","start training:  2021-10-10 09:03:54.540505\n","[0/7195] Loss: 4.1088\n","[1440/7195] Loss: 3.8025\n","[2880/7195] Loss: 3.5856\n","[4320/7195] Loss: 4.0085\n","[5760/7195] Loss: 3.7290\n","\tLoss:\t27613.971\n","start predicting:  2021-10-10 09:14:44.462629\n","Best Result:\n","\tRecall@20:\t51.3638\tMMR@20:\t17.1727\tEpoch:\t3,\t3\n","-------------------------------------------------------\n","epoch:  7\n","start training:  2021-10-10 09:15:13.075455\n","[0/7195] Loss: 3.8948\n","[1440/7195] Loss: 3.9878\n","[2880/7195] Loss: 4.3885\n","[4320/7195] Loss: 3.9052\n","[5760/7195] Loss: 3.6860\n","\tLoss:\t27512.902\n","start predicting:  2021-10-10 09:26:01.322491\n","Best Result:\n","\tRecall@20:\t51.3638\tMMR@20:\t17.1727\tEpoch:\t3,\t3\n","-------------------------------------------------------\n","epoch:  8\n","start training:  2021-10-10 09:26:29.612021\n","[0/7195] Loss: 3.9779\n","[1440/7195] Loss: 3.8012\n","[2880/7195] Loss: 3.7224\n","[4320/7195] Loss: 3.8879\n","[5760/7195] Loss: 3.7163\n","\tLoss:\t27426.121\n","start predicting:  2021-10-10 09:37:17.465449\n","Best Result:\n","\tRecall@20:\t51.3638\tMMR@20:\t17.1727\tEpoch:\t3,\t3\n","-------------------------------------------------------\n","epoch:  9\n","start training:  2021-10-10 09:37:45.967249\n","[0/7195] Loss: 3.6714\n","[1440/7195] Loss: 3.8326\n","[2880/7195] Loss: 3.6461\n","[4320/7195] Loss: 3.6942\n","[5760/7195] Loss: 3.9922\n","\tLoss:\t27279.148\n","start predicting:  2021-10-10 09:48:36.979269\n","Best Result:\n","\tRecall@20:\t51.3638\tMMR@20:\t17.1727\tEpoch:\t3,\t3\n","-------------------------------------------------------\n","epoch:  10\n","start training:  2021-10-10 09:49:05.290149\n","[0/7195] Loss: 3.7560\n","[1440/7195] Loss: 3.6283\n","[2880/7195] Loss: 3.7068\n","[4320/7195] Loss: 4.2068\n","[5760/7195] Loss: 3.5650\n","\tLoss:\t27270.990\n","start predicting:  2021-10-10 09:59:52.720394\n","Best Result:\n","\tRecall@20:\t51.3638\tMMR@20:\t17.1727\tEpoch:\t3,\t3\n","-------------------------------------------------------\n","epoch:  11\n","start training:  2021-10-10 10:00:20.799292\n","[0/7195] Loss: 4.1372\n","[1440/7195] Loss: 3.9532\n","[2880/7195] Loss: 3.6319\n","[4320/7195] Loss: 3.4573\n","[5760/7195] Loss: 3.6855\n","\tLoss:\t27264.115\n","start predicting:  2021-10-10 10:11:09.142454\n","Best Result:\n","\tRecall@20:\t51.3638\tMMR@20:\t17.1727\tEpoch:\t3,\t3\n","-------------------------------------------------------\n","Run time: 8152.358966 s\n"]}]}]}