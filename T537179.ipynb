{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"T537179","provenance":[],"collapsed_sections":["EuNNoZTDdkMJ","YzZjeP50kMrG"],"authorship_tag":"ABX9TyNZjqu4zsh6LE49caJ336vP"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"EuNNoZTDdkMJ"},"source":["## RecBole PyTorch Implementation"]},{"cell_type":"code","metadata":{"id":"XmCSqSQZYbVf","executionInfo":{"status":"ok","timestamp":1632917196348,"user_tz":-330,"elapsed":436,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}}},"source":["import numpy as np\n","import torch\n","import torch.nn as nn\n","\n","from enum import Enum\n","from logging import getLogger"],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"id":"imTJBE62ckPW","executionInfo":{"status":"ok","timestamp":1632917196971,"user_tz":-330,"elapsed":4,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}}},"source":["def set_color(log, color, highlight=True):\n","    color_set = ['black', 'red', 'green', 'yellow', 'blue', 'pink', 'cyan', 'white']\n","    try:\n","        index = color_set.index(color)\n","    except:\n","        index = len(color_set) - 1\n","    prev_log = '\\033['\n","    if highlight:\n","        prev_log += '1;3'\n","    else:\n","        prev_log += '0;3'\n","    prev_log += str(index) + 'm'\n","    return prev_log + log + '\\033[0m'"],"execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"id":"jLZjAMXqbX0M","executionInfo":{"status":"ok","timestamp":1632917199207,"user_tz":-330,"elapsed":687,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}}},"source":["class ModelType(Enum):\n","    \"\"\"Type of models.\n","    - ``GENERAL``: General Recommendation\n","    - ``SEQUENTIAL``: Sequential Recommendation\n","    - ``CONTEXT``: Context-aware Recommendation\n","    - ``KNOWLEDGE``: Knowledge-based Recommendation\n","    \"\"\"\n","\n","    GENERAL = 1\n","    SEQUENTIAL = 2\n","    CONTEXT = 3\n","    KNOWLEDGE = 4\n","    TRADITIONAL = 5\n","    DECISIONTREE = 6\n","\n","\n","class InputType(Enum):\n","    \"\"\"Type of Models' input.\n","    - ``POINTWISE``: Point-wise input, like ``uid, iid, label``.\n","    - ``PAIRWISE``: Pair-wise input, like ``uid, pos_iid, neg_iid``.\n","    \"\"\"\n","\n","    POINTWISE = 1\n","    PAIRWISE = 2\n","    LISTWISE = 3\n","\n","\n","class FeatureType(Enum):\n","    \"\"\"Type of features.\n","    - ``TOKEN``: Token features like user_id and item_id.\n","    - ``FLOAT``: Float features like rating and timestamp.\n","    - ``TOKEN_SEQ``: Token sequence features like review.\n","    - ``FLOAT_SEQ``: Float sequence features like pretrained vector.\n","    \"\"\"\n","\n","    TOKEN = 'token'\n","    FLOAT = 'float'\n","    TOKEN_SEQ = 'token_seq'\n","    FLOAT_SEQ = 'float_seq'\n","\n","\n","class FeatureSource(Enum):\n","    \"\"\"Source of features.\n","    - ``INTERACTION``: Features from ``.inter`` (other than ``user_id`` and ``item_id``).\n","    - ``USER``: Features from ``.user`` (other than ``user_id``).\n","    - ``ITEM``: Features from ``.item`` (other than ``item_id``).\n","    - ``USER_ID``: ``user_id`` feature in ``inter_feat`` and ``user_feat``.\n","    - ``ITEM_ID``: ``item_id`` feature in ``inter_feat`` and ``item_feat``.\n","    - ``KG``: Features from ``.kg``.\n","    - ``NET``: Features from ``.net``.\n","    \"\"\"\n","\n","    INTERACTION = 'inter'\n","    USER = 'user'\n","    ITEM = 'item'\n","    USER_ID = 'user_id'\n","    ITEM_ID = 'item_id'\n","    KG = 'kg'\n","    NET = 'net'"],"execution_count":8,"outputs":[]},{"cell_type":"code","metadata":{"id":"iRe1ktVsY9Mq","executionInfo":{"status":"ok","timestamp":1632917200583,"user_tz":-330,"elapsed":9,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}}},"source":["class FMEmbedding(nn.Module):\n","    r\"\"\" Embedding for token fields.\n","\n","    Args:\n","        field_dims: list, the number of tokens in each token fields\n","        offsets: list, the dimension offset of each token field\n","        embed_dim: int, the dimension of output embedding vectors\n","\n","    Input:\n","        input_x: tensor, A 3D tensor with shape:``(batch_size,field_size)``.\n","\n","    Return:\n","        output: tensor,  A 3D tensor with shape: ``(batch_size,field_size,embed_dim)``.\n","    \"\"\"\n","\n","    def __init__(self, field_dims, offsets, embed_dim):\n","        super(FMEmbedding, self).__init__()\n","        self.embedding = nn.Embedding(sum(field_dims), embed_dim)\n","        self.offsets = offsets\n","\n","    def forward(self, input_x):\n","        input_x = input_x + input_x.new_tensor(self.offsets).unsqueeze(0)\n","        output = self.embedding(input_x)\n","        return output"],"execution_count":9,"outputs":[]},{"cell_type":"code","metadata":{"id":"X9k11ZffY_23","executionInfo":{"status":"ok","timestamp":1632917202529,"user_tz":-330,"elapsed":6,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}}},"source":["class FMFirstOrderLinear(nn.Module):\n","    \"\"\"Calculate the first order score of the input features.\n","    This class is a member of ContextRecommender, you can call it easily when inherit ContextRecommender.\n","\n","    \"\"\"\n","\n","    def __init__(self, config, dataset, output_dim=1):\n","\n","        super(FMFirstOrderLinear, self).__init__()\n","        self.field_names = dataset.fields(\n","            source=[\n","                FeatureSource.INTERACTION,\n","                FeatureSource.USER,\n","                FeatureSource.USER_ID,\n","                FeatureSource.ITEM,\n","                FeatureSource.ITEM_ID,\n","            ]\n","        )\n","        self.LABEL = config['LABEL_FIELD']\n","        self.device = config['device']\n","        self.token_field_names = []\n","        self.token_field_dims = []\n","        self.float_field_names = []\n","        self.float_field_dims = []\n","        self.token_seq_field_names = []\n","        self.token_seq_field_dims = []\n","        for field_name in self.field_names:\n","            if field_name == self.LABEL:\n","                continue\n","            if dataset.field2type[field_name] == FeatureType.TOKEN:\n","                self.token_field_names.append(field_name)\n","                self.token_field_dims.append(dataset.num(field_name))\n","            elif dataset.field2type[field_name] == FeatureType.TOKEN_SEQ:\n","                self.token_seq_field_names.append(field_name)\n","                self.token_seq_field_dims.append(dataset.num(field_name))\n","            else:\n","                self.float_field_names.append(field_name)\n","                self.float_field_dims.append(dataset.num(field_name))\n","        if len(self.token_field_dims) > 0:\n","            self.token_field_offsets = np.array((0, *np.cumsum(self.token_field_dims)[:-1]), dtype=np.long)\n","            self.token_embedding_table = FMEmbedding(self.token_field_dims, self.token_field_offsets, output_dim)\n","        if len(self.float_field_dims) > 0:\n","            self.float_embedding_table = nn.Embedding(np.sum(self.float_field_dims, dtype=np.int32), output_dim)\n","        if len(self.token_seq_field_dims) > 0:\n","            self.token_seq_embedding_table = nn.ModuleList()\n","            for token_seq_field_dim in self.token_seq_field_dims:\n","                self.token_seq_embedding_table.append(nn.Embedding(token_seq_field_dim, output_dim))\n","\n","        self.bias = nn.Parameter(torch.zeros((output_dim,)), requires_grad=True)"],"execution_count":10,"outputs":[]},{"cell_type":"code","metadata":{"id":"ScX6LvSycsCf","executionInfo":{"status":"ok","timestamp":1632917204447,"user_tz":-330,"elapsed":6,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}}},"source":["class AbstractRecommender(nn.Module):\n","    r\"\"\"Base class for all models\n","    \"\"\"\n","\n","    def __init__(self):\n","        self.logger = getLogger()\n","        super(AbstractRecommender, self).__init__()\n","\n","    def calculate_loss(self, interaction):\n","        r\"\"\"Calculate the training loss for a batch data.\n","\n","        Args:\n","            interaction (Interaction): Interaction class of the batch.\n","\n","        Returns:\n","            torch.Tensor: Training loss, shape: []\n","        \"\"\"\n","        raise NotImplementedError\n","\n","\n","    def predict(self, interaction):\n","        r\"\"\"Predict the scores between users and items.\n","\n","        Args:\n","            interaction (Interaction): Interaction class of the batch.\n","\n","        Returns:\n","            torch.Tensor: Predicted scores for given users and items, shape: [batch_size]\n","        \"\"\"\n","        raise NotImplementedError\n","\n","\n","    def full_sort_predict(self, interaction):\n","        r\"\"\"full sort prediction function.\n","        Given users, calculate the scores between users and all candidate items.\n","\n","        Args:\n","            interaction (Interaction): Interaction class of the batch.\n","\n","        Returns:\n","            torch.Tensor: Predicted scores for given users and all candidate items,\n","            shape: [n_batch_users * n_candidate_items]\n","        \"\"\"\n","        raise NotImplementedError\n","\n","\n","    def other_parameter(self):\n","        if hasattr(self, 'other_parameter_name'):\n","            return {key: getattr(self, key) for key in self.other_parameter_name}\n","        return dict()\n","\n","\n","    def load_other_parameter(self, para):\n","        if para is None:\n","            return\n","        for key, value in para.items():\n","            setattr(self, key, value)\n","\n","\n","    def __str__(self):\n","        \"\"\"\n","        Model prints with number of trainable parameters\n","        \"\"\"\n","        model_parameters = filter(lambda p: p.requires_grad, self.parameters())\n","        params = sum([np.prod(p.size()) for p in model_parameters])\n","        return super().__str__() + set_color('\\nTrainable parameters', 'blue') + f': {params}'"],"execution_count":11,"outputs":[]},{"cell_type":"code","metadata":{"id":"9QbNMMxaXp3s","executionInfo":{"status":"ok","timestamp":1632917207213,"user_tz":-330,"elapsed":732,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}}},"source":["class ContextRecommender(AbstractRecommender):\n","    \"\"\"This is a abstract context-aware recommender. All the context-aware model should implement this class.\n","    The base context-aware recommender class provide the basic embedding function of feature fields which also\n","    contains a first-order part of feature fields.\n","    \"\"\"\n","    type = ModelType.CONTEXT\n","    input_type = InputType.POINTWISE\n","\n","    def __init__(self, config, dataset):\n","        super(ContextRecommender, self).__init__()\n","\n","        self.field_names = dataset.fields(\n","            source=[\n","                FeatureSource.INTERACTION,\n","                FeatureSource.USER,\n","                FeatureSource.USER_ID,\n","                FeatureSource.ITEM,\n","                FeatureSource.ITEM_ID,\n","            ]\n","        )\n","        self.LABEL = config['LABEL_FIELD']\n","        self.embedding_size = config['embedding_size']\n","        self.device = config['device']\n","        self.double_tower = config['double_tower']\n","        if self.double_tower is None:\n","            self.double_tower = False\n","        self.token_field_names = []\n","        self.token_field_dims = []\n","        self.float_field_names = []\n","        self.float_field_dims = []\n","        self.token_seq_field_names = []\n","        self.token_seq_field_dims = []\n","        self.num_feature_field = 0\n","\n","        if self.double_tower:\n","            self.user_field_names = dataset.fields(source=[FeatureSource.USER, FeatureSource.USER_ID])\n","            self.item_field_names = dataset.fields(source=[FeatureSource.ITEM, FeatureSource.ITEM_ID])\n","            self.field_names = self.user_field_names + self.item_field_names\n","            self.user_token_field_num = 0\n","            self.user_float_field_num = 0\n","            self.user_token_seq_field_num = 0\n","            for field_name in self.user_field_names:\n","                if dataset.field2type[field_name] == FeatureType.TOKEN:\n","                    self.user_token_field_num += 1\n","                elif dataset.field2type[field_name] == FeatureType.TOKEN_SEQ:\n","                    self.user_token_seq_field_num += 1\n","                else:\n","                    self.user_float_field_num += dataset.num(field_name)\n","            self.item_token_field_num = 0\n","            self.item_float_field_num = 0\n","            self.item_token_seq_field_num = 0\n","            for field_name in self.item_field_names:\n","                if dataset.field2type[field_name] == FeatureType.TOKEN:\n","                    self.item_token_field_num += 1\n","                elif dataset.field2type[field_name] == FeatureType.TOKEN_SEQ:\n","                    self.item_token_seq_field_num += 1\n","                else:\n","                    self.item_float_field_num += dataset.num(field_name)\n","\n","        for field_name in self.field_names:\n","            if field_name == self.LABEL:\n","                continue\n","            if dataset.field2type[field_name] == FeatureType.TOKEN:\n","                self.token_field_names.append(field_name)\n","                self.token_field_dims.append(dataset.num(field_name))\n","            elif dataset.field2type[field_name] == FeatureType.TOKEN_SEQ:\n","                self.token_seq_field_names.append(field_name)\n","                self.token_seq_field_dims.append(dataset.num(field_name))\n","            else:\n","                self.float_field_names.append(field_name)\n","                self.float_field_dims.append(dataset.num(field_name))\n","            self.num_feature_field += 1\n","        if len(self.token_field_dims) > 0:\n","            self.token_field_offsets = np.array((0, *np.cumsum(self.token_field_dims)[:-1]), dtype=np.long)\n","            self.token_embedding_table = FMEmbedding(\n","                self.token_field_dims, self.token_field_offsets, self.embedding_size\n","            )\n","        if len(self.float_field_dims) > 0:\n","            self.float_embedding_table = nn.Embedding(\n","                np.sum(self.float_field_dims, dtype=np.int32), self.embedding_size\n","            )\n","        if len(self.token_seq_field_dims) > 0:\n","            self.token_seq_embedding_table = nn.ModuleList()\n","            for token_seq_field_dim in self.token_seq_field_dims:\n","                self.token_seq_embedding_table.append(nn.Embedding(token_seq_field_dim, self.embedding_size))\n","\n","        self.first_order_linear = FMFirstOrderLinear(config, dataset)\n","\n","    def embed_float_fields(self, float_fields, embed=True):\n","        \"\"\"Embed the float feature columns\n","\n","        Args:\n","            float_fields (torch.FloatTensor): The input dense tensor. shape of [batch_size, num_float_field]\n","            embed (bool): Return the embedding of columns or just the columns itself. Defaults to ``True``.\n","\n","        Returns:\n","            torch.FloatTensor: The result embedding tensor of float columns.\n","        \"\"\"\n","        # input Tensor shape : [batch_size, num_float_field]\n","        if not embed or float_fields is None:\n","            return float_fields\n","\n","        num_float_field = float_fields.shape[1]\n","        # [batch_size, num_float_field]\n","        index = torch.arange(0, num_float_field).unsqueeze(0).expand_as(float_fields).long().to(self.device)\n","\n","        # [batch_size, num_float_field, embed_dim]\n","        float_embedding = self.float_embedding_table(index)\n","        float_embedding = torch.mul(float_embedding, float_fields.unsqueeze(2))\n","\n","        return float_embedding\n","\n","\n","    def embed_token_fields(self, token_fields):\n","        \"\"\"Embed the token feature columns\n","\n","        Args:\n","            token_fields (torch.LongTensor): The input tensor. shape of [batch_size, num_token_field]\n","\n","        Returns:\n","            torch.FloatTensor: The result embedding tensor of token columns.\n","        \"\"\"\n","        # input Tensor shape : [batch_size, num_token_field]\n","        if token_fields is None:\n","            return None\n","        # [batch_size, num_token_field, embed_dim]\n","        token_embedding = self.token_embedding_table(token_fields)\n","\n","        return token_embedding\n","\n","\n","    def embed_token_seq_fields(self, token_seq_fields, mode='mean'):\n","        \"\"\"Embed the token feature columns\n","\n","        Args:\n","            token_seq_fields (torch.LongTensor): The input tensor. shape of [batch_size, seq_len]\n","            mode (str): How to aggregate the embedding of feature in this field. default=mean\n","\n","        Returns:\n","            torch.FloatTensor: The result embedding tensor of token sequence columns.\n","        \"\"\"\n","        # input is a list of Tensor shape of [batch_size, seq_len]\n","        fields_result = []\n","        for i, token_seq_field in enumerate(token_seq_fields):\n","            embedding_table = self.token_seq_embedding_table[i]\n","            mask = token_seq_field != 0  # [batch_size, seq_len]\n","            mask = mask.float()\n","            value_cnt = torch.sum(mask, dim=1, keepdim=True)  # [batch_size, 1]\n","\n","            token_seq_embedding = embedding_table(token_seq_field)  # [batch_size, seq_len, embed_dim]\n","\n","            mask = mask.unsqueeze(2).expand_as(token_seq_embedding)  # [batch_size, seq_len, embed_dim]\n","            if mode == 'max':\n","                masked_token_seq_embedding = token_seq_embedding - (1 - mask) * 1e9  # [batch_size, seq_len, embed_dim]\n","                result = torch.max(masked_token_seq_embedding, dim=1, keepdim=True)  # [batch_size, 1, embed_dim]\n","            elif mode == 'sum':\n","                masked_token_seq_embedding = token_seq_embedding * mask.float()\n","                result = torch.sum(masked_token_seq_embedding, dim=1, keepdim=True)  # [batch_size, 1, embed_dim]\n","            else:\n","                masked_token_seq_embedding = token_seq_embedding * mask.float()\n","                result = torch.sum(masked_token_seq_embedding, dim=1)  # [batch_size, embed_dim]\n","                eps = torch.FloatTensor([1e-8]).to(self.device)\n","                result = torch.div(result, value_cnt + eps)  # [batch_size, embed_dim]\n","                result = result.unsqueeze(1)  # [batch_size, 1, embed_dim]\n","            fields_result.append(result)\n","        if len(fields_result) == 0:\n","            return None\n","        else:\n","            return torch.cat(fields_result, dim=1)  # [batch_size, num_token_seq_field, embed_dim]\n","\n","\n","    def double_tower_embed_input_fields(self, interaction):\n","        \"\"\"Embed the whole feature columns in a double tower way.\n","\n","        Args:\n","            interaction (Interaction): The input data collection.\n","\n","        Returns:\n","            torch.FloatTensor: The embedding tensor of token sequence columns in the first part.\n","            torch.FloatTensor: The embedding tensor of float sequence columns in the first part.\n","            torch.FloatTensor: The embedding tensor of token sequence columns in the second part.\n","            torch.FloatTensor: The embedding tensor of float sequence columns in the second part.\n","\n","        \"\"\"\n","        if not self.double_tower:\n","            raise RuntimeError('Please check your model hyper parameters and set \\'double tower\\' as True')\n","        sparse_embedding, dense_embedding = self.embed_input_fields(interaction)\n","        if dense_embedding is not None:\n","            first_dense_embedding, second_dense_embedding = \\\n","                torch.split(dense_embedding, [self.user_float_field_num, self.item_float_field_num], dim=1)\n","        else:\n","            first_dense_embedding, second_dense_embedding = None, None\n","\n","        if sparse_embedding is not None:\n","            sizes = [\n","                self.user_token_seq_field_num, self.item_token_seq_field_num, self.user_token_field_num,\n","                self.item_token_field_num\n","            ]\n","            first_token_seq_embedding, second_token_seq_embedding, first_token_embedding, second_token_embedding = \\\n","                torch.split(sparse_embedding, sizes, dim=1)\n","            first_sparse_embedding = torch.cat([first_token_seq_embedding, first_token_embedding], dim=1)\n","            second_sparse_embedding = torch.cat([second_token_seq_embedding, second_token_embedding], dim=1)\n","        else:\n","            first_sparse_embedding, second_sparse_embedding = None, None\n","\n","        return first_sparse_embedding, first_dense_embedding, second_sparse_embedding, second_dense_embedding\n","\n","\n","    def concat_embed_input_fields(self, interaction):\n","        sparse_embedding, dense_embedding = self.embed_input_fields(interaction)\n","        all_embeddings = []\n","        if sparse_embedding is not None:\n","            all_embeddings.append(sparse_embedding)\n","        if dense_embedding is not None and len(dense_embedding.shape) == 3:\n","            all_embeddings.append(dense_embedding)\n","        return torch.cat(all_embeddings, dim=1)  # [batch_size, num_field, embed_dim]\n","\n","\n","    def embed_input_fields(self, interaction):\n","        \"\"\"Embed the whole feature columns.\n","\n","        Args:\n","            interaction (Interaction): The input data collection.\n","\n","        Returns:\n","            torch.FloatTensor: The embedding tensor of token sequence columns.\n","            torch.FloatTensor: The embedding tensor of float sequence columns.\n","        \"\"\"\n","        float_fields = []\n","        for field_name in self.float_field_names:\n","            if len(interaction[field_name].shape) == 2:\n","                float_fields.append(interaction[field_name])\n","            else:\n","                float_fields.append(interaction[field_name].unsqueeze(1))\n","        if len(float_fields) > 0:\n","            float_fields = torch.cat(float_fields, dim=1)  # [batch_size, num_float_field]\n","        else:\n","            float_fields = None\n","        # [batch_size, num_float_field] or [batch_size, num_float_field, embed_dim] or None\n","        float_fields_embedding = self.embed_float_fields(float_fields)\n","\n","        token_fields = []\n","        for field_name in self.token_field_names:\n","            token_fields.append(interaction[field_name].unsqueeze(1))\n","        if len(token_fields) > 0:\n","            token_fields = torch.cat(token_fields, dim=1)  # [batch_size, num_token_field]\n","        else:\n","            token_fields = None\n","        # [batch_size, num_token_field, embed_dim] or None\n","        token_fields_embedding = self.embed_token_fields(token_fields)\n","\n","        token_seq_fields = []\n","        for field_name in self.token_seq_field_names:\n","            token_seq_fields.append(interaction[field_name])\n","        # [batch_size, num_token_seq_field, embed_dim] or None\n","        token_seq_fields_embedding = self.embed_token_seq_fields(token_seq_fields)\n","\n","        if token_fields_embedding is None:\n","            sparse_embedding = token_seq_fields_embedding\n","        else:\n","            if token_seq_fields_embedding is None:\n","                sparse_embedding = token_fields_embedding\n","            else:\n","                sparse_embedding = torch.cat([token_fields_embedding, token_seq_fields_embedding], dim=1)\n","\n","        dense_embedding = float_fields_embedding\n","\n","        # sparse_embedding shape: [batch_size, num_token_seq_field+num_token_field, embed_dim] or None\n","        # dense_embedding shape: [batch_size, num_float_field] or [batch_size, num_float_field, embed_dim] or None\n","        return sparse_embedding, dense_embedding"],"execution_count":12,"outputs":[]},{"cell_type":"code","metadata":{"id":"quCbaLuJZN0f","executionInfo":{"status":"ok","timestamp":1632917209473,"user_tz":-330,"elapsed":7,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}}},"source":["class AttLayer(nn.Module):\n","    \"\"\"Calculate the attention signal(weight) according the input tensor.\n","\n","    Args:\n","        infeatures (torch.FloatTensor): A 3D input tensor with shape of[batch_size, M, embed_dim].\n","\n","    Returns:\n","        torch.FloatTensor: Attention weight of input. shape of [batch_size, M].\n","    \"\"\"\n","\n","    def __init__(self, in_dim, att_dim):\n","        super(AttLayer, self).__init__()\n","        self.in_dim = in_dim\n","        self.att_dim = att_dim\n","        self.w = torch.nn.Linear(in_features=in_dim, out_features=att_dim, bias=False)\n","        self.h = nn.Parameter(torch.randn(att_dim), requires_grad=True)\n","\n","    def forward(self, infeatures):\n","        att_signal = self.w(infeatures)  # [batch_size, M, att_dim]\n","        att_signal = fn.relu(att_signal)  # [batch_size, M, att_dim]\n","\n","        att_signal = torch.mul(att_signal, self.h)  # [batch_size, M, att_dim]\n","        att_signal = torch.sum(att_signal, dim=2)  # [batch_size, M]\n","        att_signal = fn.softmax(att_signal, dim=1)  # [batch_size, M]\n","\n","        return att_signal"],"execution_count":13,"outputs":[]},{"cell_type":"code","metadata":{"id":"cKd7R3zmc4f-","executionInfo":{"status":"ok","timestamp":1632917356706,"user_tz":-330,"elapsed":482,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}}},"source":["class AFM(ContextRecommender):\n","    \"\"\" AFM is a attention based FM model that predict the final score with the attention of input feature.\n","\n","    \"\"\"\n","\n","    def __init__(self, config, dataset):\n","        super(AFM, self).__init__(config, dataset)\n","\n","        # load parameters info\n","        self.attention_size = config['attention_size']\n","        self.dropout_prob = config['dropout_prob']\n","        self.reg_weight = config['reg_weight']\n","        self.num_pair = self.num_feature_field * (self.num_feature_field - 1) / 2\n","\n","        # define layers and loss\n","        self.attlayer = AttLayer(self.embedding_size, self.attention_size)\n","        self.p = nn.Parameter(torch.randn(self.embedding_size), requires_grad=True)\n","        self.dropout_layer = nn.Dropout(p=self.dropout_prob)\n","        self.sigmoid = nn.Sigmoid()\n","        self.loss = nn.BCELoss()\n","\n","        # parameters initialization\n","        self.apply(self._init_weights)\n","\n","    def _init_weights(self, module):\n","        if isinstance(module, nn.Embedding):\n","            xavier_normal_(module.weight.data)\n","        elif isinstance(module, nn.Linear):\n","            xavier_normal_(module.weight.data)\n","            if module.bias is not None:\n","                constant_(module.bias.data, 0)\n","\n","    def build_cross(self, feat_emb):\n","        \"\"\" Build the cross feature columns of feature columns\n","\n","        Args:\n","            feat_emb (torch.FloatTensor): input feature embedding tensor. shape of [batch_size, field_size, embed_dim].\n","\n","        Returns:\n","            tuple:\n","                - torch.FloatTensor: Left part of the cross feature. shape of [batch_size, num_pairs, emb_dim].\n","                - torch.FloatTensor: Right part of the cross feature. shape of [batch_size, num_pairs, emb_dim].\n","        \"\"\"\n","        # num_pairs = num_feature_field * (num_feature_field-1) / 2\n","        row = []\n","        col = []\n","        for i in range(self.num_feature_field - 1):\n","            for j in range(i + 1, self.num_feature_field):\n","                row.append(i)\n","                col.append(j)\n","        p = feat_emb[:, row]  # [batch_size, num_pairs, emb_dim]\n","        q = feat_emb[:, col]  # [batch_size, num_pairs, emb_dim]\n","        return p, q\n","\n","\n","    def afm_layer(self, infeature):\n","        \"\"\" Get the attention-based feature interaction score\n","\n","        Args:\n","            infeature (torch.FloatTensor): input feature embedding tensor. shape of [batch_size, field_size, embed_dim].\n","\n","        Returns:\n","            torch.FloatTensor: Result of score. shape of [batch_size, 1].\n","        \"\"\"\n","        p, q = self.build_cross(infeature)\n","        pair_wise_inter = torch.mul(p, q)  # [batch_size, num_pairs, emb_dim]\n","\n","        # [batch_size, num_pairs, 1]\n","        att_signal = self.attlayer(pair_wise_inter).unsqueeze(dim=2)\n","\n","        att_inter = torch.mul(att_signal, pair_wise_inter)  # [batch_size, num_pairs, emb_dim]\n","        att_pooling = torch.sum(att_inter, dim=1)  # [batch_size, emb_dim]\n","        att_pooling = self.dropout_layer(att_pooling)  # [batch_size, emb_dim]\n","\n","        att_pooling = torch.mul(att_pooling, self.p)  # [batch_size, emb_dim]\n","        att_pooling = torch.sum(att_pooling, dim=1, keepdim=True)  # [batch_size, 1]\n","\n","        return att_pooling\n","\n","\n","    def forward(self, interaction):\n","        afm_all_embeddings = self.concat_embed_input_fields(interaction)  # [batch_size, num_field, embed_dim]\n","\n","        output = self.sigmoid(self.first_order_linear(interaction) + self.afm_layer(afm_all_embeddings))\n","        return output.squeeze()\n","\n","\n","    def calculate_loss(self, interaction):\n","        label = interaction[self.LABEL]\n","\n","        output = self.forward(interaction)\n","        l2_loss = self.reg_weight * torch.norm(self.attlayer.w.weight, p=2)\n","        return self.loss(output, label) + l2_loss\n","\n","\n","    def predict(self, interaction):\n","        return self.forward(interaction)"],"execution_count":15,"outputs":[]},{"cell_type":"code","metadata":{"id":"v2THaafeb94t"},"source":["# import os\n","# import unittest\n","\n","# from recbole.quick_start import objective_function\n","\n","# current_path = os.path.dirname(os.path.realpath(__file__))\n","# config_file_list = [os.path.join(current_path, 'test_model.yaml')]\n","\n","\n","# def quick_test(config_dict):\n","#     objective_function(config_dict=config_dict, config_file_list=config_file_list, saved=False)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KXqMGnKmdnww"},"source":["## DeepCTR PyTorch Implementation"]},{"cell_type":"code","metadata":{"id":"KWve8TvcdoqB","executionInfo":{"status":"ok","timestamp":1632918954455,"user_tz":-330,"elapsed":435,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}}},"source":["from __future__ import print_function\n","\n","import time\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.utils.data as Data\n","from sklearn.metrics import *\n","from torch.utils.data import DataLoader\n","from tqdm import tqdm\n","from tensorflow.python.keras.callbacks import CallbackList\n","from tensorflow.python.keras.callbacks import History\n","from tensorflow.python.keras.callbacks import EarlyStopping\n","from tensorflow.python.keras.callbacks import ModelCheckpoint\n","\n","from collections import OrderedDict, namedtuple, defaultdict\n","from itertools import chain\n","\n","import torch\n","import torch.nn as nn\n","import numpy as np\n","\n","import itertools\n","import os\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.nn.utils.rnn import PackedSequence"],"execution_count":19,"outputs":[]},{"cell_type":"code","metadata":{"id":"a8ZVDoCVjbYk","executionInfo":{"status":"ok","timestamp":1632918955220,"user_tz":-330,"elapsed":3,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}}},"source":["class ModelCheckpoint(ModelCheckpoint):\n","    \"\"\"Save the model after every epoch.\n","    `filepath` can contain named formatting options,\n","    which will be filled the value of `epoch` and\n","    keys in `logs` (passed in `on_epoch_end`).\n","    For example: if `filepath` is `weights.{epoch:02d}-{val_loss:.2f}.hdf5`,\n","    then the model checkpoints will be saved with the epoch number and\n","    the validation loss in the filename.\n","    Arguments:\n","        filepath: string, path to save the model file.\n","        monitor: quantity to monitor.\n","        verbose: verbosity mode, 0 or 1.\n","        save_best_only: if `save_best_only=True`,\n","            the latest best model according to\n","            the quantity monitored will not be overwritten.\n","        mode: one of {auto, min, max}.\n","            If `save_best_only=True`, the decision\n","            to overwrite the current save file is made\n","            based on either the maximization or the\n","            minimization of the monitored quantity. For `val_acc`,\n","            this should be `max`, for `val_loss` this should\n","            be `min`, etc. In `auto` mode, the direction is\n","            automatically inferred from the name of the monitored quantity.\n","        save_weights_only: if True, then only the model's weights will be\n","            saved (`model.save_weights(filepath)`), else the full model\n","            is saved (`model.save(filepath)`).\n","        period: Interval (number of epochs) between checkpoints.\n","    \"\"\"\n","\n","    def on_epoch_end(self, epoch, logs=None):\n","        logs = logs or {}\n","        self.epochs_since_last_save += 1\n","        if self.epochs_since_last_save >= self.period:\n","            self.epochs_since_last_save = 0\n","            filepath = self.filepath.format(epoch=epoch + 1, **logs)\n","            if self.save_best_only:\n","                current = logs.get(self.monitor)\n","                if current is None:\n","                    print('Can save best model only with %s available, skipping.' % self.monitor)\n","                else:\n","                    if self.monitor_op(current, self.best):\n","                        if self.verbose > 0:\n","                            print('Epoch %05d: %s improved from %0.5f to %0.5f,'\n","                                  ' saving model to %s' % (epoch + 1, self.monitor, self.best,\n","                                                           current, filepath))\n","                        self.best = current\n","                        if self.save_weights_only:\n","                            torch.save(self.model.state_dict(), filepath)\n","                        else:\n","                            torch.save(self.model, filepath)\n","                    else:\n","                        if self.verbose > 0:\n","                            print('Epoch %05d: %s did not improve from %0.5f' %\n","                                  (epoch + 1, self.monitor, self.best))\n","            else:\n","                if self.verbose > 0:\n","                    print('Epoch %05d: saving model to %s' %\n","                          (epoch + 1, filepath))\n","                if self.save_weights_only:\n","                    torch.save(self.model.state_dict(), filepath)\n","                else:\n","                    torch.save(self.model, filepath)"],"execution_count":20,"outputs":[]},{"cell_type":"code","metadata":{"id":"nzsAfPUofKzo","executionInfo":{"status":"ok","timestamp":1632918959589,"user_tz":-330,"elapsed":973,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}}},"source":["def concat_fun(inputs, axis=-1):\n","    if len(inputs) == 1:\n","        return inputs[0]\n","    else:\n","        return torch.cat(inputs, dim=axis)\n","        \n","\n","def slice_arrays(arrays, start=None, stop=None):\n","    \"\"\"Slice an array or list of arrays.\n","    This takes an array-like, or a list of\n","    array-likes, and outputs:\n","        - arrays[start:stop] if `arrays` is an array-like\n","        - [x[start:stop] for x in arrays] if `arrays` is a list\n","    Can also work on list/array of indices: `slice_arrays(x, indices)`\n","    Arguments:\n","        arrays: Single array or list of arrays.\n","        start: can be an integer index (start index)\n","            or a list/array of indices\n","        stop: integer (stop index); should be None if\n","            `start` was a list.\n","    Returns:\n","        A slice of the array(s).\n","    Raises:\n","        ValueError: If the value of start is a list and stop is not None.\n","    \"\"\"\n","\n","    if arrays is None:\n","        return [None]\n","\n","    if isinstance(arrays, np.ndarray):\n","        arrays = [arrays]\n","\n","    if isinstance(start, list) and stop is not None:\n","        raise ValueError('The stop argument has to be None if the value of start '\n","                         'is a list.')\n","    elif isinstance(arrays, list):\n","        if hasattr(start, '__len__'):\n","            # hdf5 datasets only support list objects as indices\n","            if hasattr(start, 'shape'):\n","                start = start.tolist()\n","            return [None if x is None else x[start] for x in arrays]\n","        else:\n","            if len(arrays) == 1:\n","                return arrays[0][start:stop]\n","            return [None if x is None else x[start:stop] for x in arrays]\n","    else:\n","        if hasattr(start, '__len__'):\n","            if hasattr(start, 'shape'):\n","                start = start.tolist()\n","            return arrays[start]\n","        elif hasattr(start, '__getitem__'):\n","            return arrays[start:stop]\n","        else:\n","            return [None]"],"execution_count":21,"outputs":[]},{"cell_type":"code","metadata":{"id":"aMLj5U65f1W8","executionInfo":{"status":"ok","timestamp":1632918959590,"user_tz":-330,"elapsed":10,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}}},"source":["class SequencePoolingLayer(nn.Module):\n","    \"\"\"The SequencePoolingLayer is used to apply pooling operation(sum,mean,max) on variable-length sequence feature/multi-value feature.\n","      Input shape\n","        - A list of two  tensor [seq_value,seq_len]\n","        - seq_value is a 3D tensor with shape: ``(batch_size, T, embedding_size)``\n","        - seq_len is a 2D tensor with shape : ``(batch_size, 1)``,indicate valid length of each sequence.\n","      Output shape\n","        - 3D tensor with shape: ``(batch_size, 1, embedding_size)``.\n","      Arguments\n","        - **mode**:str.Pooling operation to be used,can be sum,mean or max.\n","    \"\"\"\n","\n","    def __init__(self, mode='mean', supports_masking=False, device='cpu'):\n","\n","        super(SequencePoolingLayer, self).__init__()\n","        if mode not in ['sum', 'mean', 'max']:\n","            raise ValueError('parameter mode should in [sum, mean, max]')\n","        self.supports_masking = supports_masking\n","        self.mode = mode\n","        self.device = device\n","        self.eps = torch.FloatTensor([1e-8]).to(device)\n","        self.to(device)\n","\n","    def _sequence_mask(self, lengths, maxlen=None, dtype=torch.bool):\n","        # Returns a mask tensor representing the first N positions of each cell.\n","        if maxlen is None:\n","            maxlen = lengths.max()\n","        row_vector = torch.arange(0, maxlen, 1).to(lengths.device)\n","        matrix = torch.unsqueeze(lengths, dim=-1)\n","        mask = row_vector < matrix\n","\n","        mask.type(dtype)\n","        return mask\n","\n","    def forward(self, seq_value_len_list):\n","        if self.supports_masking:\n","            uiseq_embed_list, mask = seq_value_len_list  # [B, T, E], [B, 1]\n","            mask = mask.float()\n","            user_behavior_length = torch.sum(mask, dim=-1, keepdim=True)\n","            mask = mask.unsqueeze(2)\n","        else:\n","            uiseq_embed_list, user_behavior_length = seq_value_len_list  # [B, T, E], [B, 1]\n","            mask = self._sequence_mask(user_behavior_length, maxlen=uiseq_embed_list.shape[1],\n","                                       dtype=torch.float32)  # [B, 1, maxlen]\n","            mask = torch.transpose(mask, 1, 2)  # [B, maxlen, 1]\n","\n","        embedding_size = uiseq_embed_list.shape[-1]\n","\n","        mask = torch.repeat_interleave(mask, embedding_size, dim=2)  # [B, maxlen, E]\n","\n","        if self.mode == 'max':\n","            hist = uiseq_embed_list - (1 - mask) * 1e9\n","            hist = torch.max(hist, dim=1, keepdim=True)[0]\n","            return hist\n","        hist = uiseq_embed_list * mask.float()\n","        hist = torch.sum(hist, dim=1, keepdim=False)\n","\n","        if self.mode == 'mean':\n","            self.eps = self.eps.to(user_behavior_length.device)\n","            hist = torch.div(hist, user_behavior_length.type(torch.float32) + self.eps)\n","\n","        hist = torch.unsqueeze(hist, dim=1)\n","        return hist"],"execution_count":22,"outputs":[]},{"cell_type":"code","metadata":{"id":"ok-WgMxhf7W6","executionInfo":{"status":"ok","timestamp":1632918959590,"user_tz":-330,"elapsed":8,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}}},"source":["DEFAULT_GROUP_NAME = \"default_group\"\n","\n","\n","class SparseFeat(namedtuple('SparseFeat',\n","                            ['name', 'vocabulary_size', 'embedding_dim', 'use_hash', 'dtype', 'embedding_name',\n","                             'group_name'])):\n","    __slots__ = ()\n","\n","    def __new__(cls, name, vocabulary_size, embedding_dim=4, use_hash=False, dtype=\"int32\", embedding_name=None,\n","                group_name=DEFAULT_GROUP_NAME):\n","        if embedding_name is None:\n","            embedding_name = name\n","        if embedding_dim == \"auto\":\n","            embedding_dim = 6 * int(pow(vocabulary_size, 0.25))\n","        if use_hash:\n","            print(\n","                \"Notice! Feature Hashing on the fly currently is not supported in torch version,you can use tensorflow version!\")\n","        return super(SparseFeat, cls).__new__(cls, name, vocabulary_size, embedding_dim, use_hash, dtype,\n","                                              embedding_name, group_name)\n","\n","    def __hash__(self):\n","        return self.name.__hash__()\n","\n","\n","class VarLenSparseFeat(namedtuple('VarLenSparseFeat',\n","                                  ['sparsefeat', 'maxlen', 'combiner', 'length_name'])):\n","    __slots__ = ()\n","\n","    def __new__(cls, sparsefeat, maxlen, combiner=\"mean\", length_name=None):\n","        return super(VarLenSparseFeat, cls).__new__(cls, sparsefeat, maxlen, combiner, length_name)\n","\n","    @property\n","    def name(self):\n","        return self.sparsefeat.name\n","\n","    @property\n","    def vocabulary_size(self):\n","        return self.sparsefeat.vocabulary_size\n","\n","    @property\n","    def embedding_dim(self):\n","        return self.sparsefeat.embedding_dim\n","\n","    @property\n","    def use_hash(self):\n","        return self.sparsefeat.use_hash\n","\n","    @property\n","    def dtype(self):\n","        return self.sparsefeat.dtype\n","\n","    @property\n","    def embedding_name(self):\n","        return self.sparsefeat.embedding_name\n","\n","    @property\n","    def group_name(self):\n","        return self.sparsefeat.group_name\n","\n","    def __hash__(self):\n","        return self.name.__hash__()\n","\n","\n","class DenseFeat(namedtuple('DenseFeat', ['name', 'dimension', 'dtype'])):\n","    __slots__ = ()\n","\n","    def __new__(cls, name, dimension=1, dtype=\"float32\"):\n","        return super(DenseFeat, cls).__new__(cls, name, dimension, dtype)\n","\n","    def __hash__(self):\n","        return self.name.__hash__()\n","\n","\n","def get_feature_names(feature_columns):\n","    features = build_input_features(feature_columns)\n","    return list(features.keys())\n","\n","\n","# def get_inputs_list(inputs):\n","#     return list(chain(*list(map(lambda x: x.values(), filter(lambda x: x is not None, inputs)))))\n","\n","\n","def build_input_features(feature_columns):\n","    # Return OrderedDict: {feature_name:(start, start+dimension)}\n","\n","    features = OrderedDict()\n","\n","    start = 0\n","    for feat in feature_columns:\n","        feat_name = feat.name\n","        if feat_name in features:\n","            continue\n","        if isinstance(feat, SparseFeat):\n","            features[feat_name] = (start, start + 1)\n","            start += 1\n","        elif isinstance(feat, DenseFeat):\n","            features[feat_name] = (start, start + feat.dimension)\n","            start += feat.dimension\n","        elif isinstance(feat, VarLenSparseFeat):\n","            features[feat_name] = (start, start + feat.maxlen)\n","            start += feat.maxlen\n","            if feat.length_name is not None and feat.length_name not in features:\n","                features[feat.length_name] = (start, start + 1)\n","                start += 1\n","        else:\n","            raise TypeError(\"Invalid feature column type,got\", type(feat))\n","    return features\n","\n","\n","def combined_dnn_input(sparse_embedding_list, dense_value_list):\n","    if len(sparse_embedding_list) > 0 and len(dense_value_list) > 0:\n","        sparse_dnn_input = torch.flatten(\n","            torch.cat(sparse_embedding_list, dim=-1), start_dim=1)\n","        dense_dnn_input = torch.flatten(\n","            torch.cat(dense_value_list, dim=-1), start_dim=1)\n","        return concat_fun([sparse_dnn_input, dense_dnn_input])\n","    elif len(sparse_embedding_list) > 0:\n","        return torch.flatten(torch.cat(sparse_embedding_list, dim=-1), start_dim=1)\n","    elif len(dense_value_list) > 0:\n","        return torch.flatten(torch.cat(dense_value_list, dim=-1), start_dim=1)\n","    else:\n","        raise NotImplementedError\n","\n","\n","def get_varlen_pooling_list(embedding_dict, features, feature_index, varlen_sparse_feature_columns, device):\n","    varlen_sparse_embedding_list = []\n","    for feat in varlen_sparse_feature_columns:\n","        seq_emb = embedding_dict[feat.name]\n","        if feat.length_name is None:\n","            seq_mask = features[:, feature_index[feat.name][0]:feature_index[feat.name][1]].long() != 0\n","\n","            emb = SequencePoolingLayer(mode=feat.combiner, supports_masking=True, device=device)(\n","                [seq_emb, seq_mask])\n","        else:\n","            seq_length = features[:, feature_index[feat.length_name][0]:feature_index[feat.length_name][1]].long()\n","            emb = SequencePoolingLayer(mode=feat.combiner, supports_masking=False, device=device)(\n","                [seq_emb, seq_length])\n","        varlen_sparse_embedding_list.append(emb)\n","    return varlen_sparse_embedding_list\n","\n","\n","def create_embedding_matrix(feature_columns, init_std=0.0001, linear=False, sparse=False, device='cpu'):\n","    # Return nn.ModuleDict: for sparse features, {embedding_name: nn.Embedding}\n","    # for varlen sparse features, {embedding_name: nn.EmbeddingBag}\n","    sparse_feature_columns = list(\n","        filter(lambda x: isinstance(x, SparseFeat), feature_columns)) if len(feature_columns) else []\n","\n","    varlen_sparse_feature_columns = list(\n","        filter(lambda x: isinstance(x, VarLenSparseFeat), feature_columns)) if len(feature_columns) else []\n","\n","    embedding_dict = nn.ModuleDict(\n","        {feat.embedding_name: nn.Embedding(feat.vocabulary_size, feat.embedding_dim if not linear else 1, sparse=sparse)\n","         for feat in\n","         sparse_feature_columns + varlen_sparse_feature_columns}\n","    )\n","\n","    # for feat in varlen_sparse_feature_columns:\n","    #     embedding_dict[feat.embedding_name] = nn.EmbeddingBag(\n","    #         feat.dimension, embedding_size, sparse=sparse, mode=feat.combiner)\n","\n","    for tensor in embedding_dict.values():\n","        nn.init.normal_(tensor.weight, mean=0, std=init_std)\n","\n","    return embedding_dict.to(device)\n","\n","\n","def embedding_lookup(X, sparse_embedding_dict, sparse_input_dict, sparse_feature_columns, return_feat_list=(),\n","                     mask_feat_list=(), to_list=False):\n","    \"\"\"\n","        Args:\n","            X: input Tensor [batch_size x hidden_dim]\n","            sparse_embedding_dict: nn.ModuleDict, {embedding_name: nn.Embedding}\n","            sparse_input_dict: OrderedDict, {feature_name:(start, start+dimension)}\n","            sparse_feature_columns: list, sparse features\n","            return_feat_list: list, names of feature to be returned, defualt () -> return all features\n","            mask_feat_list, list, names of feature to be masked in hash transform\n","        Return:\n","            group_embedding_dict: defaultdict(list)\n","    \"\"\"\n","    group_embedding_dict = defaultdict(list)\n","    for fc in sparse_feature_columns:\n","        feature_name = fc.name\n","        embedding_name = fc.embedding_name\n","        if (len(return_feat_list) == 0 or feature_name in return_feat_list):\n","            # TODO: add hash function\n","            # if fc.use_hash:\n","            #     raise NotImplementedError(\"hash function is not implemented in this version!\")\n","            lookup_idx = np.array(sparse_input_dict[feature_name])\n","            input_tensor = X[:, lookup_idx[0]:lookup_idx[1]].long()\n","            emb = sparse_embedding_dict[embedding_name](input_tensor)\n","            group_embedding_dict[fc.group_name].append(emb)\n","    if to_list:\n","        return list(chain.from_iterable(group_embedding_dict.values()))\n","    return group_embedding_dict\n","\n","\n","def varlen_embedding_lookup(X, embedding_dict, sequence_input_dict, varlen_sparse_feature_columns):\n","    varlen_embedding_vec_dict = {}\n","    for fc in varlen_sparse_feature_columns:\n","        feature_name = fc.name\n","        embedding_name = fc.embedding_name\n","        if fc.use_hash:\n","            # lookup_idx = Hash(fc.vocabulary_size, mask_zero=True)(sequence_input_dict[feature_name])\n","            # TODO: add hash function\n","            lookup_idx = sequence_input_dict[feature_name]\n","        else:\n","            lookup_idx = sequence_input_dict[feature_name]\n","        varlen_embedding_vec_dict[feature_name] = embedding_dict[embedding_name](\n","            X[:, lookup_idx[0]:lookup_idx[1]].long())  # (lookup_idx)\n","\n","    return varlen_embedding_vec_dict\n","\n","\n","def get_dense_input(X, features, feature_columns):\n","    dense_feature_columns = list(filter(lambda x: isinstance(\n","        x, DenseFeat), feature_columns)) if feature_columns else []\n","    dense_input_list = []\n","    for fc in dense_feature_columns:\n","        lookup_idx = np.array(features[fc.name])\n","        input_tensor = X[:, lookup_idx[0]:lookup_idx[1]].float()\n","        dense_input_list.append(input_tensor)\n","    return dense_input_list\n","\n","\n","def maxlen_lookup(X, sparse_input_dict, maxlen_column):\n","    if maxlen_column is None or len(maxlen_column)==0:\n","        raise ValueError('please add max length column for VarLenSparseFeat of DIN/DIEN input')\n","    lookup_idx = np.array(sparse_input_dict[maxlen_column[0]])\n","    return X[:, lookup_idx[0]:lookup_idx[1]].long()"],"execution_count":23,"outputs":[]},{"cell_type":"code","metadata":{"id":"jb9ptJ0Qe_gS","executionInfo":{"status":"ok","timestamp":1632918959591,"user_tz":-330,"elapsed":9,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}}},"source":["class PredictionLayer(nn.Module):\n","    \"\"\"\n","      Arguments\n","         - **task**: str, ``\"binary\"`` for  binary logloss or  ``\"regression\"`` for regression loss\n","         - **use_bias**: bool.Whether add bias term or not.\n","    \"\"\"\n","\n","    def __init__(self, task='binary', use_bias=True, **kwargs):\n","        if task not in [\"binary\", \"multiclass\", \"regression\"]:\n","            raise ValueError(\"task must be binary,multiclass or regression\")\n","\n","        super(PredictionLayer, self).__init__()\n","        self.use_bias = use_bias\n","        self.task = task\n","        if self.use_bias:\n","            self.bias = nn.Parameter(torch.zeros((1,)))\n","\n","    def forward(self, X):\n","        output = X\n","        if self.use_bias:\n","            output += self.bias\n","        if self.task == \"binary\":\n","            output = torch.sigmoid(output)\n","        return output"],"execution_count":24,"outputs":[]},{"cell_type":"code","metadata":{"id":"TzVVGGvch8PF","executionInfo":{"status":"ok","timestamp":1632918959592,"user_tz":-330,"elapsed":9,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}}},"source":["class Linear(nn.Module):\n","    def __init__(self, feature_columns, feature_index, init_std=0.0001, device='cpu'):\n","        super(Linear, self).__init__()\n","        self.feature_index = feature_index\n","        self.device = device\n","        self.sparse_feature_columns = list(\n","            filter(lambda x: isinstance(x, SparseFeat), feature_columns)) if len(feature_columns) else []\n","        self.dense_feature_columns = list(\n","            filter(lambda x: isinstance(x, DenseFeat), feature_columns)) if len(feature_columns) else []\n","\n","        self.varlen_sparse_feature_columns = list(\n","            filter(lambda x: isinstance(x, VarLenSparseFeat), feature_columns)) if len(feature_columns) else []\n","\n","        self.embedding_dict = create_embedding_matrix(feature_columns, init_std, linear=True, sparse=False,\n","                                                      device=device)\n","\n","        #         nn.ModuleDict(\n","        #             {feat.embedding_name: nn.Embedding(feat.dimension, 1, sparse=True) for feat in\n","        #              self.sparse_feature_columns}\n","        #         )\n","        # .to(\"cuda:1\")\n","        for tensor in self.embedding_dict.values():\n","            nn.init.normal_(tensor.weight, mean=0, std=init_std)\n","\n","        if len(self.dense_feature_columns) > 0:\n","            self.weight = nn.Parameter(torch.Tensor(sum(fc.dimension for fc in self.dense_feature_columns), 1).to(\n","                device))\n","            torch.nn.init.normal_(self.weight, mean=0, std=init_std)\n","\n","    def forward(self, X, sparse_feat_refine_weight=None):\n","\n","        sparse_embedding_list = [self.embedding_dict[feat.embedding_name](\n","            X[:, self.feature_index[feat.name][0]:self.feature_index[feat.name][1]].long()) for\n","            feat in self.sparse_feature_columns]\n","\n","        dense_value_list = [X[:, self.feature_index[feat.name][0]:self.feature_index[feat.name][1]] for feat in\n","                            self.dense_feature_columns]\n","\n","        sequence_embed_dict = varlen_embedding_lookup(X, self.embedding_dict, self.feature_index,\n","                                                      self.varlen_sparse_feature_columns)\n","        varlen_embedding_list = get_varlen_pooling_list(sequence_embed_dict, X, self.feature_index,\n","                                                        self.varlen_sparse_feature_columns, self.device)\n","\n","        sparse_embedding_list += varlen_embedding_list\n","\n","        linear_logit = torch.zeros([X.shape[0], 1]).to(sparse_embedding_list[0].device)\n","        if len(sparse_embedding_list) > 0:\n","            sparse_embedding_cat = torch.cat(sparse_embedding_list, dim=-1)\n","            if sparse_feat_refine_weight is not None:\n","                # w_{x,i}=m_{x,i} * w_i (in IFM and DIFM)\n","                sparse_embedding_cat = sparse_embedding_cat * sparse_feat_refine_weight.unsqueeze(1)\n","            sparse_feat_logit = torch.sum(sparse_embedding_cat, dim=-1, keepdim=False)\n","            linear_logit += sparse_feat_logit\n","        if len(dense_value_list) > 0:\n","            dense_value_logit = torch.cat(\n","                dense_value_list, dim=-1).matmul(self.weight)\n","            linear_logit += dense_value_logit\n","\n","        return linear_logit"],"execution_count":25,"outputs":[]},{"cell_type":"code","metadata":{"id":"a9afDIVWeDP1","executionInfo":{"status":"ok","timestamp":1632918961435,"user_tz":-330,"elapsed":1851,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}}},"source":["class BaseModel(nn.Module):\n","    def __init__(self, linear_feature_columns, dnn_feature_columns, l2_reg_linear=1e-5, l2_reg_embedding=1e-5,\n","                 init_std=0.0001, seed=1024, task='binary', device='cpu', gpus=None):\n","\n","        super(BaseModel, self).__init__()\n","        torch.manual_seed(seed)\n","        self.dnn_feature_columns = dnn_feature_columns\n","\n","        self.reg_loss = torch.zeros((1,), device=device)\n","        self.aux_loss = torch.zeros((1,), device=device)\n","        self.device = device\n","        self.gpus = gpus\n","        if gpus and str(self.gpus[0]) not in self.device:\n","            raise ValueError(\n","                \"`gpus[0]` should be the same gpu with `device`\")\n","\n","        self.feature_index = build_input_features(\n","            linear_feature_columns + dnn_feature_columns)\n","        self.dnn_feature_columns = dnn_feature_columns\n","\n","        self.embedding_dict = create_embedding_matrix(dnn_feature_columns, init_std, sparse=False, device=device)\n","        #         nn.ModuleDict(\n","        #             {feat.embedding_name: nn.Embedding(feat.dimension, embedding_size, sparse=True) for feat in\n","        #              self.dnn_feature_columns}\n","        #         )\n","\n","        self.linear_model = Linear(\n","            linear_feature_columns, self.feature_index, device=device)\n","\n","        self.regularization_weight = []\n","\n","        self.add_regularization_weight(self.embedding_dict.parameters(), l2=l2_reg_embedding)\n","        self.add_regularization_weight(self.linear_model.parameters(), l2=l2_reg_linear)\n","\n","        self.out = PredictionLayer(task, )\n","        self.to(device)\n","\n","        # parameters for callbacks\n","        self._is_graph_network = True  # used for ModelCheckpoint in tf2\n","        self._ckpt_saved_epoch = False  # used for EarlyStopping in tf1.14\n","        self.history = History()\n","\n","    def fit(self, x=None, y=None, batch_size=None, epochs=1, verbose=1, initial_epoch=0, validation_split=0.,\n","            validation_data=None, shuffle=True, callbacks=None):\n","        \"\"\"\n","\n","        :param x: Numpy array of training data (if the model has a single input), or list of Numpy arrays (if the model has multiple inputs).If input layers in the model are named, you can also pass a\n","            dictionary mapping input names to Numpy arrays.\n","        :param y: Numpy array of target (label) data (if the model has a single output), or list of Numpy arrays (if the model has multiple outputs).\n","        :param batch_size: Integer or `None`. Number of samples per gradient update. If unspecified, `batch_size` will default to 256.\n","        :param epochs: Integer. Number of epochs to train the model. An epoch is an iteration over the entire `x` and `y` data provided. Note that in conjunction with `initial_epoch`, `epochs` is to be understood as \"final epoch\". The model is not trained for a number of iterations given by `epochs`, but merely until the epoch of index `epochs` is reached.\n","        :param verbose: Integer. 0, 1, or 2. Verbosity mode. 0 = silent, 1 = progress bar, 2 = one line per epoch.\n","        :param initial_epoch: Integer. Epoch at which to start training (useful for resuming a previous training run).\n","        :param validation_split: Float between 0 and 1. Fraction of the training data to be used as validation data. The model will set apart this fraction of the training data, will not train on it, and will evaluate the loss and any model metrics on this data at the end of each epoch. The validation data is selected from the last samples in the `x` and `y` data provided, before shuffling.\n","        :param validation_data: tuple `(x_val, y_val)` or tuple `(x_val, y_val, val_sample_weights)` on which to evaluate the loss and any model metrics at the end of each epoch. The model will not be trained on this data. `validation_data` will override `validation_split`.\n","        :param shuffle: Boolean. Whether to shuffle the order of the batches at the beginning of each epoch.\n","        :param callbacks: List of `deepctr_torch.callbacks.Callback` instances. List of callbacks to apply during training and validation (if ). See [callbacks](https://tensorflow.google.cn/api_docs/python/tf/keras/callbacks). Now available: `EarlyStopping` , `ModelCheckpoint`\n","\n","        :return: A `History` object. Its `History.history` attribute is a record of training loss values and metrics values at successive epochs, as well as validation loss values and validation metrics values (if applicable).\n","        \"\"\"\n","        if isinstance(x, dict):\n","            x = [x[feature] for feature in self.feature_index]\n","\n","        do_validation = False\n","        if validation_data:\n","            do_validation = True\n","            if len(validation_data) == 2:\n","                val_x, val_y = validation_data\n","                val_sample_weight = None\n","            elif len(validation_data) == 3:\n","                val_x, val_y, val_sample_weight = validation_data  # pylint: disable=unpacking-non-sequence\n","            else:\n","                raise ValueError(\n","                    'When passing a `validation_data` argument, '\n","                    'it must contain either 2 items (x_val, y_val), '\n","                    'or 3 items (x_val, y_val, val_sample_weights), '\n","                    'or alternatively it could be a dataset or a '\n","                    'dataset or a dataset iterator. '\n","                    'However we received `validation_data=%s`' % validation_data)\n","            if isinstance(val_x, dict):\n","                val_x = [val_x[feature] for feature in self.feature_index]\n","\n","        elif validation_split and 0. < validation_split < 1.:\n","            do_validation = True\n","            if hasattr(x[0], 'shape'):\n","                split_at = int(x[0].shape[0] * (1. - validation_split))\n","            else:\n","                split_at = int(len(x[0]) * (1. - validation_split))\n","            x, val_x = (slice_arrays(x, 0, split_at),\n","                        slice_arrays(x, split_at))\n","            y, val_y = (slice_arrays(y, 0, split_at),\n","                        slice_arrays(y, split_at))\n","\n","        else:\n","            val_x = []\n","            val_y = []\n","        for i in range(len(x)):\n","            if len(x[i].shape) == 1:\n","                x[i] = np.expand_dims(x[i], axis=1)\n","\n","        train_tensor_data = Data.TensorDataset(\n","            torch.from_numpy(\n","                np.concatenate(x, axis=-1)),\n","            torch.from_numpy(y))\n","        if batch_size is None:\n","            batch_size = 256\n","\n","        model = self.train()\n","        loss_func = self.loss_func\n","        optim = self.optim\n","\n","        if self.gpus:\n","            print('parallel running on these gpus:', self.gpus)\n","            model = torch.nn.DataParallel(model, device_ids=self.gpus)\n","            batch_size *= len(self.gpus)  # input `batch_size` is batch_size per gpu\n","        else:\n","            print(self.device)\n","\n","        train_loader = DataLoader(\n","            dataset=train_tensor_data, shuffle=shuffle, batch_size=batch_size)\n","\n","        sample_num = len(train_tensor_data)\n","        steps_per_epoch = (sample_num - 1) // batch_size + 1\n","\n","        # configure callbacks\n","        callbacks = (callbacks or []) + [self.history]  # add history callback\n","        callbacks = CallbackList(callbacks)\n","        callbacks.set_model(self)\n","        callbacks.on_train_begin()\n","        callbacks.set_model(self)\n","        if not hasattr(callbacks, 'model'):  # for tf1.4\n","            callbacks.__setattr__('model', self)\n","        callbacks.model.stop_training = False\n","\n","        # Train\n","        print(\"Train on {0} samples, validate on {1} samples, {2} steps per epoch\".format(\n","            len(train_tensor_data), len(val_y), steps_per_epoch))\n","        for epoch in range(initial_epoch, epochs):\n","            callbacks.on_epoch_begin(epoch)\n","            epoch_logs = {}\n","            start_time = time.time()\n","            loss_epoch = 0\n","            total_loss_epoch = 0\n","            train_result = {}\n","            try:\n","                with tqdm(enumerate(train_loader), disable=verbose != 1) as t:\n","                    for _, (x_train, y_train) in t:\n","                        x = x_train.to(self.device).float()\n","                        y = y_train.to(self.device).float()\n","\n","                        y_pred = model(x).squeeze()\n","\n","                        optim.zero_grad()\n","                        loss = loss_func(y_pred, y.squeeze(), reduction='sum')\n","                        reg_loss = self.get_regularization_loss()\n","\n","                        total_loss = loss + reg_loss + self.aux_loss\n","\n","                        loss_epoch += loss.item()\n","                        total_loss_epoch += total_loss.item()\n","                        total_loss.backward()\n","                        optim.step()\n","\n","                        if verbose > 0:\n","                            for name, metric_fun in self.metrics.items():\n","                                if name not in train_result:\n","                                    train_result[name] = []\n","                                train_result[name].append(metric_fun(\n","                                    y.cpu().data.numpy(), y_pred.cpu().data.numpy().astype(\"float64\")))\n","\n","\n","            except KeyboardInterrupt:\n","                t.close()\n","                raise\n","            t.close()\n","\n","            # Add epoch_logs\n","            epoch_logs[\"loss\"] = total_loss_epoch / sample_num\n","            for name, result in train_result.items():\n","                epoch_logs[name] = np.sum(result) / steps_per_epoch\n","\n","            if do_validation:\n","                eval_result = self.evaluate(val_x, val_y, batch_size)\n","                for name, result in eval_result.items():\n","                    epoch_logs[\"val_\" + name] = result\n","            # verbose\n","            if verbose > 0:\n","                epoch_time = int(time.time() - start_time)\n","                print('Epoch {0}/{1}'.format(epoch + 1, epochs))\n","\n","                eval_str = \"{0}s - loss: {1: .4f}\".format(\n","                    epoch_time, epoch_logs[\"loss\"])\n","\n","                for name in self.metrics:\n","                    eval_str += \" - \" + name + \\\n","                                \": {0: .4f}\".format(epoch_logs[name])\n","\n","                if do_validation:\n","                    for name in self.metrics:\n","                        eval_str += \" - \" + \"val_\" + name + \\\n","                                    \": {0: .4f}\".format(epoch_logs[\"val_\" + name])\n","                print(eval_str)\n","            callbacks.on_epoch_end(epoch, epoch_logs)\n","            if self.stop_training:\n","                break\n","\n","        callbacks.on_train_end()\n","\n","        return self.history\n","\n","\n","    def evaluate(self, x, y, batch_size=256):\n","        \"\"\"\n","\n","        :param x: Numpy array of test data (if the model has a single input), or list of Numpy arrays (if the model has multiple inputs).\n","        :param y: Numpy array of target (label) data (if the model has a single output), or list of Numpy arrays (if the model has multiple outputs).\n","        :param batch_size: Integer or `None`. Number of samples per evaluation step. If unspecified, `batch_size` will default to 256.\n","        :return: Dict contains metric names and metric values.\n","        \"\"\"\n","        pred_ans = self.predict(x, batch_size)\n","        eval_result = {}\n","        for name, metric_fun in self.metrics.items():\n","            eval_result[name] = metric_fun(y, pred_ans)\n","        return eval_result\n","\n","\n","    def predict(self, x, batch_size=256):\n","        \"\"\"\n","\n","        :param x: The input data, as a Numpy array (or list of Numpy arrays if the model has multiple inputs).\n","        :param batch_size: Integer. If unspecified, it will default to 256.\n","        :return: Numpy array(s) of predictions.\n","        \"\"\"\n","        model = self.eval()\n","        if isinstance(x, dict):\n","            x = [x[feature] for feature in self.feature_index]\n","        for i in range(len(x)):\n","            if len(x[i].shape) == 1:\n","                x[i] = np.expand_dims(x[i], axis=1)\n","\n","        tensor_data = Data.TensorDataset(\n","            torch.from_numpy(np.concatenate(x, axis=-1)))\n","        test_loader = DataLoader(\n","            dataset=tensor_data, shuffle=False, batch_size=batch_size)\n","\n","        pred_ans = []\n","        with torch.no_grad():\n","            for _, x_test in enumerate(test_loader):\n","                x = x_test[0].to(self.device).float()\n","\n","                y_pred = model(x).cpu().data.numpy()  # .squeeze()\n","                pred_ans.append(y_pred)\n","\n","        return np.concatenate(pred_ans).astype(\"float64\")\n","\n","\n","    def input_from_feature_columns(self, X, feature_columns, embedding_dict, support_dense=True):\n","\n","        sparse_feature_columns = list(\n","            filter(lambda x: isinstance(x, SparseFeat), feature_columns)) if len(feature_columns) else []\n","        dense_feature_columns = list(\n","            filter(lambda x: isinstance(x, DenseFeat), feature_columns)) if len(feature_columns) else []\n","\n","        varlen_sparse_feature_columns = list(\n","            filter(lambda x: isinstance(x, VarLenSparseFeat), feature_columns)) if feature_columns else []\n","\n","        if not support_dense and len(dense_feature_columns) > 0:\n","            raise ValueError(\n","                \"DenseFeat is not supported in dnn_feature_columns\")\n","\n","        sparse_embedding_list = [embedding_dict[feat.embedding_name](\n","            X[:, self.feature_index[feat.name][0]:self.feature_index[feat.name][1]].long()) for\n","            feat in sparse_feature_columns]\n","\n","        sequence_embed_dict = varlen_embedding_lookup(X, self.embedding_dict, self.feature_index,\n","                                                      varlen_sparse_feature_columns)\n","        varlen_sparse_embedding_list = get_varlen_pooling_list(sequence_embed_dict, X, self.feature_index,\n","                                                               varlen_sparse_feature_columns, self.device)\n","\n","        dense_value_list = [X[:, self.feature_index[feat.name][0]:self.feature_index[feat.name][1]] for feat in\n","                            dense_feature_columns]\n","\n","        return sparse_embedding_list + varlen_sparse_embedding_list, dense_value_list\n","\n","    def compute_input_dim(self, feature_columns, include_sparse=True, include_dense=True, feature_group=False):\n","        sparse_feature_columns = list(\n","            filter(lambda x: isinstance(x, (SparseFeat, VarLenSparseFeat)), feature_columns)) if len(\n","            feature_columns) else []\n","        dense_feature_columns = list(\n","            filter(lambda x: isinstance(x, DenseFeat), feature_columns)) if len(feature_columns) else []\n","\n","        dense_input_dim = sum(\n","            map(lambda x: x.dimension, dense_feature_columns))\n","        if feature_group:\n","            sparse_input_dim = len(sparse_feature_columns)\n","        else:\n","            sparse_input_dim = sum(feat.embedding_dim for feat in sparse_feature_columns)\n","        input_dim = 0\n","        if include_sparse:\n","            input_dim += sparse_input_dim\n","        if include_dense:\n","            input_dim += dense_input_dim\n","        return input_dim\n","\n","    def add_regularization_weight(self, weight_list, l1=0.0, l2=0.0):\n","        # For a Parameter, put it in a list to keep Compatible with get_regularization_loss()\n","        if isinstance(weight_list, torch.nn.parameter.Parameter):\n","            weight_list = [weight_list]\n","        # For generators, filters and ParameterLists, convert them to a list of tensors to avoid bugs.\n","        # e.g., we can't pickle generator objects when we save the model.\n","        else:\n","            weight_list = list(weight_list)\n","        self.regularization_weight.append((weight_list, l1, l2))\n","\n","    def get_regularization_loss(self, ):\n","        total_reg_loss = torch.zeros((1,), device=self.device)\n","        for weight_list, l1, l2 in self.regularization_weight:\n","            for w in weight_list:\n","                if isinstance(w, tuple):\n","                    parameter = w[1]  # named_parameters\n","                else:\n","                    parameter = w\n","                if l1 > 0:\n","                    total_reg_loss += torch.sum(l1 * torch.abs(parameter))\n","                if l2 > 0:\n","                    try:\n","                        total_reg_loss += torch.sum(l2 * torch.square(parameter))\n","                    except AttributeError:\n","                        total_reg_loss += torch.sum(l2 * parameter * parameter)\n","\n","        return total_reg_loss\n","\n","    def add_auxiliary_loss(self, aux_loss, alpha):\n","        self.aux_loss = aux_loss * alpha\n","\n","    def compile(self, optimizer,\n","                loss=None,\n","                metrics=None,\n","                ):\n","        \"\"\"\n","        :param optimizer: String (name of optimizer) or optimizer instance. See [optimizers](https://pytorch.org/docs/stable/optim.html).\n","        :param loss: String (name of objective function) or objective function. See [losses](https://pytorch.org/docs/stable/nn.functional.html#loss-functions).\n","        :param metrics: List of metrics to be evaluated by the model during training and testing. Typically you will use `metrics=['accuracy']`.\n","        \"\"\"\n","        self.metrics_names = [\"loss\"]\n","        self.optim = self._get_optim(optimizer)\n","        self.loss_func = self._get_loss_func(loss)\n","        self.metrics = self._get_metrics(metrics)\n","\n","\n","    def _get_optim(self, optimizer):\n","        if isinstance(optimizer, str):\n","            if optimizer == \"sgd\":\n","                optim = torch.optim.SGD(self.parameters(), lr=0.01)\n","            elif optimizer == \"adam\":\n","                optim = torch.optim.Adam(self.parameters())  # 0.001\n","            elif optimizer == \"adagrad\":\n","                optim = torch.optim.Adagrad(self.parameters())  # 0.01\n","            elif optimizer == \"rmsprop\":\n","                optim = torch.optim.RMSprop(self.parameters())\n","            else:\n","                raise NotImplementedError\n","        else:\n","            optim = optimizer\n","        return optim\n","\n","    def _get_loss_func(self, loss):\n","        if isinstance(loss, str):\n","            if loss == \"binary_crossentropy\":\n","                loss_func = F.binary_cross_entropy\n","            elif loss == \"mse\":\n","                loss_func = F.mse_loss\n","            elif loss == \"mae\":\n","                loss_func = F.l1_loss\n","            else:\n","                raise NotImplementedError\n","        else:\n","            loss_func = loss\n","        return loss_func\n","\n","    def _log_loss(self, y_true, y_pred, eps=1e-7, normalize=True, sample_weight=None, labels=None):\n","        # change eps to improve calculation accuracy\n","        return log_loss(y_true,\n","                        y_pred,\n","                        eps,\n","                        normalize,\n","                        sample_weight,\n","                        labels)\n","\n","    def _get_metrics(self, metrics, set_eps=False):\n","        metrics_ = {}\n","        if metrics:\n","            for metric in metrics:\n","                if metric == \"binary_crossentropy\" or metric == \"logloss\":\n","                    if set_eps:\n","                        metrics_[metric] = self._log_loss\n","                    else:\n","                        metrics_[metric] = log_loss\n","                if metric == \"auc\":\n","                    metrics_[metric] = roc_auc_score\n","                if metric == \"mse\":\n","                    metrics_[metric] = mean_squared_error\n","                if metric == \"accuracy\" or metric == \"acc\":\n","                    metrics_[metric] = lambda y_true, y_pred: accuracy_score(\n","                        y_true, np.where(y_pred > 0.5, 1, 0))\n","                self.metrics_names.append(metric)\n","        return metrics_\n","\n","    def _in_multi_worker_mode(self):\n","        # used for EarlyStopping in tf1.15\n","        return None\n","\n","    @property\n","    def embedding_size(self, ):\n","        feature_columns = self.dnn_feature_columns\n","        sparse_feature_columns = list(\n","            filter(lambda x: isinstance(x, (SparseFeat, VarLenSparseFeat)), feature_columns)) if len(\n","            feature_columns) else []\n","        embedding_size_set = set([feat.embedding_dim for feat in sparse_feature_columns])\n","        if len(embedding_size_set) > 1:\n","            raise ValueError(\"embedding_dim of SparseFeat and VarlenSparseFeat must be same in this model!\")\n","        return list(embedding_size_set)[0]"],"execution_count":26,"outputs":[]},{"cell_type":"code","metadata":{"id":"SUstHzcSgJDf","executionInfo":{"status":"ok","timestamp":1632918961441,"user_tz":-330,"elapsed":14,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}}},"source":["class FM(nn.Module):\n","    \"\"\"Factorization Machine models pairwise (order-2) feature interactions\n","     without linear term and bias.\n","      Input shape\n","        - 3D tensor with shape: ``(batch_size,field_size,embedding_size)``.\n","      Output shape\n","        - 2D tensor with shape: ``(batch_size, 1)``.\n","      References\n","        - [Factorization Machines](https://www.csie.ntu.edu.tw/~b97053/paper/Rendle2010FM.pdf)\n","    \"\"\"\n","\n","    def __init__(self):\n","        super(FM, self).__init__()\n","\n","    def forward(self, inputs):\n","        fm_input = inputs\n","\n","        square_of_sum = torch.pow(torch.sum(fm_input, dim=1, keepdim=True), 2)\n","        sum_of_square = torch.sum(fm_input * fm_input, dim=1, keepdim=True)\n","        cross_term = square_of_sum - sum_of_square\n","        cross_term = 0.5 * torch.sum(cross_term, dim=2, keepdim=False)\n","\n","        return cross_term"],"execution_count":27,"outputs":[]},{"cell_type":"code","metadata":{"id":"osBcuoKjgI_k","executionInfo":{"status":"ok","timestamp":1632918961444,"user_tz":-330,"elapsed":15,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}}},"source":["class AFMLayer(nn.Module):\n","    \"\"\"Attentonal Factorization Machine models pairwise (order-2) feature\n","    interactions without linear term and bias.\n","      Input shape\n","        - A list of 3D tensor with shape: ``(batch_size,1,embedding_size)``.\n","      Output shape\n","        - 2D tensor with shape: ``(batch_size, 1)``.\n","      Arguments\n","        - **in_features** : Positive integer, dimensionality of input features.\n","        - **attention_factor** : Positive integer, dimensionality of the\n","         attention network output space.\n","        - **l2_reg_w** : float between 0 and 1. L2 regularizer strength\n","         applied to attention network.\n","        - **dropout_rate** : float between in [0,1). Fraction of the attention net output units to dropout.\n","        - **seed** : A Python integer to use as random seed.\n","      References\n","        - [Attentional Factorization Machines : Learning the Weight of Feature\n","        Interactions via Attention Networks](https://arxiv.org/pdf/1708.04617.pdf)\n","    \"\"\"\n","\n","    def __init__(self, in_features, attention_factor=4, l2_reg_w=0, dropout_rate=0, seed=1024, device='cpu'):\n","        super(AFMLayer, self).__init__()\n","        self.attention_factor = attention_factor\n","        self.l2_reg_w = l2_reg_w\n","        self.dropout_rate = dropout_rate\n","        self.seed = seed\n","        embedding_size = in_features\n","\n","        self.attention_W = nn.Parameter(torch.Tensor(\n","            embedding_size, self.attention_factor))\n","\n","        self.attention_b = nn.Parameter(torch.Tensor(self.attention_factor))\n","\n","        self.projection_h = nn.Parameter(\n","            torch.Tensor(self.attention_factor, 1))\n","\n","        self.projection_p = nn.Parameter(torch.Tensor(embedding_size, 1))\n","\n","        for tensor in [self.attention_W, self.projection_h, self.projection_p]:\n","            nn.init.xavier_normal_(tensor, )\n","\n","        for tensor in [self.attention_b]:\n","            nn.init.zeros_(tensor, )\n","\n","        self.dropout = nn.Dropout(dropout_rate)\n","\n","        self.to(device)\n","\n","    def forward(self, inputs):\n","        embeds_vec_list = inputs\n","        row = []\n","        col = []\n","\n","        for r, c in itertools.combinations(embeds_vec_list, 2):\n","            row.append(r)\n","            col.append(c)\n","\n","        p = torch.cat(row, dim=1)\n","        q = torch.cat(col, dim=1)\n","        inner_product = p * q\n","\n","        bi_interaction = inner_product\n","        attention_temp = F.relu(torch.tensordot(\n","            bi_interaction, self.attention_W, dims=([-1], [0])) + self.attention_b)\n","\n","        self.normalized_att_score = F.softmax(torch.tensordot(\n","            attention_temp, self.projection_h, dims=([-1], [0])), dim=1)\n","        attention_output = torch.sum(\n","            self.normalized_att_score * bi_interaction, dim=1)\n","\n","        attention_output = self.dropout(attention_output)  # training\n","\n","        afm_out = torch.tensordot(\n","            attention_output, self.projection_p, dims=([-1], [0]))\n","        return afm_out"],"execution_count":28,"outputs":[]},{"cell_type":"code","metadata":{"id":"-gOzxAWWgVy6","executionInfo":{"status":"ok","timestamp":1632918961445,"user_tz":-330,"elapsed":15,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}}},"source":["class AFM(BaseModel):\n","    \"\"\"Instantiates the Attentional Factorization Machine architecture.\n","\n","    :param linear_feature_columns: An iterable containing all the features used by linear part of the model.\n","    :param dnn_feature_columns: An iterable containing all the features used by deep part of the model.\n","    :param use_attention: bool,whether use attention or not,if set to ``False``.it is the same as **standard Factorization Machine**\n","    :param attention_factor: positive integer,units in attention net\n","    :param l2_reg_linear: float. L2 regularizer strength applied to linear part\n","    :param l2_reg_embedding: float. L2 regularizer strength applied to embedding vector\n","    :param l2_reg_att: float. L2 regularizer strength applied to attention net\n","    :param afm_dropout: float in [0,1), Fraction of the attention net output units to dropout.\n","    :param init_std: float,to use as the initialize std of embedding vector\n","    :param seed: integer ,to use as random seed.\n","    :param task: str, ``\"binary\"`` for  binary logloss or  ``\"regression\"`` for regression loss\n","    :param device: str, ``\"cpu\"`` or ``\"cuda:0\"``\n","    :param gpus: list of int or torch.device for multiple gpus. If None, run on `device`. `gpus[0]` should be the same gpu with `device`.\n","    :return: A PyTorch model instance.\n","\n","    \"\"\"\n","\n","    def __init__(self, linear_feature_columns, dnn_feature_columns, use_attention=True, attention_factor=8,\n","                 l2_reg_linear=1e-5, l2_reg_embedding=1e-5, l2_reg_att=1e-5, afm_dropout=0, init_std=0.0001, seed=1024,\n","                 task='binary', device='cpu', gpus=None):\n","        super(AFM, self).__init__(linear_feature_columns, dnn_feature_columns, l2_reg_linear=l2_reg_linear,\n","                                  l2_reg_embedding=l2_reg_embedding, init_std=init_std, seed=seed, task=task,\n","                                  device=device, gpus=gpus)\n","\n","        self.use_attention = use_attention\n","\n","        if use_attention:\n","            self.fm = AFMLayer(self.embedding_size, attention_factor, l2_reg_att, afm_dropout,\n","                               seed, device)\n","            self.add_regularization_weight(self.fm.attention_W, l2=l2_reg_att)\n","        else:\n","            self.fm = FM()\n","\n","        self.to(device)\n","\n","    def forward(self, X):\n","\n","        sparse_embedding_list, _ = self.input_from_feature_columns(X, self.dnn_feature_columns,\n","                                                                   self.embedding_dict, support_dense=False)\n","        logit = self.linear_model(X)\n","        if len(sparse_embedding_list) > 0:\n","            if self.use_attention:\n","                logit += self.fm(sparse_embedding_list)\n","            else:\n","                logit += self.fm(torch.cat(sparse_embedding_list, dim=1))\n","\n","        y_pred = self.out(logit)\n","\n","        return y_pred"],"execution_count":29,"outputs":[]},{"cell_type":"code","metadata":{"id":"ETziN7SghPel","executionInfo":{"status":"ok","timestamp":1632918515246,"user_tz":-330,"elapsed":560,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}}},"source":["# !pip install ipytest\n","import ipytest\n","ipytest.autoconfig()"],"execution_count":11,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":0},"id":"9FhcqcHLggCP","executionInfo":{"status":"ok","timestamp":1632919064899,"user_tz":-330,"elapsed":437,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}},"outputId":"a9732310-8139-4d0c-b2c3-92392a6cb127"},"source":["%%ipytest\n","\n","import pytest\n","\n","SAMPLE_SIZE = 64\n","\n","\n","def gen_sequence(dim, max_len, sample_size):\n","    return np.array([np.random.randint(0, dim, max_len) for _ in range(sample_size)]), np.random.randint(1, max_len + 1,\n","                                                                                                         sample_size)\n","\n","\n","def get_test_data(sample_size=1000, embedding_size=4, sparse_feature_num=1, dense_feature_num=1,\n","                  sequence_feature=['sum', 'mean', 'max'], classification=True, include_length=False,\n","                  hash_flag=False, prefix=''):\n","\n","\n","    feature_columns = []\n","    model_input = {}\n","\n","\n","    if 'weight'  in sequence_feature:\n","        feature_columns.append(VarLenSparseFeat(SparseFeat(prefix+\"weighted_seq\",vocabulary_size=2,embedding_dim=embedding_size),maxlen=3,length_name=prefix+\"weighted_seq\"+\"_seq_length\",weight_name=prefix+\"weight\"))\n","        s_input, s_len_input = gen_sequence(\n","            2, 3, sample_size)\n","\n","        model_input[prefix+\"weighted_seq\"] = s_input\n","        model_input[prefix+'weight'] = np.random.randn(sample_size,3,1)\n","        model_input[prefix+\"weighted_seq\"+\"_seq_length\"] = s_len_input\n","        sequence_feature.pop(sequence_feature.index('weight'))\n","\n","\n","    for i in range(sparse_feature_num):\n","        dim = np.random.randint(1, 10)\n","        feature_columns.append(SparseFeat(prefix+'sparse_feature_'+str(i), dim,embedding_size,dtype=torch.int32))\n","    for i in range(dense_feature_num):\n","        feature_columns.append(DenseFeat(prefix+'dense_feature_'+str(i), 1,dtype=torch.float32))\n","    for i, mode in enumerate(sequence_feature):\n","        dim = np.random.randint(1, 10)\n","        maxlen = np.random.randint(1, 10)\n","        feature_columns.append(\n","            VarLenSparseFeat(SparseFeat(prefix +'sequence_' + mode,vocabulary_size=dim,  embedding_dim=embedding_size), maxlen=maxlen, combiner=mode))\n","\n","    for fc in feature_columns:\n","        if isinstance(fc,SparseFeat):\n","            model_input[fc.name]= np.random.randint(0, fc.vocabulary_size, sample_size)\n","        elif isinstance(fc,DenseFeat):\n","            model_input[fc.name] = np.random.random(sample_size)\n","        else:\n","            s_input, s_len_input = gen_sequence(\n","                fc.vocabulary_size, fc.maxlen, sample_size)\n","            model_input[fc.name] = s_input\n","            if include_length:\n","                fc.length_name = prefix+\"sequence_\"+str(i)+'_seq_length'\n","                model_input[prefix+\"sequence_\"+str(i)+'_seq_length'] = s_len_input\n","\n","    if classification:\n","        y = np.random.randint(0, 2, sample_size)\n","    else:\n","        y = np.random.random(sample_size)\n","\n","    return model_input, y, feature_columns\n","\n","\n","def layer_test(layer_cls, kwargs = {}, input_shape=None, \n","               input_dtype=torch.float32, input_data=None, expected_output=None,\n","               expected_output_shape=None, expected_output_dtype=None, fixed_batch_size=False):\n","    '''check layer is valid or not\n","    :param layer_cls:\n","    :param input_shape:\n","    :param input_dtype:\n","    :param input_data:\n","    :param expected_output:\n","    :param expected_output_dtype:\n","    :param fixed_batch_size:\n","    :return: output of the layer\n","    '''\n","    if input_data is None:\n","        # generate input data\n","        if not input_shape:\n","            raise ValueError(\"input shape should not be none\")\n","\n","        input_data_shape = list(input_shape)\n","        for i, e in enumerate(input_data_shape):\n","            if e is None:\n","                input_data_shape[i] = np.random.randint(1, 4)\n","        \n","        if all(isinstance(e, tuple) for e in input_data_shape):\n","            input_data = []\n","            for e in input_data_shape:\n","                rand_input = (10 * np.random.random(e))\n","                input_data.append(rand_input)\n","        else:\n","            rand_input = 10 * np.random.random(input_data_shape)\n","            input_data = rand_input\n","\n","    else:\n","        # use input_data to update other parameters\n","        if input_shape is None:\n","            input_shape = input_data.shape\n","    \n","    if expected_output_dtype is None:\n","        expected_output_dtype = input_dtype\n","    \n","    # layer initialization\n","    layer = layer_cls(**kwargs)\n","    \n","    if fixed_batch_size:\n","        inputs = torch.tensor(input_data.unsqueeze(0), dtype=input_dtype)\n","    else:\n","        inputs = torch.tensor(input_data, dtype=input_dtype)\n","    \n","    # calculate layer's output\n","    output = layer(inputs)\n","\n","    if not output.dtype == expected_output_dtype:\n","        raise AssertionError(\"layer output dtype does not match with the expected one\")\n","    \n","    if not expected_output_shape:\n","            raise ValueError(\"expected output shape should not be none\")\n","\n","    actual_output_shape = output.shape\n","    for expected_dim, actual_dim in zip(expected_output_shape, actual_output_shape):\n","        if expected_dim is not None:\n","            if not expected_dim == actual_dim:\n","                raise AssertionError(f\"expected_dim:{expected_dim}, actual_dim:{actual_dim}\")\n","    \n","    if expected_output is not None:\n","        # check whether output equals to expected output\n","        assert_allclose(output, expected_output, rtol=1e-3)\n","    \n","    return output\n","\n","\n","def check_model(model, model_name, x, y, check_model_io=True):\n","    '''\n","    compile model,train and evaluate it,then save/load weight and model file.\n","    :param model:\n","    :param model_name:\n","    :param x:\n","    :param y:\n","    :param check_model_io:\n","    :return:\n","    '''\n","\n","    model.compile('adam', 'binary_crossentropy',\n","                  metrics=['binary_crossentropy'])\n","    model.fit(x, y, batch_size=100, epochs=1, validation_split=0.5)\n","\n","    print(model_name + 'test, train valid pass!')\n","    torch.save(model.state_dict(), model_name + '_weights.h5')\n","    model.load_state_dict(torch.load(model_name + '_weights.h5'))\n","    os.remove(model_name + '_weights.h5')\n","    print(model_name + 'test save load weight pass!')\n","    if check_model_io:\n","        torch.save(model, model_name + '.h5')\n","        model = torch.load(model_name + '.h5')\n","        os.remove(model_name + '.h5')\n","        print(model_name + 'test save load model pass!')\n","    print(model_name + 'test pass!')\n","\n","def get_device(use_cuda = True):\n","    device = 'cpu'\n","    if use_cuda and torch.cuda.is_available():\n","        print('cuda ready...')\n","        device = 'cuda:0'\n","    return device\n","\n","\n","@pytest.mark.parametrize(\n","    'use_attention, sparse_feature_num, dense_feature_num',\n","    [(True, 3, 0), ]\n",")\n","def test_AFM(use_attention, sparse_feature_num, dense_feature_num):\n","    model_name = 'AFM'\n","    sample_size = SAMPLE_SIZE\n","    x, y, feature_columns = get_test_data(\n","        sample_size, sparse_feature_num=sparse_feature_num, dense_feature_num=dense_feature_num)\n","\n","    model = AFM(linear_feature_columns=feature_columns, dnn_feature_columns=feature_columns,\n","                use_attention=use_attention, afm_dropout=0.5, device=get_device())\n","\n","    check_model(model, model_name, x, y)\n","\n","    early_stopping = EarlyStopping(monitor='val_binary_crossentropy', min_delta=0, verbose=1, patience=0, mode='min')\n","\n","    # test callbacks\n","    model_checkpoint = ModelCheckpoint(filepath='model.ckpt', monitor='val_binary_crossentropy', verbose=1,\n","                                       save_best_only=True,\n","                                       save_weights_only=False, mode='max', save_freq=1)\n","    model.fit(x, y, batch_size=64, epochs=3, validation_split=0.5, callbacks=[early_stopping, model_checkpoint])\n","\n","    model_checkpoint = ModelCheckpoint(filepath='model.ckpt', monitor='val_binary_crossentropy', verbose=1,\n","                                       save_best_only=False,\n","                                       save_weights_only=False, mode='max', save_freq=1)\n","\n","    model.fit(x, y, batch_size=64, epochs=3, validation_split=0.5, callbacks=[early_stopping, model_checkpoint])"],"execution_count":31,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[32m.\u001b[0m\u001b[32m                                                                                            [100%]\u001b[0m\n","\u001b[32m\u001b[32m\u001b[1m1 passed\u001b[0m\u001b[32m in 0.08s\u001b[0m\u001b[0m\n"]}]},{"cell_type":"markdown","metadata":{"id":"YzZjeP50kMrG"},"source":["## DeepCTR Tensorflow Implementation"]},{"cell_type":"code","metadata":{"id":"N1p8Af3QkSVK","executionInfo":{"status":"ok","timestamp":1632919960636,"user_tz":-330,"elapsed":2483,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}}},"source":["import tensorflow as tf\n","from collections import namedtuple, OrderedDict\n","from copy import copy\n","from itertools import chain\n","\n","from tensorflow.python.keras.initializers import RandomNormal, Zeros\n","from tensorflow.python.keras.layers import Input, Lambda\n","\n","import numpy as np\n","import tensorflow as tf\n","from tensorflow.python.keras import backend as K\n","from tensorflow.python.keras.initializers import TruncatedNormal\n","from tensorflow.python.keras.layers import LSTM, Lambda, Layer\n","\n","import tensorflow as tf\n","from tensorflow.python.keras.layers import Flatten\n","from tensorflow.python.ops.lookup_ops import TextFileInitializer\n","\n","try:\n","    from tensorflow.python.ops.lookup_ops import StaticHashTable\n","except ImportError:\n","    from tensorflow.python.ops.lookup_ops import HashTable as StaticHashTable\n","\n","import tensorflow as tf\n","from tensorflow.python.keras import backend as K\n","from tensorflow.python.keras.initializers import Zeros, glorot_normal\n","from tensorflow.python.keras.layers import Layer\n","from tensorflow.python.keras.regularizers import l2\n","\n","import itertools\n","\n","import tensorflow as tf\n","from tensorflow.python.keras import backend as K\n","from tensorflow.python.keras.backend import batch_dot\n","from tensorflow.python.keras.initializers import (Zeros, glorot_normal,\n","                                                  glorot_uniform, TruncatedNormal)\n","from tensorflow.python.keras.layers import Layer\n","from tensorflow.python.keras.regularizers import l2\n","from tensorflow.python.layers import utils"],"execution_count":1,"outputs":[]},{"cell_type":"code","metadata":{"id":"_tWuZGftlYA_","executionInfo":{"status":"ok","timestamp":1632919964121,"user_tz":-330,"elapsed":1763,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}}},"source":["class NoMask(tf.keras.layers.Layer):\n","    def __init__(self, **kwargs):\n","        super(NoMask, self).__init__(**kwargs)\n","\n","    def build(self, input_shape):\n","        # Be sure to call this somewhere!\n","        super(NoMask, self).build(input_shape)\n","\n","    def call(self, x, mask=None, **kwargs):\n","        return x\n","\n","    def compute_mask(self, inputs, mask):\n","        return None\n","\n","\n","class Hash(tf.keras.layers.Layer):\n","    \"\"\"Looks up keys in a table when setup `vocabulary_path`, which outputs the corresponding values.\n","    If `vocabulary_path` is not set, `Hash` will hash the input to [0,num_buckets). When `mask_zero` = True,\n","    input value `0` or `0.0` will be set to `0`, and other value will be set in range [1,num_buckets).\n","    The following snippet initializes a `Hash` with `vocabulary_path` file with the first column as keys and\n","    second column as values:\n","    * `1,emerson`\n","    * `2,lake`\n","    * `3,palmer`\n","    >>> hash = Hash(\n","    ...   num_buckets=3+1,\n","    ...   vocabulary_path=filename,\n","    ...   default_value=0)\n","    >>> hash(tf.constant('lake')).numpy()\n","    2\n","    >>> hash(tf.constant('lakeemerson')).numpy()\n","    0\n","    Args:\n","        num_buckets: An `int` that is >= 1. The number of buckets or the vocabulary size + 1\n","            when `vocabulary_path` is setup.\n","        mask_zero: default is False. The `Hash` value will hash input `0` or `0.0` to value `0` when\n","            the `mask_zero` is `True`. `mask_zero` is not used when `vocabulary_path` is setup.\n","        vocabulary_path: default `None`. The `CSV` text file path of the vocabulary hash, which contains\n","            two columns seperated by delimiter `comma`, the first column is the value and the second is\n","            the key. The key data type is `string`, the value data type is `int`. The path must\n","            be accessible from wherever `Hash` is initialized.\n","        default_value: default '0'. The default value if a key is missing in the table.\n","        **kwargs: Additional keyword arguments.\n","    \"\"\"\n","\n","    def __init__(self, num_buckets, mask_zero=False, vocabulary_path=None, default_value=0, **kwargs):\n","        self.num_buckets = num_buckets\n","        self.mask_zero = mask_zero\n","        self.vocabulary_path = vocabulary_path\n","        self.default_value = default_value\n","        if self.vocabulary_path:\n","            initializer = TextFileInitializer(vocabulary_path, 'string', 1, 'int64', 0, delimiter=',')\n","            self.hash_table = StaticHashTable(initializer, default_value=self.default_value)\n","        super(Hash, self).__init__(**kwargs)\n","\n","    def build(self, input_shape):\n","        # Be sure to call this somewhere!\n","        super(Hash, self).build(input_shape)\n","\n","    def call(self, x, mask=None, **kwargs):\n","\n","        if x.dtype != tf.string:\n","            zero = tf.as_string(tf.zeros([1], dtype=x.dtype))\n","            x = tf.as_string(x, )\n","        else:\n","            zero = tf.as_string(tf.zeros([1], dtype='int32'))\n","\n","        if self.vocabulary_path:\n","            hash_x = self.hash_table.lookup(x)\n","            return hash_x\n","\n","        num_buckets = self.num_buckets if not self.mask_zero else self.num_buckets - 1\n","        try:\n","            hash_x = tf.string_to_hash_bucket_fast(x, num_buckets,\n","                                                   name=None)  # weak hash\n","        except AttributeError:\n","            hash_x = tf.strings.to_hash_bucket_fast(x, num_buckets,\n","                                                    name=None)  # weak hash\n","        if self.mask_zero:\n","            mask = tf.cast(tf.not_equal(x, zero), dtype='int64')\n","            hash_x = (hash_x + 1) * mask\n","\n","        return hash_x\n","\n","    def compute_output_shape(self, input_shape):\n","        return input_shape\n","\n","    def get_config(self, ):\n","        config = {'num_buckets': self.num_buckets, 'mask_zero': self.mask_zero, 'vocabulary_path': self.vocabulary_path,\n","                  'default_value': self.default_value}\n","        base_config = super(Hash, self).get_config()\n","        return dict(list(base_config.items()) + list(config.items()))\n","\n","\n","class Linear(tf.keras.layers.Layer):\n","\n","    def __init__(self, l2_reg=0.0, mode=0, use_bias=False, seed=1024, **kwargs):\n","\n","        self.l2_reg = l2_reg\n","        # self.l2_reg = tf.contrib.layers.l2_regularizer(float(l2_reg_linear))\n","        if mode not in [0, 1, 2]:\n","            raise ValueError(\"mode must be 0,1 or 2\")\n","        self.mode = mode\n","        self.use_bias = use_bias\n","        self.seed = seed\n","        super(Linear, self).__init__(**kwargs)\n","\n","    def build(self, input_shape):\n","        if self.use_bias:\n","            self.bias = self.add_weight(name='linear_bias',\n","                                        shape=(1,),\n","                                        initializer=tf.keras.initializers.Zeros(),\n","                                        trainable=True)\n","        if self.mode == 1:\n","            self.kernel = self.add_weight(\n","                'linear_kernel',\n","                shape=[int(input_shape[-1]), 1],\n","                initializer=tf.keras.initializers.glorot_normal(self.seed),\n","                regularizer=tf.keras.regularizers.l2(self.l2_reg),\n","                trainable=True)\n","        elif self.mode == 2:\n","            self.kernel = self.add_weight(\n","                'linear_kernel',\n","                shape=[int(input_shape[1][-1]), 1],\n","                initializer=tf.keras.initializers.glorot_normal(self.seed),\n","                regularizer=tf.keras.regularizers.l2(self.l2_reg),\n","                trainable=True)\n","\n","        super(Linear, self).build(input_shape)  # Be sure to call this somewhere!\n","\n","    def call(self, inputs, **kwargs):\n","        if self.mode == 0:\n","            sparse_input = inputs\n","            linear_logit = reduce_sum(sparse_input, axis=-1, keep_dims=True)\n","        elif self.mode == 1:\n","            dense_input = inputs\n","            fc = tf.tensordot(dense_input, self.kernel, axes=(-1, 0))\n","            linear_logit = fc\n","        else:\n","            sparse_input, dense_input = inputs\n","            fc = tf.tensordot(dense_input, self.kernel, axes=(-1, 0))\n","            linear_logit = reduce_sum(sparse_input, axis=-1, keep_dims=False) + fc\n","        if self.use_bias:\n","            linear_logit += self.bias\n","\n","        return linear_logit\n","\n","    def compute_output_shape(self, input_shape):\n","        return (None, 1)\n","\n","    def compute_mask(self, inputs, mask):\n","        return None\n","\n","    def get_config(self, ):\n","        config = {'mode': self.mode, 'l2_reg': self.l2_reg, 'use_bias': self.use_bias, 'seed': self.seed}\n","        base_config = super(Linear, self).get_config()\n","        return dict(list(base_config.items()) + list(config.items()))\n","\n","\n","def concat_func(inputs, axis=-1, mask=False):\n","    if not mask:\n","        inputs = list(map(NoMask(), inputs))\n","    if len(inputs) == 1:\n","        return inputs[0]\n","    else:\n","        return tf.keras.layers.Concatenate(axis=axis)(inputs)\n","\n","\n","def reduce_mean(input_tensor,\n","                axis=None,\n","                keep_dims=False,\n","                name=None,\n","                reduction_indices=None):\n","    try:\n","        return tf.reduce_mean(input_tensor,\n","                              axis=axis,\n","                              keep_dims=keep_dims,\n","                              name=name,\n","                              reduction_indices=reduction_indices)\n","    except TypeError:\n","        return tf.reduce_mean(input_tensor,\n","                              axis=axis,\n","                              keepdims=keep_dims,\n","                              name=name)\n","\n","\n","def reduce_sum(input_tensor,\n","               axis=None,\n","               keep_dims=False,\n","               name=None,\n","               reduction_indices=None):\n","    try:\n","        return tf.reduce_sum(input_tensor,\n","                             axis=axis,\n","                             keep_dims=keep_dims,\n","                             name=name,\n","                             reduction_indices=reduction_indices)\n","    except TypeError:\n","        return tf.reduce_sum(input_tensor,\n","                             axis=axis,\n","                             keepdims=keep_dims,\n","                             name=name)\n","\n","\n","def reduce_max(input_tensor,\n","               axis=None,\n","               keep_dims=False,\n","               name=None,\n","               reduction_indices=None):\n","    try:\n","        return tf.reduce_max(input_tensor,\n","                             axis=axis,\n","                             keep_dims=keep_dims,\n","                             name=name,\n","                             reduction_indices=reduction_indices)\n","    except TypeError:\n","        return tf.reduce_max(input_tensor,\n","                             axis=axis,\n","                             keepdims=keep_dims,\n","                             name=name)\n","\n","\n","def div(x, y, name=None):\n","    try:\n","        return tf.div(x, y, name=name)\n","    except AttributeError:\n","        return tf.divide(x, y, name=name)\n","\n","\n","def softmax(logits, dim=-1, name=None):\n","    try:\n","        return tf.nn.softmax(logits, dim=dim, name=name)\n","    except TypeError:\n","        return tf.nn.softmax(logits, axis=dim, name=name)\n","\n","\n","class Add(tf.keras.layers.Layer):\n","    def __init__(self, **kwargs):\n","        super(Add, self).__init__(**kwargs)\n","\n","    def build(self, input_shape):\n","        # Be sure to call this somewhere!\n","        super(Add, self).build(input_shape)\n","\n","    def call(self, inputs, **kwargs):\n","        if not isinstance(inputs, list):\n","            return inputs\n","        if len(inputs) == 1:\n","            return inputs[0]\n","        if len(inputs) == 0:\n","            return tf.constant([[0.0]])\n","\n","        return tf.keras.layers.add(inputs)\n","\n","\n","def add_func(inputs):\n","    return Add()(inputs)\n","\n","\n","def combined_dnn_input(sparse_embedding_list, dense_value_list):\n","    if len(sparse_embedding_list) > 0 and len(dense_value_list) > 0:\n","        sparse_dnn_input = Flatten()(concat_func(sparse_embedding_list))\n","        dense_dnn_input = Flatten()(concat_func(dense_value_list))\n","        return concat_func([sparse_dnn_input, dense_dnn_input])\n","    elif len(sparse_embedding_list) > 0:\n","        return Flatten()(concat_func(sparse_embedding_list))\n","    elif len(dense_value_list) > 0:\n","        return Flatten()(concat_func(dense_value_list))\n","    else:\n","        raise NotImplementedError(\"dnn_feature_columns can not be empty list\")"],"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"id":"rIMdpmTXkfJh","executionInfo":{"status":"ok","timestamp":1632919965604,"user_tz":-330,"elapsed":982,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}}},"source":["class SequencePoolingLayer(Layer):\n","    \"\"\"The SequencePoolingLayer is used to apply pooling operation(sum,mean,max) on variable-length sequence feature/multi-value feature.\n","      Input shape\n","        - A list of two  tensor [seq_value,seq_len]\n","        - seq_value is a 3D tensor with shape: ``(batch_size, T, embedding_size)``\n","        - seq_len is a 2D tensor with shape : ``(batch_size, 1)``,indicate valid length of each sequence.\n","      Output shape\n","        - 3D tensor with shape: ``(batch_size, 1, embedding_size)``.\n","      Arguments\n","        - **mode**:str.Pooling operation to be used,can be sum,mean or max.\n","        - **supports_masking**:If True,the input need to support masking.\n","    \"\"\"\n","\n","    def __init__(self, mode='mean', supports_masking=False, **kwargs):\n","\n","        if mode not in ['sum', 'mean', 'max']:\n","            raise ValueError(\"mode must be sum or mean\")\n","        self.mode = mode\n","        self.eps = tf.constant(1e-8, tf.float32)\n","        super(SequencePoolingLayer, self).__init__(**kwargs)\n","\n","        self.supports_masking = supports_masking\n","\n","    def build(self, input_shape):\n","        if not self.supports_masking:\n","            self.seq_len_max = int(input_shape[0][1])\n","        super(SequencePoolingLayer, self).build(\n","            input_shape)  # Be sure to call this somewhere!\n","\n","    def call(self, seq_value_len_list, mask=None, **kwargs):\n","        if self.supports_masking:\n","            if mask is None:\n","                raise ValueError(\n","                    \"When supports_masking=True,input must support masking\")\n","            uiseq_embed_list = seq_value_len_list\n","            mask = tf.cast(mask, tf.float32)  # tf.to_float(mask)\n","            user_behavior_length = reduce_sum(mask, axis=-1, keep_dims=True)\n","            mask = tf.expand_dims(mask, axis=2)\n","        else:\n","            uiseq_embed_list, user_behavior_length = seq_value_len_list\n","\n","            mask = tf.sequence_mask(user_behavior_length,\n","                                    self.seq_len_max, dtype=tf.float32)\n","            mask = tf.transpose(mask, (0, 2, 1))\n","\n","        embedding_size = uiseq_embed_list.shape[-1]\n","\n","        mask = tf.tile(mask, [1, 1, embedding_size])\n","\n","        if self.mode == \"max\":\n","            hist = uiseq_embed_list - (1 - mask) * 1e9\n","            return reduce_max(hist, 1, keep_dims=True)\n","\n","        hist = reduce_sum(uiseq_embed_list * mask, 1, keep_dims=False)\n","\n","        if self.mode == \"mean\":\n","            hist = div(hist, tf.cast(user_behavior_length, tf.float32) + self.eps)\n","\n","        hist = tf.expand_dims(hist, axis=1)\n","        return hist\n","\n","    def compute_output_shape(self, input_shape):\n","        if self.supports_masking:\n","            return (None, 1, input_shape[-1])\n","        else:\n","            return (None, 1, input_shape[0][-1])\n","\n","    def compute_mask(self, inputs, mask):\n","        return None\n","\n","    def get_config(self, ):\n","        config = {'mode': self.mode, 'supports_masking': self.supports_masking}\n","        base_config = super(SequencePoolingLayer, self).get_config()\n","        return dict(list(base_config.items()) + list(config.items()))\n","\n","\n","class WeightedSequenceLayer(Layer):\n","    \"\"\"The WeightedSequenceLayer is used to apply weight score on variable-length sequence feature/multi-value feature.\n","      Input shape\n","        - A list of two  tensor [seq_value,seq_len,seq_weight]\n","        - seq_value is a 3D tensor with shape: ``(batch_size, T, embedding_size)``\n","        - seq_len is a 2D tensor with shape : ``(batch_size, 1)``,indicate valid length of each sequence.\n","        - seq_weight is a 3D tensor with shape: ``(batch_size, T, 1)``\n","      Output shape\n","        - 3D tensor with shape: ``(batch_size, T, embedding_size)``.\n","      Arguments\n","        - **weight_normalization**: bool.Whether normalize the weight score before applying to sequence.\n","        - **supports_masking**:If True,the input need to support masking.\n","    \"\"\"\n","\n","    def __init__(self, weight_normalization=True, supports_masking=False, **kwargs):\n","        super(WeightedSequenceLayer, self).__init__(**kwargs)\n","        self.weight_normalization = weight_normalization\n","        self.supports_masking = supports_masking\n","\n","    def build(self, input_shape):\n","        if not self.supports_masking:\n","            self.seq_len_max = int(input_shape[0][1])\n","        super(WeightedSequenceLayer, self).build(\n","            input_shape)  # Be sure to call this somewhere!\n","\n","    def call(self, input_list, mask=None, **kwargs):\n","        if self.supports_masking:\n","            if mask is None:\n","                raise ValueError(\n","                    \"When supports_masking=True,input must support masking\")\n","            key_input, value_input = input_list\n","            mask = tf.expand_dims(mask[0], axis=2)\n","        else:\n","            key_input, key_length_input, value_input = input_list\n","            mask = tf.sequence_mask(key_length_input,\n","                                    self.seq_len_max, dtype=tf.bool)\n","            mask = tf.transpose(mask, (0, 2, 1))\n","\n","        embedding_size = key_input.shape[-1]\n","\n","        if self.weight_normalization:\n","            paddings = tf.ones_like(value_input) * (-2 ** 32 + 1)\n","        else:\n","            paddings = tf.zeros_like(value_input)\n","        value_input = tf.where(mask, value_input, paddings)\n","\n","        if self.weight_normalization:\n","            value_input = softmax(value_input, dim=1)\n","\n","        if len(value_input.shape) == 2:\n","            value_input = tf.expand_dims(value_input, axis=2)\n","            value_input = tf.tile(value_input, [1, 1, embedding_size])\n","\n","        return tf.multiply(key_input, value_input)\n","\n","    def compute_output_shape(self, input_shape):\n","        return input_shape[0]\n","\n","    def compute_mask(self, inputs, mask):\n","        if self.supports_masking:\n","            return mask[0]\n","        else:\n","            return None\n","\n","    def get_config(self, ):\n","        config = {'weight_normalization': self.weight_normalization, 'supports_masking': self.supports_masking}\n","        base_config = super(WeightedSequenceLayer, self).get_config()\n","        return dict(list(base_config.items()) + list(config.items()))"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"id":"Qdro6HKXkfHE","executionInfo":{"status":"ok","timestamp":1632919968744,"user_tz":-330,"elapsed":515,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}}},"source":["def get_inputs_list(inputs):\n","    return list(chain(*list(map(lambda x: x.values(), filter(lambda x: x is not None, inputs)))))\n","\n","\n","def create_embedding_dict(sparse_feature_columns, varlen_sparse_feature_columns, seed, l2_reg,\n","                          prefix='sparse_', seq_mask_zero=True):\n","    sparse_embedding = {}\n","    for feat in sparse_feature_columns:\n","        emb = Embedding(feat.vocabulary_size, feat.embedding_dim,\n","                        embeddings_initializer=feat.embeddings_initializer,\n","                        embeddings_regularizer=l2(l2_reg),\n","                        name=prefix + '_emb_' + feat.embedding_name)\n","        emb.trainable = feat.trainable\n","        sparse_embedding[feat.embedding_name] = emb\n","\n","    if varlen_sparse_feature_columns and len(varlen_sparse_feature_columns) > 0:\n","        for feat in varlen_sparse_feature_columns:\n","            # if feat.name not in sparse_embedding:\n","            emb = Embedding(feat.vocabulary_size, feat.embedding_dim,\n","                            embeddings_initializer=feat.embeddings_initializer,\n","                            embeddings_regularizer=l2(\n","                                l2_reg),\n","                            name=prefix + '_seq_emb_' + feat.name,\n","                            mask_zero=seq_mask_zero)\n","            emb.trainable = feat.trainable\n","            sparse_embedding[feat.embedding_name] = emb\n","    return sparse_embedding\n","\n","\n","def get_embedding_vec_list(embedding_dict, input_dict, sparse_feature_columns, return_feat_list=(), mask_feat_list=()):\n","    embedding_vec_list = []\n","    for fg in sparse_feature_columns:\n","        feat_name = fg.name\n","        if len(return_feat_list) == 0 or feat_name in return_feat_list:\n","            if fg.use_hash:\n","                lookup_idx = Hash(fg.vocabulary_size, mask_zero=(feat_name in mask_feat_list), vocabulary_path=fg.vocabulary_path)(input_dict[feat_name])\n","            else:\n","                lookup_idx = input_dict[feat_name]\n","\n","            embedding_vec_list.append(embedding_dict[feat_name](lookup_idx))\n","\n","    return embedding_vec_list\n","\n","\n","def create_embedding_matrix(feature_columns, l2_reg, seed, prefix=\"\", seq_mask_zero=True):\n","    from . import feature_column as fc_lib\n","\n","    sparse_feature_columns = list(\n","        filter(lambda x: isinstance(x, fc_lib.SparseFeat), feature_columns)) if feature_columns else []\n","    varlen_sparse_feature_columns = list(\n","        filter(lambda x: isinstance(x, fc_lib.VarLenSparseFeat), feature_columns)) if feature_columns else []\n","    sparse_emb_dict = create_embedding_dict(sparse_feature_columns, varlen_sparse_feature_columns, seed,\n","                                            l2_reg, prefix=prefix + 'sparse', seq_mask_zero=seq_mask_zero)\n","    return sparse_emb_dict\n","\n","\n","def embedding_lookup(sparse_embedding_dict, sparse_input_dict, sparse_feature_columns, return_feat_list=(),\n","                     mask_feat_list=(), to_list=False):\n","    group_embedding_dict = defaultdict(list)\n","    for fc in sparse_feature_columns:\n","        feature_name = fc.name\n","        embedding_name = fc.embedding_name\n","        if (len(return_feat_list) == 0 or feature_name in return_feat_list):\n","            if fc.use_hash:\n","                lookup_idx = Hash(fc.vocabulary_size, mask_zero=(feature_name in mask_feat_list), vocabulary_path=fc.vocabulary_path)(\n","                    sparse_input_dict[feature_name])\n","            else:\n","                lookup_idx = sparse_input_dict[feature_name]\n","\n","            group_embedding_dict[fc.group_name].append(sparse_embedding_dict[embedding_name](lookup_idx))\n","    if to_list:\n","        return list(chain.from_iterable(group_embedding_dict.values()))\n","    return group_embedding_dict\n","\n","\n","def varlen_embedding_lookup(embedding_dict, sequence_input_dict, varlen_sparse_feature_columns):\n","    varlen_embedding_vec_dict = {}\n","    for fc in varlen_sparse_feature_columns:\n","        feature_name = fc.name\n","        embedding_name = fc.embedding_name\n","        if fc.use_hash:\n","            lookup_idx = Hash(fc.vocabulary_size, mask_zero=True, vocabulary_path=fc.vocabulary_path)(sequence_input_dict[feature_name])\n","        else:\n","            lookup_idx = sequence_input_dict[feature_name]\n","        varlen_embedding_vec_dict[feature_name] = embedding_dict[embedding_name](lookup_idx)\n","    return varlen_embedding_vec_dict\n","\n","\n","def get_varlen_pooling_list(embedding_dict, features, varlen_sparse_feature_columns, to_list=False):\n","    pooling_vec_list = defaultdict(list)\n","    for fc in varlen_sparse_feature_columns:\n","        feature_name = fc.name\n","        combiner = fc.combiner\n","        feature_length_name = fc.length_name\n","        if feature_length_name is not None:\n","            if fc.weight_name is not None:\n","                seq_input = WeightedSequenceLayer(weight_normalization=fc.weight_norm)(\n","                    [embedding_dict[feature_name], features[feature_length_name], features[fc.weight_name]])\n","            else:\n","                seq_input = embedding_dict[feature_name]\n","            vec = SequencePoolingLayer(combiner, supports_masking=False)(\n","                [seq_input, features[feature_length_name]])\n","        else:\n","            if fc.weight_name is not None:\n","                seq_input = WeightedSequenceLayer(weight_normalization=fc.weight_norm, supports_masking=True)(\n","                    [embedding_dict[feature_name], features[fc.weight_name]])\n","            else:\n","                seq_input = embedding_dict[feature_name]\n","            vec = SequencePoolingLayer(combiner, supports_masking=True)(\n","                seq_input)\n","        pooling_vec_list[fc.group_name].append(vec)\n","    if to_list:\n","        return chain.from_iterable(pooling_vec_list.values())\n","    return pooling_vec_list\n","\n","\n","def get_dense_input(features, feature_columns):\n","    from . import feature_column as fc_lib\n","    dense_feature_columns = list(\n","        filter(lambda x: isinstance(x, fc_lib.DenseFeat), feature_columns)) if feature_columns else []\n","    dense_input_list = []\n","    for fc in dense_feature_columns:\n","        if fc.transform_fn is None:\n","            dense_input_list.append(features[fc.name])\n","        else:\n","            transform_result = Lambda(fc.transform_fn)(features[fc.name])\n","            dense_input_list.append(transform_result)\n","    return dense_input_list\n","\n","\n","def mergeDict(a, b):\n","    c = defaultdict(list)\n","    for k, v in a.items():\n","        c[k].extend(v)\n","    for k, v in b.items():\n","        c[k].extend(v)\n","    return c"],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"id":"dLNszowZkfEK","executionInfo":{"status":"ok","timestamp":1632919970872,"user_tz":-330,"elapsed":8,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}}},"source":["DEFAULT_GROUP_NAME = \"default_group\"\n","\n","\n","class SparseFeat(namedtuple('SparseFeat',\n","                            ['name', 'vocabulary_size', 'embedding_dim', 'use_hash', 'vocabulary_path', 'dtype', 'embeddings_initializer',\n","                             'embedding_name',\n","                             'group_name', 'trainable'])):\n","    __slots__ = ()\n","\n","    def __new__(cls, name, vocabulary_size, embedding_dim=4, use_hash=False, vocabulary_path=None, dtype=\"int32\", embeddings_initializer=None,\n","                embedding_name=None,\n","                group_name=DEFAULT_GROUP_NAME, trainable=True):\n","\n","        if embedding_dim == \"auto\":\n","            embedding_dim = 6 * int(pow(vocabulary_size, 0.25))\n","        if embeddings_initializer is None:\n","            embeddings_initializer = RandomNormal(mean=0.0, stddev=0.0001, seed=2020)\n","\n","        if embedding_name is None:\n","            embedding_name = name\n","\n","        return super(SparseFeat, cls).__new__(cls, name, vocabulary_size, embedding_dim, use_hash, vocabulary_path, dtype,\n","                                              embeddings_initializer,\n","                                              embedding_name, group_name, trainable)\n","\n","    def __hash__(self):\n","        return self.name.__hash__()\n","\n","\n","class VarLenSparseFeat(namedtuple('VarLenSparseFeat',\n","                                  ['sparsefeat', 'maxlen', 'combiner', 'length_name', 'weight_name', 'weight_norm'])):\n","    __slots__ = ()\n","\n","    def __new__(cls, sparsefeat, maxlen, combiner=\"mean\", length_name=None, weight_name=None, weight_norm=True):\n","        return super(VarLenSparseFeat, cls).__new__(cls, sparsefeat, maxlen, combiner, length_name, weight_name,\n","                                                    weight_norm)\n","\n","    @property\n","    def name(self):\n","        return self.sparsefeat.name\n","\n","    @property\n","    def vocabulary_size(self):\n","        return self.sparsefeat.vocabulary_size\n","\n","    @property\n","    def embedding_dim(self):\n","        return self.sparsefeat.embedding_dim\n","\n","    @property\n","    def use_hash(self):\n","        return self.sparsefeat.use_hash\n","\n","    @property\n","    def vocabulary_path(self):\n","        return self.sparsefeat.vocabulary_path\n","\n","    @property\n","    def dtype(self):\n","        return self.sparsefeat.dtype\n","\n","    @property\n","    def embeddings_initializer(self):\n","        return self.sparsefeat.embeddings_initializer\n","\n","    @property\n","    def embedding_name(self):\n","        return self.sparsefeat.embedding_name\n","\n","    @property\n","    def group_name(self):\n","        return self.sparsefeat.group_name\n","\n","    @property\n","    def trainable(self):\n","        return self.sparsefeat.trainable\n","\n","    def __hash__(self):\n","        return self.name.__hash__()\n","\n","\n","class DenseFeat(namedtuple('DenseFeat', ['name', 'dimension', 'dtype', 'transform_fn'])):\n","    \"\"\" Dense feature\n","    Args:\n","        name: feature name,\n","        dimension: dimension of the feature, default = 1.\n","        dtype: dtype of the feature, default=\"float32\".\n","        transform_fn: If not `None` , a function that can be used to transform\n","        values of the feature.  the function takes the input Tensor as its\n","        argument, and returns the output Tensor.\n","        (e.g. lambda x: (x - 3.0) / 4.2).\n","    \"\"\"\n","    __slots__ = ()\n","\n","    def __new__(cls, name, dimension=1, dtype=\"float32\", transform_fn=None):\n","        return super(DenseFeat, cls).__new__(cls, name, dimension, dtype, transform_fn)\n","\n","    def __hash__(self):\n","        return self.name.__hash__()\n","\n","    # def __eq__(self, other):\n","    #     if self.name == other.name:\n","    #         return True\n","    #     return False\n","\n","    # def __repr__(self):\n","    #     return 'DenseFeat:'+self.name\n","\n","\n","def get_feature_names(feature_columns):\n","    features = build_input_features(feature_columns)\n","    return list(features.keys())\n","\n","\n","def build_input_features(feature_columns, prefix=''):\n","    input_features = OrderedDict()\n","    for fc in feature_columns:\n","        if isinstance(fc, SparseFeat):\n","            input_features[fc.name] = Input(\n","                shape=(1,), name=prefix + fc.name, dtype=fc.dtype)\n","        elif isinstance(fc, DenseFeat):\n","            input_features[fc.name] = Input(\n","                shape=(fc.dimension,), name=prefix + fc.name, dtype=fc.dtype)\n","        elif isinstance(fc, VarLenSparseFeat):\n","            input_features[fc.name] = Input(shape=(fc.maxlen,), name=prefix + fc.name,\n","                                            dtype=fc.dtype)\n","            if fc.weight_name is not None:\n","                input_features[fc.weight_name] = Input(shape=(fc.maxlen, 1), name=prefix + fc.weight_name,\n","                                                       dtype=\"float32\")\n","            if fc.length_name is not None:\n","                input_features[fc.length_name] = Input((1,), name=prefix + fc.length_name, dtype='int32')\n","\n","        else:\n","            raise TypeError(\"Invalid feature column type,got\", type(fc))\n","\n","    return input_features\n","\n","\n","def get_linear_logit(features, feature_columns, units=1, use_bias=False, seed=1024, prefix='linear',\n","                     l2_reg=0, sparse_feat_refine_weight=None):\n","    linear_feature_columns = copy(feature_columns)\n","    for i in range(len(linear_feature_columns)):\n","        if isinstance(linear_feature_columns[i], SparseFeat):\n","            linear_feature_columns[i] = linear_feature_columns[i]._replace(embedding_dim=1,\n","                                                                           embeddings_initializer=Zeros())\n","        if isinstance(linear_feature_columns[i], VarLenSparseFeat):\n","            linear_feature_columns[i] = linear_feature_columns[i]._replace(\n","                sparsefeat=linear_feature_columns[i].sparsefeat._replace(embedding_dim=1,\n","                                                                         embeddings_initializer=Zeros()))\n","\n","    linear_emb_list = [input_from_feature_columns(features, linear_feature_columns, l2_reg, seed,\n","                                                  prefix=prefix + str(i))[0] for i in range(units)]\n","    _, dense_input_list = input_from_feature_columns(features, linear_feature_columns, l2_reg, seed, prefix=prefix)\n","\n","    linear_logit_list = []\n","    for i in range(units):\n","\n","        if len(linear_emb_list[i]) > 0 and len(dense_input_list) > 0:\n","            sparse_input = concat_func(linear_emb_list[i])\n","            dense_input = concat_func(dense_input_list)\n","            if sparse_feat_refine_weight is not None:\n","                sparse_input = Lambda(lambda x: x[0] * tf.expand_dims(x[1], axis=1))(\n","                    [sparse_input, sparse_feat_refine_weight])\n","            linear_logit = Linear(l2_reg, mode=2, use_bias=use_bias, seed=seed)([sparse_input, dense_input])\n","        elif len(linear_emb_list[i]) > 0:\n","            sparse_input = concat_func(linear_emb_list[i])\n","            if sparse_feat_refine_weight is not None:\n","                sparse_input = Lambda(lambda x: x[0] * tf.expand_dims(x[1], axis=1))(\n","                    [sparse_input, sparse_feat_refine_weight])\n","            linear_logit = Linear(l2_reg, mode=0, use_bias=use_bias, seed=seed)(sparse_input)\n","        elif len(dense_input_list) > 0:\n","            dense_input = concat_func(dense_input_list)\n","            linear_logit = Linear(l2_reg, mode=1, use_bias=use_bias, seed=seed)(dense_input)\n","        else:   #empty feature_columns\n","            return Lambda(lambda x: tf.constant([[0.0]]))(list(features.values())[0])\n","        linear_logit_list.append(linear_logit)\n","\n","    return concat_func(linear_logit_list)\n","\n","\n","def input_from_feature_columns(features, feature_columns, l2_reg, seed, prefix='', seq_mask_zero=True,\n","                               support_dense=True, support_group=False):\n","    sparse_feature_columns = list(\n","        filter(lambda x: isinstance(x, SparseFeat), feature_columns)) if feature_columns else []\n","    varlen_sparse_feature_columns = list(\n","        filter(lambda x: isinstance(x, VarLenSparseFeat), feature_columns)) if feature_columns else []\n","\n","    embedding_matrix_dict = create_embedding_matrix(feature_columns, l2_reg, seed, prefix=prefix,\n","                                                    seq_mask_zero=seq_mask_zero)\n","    group_sparse_embedding_dict = embedding_lookup(embedding_matrix_dict, features, sparse_feature_columns)\n","    dense_value_list = get_dense_input(features, feature_columns)\n","    if not support_dense and len(dense_value_list) > 0:\n","        raise ValueError(\"DenseFeat is not supported in dnn_feature_columns\")\n","\n","    sequence_embed_dict = varlen_embedding_lookup(embedding_matrix_dict, features, varlen_sparse_feature_columns)\n","    group_varlen_sparse_embedding_dict = get_varlen_pooling_list(sequence_embed_dict, features,\n","                                                                 varlen_sparse_feature_columns)\n","    group_embedding_dict = mergeDict(group_sparse_embedding_dict, group_varlen_sparse_embedding_dict)\n","    if not support_group:\n","        group_embedding_dict = list(chain.from_iterable(group_embedding_dict.values()))\n","    return group_embedding_dict, dense_value_list"],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"id":"Ophzgitoke8H","executionInfo":{"status":"ok","timestamp":1632919972705,"user_tz":-330,"elapsed":11,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}}},"source":["class PredictionLayer(Layer):\n","    \"\"\"\n","      Arguments\n","         - **task**: str, ``\"binary\"`` for  binary logloss or  ``\"regression\"`` for regression loss\n","         - **use_bias**: bool.Whether add bias term or not.\n","    \"\"\"\n","\n","    def __init__(self, task='binary', use_bias=True, **kwargs):\n","        if task not in [\"binary\", \"multiclass\", \"regression\"]:\n","            raise ValueError(\"task must be binary,multiclass or regression\")\n","        self.task = task\n","        self.use_bias = use_bias\n","        super(PredictionLayer, self).__init__(**kwargs)\n","\n","    def build(self, input_shape):\n","\n","        if self.use_bias:\n","            self.global_bias = self.add_weight(\n","                shape=(1,), initializer=Zeros(), name=\"global_bias\")\n","\n","        # Be sure to call this somewhere!\n","        super(PredictionLayer, self).build(input_shape)\n","\n","    def call(self, inputs, **kwargs):\n","        x = inputs\n","        if self.use_bias:\n","            x = tf.nn.bias_add(x, self.global_bias, data_format='NHWC')\n","        if self.task == \"binary\":\n","            x = tf.sigmoid(x)\n","\n","        output = tf.reshape(x, (-1, 1))\n","\n","        return output\n","\n","    def compute_output_shape(self, input_shape):\n","        return (None, 1)\n","\n","    def get_config(self, ):\n","        config = {'task': self.task, 'use_bias': self.use_bias}\n","        base_config = super(PredictionLayer, self).get_config()\n","        return dict(list(base_config.items()) + list(config.items()))"],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"id":"VWVj5RVCke5a","executionInfo":{"status":"ok","timestamp":1632919974254,"user_tz":-330,"elapsed":7,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}}},"source":["class FM(Layer):\n","    \"\"\"Factorization Machine models pairwise (order-2) feature interactions\n","     without linear term and bias.\n","      Input shape\n","        - 3D tensor with shape: ``(batch_size,field_size,embedding_size)``.\n","      Output shape\n","        - 2D tensor with shape: ``(batch_size, 1)``.\n","      References\n","        - [Factorization Machines](https://www.csie.ntu.edu.tw/~b97053/paper/Rendle2010FM.pdf)\n","    \"\"\"\n","\n","    def __init__(self, **kwargs):\n","\n","        super(FM, self).__init__(**kwargs)\n","\n","    def build(self, input_shape):\n","        if len(input_shape) != 3:\n","            raise ValueError(\"Unexpected inputs dimensions % d,\\\n","                             expect to be 3 dimensions\" % (len(input_shape)))\n","\n","        super(FM, self).build(input_shape)  # Be sure to call this somewhere!\n","\n","    def call(self, inputs, **kwargs):\n","\n","        if K.ndim(inputs) != 3:\n","            raise ValueError(\n","                \"Unexpected inputs dimensions %d, expect to be 3 dimensions\"\n","                % (K.ndim(inputs)))\n","\n","        concated_embeds_value = inputs\n","\n","        square_of_sum = tf.square(reduce_sum(\n","            concated_embeds_value, axis=1, keep_dims=True))\n","        sum_of_square = reduce_sum(\n","            concated_embeds_value * concated_embeds_value, axis=1, keep_dims=True)\n","        cross_term = square_of_sum - sum_of_square\n","        cross_term = 0.5 * reduce_sum(cross_term, axis=2, keep_dims=False)\n","\n","        return cross_term\n","\n","    def compute_output_shape(self, input_shape):\n","        return (None, 1)"],"execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"id":"PwdaCEnKke1c","executionInfo":{"status":"ok","timestamp":1632919976281,"user_tz":-330,"elapsed":8,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}}},"source":["class AFMLayer(Layer):\n","    \"\"\"Attentonal Factorization Machine models pairwise (order-2) feature\n","    interactions without linear term and bias.\n","      Input shape\n","        - A list of 3D tensor with shape: ``(batch_size,1,embedding_size)``.\n","      Output shape\n","        - 2D tensor with shape: ``(batch_size, 1)``.\n","      Arguments\n","        - **attention_factor** : Positive integer, dimensionality of the\n","         attention network output space.\n","        - **l2_reg_w** : float between 0 and 1. L2 regularizer strength\n","         applied to attention network.\n","        - **dropout_rate** : float between in [0,1). Fraction of the attention net output units to dropout.\n","        - **seed** : A Python integer to use as random seed.\n","      References\n","        - [Attentional Factorization Machines : Learning the Weight of Feature\n","        Interactions via Attention Networks](https://arxiv.org/pdf/1708.04617.pdf)\n","    \"\"\"\n","\n","    def __init__(self, attention_factor=4, l2_reg_w=0, dropout_rate=0, seed=1024, **kwargs):\n","        self.attention_factor = attention_factor\n","        self.l2_reg_w = l2_reg_w\n","        self.dropout_rate = dropout_rate\n","        self.seed = seed\n","        super(AFMLayer, self).__init__(**kwargs)\n","\n","    def build(self, input_shape):\n","\n","        if not isinstance(input_shape, list) or len(input_shape) < 2:\n","            # input_shape = input_shape[0]\n","            # if not isinstance(input_shape, list) or len(input_shape) < 2:\n","            raise ValueError('A `AttentionalFM` layer should be called '\n","                             'on a list of at least 2 inputs')\n","\n","        shape_set = set()\n","        reduced_input_shape = [shape.as_list() for shape in input_shape]\n","        for i in range(len(input_shape)):\n","            shape_set.add(tuple(reduced_input_shape[i]))\n","\n","        if len(shape_set) > 1:\n","            raise ValueError('A `AttentionalFM` layer requires '\n","                             'inputs with same shapes '\n","                             'Got different shapes: %s' % (shape_set))\n","\n","        if len(input_shape[0]) != 3 or input_shape[0][1] != 1:\n","            raise ValueError('A `AttentionalFM` layer requires '\n","                             'inputs of a list with same shape tensor like\\\n","                             (None, 1, embedding_size)'\n","                             'Got different shapes: %s' % (input_shape[0]))\n","\n","        embedding_size = int(input_shape[0][-1])\n","\n","        self.attention_W = self.add_weight(shape=(embedding_size,\n","                                                  self.attention_factor), initializer=glorot_normal(seed=self.seed),\n","                                           regularizer=l2(self.l2_reg_w), name=\"attention_W\")\n","        self.attention_b = self.add_weight(\n","            shape=(self.attention_factor,), initializer=Zeros(), name=\"attention_b\")\n","        self.projection_h = self.add_weight(shape=(self.attention_factor, 1),\n","                                            initializer=glorot_normal(seed=self.seed), name=\"projection_h\")\n","        self.projection_p = self.add_weight(shape=(\n","            embedding_size, 1), initializer=glorot_normal(seed=self.seed), name=\"projection_p\")\n","        self.dropout = tf.keras.layers.Dropout(\n","            self.dropout_rate, seed=self.seed)\n","\n","        self.tensordot = tf.keras.layers.Lambda(\n","            lambda x: tf.tensordot(x[0], x[1], axes=(-1, 0)))\n","\n","        # Be sure to call this somewhere!\n","        super(AFMLayer, self).build(input_shape)\n","\n","    def call(self, inputs, training=None, **kwargs):\n","\n","        if K.ndim(inputs[0]) != 3:\n","            raise ValueError(\n","                \"Unexpected inputs dimensions %d, expect to be 3 dimensions\" % (K.ndim(inputs)))\n","\n","        embeds_vec_list = inputs\n","        row = []\n","        col = []\n","\n","        for r, c in itertools.combinations(embeds_vec_list, 2):\n","            row.append(r)\n","            col.append(c)\n","\n","        p = tf.concat(row, axis=1)\n","        q = tf.concat(col, axis=1)\n","        inner_product = p * q\n","\n","        bi_interaction = inner_product\n","        attention_temp = tf.nn.relu(tf.nn.bias_add(tf.tensordot(\n","            bi_interaction, self.attention_W, axes=(-1, 0)), self.attention_b))\n","        #  Dense(self.attention_factor,'relu',kernel_regularizer=l2(self.l2_reg_w))(bi_interaction)\n","        self.normalized_att_score = softmax(tf.tensordot(\n","            attention_temp, self.projection_h, axes=(-1, 0)), dim=1)\n","        attention_output = reduce_sum(\n","            self.normalized_att_score * bi_interaction, axis=1)\n","\n","        attention_output = self.dropout(attention_output, training=training)  # training\n","\n","        afm_out = self.tensordot([attention_output, self.projection_p])\n","        return afm_out\n","\n","    def compute_output_shape(self, input_shape):\n","\n","        if not isinstance(input_shape, list):\n","            raise ValueError('A `AFMLayer` layer should be called '\n","                             'on a list of inputs.')\n","        return (None, 1)\n","\n","    def get_config(self, ):\n","        config = {'attention_factor': self.attention_factor,\n","                  'l2_reg_w': self.l2_reg_w, 'dropout_rate': self.dropout_rate, 'seed': self.seed}\n","        base_config = super(AFMLayer, self).get_config()\n","        base_config.update(config)\n","        return base_config"],"execution_count":8,"outputs":[]},{"cell_type":"code","metadata":{"id":"g075nfXAmn73","executionInfo":{"status":"ok","timestamp":1632919978572,"user_tz":-330,"elapsed":738,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}}},"source":["def AFM(linear_feature_columns, dnn_feature_columns, fm_group=DEFAULT_GROUP_NAME, use_attention=True,\n","        attention_factor=8,\n","        l2_reg_linear=1e-5, l2_reg_embedding=1e-5, l2_reg_att=1e-5, afm_dropout=0, seed=1024,\n","        task='binary'):\n","    \"\"\"Instantiates the Attentional Factorization Machine architecture.\n","\n","    :param linear_feature_columns: An iterable containing all the features used by linear part of the model.\n","    :param dnn_feature_columns: An iterable containing all the features used by deep part of the model.\n","    :param fm_group: list, group_name of features that will be used to do feature interactions.\n","    :param use_attention: bool,whether use attention or not,if set to ``False``.it is the same as **standard Factorization Machine**\n","    :param attention_factor: positive integer,units in attention net\n","    :param l2_reg_linear: float. L2 regularizer strength applied to linear part\n","    :param l2_reg_embedding: float. L2 regularizer strength applied to embedding vector\n","    :param l2_reg_att: float. L2 regularizer strength applied to attention net\n","    :param afm_dropout: float in [0,1), Fraction of the attention net output units to dropout.\n","    :param seed: integer ,to use as random seed.\n","    :param task: str, ``\"binary\"`` for  binary logloss or  ``\"regression\"`` for regression loss\n","    :return: A Keras model instance.\n","    \"\"\"\n","\n","    features = build_input_features(\n","        linear_feature_columns + dnn_feature_columns)\n","\n","    inputs_list = list(features.values())\n","\n","    group_embedding_dict, _ = input_from_feature_columns(features, dnn_feature_columns, l2_reg_embedding,\n","                                                         seed, support_dense=False, support_group=True)\n","\n","    linear_logit = get_linear_logit(features, linear_feature_columns, seed=seed, prefix='linear',\n","                                    l2_reg=l2_reg_linear)\n","\n","    if use_attention:\n","        fm_logit = add_func([AFMLayer(attention_factor, l2_reg_att, afm_dropout,\n","                                      seed)(list(v)) for k, v in group_embedding_dict.items() if k in fm_group])\n","    else:\n","        fm_logit = add_func([FM()(concat_func(v, axis=1))\n","                             for k, v in group_embedding_dict.items() if k in fm_group])\n","\n","    final_logit = add_func([linear_logit, fm_logit])\n","    output = PredictionLayer(task)(final_logit)\n","\n","    model = tf.keras.models.Model(inputs=inputs_list, outputs=output)\n","    return model"],"execution_count":9,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lFGOAHjgnOBL"},"source":["## Official Tensorflow 1.x Implementation on MovieLens and Frappe dataset"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8ltrldXnnRFN","executionInfo":{"status":"ok","timestamp":1632920855335,"user_tz":-330,"elapsed":1849,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}},"outputId":"e373b32f-1e4b-4209-8fd4-b3825e9bfc02"},"source":["%tensorflow_version 1.x"],"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["TensorFlow 1.x selected.\n"]}]},{"cell_type":"code","metadata":{"id":"uEi3zBv7qtcp","executionInfo":{"status":"ok","timestamp":1632921849412,"user_tz":-330,"elapsed":1391,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}}},"source":["'''\n","Data pre process for AFM and FM\n","@author: \n","Lizi Liao (liaolizi.llz@gmail.com)\n","Xiangnan He (xiangnanhe@gmail.com)\n","'''\n","import numpy as np\n","import os\n","\n","class LoadData(object):\n","    '''given the path of data, return the data format for AFM and FM\n","    :param path\n","    return:\n","    Train_data: a dictionary, 'Y' refers to a list of y values; 'X' refers to a list of features_M dimension vectors with 0 or 1 entries\n","    Test_data: same as Train_data\n","    Validation_data: same as Train_data\n","    '''\n","\n","    # Three files are needed in the path\n","    def __init__(self, path, dataset, loss_type=\"square_loss\"):\n","        self.path = path + \"/\"\n","        self.trainfile = self.path + dataset +\".train.libfm\"\n","        self.testfile = self.path + dataset + \".test.libfm\"\n","        self.validationfile = self.path + dataset + \".validation.libfm\"\n","        self.features_M = self.map_features( )\n","        self.Train_data, self.Validation_data, self.Test_data = self.construct_data( loss_type )\n","\n","    def map_features(self): # map the feature entries in all files, kept in self.features dictionary\n","        self.features = {}\n","        self.read_features(self.trainfile)\n","        self.read_features(self.testfile)\n","        self.read_features(self.validationfile)\n","        # print(\"features_M:\", len(self.features))\n","        return  len(self.features)\n","\n","    def read_features(self, file): # read a feature file\n","        f = open( file )\n","        line = f.readline()\n","        i = len(self.features)\n","        while line:\n","            items = line.strip().split(' ')\n","            for item in items[1:]:\n","                if item not in self.features:\n","                    self.features[ item ] = i\n","                    i = i + 1\n","            line = f.readline()\n","        f.close()\n","\n","    def construct_data(self, loss_type):\n","        X_, Y_ , Y_for_logloss= self.read_data(self.trainfile)\n","        if loss_type == 'log_loss':\n","            Train_data = self.construct_dataset(X_, Y_for_logloss)\n","        else:\n","            Train_data = self.construct_dataset(X_, Y_)\n","        #print(\"Number of samples in Train:\" , len(Y_))\n","\n","        X_, Y_ , Y_for_logloss= self.read_data(self.validationfile)\n","        if loss_type == 'log_loss':\n","            Validation_data = self.construct_dataset(X_, Y_for_logloss)\n","        else:\n","            Validation_data = self.construct_dataset(X_, Y_)\n","        #print(\"Number of samples in Validation:\", len(Y_))\n","\n","        X_, Y_ , Y_for_logloss = self.read_data(self.testfile)\n","        if loss_type == 'log_loss':\n","            Test_data = self.construct_dataset(X_, Y_for_logloss)\n","        else:\n","            Test_data = self.construct_dataset(X_, Y_)\n","        #print(\"Number of samples in Test:\", len(Y_))\n","\n","        return Train_data,  Validation_data,  Test_data\n","\n","    def read_data(self, file):\n","        # read a data file. For a row, the first column goes into Y_;\n","        # the other columns become a row in X_ and entries are maped to indexs in self.features\n","        f = open( file )\n","        X_ = []\n","        Y_ = []\n","        Y_for_logloss = []\n","        line = f.readline()\n","        while line:\n","            items = line.strip().split(' ')\n","            Y_.append( 1.0*float(items[0]) )\n","\n","            if float(items[0]) > 0:# > 0 as 1; others as 0\n","                v = 1.0\n","            else:\n","                v = 0.0\n","            Y_for_logloss.append( v )\n","\n","            X_.append( [ self.features[item] for item in items[1:]] )\n","            line = f.readline()\n","        f.close()\n","        return X_, Y_, Y_for_logloss\n","\n","    def construct_dataset(self, X_, Y_):\n","        Data_Dic = {}\n","        X_lens = [ len(line) for line in X_]\n","        indexs = np.argsort(X_lens)\n","        Data_Dic['Y'] = [ Y_[i] for i in indexs]\n","        Data_Dic['X'] = [ X_[i] for i in indexs]\n","        return Data_Dic\n","    \n","    def truncate_features(self):\n","        \"\"\"\n","        Make sure each feature vector is of the same length\n","        \"\"\"\n","        num_variable = len(self.Train_data['X'][0])\n","        for i in range(len(self.Train_data['X'])):\n","            num_variable = min([num_variable, len(self.Train_data['X'][i])])\n","        # truncate train, validation and test\n","        for i in range(len(self.Train_data['X'])):\n","            self.Train_data['X'][i] = self.Train_data['X'][i][0:num_variable]\n","        for i in range(len(self.Validation_data['X'])):\n","            self.Validation_data['X'][i] = self.Validation_data['X'][i][0:num_variable]\n","        for i in range(len(self.Test_data['X'])):\n","            self.Test_data['X'][i] = self.Test_data['X'][i][0:num_variable]\n","        return num_variable"],"execution_count":17,"outputs":[]},{"cell_type":"code","metadata":{"id":"RwVv9iqHtWLe","executionInfo":{"status":"ok","timestamp":1632921851829,"user_tz":-330,"elapsed":8,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}}},"source":["# !wget -q --show-progress https://github.com/hexiangnan/attentional_factorization_machine/raw/master/data/ml-tag/ml-tag.test.libfm\n","# !wget -q --show-progress https://github.com/hexiangnan/attentional_factorization_machine/raw/master/data/ml-tag/ml-tag.validation.libfm\n","# !wget -q --show-progress https://github.com/hexiangnan/attentional_factorization_machine/raw/master/data/ml-tag/ml-tag.train.libfm"],"execution_count":18,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oazlrOdDscw1","executionInfo":{"status":"ok","timestamp":1632923075518,"user_tz":-330,"elapsed":1223695,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}},"outputId":"0c8992f2-1758-4f8b-9991-a8c29df00f72"},"source":["'''\n","Tensorflow implementation of Factorization Machines (FM)\n","@author: \n","Xiangnan He (xiangnanhe@gmail.com)\n","Hao Ye (tonyfd26@gmail.com)\n","@references:\n","'''\n","import math\n","import os\n","import numpy as np\n","import tensorflow as tf\n","from sklearn.base import BaseEstimator, TransformerMixin\n","from sklearn.metrics import mean_squared_error\n","from sklearn.metrics import accuracy_score\n","from time import time\n","import argparse\n","from tensorflow.contrib.layers.python.layers import batch_norm as batch_norm\n","\n","#################### Arguments ####################\n","def parse_args():\n","    parser = argparse.ArgumentParser(description=\"Run DeepFM.\")\n","    parser.add_argument('--process', nargs='?', default='train',\n","                        help='Process type: train, evaluate.')\n","    parser.add_argument('--mla', type=int, default=0,\n","                        help='Set the experiment mode to be Micro Level Analysis or not: 0-disable, 1-enable.')\n","    parser.add_argument('--path', nargs='?', default='/content',\n","                        help='Input data path.')\n","    parser.add_argument('--dataset', nargs='?', default='ml-tag',\n","                        help='Choose a dataset.')\n","    parser.add_argument('--epoch', type=int, default=20,\n","                        help='Number of epochs.')\n","    parser.add_argument('--pretrain', type=int, default=-1,\n","                        help='flag for pretrain. 1: initialize from pretrain; 0: randomly initialize; -1: save to pretrain file')\n","    parser.add_argument('--batch_size', type=int, default=4096,\n","                        help='Batch size.')\n","    parser.add_argument('--hidden_factor', type=int, default=256,\n","                        help='Number of hidden factors.')\n","    parser.add_argument('--lamda', type=float, default=0,\n","                        help='Regularizer for bilinear part.')\n","    parser.add_argument('--keep', type=float, default=0.7, \n","                    help='Keep probility (1-dropout) for the bilinear interaction layer. 1: no dropout')\n","    parser.add_argument('--lr', type=float, default=0.01,\n","                        help='Learning rate.')\n","    parser.add_argument('--optimizer', nargs='?', default='AdagradOptimizer',\n","                        help='Specify an optimizer type (AdamOptimizer, AdagradOptimizer, GradientDescentOptimizer, MomentumOptimizer).')\n","    parser.add_argument('--verbose', type=int, default=1,\n","                        help='Whether to show the performance of each epoch (0 or 1)')\n","    parser.add_argument('--batch_norm', type=int, default=1,\n","                    help='Whether to perform batch normaization (0 or 1)')\n","\n","    return parser.parse_args(args={})\n","\n","class FM(BaseEstimator, TransformerMixin):\n","    def __init__(self, features_M, pretrain_flag, save_file, hidden_factor, epoch, batch_size, learning_rate, lamda_bilinear, keep,\n","                 optimizer_type, batch_norm, verbose, micro_level_analysis, random_seed=2016):\n","        # bind params to class\n","        self.batch_size = batch_size\n","        self.learning_rate = learning_rate\n","        self.hidden_factor = hidden_factor\n","        self.save_file = save_file\n","        self.pretrain_flag = pretrain_flag\n","        self.features_M = features_M\n","        self.lamda_bilinear = lamda_bilinear\n","        self.keep = keep\n","        self.epoch = epoch\n","        self.random_seed = random_seed\n","        self.optimizer_type = optimizer_type\n","        self.batch_norm = batch_norm\n","        self.verbose = verbose\n","        self.micro_level_analysis = micro_level_analysis\n","        # performance of each epoch\n","        self.train_rmse, self.valid_rmse, self.test_rmse = [], [], []\n","\n","        # init all variables in a tensorflow graph\n","        self._init_graph()\n","\n","    def _init_graph(self):\n","        '''\n","        Init a tensorflow Graph containing: input data, variables, model, loss, optimizer\n","        '''\n","        self.graph = tf.Graph()\n","        with self.graph.as_default():  # , tf.device('/cpu:0'):\n","            # Set graph level random seed\n","            tf.set_random_seed(self.random_seed)\n","            # Input data.\n","            self.train_features = tf.placeholder(tf.int32, shape=[None, None], name=\"train_features_fm\")  # None * features_M\n","            self.train_labels = tf.placeholder(tf.float32, shape=[None, 1], name=\"train_labels_fm\")  # None * 1\n","            self.dropout_keep = tf.placeholder(tf.float32, name=\"dropout_keep_fm\")\n","            self.train_phase = tf.placeholder(tf.bool, name=\"train_phase_fm\")\n","\n","            # Variables.\n","            self.weights = self._initialize_weights()\n","\n","            # Model.\n","            # get the summed up embeddings of features.\n","            self.nonzero_embeddings = tf.nn.embedding_lookup(self.weights['feature_embeddings'], self.train_features, name='nonzero_embeddings')\n","            self.summed_features_emb = tf.reduce_sum(self.nonzero_embeddings, 1, keep_dims=True) # None * 1 * K\n","            # get the element-multiplication\n","            self.summed_features_emb_square = tf.square(self.summed_features_emb)  # None * 1 * K\n","\n","            # _________ square_sum part _____________\n","            self.squared_features_emb = tf.square(self.nonzero_embeddings)\n","            self.squared_sum_features_emb = tf.reduce_sum(self.squared_features_emb, 1, keep_dims=True)  # None * 1 * K\n","\n","            # ________ FM __________\n","            self.FM = 0.5 * tf.subtract(self.summed_features_emb_square, self.squared_sum_features_emb, name=\"fm\")  # None * 1 * K\n","            # ml-tag has 3 interactions. divided by 3 to make sure that the sum of the weights is 1\n","            if self.micro_level_analysis:\n","                self.FM = self.FM / 3.0\n","            if self.batch_norm and not self.micro_level_analysis:\n","                self.FM = self.batch_norm_layer(self.FM, train_phase=self.train_phase, scope_bn='bn_fm')\n","            self.FM_OUT = tf.reduce_sum(self.FM, 1, name=\"fm_out\") # None * K\n","            self.FM_OUT = tf.nn.dropout(self.FM_OUT, self.dropout_keep) # dropout at the FM layer\n","\n","            # _________out _________\n","            if self.micro_level_analysis:\n","                # ml-tag has 3 interactions. divided by 3 to make sure that the total weight of the sum is 1\n","                self.out = tf.reduce_sum(self.FM_OUT, 1, keep_dims=True, name=\"out\")  # None * 1\n","            else:\n","                Bilinear = tf.reduce_sum(self.FM_OUT, 1, keep_dims=True)  # None * 1\n","                self.Feature_bias = tf.reduce_sum(tf.nn.embedding_lookup(self.weights['feature_bias'], self.train_features) , 1)  # None * 1\n","                Bias = self.weights['bias'] * tf.ones_like(self.train_labels)  # None * 1\n","                self.out = tf.add_n([Bilinear, self.Feature_bias, Bias], name=\"out\")  # None * 1\n","\n","            # Compute the square loss.\n","            if self.lamda_bilinear > 0:\n","                self.loss = tf.nn.l2_loss(tf.subtract(self.train_labels, self.out)) + tf.contrib.layers.l2_regularizer(self.lamda_bilinear)(self.weights['feature_embeddings'])  # regulizer\n","            else:\n","                self.loss = tf.nn.l2_loss(tf.subtract(self.train_labels, self.out))\n","\n","            # Optimizer.\n","            if self.optimizer_type == 'AdamOptimizer':\n","                self.optimizer = tf.train.AdamOptimizer(learning_rate=self.learning_rate, beta1=0.9, beta2=0.999, epsilon=1e-8).minimize(self.loss)\n","            elif self.optimizer_type == 'AdagradOptimizer':\n","                self.optimizer = tf.train.AdagradOptimizer(learning_rate=self.learning_rate, initial_accumulator_value=1e-8).minimize(self.loss)\n","            elif self.optimizer_type == 'GradientDescentOptimizer':\n","                self.optimizer = tf.train.GradientDescentOptimizer(learning_rate=self.learning_rate).minimize(self.loss)\n","            elif self.optimizer_type == 'MomentumOptimizer':\n","                self.optimizer = tf.train.MomentumOptimizer(learning_rate=self.learning_rate, momentum=0.95).minimize(self.loss)\n","\n","            # init\n","            self.sess = self._init_session()\n","            self.saver = tf.train.Saver()\n","            init = tf.global_variables_initializer()\n","            self.sess.run(init)\n","\n","            # number of params\n","            total_parameters = 0\n","            for variable in self.weights.values():\n","                shape = variable.get_shape() # shape is an array of tf.Dimension\n","                variable_parameters = 1\n","                for dim in shape:\n","                    variable_parameters *= dim.value\n","                total_parameters += variable_parameters\n","            if self.verbose > 0:\n","                print(\"#params: %d\" %total_parameters)\n","    \n","    def _init_session(self):\n","        # adaptively growing video memory\n","        config = tf.ConfigProto()\n","        config.gpu_options.allow_growth = True\n","        return tf.Session(config=config)\n","\n","    def _initialize_weights(self):\n","        all_weights = dict()\n","        if self.pretrain_flag > 0:\n","            weight_saver = tf.train.import_meta_graph(self.save_file + '.meta')\n","            pretrain_graph = tf.get_default_graph()\n","            feature_embeddings = pretrain_graph.get_tensor_by_name('feature_embeddings:0')\n","            feature_bias = pretrain_graph.get_tensor_by_name('feature_bias:0')\n","            bias = pretrain_graph.get_tensor_by_name('bias:0')\n","\n","            with self._init_session() as sess:\n","                weight_saver.restore(sess, self.save_file)\n","                fe, fb, b = sess.run([feature_embeddings, feature_bias, bias])\n","\n","            all_weights['feature_embeddings'] = tf.Variable(fe, dtype=tf.float32, name='feature_embeddings')\n","            all_weights['feature_bias'] = tf.Variable(fb, dtype=tf.float32, name='feature_bias')\n","            all_weights['bias'] = tf.Variable(b, dtype=tf.float32, name='bias')\n","        else:\n","            all_weights['feature_embeddings'] = tf.Variable(\n","                tf.random_normal([self.features_M, self.hidden_factor], 0.0, 0.01),\n","                name='feature_embeddings')  # features_M * K\n","            all_weights['feature_bias'] = tf.Variable(\n","                tf.random_uniform([self.features_M, 1], 0.0, 0.0), name='feature_bias')  # features_M * 1\n","            all_weights['bias'] = tf.Variable(tf.constant(0.0), name='bias')  # 1 * 1\n","        return all_weights\n","\n","    def batch_norm_layer(self, x, train_phase, scope_bn):\n","        bn_train = batch_norm(x, decay=0.9, center=True, scale=True, updates_collections=None,\n","            is_training=True, reuse=None, trainable=True, scope=scope_bn)\n","        bn_inference = batch_norm(x, decay=0.9, center=True, scale=True, updates_collections=None,\n","            is_training=False, reuse=True, trainable=True, scope=scope_bn)\n","        z = tf.cond(train_phase, lambda: bn_train, lambda: bn_inference)\n","        return z\n","\n","    def partial_fit(self, data):  # fit a batch\n","        feed_dict = {self.train_features: data['X'], self.train_labels: data['Y'], self.dropout_keep: self.keep, self.train_phase: True}\n","        loss, opt = self.sess.run((self.loss, self.optimizer), feed_dict=feed_dict)\n","        return loss\n","\n","    def get_random_block_from_data(self, data, batch_size):  # generate a random block of training data\n","        start_index = np.random.randint(0, len(data['Y']) - batch_size)\n","        X , Y = [], []\n","        # forward get sample\n","        i = start_index\n","        while len(X) < batch_size and i < len(data['X']):\n","            if len(data['X'][i]) == len(data['X'][start_index]):\n","                Y.append([data['Y'][i]])\n","                X.append(data['X'][i])\n","                i = i + 1\n","            else:\n","                break\n","        # backward get sample\n","        i = start_index\n","        while len(X) < batch_size and i >= 0:\n","            if len(data['X'][i]) == len(data['X'][start_index]):\n","                Y.append([data['Y'][i]])\n","                X.append(data['X'][i])\n","                i = i - 1\n","            else:\n","                break\n","        return {'X': X, 'Y': Y}\n","\n","    def shuffle_in_unison_scary(self, a, b): # shuffle two lists simutaneously\n","        rng_state = np.random.get_state()\n","        np.random.shuffle(a)\n","        np.random.set_state(rng_state)\n","        np.random.shuffle(b)\n","\n","    def train(self, Train_data, Validation_data, Test_data):  # fit a dataset\n","        # Check Init performance\n","        if self.verbose > 0:\n","            t2 = time()\n","            init_train = self.evaluate(Train_data)\n","            init_valid = self.evaluate(Validation_data)\n","            print(\"Init: \\t train=%.4f, validation=%.4f [%.1f s]\" %(init_train, init_valid, time()-t2))\n","\n","        for epoch in range(self.epoch):\n","            t1 = time()\n","            self.shuffle_in_unison_scary(Train_data['X'], Train_data['Y'])\n","            total_batch = int(len(Train_data['Y']) / self.batch_size)\n","            for i in range(total_batch):\n","                # generate a batch\n","                batch_xs = self.get_random_block_from_data(Train_data, self.batch_size)\n","                # Fit training\n","                self.partial_fit(batch_xs)\n","            t2 = time()\n","            \n","            # output validation\n","            train_result = self.evaluate(Train_data)\n","            valid_result = self.evaluate(Validation_data)\n","\n","            self.train_rmse.append(train_result)\n","            self.valid_rmse.append(valid_result)\n","\n","            if self.verbose > 0 and epoch%self.verbose == 0:\n","                print(\"Epoch %d [%.1f s]\\ttrain=%.4f, validation=%.4f [%.1f s]\"\n","                      %(epoch+1, t2-t1, train_result, valid_result, time()-t2))\n","            if self.eva_termination(self.valid_rmse):\n","                break\n","\n","        if self.pretrain_flag < 0:\n","            print(\"Save model to file as pretrain.\")\n","            self.saver.save(self.sess, self.save_file)\n","\n","    def eva_termination(self, valid):\n","        if len(valid) > 5:\n","            if valid[-1] > valid[-2] and valid[-2] > valid[-3] and valid[-3] > valid[-4] and valid[-4] > valid[-5]:\n","                return True\n","        return False\n","\n","    def evaluate(self, data):  # evaluate the results for an input set\n","        num_example = len(data['Y'])\n","        feed_dict = {self.train_features: data['X'], self.train_labels: [[y] for y in data['Y']], self.dropout_keep: 1.0, self.train_phase: False}\n","        predictions = self.sess.run((self.out), feed_dict=feed_dict)\n","        y_pred = np.reshape(predictions, (num_example,))\n","        y_true = np.reshape(data['Y'], (num_example,))\n","        \n","        predictions_bounded = np.maximum(y_pred, np.ones(num_example) * min(y_true))  # bound the lower values\n","        predictions_bounded = np.minimum(predictions_bounded, np.ones(num_example) * max(y_true))  # bound the higher values\n","        RMSE = math.sqrt(mean_squared_error(y_true, predictions_bounded))\n","        return RMSE\n","\n","\n","def make_save_file(args):\n","    pretrain_path = '/content/fm_%s_%d' %(args.dataset, args.hidden_factor)\n","    if args.mla:\n","        pretrain_path += '_mla'\n","    if not os.path.exists(pretrain_path):\n","        os.makedirs(pretrain_path)\n","    save_file = pretrain_path+'/%s_%d' %(args.dataset, args.hidden_factor)\n","    return save_file\n","\n","def train(args):\n","    # Data loading\n","    data = LoadData(args.path, args.dataset)\n","    if args.verbose > 0:\n","        print(\"FM: dataset=%s, factors=%d, #epoch=%d, batch=%d, lr=%.4f, lambda=%.1e, keep=%.2f, optimizer=%s, batch_norm=%d\"\n","              %(args.dataset, args.hidden_factor, args.epoch, args.batch_size, args.lr, args.lamda, args.keep, args.optimizer, args.batch_norm))\n","\n","    # Training\n","    t1 = time()\n","    model = FM(data.features_M, args.pretrain, make_save_file(args), args.hidden_factor, args.epoch, args.batch_size, args.lr, args.lamda, args.keep, args.optimizer, args.batch_norm, args.verbose, args.mla)\n","    model.train(data.Train_data, data.Validation_data, data.Test_data)\n","    \n","    # Find the best validation result across iterations\n","    best_valid_score = 0\n","    best_valid_score = min(model.valid_rmse)\n","    best_epoch = model.valid_rmse.index(best_valid_score)\n","    print(\"Best Iter(validation)= %d\\t train = %.4f, valid = %.4f [%.1f s]\" \n","           %(best_epoch+1, model.train_rmse[best_epoch], model.valid_rmse[best_epoch], time()-t1))\n","\n","def evaluate(args):\n","    # load test data\n","    data = DATA.LoadData(args.path, args.dataset).Test_data\n","    save_file = make_save_file(args)\n","    \n","    # load the graph\n","    weight_saver = tf.train.import_meta_graph(save_file + '.meta')\n","    pretrain_graph = tf.get_default_graph()\n","\n","    # load tensors \n","    feature_embeddings = pretrain_graph.get_tensor_by_name('feature_embeddings:0')\n","    nonzero_embeddings = pretrain_graph.get_tensor_by_name('nonzero_embeddings:0')\n","    feature_bias = pretrain_graph.get_tensor_by_name('feature_bias:0')\n","    bias = pretrain_graph.get_tensor_by_name('bias:0')\n","    fm = pretrain_graph.get_tensor_by_name('fm:0')\n","    fm_out = pretrain_graph.get_tensor_by_name('fm_out:0')\n","    out = pretrain_graph.get_tensor_by_name('out:0')\n","    train_features = pretrain_graph.get_tensor_by_name('train_features_fm:0')\n","    train_labels = pretrain_graph.get_tensor_by_name('train_labels_fm:0')\n","    dropout_keep = pretrain_graph.get_tensor_by_name('dropout_keep_fm:0')\n","    train_phase = pretrain_graph.get_tensor_by_name('train_phase_fm:0')\n","\n","\n","    # restore session\n","    config = tf.ConfigProto()\n","    config.gpu_options.allow_growth = True\n","    sess = tf.Session(config=config)\n","    weight_saver.restore(sess, save_file)\n","\n","    # start evaluation\n","    num_example = len(data['Y'])\n","    feed_dict = {train_features: data['X'], train_labels: [[y] for y in data['Y']], dropout_keep: 1.0, train_phase: False}\n","    ne, fe = sess.run((nonzero_embeddings, feature_embeddings), feed_dict=feed_dict)\n","    _fm, _fm_out, predictions = sess.run((fm, fm_out, out), feed_dict=feed_dict)\n","\n","    # calculate rmse\n","    y_pred = np.reshape(predictions, (num_example,))\n","    y_true = np.reshape(data['Y'], (num_example,))\n","    \n","    predictions_bounded = np.maximum(y_pred, np.ones(num_example) * min(y_true))  # bound the lower values\n","    predictions_bounded = np.minimum(predictions_bounded, np.ones(num_example) * max(y_true))  # bound the higher values\n","    RMSE = math.sqrt(mean_squared_error(y_true, predictions_bounded))\n","\n","    print(\"Test RMSE: %.4f\"%(RMSE))\n","\n","\n","if __name__ == '__main__':\n","    args = parse_args()\n","\n","    # initialize the optimal parameters\n","    # if args.mla:\n","    #     args.lr = 0.05\n","    #     args.keep = 0.7\n","    #     args.batch_norm = 0\n","    # else:\n","    #     args.lr = 0.01\n","    #     args.keep = 0.7\n","    #     args.batch_norm = 1\n","\n","    if args.process == 'train':\n","        train(args)\n","    elif args.process == 'evaluate':\n","        evaluate(args)"],"execution_count":19,"outputs":[{"output_type":"stream","name":"stdout","text":["FM: dataset=ml-tag, factors=256, #epoch=20, batch=4096, lr=0.0100, lambda=0.0e+00, keep=0.70, optimizer=AdagradOptimizer, batch_norm=1\n","#params: 23244366\n","Init: \t train=1.0000, validation=1.0000 [25.6 s]\n","Epoch 1 [28.9 s]\ttrain=0.3983, validation=0.5695 [28.4 s]\n","Epoch 2 [32.8 s]\ttrain=0.2864, validation=0.5344 [27.8 s]\n","Epoch 3 [31.6 s]\ttrain=0.2248, validation=0.5205 [25.5 s]\n","Epoch 4 [30.3 s]\ttrain=0.1845, validation=0.5109 [27.3 s]\n","Epoch 5 [29.1 s]\ttrain=0.1571, validation=0.5030 [26.2 s]\n","Epoch 6 [29.6 s]\ttrain=0.1406, validation=0.5020 [25.2 s]\n","Epoch 7 [29.4 s]\ttrain=0.1273, validation=0.4993 [24.9 s]\n","Epoch 8 [29.4 s]\ttrain=0.1183, validation=0.4973 [27.0 s]\n","Epoch 9 [31.0 s]\ttrain=0.1109, validation=0.4936 [29.0 s]\n","Epoch 10 [29.9 s]\ttrain=0.1053, validation=0.4930 [25.4 s]\n","Epoch 11 [30.0 s]\ttrain=0.1029, validation=0.4939 [25.8 s]\n","Epoch 12 [32.5 s]\ttrain=0.0990, validation=0.4931 [28.3 s]\n","Epoch 13 [33.0 s]\ttrain=0.0940, validation=0.4895 [28.1 s]\n","Epoch 14 [33.3 s]\ttrain=0.0916, validation=0.4899 [27.4 s]\n","Epoch 15 [33.0 s]\ttrain=0.0886, validation=0.4873 [28.1 s]\n","Epoch 16 [30.9 s]\ttrain=0.0866, validation=0.4853 [28.6 s]\n","Epoch 17 [30.9 s]\ttrain=0.0841, validation=0.4871 [26.1 s]\n","Epoch 18 [32.5 s]\ttrain=0.0825, validation=0.4866 [28.0 s]\n","Epoch 19 [34.8 s]\ttrain=0.0813, validation=0.4836 [36.6 s]\n","Epoch 20 [34.7 s]\ttrain=0.0819, validation=0.4876 [27.8 s]\n","Save model to file as pretrain.\n","Best Iter(validation)= 19\t train = 0.0813, valid = 0.4836 [1207.2 s]\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GGniWZBLvZll","executionInfo":{"status":"ok","timestamp":1632924310557,"user_tz":-330,"elapsed":729578,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}},"outputId":"b65ee243-6758-444f-beda-0a905ab2b445"},"source":["'''\n","Tensorflow implementation of Attentional Factorization Machines (AFM)\n","@author: \n","Xiangnan He (xiangnanhe@gmail.com)\n","Hao Ye (tonyfd26@gmail.com)\n","@references:\n","'''\n","import math\n","import os, sys\n","import numpy as np\n","import tensorflow as tf\n","from sklearn.base import BaseEstimator, TransformerMixin\n","from sklearn.metrics import mean_squared_error\n","from sklearn.metrics import accuracy_score\n","from time import time\n","import argparse\n","from tensorflow.contrib.layers.python.layers import batch_norm as batch_norm\n","\n","#################### Arguments ####################\n","def parse_args():\n","    parser = argparse.ArgumentParser(description=\"Run DeepFM.\")\n","    parser.add_argument('--process', nargs='?', default='train',\n","                        help='Process type: train, evaluate.')\n","    parser.add_argument('--mla', type=int, default=0,\n","                        help='Set the experiment mode to be Micro Level Analysis or not: 0-disable, 1-enable.')\n","    parser.add_argument('--path', nargs='?', default='/content',\n","                        help='Input data path.')\n","    parser.add_argument('--dataset', nargs='?', default='ml-tag',\n","                        help='Choose a dataset.')\n","    parser.add_argument('--valid_dimen', type=int, default=3,\n","                        help='Valid dimension of the dataset. (e.g. frappe=10, ml-tag=3)')\n","    parser.add_argument('--epoch', type=int, default=20,\n","                        help='Number of epochs.')\n","    parser.add_argument('--pretrain', type=int, default=-1,\n","                        help='flag for pretrain. 1: initialize from pretrain; 0: randomly initialize; -1: save to pretrain file; 2: initialize from pretrain and save to pretrain file')\n","    parser.add_argument('--batch_size', type=int, default=4096,\n","                        help='Batch size.')\n","    parser.add_argument('--attention', type=int, default=1,\n","                        help='flag for attention. 1: use attention; 0: no attention')\n","    parser.add_argument('--hidden_factor', nargs='?', default='[16,16]',\n","                        help='Number of hidden factors.')\n","    parser.add_argument('--lamda_attention', type=float, default=1e+2,\n","                        help='Regularizer for attention part.')\n","    parser.add_argument('--keep', nargs='?', default='[1.0,0.5]',\n","                        help='Keep probility (1-dropout) of each layer. 1: no dropout. The first index is for the attention-aware pairwise interaction layer.')\n","    parser.add_argument('--lr', type=float, default=0.1,\n","                        help='Learning rate.')\n","    parser.add_argument('--freeze_fm', type=int, default=0,\n","                        help='Freese all params of fm and learn attention params only.')\n","    parser.add_argument('--optimizer', nargs='?', default='AdagradOptimizer',\n","                        help='Specify an optimizer type (AdamOptimizer, AdagradOptimizer, GradientDescentOptimizer, MomentumOptimizer).')\n","    parser.add_argument('--verbose', type=int, default=1,\n","                        help='Whether to show the performance of each epoch (0 or 1)')\n","    parser.add_argument('--batch_norm', type=int, default=0,\n","                    help='Whether to perform batch normaization (0 or 1)')\n","    parser.add_argument('--decay', type=float, default=0.999,\n","                    help='Decay value for batch norm')\n","    parser.add_argument('--activation', nargs='?', default='relu',\n","                    help='Which activation function to use for deep layers: relu, sigmoid, tanh, identity')\n","\n","    return parser.parse_args(args={})\n","\n","class AFM(BaseEstimator, TransformerMixin):\n","    def __init__(self, features_M, pretrain_flag, save_file, attention, hidden_factor, valid_dimension, activation_function, num_variable, \n","                 freeze_fm, epoch, batch_size, learning_rate, lamda_attention, keep, optimizer_type, batch_norm, decay, verbose, micro_level_analysis, random_seed=2016):\n","        # bind params to class\n","        self.batch_size = batch_size\n","        self.learning_rate = learning_rate\n","        self.attention = attention\n","        self.hidden_factor = hidden_factor\n","        self.valid_dimension = valid_dimension\n","        self.activation_function = activation_function\n","        self.num_variable = num_variable\n","        self.save_file = save_file\n","        self.pretrain_flag = pretrain_flag\n","        self.features_M = features_M\n","        self.lamda_attention = lamda_attention\n","        self.keep = keep\n","        self.freeze_fm = freeze_fm\n","        self.epoch = epoch\n","        self.random_seed = random_seed\n","        self.optimizer_type = optimizer_type\n","        self.batch_norm = batch_norm\n","        self.decay = decay\n","        self.verbose = verbose\n","        self.micro_level_analysis = micro_level_analysis\n","        # performance of each epoch\n","        self.train_rmse, self.valid_rmse, self.test_rmse = [], [], []\n","\n","        # init all variables in a tensorflow graph\n","        self._init_graph()\n","\n","    def _init_graph(self):\n","        '''\n","        Init a tensorflow Graph containing: input data, variables, model, loss, optimizer\n","        '''\n","        self.graph = tf.Graph()\n","        with self.graph.as_default():  # , tf.device('/cpu:0'):\n","            # Set graph level random seed\n","            tf.set_random_seed(self.random_seed)\n","            # Input data.\n","            self.train_features = tf.placeholder(tf.int32, shape=[None, None], name=\"train_features_afm\")  # None * features_M\n","            self.train_labels = tf.placeholder(tf.float32, shape=[None, 1], name=\"train_labels_afm\")  # None * 1\n","            self.dropout_keep = tf.placeholder(tf.float32, shape=[None], name=\"dropout_keep_afm\")\n","            self.train_phase = tf.placeholder(tf.bool, name=\"train_phase_afm\")\n","\n","            # Variables.\n","            self.weights = self._initialize_weights()\n","\n","            # Model.\n","            self.nonzero_embeddings = tf.nn.embedding_lookup(self.weights['feature_embeddings'], self.train_features) # None * M' * K\n","            \n","            element_wise_product_list = []\n","            count = 0\n","            for i in range(0, self.valid_dimension):\n","                for j in range(i+1, self.valid_dimension):\n","                    element_wise_product_list.append(tf.multiply(self.nonzero_embeddings[:,i,:], self.nonzero_embeddings[:,j,:]))\n","                    count += 1\n","            self.element_wise_product = tf.stack(element_wise_product_list) # (M'*(M'-1)) * None * K\n","            self.element_wise_product = tf.transpose(self.element_wise_product, perm=[1,0,2], name=\"element_wise_product\") # None * (M'*(M'-1)) * K\n","            self.interactions = tf.reduce_sum(self.element_wise_product, 2, name=\"interactions\")\n","            # _________ MLP Layer / attention part _____________\n","            num_interactions = self.valid_dimension*(self.valid_dimension-1)/2\n","            if self.attention:\n","                self.attention_mul = tf.reshape(tf.matmul(tf.reshape(self.element_wise_product, shape=[-1, self.hidden_factor[1]]), \\\n","                    self.weights['attention_W']), shape=[-1, int(num_interactions), self.hidden_factor[0]])\n","                # self.attention_exp = tf.exp(tf.reduce_sum(tf.multiply(self.weights['attention_p'], tf.nn.relu(self.attention_mul + \\\n","                #     self.weights['attention_b'])), 2, keep_dims=True)) # None * (M'*(M'-1)) * 1\n","                # self.attention_sum = tf.reduce_sum(self.attention_exp, 1, keep_dims=True) # None * 1 * 1\n","                # self.attention_out = tf.div(self.attention_exp, self.attention_sum, name=\"attention_out\") # None * (M'*(M'-1)) * 1\n","                self.attention_relu = tf.reduce_sum(tf.multiply(self.weights['attention_p'], tf.nn.relu(self.attention_mul + \\\n","                    self.weights['attention_b'])), 2, keep_dims=True) # None * (M'*(M'-1)) * 1\n","                self.attention_out = tf.nn.softmax(self.attention_relu)\n","                self.attention_out = tf.nn.dropout(self.attention_out, self.dropout_keep[0]) # dropout\n","            \n","            # _________ Attention-aware Pairwise Interaction Layer _____________\n","            if self.attention:\n","                self.AFM = tf.reduce_sum(tf.multiply(self.attention_out, self.element_wise_product), 1, name=\"afm\") # None * K\n","            else:\n","                self.AFM = tf.reduce_sum(self.element_wise_product, 1, name=\"afm\") # None * K\n","            self.AFM_FM = tf.reduce_sum(self.element_wise_product, 1, name=\"afm_fm\") # None * K\n","            self.AFM_FM = self.AFM_FM / num_interactions\n","            self.AFM = tf.nn.dropout(self.AFM, self.dropout_keep[1]) # dropout\n","\n","            # _________ out _____________\n","            if self.micro_level_analysis:\n","                self.out = tf.reduce_sum(self.AFM, 1, keep_dims=True, name=\"out_afm\")\n","                self.out_fm = tf.reduce_sum(self.AFM_FM, 1, keep_dims=True, name=\"out_fm\")\n","            else:\n","                self.prediction = tf.matmul(self.AFM, self.weights['prediction']) # None * 1\n","                Bilinear = tf.reduce_sum(self.prediction, 1, keep_dims=True)  # None * 1\n","                self.Feature_bias = tf.reduce_sum(tf.nn.embedding_lookup(self.weights['feature_bias'], self.train_features) , 1)  # None * 1\n","                Bias = self.weights['bias'] * tf.ones_like(self.train_labels)  # None * 1\n","                self.out = tf.add_n([Bilinear, self.Feature_bias, Bias], name=\"out_afm\")  # None * 1\n","\n","            # Compute the loss.\n","            if self.attention and self.lamda_attention > 0:\n","                self.loss = tf.nn.l2_loss(tf.subtract(self.train_labels, self.out)) + tf.contrib.layers.l2_regularizer(self.lamda_attention)(self.weights['attention_W'])  # regulizer\n","            else:\n","                self.loss = tf.nn.l2_loss(tf.subtract(self.train_labels, self.out))\n","\n","            # Optimizer.\n","            if self.optimizer_type == 'AdamOptimizer':\n","                self.optimizer = tf.train.AdamOptimizer(learning_rate=self.learning_rate, beta1=0.9, beta2=0.999, epsilon=1e-8).minimize(self.loss)\n","            elif self.optimizer_type == 'AdagradOptimizer':\n","                self.optimizer = tf.train.AdagradOptimizer(learning_rate=self.learning_rate, initial_accumulator_value=1e-8).minimize(self.loss)\n","            elif self.optimizer_type == 'GradientDescentOptimizer':\n","                self.optimizer = tf.train.GradientDescentOptimizer(learning_rate=self.learning_rate).minimize(self.loss)\n","            elif self.optimizer_type == 'MomentumOptimizer':\n","                self.optimizer = tf.train.MomentumOptimizer(learning_rate=self.learning_rate, momentum=0.95).minimize(self.loss)\n","\n","            # init\n","            self.saver = tf.train.Saver()\n","            init = tf.global_variables_initializer()\n","            self.sess = self._init_session()\n","            self.sess.run(init)\n","\n","            # number of params\n","            total_parameters = 0\n","            for variable in self.weights.values():\n","                shape = variable.get_shape() # shape is an array of tf.Dimension\n","                variable_parameters = 1\n","                for dim in shape:\n","                    variable_parameters *= dim.value\n","                total_parameters += variable_parameters\n","            if self.verbose > 0:\n","                print(\"#params: %d\" %total_parameters)\n","    \n","    def _init_session(self):\n","        # adaptively growing video memory\n","        config = tf.ConfigProto()\n","        config.gpu_options.allow_growth = True\n","        return tf.Session(config=config)\n","\n","    def _initialize_weights(self):\n","        all_weights = dict()\n","        # if freeze_fm, set all other params untrainable\n","        trainable = self.freeze_fm == 0\n","        if self.pretrain_flag > 0 or self.micro_level_analysis:\n","            from_file = self.save_file\n","            # if self.micro_level_analysis:\n","            from_file = self.save_file.replace('afm', 'fm')\n","            weight_saver = tf.train.import_meta_graph(from_file + '.meta')\n","            pretrain_graph = tf.get_default_graph()\n","            feature_embeddings = pretrain_graph.get_tensor_by_name('feature_embeddings:0')\n","            feature_bias = pretrain_graph.get_tensor_by_name('feature_bias:0')\n","            bias = pretrain_graph.get_tensor_by_name('bias:0')\n","            with self._init_session() as sess:\n","                weight_saver.restore(sess, from_file)\n","                fe, fb, b = sess.run([feature_embeddings, feature_bias, bias])\n","            # all_weights['feature_embeddings'] = tf.Variable(fe, dtype=tf.float32, name='feature_embeddings')\n","            all_weights['feature_embeddings'] = tf.Variable(fe, dtype=tf.float32, name='feature_embeddings', trainable=trainable)\n","            all_weights['feature_bias'] = tf.Variable(fb, dtype=tf.float32, name='feature_bias', trainable=trainable)\n","            all_weights['bias'] = tf.Variable(b, dtype=tf.float32, name='bias', trainable=trainable)\n","        else:\n","            all_weights['feature_embeddings'] = tf.Variable(\n","                tf.random_normal([self.features_M, self.hidden_factor[1]], 0.0, 0.01),\n","                name='feature_embeddings', trainable=trainable)  # features_M * K\n","            all_weights['feature_bias'] = tf.Variable(\n","                tf.random_uniform([self.features_M, 1], 0.0, 0.0), name='feature_bias', trainable=trainable)  # features_M * 1\n","            all_weights['bias'] = tf.Variable(tf.constant(0.0), name='bias', trainable=trainable)  # 1 * 1\n","\n","        # attention\n","        if self.attention:\n","            glorot = np.sqrt(2.0 / (self.hidden_factor[0]+self.hidden_factor[1]))\n","            all_weights['attention_W'] = tf.Variable(\n","                np.random.normal(loc=0, scale=glorot, size=(self.hidden_factor[1], self.hidden_factor[0])), dtype=np.float32, name=\"attention_W\")  # K * AK\n","            all_weights['attention_b'] = tf.Variable(\n","                np.random.normal(loc=0, scale=glorot, size=(1, self.hidden_factor[0])), dtype=np.float32, name=\"attention_b\")  # 1 * AK\n","            all_weights['attention_p'] = tf.Variable(\n","                np.random.normal(loc=0, scale=1, size=(self.hidden_factor[0])), dtype=np.float32, name=\"attention_p\") # AK\n","\n","        # prediction layer\n","        all_weights['prediction'] = tf.Variable(np.ones((self.hidden_factor[1], 1), dtype=np.float32))  # hidden_factor * 1\n","\n","        return all_weights\n","\n","    def batch_norm_layer(self, x, train_phase, scope_bn):\n","        bn_train = batch_norm(x, decay=self.decay, center=True, scale=True, updates_collections=None,\n","            is_training=True, reuse=None, trainable=True, scope=scope_bn)\n","        bn_inference = batch_norm(x, decay=self.decay, center=True, scale=True, updates_collections=None,\n","            is_training=False, reuse=True, trainable=True, scope=scope_bn)\n","        z = tf.cond(train_phase, lambda: bn_train, lambda: bn_inference)\n","        return z\n","\n","    def partial_fit(self, data):  # fit a batch\n","        feed_dict = {self.train_features: data['X'], self.train_labels: data['Y'], self.dropout_keep: self.keep, self.train_phase: True}\n","        loss, opt = self.sess.run((self.loss, self.optimizer), feed_dict=feed_dict)\n","        return loss\n","\n","    def get_random_block_from_data(self, data, batch_size):  # generate a random block of training data\n","        start_index = np.random.randint(0, len(data['Y']) - batch_size)\n","        X , Y = [], []\n","        # forward get sample\n","        i = start_index\n","        while len(X) < batch_size and i < len(data['X']):\n","            if len(data['X'][i]) == len(data['X'][start_index]):\n","                Y.append([data['Y'][i]])\n","                X.append(data['X'][i])\n","                i = i + 1\n","            else:\n","                break\n","        # backward get sample\n","        i = start_index\n","        while len(X) < batch_size and i >= 0:\n","            if len(data['X'][i]) == len(data['X'][start_index]):\n","                Y.append([data['Y'][i]])\n","                X.append(data['X'][i])\n","                i = i - 1\n","            else:\n","                break\n","        return {'X': X, 'Y': Y}\n","    \n","    def get_ordered_block_from_data(self, data, batch_size, index):  # generate a ordered block of data\n","        start_index = index*batch_size\n","        X , Y = [], []\n","        # get sample\n","        i = start_index\n","        while len(X) < batch_size and i < len(data['X']):\n","            if len(data['X'][i]) == len(data['X'][start_index]):\n","                Y.append(data['Y'][i])\n","                X.append(data['X'][i])\n","                i = i + 1\n","            else:\n","                break\n","        return {'X': X, 'Y': Y}\n","\n","    def shuffle_in_unison_scary(self, a, b): # shuffle two lists simutaneously\n","        rng_state = np.random.get_state()\n","        np.random.shuffle(a)\n","        np.random.set_state(rng_state)\n","        np.random.shuffle(b)\n","\n","    def train(self, Train_data, Validation_data, Test_data):  # fit a dataset\n","        # Check Init performance\n","        if self.verbose > 0:\n","            t2 = time()\n","            init_train = self.evaluate(Train_data)\n","            init_valid = self.evaluate(Validation_data)\n","            print(\"Init: \\t train=%.4f, validation=%.4f [%.1f s]\" %(init_train, init_valid, time()-t2))\n","\n","        for epoch in range(self.epoch):\n","            t1 = time()\n","            self.shuffle_in_unison_scary(Train_data['X'], Train_data['Y'])\n","            total_batch = int(len(Train_data['Y']) / self.batch_size)\n","            for i in range(total_batch):\n","                # generate a batch\n","                batch_xs = self.get_random_block_from_data(Train_data, self.batch_size)\n","                # Fit training\n","                self.partial_fit(batch_xs)\n","            t2 = time()\n","\n","            # evaluate training and validation datasets\n","            train_result = self.evaluate(Train_data)\n","            valid_result = self.evaluate(Validation_data)\n","            self.train_rmse.append(train_result)\n","            self.valid_rmse.append(valid_result)\n","            if self.verbose > 0 and epoch%self.verbose == 0:\n","                print(\"Epoch %d [%.1f s]\\ttrain=%.4f, validation=%.4f [%.1f s]\"\n","                      %(epoch+1, t2-t1, train_result, valid_result, time()-t2))\n","\n","            # test_result = self.evaluate(Test_data)\n","            # print(\"Epoch %d [%.1f s]\\ttest=%.4f [%.1f s]\"\n","            #       %(epoch+1, t2-t1, test_result, time()-t2))\n","            if self.eva_termination(self.valid_rmse):\n","                break\n","\n","        if self.pretrain_flag < 0 or self.pretrain_flag == 2:\n","            print(\"Save model to file as pretrain.\")\n","            self.saver.save(self.sess, self.save_file)\n","\n","    def eva_termination(self, valid):\n","        if len(valid) > 5:\n","            if valid[-1] > valid[-2] and valid[-2] > valid[-3] and valid[-3] > valid[-4] and valid[-4] > valid[-5]:\n","                return True\n","        return False\n","\n","    def evaluate(self, data):  # evaluate the results for an input set\n","        num_example = len(data['Y'])\n","        # fetch the first batch\n","        batch_index = 0\n","        batch_xs = self.get_ordered_block_from_data(data, self.batch_size, batch_index)\n","        # batch_xs = data\n","        y_pred = None\n","        # if len(batch_xs['X']) > 0:\n","        while len(batch_xs['X']) > 0:\n","            num_batch = len(batch_xs['Y'])\n","            feed_dict = {self.train_features: batch_xs['X'], self.train_labels: [[y] for y in batch_xs['Y']], self.dropout_keep: list(1.0 for i in range(len(self.keep))), self.train_phase: False}\n","            a_out, batch_out = self.sess.run((self.attention_out, self.out), feed_dict=feed_dict)\n","            \n","            if batch_index == 0:\n","                y_pred = np.reshape(batch_out, (num_batch,))\n","            else:\n","                y_pred = np.concatenate((y_pred, np.reshape(batch_out, (num_batch,))))\n","            # fetch the next batch\n","            batch_index += 1\n","            batch_xs = self.get_ordered_block_from_data(data, self.batch_size, batch_index)\n","\n","        y_true = np.reshape(data['Y'], (num_example,))\n","\n","        predictions_bounded = np.maximum(y_pred, np.ones(num_example) * min(y_true))  # bound the lower values\n","        predictions_bounded = np.minimum(predictions_bounded, np.ones(num_example) * max(y_true))  # bound the higher values\n","        RMSE = math.sqrt(mean_squared_error(y_true, predictions_bounded))\n","        return RMSE\n","\n","def make_save_file(args):\n","    pretrain_path = '/content/pretrain/%s_%d' %(args.dataset, eval(args.hidden_factor)[1])\n","    if args.mla:\n","        pretrain_path += '_mla'\n","    if not os.path.exists(pretrain_path):\n","        os.makedirs(pretrain_path)\n","    save_file = pretrain_path+'/%s_%d' %(args.dataset, eval(args.hidden_factor)[1])\n","    return save_file\n","\n","def train(args):\n","    # Data loading\n","    data = LoadData(args.path, args.dataset)\n","    if args.verbose > 0:\n","        print(\"AFM: dataset=%s, factors=%s, attention=%d, freeze_fm=%d, #epoch=%d, batch=%d, lr=%.4f, lambda_attention=%.1e, keep=%s, optimizer=%s, batch_norm=%d, decay=%f, activation=%s\"\n","              %(args.dataset, args.hidden_factor, args.attention, args.freeze_fm, args.epoch, args.batch_size, args.lr, args.lamda_attention, args.keep, args.optimizer, \n","              args.batch_norm, args.decay, args.activation))\n","    activation_function = tf.nn.relu\n","    if args.activation == 'sigmoid':\n","        activation_function = tf.sigmoid\n","    elif args.activation == 'tanh':\n","        activation_function == tf.tanh\n","    elif args.activation == 'identity':\n","        activation_function = tf.identity\n","    \n","    save_file = make_save_file(args)\n","    # Training\n","    t1 = time()\n","\n","    num_variable = data.truncate_features()\n","    if args.mla:\n","        args.freeze_fm = 1\n","    model = AFM(data.features_M, args.pretrain, save_file, args.attention, eval(args.hidden_factor), args.valid_dimen, \n","        activation_function, num_variable, args.freeze_fm, args.epoch, args.batch_size, args.lr, args.lamda_attention, eval(args.keep), args.optimizer, \n","        args.batch_norm, args.decay, args.verbose, args.mla)\n","    \n","    model.train(data.Train_data, data.Validation_data, data.Test_data)\n","    \n","    # Find the best validation result across iterations\n","    best_valid_score = 0\n","    best_valid_score = min(model.valid_rmse)\n","    best_epoch = model.valid_rmse.index(best_valid_score)\n","    print(\"Best Iter(validation)= %d\\t train = %.4f, valid = %.4f [%.1f s]\" \n","           %(best_epoch+1, model.train_rmse[best_epoch], model.valid_rmse[best_epoch], time()-t1))\n","\n","def evaluate(args):\n","    # load test data\n","    data = DATA.LoadData(args.path, args.dataset).Test_data\n","    save_file = make_save_file(args)\n","    \n","    # load the graph\n","    weight_saver = tf.train.import_meta_graph(save_file + '.meta')\n","    pretrain_graph = tf.get_default_graph()\n","    # load tensors \n","    # feature_embeddings = pretrain_graph.get_tensor_by_name('feature_embeddings:0')\n","    # feature_bias = pretrain_graph.get_tensor_by_name('feature_bias:0')\n","    # bias = pretrain_graph.get_tensor_by_name('bias:0')\n","    # afm = pretrain_graph.get_tensor_by_name('afm:0')\n","    out_of_afm = pretrain_graph.get_tensor_by_name('out_afm:0')\n","    interactions = pretrain_graph.get_tensor_by_name('interactions:0')\n","    attention_out = pretrain_graph.get_tensor_by_name('attention_out:0')\n","    # placeholders for afm\n","    train_features_afm = pretrain_graph.get_tensor_by_name('train_features_afm:0')\n","    train_labels_afm = pretrain_graph.get_tensor_by_name('train_labels_afm:0')\n","    dropout_keep_afm = pretrain_graph.get_tensor_by_name('dropout_keep_afm:0')\n","    train_phase_afm = pretrain_graph.get_tensor_by_name('train_phase_afm:0')\n","\n","    # tensors and placeholders for fm\n","    if args.mla:\n","         out_of_fm = pretrain_graph.get_tensor_by_name('out_fm:0')\n","         element_wise_product = pretrain_graph.get_tensor_by_name('element_wise_product:0')\n","         train_features_fm = pretrain_graph.get_tensor_by_name('train_features_fm:0')\n","         train_labels_fm = pretrain_graph.get_tensor_by_name('train_labels_fm:0')\n","         dropout_keep_fm = pretrain_graph.get_tensor_by_name('dropout_keep_fm:0')\n","         train_phase_fm = pretrain_graph.get_tensor_by_name('train_phase_fm:0')\n","\n","    # restore session\n","    config = tf.ConfigProto()\n","    config.gpu_options.allow_growth = True\n","    sess = tf.Session(config=config)\n","    weight_saver.restore(sess, save_file)\n","\n","    # start evaluation\n","    num_example = len(data['Y'])\n","    if args.mla:\n","        feed_dict = {train_features_afm: data['X'], train_labels_afm: [[y] for y in data['Y']], dropout_keep_afm: [1.0,1.0], train_phase_afm: False, \\\n","                     train_features_fm: data['X'], train_labels_fm: [[y] for y in data['Y']], dropout_keep_fm: 1.0, train_phase_fm: False}\n","        ao, inter, out_fm, predictions = sess.run((attention_out, interactions, out_of_fm, out_of_afm), feed_dict=feed_dict)\n","    else:\n","        feed_dict = {train_features_afm: data['X'], train_labels_afm: [[y] for y in data['Y']], dropout_keep_afm: [1.0,1.0], train_phase_afm: False}\n","        predictions = sess.run((out_of_afm), feed_dict=feed_dict)\n","\n","    # calculate rmse\n","    y_pred_afm = np.reshape(predictions, (num_example,))\n","    y_true = np.reshape(data['Y'], (num_example,))\n","    \n","    predictions_bounded = np.maximum(y_pred_afm, np.ones(num_example) * min(y_true))  # bound the lower values\n","    predictions_bounded = np.minimum(predictions_bounded, np.ones(num_example) * max(y_true))  # bound the higher values\n","    RMSE = math.sqrt(mean_squared_error(y_true, predictions_bounded))\n","\n","    print(\"Test RMSE: %.4f\"%(RMSE))\n","\n","    if args.mla:\n","        # select significant cases\n","        ao = np.reshape(ao, (num_example, 3))\n","        y_pred_fm = np.reshape(out_fm, (num_example,))\n","        pred_abs_fm = abs(y_pred_fm - y_true)\n","        pred_abs_afm = abs(y_pred_afm - y_true)\n","        pred_abs = pred_abs_afm - pred_abs_fm\n","\n","        ids = np.arange(0, num_example, 1)\n","\n","        sorted_ids = sorted(ids, key=lambda k: pred_abs_afm[k]+abs(ao[k][0]*ao[k][1]*ao[k][2]))\n","        # sorted_ids = sorted(ids, key=lambda k: abs(ao[k][0]*ao[k][1]*ao[k][2]))\n","        for i in range(3):\n","            _id = sorted_ids[i]\n","            print('## %d: %d'%(i+1, y_true[_id]))\n","            print('0.33*%.2f + 0.33*%.2f + 0.33*%.2f = %.2f'%(inter[_id][0], inter[_id][1], inter[_id][2], y_pred_fm[_id]))\n","            print('%.2f*%.2f + %.2f*%.2f + %.2f*%.2f = %.2f\\n'%(\\\n","                          ao[_id][0], inter[_id][0], \\\n","                          ao[_id][1], inter[_id][1], \\\n","                          ao[_id][2], inter[_id][2], y_pred_afm[_id]))\n","\n","\n","if __name__ == '__main__':\n","    args = parse_args()\n","\n","    # if args.mla:\n","    #     args.lr = 0.1\n","    #     args.keep = '[1.0,1.0]'\n","    #     args.lamda_attention = 10.0\n","    # else:\n","    #     args.lr = 0.1\n","    #     args.keep = '[1.0,0.5]'\n","    #     args.lamda_attention = 100.0\n","\n","    if args.process == 'train':\n","        train(args)\n","    elif args.process == 'evaluate':\n","        evaluate(args)"],"execution_count":25,"outputs":[{"output_type":"stream","name":"stdout","text":["AFM: dataset=ml-tag, factors=[16,16], attention=1, freeze_fm=0, #epoch=20, batch=4096, lr=0.1000, lambda_attention=1.0e+02, keep=[1.0,0.5], optimizer=AdagradOptimizer, batch_norm=0, decay=0.999000, activation=relu\n","#params: 1537870\n","Init: \t train=1.0000, validation=1.0000 [13.2 s]\n","Epoch 1 [17.4 s]\ttrain=0.5278, validation=0.5862 [15.6 s]\n","Epoch 2 [18.2 s]\ttrain=0.4806, validation=0.5704 [15.2 s]\n","Epoch 3 [17.8 s]\ttrain=0.4494, validation=0.5633 [15.1 s]\n","Epoch 4 [17.7 s]\ttrain=0.4263, validation=0.5585 [15.3 s]\n","Epoch 5 [18.3 s]\ttrain=0.4098, validation=0.5550 [16.0 s]\n","Epoch 6 [19.2 s]\ttrain=0.3966, validation=0.5527 [16.2 s]\n","Epoch 7 [18.8 s]\ttrain=0.3853, validation=0.5508 [16.7 s]\n","Epoch 8 [19.5 s]\ttrain=0.3776, validation=0.5494 [16.5 s]\n","Epoch 9 [18.0 s]\ttrain=0.3697, validation=0.5486 [18.2 s]\n","Epoch 10 [18.1 s]\ttrain=0.3639, validation=0.5479 [15.8 s]\n","Epoch 11 [20.3 s]\ttrain=0.3596, validation=0.5475 [17.0 s]\n","Epoch 12 [18.7 s]\ttrain=0.3551, validation=0.5468 [15.3 s]\n","Epoch 13 [19.5 s]\ttrain=0.3510, validation=0.5465 [15.7 s]\n","Epoch 14 [19.0 s]\ttrain=0.3474, validation=0.5459 [15.5 s]\n","Epoch 15 [18.5 s]\ttrain=0.3451, validation=0.5454 [16.4 s]\n","Epoch 16 [18.0 s]\ttrain=0.3417, validation=0.5452 [14.9 s]\n","Epoch 17 [17.3 s]\ttrain=0.3382, validation=0.5452 [19.6 s]\n","Epoch 18 [16.2 s]\ttrain=0.3370, validation=0.5449 [16.3 s]\n","Epoch 19 [15.7 s]\ttrain=0.3349, validation=0.5447 [14.4 s]\n","Epoch 20 [17.9 s]\ttrain=0.3331, validation=0.5441 [14.9 s]\n","Save model to file as pretrain.\n","Best Iter(validation)= 20\t train = 0.3331, valid = 0.5441 [700.9 s]\n"]}]}]}