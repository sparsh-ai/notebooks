{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"2022-01-21-objectworld.ipynb","provenance":[{"file_id":"https://github.com/recohut/nbs/blob/main/raw/T124583%20%7C%20Objectworld%20RL%20MDP%20Environment.ipynb","timestamp":1644659344991}],"collapsed_sections":[],"authorship_tag":"ABX9TyNsHfZ2+8IiVjuHtG5hp/+7"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"sG1aHc6O0c3s"},"source":["# Objectworld RL MDP Environment"]},{"cell_type":"code","metadata":{"id":"9a89YboZ0xNs"},"source":["import numpy as np\n","import numpy.random as rn\n","import math\n","from itertools import product\n","import matplotlib.pyplot as plt\n","import warnings\n","warnings.filterwarnings('ignore')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"iqUoPCPH04J5"},"source":["## Gridworld\n","\n","Implements the gridworld MDP. Matthew Alger, 2015."]},{"cell_type":"code","metadata":{"id":"G2Mtf2yW0_-b"},"source":["class Gridworld(object):\n","    \"\"\"\n","    Gridworld MDP.\n","    \"\"\"\n","\n","    def __init__(self, grid_size, wind, discount):\n","        \"\"\"\n","        grid_size: Grid size. int.\n","        wind: Chance of moving randomly. float.\n","        discount: MDP discount. float.\n","        -> Gridworld\n","        \"\"\"\n","\n","        self.actions = ((1, 0), (0, 1), (-1, 0), (0, -1))\n","        self.n_actions = len(self.actions)\n","        self.n_states = grid_size**2\n","        self.grid_size = grid_size\n","        self.wind = wind\n","        self.discount = discount\n","\n","        # Preconstruct the transition probability array.\n","        self.transition_probability = np.array(\n","            [[[self._transition_probability(i, j, k)\n","               for k in range(self.n_states)]\n","              for j in range(self.n_actions)]\n","             for i in range(self.n_states)])\n","\n","    def __str__(self):\n","        return \"Gridworld({}, {}, {})\".format(self.grid_size, self.wind,\n","                                              self.discount)\n","\n","    def feature_vector(self, i, feature_map=\"ident\"):\n","        \"\"\"\n","        Get the feature vector associated with a state integer.\n","\n","        i: State int.\n","        feature_map: Which feature map to use (default ident). String in {ident,\n","            coord, proxi}.\n","        -> Feature vector.\n","        \"\"\"\n","\n","        if feature_map == \"coord\":\n","            f = np.zeros(self.grid_size)\n","            x, y = i % self.grid_size, i // self.grid_size\n","            f[x] += 1\n","            f[y] += 1\n","            return f\n","        if feature_map == \"proxi\":\n","            f = np.zeros(self.n_states)\n","            x, y = i % self.grid_size, i // self.grid_size\n","            for b in range(self.grid_size):\n","                for a in range(self.grid_size):\n","                    dist = abs(x - a) + abs(y - b)\n","                    f[self.point_to_int((a, b))] = dist\n","            return f\n","        # Assume identity map.\n","        f = np.zeros(self.n_states)\n","        f[i] = 1\n","        return f\n","\n","    def feature_matrix(self, feature_map=\"ident\"):\n","        \"\"\"\n","        Get the feature matrix for this gridworld.\n","\n","        feature_map: Which feature map to use (default ident). String in {ident,\n","            coord, proxi}.\n","        -> NumPy array with shape (n_states, d_states).\n","        \"\"\"\n","\n","        features = []\n","        for n in range(self.n_states):\n","            f = self.feature_vector(n, feature_map)\n","            features.append(f)\n","        return np.array(features)\n","\n","    def int_to_point(self, i):\n","        \"\"\"\n","        Convert a state int into the corresponding coordinate.\n","\n","        i: State int.\n","        -> (x, y) int tuple.\n","        \"\"\"\n","\n","        return (i % self.grid_size, i // self.grid_size)\n","\n","    def point_to_int(self, p):\n","        \"\"\"\n","        Convert a coordinate into the corresponding state int.\n","\n","        p: (x, y) tuple.\n","        -> State int.\n","        \"\"\"\n","\n","        return p[0] + p[1]*self.grid_size\n","\n","    def neighbouring(self, i, k):\n","        \"\"\"\n","        Get whether two points neighbour each other. Also returns true if they\n","        are the same point.\n","\n","        i: (x, y) int tuple.\n","        k: (x, y) int tuple.\n","        -> bool.\n","        \"\"\"\n","\n","        return abs(i[0] - k[0]) + abs(i[1] - k[1]) <= 1\n","\n","    def _transition_probability(self, i, j, k):\n","        \"\"\"\n","        Get the probability of transitioning from state i to state k given\n","        action j.\n","\n","        i: State int.\n","        j: Action int.\n","        k: State int.\n","        -> p(s_k | s_i, a_j)\n","        \"\"\"\n","\n","        xi, yi = self.int_to_point(i)\n","        xj, yj = self.actions[j]\n","        xk, yk = self.int_to_point(k)\n","\n","        if not self.neighbouring((xi, yi), (xk, yk)):\n","            return 0.0\n","\n","        # Is k the intended state to move to?\n","        if (xi + xj, yi + yj) == (xk, yk):\n","            return 1 - self.wind + self.wind/self.n_actions\n","\n","        # If these are not the same point, then we can move there by wind.\n","        if (xi, yi) != (xk, yk):\n","            return self.wind/self.n_actions\n","\n","        # If these are the same point, we can only move here by either moving\n","        # off the grid or being blown off the grid. Are we on a corner or not?\n","        if (xi, yi) in {(0, 0), (self.grid_size-1, self.grid_size-1),\n","                        (0, self.grid_size-1), (self.grid_size-1, 0)}:\n","            # Corner.\n","            # Can move off the edge in two directions.\n","            # Did we intend to move off the grid?\n","            if not (0 <= xi + xj < self.grid_size and\n","                    0 <= yi + yj < self.grid_size):\n","                # We intended to move off the grid, so we have the regular\n","                # success chance of staying here plus an extra chance of blowing\n","                # onto the *other* off-grid square.\n","                return 1 - self.wind + 2*self.wind/self.n_actions\n","            else:\n","                # We can blow off the grid in either direction only by wind.\n","                return 2*self.wind/self.n_actions\n","        else:\n","            # Not a corner. Is it an edge?\n","            if (xi not in {0, self.grid_size-1} and\n","                yi not in {0, self.grid_size-1}):\n","                # Not an edge.\n","                return 0.0\n","\n","            # Edge.\n","            # Can only move off the edge in one direction.\n","            # Did we intend to move off the grid?\n","            if not (0 <= xi + xj < self.grid_size and\n","                    0 <= yi + yj < self.grid_size):\n","                # We intended to move off the grid, so we have the regular\n","                # success chance of staying here.\n","                return 1 - self.wind + self.wind/self.n_actions\n","            else:\n","                # We can blow off the grid only by wind.\n","                return self.wind/self.n_actions\n","\n","    def reward(self, state_int):\n","        \"\"\"\n","        Reward for being in state state_int.\n","\n","        state_int: State integer. int.\n","        -> Reward.\n","        \"\"\"\n","\n","        if state_int == self.n_states - 1:\n","            return 1\n","        return 0\n","\n","    def average_reward(self, n_trajectories, trajectory_length, policy):\n","        \"\"\"\n","        Calculate the average total reward obtained by following a given policy\n","        over n_paths paths.\n","\n","        policy: Map from state integers to action integers.\n","        n_trajectories: Number of trajectories. int.\n","        trajectory_length: Length of an episode. int.\n","        -> Average reward, standard deviation.\n","        \"\"\"\n","\n","        trajectories = self.generate_trajectories(n_trajectories,\n","                                             trajectory_length, policy)\n","        rewards = [[r for _, _, r in trajectory] for trajectory in trajectories]\n","        rewards = np.array(rewards)\n","\n","        # Add up all the rewards to find the total reward.\n","        total_reward = rewards.sum(axis=1)\n","\n","        # Return the average reward and standard deviation.\n","        return total_reward.mean(), total_reward.std()\n","\n","    def optimal_policy(self, state_int):\n","        \"\"\"\n","        The optimal policy for this gridworld.\n","\n","        state_int: What state we are in. int.\n","        -> Action int.\n","        \"\"\"\n","\n","        sx, sy = self.int_to_point(state_int)\n","\n","        if sx < self.grid_size and sy < self.grid_size:\n","            return rn.randint(0, 2)\n","        if sx < self.grid_size-1:\n","            return 0\n","        if sy < self.grid_size-1:\n","            return 1\n","        raise ValueError(\"Unexpected state.\")\n","\n","    def optimal_policy_deterministic(self, state_int):\n","        \"\"\"\n","        Deterministic version of the optimal policy for this gridworld.\n","\n","        state_int: What state we are in. int.\n","        -> Action int.\n","        \"\"\"\n","\n","        sx, sy = self.int_to_point(state_int)\n","        if sx < sy:\n","            return 0\n","        return 1\n","\n","    def generate_trajectories(self, n_trajectories, trajectory_length, policy,\n","                                    random_start=False):\n","        \"\"\"\n","        Generate n_trajectories trajectories with length trajectory_length,\n","        following the given policy.\n","\n","        n_trajectories: Number of trajectories. int.\n","        trajectory_length: Length of an episode. int.\n","        policy: Map from state integers to action integers.\n","        random_start: Whether to start randomly (default False). bool.\n","        -> [[(state int, action int, reward float)]]\n","        \"\"\"\n","\n","        trajectories = []\n","        for _ in range(n_trajectories):\n","            if random_start:\n","                sx, sy = rn.randint(self.grid_size), rn.randint(self.grid_size)\n","            else:\n","                sx, sy = 0, 0\n","\n","            trajectory = []\n","            for _ in range(trajectory_length):\n","                if rn.random() < self.wind:\n","                    action = self.actions[rn.randint(0, 4)]\n","                else:\n","                    # Follow the given policy.\n","                    action = self.actions[policy(self.point_to_int((sx, sy)))]\n","\n","                if (0 <= sx + action[0] < self.grid_size and\n","                        0 <= sy + action[1] < self.grid_size):\n","                    next_sx = sx + action[0]\n","                    next_sy = sy + action[1]\n","                else:\n","                    next_sx = sx\n","                    next_sy = sy\n","\n","                state_int = self.point_to_int((sx, sy))\n","                action_int = self.actions.index(action)\n","                next_state_int = self.point_to_int((next_sx, next_sy))\n","                reward = self.reward(next_state_int)\n","                trajectory.append((state_int, action_int, reward))\n","\n","                sx = next_sx\n","                sy = next_sy\n","\n","            trajectories.append(trajectory)\n","\n","        return np.array(trajectories)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ZJSTEj0o1GMc"},"source":["## Objectworld\n","\n","Implements the objectworld MDP described in Levine et al. 2011. Matthew Alger, 2015."]},{"cell_type":"code","metadata":{"id":"3Rg1qVul1Pzt"},"source":["class OWObject(object):\n","    \"\"\"\n","    Object in objectworld.\n","    \"\"\"\n","\n","    def __init__(self, inner_colour, outer_colour):\n","        \"\"\"\n","        inner_colour: Inner colour of object. int.\n","        outer_colour: Outer colour of object. int.\n","        -> OWObject\n","        \"\"\"\n","\n","        self.inner_colour = inner_colour\n","        self.outer_colour = outer_colour\n","\n","    def __str__(self):\n","        \"\"\"\n","        A string representation of this object.\n","        -> __str__\n","        \"\"\"\n","\n","        return \"<OWObject (In: {}) (Out: {})>\".format(self.inner_colour,\n","                                                      self.outer_colour)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fNhr-UkX1WMo"},"source":["class Objectworld(Gridworld):\n","    \"\"\"\n","    Objectworld MDP.\n","    \"\"\"\n","\n","    def __init__(self, grid_size, n_objects, n_colours, wind, discount):\n","        \"\"\"\n","        grid_size: Grid size. int.\n","        n_objects: Number of objects in the world. int.\n","        n_colours: Number of colours to colour objects with. int.\n","        wind: Chance of moving randomly. float.\n","        discount: MDP discount. float.\n","        -> Objectworld\n","        \"\"\"\n","\n","        super().__init__(grid_size, wind, discount)\n","\n","        self.actions = ((1, 0), (0, 1), (-1, 0), (0, -1), (0, 0))\n","        self.n_actions = len(self.actions)\n","        self.n_objects = n_objects\n","        self.n_colours = n_colours\n","\n","        # Generate objects.\n","        self.objects = {}\n","        for _ in range(self.n_objects):\n","            obj = OWObject(rn.randint(self.n_colours),\n","                           rn.randint(self.n_colours))\n","\n","            while True:\n","                x = rn.randint(self.grid_size)\n","                y = rn.randint(self.grid_size)\n","\n","                if (x, y) not in self.objects:\n","                    break\n","\n","            self.objects[x, y] = obj\n","\n","        # Preconstruct the transition probability array.\n","        self.transition_probability = np.array(\n","            [[[self._transition_probability(i, j, k)\n","               for k in range(self.n_states)]\n","              for j in range(self.n_actions)]\n","             for i in range(self.n_states)])\n","\n","    def feature_vector(self, i, discrete=True):\n","        \"\"\"\n","        Get the feature vector associated with a state integer.\n","        i: State int.\n","        discrete: Whether the feature vectors should be discrete (default True).\n","            bool.\n","        -> Feature vector.\n","        \"\"\"\n","\n","        sx, sy = self.int_to_point(i)\n","\n","        nearest_inner = {}  # colour: distance\n","        nearest_outer = {}  # colour: distance\n","\n","        for y in range(self.grid_size):\n","            for x in range(self.grid_size):\n","                if (x, y) in self.objects:\n","                    dist = math.hypot((x - sx), (y - sy))\n","                    obj = self.objects[x, y]\n","                    if obj.inner_colour in nearest_inner:\n","                        if dist < nearest_inner[obj.inner_colour]:\n","                            nearest_inner[obj.inner_colour] = dist\n","                    else:\n","                        nearest_inner[obj.inner_colour] = dist\n","                    if obj.outer_colour in nearest_outer:\n","                        if dist < nearest_outer[obj.outer_colour]:\n","                            nearest_outer[obj.outer_colour] = dist\n","                    else:\n","                        nearest_outer[obj.outer_colour] = dist\n","\n","        # Need to ensure that all colours are represented.\n","        for c in range(self.n_colours):\n","            if c not in nearest_inner:\n","                nearest_inner[c] = 0\n","            if c not in nearest_outer:\n","                nearest_outer[c] = 0\n","\n","        if discrete:\n","            state = np.zeros((2*self.n_colours*self.grid_size,))\n","            i = 0\n","            for c in range(self.n_colours):\n","                for d in range(1, self.grid_size+1):\n","                    if nearest_inner[c] < d:\n","                        state[i] = 1\n","                    i += 1\n","                    if nearest_outer[c] < d:\n","                        state[i] = 1\n","                    i += 1\n","            assert i == 2*self.n_colours*self.grid_size\n","            assert (state >= 0).all()\n","        else:\n","            # Continuous features.\n","            state = np.zeros((2*self.n_colours))\n","            i = 0\n","            for c in range(self.n_colours):\n","                state[i] = nearest_inner[c]\n","                i += 1\n","                state[i] = nearest_outer[c]\n","                i += 1\n","\n","        return state\n","\n","    def feature_matrix(self, discrete=True):\n","        \"\"\"\n","        Get the feature matrix for this objectworld.\n","        discrete: Whether the feature vectors should be discrete (default True).\n","            bool.\n","        -> NumPy array with shape (n_states, n_states).\n","        \"\"\"\n","\n","        return np.array([self.feature_vector(i, discrete)\n","                         for i in range(self.n_states)])\n","\n","    def reward(self, state_int):\n","        \"\"\"\n","        Get the reward for a state int.\n","        state_int: State int.\n","        -> reward float\n","        \"\"\"\n","\n","        x, y = self.int_to_point(state_int)\n","\n","        near_c0 = False\n","        near_c1 = False\n","        for (dx, dy) in product(range(-3, 4), range(-3, 4)):\n","            if 0 <= x + dx < self.grid_size and 0 <= y + dy < self.grid_size:\n","                if (abs(dx) + abs(dy) <= 3 and\n","                        (x+dx, y+dy) in self.objects and\n","                        self.objects[x+dx, y+dy].outer_colour == 0):\n","                    near_c0 = True\n","                if (abs(dx) + abs(dy) <= 2 and\n","                        (x+dx, y+dy) in self.objects and\n","                        self.objects[x+dx, y+dy].outer_colour == 1):\n","                    near_c1 = True\n","\n","        if near_c0 and near_c1:\n","            return 1\n","        if near_c0:\n","            return -1\n","        return 0\n","\n","    def generate_trajectories(self, n_trajectories, trajectory_length, policy):\n","        \"\"\"\n","        Generate n_trajectories trajectories with length trajectory_length.\n","        n_trajectories: Number of trajectories. int.\n","        trajectory_length: Length of an episode. int.\n","        policy: Map from state integers to action integers.\n","        -> [[(state int, action int, reward float)]]\n","        \"\"\"\n","\n","        return super().generate_trajectories(n_trajectories, trajectory_length,\n","                                             policy,\n","                                             True)\n","\n","    def optimal_policy(self, state_int):\n","        raise NotImplementedError(\n","            \"Optimal policy is not implemented for Objectworld.\")\n","    def optimal_policy_deterministic(self, state_int):\n","        raise NotImplementedError(\n","            \"Optimal policy is not implemented for Objectworld.\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8aYUMe2E3by2"},"source":["## MaxEnt Algorithm"]},{"cell_type":"code","metadata":{"id":"RYdXyReh2LxM"},"source":["def irl(feature_matrix, n_actions, discount, transition_probability,\n","        trajectories, epochs, learning_rate):\n","    \"\"\"\n","    Find the reward function for the given trajectories.\n","    feature_matrix: Matrix with the nth row representing the nth state. NumPy\n","        array with shape (N, D) where N is the number of states and D is the\n","        dimensionality of the state.\n","    n_actions: Number of actions A. int.\n","    discount: Discount factor of the MDP. float.\n","    transition_probability: NumPy array mapping (state_i, action, state_k) to\n","        the probability of transitioning from state_i to state_k under action.\n","        Shape (N, A, N).\n","    trajectories: 3D array of state/action pairs. States are ints, actions\n","        are ints. NumPy array with shape (T, L, 2) where T is the number of\n","        trajectories and L is the trajectory length.\n","    epochs: Number of gradient descent steps. int.\n","    learning_rate: Gradient descent learning rate. float.\n","    -> Reward vector with shape (N,).\n","    \"\"\"\n","\n","    n_states, d_states = feature_matrix.shape\n","\n","    # Initialise weights.\n","    alpha = rn.uniform(size=(d_states,))\n","\n","    # Calculate the feature expectations \\tilde{phi}.\n","    feature_expectations = find_feature_expectations(feature_matrix,\n","                                                     trajectories)\n","\n","    # Gradient descent on alpha.\n","    for i in range(epochs):\n","        # print(\"i: {}\".format(i))\n","        r = feature_matrix.dot(alpha)\n","        expected_svf = find_expected_svf(n_states, r, n_actions, discount,\n","                                         transition_probability, trajectories)\n","        grad = feature_expectations - feature_matrix.T.dot(expected_svf)\n","\n","        alpha += learning_rate * grad\n","\n","    return feature_matrix.dot(alpha).reshape((n_states,))\n","\n","def find_svf(n_states, trajectories):\n","    \"\"\"\n","    Find the state visitation frequency from trajectories.\n","    n_states: Number of states. int.\n","    trajectories: 3D array of state/action pairs. States are ints, actions\n","        are ints. NumPy array with shape (T, L, 2) where T is the number of\n","        trajectories and L is the trajectory length.\n","    -> State visitation frequencies vector with shape (N,).\n","    \"\"\"\n","\n","    svf = np.zeros(n_states)\n","\n","    for trajectory in trajectories:\n","        for state, _, _ in trajectory:\n","            svf[state] += 1\n","\n","    svf /= trajectories.shape[0]\n","\n","    return svf\n","\n","def find_feature_expectations(feature_matrix, trajectories):\n","    \"\"\"\n","    Find the feature expectations for the given trajectories. This is the\n","    average path feature vector.\n","    feature_matrix: Matrix with the nth row representing the nth state. NumPy\n","        array with shape (N, D) where N is the number of states and D is the\n","        dimensionality of the state.\n","    trajectories: 3D array of state/action pairs. States are ints, actions\n","        are ints. NumPy array with shape (T, L, 2) where T is the number of\n","        trajectories and L is the trajectory length.\n","    -> Feature expectations vector with shape (D,).\n","    \"\"\"\n","\n","    feature_expectations = np.zeros(feature_matrix.shape[1])\n","\n","    for trajectory in trajectories:\n","        for state, _, _ in trajectory:\n","            feature_expectations += feature_matrix[state]\n","\n","    feature_expectations /= trajectories.shape[0]\n","\n","    return feature_expectations\n","\n","def find_expected_svf(n_states, r, n_actions, discount,\n","                      transition_probability, trajectories):\n","    \"\"\"\n","    Find the expected state visitation frequencies using algorithm 1 from\n","    Ziebart et al. 2008.\n","    n_states: Number of states N. int.\n","    alpha: Reward. NumPy array with shape (N,).\n","    n_actions: Number of actions A. int.\n","    discount: Discount factor of the MDP. float.\n","    transition_probability: NumPy array mapping (state_i, action, state_k) to\n","        the probability of transitioning from state_i to state_k under action.\n","        Shape (N, A, N).\n","    trajectories: 3D array of state/action pairs. States are ints, actions\n","        are ints. NumPy array with shape (T, L, 2) where T is the number of\n","        trajectories and L is the trajectory length.\n","    -> Expected state visitation frequencies vector with shape (N,).\n","    \"\"\"\n","\n","    n_trajectories = trajectories.shape[0]\n","    trajectory_length = trajectories.shape[1]\n","\n","    # policy = find_policy(n_states, r, n_actions, discount,\n","    #                                 transition_probability)\n","    policy = find_policy(n_states, n_actions,\n","                                         transition_probability, r, discount)\n","\n","    start_state_count = np.zeros(n_states)\n","    for trajectory in trajectories:\n","        start_state_count[trajectory[0, 0]] += 1\n","    p_start_state = start_state_count/n_trajectories\n","\n","    expected_svf = np.tile(p_start_state, (trajectory_length, 1)).T\n","    for t in range(1, trajectory_length):\n","        expected_svf[:, t] = 0\n","        for i, j, k in product(range(n_states), range(n_actions), range(n_states)):\n","            expected_svf[k, t] += (expected_svf[i, t-1] *\n","                                  policy[i, j] * # Stochastic policy\n","                                  transition_probability[i, j, k])\n","\n","    return expected_svf.sum(axis=1)\n","\n","def softmax(x1, x2):\n","    \"\"\"\n","    Soft-maximum calculation, from algorithm 9.2 in Ziebart's PhD thesis.\n","    x1: float.\n","    x2: float.\n","    -> softmax(x1, x2)\n","    \"\"\"\n","\n","    max_x = max(x1, x2)\n","    min_x = min(x1, x2)\n","    return max_x + np.log(1 + np.exp(min_x - max_x))\n","\n","def find_policy(n_states, r, n_actions, discount,\n","                           transition_probability):\n","    \"\"\"\n","    Find a policy with linear value iteration. Based on the code accompanying\n","    the Levine et al. GPIRL paper and on Ziebart's PhD thesis (algorithm 9.1).\n","    n_states: Number of states N. int.\n","    r: Reward. NumPy array with shape (N,).\n","    n_actions: Number of actions A. int.\n","    discount: Discount factor of the MDP. float.\n","    transition_probability: NumPy array mapping (state_i, action, state_k) to\n","        the probability of transitioning from state_i to state_k under action.\n","        Shape (N, A, N).\n","    -> NumPy array of states and the probability of taking each action in that\n","        state, with shape (N, A).\n","    \"\"\"\n","\n","    # V = value(n_states, transition_probability, r, discount)\n","\n","    # NumPy's dot really dislikes using inf, so I'm making everything finite\n","    # using nan_to_num.\n","    V = np.nan_to_num(np.ones((n_states, 1)) * float(\"-inf\"))\n","\n","    diff = np.ones((n_states,))\n","    while (diff > 1e-4).all():  # Iterate until convergence.\n","        new_V = r.copy()\n","        for j in range(n_actions):\n","            for i in range(n_states):\n","                new_V[i] = softmax(new_V[i], r[i] + discount*\n","                    np.sum(transition_probability[i, j, k] * V[k]\n","                           for k in range(n_states)))\n","\n","        # # This seems to diverge, so we z-score it (engineering hack).\n","        new_V = (new_V - new_V.mean())/new_V.std()\n","\n","        diff = abs(V - new_V)\n","        V = new_V\n","\n","    # We really want Q, not V, so grab that using equation 9.2 from the thesis.\n","    Q = np.zeros((n_states, n_actions))\n","    for i in range(n_states):\n","        for j in range(n_actions):\n","            p = np.array([transition_probability[i, j, k]\n","                          for k in range(n_states)])\n","            Q[i, j] = p.dot(r + discount*V)\n","\n","    # Softmax by row to interpret these values as probabilities.\n","    Q -= Q.max(axis=1).reshape((n_states, 1))  # For numerical stability.\n","    Q = np.exp(Q)/np.exp(Q).sum(axis=1).reshape((n_states, 1))\n","    return Q\n","\n","def expected_value_difference(n_states, n_actions, transition_probability,\n","    reward, discount, p_start_state, optimal_value, true_reward):\n","    \"\"\"\n","    Calculate the expected value difference, which is a proxy to how good a\n","    recovered reward function is.\n","    n_states: Number of states. int.\n","    n_actions: Number of actions. int.\n","    transition_probability: NumPy array mapping (state_i, action, state_k) to\n","        the probability of transitioning from state_i to state_k under action.\n","        Shape (N, A, N).\n","    reward: Reward vector mapping state int to reward. Shape (N,).\n","    discount: Discount factor. float.\n","    p_start_state: Probability vector with the ith component as the probability\n","        that the ith state is the start state. Shape (N,).\n","    optimal_value: Value vector for the ground reward with optimal policy.\n","        The ith component is the value of the ith state. Shape (N,).\n","    true_reward: True reward vector. Shape (N,).\n","    -> Expected value difference. float.\n","    \"\"\"\n","\n","    policy = find_policy(n_states, n_actions,\n","        transition_probability, reward, discount)\n","    value = value(policy.argmax(axis=1), n_states,\n","        transition_probability, true_reward, discount)\n","\n","    evd = optimal_value.dot(p_start_state) - value.dot(p_start_state)\n","    return evd"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"faJWkbFq3X6m"},"source":["## Value Iteration"]},{"cell_type":"code","metadata":{"id":"1Q0K0fuv1YeJ"},"source":["def value(policy, n_states, transition_probabilities, reward, discount,\n","                    threshold=1e-2):\n","    \"\"\"\n","    Find the value function associated with a policy.\n","    policy: List of action ints for each state.\n","    n_states: Number of states. int.\n","    transition_probabilities: Function taking (state, action, state) to\n","        transition probabilities.\n","    reward: Vector of rewards for each state.\n","    discount: MDP discount factor. float.\n","    threshold: Convergence threshold, default 1e-2. float.\n","    -> Array of values for each state\n","    \"\"\"\n","    v = np.zeros(n_states)\n","\n","    diff = float(\"inf\")\n","    while diff > threshold:\n","        diff = 0\n","        for s in range(n_states):\n","            vs = v[s]\n","            a = policy[s]\n","            v[s] = sum(transition_probabilities[s, a, k] *\n","                       (reward[k] + discount * v[k])\n","                       for k in range(n_states))\n","            diff = max(diff, abs(vs - v[s]))\n","\n","    return v\n","\n","def optimal_value(n_states, n_actions, transition_probabilities, reward,\n","                  discount, threshold=1e-2):\n","    \"\"\"\n","    Find the optimal value function.\n","    n_states: Number of states. int.\n","    n_actions: Number of actions. int.\n","    transition_probabilities: Function taking (state, action, state) to\n","        transition probabilities.\n","    reward: Vector of rewards for each state.\n","    discount: MDP discount factor. float.\n","    threshold: Convergence threshold, default 1e-2. float.\n","    -> Array of values for each state\n","    \"\"\"\n","\n","    v = np.zeros(n_states)\n","\n","    diff = float(\"inf\")\n","    while diff > threshold:\n","        diff = 0\n","        for s in range(n_states):\n","            max_v = float(\"-inf\")\n","            for a in range(n_actions):\n","                tp = transition_probabilities[s, a, :]\n","                max_v = max(max_v, np.dot(tp, reward + discount*v))\n","\n","            new_diff = abs(v[s] - max_v)\n","            if new_diff > diff:\n","                diff = new_diff\n","            v[s] = max_v\n","\n","    return v\n","\n","def find_policy(n_states, n_actions, transition_probabilities, reward, discount,\n","                threshold=1e-2, v=None, stochastic=True):\n","    \"\"\"\n","    Find the optimal policy.\n","    n_states: Number of states. int.\n","    n_actions: Number of actions. int.\n","    transition_probabilities: Function taking (state, action, state) to\n","        transition probabilities.\n","    reward: Vector of rewards for each state.\n","    discount: MDP discount factor. float.\n","    threshold: Convergence threshold, default 1e-2. float.\n","    v: Value function (if known). Default None.\n","    stochastic: Whether the policy should be stochastic. Default True.\n","    -> Action probabilities for each state or action int for each state\n","        (depending on stochasticity).\n","    \"\"\"\n","\n","    if v is None:\n","        v = optimal_value(n_states, n_actions, transition_probabilities, reward,\n","                          discount, threshold)\n","\n","    if stochastic:\n","        # Get Q using equation 9.2 from Ziebart's thesis.\n","        Q = np.zeros((n_states, n_actions))\n","        for i in range(n_states):\n","            for j in range(n_actions):\n","                p = transition_probabilities[i, j, :]\n","                Q[i, j] = p.dot(reward + discount*v)\n","        Q -= Q.max(axis=1).reshape((n_states, 1))  # For numerical stability.\n","        Q = np.exp(Q)/np.exp(Q).sum(axis=1).reshape((n_states, 1))\n","        return Q\n","\n","    def _policy(s):\n","        return max(range(n_actions),\n","                   key=lambda a: sum(transition_probabilities[s, a, k] *\n","                                     (reward[k] + discount * v[k])\n","                                     for k in range(n_states)))\n","    policy = np.array([_policy(s) for s in range(n_states)])\n","    return policy"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"i4j9kedB3hvK"},"source":["## MaxEnt Objectworld"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":281},"id":"xhX129Vd2RA0","executionInfo":{"status":"ok","timestamp":1636699027960,"user_tz":-330,"elapsed":26091,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}},"outputId":"c08b32ed-8877-4c17-8e5a-d6d8aeaa76dc"},"source":["def main(grid_size, discount, n_objects, n_colours, n_trajectories, epochs,\n","         learning_rate):\n","    \"\"\"\n","    Run maximum entropy inverse reinforcement learning on the objectworld MDP.\n","    Plots the reward function.\n","    grid_size: Grid size. int.\n","    discount: MDP discount factor. float.\n","    n_objects: Number of objects. int.\n","    n_colours: Number of colours. int.\n","    n_trajectories: Number of sampled trajectories. int.\n","    epochs: Gradient descent iterations. int.\n","    learning_rate: Gradient descent learning rate. float.\n","    \"\"\"\n","\n","    wind = 0.3\n","    trajectory_length = 8\n","\n","    ow = Objectworld(grid_size, n_objects, n_colours, wind,\n","                                 discount)\n","    ground_r = np.array([ow.reward(s) for s in range(ow.n_states)])\n","    policy = find_policy(ow.n_states, ow.n_actions, ow.transition_probability,\n","                         ground_r, ow.discount, stochastic=False)\n","    trajectories = ow.generate_trajectories(n_trajectories,\n","                                            trajectory_length,\n","                                            lambda s: policy[s])\n","    feature_matrix = ow.feature_matrix(discrete=False)\n","    r = irl(feature_matrix, ow.n_actions, discount,\n","        ow.transition_probability, trajectories, epochs, learning_rate)\n","\n","    plt.subplot(1, 2, 1)\n","    plt.pcolor(ground_r.reshape((grid_size, grid_size)))\n","    plt.colorbar()\n","    plt.title(\"Groundtruth reward\")\n","    plt.subplot(1, 2, 2)\n","    plt.pcolor(r.reshape((grid_size, grid_size)))\n","    plt.colorbar()\n","    plt.title(\"Recovered reward\")\n","    plt.show()\n","\n","if __name__ == '__main__':\n","    main(10, 0.9, 15, 2, 20, 50, 0.01)"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 4 Axes>"]},"metadata":{"needs_background":"light"}}]}]}