{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"T644011 | Incremental Update Retraining on Sobazaar dataset","provenance":[],"collapsed_sections":[],"mount_file_id":"1RRiC55q03vJg8vr-Fr9MbDVr8VHTN8XZ","authorship_tag":"ABX9TyPxMSvoYeGk2UmM+LPMtbOf"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"83c9f2d6bc6a42109250d55a39b4b3e7":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_6af9c2d14cd344408ebeb165aecd5812","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_14a3c2eab70348fd8336ae11dbbfcffd","IPY_MODEL_1f7c997368b34aedb5ecafc1ad6b1121","IPY_MODEL_712fb93cec584db29a18f17f48ce85dc"]}},"6af9c2d14cd344408ebeb165aecd5812":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"14a3c2eab70348fd8336ae11dbbfcffd":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_6149942fe6ad4e0ea119545de5a8d242","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"100%","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_f4349bb599ab413c9c087714eb7ba11f"}},"1f7c997368b34aedb5ecafc1ad6b1121":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_7b90eb42b83641b4890847c92356a8f6","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":842660,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":842660,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_8d3fb8b1ab5742ec983d41c822319729"}},"712fb93cec584db29a18f17f48ce85dc":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_27e8cfe72e7a4a93b1f2e1f705a551a5","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 842660/842660 [35:09&lt;00:00, 407.13it/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_74ce21ef75514a30a12822ebc5165352"}},"6149942fe6ad4e0ea119545de5a8d242":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"f4349bb599ab413c9c087714eb7ba11f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"7b90eb42b83641b4890847c92356a8f6":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"8d3fb8b1ab5742ec983d41c822319729":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"27e8cfe72e7a4a93b1f2e1f705a551a5":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"74ce21ef75514a30a12822ebc5165352":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"30c470ccdef34a2999ab74aad3503ba7":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_7ef1f03aed684fdbbc6602ec174cd2cd","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_45c6daf562174f79ad42975093c84210","IPY_MODEL_d3fc9033cc8f45c9a9bc114a6c590fd9","IPY_MODEL_53e9c4c383124944a89e833d7f0a56df"]}},"7ef1f03aed684fdbbc6602ec174cd2cd":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"45c6daf562174f79ad42975093c84210":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_e69ef6e531314efe9a9ee6dc58e5998b","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"100%","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_813103a929a84c78aa67e72419563cb7"}},"d3fc9033cc8f45c9a9bc114a6c590fd9":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_0fa28afc080a4e98a0f5ccd7d083e3b3","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":842660,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":842660,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_60fe4254494e4bc589c971c5e287ec88"}},"53e9c4c383124944a89e833d7f0a56df":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_bb7538a3fc0045c28b51edc7f7cbbc00","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 842660/842660 [00:03&lt;00:00, 266169.09it/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_d30f88908fe54f779659b41bd1e5ba6f"}},"e69ef6e531314efe9a9ee6dc58e5998b":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"813103a929a84c78aa67e72419563cb7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"0fa28afc080a4e98a0f5ccd7d083e3b3":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"60fe4254494e4bc589c971c5e287ec88":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"bb7538a3fc0045c28b51edc7f7cbbc00":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"d30f88908fe54f779659b41bd1e5ba6f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}}},"cells":[{"cell_type":"markdown","metadata":{"id":"1QG8dU-gzPyr"},"source":["# Incremental Update Retraining on Sobazaar dataset"]},{"cell_type":"markdown","metadata":{"id":"CxiWmRiFzT2X"},"source":["## Setup"]},{"cell_type":"markdown","metadata":{"id":"zVtJ4JTGH353"},"source":["### Git"]},{"cell_type":"code","metadata":{"id":"Z3qjPp055tXf","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1636552445257,"user_tz":-330,"elapsed":1642,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}},"outputId":"c950f0d4-0b6b-40dc-e267-25e09b0bf937"},"source":["import os\n","project_name = \"incremental-learning\"; branch = \"T644011\"; account = \"sparsh-ai\"\n","project_path = os.path.join('/content', branch)\n","\n","if not os.path.exists(project_path):\n","    !cp -r /content/drive/MyDrive/git_credentials/. ~\n","    !mkdir \"{project_path}\"\n","    %cd \"{project_path}\"\n","    !git init\n","    !git remote add origin https://github.com/\"{account}\"/\"{project_name}\".git\n","    !git pull origin \"{branch}\"\n","    !git checkout -b \"{branch}\"\n","else:\n","    %cd \"{project_path}\""],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/T644011\n","Initialized empty Git repository in /content/T644011/.git/\n","fatal: Couldn't find remote ref T644011\n","Switched to a new branch 'T644011'\n"]}]},{"cell_type":"code","metadata":{"id":"xoKGydGDIwSX"},"source":["%cd /content"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7lAKlgUD5tXi"},"source":["!cd /content/T644011 && git add .\n","!cd /content/T644011 && git commit -m 'commit'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"IqqZ6Do-uswE"},"source":["!cd /content/T644011 && git pull --rebase origin \"{branch}\"\n","!cd /content/T644011 && git push origin \"{branch}\""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"evKxFrICIpy_"},"source":["# !mv /content/ckpts .\n","# !mv /content/soba_4mth_2014_1neg_30seq_1.parquet.snappy ."],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BXJY8c9d4Xi5"},"source":["### Installations"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DctyNOSdx-7h","executionInfo":{"status":"ok","timestamp":1636547390860,"user_tz":-330,"elapsed":5352,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}},"outputId":"86a0cc18-1db8-443e-9f08-779d567b1414"},"source":["!pip install -q wget"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["  Building wheel for wget (setup.py) ... \u001b[?25l\u001b[?25hdone\n"]}]},{"cell_type":"markdown","metadata":{"id":"BK-ZCkf00xZt"},"source":["### Dataset"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5eJSFty70xW-","executionInfo":{"status":"ok","timestamp":1636547571751,"user_tz":-330,"elapsed":1335,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}},"outputId":"0bf7afa6-fa7d-4bec-989c-721d461fa242"},"source":["!wget -q --show-progress https://github.com/RecoHut-Datasets/sobazaar/raw/main/Data/Sobazaar-hashID.csv.gz"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\rSobazaar-hashID.csv   0%[                    ]       0  --.-KB/s               \rSobazaar-hashID.csv 100%[===================>]  17.11M  --.-KB/s    in 0.1s    \n"]}]},{"cell_type":"markdown","metadata":{"id":"GB_yDppW3_Yt"},"source":["### Imports"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3lFs8AyO1IWc","executionInfo":{"status":"ok","timestamp":1636548677008,"user_tz":-330,"elapsed":1010,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}},"outputId":"b2992811-db3f-4f5c-c9a9-abcbfbd6bb4c"},"source":["%tensorflow_version 1.x"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["TensorFlow 1.x selected.\n"]}]},{"cell_type":"code","metadata":{"id":"vrEmNkAAsQlM"},"source":["import numpy as np\n","from tqdm.notebook import tqdm\n","import sys\n","import wget\n","import os\n","import logging\n","import pandas as pd\n","from os import path as osp\n","from pathlib import Path\n","import random\n","import datetime\n","import time\n","import glob\n","\n","import bz2\n","import pickle\n","import _pickle as cPickle\n","\n","import tensorflow as tf"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NyxCtlrJ3_Ta"},"source":["### Params"]},{"cell_type":"code","metadata":{"id":"MXBwnUCD3_RD"},"source":["class Args:\n","    path_bronze = '/content'\n","    path_silver = '/content'\n","\n","args = Args()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"K5cAMUaO2H8W"},"source":["random.seed(1234)\n","np.random.seed(1234)\n","tf.set_random_seed(123)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Q40X4lHf4JHw"},"source":["### Logger"]},{"cell_type":"code","metadata":{"id":"cibwpV5L4JFb"},"source":["logging.basicConfig(stream=sys.stdout,\n","                    level = logging.DEBUG,\n","                    format='%(asctime)s [%(levelname)s] : %(message)s',\n","                    datefmt='%d-%b-%y %H:%M:%S')\n","\n","logger = logging.getLogger('Logger')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2M0-cN2ZzWE-"},"source":["## Modules"]},{"cell_type":"markdown","metadata":{"id":"qY9Y0q2sz1MS"},"source":["### Utils"]},{"cell_type":"code","metadata":{"id":"tH7lmOJbAOIf"},"source":["def save_pickle(data, title):\n"," with bz2.BZ2File(title + '.pbz2', 'w') as f: \n","    cPickle.dump(data, f)\n","\n","def load_pickle(path):\n","    data = bz2.BZ2File(path+'.pbz2', 'rb')\n","    data = cPickle.load(data)\n","    return data"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5lHX1pHU7fvN"},"source":["class BatchLoader:\n","    \"\"\"\n","    batch data loader by batch size\n","    return: [[users], [items], np.array(item_seqs_matrix), [seq_lens], [labels]] in batch iterator\n","    \"\"\"\n","\n","    def __init__(self, data_df, batch_size):\n","\n","        self.data_df = data_df.reset_index(drop=True)  # df ['userId', 'itemId', 'label']\n","        self.data_df['index'] = self.data_df.index\n","        self.data_df['batch'] = self.data_df['index'].apply(lambda x: int(x / batch_size) + 1)\n","        self.num_batches = self.data_df['batch'].max()\n","\n","    def get_batch(self, batch_id):\n","\n","        batch = self.data_df[self.data_df['batch'] == batch_id]\n","        users = batch['userId'].tolist()\n","        items = batch['itemId'].tolist()\n","        labels = batch['label'].tolist()\n","        seq_lens = batch['itemSeq'].apply(len).tolist()\n","\n","        item_seqs_matrix = np.zeros([len(batch), 30], np.int32)\n","\n","        i = 0\n","        for itemSeq in batch['itemSeq'].tolist():\n","            for j in range(len(itemSeq)):\n","                item_seqs_matrix[i][j] = itemSeq[j]  # convert list of itemSeq into a matrix with zero padding\n","            i += 1\n","\n","        return [users, items, item_seqs_matrix, seq_lens, labels]\n","\n","\n","def cal_roc_auc(scores, labels):\n","\n","    arr = sorted(zip(scores, labels), key=lambda d: d[0], reverse=True)\n","    pos, neg = 0., 0.\n","    for record in arr:\n","        if record[1] == 1.:\n","            pos += 1\n","        else:\n","            neg += 1\n","\n","    if pos == 0 or neg == 0:\n","        return None\n","\n","    fp, tp = 0., 0.\n","    xy_arr = []\n","    for record in arr:\n","        if record[1] == 1.:\n","            tp += 1\n","        else:\n","            fp += 1\n","        xy_arr.append([fp/neg, tp/pos])\n","\n","    auc = 0.\n","    prev_x = 0.\n","    prev_y = 0.\n","    for x, y in xy_arr:\n","        auc += ((x - prev_x) * (y + prev_y) / 2.)\n","        prev_x = x\n","        prev_y = y\n","    return auc\n","\n","\n","def cal_roc_gauc(users, scores, labels):\n","    # weighted sum of individual auc\n","    df = pd.DataFrame({'user': users,\n","                       'score': scores,\n","                       'label': labels})\n","\n","    df_gb = df.groupby('user').agg(lambda x: x.tolist())\n","\n","    auc_ls = []  # collect auc for all users\n","    user_imp_ls = []\n","\n","    for row in df_gb.itertuples():\n","        auc = cal_roc_auc(row.score, row.label)\n","        if auc is None:\n","            pass\n","        else:\n","            auc_ls.append(auc)\n","            user_imp = len(row.label)\n","            user_imp_ls.append(user_imp)\n","\n","    total_imp = sum(user_imp_ls)\n","    weighted_auc_ls = [auc * user_imp / total_imp for auc, user_imp in zip(auc_ls, user_imp_ls)]\n","\n","    return sum(weighted_auc_ls)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"SvNkS4mP7T7m"},"source":["def average_pooling(emb, seq_len):\n","    mask = tf.sequence_mask(seq_len, tf.shape(emb)[-2], dtype=tf.float32)  # [B, T]\n","    mask = tf.expand_dims(mask, -1)  # [B, T, 1]\n","    emb *= mask  # [B, T, H]\n","    sum_pool = tf.reduce_sum(emb, -2)  # [B, H]\n","    avg_pool = tf.div(sum_pool, tf.expand_dims(tf.cast(seq_len, tf.float32), -1) + 1e-8)  # [B, H]\n","    return avg_pool"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"WQjMHSPwCTgU"},"source":["def search_ckpt(search_alias, mode='last'):\n","    ckpt_ls = glob.glob(search_alias)\n","\n","    if mode == 'best logloss':\n","        metrics_ls = [float(ckpt.split('.ckpt')[0].split('_')[-1].split('TestLOGLOSS')[-1]) for ckpt in ckpt_ls]  # logloss\n","        selected_metrics_pos_ls = [i for i, x in enumerate(metrics_ls) if x == min(metrics_ls)]  # find all positions of the selected ckpts\n","    elif mode == 'best auc':\n","        metrics_ls = [float(ckpt.split('.ckpt')[0].split('_')[-2].split('TestAUC')[-1]) for ckpt in ckpt_ls]  # auc\n","        selected_metrics_pos_ls = [i for i, x in enumerate(metrics_ls) if x == max(metrics_ls)]  # find all positions of the selected ckpts\n","    else:  # mode == 'last'\n","        metrics_ls = [float(ckpt.split('.ckpt')[0].split('_')[-3].split('Epoch')[-1]) for ckpt in ckpt_ls]  # epoch no.\n","        selected_metrics_pos_ls = [i for i, x in enumerate(metrics_ls) if x == max(metrics_ls)]  # find all positions of the selected ckpts\n","    ckpt = ckpt_ls[max(selected_metrics_pos_ls)]  # get the full path of the last selected ckpt\n","\n","    ckpt = ckpt.split('.ckpt')[0]  # get the path name before .ckpt\n","    ckpt = ckpt + '.ckpt'  # get the path with .ckpt\n","    return ckpt"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"PguTj6gN2oj8"},"source":["### Dataset"]},{"cell_type":"code","metadata":{"id":"XQbXoMDO26pN"},"source":["def _gen_neg(num_items, pos_ls, num_neg):\n","    neg_ls = []\n","    for n in range(num_neg):  # generate num_neg\n","        neg = pos_ls[0]\n","        while neg in pos_ls:\n","            neg = random.randint(0, num_items - 1)\n","        neg_ls.append(neg)\n","    return neg_ls"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jnzORRSw2p_v"},"source":["def preprocess_sobazaar():\n","    # convert csv into pandas dataframe\n","    data_path = osp.join(args.path_bronze,'Sobazaar-hashID.csv.gz')\n","    save_path = osp.join(args.path_silver,'soba_4mth_2014_1neg_30seq_1.csv')\n","\n","    if not osp.exists(save_path):\n","        df = pd.read_csv(data_path)\n","        \n","        # preprocess\n","        df['date'] = df['Timestamp'].apply(lambda x: int(''.join(c for c in x.split('T')[0] if c.isdigit())))  # extract date and convert to int\n","        df['timestamp'] = df['Timestamp'].apply(lambda x: int(datetime.datetime.strptime(x.split('.')[0], '%Y-%m-%dT%H:%M:%S').timestamp()))\n","        df = df.drop(['Action', 'Timestamp'], axis=1)  # drop useless\n","        df.columns = ['itemId', 'userId', 'date', 'timestamp']  # rename\n","        df = df[['userId', 'itemId', 'date', 'timestamp']]  # switch columns\n","\n","        # remap id\n","        user_id = sorted(df['userId'].unique().tolist())  # sort column\n","        user_map = dict(zip(user_id, range(len(user_id))))  # create map, key is original id, value is mapped id starting from 0\n","        df['userId'] = df['userId'].map(lambda x: user_map[x])  # map key to value in df\n","\n","        item_id = sorted(df['itemId'].unique().tolist())  # sort column\n","        item_map = dict(zip(item_id, range(len(item_id))))  # create map, key is original id, value is mapped id starting from 0\n","        df['itemId'] = df['itemId'].map(lambda x: item_map[x])  # map key to value in df\n","\n","        logger.info('dataframe head - {}'.format(df.head(20)))\n","        logger.info('num_users: {}'.format(len(user_map)))  # 17126\n","        logger.info('num_items: {}'.format(len(item_map)))  # 24785\n","        logger.info('num_records: {}'.format(len(df)))  # 842660\n","\n","        # collect user history\n","        df_gb = df.groupby(['userId'])\n","        neg_lss = []\n","        num_neg = 1\n","        item_seqs = []\n","        max_len = 30\n","        count = 0\n","        for row in tqdm(df.itertuples(), total=df.shape[0]):\n","            user_df = df_gb.get_group(row.userId)\n","            user_history_df = user_df[user_df['timestamp'] <= row.timestamp].sort_values(['timestamp'], ascending=False).reset_index(drop=True)\n","            userHist = user_history_df['itemId'].unique().tolist()\n","            neg_lss.append(_gen_neg(len(item_map), userHist, num_neg))\n","\n","            user_history_df = user_history_df[user_history_df['timestamp'] < row.timestamp].sort_values(['timestamp'], ascending=False).reset_index(drop=True)\n","            item_seq_ls = user_history_df['itemId'][:max_len].tolist()\n","            itemSeq = '#'.join(str(i) for i in item_seq_ls)\n","            item_seqs.append(itemSeq)\n","\n","            count += 1\n","            if count % 100000 == 0:\n","                logger.info('done row {}'.format(count))\n","\n","        df['neg_itemId_ls'] = neg_lss\n","        df['itemSeq'] = item_seqs\n","\n","        users, itemseqs, items, labels, dates, timestamps = [], [], [], [], [], []\n","        for row in tqdm(df.itertuples(), total=df.shape[0]):\n","            users.append(row.userId)\n","            itemseqs.append(row.itemSeq)\n","            items.append(row.itemId)\n","            labels.append(1)  # positive samples have label 1\n","            dates.append(row.date)\n","            timestamps.append(row.timestamp)\n","            for j in range(num_neg):\n","                users.append(row.userId)\n","                itemseqs.append(row.itemSeq)\n","                items.append(row.neg_itemId_ls[j])\n","                labels.append(0)  # negative samples have label 0\n","                dates.append(row.date)\n","                timestamps.append(row.timestamp)\n","\n","        df = pd.DataFrame({'userId': users,\n","                        'itemSeq': itemseqs,\n","                        'itemId': items,\n","                        'label': labels,\n","                        'date': dates,\n","                        'timestamp': timestamps})\n","\n","        logger.info('dataframe head - {}'.format(df.head(20)))\n","        logger.info(len(df))  # 1685320\n","\n","        # save csv and pickle\n","        # ['userId', 'itemSeq', 'itemId', 'label', 'date', 'timestamp']\n","        df.to_csv(save_path, index=False)\n","        logger.info('processed data saved at {}'.format(save_path))\n","    else:\n","        logger.info('File already exists at {}'.format(save_path))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"x5we-S657T-E"},"source":["### Pretraining"]},{"cell_type":"code","metadata":{"id":"nxC7vY0i-DtE"},"source":["class EmbMLPnocate(object):\n","    \"\"\"\n","        Embedding&MLP base model without item category\n","    \"\"\"\n","    def __init__(self, hyperparams, train_config=None):\n","\n","        self.train_config = train_config\n","\n","        # create placeholder\n","        self.u = tf.placeholder(tf.int32, [None])  # [B]\n","        self.i = tf.placeholder(tf.int32, [None])  # [B]\n","        self.hist_i = tf.placeholder(tf.int32, [None, None])  # [B, T]\n","        self.hist_len = tf.placeholder(tf.int32, [None])  # [B]\n","        self.y = tf.placeholder(tf.float32, [None])  # [B]\n","        self.base_lr = tf.placeholder(tf.float32, [], name='base_lr')  # scalar\n","\n","        # -- create emb begin -------\n","        user_emb_w = tf.get_variable(\"user_emb_w\", [hyperparams['num_users'], hyperparams['user_embed_dim']])\n","        item_emb_w = tf.get_variable(\"item_emb_w\", [hyperparams['num_items'], hyperparams['item_embed_dim']])\n","        # -- create emb end -------\n","\n","        # -- create mlp begin ---\n","        concat_dim = hyperparams['user_embed_dim'] + hyperparams['item_embed_dim'] * 2\n","        with tf.variable_scope('fcn1'):\n","            fcn1_kernel = tf.get_variable(name='kernel', shape=[concat_dim, hyperparams['layers'][1]])\n","            fcn1_bias = tf.get_variable(name='bias', shape=[hyperparams['layers'][1]])\n","        with tf.variable_scope('fcn2'):\n","            fcn2_kernel = tf.get_variable(name='kernel', shape=[hyperparams['layers'][1], hyperparams['layers'][2]])\n","            fcn2_bias = tf.get_variable(name='bias', shape=[hyperparams['layers'][2]])\n","        with tf.variable_scope('fcn3'):\n","            fcn3_kernel = tf.get_variable(name='kernel', shape=[hyperparams['layers'][2], 1])\n","            fcn3_bias = tf.get_variable(name='bias', shape=[1])\n","        # -- create mlp end ---\n","\n","        # -- emb begin -------\n","        u_emb = tf.nn.embedding_lookup(user_emb_w, self.u)  # [B, H]\n","        i_emb = tf.nn.embedding_lookup(item_emb_w, self.i)  # [B, H]\n","        h_emb = tf.nn.embedding_lookup(item_emb_w, self.hist_i)  # [B, T, H]\n","        u_hist = average_pooling(h_emb, self.hist_len)  # [B, H]\n","        # -- emb end -------\n","\n","        # -- mlp begin -------\n","        fcn = tf.concat([u_emb, u_hist, i_emb], axis=-1)  # [B, H x 3]\n","        fcn_layer_1 = tf.nn.relu(tf.matmul(fcn, fcn1_kernel) + fcn1_bias)  # [B, l1]\n","        fcn_layer_2 = tf.nn.relu(tf.matmul(fcn_layer_1, fcn2_kernel) + fcn2_bias)  # [B, l2]\n","        fcn_layer_3 = tf.matmul(fcn_layer_2, fcn3_kernel) + fcn3_bias  # [B, 1]\n","        # -- mlp end -------\n","\n","        logits = tf.reshape(fcn_layer_3, [-1])  # [B]\n","        self.scores = tf.sigmoid(logits)  # [B]\n","\n","        # return same dimension as input tensors, let x = logits, z = labels, z * -log(sigmoid(x)) + (1 - z) * -log(1 - sigmoid(x))\n","        self.losses = tf.nn.sigmoid_cross_entropy_with_logits(logits=logits, labels=self.y)\n","        self.loss = tf.reduce_mean(self.losses)\n","\n","        # base_optimizer\n","        if train_config['base_optimizer'] == 'adam':\n","            base_optimizer = tf.train.AdamOptimizer(learning_rate=self.base_lr)\n","        elif train_config['base_optimizer'] == 'rmsprop':\n","            base_optimizer = tf.train.RMSPropOptimizer(learning_rate=self.base_lr)\n","        else:\n","            base_optimizer = tf.train.GradientDescentOptimizer(learning_rate=self.base_lr)\n","\n","        trainable_params = tf.trainable_variables()\n","\n","        # update base model\n","        update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n","        with tf.control_dependencies(update_ops):\n","            base_grads = tf.gradients(self.loss, trainable_params)  # return a list of gradients (A list of `sum(dy/dx)` for each x in `xs`)\n","            base_grads_tuples = zip(base_grads, trainable_params)\n","            self.train_base_op = base_optimizer.apply_gradients(base_grads_tuples)\n","\n","    def train_base(self, sess, batch):\n","        loss, _ = sess.run([self.loss, self.train_base_op], feed_dict={\n","            self.u: batch[0],\n","            self.i: batch[1],\n","            self.hist_i: batch[2],\n","            self.hist_len: batch[3],\n","            self.y: batch[4],\n","            self.base_lr: self.train_config['base_lr'],\n","        })\n","        return loss\n","\n","    def inference(self, sess, batch):\n","        scores, losses = sess.run([self.scores, self.losses], feed_dict={\n","            self.u: batch[0],\n","            self.i: batch[1],\n","            self.hist_i: batch[2],\n","            self.hist_len: batch[3],\n","            self.y: batch[4],\n","        })\n","        return scores, losses"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"rzeAyivB-DrL"},"source":["class Engine(object):\n","    \"\"\"\n","    Training epoch and test\n","    \"\"\"\n","\n","    def __init__(self, sess, model):\n","\n","        self.sess = sess\n","        self.model = model\n","\n","    def base_train_an_epoch(self, epoch_id, cur_set, train_config):\n","\n","        train_start_time = time.time()\n","\n","        if train_config['shuffle']:\n","            cur_set = cur_set.sample(frac=1)\n","\n","        cur_batch_loader = BatchLoader(cur_set, train_config['base_bs'])\n","\n","        base_loss_cur_sum = 0\n","\n","        for i in range(1, cur_batch_loader.num_batches + 1):\n","\n","            cur_batch = cur_batch_loader.get_batch(batch_id=i)\n","\n","            base_loss_cur = self.model.train_base(self.sess, cur_batch)  # sess.run\n","\n","            if (i - 1) % 100 == 0:\n","                print('[Epoch {} Batch {}] base_loss_cur {:.4f}, time elapsed {}'.format(epoch_id,\n","                                                                                         i,\n","                                                                                         base_loss_cur,\n","                                                                                         time.strftime('%H:%M:%S',\n","                                                                                                       time.gmtime(\n","                                                                                                           time.time() - train_start_time))))\n","\n","            base_loss_cur_sum += base_loss_cur\n","\n","        # epoch done, compute average loss\n","        base_loss_cur_avg = base_loss_cur_sum / cur_batch_loader.num_batches\n","\n","        return base_loss_cur_avg\n","\n","    def test(self, test_set, train_config):\n","\n","        test_batch_loader = BatchLoader(test_set, train_config['base_bs'])\n","\n","        scores, losses, labels = [], [], []\n","        for i in range(1, test_batch_loader.num_batches + 1):\n","            test_batch = test_batch_loader.get_batch(batch_id=i)\n","            batch_scores, batch_losses = self.model.inference(self.sess, test_batch)  # sees.run\n","            scores.extend(batch_scores.tolist())\n","            losses.extend(batch_losses.tolist())\n","            labels.extend(test_batch[4])\n","\n","        test_auc = cal_roc_auc(scores, labels)\n","        test_logloss = sum(losses) / len(losses)\n","\n","        return test_auc, test_logloss"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YECKrjGc-Dow"},"source":["def pretrain_sobazaar():\n","    # load data to df\n","    start_time = time.time()\n","\n","    load_path = osp.join(args.path_silver,'soba_4mth_2014_1neg_30seq_1.csv')\n","    data_df = pd.read_csv(load_path)\n","\n","    data_df['itemSeq'] = data_df['itemSeq'].fillna('')  # empty seq are NaN\n","    data_df['itemSeq'] = data_df['itemSeq'].apply(lambda x: [int(item) for item in x.split('#') if item != ''])\n","\n","    logger.info('Done loading data! time elapsed: {}'.format(time.strftime('%H:%M:%S', time.gmtime(time.time() - start_time))))\n","\n","    num_users = data_df['userId'].max() + 1\n","    num_items = data_df['itemId'].max() + 1\n","\n","    train_config = {'method': 'pretrain',\n","                    'dir_name': 'pretrain_train1-10_test11_10epoch',  # edit train test period range, number of epochs\n","                    'start_date': 20140901,  # overall train start date\n","                    'end_date': 20141231,  # overall train end date\n","                    'num_periods': 31,  # number of periods divided into\n","                    'train_start_period': 1,\n","                    'train_end_period': 10,\n","                    'test_period': 11,\n","                    'train_set_size': None,\n","                    'test_set_size': None,\n","\n","                    'base_optimizer': 'adam',  # base model optimizer: adam, rmsprop, sgd\n","                    'base_lr': None,  # base model learning rate\n","                    'base_bs': 256,  # base model batch size\n","                    'base_num_epochs': 10,  # base model number of epochs\n","                    'shuffle': True,  # whether to shuffle the dataset for each epoch\n","                    }\n","\n","    EmbMLPnocate_hyperparams = {'num_users': num_users,\n","                                'num_items': num_items,\n","                                'user_embed_dim': 8,\n","                                'item_embed_dim': 8,\n","                                'layers': [24, 12, 6, 1]  # input dim is user_embed_dim + item_embed_dim x 2\n","                                }\n","\n","    # sort train data into periods based on num_periods\n","    data_df = data_df[(data_df['date'] >= train_config['start_date']) & (data_df['date'] <= train_config['end_date'])]\n","    data_df = data_df.sort_values(['timestamp']).reset_index(drop=True)\n","    records_per_period = int(len(data_df) / train_config['num_periods'])\n","    data_df['index'] = data_df.index\n","    data_df['period'] = data_df['index'].apply(lambda x: int(x / records_per_period) + 1)\n","    data_df = data_df[data_df.period != train_config['num_periods'] + 1]  # delete last extra period\n","    period_df = data_df.groupby('period')['date'].agg(['count', 'min', 'max'])\n","    data_df = data_df.drop(['index', 'date', 'timestamp'], axis=1)\n","\n","    # build base model computation graph\n","    base_model = EmbMLPnocate(EmbMLPnocate_hyperparams, train_config=train_config)\n","\n","    # create session\n","    sess = tf.Session()\n","\n","    # create saver\n","    saver = tf.train.Saver(max_to_keep=80)\n","\n","    orig_dir_name = train_config['dir_name']\n","\n","    for base_lr in [1e-3]:\n","\n","        print('')\n","        print('base_lr', base_lr)\n","\n","        train_config['base_lr'] = base_lr\n","\n","        train_config['dir_name'] = orig_dir_name + '_' + str(base_lr)\n","        print('dir_name: ', train_config['dir_name'])\n","\n","        # create current and next set\n","        train_set = data_df[(data_df['period'] >= train_config['train_start_period']) &\n","                            (data_df['period'] <= train_config['train_end_period'])]\n","        test_set = data_df[data_df['period'] == train_config['test_period']]\n","        train_config['train_set_size'] = len(train_set)\n","        train_config['test_set_size'] = len(test_set)\n","        print('train set size', len(train_set), 'test set size', len(test_set))\n","\n","        # checkpoints directory\n","        checkpoints_dir = os.path.join('ckpts', train_config['dir_name'])\n","        if not os.path.exists(checkpoints_dir):\n","            os.makedirs(checkpoints_dir)\n","\n","        # write train_config to text file\n","        with open(os.path.join(checkpoints_dir, 'config.txt'), mode='w') as f:\n","            f.write('train_config: ' + str(train_config) + '\\n')\n","            f.write('\\n')\n","            f.write('EmbMLPnocate_hyperparams: ' + str(EmbMLPnocate_hyperparams) + '\\n')\n","\n","        sess.run([tf.global_variables_initializer(), tf.local_variables_initializer()])\n","\n","        # create an engine instance\n","        engine = Engine(sess, base_model)\n","\n","        train_start_time = time.time()\n","\n","        for epoch_id in range(1, train_config['base_num_epochs'] + 1):\n","\n","            print('Training Base Model Epoch {} Start!'.format(epoch_id))\n","\n","            base_loss_cur_avg = engine.base_train_an_epoch(epoch_id, train_set, train_config)\n","            print('Epoch {} Done! time elapsed: {}, base_loss_cur_avg {:.4f}'.format(\n","                epoch_id,\n","                time.strftime('%H:%M:%S', time.gmtime(time.time() - train_start_time)),\n","                base_loss_cur_avg))\n","\n","            test_auc, test_logloss = engine.test(test_set, train_config)\n","            print('test_auc {:.4f}, test_logloss {:.4f}'.format(\n","                test_auc,\n","                test_logloss))\n","            print('time elapsed {}'.format(time.strftime('%H:%M:%S', time.gmtime(time.time() - train_start_time))))\n","\n","            print('')\n","\n","            # save checkpoint\n","            checkpoint_alias = 'Epoch{}_TestAUC{:.4f}_TestLOGLOSS{:.4f}.ckpt'.format(\n","                epoch_id,\n","                test_auc,\n","                test_logloss)\n","            checkpoint_path = os.path.join(checkpoints_dir, checkpoint_alias)\n","            saver.save(sess, checkpoint_path)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hr7X8V0h-DmW"},"source":["### Incremental Update"]},{"cell_type":"code","metadata":{"id":"l41GttlhB0Ry"},"source":["def iu_sobazaar():\n","    # load data to df\n","    start_time = time.time()\n","\n","    load_path = osp.join(args.path_silver,'soba_4mth_2014_1neg_30seq_1.csv')\n","    data_df = pd.read_csv(load_path)\n","\n","    data_df['itemSeq'] = data_df['itemSeq'].fillna('')  # empty seq are NaN\n","    data_df['itemSeq'] = data_df['itemSeq'].apply(lambda x: [int(item) for item in x.split('#') if item != ''])\n","\n","    logger.info('Done loading data! time elapsed: {}'.format(time.strftime('%H:%M:%S', time.gmtime(time.time() - start_time))))\n","\n","    num_users = data_df['userId'].max() + 1\n","    num_items = data_df['itemId'].max() + 1\n","\n","    train_config = {'method': 'IU_by_period',\n","                    'dir_name': 'IU_train11-23_test24-30_1epoch',  # edit train test period, number of epochs\n","                    'pretrain_model': 'pretrain_train1-10_test11_10epoch_0.001',\n","                    'start_date': 20140901,  # overall train start date\n","                    'end_date': 20141231,  # overall train end date\n","                    'num_periods': 31,  # number of periods divided into\n","                    'train_start_period': 11,\n","                    'test_start_period': 24,\n","                    'cur_period': None,  # current incremental period\n","                    'next_period': None,  # next incremental period\n","                    'cur_set_size': None,  # current incremental dataset size\n","                    'next_set_size': None,  # next incremental dataset size\n","                    'period_alias': None,  # individual period directory alias to save ckpts\n","                    'restored_ckpt_mode': 'best auc',  # mode to search the checkpoint to restore, 'best auc', 'best gauc', 'last'\n","                    'restored_ckpt': None,  # configure in the for loop\n","\n","                    'base_optimizer': 'adam',  # base model optimizer: adam, rmsprop, sgd\n","                    'base_lr': None,  # base model learning rate\n","                    'base_bs': 256,  # base model batch size\n","                    'base_num_epochs': 1,  # base model number of epochs\n","                    'shuffle': True,  # whether to shuffle the dataset for each epoch\n","                    }\n","\n","    EmbMLPnocate_hyperparams = {'num_users': num_users,\n","                                'num_items': num_items,\n","                                'user_embed_dim': 8,\n","                                'item_embed_dim': 8,\n","                                'layers': [24, 12, 6, 1]  # input dim is user_embed_dim + item_embed_dim x 2\n","                                }\n","\n","    # sort train data into periods based on num_periods\n","    data_df = data_df[(data_df['date'] >= train_config['start_date']) & (data_df['date'] <= train_config['end_date'])]\n","    data_df = data_df.sort_values(['timestamp']).reset_index(drop=True)\n","    records_per_period = int(len(data_df) / train_config['num_periods'])\n","    data_df['index'] = data_df.index\n","    data_df['period'] = data_df['index'].apply(lambda x: int(x / records_per_period) + 1)\n","    data_df = data_df[data_df.period != train_config['num_periods'] + 1]  # delete last extra period\n","    period_df = data_df.groupby('period')['date'].agg(['count', 'min', 'max'])\n","    data_df = data_df.drop(['index', 'date', 'timestamp'], axis=1)\n","\n","    orig_dir_name = train_config['dir_name']\n","\n","    for base_lr in [1e-3]:\n","\n","        print('')\n","        print('base_lr', base_lr)\n","\n","        train_config['base_lr'] = base_lr\n","\n","        train_config['dir_name'] = orig_dir_name + '_' + str(base_lr)\n","        print('dir_name: ', train_config['dir_name'])\n","\n","        test_aucs = []\n","        test_loglosses = []\n","\n","        for i in range(train_config['train_start_period'], train_config['num_periods']):\n","\n","            # configure cur_period, next_period\n","            train_config['cur_period'] = i\n","            train_config['next_period'] = i + 1\n","            print('')\n","            print('current period: {}, next period: {}'.format(\n","                train_config['cur_period'],\n","                train_config['next_period']))\n","            print('')\n","\n","            # create current and next set\n","            cur_set = data_df[data_df['period'] == train_config['cur_period']]\n","            next_set = data_df[data_df['period'] == train_config['next_period']]\n","            train_config['cur_set_size'] = len(cur_set)\n","            train_config['next_set_size'] = len(next_set)\n","            print('current set size', len(cur_set), 'next set size', len(next_set))\n","\n","            train_config['period_alias'] = 'period' + str(i)\n","\n","            # checkpoints directory\n","            ckpts_dir = os.path.join('ckpts', train_config['dir_name'], train_config['period_alias'])\n","            if not os.path.exists(ckpts_dir):\n","                os.makedirs(ckpts_dir)\n","\n","            if i == train_config['train_start_period']:\n","                search_alias = os.path.join('ckpts', train_config['pretrain_model'], 'Epoch*')\n","                train_config['restored_ckpt'] = search_ckpt(search_alias, mode=train_config['restored_ckpt_mode'])\n","            else:\n","                prev_period_alias = 'period' + str(i - 1)\n","                search_alias = os.path.join('ckpts', train_config['dir_name'], prev_period_alias, 'Epoch*')\n","                train_config['restored_ckpt'] = search_ckpt(search_alias, mode=train_config['restored_ckpt_mode'])\n","            print('restored checkpoint: {}'.format(train_config['restored_ckpt']))\n","\n","            # write train_config to text file\n","            with open(os.path.join(ckpts_dir, 'config.txt'), mode='w') as f:\n","                f.write('train_config: ' + str(train_config) + '\\n')\n","                f.write('\\n')\n","                f.write('EmbMLPnocate_hyperparams: ' + str(EmbMLPnocate_hyperparams) + '\\n')\n","\n","            # build base model computation graph\n","            tf.reset_default_graph()\n","            base_model = EmbMLPnocate(EmbMLPnocate_hyperparams, train_config=train_config)\n","\n","            # create session\n","            with tf.Session() as sess:\n","                \n","                saver = tf.train.Saver()\n","                saver.restore(sess, train_config['restored_ckpt'])\n","                # create an engine instance with base_model\n","                engine = Engine(sess, base_model)\n","                train_start_time = time.time()\n","                max_auc = 0\n","                best_logloss = 0\n","\n","                for epoch_id in range(1, train_config['base_num_epochs'] + 1):\n","                    print('Training Base Model Epoch {} Start!'.format(epoch_id))\n","                    base_loss_cur_avg = engine.base_train_an_epoch(epoch_id, cur_set, train_config)\n","                    print('Epoch {} Done! time elapsed: {}, base_loss_cur_avg {:.4f}'.format(\n","                        epoch_id,\n","                        time.strftime('%H:%M:%S', time.gmtime(time.time() - train_start_time)),\n","                        base_loss_cur_avg))\n","                    cur_auc, cur_logloss = engine.test(cur_set, train_config)\n","                    next_auc, next_logloss = engine.test(next_set, train_config)\n","                    print('cur_auc {:.4f}, cur_logloss {:.4f}, next_auc {:.4f}, next_logloss {:.4f}'.format(\n","                        cur_auc,\n","                        cur_logloss,\n","                        next_auc,\n","                        next_logloss))\n","                    print('time elapsed {}'.format(time.strftime('%H:%M:%S', time.gmtime(time.time() - train_start_time))))\n","                    print('')\n","                    # save checkpoint\n","                    checkpoint_alias = 'Epoch{}_TestAUC{:.4f}_TestLOGLOSS{:.4f}.ckpt'.format(\n","                        epoch_id,\n","                        next_auc,\n","                        next_logloss)\n","                    checkpoint_path = os.path.join(ckpts_dir, checkpoint_alias)\n","                    saver.save(sess, checkpoint_path)\n","                    if next_auc > max_auc:\n","                        max_auc = next_auc\n","                        best_logloss = next_logloss\n","\n","                if i >= train_config['test_start_period']:\n","                    test_aucs.append(max_auc)\n","                    test_loglosses.append(best_logloss)\n","\n","            if i >= train_config['test_start_period']:\n","                average_auc = sum(test_aucs) / len(test_aucs)\n","                average_logloss = sum(test_loglosses) / len(test_loglosses)\n","                print('test aucs', test_aucs)\n","                print('average auc', average_auc)\n","                print('')\n","                print('test loglosses', test_loglosses)\n","                print('average logloss', average_logloss)\n","\n","                # write metrics to text file\n","                with open(os.path.join(ckpts_dir, 'test_metrics.txt'), mode='w') as f:\n","                    f.write('test_aucs: ' + str(test_aucs) + '\\n')\n","                    f.write('average_auc: ' + str(average_auc) + '\\n')\n","                    f.write('test_loglosses: ' + str(test_loglosses) + '\\n')\n","                    f.write('average_logloss: ' + str(average_logloss) + '\\n')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"P0nc1kyjzX9T"},"source":["## Jobs"]},{"cell_type":"code","metadata":{"id":"2y8mdDjds6dr","colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["83c9f2d6bc6a42109250d55a39b4b3e7","6af9c2d14cd344408ebeb165aecd5812","14a3c2eab70348fd8336ae11dbbfcffd","1f7c997368b34aedb5ecafc1ad6b1121","712fb93cec584db29a18f17f48ce85dc","6149942fe6ad4e0ea119545de5a8d242","f4349bb599ab413c9c087714eb7ba11f","7b90eb42b83641b4890847c92356a8f6","8d3fb8b1ab5742ec983d41c822319729","27e8cfe72e7a4a93b1f2e1f705a551a5","74ce21ef75514a30a12822ebc5165352","30c470ccdef34a2999ab74aad3503ba7","7ef1f03aed684fdbbc6602ec174cd2cd","45c6daf562174f79ad42975093c84210","d3fc9033cc8f45c9a9bc114a6c590fd9","53e9c4c383124944a89e833d7f0a56df","e69ef6e531314efe9a9ee6dc58e5998b","813103a929a84c78aa67e72419563cb7","0fa28afc080a4e98a0f5ccd7d083e3b3","60fe4254494e4bc589c971c5e287ec88","bb7538a3fc0045c28b51edc7f7cbbc00","d30f88908fe54f779659b41bd1e5ba6f"]},"executionInfo":{"status":"ok","timestamp":1636550822729,"user_tz":-330,"elapsed":2141725,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}},"outputId":"af2586b2-0874-4753-f5f5-7da803ca06aa"},"source":["logger.info('JOB START: PREPROCESS_DATASET')\n","preprocess_sobazaar()\n","logger.info('JOB END: PREPROCESS_DATASET')"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["10-Nov-21 12:51:22 [INFO] : JOB START: PREPROCESS_DATASET\n","10-Nov-21 12:51:37 [INFO] : dataframe head -     userId  itemId      date   timestamp\n","0     3192   14808  20140901  1409596736\n","1     3192   14808  20140901  1409596739\n","2     3192   18402  20140901  1409596746\n","3    13749    4020  20140901  1409596746\n","4     9380    5935  20140901  1409596753\n","5     3192   18402  20140901  1409596756\n","6    11381    1603  20140901  1409596756\n","7    11381   20984  20140901  1409596762\n","8     6149    6092  20140901  1409596766\n","9    11381    1204  20140901  1409596772\n","10   11381    1204  20140901  1409596774\n","11    9380    4576  20140901  1409596774\n","12   11381    1204  20140901  1409596774\n","13    3192   11779  20140901  1409596789\n","14   10241    9656  20140901  1409596795\n","15    9380   18238  20140901  1409596813\n","16   10241   16933  20140901  1409596816\n","17   13749    3756  20140901  1409596820\n","18   13749    3756  20140901  1409596822\n","19    6149   20603  20140901  1409596825\n","10-Nov-21 12:51:37 [INFO] : num_users: 17126\n","10-Nov-21 12:51:37 [INFO] : num_items: 24785\n","10-Nov-21 12:51:37 [INFO] : num_records: 842660\n"]},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"83c9f2d6bc6a42109250d55a39b4b3e7","version_minor":0,"version_major":2},"text/plain":["  0%|          | 0/842660 [00:00<?, ?it/s]"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["10-Nov-21 12:51:49 [INFO] : NumExpr defaulting to 2 threads.\n","10-Nov-21 12:55:52 [INFO] : done row 100000\n","10-Nov-21 12:59:54 [INFO] : done row 200000\n","10-Nov-21 13:03:55 [INFO] : done row 300000\n","10-Nov-21 13:07:59 [INFO] : done row 400000\n","10-Nov-21 13:12:17 [INFO] : done row 500000\n","10-Nov-21 13:16:34 [INFO] : done row 600000\n","10-Nov-21 13:20:47 [INFO] : done row 700000\n","10-Nov-21 13:25:00 [INFO] : done row 800000\n"]},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"30c470ccdef34a2999ab74aad3503ba7","version_minor":0,"version_major":2},"text/plain":["  0%|          | 0/842660 [00:00<?, ?it/s]"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["10-Nov-21 13:26:54 [INFO] : dataframe head -     userId  ...   timestamp\n","0     3192  ...  1409596736\n","1     3192  ...  1409596736\n","2     3192  ...  1409596739\n","3     3192  ...  1409596739\n","4     3192  ...  1409596746\n","5     3192  ...  1409596746\n","6    13749  ...  1409596746\n","7    13749  ...  1409596746\n","8     9380  ...  1409596753\n","9     9380  ...  1409596753\n","10    3192  ...  1409596756\n","11    3192  ...  1409596756\n","12   11381  ...  1409596756\n","13   11381  ...  1409596756\n","14   11381  ...  1409596762\n","15   11381  ...  1409596762\n","16    6149  ...  1409596766\n","17    6149  ...  1409596766\n","18   11381  ...  1409596772\n","19   11381  ...  1409596772\n","\n","[20 rows x 6 columns]\n","10-Nov-21 13:26:54 [INFO] : 1685320\n","10-Nov-21 13:27:03 [INFO] : processed data saved at /content/soba_4mth_2014_1neg_30seq_1.csv\n","10-Nov-21 13:27:03 [INFO] : JOB END: PREPROCESS_DATASET\n"]}]},{"cell_type":"code","metadata":{"id":"3ig3tPpB2Fx-","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1636551549040,"user_tz":-330,"elapsed":294427,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}},"outputId":"59104a12-2943-4b0d-da80-edb832a4ff2b"},"source":["logger.info('JOB START: EMBEDMLP_PRETRAINING')\n","pretrain_sobazaar()\n","logger.info('JOB END: EMBEDMLP_PRETRAINING')"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["10-Nov-21 13:34:15 [INFO] : JOB START: EMBEDMLP_PRETRAINING\n","10-Nov-21 13:34:36 [INFO] : Done loading data! time elapsed: 00:00:20\n","10-Nov-21 13:34:37 [WARNING] : From <ipython-input-20-dafab32e42db>:6: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Deprecated in favor of operator or tf.math.divide.\n","10-Nov-21 13:34:37 [WARNING] : From /tensorflow-1.15.2/python3.7/tensorflow_core/python/ops/nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use tf.where in 2.0, which has the same broadcast rule as np.where\n","\n","base_lr 0.001\n","dir_name:  pretrain_train1-10_test11_10epoch_0.001\n","train set size 543650 test set size 54365\n","Training Base Model Epoch 1 Start!\n","[Epoch 1 Batch 1] base_loss_cur 0.6939, time elapsed 00:00:00\n","[Epoch 1 Batch 101] base_loss_cur 0.6702, time elapsed 00:00:02\n","[Epoch 1 Batch 201] base_loss_cur 0.5350, time elapsed 00:00:03\n","[Epoch 1 Batch 301] base_loss_cur 0.4329, time elapsed 00:00:04\n","[Epoch 1 Batch 401] base_loss_cur 0.3993, time elapsed 00:00:05\n","[Epoch 1 Batch 501] base_loss_cur 0.3476, time elapsed 00:00:07\n","[Epoch 1 Batch 601] base_loss_cur 0.3895, time elapsed 00:00:08\n","[Epoch 1 Batch 701] base_loss_cur 0.3397, time elapsed 00:00:09\n","[Epoch 1 Batch 801] base_loss_cur 0.3630, time elapsed 00:00:11\n","[Epoch 1 Batch 901] base_loss_cur 0.4045, time elapsed 00:00:12\n","[Epoch 1 Batch 1001] base_loss_cur 0.3445, time elapsed 00:00:13\n","[Epoch 1 Batch 1101] base_loss_cur 0.3667, time elapsed 00:00:15\n","[Epoch 1 Batch 1201] base_loss_cur 0.3560, time elapsed 00:00:16\n","[Epoch 1 Batch 1301] base_loss_cur 0.3409, time elapsed 00:00:17\n","[Epoch 1 Batch 1401] base_loss_cur 0.3406, time elapsed 00:00:18\n","[Epoch 1 Batch 1501] base_loss_cur 0.4253, time elapsed 00:00:20\n","[Epoch 1 Batch 1601] base_loss_cur 0.3244, time elapsed 00:00:21\n","[Epoch 1 Batch 1701] base_loss_cur 0.3322, time elapsed 00:00:22\n","[Epoch 1 Batch 1801] base_loss_cur 0.3294, time elapsed 00:00:24\n","[Epoch 1 Batch 1901] base_loss_cur 0.4084, time elapsed 00:00:25\n","[Epoch 1 Batch 2001] base_loss_cur 0.3293, time elapsed 00:00:26\n","[Epoch 1 Batch 2101] base_loss_cur 0.3467, time elapsed 00:00:28\n","Epoch 1 Done! time elapsed: 00:00:28, base_loss_cur_avg 0.3935\n","test_auc 0.8527, test_logloss 0.5146\n","time elapsed 00:00:30\n","\n","Training Base Model Epoch 2 Start!\n","[Epoch 2 Batch 1] base_loss_cur 0.3220, time elapsed 00:00:00\n","[Epoch 2 Batch 101] base_loss_cur 0.3123, time elapsed 00:00:01\n","[Epoch 2 Batch 201] base_loss_cur 0.3561, time elapsed 00:00:02\n","[Epoch 2 Batch 301] base_loss_cur 0.3080, time elapsed 00:00:04\n","[Epoch 2 Batch 401] base_loss_cur 0.3184, time elapsed 00:00:05\n","[Epoch 2 Batch 501] base_loss_cur 0.3263, time elapsed 00:00:06\n","[Epoch 2 Batch 601] base_loss_cur 0.3755, time elapsed 00:00:07\n","[Epoch 2 Batch 701] base_loss_cur 0.3066, time elapsed 00:00:08\n","[Epoch 2 Batch 801] base_loss_cur 0.2869, time elapsed 00:00:09\n","[Epoch 2 Batch 901] base_loss_cur 0.3242, time elapsed 00:00:11\n","[Epoch 2 Batch 1001] base_loss_cur 0.2783, time elapsed 00:00:12\n","[Epoch 2 Batch 1101] base_loss_cur 0.3272, time elapsed 00:00:13\n","[Epoch 2 Batch 1201] base_loss_cur 0.3345, time elapsed 00:00:14\n","[Epoch 2 Batch 1301] base_loss_cur 0.3213, time elapsed 00:00:15\n","[Epoch 2 Batch 1401] base_loss_cur 0.3662, time elapsed 00:00:16\n","[Epoch 2 Batch 1501] base_loss_cur 0.3300, time elapsed 00:00:17\n","[Epoch 2 Batch 1601] base_loss_cur 0.3179, time elapsed 00:00:18\n","[Epoch 2 Batch 1701] base_loss_cur 0.3566, time elapsed 00:00:20\n","[Epoch 2 Batch 1801] base_loss_cur 0.3543, time elapsed 00:00:21\n","[Epoch 2 Batch 1901] base_loss_cur 0.3086, time elapsed 00:00:22\n","[Epoch 2 Batch 2001] base_loss_cur 0.2965, time elapsed 00:00:23\n","[Epoch 2 Batch 2101] base_loss_cur 0.2859, time elapsed 00:00:24\n","Epoch 2 Done! time elapsed: 00:00:55, base_loss_cur_avg 0.3184\n","test_auc 0.8534, test_logloss 0.5565\n","time elapsed 00:00:56\n","\n","Training Base Model Epoch 3 Start!\n","[Epoch 3 Batch 1] base_loss_cur 0.2299, time elapsed 00:00:00\n","[Epoch 3 Batch 101] base_loss_cur 0.2440, time elapsed 00:00:01\n","[Epoch 3 Batch 201] base_loss_cur 0.2800, time elapsed 00:00:02\n","[Epoch 3 Batch 301] base_loss_cur 0.3149, time elapsed 00:00:03\n","[Epoch 3 Batch 401] base_loss_cur 0.3340, time elapsed 00:00:05\n","[Epoch 3 Batch 501] base_loss_cur 0.3426, time elapsed 00:00:06\n","[Epoch 3 Batch 601] base_loss_cur 0.2689, time elapsed 00:00:07\n","[Epoch 3 Batch 701] base_loss_cur 0.3071, time elapsed 00:00:08\n","[Epoch 3 Batch 801] base_loss_cur 0.3395, time elapsed 00:00:09\n","[Epoch 3 Batch 901] base_loss_cur 0.3329, time elapsed 00:00:10\n","[Epoch 3 Batch 1001] base_loss_cur 0.2455, time elapsed 00:00:12\n","[Epoch 3 Batch 1101] base_loss_cur 0.3390, time elapsed 00:00:13\n","[Epoch 3 Batch 1201] base_loss_cur 0.2570, time elapsed 00:00:14\n","[Epoch 3 Batch 1301] base_loss_cur 0.2111, time elapsed 00:00:15\n","[Epoch 3 Batch 1401] base_loss_cur 0.2443, time elapsed 00:00:16\n","[Epoch 3 Batch 1501] base_loss_cur 0.2787, time elapsed 00:00:17\n","[Epoch 3 Batch 1601] base_loss_cur 0.3412, time elapsed 00:00:18\n","[Epoch 3 Batch 1701] base_loss_cur 0.3364, time elapsed 00:00:20\n","[Epoch 3 Batch 1801] base_loss_cur 0.3357, time elapsed 00:00:21\n","[Epoch 3 Batch 1901] base_loss_cur 0.2918, time elapsed 00:00:22\n","[Epoch 3 Batch 2001] base_loss_cur 0.2914, time elapsed 00:00:23\n","[Epoch 3 Batch 2101] base_loss_cur 0.2970, time elapsed 00:00:24\n","Epoch 3 Done! time elapsed: 00:01:21, base_loss_cur_avg 0.3065\n","test_auc 0.8515, test_logloss 0.5886\n","time elapsed 00:01:22\n","\n","Training Base Model Epoch 4 Start!\n","[Epoch 4 Batch 1] base_loss_cur 0.2537, time elapsed 00:00:00\n","[Epoch 4 Batch 101] base_loss_cur 0.2859, time elapsed 00:00:01\n","[Epoch 4 Batch 201] base_loss_cur 0.3337, time elapsed 00:00:02\n","[Epoch 4 Batch 301] base_loss_cur 0.3005, time elapsed 00:00:04\n","[Epoch 4 Batch 401] base_loss_cur 0.2767, time elapsed 00:00:05\n","[Epoch 4 Batch 501] base_loss_cur 0.2840, time elapsed 00:00:06\n","[Epoch 4 Batch 601] base_loss_cur 0.2689, time elapsed 00:00:07\n","[Epoch 4 Batch 701] base_loss_cur 0.2740, time elapsed 00:00:09\n","[Epoch 4 Batch 801] base_loss_cur 0.3485, time elapsed 00:00:10\n","[Epoch 4 Batch 901] base_loss_cur 0.2831, time elapsed 00:00:11\n","[Epoch 4 Batch 1001] base_loss_cur 0.2795, time elapsed 00:00:13\n","[Epoch 4 Batch 1101] base_loss_cur 0.2647, time elapsed 00:00:14\n","[Epoch 4 Batch 1201] base_loss_cur 0.3016, time elapsed 00:00:16\n","[Epoch 4 Batch 1301] base_loss_cur 0.2658, time elapsed 00:00:17\n","[Epoch 4 Batch 1401] base_loss_cur 0.2967, time elapsed 00:00:18\n","[Epoch 4 Batch 1501] base_loss_cur 0.2713, time elapsed 00:00:19\n","[Epoch 4 Batch 1601] base_loss_cur 0.2784, time elapsed 00:00:20\n","[Epoch 4 Batch 1701] base_loss_cur 0.2673, time elapsed 00:00:22\n","[Epoch 4 Batch 1801] base_loss_cur 0.2952, time elapsed 00:00:23\n","[Epoch 4 Batch 1901] base_loss_cur 0.2325, time elapsed 00:00:24\n","[Epoch 4 Batch 2001] base_loss_cur 0.2741, time elapsed 00:00:25\n","[Epoch 4 Batch 2101] base_loss_cur 0.3014, time elapsed 00:00:27\n","Epoch 4 Done! time elapsed: 00:01:50, base_loss_cur_avg 0.2962\n","test_auc 0.8516, test_logloss 0.6333\n","time elapsed 00:01:51\n","\n","Training Base Model Epoch 5 Start!\n","[Epoch 5 Batch 1] base_loss_cur 0.2729, time elapsed 00:00:00\n","[Epoch 5 Batch 101] base_loss_cur 0.2985, time elapsed 00:00:01\n","[Epoch 5 Batch 201] base_loss_cur 0.2536, time elapsed 00:00:03\n","[Epoch 5 Batch 301] base_loss_cur 0.2560, time elapsed 00:00:04\n","[Epoch 5 Batch 401] base_loss_cur 0.3309, time elapsed 00:00:05\n","[Epoch 5 Batch 501] base_loss_cur 0.3220, time elapsed 00:00:06\n","[Epoch 5 Batch 601] base_loss_cur 0.2535, time elapsed 00:00:07\n","[Epoch 5 Batch 701] base_loss_cur 0.2822, time elapsed 00:00:09\n","[Epoch 5 Batch 801] base_loss_cur 0.3051, time elapsed 00:00:10\n","[Epoch 5 Batch 901] base_loss_cur 0.2465, time elapsed 00:00:11\n","[Epoch 5 Batch 1001] base_loss_cur 0.2719, time elapsed 00:00:12\n","[Epoch 5 Batch 1101] base_loss_cur 0.2592, time elapsed 00:00:14\n","[Epoch 5 Batch 1201] base_loss_cur 0.3098, time elapsed 00:00:15\n","[Epoch 5 Batch 1301] base_loss_cur 0.3655, time elapsed 00:00:16\n","[Epoch 5 Batch 1401] base_loss_cur 0.2837, time elapsed 00:00:17\n","[Epoch 5 Batch 1501] base_loss_cur 0.2567, time elapsed 00:00:19\n","[Epoch 5 Batch 1601] base_loss_cur 0.3014, time elapsed 00:00:20\n","[Epoch 5 Batch 1701] base_loss_cur 0.2919, time elapsed 00:00:21\n","[Epoch 5 Batch 1801] base_loss_cur 0.2702, time elapsed 00:00:22\n","[Epoch 5 Batch 1901] base_loss_cur 0.3417, time elapsed 00:00:24\n","[Epoch 5 Batch 2001] base_loss_cur 0.3222, time elapsed 00:00:25\n","[Epoch 5 Batch 2101] base_loss_cur 0.2494, time elapsed 00:00:26\n","Epoch 5 Done! time elapsed: 00:02:18, base_loss_cur_avg 0.2860\n","test_auc 0.8483, test_logloss 0.7138\n","time elapsed 00:02:19\n","\n","Training Base Model Epoch 6 Start!\n","[Epoch 6 Batch 1] base_loss_cur 0.3201, time elapsed 00:00:00\n","[Epoch 6 Batch 101] base_loss_cur 0.2759, time elapsed 00:00:01\n","[Epoch 6 Batch 201] base_loss_cur 0.2802, time elapsed 00:00:02\n","[Epoch 6 Batch 301] base_loss_cur 0.2631, time elapsed 00:00:04\n","[Epoch 6 Batch 401] base_loss_cur 0.2509, time elapsed 00:00:05\n","[Epoch 6 Batch 501] base_loss_cur 0.2808, time elapsed 00:00:06\n","[Epoch 6 Batch 601] base_loss_cur 0.3126, time elapsed 00:00:07\n","[Epoch 6 Batch 701] base_loss_cur 0.3052, time elapsed 00:00:08\n","[Epoch 6 Batch 801] base_loss_cur 0.2341, time elapsed 00:00:10\n","[Epoch 6 Batch 901] base_loss_cur 0.2643, time elapsed 00:00:11\n","[Epoch 6 Batch 1001] base_loss_cur 0.3002, time elapsed 00:00:12\n","[Epoch 6 Batch 1101] base_loss_cur 0.2327, time elapsed 00:00:14\n","[Epoch 6 Batch 1201] base_loss_cur 0.2927, time elapsed 00:00:15\n","[Epoch 6 Batch 1301] base_loss_cur 0.2460, time elapsed 00:00:16\n","[Epoch 6 Batch 1401] base_loss_cur 0.2684, time elapsed 00:00:18\n","[Epoch 6 Batch 1501] base_loss_cur 0.2466, time elapsed 00:00:19\n","[Epoch 6 Batch 1601] base_loss_cur 0.2865, time elapsed 00:00:20\n","[Epoch 6 Batch 1701] base_loss_cur 0.2705, time elapsed 00:00:21\n","[Epoch 6 Batch 1801] base_loss_cur 0.2579, time elapsed 00:00:23\n","[Epoch 6 Batch 1901] base_loss_cur 0.3206, time elapsed 00:00:24\n","[Epoch 6 Batch 2001] base_loss_cur 0.2727, time elapsed 00:00:25\n","[Epoch 6 Batch 2101] base_loss_cur 0.2345, time elapsed 00:00:27\n","Epoch 6 Done! time elapsed: 00:02:47, base_loss_cur_avg 0.2759\n","test_auc 0.8477, test_logloss 0.7586\n","time elapsed 00:02:48\n","\n","Training Base Model Epoch 7 Start!\n","[Epoch 7 Batch 1] base_loss_cur 0.2357, time elapsed 00:00:00\n","[Epoch 7 Batch 101] base_loss_cur 0.1882, time elapsed 00:00:01\n","[Epoch 7 Batch 201] base_loss_cur 0.2704, time elapsed 00:00:02\n","[Epoch 7 Batch 301] base_loss_cur 0.2365, time elapsed 00:00:03\n","[Epoch 7 Batch 401] base_loss_cur 0.2665, time elapsed 00:00:05\n","[Epoch 7 Batch 501] base_loss_cur 0.2462, time elapsed 00:00:06\n","[Epoch 7 Batch 601] base_loss_cur 0.2297, time elapsed 00:00:07\n","[Epoch 7 Batch 701] base_loss_cur 0.2262, time elapsed 00:00:08\n","[Epoch 7 Batch 801] base_loss_cur 0.2320, time elapsed 00:00:09\n","[Epoch 7 Batch 901] base_loss_cur 0.2631, time elapsed 00:00:10\n","[Epoch 7 Batch 1001] base_loss_cur 0.3064, time elapsed 00:00:11\n","[Epoch 7 Batch 1101] base_loss_cur 0.2206, time elapsed 00:00:13\n","[Epoch 7 Batch 1201] base_loss_cur 0.2566, time elapsed 00:00:14\n","[Epoch 7 Batch 1301] base_loss_cur 0.2444, time elapsed 00:00:15\n","[Epoch 7 Batch 1401] base_loss_cur 0.2925, time elapsed 00:00:16\n","[Epoch 7 Batch 1501] base_loss_cur 0.2534, time elapsed 00:00:17\n","[Epoch 7 Batch 1601] base_loss_cur 0.3372, time elapsed 00:00:18\n","[Epoch 7 Batch 1701] base_loss_cur 0.2865, time elapsed 00:00:19\n","[Epoch 7 Batch 1801] base_loss_cur 0.2217, time elapsed 00:00:20\n","[Epoch 7 Batch 1901] base_loss_cur 0.2254, time elapsed 00:00:21\n","[Epoch 7 Batch 2001] base_loss_cur 0.3182, time elapsed 00:00:23\n","[Epoch 7 Batch 2101] base_loss_cur 0.2166, time elapsed 00:00:24\n","Epoch 7 Done! time elapsed: 00:03:12, base_loss_cur_avg 0.2656\n","test_auc 0.8443, test_logloss 0.8582\n","time elapsed 00:03:14\n","\n","Training Base Model Epoch 8 Start!\n","[Epoch 8 Batch 1] base_loss_cur 0.2353, time elapsed 00:00:00\n","[Epoch 8 Batch 101] base_loss_cur 0.2786, time elapsed 00:00:01\n","[Epoch 8 Batch 201] base_loss_cur 0.2320, time elapsed 00:00:02\n","[Epoch 8 Batch 301] base_loss_cur 0.2720, time elapsed 00:00:03\n","[Epoch 8 Batch 401] base_loss_cur 0.2563, time elapsed 00:00:04\n","[Epoch 8 Batch 501] base_loss_cur 0.2957, time elapsed 00:00:06\n","[Epoch 8 Batch 601] base_loss_cur 0.2256, time elapsed 00:00:07\n","[Epoch 8 Batch 701] base_loss_cur 0.1951, time elapsed 00:00:08\n","[Epoch 8 Batch 801] base_loss_cur 0.2153, time elapsed 00:00:09\n","[Epoch 8 Batch 901] base_loss_cur 0.2439, time elapsed 00:00:10\n","[Epoch 8 Batch 1001] base_loss_cur 0.2639, time elapsed 00:00:11\n","[Epoch 8 Batch 1101] base_loss_cur 0.2707, time elapsed 00:00:12\n","[Epoch 8 Batch 1201] base_loss_cur 0.2013, time elapsed 00:00:13\n","[Epoch 8 Batch 1301] base_loss_cur 0.2723, time elapsed 00:00:14\n","[Epoch 8 Batch 1401] base_loss_cur 0.2566, time elapsed 00:00:16\n","[Epoch 8 Batch 1501] base_loss_cur 0.2531, time elapsed 00:00:17\n","[Epoch 8 Batch 1601] base_loss_cur 0.2809, time elapsed 00:00:18\n","[Epoch 8 Batch 1701] base_loss_cur 0.2876, time elapsed 00:00:19\n","[Epoch 8 Batch 1801] base_loss_cur 0.2593, time elapsed 00:00:20\n","[Epoch 8 Batch 1901] base_loss_cur 0.2148, time elapsed 00:00:21\n","[Epoch 8 Batch 2001] base_loss_cur 0.2814, time elapsed 00:00:22\n","[Epoch 8 Batch 2101] base_loss_cur 0.2919, time elapsed 00:00:23\n","Epoch 8 Done! time elapsed: 00:03:38, base_loss_cur_avg 0.2550\n","test_auc 0.8400, test_logloss 0.9319\n","time elapsed 00:03:39\n","\n","Training Base Model Epoch 9 Start!\n","[Epoch 9 Batch 1] base_loss_cur 0.2046, time elapsed 00:00:00\n","[Epoch 9 Batch 101] base_loss_cur 0.2312, time elapsed 00:00:01\n","[Epoch 9 Batch 201] base_loss_cur 0.2291, time elapsed 00:00:02\n","[Epoch 9 Batch 301] base_loss_cur 0.1805, time elapsed 00:00:03\n","[Epoch 9 Batch 401] base_loss_cur 0.2452, time elapsed 00:00:04\n","[Epoch 9 Batch 501] base_loss_cur 0.2902, time elapsed 00:00:06\n","[Epoch 9 Batch 601] base_loss_cur 0.2315, time elapsed 00:00:07\n","[Epoch 9 Batch 701] base_loss_cur 0.1601, time elapsed 00:00:08\n","[Epoch 9 Batch 801] base_loss_cur 0.2103, time elapsed 00:00:09\n","[Epoch 9 Batch 901] base_loss_cur 0.2347, time elapsed 00:00:10\n","[Epoch 9 Batch 1001] base_loss_cur 0.2099, time elapsed 00:00:11\n","[Epoch 9 Batch 1101] base_loss_cur 0.2622, time elapsed 00:00:12\n","[Epoch 9 Batch 1201] base_loss_cur 0.2266, time elapsed 00:00:13\n","[Epoch 9 Batch 1301] base_loss_cur 0.2768, time elapsed 00:00:14\n","[Epoch 9 Batch 1401] base_loss_cur 0.3167, time elapsed 00:00:15\n","[Epoch 9 Batch 1501] base_loss_cur 0.2752, time elapsed 00:00:17\n","[Epoch 9 Batch 1601] base_loss_cur 0.2620, time elapsed 00:00:18\n","[Epoch 9 Batch 1701] base_loss_cur 0.2315, time elapsed 00:00:19\n","[Epoch 9 Batch 1801] base_loss_cur 0.3015, time elapsed 00:00:20\n","[Epoch 9 Batch 1901] base_loss_cur 0.2331, time elapsed 00:00:21\n","[Epoch 9 Batch 2001] base_loss_cur 0.2653, time elapsed 00:00:22\n","[Epoch 9 Batch 2101] base_loss_cur 0.2663, time elapsed 00:00:23\n","Epoch 9 Done! time elapsed: 00:04:03, base_loss_cur_avg 0.2453\n","test_auc 0.8366, test_logloss 0.9821\n","time elapsed 00:04:04\n","\n","Training Base Model Epoch 10 Start!\n","[Epoch 10 Batch 1] base_loss_cur 0.1947, time elapsed 00:00:00\n","[Epoch 10 Batch 101] base_loss_cur 0.2018, time elapsed 00:00:01\n","[Epoch 10 Batch 201] base_loss_cur 0.2535, time elapsed 00:00:02\n","[Epoch 10 Batch 301] base_loss_cur 0.2618, time elapsed 00:00:03\n","[Epoch 10 Batch 401] base_loss_cur 0.1846, time elapsed 00:00:04\n","[Epoch 10 Batch 501] base_loss_cur 0.2523, time elapsed 00:00:06\n","[Epoch 10 Batch 601] base_loss_cur 0.2986, time elapsed 00:00:07\n","[Epoch 10 Batch 701] base_loss_cur 0.2269, time elapsed 00:00:08\n","[Epoch 10 Batch 801] base_loss_cur 0.1886, time elapsed 00:00:09\n","[Epoch 10 Batch 901] base_loss_cur 0.2367, time elapsed 00:00:10\n","[Epoch 10 Batch 1001] base_loss_cur 0.2861, time elapsed 00:00:11\n","[Epoch 10 Batch 1101] base_loss_cur 0.2059, time elapsed 00:00:12\n","[Epoch 10 Batch 1201] base_loss_cur 0.2505, time elapsed 00:00:14\n","[Epoch 10 Batch 1301] base_loss_cur 0.1815, time elapsed 00:00:15\n","[Epoch 10 Batch 1401] base_loss_cur 0.2273, time elapsed 00:00:16\n","[Epoch 10 Batch 1501] base_loss_cur 0.2627, time elapsed 00:00:17\n","[Epoch 10 Batch 1601] base_loss_cur 0.2774, time elapsed 00:00:18\n","[Epoch 10 Batch 1701] base_loss_cur 0.2579, time elapsed 00:00:19\n","[Epoch 10 Batch 1801] base_loss_cur 0.2022, time elapsed 00:00:20\n","[Epoch 10 Batch 1901] base_loss_cur 0.3407, time elapsed 00:00:22\n","[Epoch 10 Batch 2001] base_loss_cur 0.2372, time elapsed 00:00:23\n","[Epoch 10 Batch 2101] base_loss_cur 0.2344, time elapsed 00:00:24\n","Epoch 10 Done! time elapsed: 00:04:29, base_loss_cur_avg 0.2362\n","test_auc 0.8321, test_logloss 1.0884\n","time elapsed 00:04:30\n","\n","10-Nov-21 13:39:09 [INFO] : JOB END: EMBEDMLP_PRETRAINING\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Y3quOeuqFHwF","executionInfo":{"status":"ok","timestamp":1636552312547,"user_tz":-330,"elapsed":146736,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}},"outputId":"81947f97-65ce-4aa0-e9de-1e12d5e5645a"},"source":["logger.info('JOB START: INCREMENTAL_UPDATE')\n","iu_sobazaar()\n","logger.info('JOB END: INCREMENTAL_UPDATE')"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["10-Nov-21 13:49:27 [INFO] : JOB START: INCREMENTAL_UPDATE\n","10-Nov-21 13:49:49 [INFO] : Done loading data! time elapsed: 00:00:22\n","\n","base_lr 0.001\n","dir_name:  IU_train11-23_test24-30_1epoch_0.001\n","\n","current period: 11, next period: 12\n","\n","current set size 54365 next set size 54365\n","restored checkpoint: ckpts/pretrain_train1-10_test11_10epoch_0.001/Epoch2_TestAUC0.8534_TestLOGLOSS0.5565.ckpt\n","10-Nov-21 13:49:51 [INFO] : Restoring parameters from ckpts/pretrain_train1-10_test11_10epoch_0.001/Epoch2_TestAUC0.8534_TestLOGLOSS0.5565.ckpt\n","Training Base Model Epoch 1 Start!\n","[Epoch 1 Batch 1] base_loss_cur 0.5439, time elapsed 00:00:00\n","[Epoch 1 Batch 101] base_loss_cur 0.3746, time elapsed 00:00:01\n","[Epoch 1 Batch 201] base_loss_cur 0.4302, time elapsed 00:00:02\n","Epoch 1 Done! time elapsed: 00:00:02, base_loss_cur_avg 0.4117\n","cur_auc 0.9282, cur_logloss 0.3278, next_auc 0.8038, next_logloss 0.6622\n","time elapsed 00:00:06\n","\n","\n","current period: 12, next period: 13\n","\n","current set size 54365 next set size 54365\n","restored checkpoint: ckpts/IU_train11-23_test24-30_1epoch_0.001/period11/Epoch1_TestAUC0.8038_TestLOGLOSS0.6622.ckpt\n","10-Nov-21 13:49:58 [INFO] : Restoring parameters from ckpts/IU_train11-23_test24-30_1epoch_0.001/period11/Epoch1_TestAUC0.8038_TestLOGLOSS0.6622.ckpt\n","Training Base Model Epoch 1 Start!\n","[Epoch 1 Batch 1] base_loss_cur 0.6812, time elapsed 00:00:00\n","[Epoch 1 Batch 101] base_loss_cur 0.4623, time elapsed 00:00:01\n","[Epoch 1 Batch 201] base_loss_cur 0.4474, time elapsed 00:00:02\n","Epoch 1 Done! time elapsed: 00:00:02, base_loss_cur_avg 0.4554\n","cur_auc 0.9214, cur_logloss 0.3502, next_auc 0.8647, next_logloss 0.4722\n","time elapsed 00:00:05\n","\n","\n","current period: 13, next period: 14\n","\n","current set size 54365 next set size 54365\n","restored checkpoint: ckpts/IU_train11-23_test24-30_1epoch_0.001/period12/Epoch1_TestAUC0.8647_TestLOGLOSS0.4722.ckpt\n","10-Nov-21 13:50:04 [INFO] : Restoring parameters from ckpts/IU_train11-23_test24-30_1epoch_0.001/period12/Epoch1_TestAUC0.8647_TestLOGLOSS0.4722.ckpt\n","Training Base Model Epoch 1 Start!\n","[Epoch 1 Batch 1] base_loss_cur 0.5339, time elapsed 00:00:00\n","[Epoch 1 Batch 101] base_loss_cur 0.4281, time elapsed 00:00:01\n","[Epoch 1 Batch 201] base_loss_cur 0.2722, time elapsed 00:00:02\n","Epoch 1 Done! time elapsed: 00:00:02, base_loss_cur_avg 0.3874\n","cur_auc 0.9531, cur_logloss 0.2829, next_auc 0.8722, next_logloss 0.4688\n","time elapsed 00:00:05\n","\n","\n","current period: 14, next period: 15\n","\n","current set size 54365 next set size 54365\n","restored checkpoint: ckpts/IU_train11-23_test24-30_1epoch_0.001/period13/Epoch1_TestAUC0.8722_TestLOGLOSS0.4688.ckpt\n","10-Nov-21 13:50:10 [INFO] : Restoring parameters from ckpts/IU_train11-23_test24-30_1epoch_0.001/period13/Epoch1_TestAUC0.8722_TestLOGLOSS0.4688.ckpt\n","Training Base Model Epoch 1 Start!\n","[Epoch 1 Batch 1] base_loss_cur 0.4622, time elapsed 00:00:00\n","[Epoch 1 Batch 101] base_loss_cur 0.3861, time elapsed 00:00:01\n","[Epoch 1 Batch 201] base_loss_cur 0.3179, time elapsed 00:00:02\n","Epoch 1 Done! time elapsed: 00:00:02, base_loss_cur_avg 0.3641\n","cur_auc 0.9610, cur_logloss 0.2516, next_auc 0.8945, next_logloss 0.4223\n","time elapsed 00:00:05\n","\n","\n","current period: 15, next period: 16\n","\n","current set size 54365 next set size 54365\n","restored checkpoint: ckpts/IU_train11-23_test24-30_1epoch_0.001/period14/Epoch1_TestAUC0.8945_TestLOGLOSS0.4223.ckpt\n","10-Nov-21 13:50:16 [INFO] : Restoring parameters from ckpts/IU_train11-23_test24-30_1epoch_0.001/period14/Epoch1_TestAUC0.8945_TestLOGLOSS0.4223.ckpt\n","Training Base Model Epoch 1 Start!\n","[Epoch 1 Batch 1] base_loss_cur 0.4343, time elapsed 00:00:00\n","[Epoch 1 Batch 101] base_loss_cur 0.3482, time elapsed 00:00:01\n","[Epoch 1 Batch 201] base_loss_cur 0.3341, time elapsed 00:00:02\n","Epoch 1 Done! time elapsed: 00:00:02, base_loss_cur_avg 0.3551\n","cur_auc 0.9576, cur_logloss 0.2604, next_auc 0.7871, next_logloss 0.7078\n","time elapsed 00:00:05\n","\n","\n","current period: 16, next period: 17\n","\n","current set size 54365 next set size 54365\n","restored checkpoint: ckpts/IU_train11-23_test24-30_1epoch_0.001/period15/Epoch1_TestAUC0.7871_TestLOGLOSS0.7078.ckpt\n","10-Nov-21 13:50:22 [INFO] : Restoring parameters from ckpts/IU_train11-23_test24-30_1epoch_0.001/period15/Epoch1_TestAUC0.7871_TestLOGLOSS0.7078.ckpt\n","Training Base Model Epoch 1 Start!\n","[Epoch 1 Batch 1] base_loss_cur 0.7339, time elapsed 00:00:00\n","[Epoch 1 Batch 101] base_loss_cur 0.4305, time elapsed 00:00:01\n","[Epoch 1 Batch 201] base_loss_cur 0.4319, time elapsed 00:00:02\n","Epoch 1 Done! time elapsed: 00:00:02, base_loss_cur_avg 0.4493\n","cur_auc 0.9448, cur_logloss 0.3071, next_auc 0.8295, next_logloss 0.5270\n","time elapsed 00:00:05\n","\n","\n","current period: 17, next period: 18\n","\n","current set size 54365 next set size 54365\n","restored checkpoint: ckpts/IU_train11-23_test24-30_1epoch_0.001/period16/Epoch1_TestAUC0.8295_TestLOGLOSS0.5270.ckpt\n","10-Nov-21 13:50:27 [INFO] : Restoring parameters from ckpts/IU_train11-23_test24-30_1epoch_0.001/period16/Epoch1_TestAUC0.8295_TestLOGLOSS0.5270.ckpt\n","Training Base Model Epoch 1 Start!\n","[Epoch 1 Batch 1] base_loss_cur 0.5467, time elapsed 00:00:00\n","[Epoch 1 Batch 101] base_loss_cur 0.4064, time elapsed 00:00:01\n","[Epoch 1 Batch 201] base_loss_cur 0.3949, time elapsed 00:00:02\n","Epoch 1 Done! time elapsed: 00:00:02, base_loss_cur_avg 0.4292\n","cur_auc 0.9445, cur_logloss 0.3090, next_auc 0.8394, next_logloss 0.5127\n","time elapsed 00:00:05\n","\n","\n","current period: 18, next period: 19\n","\n","current set size 54365 next set size 54365\n","restored checkpoint: ckpts/IU_train11-23_test24-30_1epoch_0.001/period17/Epoch1_TestAUC0.8394_TestLOGLOSS0.5127.ckpt\n","10-Nov-21 13:50:33 [INFO] : Restoring parameters from ckpts/IU_train11-23_test24-30_1epoch_0.001/period17/Epoch1_TestAUC0.8394_TestLOGLOSS0.5127.ckpt\n","Training Base Model Epoch 1 Start!\n","[Epoch 1 Batch 1] base_loss_cur 0.5509, time elapsed 00:00:00\n","[Epoch 1 Batch 101] base_loss_cur 0.3585, time elapsed 00:00:01\n","[Epoch 1 Batch 201] base_loss_cur 0.3946, time elapsed 00:00:02\n","Epoch 1 Done! time elapsed: 00:00:02, base_loss_cur_avg 0.4247\n","cur_auc 0.9430, cur_logloss 0.3122, next_auc 0.7742, next_logloss 0.6339\n","time elapsed 00:00:05\n","\n","\n","current period: 19, next period: 20\n","\n","current set size 54365 next set size 54365\n","restored checkpoint: ckpts/IU_train11-23_test24-30_1epoch_0.001/period18/Epoch1_TestAUC0.7742_TestLOGLOSS0.6339.ckpt\n","10-Nov-21 13:50:39 [INFO] : Restoring parameters from ckpts/IU_train11-23_test24-30_1epoch_0.001/period18/Epoch1_TestAUC0.7742_TestLOGLOSS0.6339.ckpt\n","Training Base Model Epoch 1 Start!\n","[Epoch 1 Batch 1] base_loss_cur 0.5971, time elapsed 00:00:00\n","[Epoch 1 Batch 101] base_loss_cur 0.5197, time elapsed 00:00:01\n","[Epoch 1 Batch 201] base_loss_cur 0.4176, time elapsed 00:00:02\n","Epoch 1 Done! time elapsed: 00:00:02, base_loss_cur_avg 0.4812\n","cur_auc 0.9278, cur_logloss 0.3522, next_auc 0.8324, next_logloss 0.5112\n","time elapsed 00:00:05\n","\n","\n","current period: 20, next period: 21\n","\n","current set size 54365 next set size 54365\n","restored checkpoint: ckpts/IU_train11-23_test24-30_1epoch_0.001/period19/Epoch1_TestAUC0.8324_TestLOGLOSS0.5112.ckpt\n","10-Nov-21 13:50:47 [INFO] : Restoring parameters from ckpts/IU_train11-23_test24-30_1epoch_0.001/period19/Epoch1_TestAUC0.8324_TestLOGLOSS0.5112.ckpt\n","Training Base Model Epoch 1 Start!\n","[Epoch 1 Batch 1] base_loss_cur 0.5218, time elapsed 00:00:00\n","[Epoch 1 Batch 101] base_loss_cur 0.4317, time elapsed 00:00:01\n","[Epoch 1 Batch 201] base_loss_cur 0.3618, time elapsed 00:00:02\n","Epoch 1 Done! time elapsed: 00:00:02, base_loss_cur_avg 0.4261\n","cur_auc 0.9431, cur_logloss 0.3140, next_auc 0.8216, next_logloss 0.5417\n","time elapsed 00:00:05\n","\n","\n","current period: 21, next period: 22\n","\n","current set size 54365 next set size 54365\n","restored checkpoint: ckpts/IU_train11-23_test24-30_1epoch_0.001/period20/Epoch1_TestAUC0.8216_TestLOGLOSS0.5417.ckpt\n","10-Nov-21 13:50:52 [INFO] : Restoring parameters from ckpts/IU_train11-23_test24-30_1epoch_0.001/period20/Epoch1_TestAUC0.8216_TestLOGLOSS0.5417.ckpt\n","Training Base Model Epoch 1 Start!\n","[Epoch 1 Batch 1] base_loss_cur 0.5755, time elapsed 00:00:00\n","[Epoch 1 Batch 101] base_loss_cur 0.4493, time elapsed 00:00:01\n","[Epoch 1 Batch 201] base_loss_cur 0.4625, time elapsed 00:00:02\n","Epoch 1 Done! time elapsed: 00:00:02, base_loss_cur_avg 0.4464\n","cur_auc 0.9329, cur_logloss 0.3394, next_auc 0.8211, next_logloss 0.5339\n","time elapsed 00:00:05\n","\n","\n","current period: 22, next period: 23\n","\n","current set size 54365 next set size 54365\n","restored checkpoint: ckpts/IU_train11-23_test24-30_1epoch_0.001/period21/Epoch1_TestAUC0.8211_TestLOGLOSS0.5339.ckpt\n","10-Nov-21 13:50:58 [INFO] : Restoring parameters from ckpts/IU_train11-23_test24-30_1epoch_0.001/period21/Epoch1_TestAUC0.8211_TestLOGLOSS0.5339.ckpt\n","Training Base Model Epoch 1 Start!\n","[Epoch 1 Batch 1] base_loss_cur 0.5113, time elapsed 00:00:00\n","[Epoch 1 Batch 101] base_loss_cur 0.4728, time elapsed 00:00:01\n","[Epoch 1 Batch 201] base_loss_cur 0.3950, time elapsed 00:00:02\n","Epoch 1 Done! time elapsed: 00:00:02, base_loss_cur_avg 0.4625\n","cur_auc 0.9240, cur_logloss 0.3603, next_auc 0.8104, next_logloss 0.5455\n","time elapsed 00:00:05\n","\n","\n","current period: 23, next period: 24\n","\n","current set size 54365 next set size 54365\n","restored checkpoint: ckpts/IU_train11-23_test24-30_1epoch_0.001/period22/Epoch1_TestAUC0.8104_TestLOGLOSS0.5455.ckpt\n","10-Nov-21 13:51:04 [INFO] : Restoring parameters from ckpts/IU_train11-23_test24-30_1epoch_0.001/period22/Epoch1_TestAUC0.8104_TestLOGLOSS0.5455.ckpt\n","Training Base Model Epoch 1 Start!\n","[Epoch 1 Batch 1] base_loss_cur 0.5670, time elapsed 00:00:00\n","[Epoch 1 Batch 101] base_loss_cur 0.4367, time elapsed 00:00:01\n","[Epoch 1 Batch 201] base_loss_cur 0.4409, time elapsed 00:00:02\n","Epoch 1 Done! time elapsed: 00:00:02, base_loss_cur_avg 0.4735\n","cur_auc 0.9183, cur_logloss 0.3726, next_auc 0.8332, next_logloss 0.5104\n","time elapsed 00:00:05\n","\n","\n","current period: 24, next period: 25\n","\n","current set size 54365 next set size 54365\n","restored checkpoint: ckpts/IU_train11-23_test24-30_1epoch_0.001/period23/Epoch1_TestAUC0.8332_TestLOGLOSS0.5104.ckpt\n","10-Nov-21 13:51:10 [INFO] : Restoring parameters from ckpts/IU_train11-23_test24-30_1epoch_0.001/period23/Epoch1_TestAUC0.8332_TestLOGLOSS0.5104.ckpt\n","Training Base Model Epoch 1 Start!\n","[Epoch 1 Batch 1] base_loss_cur 0.4952, time elapsed 00:00:00\n","[Epoch 1 Batch 101] base_loss_cur 0.4732, time elapsed 00:00:01\n","[Epoch 1 Batch 201] base_loss_cur 0.3834, time elapsed 00:00:02\n","Epoch 1 Done! time elapsed: 00:00:02, base_loss_cur_avg 0.4599\n","cur_auc 0.9189, cur_logloss 0.3689, next_auc 0.7930, next_logloss 0.5735\n","time elapsed 00:00:05\n","\n","test aucs [0.7930152747606196]\n","average auc 0.7930152747606196\n","\n","test loglosses [0.5734598758656194]\n","average logloss 0.5734598758656194\n","\n","current period: 25, next period: 26\n","\n","current set size 54365 next set size 54365\n","restored checkpoint: ckpts/IU_train11-23_test24-30_1epoch_0.001/period24/Epoch1_TestAUC0.7930_TestLOGLOSS0.5735.ckpt\n","10-Nov-21 13:51:16 [INFO] : Restoring parameters from ckpts/IU_train11-23_test24-30_1epoch_0.001/period24/Epoch1_TestAUC0.7930_TestLOGLOSS0.5735.ckpt\n","Training Base Model Epoch 1 Start!\n","[Epoch 1 Batch 1] base_loss_cur 0.5149, time elapsed 00:00:00\n","[Epoch 1 Batch 101] base_loss_cur 0.4909, time elapsed 00:00:01\n","[Epoch 1 Batch 201] base_loss_cur 0.4634, time elapsed 00:00:02\n","Epoch 1 Done! time elapsed: 00:00:02, base_loss_cur_avg 0.4987\n","cur_auc 0.9029, cur_logloss 0.4036, next_auc 0.8031, next_logloss 0.5468\n","time elapsed 00:00:05\n","\n","test aucs [0.7930152747606196, 0.8031204597248066]\n","average auc 0.7980678672427131\n","\n","test loglosses [0.5734598758656194, 0.5468138748356245]\n","average logloss 0.560136875350622\n","\n","current period: 26, next period: 27\n","\n","current set size 54365 next set size 54365\n","restored checkpoint: ckpts/IU_train11-23_test24-30_1epoch_0.001/period25/Epoch1_TestAUC0.8031_TestLOGLOSS0.5468.ckpt\n","10-Nov-21 13:51:22 [INFO] : Restoring parameters from ckpts/IU_train11-23_test24-30_1epoch_0.001/period25/Epoch1_TestAUC0.8031_TestLOGLOSS0.5468.ckpt\n","Training Base Model Epoch 1 Start!\n","[Epoch 1 Batch 1] base_loss_cur 0.5515, time elapsed 00:00:00\n","[Epoch 1 Batch 101] base_loss_cur 0.4103, time elapsed 00:00:01\n","[Epoch 1 Batch 201] base_loss_cur 0.4439, time elapsed 00:00:02\n","Epoch 1 Done! time elapsed: 00:00:02, base_loss_cur_avg 0.4724\n","cur_auc 0.9125, cur_logloss 0.3846, next_auc 0.7768, next_logloss 0.5834\n","time elapsed 00:00:05\n","\n","test aucs [0.7930152747606196, 0.8031204597248066, 0.7768038245282433]\n","average auc 0.7909798530045564\n","\n","test loglosses [0.5734598758656194, 0.5468138748356245, 0.583421893034745]\n","average logloss 0.5678985479119963\n","\n","current period: 27, next period: 28\n","\n","current set size 54365 next set size 54365\n","restored checkpoint: ckpts/IU_train11-23_test24-30_1epoch_0.001/period26/Epoch1_TestAUC0.7768_TestLOGLOSS0.5834.ckpt\n","10-Nov-21 13:51:28 [INFO] : Restoring parameters from ckpts/IU_train11-23_test24-30_1epoch_0.001/period26/Epoch1_TestAUC0.7768_TestLOGLOSS0.5834.ckpt\n","Training Base Model Epoch 1 Start!\n","[Epoch 1 Batch 1] base_loss_cur 0.5722, time elapsed 00:00:00\n","[Epoch 1 Batch 101] base_loss_cur 0.4500, time elapsed 00:00:01\n","[Epoch 1 Batch 201] base_loss_cur 0.4577, time elapsed 00:00:02\n","Epoch 1 Done! time elapsed: 00:00:02, base_loss_cur_avg 0.5076\n","cur_auc 0.8997, cur_logloss 0.4134, next_auc 0.8051, next_logloss 0.5413\n","time elapsed 00:00:05\n","\n","test aucs [0.7930152747606196, 0.8031204597248066, 0.7768038245282433, 0.8050636676473705]\n","average auc 0.7945008066652599\n","\n","test loglosses [0.5734598758656194, 0.5468138748356245, 0.583421893034745, 0.5412800657791181]\n","average logloss 0.5612439273787768\n","\n","current period: 28, next period: 29\n","\n","current set size 54365 next set size 54365\n","restored checkpoint: ckpts/IU_train11-23_test24-30_1epoch_0.001/period27/Epoch1_TestAUC0.8051_TestLOGLOSS0.5413.ckpt\n","10-Nov-21 13:51:34 [INFO] : Restoring parameters from ckpts/IU_train11-23_test24-30_1epoch_0.001/period27/Epoch1_TestAUC0.8051_TestLOGLOSS0.5413.ckpt\n","Training Base Model Epoch 1 Start!\n","[Epoch 1 Batch 1] base_loss_cur 0.5396, time elapsed 00:00:00\n","[Epoch 1 Batch 101] base_loss_cur 0.5160, time elapsed 00:00:01\n","[Epoch 1 Batch 201] base_loss_cur 0.3993, time elapsed 00:00:02\n","Epoch 1 Done! time elapsed: 00:00:02, base_loss_cur_avg 0.4951\n","cur_auc 0.9024, cur_logloss 0.4062, next_auc 0.7931, next_logloss 0.5562\n","time elapsed 00:00:07\n","\n","test aucs [0.7930152747606196, 0.8031204597248066, 0.7768038245282433, 0.8050636676473705, 0.7930534288170402]\n","average auc 0.7942113310956159\n","\n","test loglosses [0.5734598758656194, 0.5468138748356245, 0.583421893034745, 0.5412800657791181, 0.5561595460270304]\n","average logloss 0.5602270511084275\n","\n","current period: 29, next period: 30\n","\n","current set size 54365 next set size 54365\n","restored checkpoint: ckpts/IU_train11-23_test24-30_1epoch_0.001/period28/Epoch1_TestAUC0.7931_TestLOGLOSS0.5562.ckpt\n","10-Nov-21 13:51:41 [INFO] : Restoring parameters from ckpts/IU_train11-23_test24-30_1epoch_0.001/period28/Epoch1_TestAUC0.7931_TestLOGLOSS0.5562.ckpt\n","Training Base Model Epoch 1 Start!\n","[Epoch 1 Batch 1] base_loss_cur 0.6168, time elapsed 00:00:00\n","[Epoch 1 Batch 101] base_loss_cur 0.4806, time elapsed 00:00:01\n","[Epoch 1 Batch 201] base_loss_cur 0.4100, time elapsed 00:00:02\n","Epoch 1 Done! time elapsed: 00:00:02, base_loss_cur_avg 0.5026\n","cur_auc 0.9048, cur_logloss 0.4050, next_auc 0.8125, next_logloss 0.5332\n","time elapsed 00:00:05\n","\n","test aucs [0.7930152747606196, 0.8031204597248066, 0.7768038245282433, 0.8050636676473705, 0.7930534288170402, 0.812495033586318]\n","average auc 0.7972586148440662\n","\n","test loglosses [0.5734598758656194, 0.5468138748356245, 0.583421893034745, 0.5412800657791181, 0.5561595460270304, 0.5331583915564873]\n","average logloss 0.5557156078497708\n","\n","current period: 30, next period: 31\n","\n","current set size 54365 next set size 54365\n","restored checkpoint: ckpts/IU_train11-23_test24-30_1epoch_0.001/period29/Epoch1_TestAUC0.8125_TestLOGLOSS0.5332.ckpt\n","10-Nov-21 13:51:47 [INFO] : Restoring parameters from ckpts/IU_train11-23_test24-30_1epoch_0.001/period29/Epoch1_TestAUC0.8125_TestLOGLOSS0.5332.ckpt\n","Training Base Model Epoch 1 Start!\n","[Epoch 1 Batch 1] base_loss_cur 0.5635, time elapsed 00:00:00\n","[Epoch 1 Batch 101] base_loss_cur 0.4759, time elapsed 00:00:01\n","[Epoch 1 Batch 201] base_loss_cur 0.3587, time elapsed 00:00:02\n","Epoch 1 Done! time elapsed: 00:00:02, base_loss_cur_avg 0.4834\n","cur_auc 0.9171, cur_logloss 0.3812, next_auc 0.8046, next_logloss 0.5446\n","time elapsed 00:00:05\n","\n","test aucs [0.7930152747606196, 0.8031204597248066, 0.7768038245282433, 0.8050636676473705, 0.7930534288170402, 0.812495033586318, 0.8046347603178905]\n","average auc 0.7983123499117555\n","\n","test loglosses [0.5734598758656194, 0.5468138748356245, 0.583421893034745, 0.5412800657791181, 0.5561595460270304, 0.5331583915564873, 0.5445928974663797]\n","average logloss 0.5541266492235721\n","10-Nov-21 13:51:53 [INFO] : JOB END: INCREMENTAL_UPDATE\n"]}]}]}