{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"textsum_playground_enronemail","provenance":[],"toc_visible":true,"authorship_tag":"ABX9TyMbou2UZ5u0mzCjcZf5r+Z5"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"WslXEGMqm4dF","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":922},"outputId":"91d1a01b-c8f0-4ab6-ccb8-cacbaa8100d2","executionInfo":{"status":"ok","timestamp":1587068898255,"user_tz":-330,"elapsed":23344,"user":{"displayName":"Sparsh Agarwal","photoUrl":"","userId":"13037694610922482904"}}},"source":["!pip install mail-parser\n","!pip install talon"],"execution_count":8,"outputs":[{"output_type":"stream","text":["Collecting mail-parser\n","  Downloading https://files.pythonhosted.org/packages/a9/e9/d8c5df0f799b025e7eaf1282d099d7a07cbb4bfcd11e3669fc69ff1d4fb5/mail_parser-3.12.0-py3-none-any.whl\n","Collecting ipaddress==1.0.23\n","  Downloading https://files.pythonhosted.org/packages/c2/f8/49697181b1651d8347d24c095ce46c7346c37335ddc7d255833e7cde674d/ipaddress-1.0.23-py2.py3-none-any.whl\n","Collecting six==1.14.0\n","  Downloading https://files.pythonhosted.org/packages/65/eb/1f97cb97bfc2390a276969c6fae16075da282f5058082d4cb10c6c5c1dba/six-1.14.0-py2.py3-none-any.whl\n","Collecting simplejson==3.17.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/98/87/a7b98aa9256c8843f92878966dc3d8d914c14aad97e2c5ce4798d5743e07/simplejson-3.17.0.tar.gz (83kB)\n","\u001b[K     |████████████████████████████████| 92kB 4.2MB/s \n","\u001b[?25hBuilding wheels for collected packages: simplejson\n","  Building wheel for simplejson (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for simplejson: filename=simplejson-3.17.0-cp36-cp36m-linux_x86_64.whl size=114194 sha256=9ac9eaf6c591622ec35f63a6243a84934ab919c094bf0e39e0b5156cfe10074e\n","  Stored in directory: /root/.cache/pip/wheels/86/c0/83/dcd0339abb2640544bb8e0938aab2d069cef55e5647ce6e097\n","Successfully built simplejson\n","\u001b[31mERROR: google-colab 1.0.0 has requirement six~=1.12.0, but you'll have six 1.14.0 which is incompatible.\u001b[0m\n","\u001b[31mERROR: datascience 0.10.6 has requirement folium==0.2.1, but you'll have folium 0.8.3 which is incompatible.\u001b[0m\n","\u001b[31mERROR: albumentations 0.1.12 has requirement imgaug<0.2.7,>=0.2.5, but you'll have imgaug 0.2.9 which is incompatible.\u001b[0m\n","Installing collected packages: ipaddress, six, simplejson, mail-parser\n","  Found existing installation: six 1.12.0\n","    Uninstalling six-1.12.0:\n","      Successfully uninstalled six-1.12.0\n","Successfully installed ipaddress-1.0.23 mail-parser-3.12.0 simplejson-3.17.0 six-1.14.0\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.colab-display-data+json":{"pip_warning":{"packages":["ipaddress","six"]}}},"metadata":{"tags":[]}},{"output_type":"stream","text":["Collecting talon\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/eb/f7/3348324b489f74500054a1a1ea18b86cc40d19994f789311228c12a6bcc7/talon-1.4.4.tar.gz (62kB)\n","\r\u001b[K     |█████▎                          | 10kB 16.7MB/s eta 0:00:01\r\u001b[K     |██████████▌                     | 20kB 1.7MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 30kB 2.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 40kB 3.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▎     | 51kB 2.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▋| 61kB 2.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 71kB 2.3MB/s \n","\u001b[?25hRequirement already satisfied: lxml>=2.3.3 in /usr/local/lib/python3.6/dist-packages (from talon) (4.2.6)\n","Requirement already satisfied: regex>=1 in /usr/local/lib/python3.6/dist-packages (from talon) (2019.12.20)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from talon) (1.18.2)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from talon) (1.4.1)\n","Requirement already satisfied: scikit-learn>=0.16.1 in /usr/local/lib/python3.6/dist-packages (from talon) (0.22.2.post1)\n","Requirement already satisfied: chardet>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from talon) (3.0.4)\n","Collecting cchardet>=0.3.5\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1e/c5/7e1a0d7b4afd83d6f8de794fce82820ec4c5136c6d52e14000822681a842/cchardet-2.1.6-cp36-cp36m-manylinux2010_x86_64.whl (241kB)\n","\u001b[K     |████████████████████████████████| 245kB 8.1MB/s \n","\u001b[?25hCollecting cssselect\n","  Downloading https://files.pythonhosted.org/packages/3b/d4/3b5c17f00cce85b9a1e6f91096e1cc8e8ede2e1be8e96b87ce1ed09e92c5/cssselect-1.1.0-py2.py3-none-any.whl\n","Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from talon) (1.14.0)\n","Requirement already satisfied: html5lib in /usr/local/lib/python3.6/dist-packages (from talon) (1.0.1)\n","Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn>=0.16.1->talon) (0.14.1)\n","Requirement already satisfied: webencodings in /usr/local/lib/python3.6/dist-packages (from html5lib->talon) (0.5.1)\n","Building wheels for collected packages: talon\n","  Building wheel for talon (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for talon: filename=talon-1.4.4-cp36-none-any.whl size=35288 sha256=3498a3bf4202754bb5919ff83df54397005b1d65542e583afe4667728cf6b11b\n","  Stored in directory: /root/.cache/pip/wheels/3f/ca/f9/748f5328839c03fe055f69110357b4718853dde8d872896209\n","Successfully built talon\n","Installing collected packages: cchardet, cssselect, talon\n","Successfully installed cchardet-2.1.6 cssselect-1.1.0 talon-1.4.4\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"iqnYRN2vmUI8","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":71},"outputId":"1f9bcc0f-719a-4264-f758-5a861b4681e1","executionInfo":{"status":"ok","timestamp":1587068899901,"user_tz":-330,"elapsed":24789,"user":{"displayName":"Sparsh Agarwal","photoUrl":"","userId":"13037694610922482904"}}},"source":["import sys\n","from os import listdir\n","from os.path import isfile, join\n","import configparser\n","from sqlalchemy import create_engine\n","\n","import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","\n","import email\n","import mailparser\n","import xml.etree.ElementTree as ET\n","from talon.signature.bruteforce import extract_signature\n","from nltk.tokenize import word_tokenize, sent_tokenize\n","from nltk.corpus import stopwords\n","import re\n","\n","import dask.dataframe as dd\n","from distributed import Client\n","import multiprocessing as mp"],"execution_count":9,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n","  warnings.warn(msg, category=FutureWarning)\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"eLybM5WKm1vJ","colab_type":"code","colab":{}},"source":["# !wget https://github.com/dailykirt/ML_Enron_email_summary/blob/master/data/enron_mail_20150507.tar.gz?raw=true\n","# !tar -xf /content/enron_mail_20150507.tar.gz?raw=true\n","mail_dir = '/content/maildir/'"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3PabXGqnoRyJ","colab_type":"text"},"source":["## 2. Data Input \n","### A. Enron Email Dataset\n","The raw enron email dataset contains a maildir directory that contains folders seperated by employee which contain the emails. The following processes the raw text of each email into a dask dataframe with the following columns: \n","\n","Employee: The username of the email owner. <br>\n","Body: Cleaned body of the email. <br>\n","Subject: The title of the email. <br>\n","From: The original sender of the email <br>\n","Message-ID: Used to remove duplicate emails, as each email has a unique ID. <br>\n","Chain: The parsed out email chain from a email that was forwarded. <br>\n","Signature: The extracted signature from the body.<br>\n","Date: Time the email was sent. <br>\n","\n","All of the Enron emails were sent using the Multipurpose Internet Mail Extensions 1.0 (MIME) format. Keeping this in mind helps find the correct libraries and methods to clean the emails in a standardized fashion. "]},{"cell_type":"code","metadata":{"id":"m-jg5SyMoNMz","colab_type":"code","colab":{}},"source":["def process_email(index):\n","    '''\n","    This function splits a raw email into constituent parts that can be used as features.\n","    '''\n","    email_path = index[0]\n","    employee = index[1]\n","    folder = index[2]\n","    \n","    mail = mailparser.parse_from_file(email_path)\n","    full_body = email.message_from_string(mail.body)\n","    \n","    #Only retrieve the body of the email. \n","    if full_body.is_multipart():\n","        return\n","    else:\n","        mail_body = full_body.get_payload()    \n","    \n","    split_body = clean_body(mail_body)\n","    headers = mail.headers\n","    #Reformating date to be more pandas readable\n","    date_time = process_date(headers.get('Date'))\n","\n","    email_dict = {\n","                \"employee\" : employee,\n","                \"email_folder\": folder,\n","                \"message_id\": headers.get('Message-ID'),\n","                \"date\" : date_time,\n","                \"from\" : headers.get('From'),\n","                \"subject\": headers.get('Subject'),\n","                \"body\" : split_body['body'],\n","                \"chain\" : split_body['chain'],\n","                \"signature\": split_body['signature'],\n","                \"full_email_path\" : email_path #for debug purposes. \n","    }\n","    \n","    #Append row to dataframe. \n","    return email_dict"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"nKNuNJpQolgt","colab_type":"code","colab":{}},"source":["def clean_body(mail_body):\n","    '''\n","    This extracts both the email signature, and the forwarding email chain if it exists. \n","    '''\n","    delimiters = [\"-----Original Message-----\",\"To:\",\"From\"]\n","    \n","    #Trying to split string by biggest delimiter. \n","    old_len = sys.maxsize\n","    \n","    for delimiter in delimiters:\n","        split_body = mail_body.split(delimiter,1)\n","        new_len = len(split_body[0])\n","        if new_len <= old_len:\n","            old_len = new_len\n","            final_split = split_body\n","            \n","    #Then pull chain message\n","    if (len(final_split) == 1):\n","        mail_chain = None\n","    else:\n","        mail_chain = final_split[1] \n","    \n","    #The following uses Talon to try to get a clean body, and seperate out the rest of the email. \n","    clean_body, sig = extract_signature(final_split[0])\n","    \n","    return {'body': clean_body, 'chain' : mail_chain, 'signature': sig}"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ysBpNTEmopSi","colab_type":"code","colab":{}},"source":["def process_date(date_time):\n","    '''\n","    Converts the MIME date format to a more pandas friendly type. \n","    '''\n","    try:\n","        date_time = email.utils.format_datetime(email.utils.parsedate_to_datetime(date_time))\n","    except:\n","        date_time = None\n","    return date_time"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"WbNXr6KFowj0","colab_type":"code","colab":{}},"source":["def generate_email_paths(mail_dir):\n","    '''\n","    Given a mail directory, this will generate the file paths to each email in each inbox. \n","    '''\n","    mailboxes = listdir(mail_dir)\n","    for mailbox in mailboxes:\n","        inbox = listdir(mail_dir + mailbox)\n","        for folder in inbox:\n","            path = mail_dir + mailbox + \"/\" + folder\n","            emails = listdir(path)\n","            for single_email in emails:\n","                full_path = path + \"/\" + single_email\n","                if isfile(full_path): #Skip directories.\n","                    yield (full_path, mailbox, folder)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"88rXFkpxo2ox","colab_type":"code","colab":{}},"source":["#bug-patch\n","# !rm maildir/lokay-m/1.\n","# !rm maildir/scholtes-d/1.\n","# !rm maildir/baughman-d/1.\n","# !rm maildir/corman-s/1.\n","# !rm maildir/shively-h/2.\n","# !rm maildir/shively-h/1.\n","# !rm maildir/richey-c/1."],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"fYFTqXsmoz0K","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"9dc7a0e7-0b54-4fc3-decc-fcae32df2db9"},"source":["#Use multiprocessing to speed up initial data load and processing. Also helps partition DASK dataframe. \n","# try:\n","#     cpus = mp.cpu_count()\n","# except NotImplementedError:\n","#     cpus = 2\n","cpus = 8\n","pool = mp.Pool(processes=cpus)\n","print(\"CPUS: \" + str(cpus))\n","\n","indexes = generate_email_paths(mail_dir)\n","enron_email_df = pool.map(process_email,indexes)\n","#Remove Nones from the list\n","enron_email_df = [i for i in enron_email_df if i]\n","enron_email_df = pd.DataFrame(enron_email_df)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["CPUS: 8\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"PUP5A-BasTX4","colab_type":"code","colab":{}},"source":["enron_email_df.describe()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"TsQhe_Z3vTsC","colab_type":"text"},"source":["## BC3 corpus\n","This dataset is split into two xml files. One contains the original emails split line by line, and the other contains the summarizations created by the annotators. Each email may contain several summarizations from different annotators and summarizations may also be over several emails. This will create a data frame for both xml files, then join them together using the thread number in combination of the email number for a single final dataframe. \n","\n","The first dataframe will contain the wrangled original emails containing the following information:\n","\n","Listno: Thread identifier <br>\n","Email_num: Email in thread sequence <br>\n","From: The original sender of the email <br>\n","To: The recipient of the email. <br>\n","Recieved: Time email was recieved. <br>\n","Subject: Title of email. <br>\n","Body: Original body. <br>"]},{"cell_type":"code","metadata":{"id":"QpvMS8JhvVyc","colab_type":"code","colab":{}},"source":["def parse_bc3_emails(root):\n","    '''\n","    This adds every BC3 email to a newly created dataframe. \n","    '''\n","    BC3_email_list = []\n","    #The emails are seperated by threads.\n","    for thread in root:\n","        email_num = 0\n","        #Iterate through the thread elements <name, listno, Doc>\n","        for thread_element in thread:\n","            #Getting the listno allows us to link the summaries to the correct emails\n","            if thread_element.tag == \"listno\":\n","                listno = thread_element.text\n","            #Each Doc element is a single email\n","            if thread_element.tag == \"DOC\":\n","                email_num += 1\n","                email_metadata = []\n","                for email_attribute in thread_element:\n","                    #If the email_attri is text, then each child contains a line from the body of the email\n","                    if email_attribute.tag == \"Text\":\n","                        email_body = \"\"\n","                        for sentence in email_attribute:\n","                            email_body += sentence.text\n","                    else:\n","                        #The attributes of the Email <Recieved, From, To, Subject, Text> appends in this order. \n","                        email_metadata.append(email_attribute.text)\n","                        \n","                #Use same enron cleaning methods on the body of the email\n","                split_body = clean_body(email_body)\n","                    \n","                email_dict = {\n","                    \"listno\" : listno,\n","                    \"date\" : process_date(email_metadata[0]),\n","                    \"from\" : email_metadata[1],\n","                    \"to\" : email_metadata[2],\n","                    \"subject\" : email_metadata[3],\n","                    \"body\" : split_body['body'],\n","                    \"email_num\": email_num\n","                }\n","                \n","                BC3_email_list.append(email_dict)           \n","    return pd.DataFrame(BC3_email_list)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"jhvJw3WavZ7k","colab_type":"code","colab":{}},"source":["#load BC3 Email Corpus. Much smaller dataset has no need for parallel processing. \n","parsedXML = ET.parse( \"/content/ML_Enron_email_summary/data/BC3_Email_Corpus/corpus.xml\" )\n","root = parsedXML.getroot()\n","\n","#Clean up BC3 emails the same way as the Enron emails. \n","bc3_email_df = parse_bc3_emails(root)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"FsQqvnn5vZ5u","colab_type":"code","colab":{}},"source":["bc3_email_df.head(3)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3CPb_hORvtS-","colab_type":"text"},"source":["The second dataframe contains the summarizations of each email:\n","\n","Annotator: Person who created summarization. <br>\n","Email_num: Email in thread sequence. <br>\n","Listno: Thread identifier. <br>\n","Summary: Human summarization of the email. <br>"]},{"cell_type":"code","metadata":{"id":"t0d5VvBxvZ48","colab_type":"code","colab":{}},"source":["def parse_bc3_summaries(root):\n","    '''\n","    This parses every BC3 Human summary that is contained in the dataset. \n","    '''\n","    BC3_summary_list = []\n","    for thread in root:\n","        #Iterate through the thread elements <listno, name, annotation>\n","        for thread_element in thread:\n","            if thread_element.tag == \"listno\":\n","                listno = thread_element.text\n","            #Each Doc element is a single email\n","            if thread_element.tag == \"annotation\":\n","                for annotation in thread_element:\n","                #If the email_attri is summary, then each child contains a summarization line\n","                    if annotation.tag == \"summary\":\n","                        summary_dict = {}\n","                        for summary in annotation:\n","                            #Generate the set of emails the summary sentence belongs to (often a single email)\n","                            email_nums = summary.attrib['link'].split(',')\n","                            s = set()\n","                            for num in email_nums:\n","                                s.add(num.split('.')[0].strip()) \n","                            #Remove empty strings, since they summarize whole threads instead of emails. \n","                            s = [x for x in set(s) if x]\n","                            for email_num in s:\n","                                if email_num in summary_dict:\n","                                    summary_dict[email_num] += ' ' + summary.text\n","                                else:\n","                                    summary_dict[email_num] = summary.text\n","                    #get annotator description\n","                    elif annotation.tag == \"desc\":\n","                        annotator = annotation.text\n","                #For each email summarizaiton create an entry\n","                for email_num, summary in summary_dict.items():\n","                    email_dict = {\n","                        \"listno\" : listno,\n","                        \"annotator\" : annotator,\n","                        \"email_num\" : email_num,\n","                        \"summary\" : summary\n","                    }      \n","                    BC3_summary_list.append(email_dict)\n","    return pd.DataFrame(BC3_summary_list)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"5K9AzTzVvxpu","colab_type":"code","colab":{}},"source":["#Load summaries and process\n","parsedXML = ET.parse( \"/content/ML_Enron_email_summary/data/BC3_Email_Corpus/annotation.xml\" )\n","root = parsedXML.getroot()\n","bc3_summary_df = parse_bc3_summaries(root)\n","bc3_summary_df['email_num'] = bc3_summary_df['email_num'].astype(int)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"YCP2ifmIvxop","colab_type":"code","colab":{}},"source":["bc3_summary_df.info()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"hMMpN_Jyvxno","colab_type":"code","colab":{}},"source":["#merge the dataframes together\n","bc3_df = pd.merge(bc3_email_df, \n","                  bc3_summary_df[['annotator', 'email_num', 'listno', 'summary']],\n","                 on=['email_num', 'listno'])\n","bc3_df.head()"],"execution_count":0,"outputs":[]}]}