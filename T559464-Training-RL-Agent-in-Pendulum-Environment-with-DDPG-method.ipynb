{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"T559464 | Training RL Agent in Pendulum Environment with DDPG method","provenance":[{"file_id":"12DkJMCT5alSDk2Ybzd9BNRWb9tSl0dVH","timestamp":1638449794834},{"file_id":"1Uy9HVE3IdljpGYBZLjSeWHi6ePRnB1Ys","timestamp":1638449179447},{"file_id":"1yNQHhhvOVs_VM48DN7r6K5em_RaSLllG","timestamp":1638448522366},{"file_id":"1PG1q81afxiJCXPZbQKhgfgSBbDx9HITY","timestamp":1638447916295},{"file_id":"11_5lxJoi19PUPuTzS5RBlSitYErAUY3h","timestamp":1638447606203}],"collapsed_sections":[],"authorship_tag":"ABX9TyOX/9kPloPCfoHuKVC/4ahA"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"tyMytwWZFUhp"},"source":["# Training RL Agent in Pendulum Environment with DDPG method"]},{"cell_type":"code","metadata":{"id":"ysNR-nNZEHYs","executionInfo":{"status":"ok","timestamp":1638449957580,"user_tz":-330,"elapsed":3143,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}}},"source":["import argparse\n","import os\n","import random\n","from collections import deque\n","from datetime import datetime\n","import gym\n","import numpy as np\n","import tensorflow as tf\n","from tensorflow.keras.layers import Dense, Input, Lambda, concatenate"],"execution_count":1,"outputs":[]},{"cell_type":"code","metadata":{"id":"doMRsX9OEImC","executionInfo":{"status":"ok","timestamp":1638449957582,"user_tz":-330,"elapsed":5,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}}},"source":["tf.keras.backend.set_floatx(\"float64\")"],"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dCLXisl6ELQ0","executionInfo":{"status":"ok","timestamp":1638449966826,"user_tz":-330,"elapsed":733,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}},"outputId":"81c04f44-9b2a-4ab4-b330-571ce9fc82b7"},"source":["parser = argparse.ArgumentParser()\n","parser.add_argument(\"--env\", default=\"Pendulum-v0\")\n","parser.add_argument(\"--actor_lr\", type=float, default=0.0005)\n","parser.add_argument(\"--critic_lr\", type=float, default=0.001)\n","parser.add_argument(\"--batch_size\", type=int, default=64)\n","parser.add_argument(\"--tau\", type=float, default=0.05)\n","parser.add_argument(\"--gamma\", type=float, default=0.99)\n","parser.add_argument(\"--train_start\", type=int, default=2000)\n","parser.add_argument(\"--logdir\", default=\"logs\")\n","\n","args = parser.parse_args([])\n","logdir = os.path.join(\n","    args.logdir, parser.prog, args.env, datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",")\n","print(f\"Saving training logs to:{logdir}\")\n","writer = tf.summary.create_file_writer(logdir)"],"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Saving training logs to:logs/ipykernel_launcher.py/Pendulum-v0/20211202-125928\n"]}]},{"cell_type":"code","metadata":{"id":"Y56QUuw5K2EH","executionInfo":{"status":"ok","timestamp":1638449968294,"user_tz":-330,"elapsed":6,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}}},"source":["class ReplayBuffer:\n","    def __init__(self, capacity=10000):\n","        self.buffer = deque(maxlen=capacity)\n","\n","    def store(self, state, action, reward, next_state, done):\n","        self.buffer.append([state, action, reward, next_state, done])\n","\n","    def sample(self):\n","        sample = random.sample(self.buffer, args.batch_size)\n","        states, actions, rewards, next_states, done = map(np.asarray, zip(*sample))\n","        states = np.array(states).reshape(args.batch_size, -1)\n","        next_states = np.array(next_states).reshape(args.batch_size, -1)\n","        return states, actions, rewards, next_states, done\n","\n","    def size(self):\n","        return len(self.buffer)"],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"id":"p9IrVspCMzrU","executionInfo":{"status":"ok","timestamp":1638449975628,"user_tz":-330,"elapsed":624,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}}},"source":["class Actor:\n","    def __init__(self, state_dim, action_dim, action_bound):\n","        self.state_dim = state_dim\n","        self.action_dim = action_dim\n","        self.action_bound = action_bound\n","        self.model = self.nn_model()\n","        self.opt = tf.keras.optimizers.Adam(args.actor_lr)\n","\n","    def nn_model(self):\n","        return tf.keras.Sequential(\n","            [\n","                Input((self.state_dim,)),\n","                Dense(32, activation=\"relu\"),\n","                Dense(32, activation=\"relu\"),\n","                Dense(self.action_dim, activation=\"tanh\"),\n","                Lambda(lambda x: x * self.action_bound),\n","            ]\n","        )\n","\n","    def train(self, states, q_grads):\n","        with tf.GradientTape() as tape:\n","            grads = tape.gradient(\n","                self.model(states), self.model.trainable_variables, -q_grads\n","            )\n","        self.opt.apply_gradients(zip(grads, self.model.trainable_variables))\n","\n","    def predict(self, state):\n","        return self.model.predict(state)\n","\n","    def get_action(self, state):\n","        state = np.reshape(state, [1, self.state_dim])\n","        return self.model.predict(state)[0]"],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"id":"uSFhh4DuM0v4","executionInfo":{"status":"ok","timestamp":1638449984893,"user_tz":-330,"elapsed":810,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}}},"source":["class Critic:\n","    def __init__(self, state_dim, action_dim):\n","        self.state_dim = state_dim\n","        self.action_dim = action_dim\n","        self.model = self.nn_model()\n","        self.opt = tf.keras.optimizers.Adam(args.critic_lr)\n","\n","    def nn_model(self):\n","        state_input = Input((self.state_dim,))\n","        s1 = Dense(64, activation=\"relu\")(state_input)\n","        s2 = Dense(32, activation=\"relu\")(s1)\n","        action_input = Input((self.action_dim,))\n","        a1 = Dense(32, activation=\"relu\")(action_input)\n","        c1 = concatenate([s2, a1], axis=-1)\n","        c2 = Dense(16, activation=\"relu\")(c1)\n","        output = Dense(1, activation=\"linear\")(c2)\n","        return tf.keras.Model([state_input, action_input], output)\n","\n","    def predict(self, inputs):\n","        return self.model.predict(inputs)\n","\n","    def q_gradients(self, states, actions):\n","        actions = tf.convert_to_tensor(actions)\n","        with tf.GradientTape() as tape:\n","            tape.watch(actions)\n","            q_values = self.model([states, actions])\n","            q_values = tf.squeeze(q_values)\n","        return tape.gradient(q_values, actions)\n","\n","    def compute_loss(self, v_pred, td_targets):\n","        mse = tf.keras.losses.MeanSquaredError()\n","        return mse(td_targets, v_pred)\n","\n","    def train(self, states, actions, td_targets):\n","        with tf.GradientTape() as tape:\n","            v_pred = self.model([states, actions], training=True)\n","            assert v_pred.shape == td_targets.shape\n","            loss = self.compute_loss(v_pred, tf.stop_gradient(td_targets))\n","        grads = tape.gradient(loss, self.model.trainable_variables)\n","        self.opt.apply_gradients(zip(grads, self.model.trainable_variables))\n","        return loss"],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"id":"1JQRAq0yM2gq","executionInfo":{"status":"ok","timestamp":1638449995479,"user_tz":-330,"elapsed":970,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}}},"source":["class Agent:\n","    def __init__(self, env):\n","        self.env = env\n","        self.state_dim = self.env.observation_space.shape[0]\n","        self.action_dim = self.env.action_space.shape[0]\n","        self.action_bound = self.env.action_space.high[0]\n","\n","        self.buffer = ReplayBuffer()\n","\n","        self.actor = Actor(self.state_dim, self.action_dim, self.action_bound)\n","        self.critic = Critic(self.state_dim, self.action_dim)\n","\n","        self.target_actor = Actor(self.state_dim, self.action_dim, self.action_bound)\n","        self.target_critic = Critic(self.state_dim, self.action_dim)\n","\n","        actor_weights = self.actor.model.get_weights()\n","        critic_weights = self.critic.model.get_weights()\n","        self.target_actor.model.set_weights(actor_weights)\n","        self.target_critic.model.set_weights(critic_weights)\n","\n","    def update_target(self):\n","        actor_weights = self.actor.model.get_weights()\n","        t_actor_weights = self.target_actor.model.get_weights()\n","        critic_weights = self.critic.model.get_weights()\n","        t_critic_weights = self.target_critic.model.get_weights()\n","\n","        for i in range(len(actor_weights)):\n","            t_actor_weights[i] = (\n","                args.tau * actor_weights[i] + (1 - args.tau) * t_actor_weights[i]\n","            )\n","\n","        for i in range(len(critic_weights)):\n","            t_critic_weights[i] = (\n","                args.tau * critic_weights[i] + (1 - args.tau) * t_critic_weights[i]\n","            )\n","\n","        self.target_actor.model.set_weights(t_actor_weights)\n","        self.target_critic.model.set_weights(t_critic_weights)\n","\n","    def get_td_target(self, rewards, q_values, dones):\n","        targets = np.asarray(q_values)\n","        for i in range(q_values.shape[0]):\n","            if dones[i]:\n","                targets[i] = rewards[i]\n","            else:\n","                targets[i] = rewards[i] + args.gamma * q_values[i]\n","        return targets\n","\n","    def add_ou_noise(self, x, rho=0.15, mu=0, dt=1e-1, sigma=0.2, dim=1):\n","        return (\n","            x + rho * (mu - x) * dt + sigma * np.sqrt(dt) * np.random.normal(size=dim)\n","        )\n","\n","    def replay_experience(self):\n","        for _ in range(10):\n","            states, actions, rewards, next_states, dones = self.buffer.sample()\n","            target_q_values = self.target_critic.predict(\n","                [next_states, self.target_actor.predict(next_states)]\n","            )\n","            td_targets = self.get_td_target(rewards, target_q_values, dones)\n","\n","            self.critic.train(states, actions, td_targets)\n","\n","            s_actions = self.actor.predict(states)\n","            s_grads = self.critic.q_gradients(states, s_actions)\n","            grads = np.array(s_grads).reshape((-1, self.action_dim))\n","            self.actor.train(states, grads)\n","            self.update_target()\n","\n","    def train(self, max_episodes=1000):\n","        with writer.as_default():\n","            for ep in range(max_episodes):\n","                episode_reward, done = 0, False\n","\n","                state = self.env.reset()\n","                bg_noise = np.zeros(self.action_dim)\n","                while not done:\n","                    # self.env.render()\n","                    action = self.actor.get_action(state)\n","                    noise = self.add_ou_noise(bg_noise, dim=self.action_dim)\n","                    action = np.clip(\n","                        action + noise, -self.action_bound, self.action_bound\n","                    )\n","\n","                    next_state, reward, done, _ = self.env.step(action)\n","                    self.buffer.store(state, action, (reward + 8) / 8, next_state, done)\n","                    bg_noise = noise\n","                    episode_reward += reward\n","                    state = next_state\n","                if (\n","                    self.buffer.size() >= args.batch_size\n","                    and self.buffer.size() >= args.train_start\n","                ):\n","                    self.replay_experience()\n","                print(f\"Episode#{ep} Reward:{episode_reward}\")\n","                tf.summary.scalar(\"episode_reward\", episode_reward, step=ep)"],"execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zhaejVDlOvM4","executionInfo":{"status":"ok","timestamp":1638450024967,"user_tz":-330,"elapsed":21710,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}},"outputId":"b7658fe0-bebc-43a1-9d77-b48d3bd9be00"},"source":["if __name__ == \"__main__\":\n","    env_name = \"Pendulum-v0\"\n","    env = gym.make(env_name)\n","    agent = Agent(env)\n","    agent.train(max_episodes=2)  # Increase max_episodes value"],"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["Episode#0 Reward:-1664.7795492513724\n","Episode#1 Reward:-1438.3905919256467\n"]}]},{"cell_type":"markdown","metadata":{"id":"ixk34zYIEgB_"},"source":["---"]},{"cell_type":"code","metadata":{"id":"VDwjGZWIE5il","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1638450138115,"user_tz":-330,"elapsed":3381,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}},"outputId":"e1fa19da-2ad4-4a0a-ad90-c80cb5ba9d10"},"source":["!pip install -q watermark\n","%reload_ext watermark\n","%watermark -a \"Sparsh A.\" -m -iv -u -t -d"],"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["Author: Sparsh A.\n","\n","Last updated: 2021-12-02 13:02:19\n","\n","Compiler    : GCC 7.5.0\n","OS          : Linux\n","Release     : 5.4.104+\n","Machine     : x86_64\n","Processor   : x86_64\n","CPU cores   : 2\n","Architecture: 64bit\n","\n","IPython   : 5.5.0\n","argparse  : 1.1\n","tensorflow: 2.7.0\n","gym       : 0.17.3\n","numpy     : 1.19.5\n","\n"]}]},{"cell_type":"markdown","metadata":{"id":"haKsOAX2E1XI"},"source":["---"]},{"cell_type":"markdown","metadata":{"id":"4s5DJf8WE2as"},"source":["**END**"]}]}