{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp datasets.mts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MTS Dataset\n",
    "> data from the MTS Kion application on user interactions with content for a period of 6 months.\n",
    "\n",
    "The presented dataset contains data on users and objects (series / movies), as well as on their interactions (content viewing by the user) from the Kion online cinema. Content view data collected for ~6 months, from 2021-03-13 to 2021-08-22 inclusive, and diluted with random noise. User and content IDs are anonymised.\n",
    "\n",
    "### users.csv\n",
    "This file contains information about users:\n",
    "- user_id - User ID\n",
    "- age - user's age group, string like \"M_N\".\n",
    " - 18_24 - from 18 to 24 years old inclusive\n",
    " - 25_34 - from 25 to 34 years old inclusive\n",
    " - 35_44 - from 35 to 44 years old inclusive\n",
    " - 45_54 - from 45 to 54 years old inclusive\n",
    " - 55_64 - from 55 to 64 years old inclusive\n",
    " - 65_inf - from 65 and older\n",
    "- sex - user gender\n",
    " - M - man\n",
    " - F - woman\n",
    "- income - user's income, string like \"M_N\n",
    " - income_0_20\n",
    " - income_20_40\n",
    " - income_40_60\n",
    " - income_60_90  \n",
    " - income_90_150\n",
    " - income_150_inf\n",
    "- kids_flg - flag \"presence of a child\n",
    "\n",
    "### items.csv\n",
    "This file contains information about objects (movies/series):\n",
    "- item_id - Content ID\n",
    "- content_type - Type of content (movie, series)\n",
    "- title - Title in Russian\n",
    "- title_orig - original name\n",
    "- genres - Genres from source (online movie theaters)\n",
    "- countries - country\n",
    "- for_kids - flag \"content for children\"\n",
    "- age_rating - age rating\n",
    "- studios - studios\n",
    "- directors - directors\n",
    "- actors - actors\n",
    "- keywords - keywords\n",
    "- description - description\n",
    "\n",
    "### interactions.csv\n",
    "This file contains information about user interactions with content:\n",
    "- user_id - User ID\n",
    "- item_id - Content ID\n",
    "- last_watch_dt - Date last viewed\n",
    "- total_dur - The total duration of all views of this content in seconds\n",
    "- content_type - Type of content (movie, series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev.showdoc import *\n",
    "from fastcore.nb_imports import *\n",
    "from fastcore.test import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from typing import Any, Iterable, List, Optional, Tuple, Union, Callable\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from recohut.utils.common_utils import *\n",
    "from recohut.datasets.bases.interactions import InteractionsDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class MTSDataset(InteractionsDataset):\n",
    "    url_users = \"https://github.com/RecoHut-Datasets/mts_kion/raw/v1/users.parquet.snappy\"\n",
    "    url_items = \"https://github.com/RecoHut-Datasets/mts_kion/raw/v1/items.parquet.snappy\"\n",
    "    url_inter = \"https://github.com/RecoHut-Datasets/mts_kion/raw/v1/interactions.parquet.snappy\"\n",
    "\n",
    "    def __init__(self, sample_frac=1, **kwargs):\n",
    "        self.sample_frac = sample_frac\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "    @property\n",
    "    def raw_file_names(self):\n",
    "        return ['users.parquet.snappy',\n",
    "                'items.parquet.snappy',\n",
    "                'interactions.parquet.snappy']\n",
    "\n",
    "    @property\n",
    "    def processed_file_names(self):\n",
    "        return ['users_processed.csv',\n",
    "                'items_processed.csv',\n",
    "                'interactions_processed.csv',\n",
    "                'item_stats.csv']\n",
    "\n",
    "    def download(self):\n",
    "        _ = download_url(self.url_users, self.raw_dir)\n",
    "        _ = download_url(self.url_items, self.raw_dir)\n",
    "        _ = download_url(self.url_inter, self.raw_dir)\n",
    "\n",
    "    def load_users_df(self):\n",
    "        df = pd.read_parquet(self.raw_paths[0])\n",
    "        return df\n",
    "\n",
    "    def load_items_df(self):\n",
    "        df = pd.read_parquet(self.raw_paths[1])\n",
    "        return df\n",
    "\n",
    "    def load_ratings_df(self):\n",
    "        df = pd.read_parquet(self.raw_paths[2])\n",
    "        df = df.sample(frac=self.sample_frac)\n",
    "        return df\n",
    "\n",
    "    @staticmethod\n",
    "    def add_user_stats(interactions_df, users_df, split_name=''):\n",
    "        \"\"\"\n",
    "        Computes user watches stats for particular interactions date split\n",
    "        and adds them to users dataframe with specific name\n",
    "        \"\"\"\n",
    "        user_watch_count_all = interactions_df[\n",
    "            interactions_df['total_dur'] > 300].groupby(by='user_id')['item_id'].count()\n",
    "        max_date_df = interactions_df['last_watch_dt'].max()\n",
    "        user_watch_count_last_14 = interactions_df[\n",
    "            (interactions_df['total_dur'] > 300) &\n",
    "            (interactions_df['last_watch_dt'] >= (max_date_df - pd.Timedelta(days=14)))\n",
    "            ].groupby(by='user_id')['item_id'].count()\n",
    "        user_watch_count_all.name = split_name + \"user_watch_cnt_all\"\n",
    "        user_watch_count_last_14.name = split_name + \"user_watch_cnt_last_14\"\n",
    "        user_watches = pd.DataFrame(user_watch_count_all).join(user_watch_count_last_14,\n",
    "                                                            how='outer')\n",
    "        user_watches.fillna(0, inplace=True)\n",
    "        cols = user_watches.columns\n",
    "        user_watches[cols] = user_watches[cols].astype('int64')\n",
    "        users_df = users_df.join(user_watches, on='user_id', how='outer')\n",
    "        users_df[cols] = users_df[cols].fillna(0)\n",
    "        users_df['age'] = users_df['age'].fillna('age_unknown')\n",
    "        users_df['income'] = users_df['income'].fillna('income_unknown')\n",
    "        users_df['sex'] = users_df['sex'].fillna('sex_unknown')\n",
    "        users_df['kids_flg'] = users_df['kids_flg'].fillna(False)\n",
    "        return users_df\n",
    "\n",
    "    @staticmethod\n",
    "    def add_item_watches_stats(interactions_df, items_df, item_stats):\n",
    "        \"\"\"\n",
    "        Computes item watches stats for particular interactions date split\n",
    "        and adds them to item_stats dataframe\n",
    "        \"\"\"\n",
    "\n",
    "        def smooth(series, window_size, smoothing_func):\n",
    "            \"\"\"Computes smoothed interactions statistics for item\"\"\"\n",
    "            series = np.array(series)\n",
    "            ext = np.r_[2 * series[0] - series[window_size - 1::-1],\n",
    "                        series,\n",
    "                        2 * series[-1] - series[-1:-window_size:-1]]\n",
    "            weights = smoothing_func(window_size)\n",
    "            smoothed = np.convolve(weights / weights.sum(), ext, mode='same')\n",
    "            return smoothed[window_size:-window_size + 1]\n",
    "\n",
    "        def trend_slope(series, window_size=7, smoothing_func=np.hamming):\n",
    "            \"\"\"Computes trend slope for item interactions\"\"\"\n",
    "            smoothed = smooth(series, window_size, smoothing_func)\n",
    "            return smoothed[-1] - smoothed[-2]\n",
    "\n",
    "        keep = item_stats.columns\n",
    "        max_date = interactions_df['last_watch_dt'].max()\n",
    "        cols = list(range(7))\n",
    "        for col in cols:\n",
    "            watches = interactions_df[\n",
    "                interactions_df['last_watch_dt'] ==\n",
    "                max_date - pd.Timedelta(days=6 - col)]\n",
    "            item_stats = item_stats.join(\n",
    "                watches.groupby('item_id')['user_id'].count(), lsuffix=col)\n",
    "        item_stats.fillna(0, inplace=True)\n",
    "        new_colnames = ['user_id' + str(i) for i in range(1, 7)] + ['user_id']\n",
    "        trend_slope_to_row = lambda row: trend_slope(row[new_colnames],\n",
    "                                                    window_size=7)\n",
    "        item_stats['trend_slope'] = item_stats.apply(trend_slope_to_row,\n",
    "                                                    axis=1)\n",
    "        item_stats['watched_in_7_days'] = item_stats[new_colnames].apply(\n",
    "            sum, axis=1)\n",
    "        item_stats['watch_ts_quantile_95'] = 0\n",
    "        item_stats['watch_ts_median'] = 0\n",
    "        item_stats['watch_ts_std'] = 0\n",
    "        for item_id in item_stats.index:\n",
    "            watches = interactions_df[interactions_df['item_id'] == item_id]\n",
    "            day_of_year = watches['last_watch_dt'].apply(\n",
    "                lambda x: x.dayofyear).astype(np.int64)\n",
    "            item_stats.loc[item_id, 'watch_ts_quantile_95'] = \\\n",
    "                day_of_year.quantile(q=0.95, interpolation='nearest')\n",
    "            item_stats.loc[item_id, 'watch_ts_median'] = \\\n",
    "                day_of_year.quantile(q=0.5, interpolation='nearest')\n",
    "            item_stats.loc[item_id, 'watch_ts_std'] = day_of_year.std()\n",
    "        item_stats['watch_ts_quantile_95_diff'] = \\\n",
    "            max_date.dayofyear - item_stats['watch_ts_quantile_95']\n",
    "        item_stats['watch_ts_median_diff'] = max_date.dayofyear - \\\n",
    "                                            item_stats['watch_ts_median']\n",
    "        watched_all_time = interactions_df.groupby('item_id')['user_id'].count()\n",
    "        watched_all_time.name = 'watched_in_all_time'\n",
    "        item_stats = item_stats.join(watched_all_time, on='item_id', how='left')\n",
    "        item_stats.fillna(0, inplace=True)\n",
    "        added_cols = ['trend_slope',\n",
    "                    'watched_in_7_days',\n",
    "                    'watch_ts_quantile_95_diff',\n",
    "                    'watch_ts_median_diff',\n",
    "                    'watch_ts_std',\n",
    "                    'watched_in_all_time']\n",
    "        return item_stats[list(keep) + added_cols]\n",
    "\n",
    "    @staticmethod\n",
    "    def add_age_stats(interactions, item_stats, users_df):\n",
    "        \"\"\"\n",
    "        Computes watchers age stats for items with particular interactions \n",
    "        date split and adds them to item_stats dataframe\n",
    "        \"\"\"\n",
    "        item_stats.reset_index(inplace=True)\n",
    "        interactions = interactions.set_index('user_id').join(\n",
    "            users_df[['user_id', 'sex', 'age', 'income']].set_index('user_id'))\n",
    "        interactions.reset_index(inplace=True)\n",
    "        interactions['age_overall'] = interactions['age'].replace(\n",
    "            to_replace={'age_18_24': 'less_35',\n",
    "                        'age_25_34': 'less_35',\n",
    "                        'age_35_44': 'over_35',\n",
    "                        'age_45_54': 'over_35',\n",
    "                        'age_65_inf': 'over_35',\n",
    "                        'age_55_64': 'over_35'})\n",
    "        age_stats = interactions.groupby('item_id')['age_overall'] \\\n",
    "            .value_counts(normalize=True)\n",
    "        age_stats = pd.DataFrame(age_stats)\n",
    "        age_stats.columns = ['value']\n",
    "        age_stats.reset_index(inplace=True)\n",
    "        age_stats.columns = ['item_id', 'age_overall', 'value']\n",
    "        age_stats = age_stats.pivot(\n",
    "            index='item_id', columns='age_overall', values='value').drop(\n",
    "            'age_unknown', axis=1)\n",
    "        age_stats.fillna(0, inplace=True)\n",
    "        item_stats = item_stats.set_index('item_id').join(age_stats)\n",
    "        item_stats[['less_35', 'over_35']] = item_stats[['less_35', 'over_35']] \\\n",
    "            .fillna(0)\n",
    "        item_stats.rename(columns={'less_35': 'younger_35_fraction',\n",
    "                                'over_35': 'older_35_fraction'},\n",
    "                        inplace=True)\n",
    "        return item_stats\n",
    "\n",
    "    @staticmethod\n",
    "    def add_sex_stats(interactions, item_stats, users_df):\n",
    "        \"\"\"\n",
    "        Computes watchers sex stats for items with particular interactions date split\n",
    "        and adds them to item_stats dataframe\n",
    "        \"\"\"\n",
    "        item_stats.reset_index(inplace=True)\n",
    "        interactions = interactions.set_index('user_id') \\\n",
    "            .join(users_df[['user_id', 'sex', 'age', 'income']]\n",
    "                .set_index('user_id'))\n",
    "        interactions.reset_index(inplace=True)\n",
    "        sex_stats = interactions.groupby('item_id')['sex'] \\\n",
    "            .value_counts(normalize=True)\n",
    "        sex_stats = pd.DataFrame(sex_stats)\n",
    "        sex_stats.columns = ['value']\n",
    "        sex_stats.reset_index(inplace=True)\n",
    "        sex_stats.columns = ['item_id', 'sex', 'value']\n",
    "        sex_stats = sex_stats.pivot(index='item_id',\n",
    "                                    columns='sex',\n",
    "                                    values='value').drop('sex_unknown', axis=1)\n",
    "        sex_stats.fillna(0, inplace=True)\n",
    "        item_stats = item_stats.set_index('item_id').join(sex_stats)\n",
    "        item_stats[['F', 'M']] = item_stats[['F', 'M']].fillna(0)\n",
    "        item_stats.rename(columns={'F': 'female_watchers_fraction',\n",
    "                                'M': 'male_watchers_fraction'},\n",
    "                        inplace=True)\n",
    "        return item_stats\n",
    "\n",
    "    @staticmethod\n",
    "    def get_coo_matrix(df,\n",
    "                    user_col='user_id',\n",
    "                    item_col='item_id',\n",
    "                    weight_col=None,\n",
    "                    users_mapping={},\n",
    "                    items_mapping={}):\n",
    "        if weight_col is None:\n",
    "            weights = np.ones(len(df), dtype=np.float32)\n",
    "        else:\n",
    "            weights = df[weight_col].astype(np.float32)\n",
    "\n",
    "        interaction_matrix = sp.coo_matrix((\n",
    "            weights,\n",
    "            (\n",
    "                df[user_col].map(users_mapping.get),\n",
    "                df[item_col].map(items_mapping.get)\n",
    "            )\n",
    "        ))\n",
    "        return interaction_matrix\n",
    "\n",
    "    @staticmethod\n",
    "    def create_mapping():\n",
    "        # Creating items and users mapping\n",
    "        users_inv_mapping = dict(enumerate(df['user_id'].unique()))\n",
    "        users_mapping = {v: k for k, v in users_inv_mapping.items()}\n",
    "        items_inv_mapping = dict(enumerate(df['item_id'].unique()))\n",
    "        items_mapping = {v: k for k, v in items_inv_mapping.items()}\n",
    "\n",
    "    def process(self):\n",
    "        # load data\n",
    "        print('Loading data')\n",
    "        users_df = self.load_users_df()\n",
    "        items_df = self.load_items_df()\n",
    "        interactions_df = self.load_ratings_df()\n",
    "\n",
    "        # users info preprocessing\n",
    "        print('Processing users info')\n",
    "        users_df['age'] = users_df['age'].fillna('age_unknown')\n",
    "        users_df['age'] = users_df['age'].astype('category')\n",
    "        users_df['income'] = users_df['income'].fillna('income_unknown')\n",
    "        users_df['income'] = users_df['income'].astype('category')\n",
    "        users_df['sex'] = users_df['sex'].fillna('sex_unknown')\n",
    "        users_df.loc[users_df.sex == 'М', 'sex'] = 'M'\n",
    "        users_df.loc[users_df.sex == 'Ж', 'sex'] = 'F'\n",
    "        users_df['sex'] = users_df['sex'].astype('category')\n",
    "        users_df['kids_flg'] = users_df['kids_flg'].astype('bool')\n",
    "\n",
    "        # items info preprocessing\n",
    "        print('Processing items info')\n",
    "        items_df['content_type'] = items_df['content_type'].astype('category')\n",
    "        items_df['title'] = items_df['title'].str.lower()\n",
    "        items_df['title_orig'] = items_df['title_orig'].fillna('None')\n",
    "        items_df.loc[items_df['release_year'] < 1980, 'release_novelty'] = 1\n",
    "        items_df.loc[items_df['release_year'] >= 2020, 'release_novelty'] = 6\n",
    "        novelty = 1\n",
    "        for i in range(1980, 2020, 10):\n",
    "            novelty += 1\n",
    "            items_df.loc[(items_df['release_year'] >= i) &\n",
    "                        (items_df['release_year'] < i + 10), 'release_novelty'] = novelty\n",
    "        items_df = items_df.drop(columns=['release_year'])\n",
    "        items_df['for_kids'] = items_df['for_kids'].fillna(0)\n",
    "        items_df['for_kids'] = items_df['for_kids'].astype('bool')\n",
    "        items_df.loc[items_df.age_rating.isna(), 'age_rating'] = 0\n",
    "        items_df['age_rating'] = items_df['age_rating'].astype('category')\n",
    "        items_df['genres_list'] = items_df['genres'].apply(lambda x: x.split(', '))\n",
    "        num_genres = pd.Series(np.hstack(items_df['genres_list'].values)).value_counts()\n",
    "        items_df['genres_min'] = items_df['genres_list'].apply(\n",
    "            lambda x: min([num_genres[el] for el in x]))\n",
    "        items_df['genres_max'] = items_df['genres_list'].apply(\n",
    "            lambda x: max([num_genres[el] for el in x]))\n",
    "        items_df['genres_med'] = items_df['genres_list'].apply(\n",
    "            lambda x: (np.median([num_genres[el] for el in x])))\n",
    "        items_df['countries'].fillna('None', inplace=True)\n",
    "        items_df['countries'] = items_df['countries'].str.lower()\n",
    "        items_df['countries_list'] = items_df['countries'].apply(\n",
    "            lambda x: x.split(', ') if ', ' in x else [x])\n",
    "        num_countries = pd.Series(np.hstack(items_df['countries_list'].values)).value_counts()\n",
    "        items_df['countries_max'] = items_df['countries_list'].apply(\n",
    "            lambda x: max([num_countries[el] for el in x]))\n",
    "        items_df['studios'].fillna('None', inplace=True)\n",
    "        items_df['studios'] = items_df['studios'].str.lower()\n",
    "        items_df['studios_list'] = items_df['studios'].apply(\n",
    "            lambda x: x.split(', ') if ', ' in x else [x])\n",
    "        num_studios = pd.Series(np.hstack(items_df['studios_list'].values)).value_counts()\n",
    "        items_df['studios_max'] = items_df['studios_list'].apply(\n",
    "            lambda x: max([num_studios[el] for el in x]))\n",
    "        items_df.drop(['countries_list', 'genres_list', 'studios_list'],\n",
    "                    axis=1, inplace=True)\n",
    "        \n",
    "        # interactions preprocessing\n",
    "        print('Processing interactions')\n",
    "        interactions_df['watched_pct'] = interactions_df['watched_pct'].astype(\n",
    "            pd.Int8Dtype())\n",
    "        interactions_df['watched_pct'] = interactions_df['watched_pct'].fillna(0)\n",
    "        interactions_df['last_watch_dt'] = pd.to_datetime(\n",
    "            interactions_df['last_watch_dt'])\n",
    "        interactions_df.sort_values(by='last_watch_dt', inplace=True)\n",
    "        \n",
    "        # user stats feature engineering\n",
    "        print('Processing users stats')\n",
    "        max_date = interactions_df['last_watch_dt'].max()\n",
    "        boosting_split_date = max_date - pd.Timedelta(days=14)\n",
    "        interactions_boost = interactions_df[\n",
    "            interactions_df['last_watch_dt'] <= boosting_split_date]\n",
    "        users_df = self.add_user_stats(interactions_boost, users_df, split_name='boost_')\n",
    "        users_df = self.add_user_stats(interactions_df, users_df, split_name='')\n",
    "\n",
    "        # Item stats\n",
    "        print('Processing items stats')\n",
    "        item_stats = items_df[['item_id']]\n",
    "        item_stats = item_stats.set_index('item_id')\n",
    "        item_stats = self.add_item_watches_stats(interactions_boost, items_df, item_stats)\n",
    "        item_stats.fillna(0, inplace=True)\n",
    "        item_stats = self.add_sex_stats(interactions_boost, item_stats, users_df)\n",
    "        item_stats = self.add_age_stats(interactions_boost, item_stats, users_df)\n",
    "\n",
    "        # Saving preprocessed files\n",
    "        users_df.to_csv(self.processed_paths[0], index=False)\n",
    "        items_df.to_csv(self.processed_paths[1], index=False)\n",
    "        interactions_df.to_csv(self.processed_paths[2], index=False)\n",
    "        item_stats.to_csv(self.processed_paths[3], index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "!apt-get -qq install tree\n",
    "!pip install -q watermark\n",
    "!pip install -q pytorch-lightning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading https://github.com/RecoHut-Datasets/mts_kion/raw/v1/users.parquet.snappy\n",
      "Downloading https://github.com/RecoHut-Datasets/mts_kion/raw/v1/items.parquet.snappy\n",
      "Downloading https://github.com/RecoHut-Datasets/mts_kion/raw/v1/interactions.parquet.snappy\n",
      "Processing...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data\n",
      "Processing users info\n",
      "Processing items info\n",
      "Processing interactions\n",
      "Processing users stats\n",
      "Processing items stats\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    }
   ],
   "source": [
    "ds = MTSDataset(data_dir='/content/data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[01;34m./data\u001b[00m\n",
      "├── [250M]  \u001b[01;34mprocessed\u001b[00m\n",
      "│   ├── [161M]  interactions_processed.csv\n",
      "│   ├── [ 31M]  items_processed.csv\n",
      "│   ├── [1.5M]  item_stats_for_boost_train.csv\n",
      "│   ├── [1.5M]  item_stats_for_submit.csv\n",
      "│   └── [ 55M]  users_processed.csv\n",
      "└── [ 77M]  \u001b[01;34mraw\u001b[00m\n",
      "    ├── [ 56M]  interactions.parquet.snappy\n",
      "    ├── [ 15M]  items.parquet.snappy\n",
      "    └── [5.1M]  users.parquet.snappy\n",
      "\n",
      " 327M used in 2 directories, 8 files\n"
     ]
    }
   ],
   "source": [
    "!tree --du -h -C ./data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **References**\n",
    "> - https://ods.ai/tracks/recsys-course2021/competitions/competition-recsys-21\n",
    "> - https://github.com/blondered/ods_MTS_RecSys_Challenge_solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Author: Sparsh A.\n",
      "\n",
      "Last updated: 2022-01-14 10:32:26\n",
      "\n",
      "recohut: 0.0.11\n",
      "\n",
      "Compiler    : GCC 7.5.0\n",
      "OS          : Linux\n",
      "Release     : 5.4.144+\n",
      "Machine     : x86_64\n",
      "Processor   : x86_64\n",
      "CPU cores   : 2\n",
      "Architecture: 64bit\n",
      "\n",
      "IPython: 5.5.0\n",
      "pandas : 1.1.5\n",
      "numpy  : 1.19.5\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "%reload_ext watermark\n",
    "%watermark -a \"Sparsh A.\" -m -iv -u -t -d -p recohut"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
