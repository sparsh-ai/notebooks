{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"T457846 | LightFM Cold-start on ML-10m","provenance":[],"collapsed_sections":[],"toc_visible":true,"authorship_tag":"ABX9TyNU87AcOZbUyiym3nQQkLVW"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"K-3LaItSDleH"},"source":["# LightFM Cold-start on ML-10m"]},{"cell_type":"markdown","metadata":{"id":"U-NnUhl3zfk5"},"source":["## Setup"]},{"cell_type":"markdown","metadata":{"id":"GL7U8z4wEmln"},"source":["### Installations"]},{"cell_type":"code","metadata":{"id":"_7xq83S8BMAN"},"source":["!pip install scikit-learn==0.19.2\n","!pip install lightfm"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4YeMFsAYEn78"},"source":["### Datasets"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"r11m3EgHxjWZ","executionInfo":{"status":"ok","timestamp":1635678419158,"user_tz":-330,"elapsed":8023,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}},"outputId":"2bfdce82-a5a8-4854-810b-4cb591451c84"},"source":["!wget -q --show-progress http://files.grouplens.org/datasets/movielens/ml-10m.zip\n","!wget -q --show-progress http://files.grouplens.org/datasets/tag-genome/tag-genome.zip\n","!unzip ml-10m.zip\n","!unzip tag-genome.zip"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["ml-10m.zip          100%[===================>]  62.53M  41.5MB/s    in 1.5s    \n","tag-genome.zip      100%[===================>]  41.49M  36.6MB/s    in 1.1s    \n","Archive:  ml-10m.zip\n","   creating: ml-10M100K/\n","  inflating: ml-10M100K/allbut.pl    \n","  inflating: ml-10M100K/movies.dat   \n","  inflating: ml-10M100K/ratings.dat  \n","  inflating: ml-10M100K/README.html  \n","  inflating: ml-10M100K/split_ratings.sh  \n","  inflating: ml-10M100K/tags.dat     \n","Archive:  tag-genome.zip\n","   creating: tag-genome/\n","  inflating: tag-genome/README.htm   \n","  inflating: tag-genome/movies.dat   \n","  inflating: tag-genome/tags.dat     \n","  inflating: tag-genome/tag_relevance.dat  \n"]}]},{"cell_type":"markdown","metadata":{"id":"mc5yuRfAEt9S"},"source":["### Imports"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JeivkMQsxrJE","executionInfo":{"status":"ok","timestamp":1635678603628,"user_tz":-330,"elapsed":569,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}},"outputId":"0fbda86a-ea3c-4f03-c023-56d715c9decb"},"source":["import array\n","import collections\n","import numpy as np\n","import os\n","import re\n","import scipy.sparse as sp\n","import subprocess\n","import itertools\n","\n","import logging\n","import logging.handlers\n","import logging.config\n","\n","import json\n","from pprint import pformat\n","import sys\n","\n","from lightfm import LightFM\n","\n","# from sklearn.model_selection import ShuffleSplit\n","from sklearn.cross_validation import ShuffleSplit\n","from sklearn.decomposition import TruncatedSVD\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.metrics import roc_auc_score\n","from sklearn.preprocessing import normalize\n","\n","import matplotlib as mpl\n","import matplotlib.pyplot as plt\n","from matplotlib import rc\n","import seaborn as sns\n","sns.set_palette('Set1')\n","sns.set_style('white')\n","%matplotlib inline"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["[2021-10-31 11:10:00] DEBUG [matplotlib.pyplot:225] Loaded backend module://ipykernel.pylab.backend_inline version unknown.\n"]}]},{"cell_type":"code","metadata":{"id":"HmgBVt4YyCpn"},"source":["SEPARATOR = '::'\n","DATA_DIR = 'ml-10M100K'\n","GENOME_DIR = 'tag-genome'\n","DIMS_RANGE = 10"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"vybo6QEbz7iO"},"source":["FONTSIZE = 7\n","mpl.rcParams['lines.linewidth'] = 1\n","mpl.rcParams['legend.fontsize'] = FONTSIZE\n","\n","DASHES = ['-', '--', '-.', ':']\n","MARKERS = ['.', '^', 'v', 'x', '+']\n","\n","KEYS = ('LSI-LR',\n","        'LSI-UP',\n","        'LightFM (tags)',\n","        'LightFM (tags + ids)',\n","        'LightFM (tags + about)')\n","\n","COLORS = ('#e41a1c',\n","          '#377eb8',\n","          '#4daf4a',\n","          '#984ea3',\n","          '#ff7f00')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"lD2TGLsJzmwA"},"source":["logger = logging.getLogger(__name__)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3vZwLW0azdcU"},"source":["## Utils"]},{"cell_type":"code","metadata":{"id":"JQDtLaB8z-sX"},"source":["def dim_sensitivity_plot(x, Y, fname, show_legend=True):\n","\n","    plt.rc('text', usetex=True)\n","    plt.rc('font', family='serif')\n","\n","    plt.figure(figsize=(3, 3))\n","    plt.xlabel('$d$', size=FONTSIZE)\n","    plt.ylabel('ROC AUC', size=FONTSIZE)\n","\n","    plt.set_cmap('Set2')\n","\n","    lines = []\n","    for i, label in enumerate(KEYS):\n","        line_data = Y.get(label)\n","\n","        if line_data is None:\n","            continue\n","        \n","        line, = plt.plot(x, line_data, label=label, marker=MARKERS[i],\n","                         markersize=0.5 * FONTSIZE, color=COLORS[i])\n","        lines.append(line)\n","\n","\n","\n","    if show_legend:\n","        plt.legend(handles=lines)\n","        plt.legend(loc='lower right')\n","    plt.xscale('log', basex=2)\n","    plt.xticks(x, [str(y) for y in x], size=FONTSIZE)\n","    plt.yticks(size=FONTSIZE)\n","    plt.tight_layout()\n","\n","    plt.savefig(fname)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"u45ikiiezvQs"},"source":["class StratifiedSplit(object):\n","    \"\"\"\n","    Class responsible for producing train-test splits.\n","    \"\"\"\n","\n","    def __init__(self, user_ids, item_ids, n_iter=10, \n","                 test_size=0.2, cold_start=False, random_seed=None):\n","        \"\"\"\n","        Options:\n","        - test_size: the fraction of the dataset to be used as the test set.\n","        - cold_start: if True, test_size of items will be randomly selected to\n","                      be in the test set and removed from the training set. When\n","                      False, test_size of all training pairs are moved to the\n","                      test set.\n","        \"\"\"\n","\n","        self.user_ids = user_ids\n","        self.item_ids = item_ids\n","        self.no_interactions = len(self.user_ids)\n","        self.n_iter = n_iter\n","        self.test_size = test_size\n","        self.cold_start = cold_start\n","\n","        self.shuffle_split = ShuffleSplit(self.no_interactions,\n","                                          n_iter=self.n_iter,\n","                                          test_size=self.test_size)\n","\n","    def _cold_start_iterations(self):\n","        \"\"\"\n","        Performs the cold-start splits.\n","        \"\"\"\n","\n","        for _ in range(self.n_iter):\n","            unique_item_ids = np.unique(self.item_ids)\n","            no_in_test = int(self.test_size * len(unique_item_ids))\n","\n","            item_ids_in_test = set(np.random.choice(unique_item_ids, size=no_in_test))\n","\n","            test_indices = array.array('i')\n","            train_indices = array.array('i')\n","\n","            for i, item_id in enumerate(self.item_ids):\n","                if item_id in item_ids_in_test:\n","                    test_indices.append(i)\n","                else:\n","                    train_indices.append(i)\n","\n","            train = np.frombuffer(train_indices, dtype=np.int32)\n","            test = np.frombuffer(test_indices, dtype=np.int32)\n","\n","            # Shuffle data.\n","            np.random.shuffle(train)\n","            np.random.shuffle(test)\n","\n","            yield train, test\n","\n","    def __iter__(self):\n","\n","        if self.cold_start:\n","            splits = self._cold_start_iterations()           \n","        else:\n","            splits = self.shuffle_split\n","\n","        for train, test in splits:\n","\n","            # Make sure that all the users in test\n","            # are represented in train.\n","            user_ids_in_train = collections.defaultdict(lambda: 0)\n","            item_ids_in_train = collections.defaultdict(lambda: 0)\n","\n","            for uid in self.user_ids[train]:\n","                user_ids_in_train[uid] += 1\n","\n","            for iid in self.item_ids[train]:\n","                item_ids_in_train[iid] += 1\n","\n","            if self.cold_start:\n","                test = [x for x in test if self.user_ids[x] in user_ids_in_train]\n","            else:\n","                # For the non-cold start scenario, make sure that both users\n","                # and items are represented in the train set.\n","                test = [x for x in test if (self.user_ids[x] in user_ids_in_train\n","                                            and self.item_ids[x] in item_ids_in_train)]\n","\n","            test = np.array(test)\n","\n","            yield train, test\n","\n","\n","def stratified_roc_auc_score(y, yhat, user_indices):\n","    \"\"\"\n","    Compute ROC AUC for each user individually, then average.\n","    \"\"\"\n","\n","    aucs = []\n","\n","    y_dict = collections.defaultdict(lambda: array.array('d'))\n","    yhat_dict = collections.defaultdict(lambda: array.array('d'))\n","\n","    for i, uid in enumerate(user_indices):\n","        y_dict[uid].append(y[i])\n","        yhat_dict[uid].append(yhat[i])\n","\n","    for uid in y_dict:\n","\n","        user_y = np.frombuffer(y_dict[uid], dtype=np.float64)\n","        user_yhat = np.frombuffer(yhat_dict[uid], dtype=np.float64)\n","\n","        if len(user_y) and len(user_yhat) and len(np.unique(user_y)) == 2:\n","            aucs.append(roc_auc_score(user_y, user_yhat))\n","\n","    logger.debug('%s users in stratified ROC AUC evaluation.', len(aucs))\n","    \n","    return np.mean(aucs)\n","\n","\n","def build_user_feature_matrix(user_ids):\n","\n","    n = len(user_ids)\n","\n","    return sp.coo_matrix((np.ones(n, dtype=np.int32), (np.arange(n), user_ids))).tocsr()\n","\n","\n","def fit_model(interactions, item_features_matrix,\n","              n_iter, epochs, modelfnc, test_size,\n","              cold_start, user_features_matrix=None):\n","    \"\"\"\n","    Fits the model provided by modelfnc.\n","    \"\"\"\n","\n","    kf = StratifiedSplit(interactions.user_id, interactions.item_id,\n","                         n_iter=n_iter, test_size=test_size, cold_start=cold_start)\n","\n","    logger.debug('Interaction density across all data: %s',\n","                 (float(len(interactions.data)) / (len(interactions.user_ids)\n","                                                   * len(interactions.item_ids))))\n","    logger.debug('Training model')\n","\n","    # Store ROC AUC scores for all iterations.\n","    aucs = []\n","\n","    # Iterate over train-test splits.\n","    for i, (train, test) in enumerate(kf):\n","\n","        logger.debug('Split no %s', i)\n","        logger.debug('%s examples in training set, %s in test set. Interaction density: %s',\n","                    len(train), len(test), float(len(train)) / (len(interactions.user_ids)\n","                                                                * len(interactions.item_ids)))\n","\n","        # For every split, get a new model instance.\n","        model = modelfnc()\n","\n","        if isinstance(model, CFModel):\n","            logger.debug('Evaluating a CF model')\n","            test_auc, train_auc = evaluate_cf_model(model,\n","                                                    item_features_matrix,\n","                                                    interactions.user_id[train],\n","                                                    interactions.item_id[train],\n","                                                    interactions.data[train],\n","                                                    interactions.user_id[test],\n","                                                    interactions.item_id[test],\n","                                                    interactions.data[test])\n","            logger.debug('CF model test AUC %s, train AUC %s', test_auc, train_auc)\n","            aucs.append(test_auc)\n","\n","        elif isinstance(model, LsiUpModel):\n","            logger.debug('Evaluating a LSI-UP model')\n","\n","            # Prepare data.\n","            y = interactions.data\n","            no_users = np.max(interactions.user_id) + 1\n","            no_items = item_features_matrix.shape[0]\n","\n","            train_user_ids = interactions.user_id[train]\n","            train_item_ids = interactions.item_id[train]\n","\n","            user_features = sp.coo_matrix((interactions.data[train],\n","                                           (train_user_ids, train_item_ids)),\n","                                           shape=(no_users, no_items)).tocsr()\n","            user_feature_matrix = user_features * item_features_matrix\n","\n","            # Fit model.\n","            model.fit(user_feature_matrix, item_features_matrix)\n","            \n","            # For larger datasets use incremental prediction. Slower, but\n","            # fits in far less memory.\n","            if len(train) or len(test) > 200000:\n","                train_predictions = model.predict(interactions.user_id[train],\n","                                                  interactions.item_id[train],\n","                                                  incremental=True)\n","                test_predictions = model.predict(interactions.user_id[test],\n","                                                 interactions.item_id[test],\n","                                                 incremental=True)\n","            else:\n","                train_predictions = model.predict(interactions.user_id[train],\n","                                                  interactions.item_id[train])\n","                test_predictions = model.predict(interactions.user_id[test],\n","                                                 interactions.item_id[test])\n","\n","            # Compute mean ROC AUC scores on both test and train data.\n","            train_auc = stratified_roc_auc_score(y[train],\n","                                                 train_predictions,\n","                                                 interactions.user_id[train])\n","            test_auc = stratified_roc_auc_score(y[test],\n","                                                test_predictions,\n","                                                interactions.user_id[test])\n","\n","            logger.debug('Test AUC %s, train AUC %s', test_auc, train_auc)\n","\n","            aucs.append(test_auc)\n","\n","        else:\n","            # LightFM and MF models using the LightFM implementation.\n","            if user_features_matrix is not None:\n","                user_features = user_features_matrix\n","            else:\n","                user_features = build_user_feature_matrix(interactions.user_id)\n","\n","            item_features = item_features_matrix\n","\n","            previous_auc = 0.0\n","\n","            interactions.data[interactions.data == 0] = -1\n","\n","            train_interactions = sp.coo_matrix((interactions.data[train],\n","                                                (interactions.user_id[train],\n","                                                 interactions.item_id[train])))\n","\n","            # Run for a maximum of epochs epochs.\n","            # Stop if the test score starts falling, take the best result.\n","            for x in range(epochs):\n","                model.fit_partial(train_interactions,\n","                                  item_features=item_features,\n","                                  user_features=user_features,\n","                                  epochs=1, num_threads=1)\n","\n","                train_predictions = model.predict(interactions.user_id[train],\n","                                                  interactions.item_id[train],\n","                                                  user_features=user_features,\n","                                                  item_features=item_features,\n","                                                  num_threads=4)\n","                test_predictions = model.predict(interactions.user_id[test],\n","                                                 interactions.item_id[test],\n","                                                 user_features=user_features,\n","                                                 item_features=item_features,\n","                                                 num_threads=4)\n","\n","                train_auc = stratified_roc_auc_score(interactions.data[train],\n","                                                     train_predictions,\n","                                                     interactions.user_id[train])\n","                test_auc = stratified_roc_auc_score(interactions.data[test],\n","                                                    test_predictions,\n","                                                    interactions.user_id[test])\n","\n","                logger.debug('Epoch %s, test AUC %s, train AUC %s', x, test_auc, train_auc)\n","\n","                if previous_auc > test_auc:\n","                    break\n","\n","                previous_auc = test_auc\n","\n","            aucs.append(previous_auc)\n","\n","    return model, np.mean(aucs)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"dvhfH-CEyjsk"},"source":["LOGGING = {\n","    'version': 1,\n","    'disable_existing_loggers': False,\n","    'formatters': {\n","        'verbose': {\n","            'format': \"[%(asctime)s] %(levelname)s [%(name)s:%(lineno)s] %(message)s\",\n","            'datefmt': \"%Y-%m-%d %H:%M:%S\"\n","        },\n","        'simple': {\n","            'format': '%(levelname)s %(message)s'\n","        },\n","    },\n","    'handlers': {\n","        'console': {\n","            'level': 'DEBUG',\n","            'class': 'logging.StreamHandler',\n","            'formatter': 'verbose'\n","        },\n","        'file': {\n","            'level': 'INFO',\n","            'class': 'logging.handlers.RotatingFileHandler',\n","            'formatter': 'verbose',\n","            'filename': 'model.log',\n","            'maxBytes': 10*10**6,\n","            'backupCount': 3\n","            }\n","    },\n","    'loggers': {\n","        '': {\n","            'handlers': ['console', 'file'],\n","            'level': 'DEBUG',\n","        },\n","    }\n","}\n","\n","\n","logging.config.dictConfig(LOGGING)\n","\n","\n","def getLogger(name):\n","\n","    return logging.getLogger(name)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hXXKI94Cx8LS"},"source":["class IncrementalCOOMatrix(object):\n","\n","    def __init__(self, dtype):\n","\n","        if dtype is np.int32:\n","            type_flag = 'i'\n","        elif dtype is np.int64:\n","            type_flag = 'l'\n","        elif dtype is np.float32:\n","            type_flag = 'f'\n","        elif dtype is np.float64:\n","            type_flag = 'd'\n","        else:\n","            raise Exception('Dtype not supported.')\n","\n","        self.dtype = dtype\n","        self.shape = None\n","\n","        self.rows = array.array('i')\n","        self.cols = array.array('i')\n","        self.data = array.array(type_flag)\n","\n","    def append(self, i, j, v):\n","\n","        self.rows.append(i)\n","        self.cols.append(j)\n","        self.data.append(v)\n","\n","    def tocoo(self):\n","\n","        rows = np.frombuffer(self.rows, dtype=np.int32)\n","        cols = np.frombuffer(self.cols, dtype=np.int32)\n","        data = np.frombuffer(self.data, dtype=self.dtype)\n","\n","        self.shape = self.shape or (np.max(rows) + 1, np.max(cols) + 1)\n","\n","        return sp.coo_matrix((data, (rows, cols)),\n","                             shape=self.shape)\n","\n","    def __len__(self):\n","\n","        return len(self.data)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"pc4UX5OQx_CD"},"source":["class Features(object):\n","\n","    def __init__(self):\n","\n","        self.feature_ids = {}\n","        self.item_ids = {}\n","        self.title_mapping = {}\n","\n","        self.mat = IncrementalCOOMatrix(np.int32)\n","\n","    def add_item(self, item_id):\n","\n","        iid = self.item_ids.setdefault(item_id, len(self.item_ids))\n","        \n","    def add_feature(self, item_id, feature):\n","\n","        iid = self.item_ids.setdefault(item_id, len(self.item_ids))\n","\n","        feature_id = self.feature_ids.setdefault(feature, len(self.feature_ids))\n","\n","        self.mat.append(iid, feature_id, 1)\n","\n","    def add_title(self, item_id, title):\n","\n","        iid = self.item_ids.setdefault(item_id, len(self.item_ids))\n","        self.title_mapping[iid] = title\n","\n","    def set_shape(self):\n","\n","        self.mat.shape = len(self.item_ids), len(self.feature_ids)\n","\n","    def add_latent_representations(self, latent_representations):\n","\n","        dim = latent_representations.shape[1]\n","        lrepr = np.zeros((len(self.title_mapping), dim),\n","                         dtype=np.float32)\n","\n","        for i, row in enumerate(self.mat.tocoo().tocsr()):\n","            lrepr[i] = np.sum(latent_representations[row.indices], axis=0)\n","\n","        self.lrepr = lrepr\n","        self.inverse_title_mapping = {v: k for k, v in self.title_mapping.items()}\n","\n","    def most_similar_movie(self, title, number=5):\n","\n","        iid = self.inverse_title_mapping[title]\n","\n","        vector = self.lrepr[iid]\n","\n","        dst = (np.dot(self.lrepr, vector)\n","               / np.linalg.norm(self.lrepr, axis=1) / np.linalg.norm(vector))\n","        movie_ids = np.argsort(-dst)\n","        \n","        return [(self.title_mapping[x], dst[x]) for x in movie_ids[:number]\n","                if x in self.title_mapping]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"uf6FRejKx9Gy"},"source":["class Interactions(object):\n","\n","    def __init__(self, item_ids):\n","\n","        self.item_ids = item_ids\n","        self.user_ids = {}\n","\n","        self.user_data = collections.defaultdict(lambda: {1: array.array('i'),\n","                                                          0: array.array('i')})\n","\n","        self.iids_sample_pool = np.array(item_ids.values())\n","\n","        self._user_id = array.array('i')\n","        self._item_id = array.array('i')\n","        self._data = array.array('i')\n","\n","    def add(self, user_id, item_id, value):\n","\n","        iid = self.item_ids[item_id]\n","        user_id = self.user_ids.setdefault(user_id, len(self.user_ids))\n","\n","        self.user_data[user_id][value].append(iid)\n","\n","    def fit(self, min_positives=1, sampled_negatives_ratio=0, use_observed_negatives=True):\n","        \"\"\"\n","        Constructs the training data set from raw interaction data.\n","        Parameters:\n","        - min_positives: users with fewer than min_positives interactions are excluded\n","                         from the training set\n","        - sampled_negatives_ratio: a ratio of 3 means that at most three negative examples\n","                         randomly sampled for the pids_sample_pool will be included.\n","        \"\"\"\n","\n","        for user_id, user_data in self.user_data.items():\n","\n","            positives = user_data.get(1, [])\n","            raw_negatives = user_data.get(0, [])\n","\n","            if len(positives) < min_positives:\n","                continue\n","\n","            if use_observed_negatives:\n","                observed_negatives = list(set(raw_negatives) - set(positives))\n","            else:\n","                observed_negatives = []\n","\n","            if sampled_negatives_ratio:\n","                sampled_negatives = np.random.choice(self.iids_sample_pool,\n","                                                     size=len(positives) * sampled_negatives_ratio)\n","                sampled_negatives = list(set(sampled_negatives) - set(positives))\n","            else:\n","                sampled_negatives = []\n","\n","            for value, pids in zip((1, 0, 0), (positives, observed_negatives, sampled_negatives)):\n","                for pid in pids:\n","                    self._user_id.append(user_id)\n","                    self._item_id.append(pid)\n","                    self._data.append(value)\n","\n","        self.user_id = np.frombuffer(self._user_id, dtype=np.int32)\n","        self.item_id = np.frombuffer(self._item_id, dtype=np.int32)\n","        self.data = np.frombuffer(self._data, dtype=np.int32)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"92RB3N3WyUuJ"},"source":["def read_genome_tags(min_popularity=20):\n","\n","    tag_dict = {}\n","\n","    with open(os.path.join(GENOME_DIR, 'tags.dat'), 'r') as tagfile:\n","        for line in tagfile:\n","\n","            tag_id, tag, popularity = line.split('\\t')\n","\n","            if int(popularity) >= min_popularity:\n","                tag_dict[int(tag_id)] = tag\n","\n","    with open(os.path.join(GENOME_DIR, 'tag_relevance.dat'), 'r') as tagfile:\n","        for line in tagfile:\n","\n","            iid, tag_id, relevance = line.split('\\t')\n","\n","            if int(tag_id) in tag_dict:\n","                yield iid, tag_dict[int(tag_id)], float(relevance)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-nXiZ2HtyS8l"},"source":["def _process_raw_tag(tag):\n","\n","    tag = re.sub('[^a-zA-Z]+', ' ', tag.lower()).strip()\n","\n","    return tag"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_6VqecWOyRU5"},"source":["def read_tags():\n","\n","    tag_dict = collections.defaultdict(lambda: 0)\n","\n","    with open(os.path.join(DATA_DIR, 'tags.dat'), 'r') as tagfile:\n","        for line in tagfile:\n","\n","            uid, iid, tag, timestamp = line.split(SEPARATOR)\n","            processed_tag = _process_raw_tag(tag)\n","            tag_dict[tag] += 1\n","\n","    with open(os.path.join(DATA_DIR, 'tags.dat'), 'r') as tagfile:\n","        for line in tagfile:\n","\n","            uid, iid, tag, timestamp = line.split(SEPARATOR)\n","            processed_tag = _process_raw_tag(tag)\n","            tag_count = tag_dict[processed_tag]\n","\n","            yield iid, processed_tag, tag_count"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"j2N6l6zDyPF3"},"source":["def read_movie_features(titles=False, genres=False, genome_tag_threshold=1.0, tag_popularity_threshold=30):\n","\n","    features = Features()\n","\n","    with open(os.path.join(DATA_DIR, 'movies.dat'), 'r') as moviefile:\n","        for line in moviefile:\n","            (iid, title, genre_list) = line.split(SEPARATOR)\n","            genres_list = genre_list.split('|')\n","\n","            features.add_item(iid)\n","\n","            if genres:\n","                for genre in genres_list:\n","                    features.add_feature(iid, 'genre:' + genre.lower().replace('\\n', ''))\n","\n","            if titles:\n","                features.add_feature(iid, 'title:' + title.lower())\n","\n","            features.add_title(iid, title)\n","\n","    for iid, tag, relevance in read_genome_tags():\n","        # Do not include any tags for movies not in the 10M dataset\n","        if relevance >= genome_tag_threshold and iid in features.item_ids:\n","            features.add_feature(iid, 'genome:' + tag.lower())\n","\n","    # Tags applied by users\n","    ## for iid, tag, count in read_tags():\n","    ##     if count >= tag_popularity_threshold and iid in features.item_ids:\n","    ##         features.add_feature(iid, 'tag:' + tag)\n","\n","    features.set_shape()\n","\n","    return features"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2ZcNWAPWyJs7"},"source":["def read_interaction_data(item_id_mapping, positive_threshold=4.0):\n","\n","    interactions = Interactions(item_id_mapping)\n","\n","    with open(os.path.join(DATA_DIR, 'ratings.dat'), 'r') as ratingfile:\n","        for line in ratingfile:\n","\n","            (uid, iid, rating, timestamp) = line.split(SEPARATOR)\n","\n","            value = 1.0 if float(rating) >= positive_threshold else 0.0\n","\n","            interactions.add(uid, iid, value)\n","\n","    return interactions"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vR3GV96ty4Lf"},"source":["## CF Model"]},{"cell_type":"code","metadata":{"id":"Cj5VHV_8y5TT"},"source":["class CFModel(object):\n","    \"\"\"\n","    The LSI-LR model.\n","    \"\"\"\n","\n","    def __init__(self, dim=64):\n","\n","        self.dim = dim\n","        self.model = None\n","        self.item_latent_features = None\n","\n","    def fit_svd(self, mat):\n","        \"\"\"\n","        Fit the feature latent factors.\n","        \"\"\"\n","\n","        model = TruncatedSVD(n_components=self.dim)\n","        model.fit(mat)\n","\n","        self.model = model\n","\n","    def fit_latent_features(self, feature_matrix):\n","        \"\"\"\n","        Project items into the latent space.\n","        \"\"\"\n","\n","        self.item_latent_features = self.model.transform(feature_matrix)\n","\n","    def fit_user(self, item_ids, y):\n","        \"\"\"\n","        Fit a logistic regression model for a single user.\n","        \"\"\"\n","\n","        model = LogisticRegression()\n","        model.fit(self.item_latent_features[item_ids], y)\n","\n","        return model\n","\n","    def predict_user(self, model, item_ids):\n","        \"\"\"\n","        Predict positive interaction probability for user represented by model.\n","        \"\"\"\n","\n","        return model.decision_function(self.item_latent_features[item_ids])\n","\n","\n","\n","def evaluate_cf_model(model, feature_matrix, train_user_ids, train_item_ids, train_data,\n","                      test_user_ids, test_item_ids, test_data):\n","    \"\"\"\n","    LSI-LR model: perform LSI (via truncated SVD on the item-feature matrix), then computer user models\n","    by fitting a logistic regression model to items represented as mixtures of LSI topics.\n","    \"\"\"\n","\n","    train_aucs = []\n","    test_aucs = []\n","\n","    train_y_dict = collections.defaultdict(lambda: array.array('d'))\n","    train_iid_dict = collections.defaultdict(lambda: array.array('i'))\n","\n","    test_y_dict = collections.defaultdict(lambda: array.array('d'))\n","    test_iid_dict = collections.defaultdict(lambda: array.array('i'))\n","\n","    # Gather training data in user-sized chunks\n","    for i, (uid, iid, y) in enumerate(zip(train_user_ids, train_item_ids, train_data)):\n","        train_y_dict[uid].append(y)\n","        train_iid_dict[uid].append(iid)\n","\n","    # Gather test data in user-sized chunks\n","    for i, (uid, iid, y) in enumerate(zip(test_user_ids, test_item_ids, test_data)):\n","        test_y_dict[uid].append(y)\n","        test_iid_dict[uid].append(iid)\n","\n","    # Only use the items in the training set for LSI\n","    model.fit_svd(feature_matrix[np.unique(train_item_ids)])\n","    model.fit_latent_features(feature_matrix)\n","\n","    # Fit models and generate predictions\n","    for uid in train_y_dict:\n","        train_iids = np.frombuffer(train_iid_dict[uid], dtype=np.int32)\n","        train_y = np.frombuffer(train_y_dict[uid], dtype=np.float64)\n","\n","        test_iids = np.frombuffer(test_iid_dict[uid], dtype=np.int32)\n","        test_y = np.frombuffer(test_y_dict[uid], dtype=np.float64)\n","\n","        if len(np.unique(test_y)) == 2 and len(np.unique(train_y)) == 2:\n","            user_model = model.fit_user(train_iids, train_y)\n","            train_yhat = model.predict_user(user_model, train_iids)\n","            test_yhat = model.predict_user(user_model, test_iids)\n","            \n","            train_aucs.append(roc_auc_score(train_y, train_yhat))\n","            test_aucs.append(roc_auc_score(test_y, test_yhat))\n","\n","    return np.mean(test_aucs), np.mean(train_aucs)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"pjvavJs8zMAu"},"source":["## LSI-UP Model"]},{"cell_type":"code","metadata":{"id":"AdqbqyMCzOlY"},"source":["class LsiUpModel(object):\n","    \"\"\"\n","    The LSI-UP model.\n","    \"\"\"\n","\n","    def __init__(self, dim=64):\n","\n","        self.dim = dim\n","        self.user_factors = None\n","        self.item_factors = None\n","\n","    def fit(self, user_feature_matrix, product_feature_matrix):\n","        \"\"\"\n","        Fit latent factors to the user-feature matrix through truncated SVD,\n","        then get item representations by projecting onto the latent feature\n","        space.\n","        \"\"\"\n","\n","        nrm = lambda x: normalize(x.astype(np.float64), norm='l2', axis=1)\n","\n","        svd = TruncatedSVD(n_components=self.dim)\n","        svd.fit(nrm(user_feature_matrix))\n","\n","        self.user_factors = svd.transform(nrm(user_feature_matrix))\n","        self.item_factors = svd.transform(nrm(product_feature_matrix))\n","\n","    def predict(self, user_ids, product_ids, incremental=False):\n","        \"\"\"\n","        Predict scores.\n","        \"\"\"\n","\n","        if not incremental:\n","            return np.inner(self.user_factors[user_ids],\n","                            self.item_factors[product_ids])\n","        else:\n","            result = array.array('f')\n","            \n","            for i in range(len(user_ids)):\n","                uid = user_ids[i]\n","                pid = product_ids[i]\n","\n","                result.append(np.dot(self.user_factors[uid],\n","                                     self.item_factors[pid]))\n","\n","            return np.frombuffer(result, dtype=np.float32)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"JS2xbTvY0aX_"},"source":["## Main"]},{"cell_type":"code","metadata":{"id":"2VRPSChH0eGW"},"source":["def read_data(titles, genres,\n","              genome_tag_threshold,\n","              positive_threshold):\n","\n","    logger.debug('Reading features')\n","    features = read_movie_features(titles=titles, genres=genres, genome_tag_threshold=genome_tag_threshold)\n","    item_features_matrix = features.mat.tocoo().tocsr()\n","\n","    logger.debug('Reading interactions')\n","    interactions = read_interaction_data(features.item_ids,\n","                                         positive_threshold=positive_threshold)\n","    interactions.fit(min_positives=1, sampled_negatives_ratio=0, use_observed_negatives=True)\n","\n","    logger.debug('%s users, %s items, %s interactions, %s item features in the dataset',\n","                len(interactions.user_ids), len(features.item_ids),\n","                len(interactions.data), len(features.feature_ids))\n","\n","    return features, item_features_matrix, interactions"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"btIC7wB20f4o"},"source":["def run(features,\n","        item_features_matrix,\n","        interactions,\n","        cf_model,\n","        lsiup_model,\n","        n_iter,\n","        test_size,\n","        cold_start,\n","        learning_rate,\n","        no_components,\n","        a_alpha,\n","        b_alpha,\n","        epochs):\n","\n","    logger.debug('Fitting the model with %s', locals())\n","\n","    no_interactions = len(interactions.data)\n","\n","    if cf_model:\n","        logger.info('Fitting the CF model')\n","        modelfnc = lambda: CFModel(dim=no_components)\n","    elif lsiup_model:\n","        logger.info('Fitting the LSI-UP model')\n","        modelfnc = lambda: LsiUpModel(dim=no_components)\n","    else:\n","        modelfnc = lambda: LightFM(learning_rate=learning_rate,\n","                                    no_components=no_components,\n","                                    item_alpha=a_alpha,\n","                                    user_alpha=b_alpha)\n","\n","    model, auc = fit_model(interactions=interactions,\n","                           item_features_matrix=item_features_matrix, \n","                           n_iter=n_iter,\n","                           epochs=epochs,\n","                           modelfnc=modelfnc,\n","                           test_size=test_size,\n","                           cold_start=cold_start)\n","    logger.debug('Average AUC: %s', auc)\n","\n","    if not cf_model and not lsiup_model:\n","        model.add_item_feature_dictionary(features.feature_ids, check=False)\n","        features.add_latent_representations(model.item_features)\n","\n","        titles = ('Lord of the Rings: The Two Towers, The (2002)',\n","                  'Toy Story (1995)',\n","                  'Terminator, The (1984)',\n","                  'Europa Europa (Hitlerjunge Salomon) (1990)')\n","\n","        for title in titles:\n","            logger.debug('Most similar movies to %s: %s', title,\n","                        features.most_similar_movie(title, number=20))\n","\n","            # Can only get similar tags if we have tag features\n","        test_features = ('genome:art house',\n","                         'genome:dystopia',\n","                         'genome:bond')\n","\n","        for test_feature in test_features:\n","            try:\n","                logger.debug('Features most similar to %s: %s',\n","                             test_feature,\n","                             model.most_similar(test_feature, 'item', number=10))\n","            except KeyError:\n","                pass\n","\n","    return auc"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"H6dEhvia1c5E"},"source":["class Args:\n","    ids = False\n","    tags = False\n","    split = 0.2\n","    cold = False\n","    lsi = False\n","    up = False\n","    dim = (64,)\n","    niter = 5\n","    plot = False\n","    table = False"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7AZCdEP20iU9"},"source":["def main(args):\n","\n","    logger.info('Running the MovieLens experiment.')\n","    logger.info('Configuration: %s', pformat(args))\n","\n","    # A large tag threshold excludes all tags.\n","    tag_threshold = 0.8 if args.tags else 100.0\n","    features, item_features_matrix, interactions = read_data(titles=args.ids,\n","                                                             genres=False,\n","                                                             genome_tag_threshold=tag_threshold,\n","                                                             positive_threshold=4.0)\n","\n","    results = {}\n","    \n","    for dim in args.dim:\n","        auc = run(features,\n","                  item_features_matrix,\n","                  interactions,\n","                  cf_model=args.lsi,\n","                  lsiup_model=args.up,\n","                  n_iter=args.niter,\n","                  test_size=args.split,\n","                  cold_start=args.cold,\n","                  learning_rate=0.05,\n","                  no_components=int(dim),\n","                  a_alpha=0.0,\n","                  b_alpha=0.0,\n","                  epochs=30)\n","\n","        results[int(dim)] = auc\n","        logger.info('AUC %s for configuration %s', auc, pformat(args))\n","\n","    sys.stdout.write(json.dumps(results))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OYDCe2cw2gQS","executionInfo":{"status":"ok","timestamp":1635680001640,"user_tz":-330,"elapsed":1131125,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}},"outputId":"754fbe9e-90a4-4b73-9ad3-cd7b4be136e5"},"source":["# run the CrossValidated experiment with 50-dimensional latent space, \n","# using the LSI-LR model with both post tags and post ids\n","args = Args()\n","args.dim = (50,)\n","args.lsi = True\n","args.tags = True\n","args.ids = True\n","args.split = 0.2\n","main(args)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["[2021-10-31 11:14:28] INFO [__main__:3] Running the MovieLens experiment.\n","[2021-10-31 11:14:28] INFO [__main__:4] Configuration: <__main__.Args object at 0x7efd8835eb10>\n","[2021-10-31 11:14:28] DEBUG [__main__:5] Reading features\n","[2021-10-31 11:14:40] DEBUG [__main__:9] Reading interactions\n","[2021-10-31 11:15:02] DEBUG [__main__:16] 69878 users, 10681 items, 9996948 interactions, 11689 item features in the dataset\n","[2021-10-31 11:15:02] DEBUG [__main__:15] Fitting the model with {'features': <__main__.Features object at 0x7efd80753ed0>, 'item_features_matrix': <10681x11689 sparse matrix of type '<class 'numpy.int32'>'\n","\twith 93434 stored elements in Compressed Sparse Row format>, 'interactions': <__main__.Interactions object at 0x7efd8835e290>, 'cf_model': True, 'lsiup_model': False, 'n_iter': 5, 'test_size': 0.2, 'cold_start': False, 'epochs': 30, 'a_alpha': 0.0, 'b_alpha': 0.0, 'learning_rate': 0.05, 'no_components': 50}\n","[2021-10-31 11:15:02] INFO [__main__:20] Fitting the CF model\n","[2021-10-31 11:15:02] DEBUG [__main__:136] Interaction density across all data: 0.013394146711095253\n","[2021-10-31 11:15:02] DEBUG [__main__:137] Training model\n","[2021-10-31 11:15:12] DEBUG [__main__:145] Split no 0\n","[2021-10-31 11:15:12] DEBUG [__main__:148] 7997558 examples in training set, 1999363 in test set. Interaction density: 0.010715316832946768\n","[2021-10-31 11:15:12] DEBUG [__main__:154] Evaluating a CF model\n","[2021-10-31 11:18:42] DEBUG [__main__:163] CF model test AUC 0.6856567219338295, train AUC 0.9436195928332399\n","[2021-10-31 11:18:52] DEBUG [__main__:145] Split no 1\n","[2021-10-31 11:18:52] DEBUG [__main__:148] 7997558 examples in training set, 1999342 in test set. Interaction density: 0.010715316832946768\n","[2021-10-31 11:18:52] DEBUG [__main__:154] Evaluating a CF model\n","[2021-10-31 11:22:21] DEBUG [__main__:163] CF model test AUC 0.6865419685612636, train AUC 0.9440696661353846\n","[2021-10-31 11:22:30] DEBUG [__main__:145] Split no 2\n","[2021-10-31 11:22:30] DEBUG [__main__:148] 7997558 examples in training set, 1999347 in test set. Interaction density: 0.010715316832946768\n","[2021-10-31 11:22:30] DEBUG [__main__:154] Evaluating a CF model\n","[2021-10-31 11:25:58] DEBUG [__main__:163] CF model test AUC 0.6861023147957814, train AUC 0.9440572387876923\n","[2021-10-31 11:26:07] DEBUG [__main__:145] Split no 3\n","[2021-10-31 11:26:07] DEBUG [__main__:148] 7997558 examples in training set, 1999368 in test set. Interaction density: 0.010715316832946768\n","[2021-10-31 11:26:07] DEBUG [__main__:154] Evaluating a CF model\n","[2021-10-31 11:29:36] DEBUG [__main__:163] CF model test AUC 0.6848108725497167, train AUC 0.944204423470421\n","[2021-10-31 11:29:46] DEBUG [__main__:145] Split no 4\n","[2021-10-31 11:29:46] DEBUG [__main__:148] 7997558 examples in training set, 1999363 in test set. Interaction density: 0.010715316832946768\n","[2021-10-31 11:29:46] DEBUG [__main__:154] Evaluating a CF model\n","[2021-10-31 11:33:18] DEBUG [__main__:163] CF model test AUC 0.6861188501319503, train AUC 0.9442085502709073\n","[2021-10-31 11:33:18] DEBUG [__main__:38] Average AUC: 0.6858461455945083\n","[2021-10-31 11:33:18] INFO [__main__:31] AUC 0.6858461455945083 for configuration <__main__.Args object at 0x7efd8835eb10>\n"]},{"output_type":"stream","name":"stdout","text":["{\"50\": 0.6858461455945083}"]}]},{"cell_type":"markdown","metadata":{"id":"p_GdOBsQDhUS"},"source":["## Citations\n","\n","Metadata Embeddings for User and Item Cold-start Recommendations. Maciej Kula. 2015. arXiv. [https://arxiv.org/abs/1507.08439](https://arxiv.org/abs/1507.08439)"]}]}