{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"2022-01-19-incremental-mf-river.ipynb","provenance":[{"file_id":"https://github.com/recohut/nbs/blob/main/raw/T012987%20%7C%20Incremental%20Matrix%20Factorization%20on%20ML-100k%20using%20River%20library.ipynb","timestamp":1644650748579},{"file_id":"1gdO4oSScAXyHRez1XeMofQfPffW8T4pA","timestamp":1635162310216},{"file_id":"1G40fgd4CmrKoFr3NRjx4vb6XhshK5MvJ","timestamp":1630503172942},{"file_id":"1FkJq95_JuB_b2VNoBPc-0xgUM9lvRlxi","timestamp":1630498362846}],"collapsed_sections":[],"mount_file_id":"1Tm-h7CFnh71CxUp_pjlevWSsY_cyUedr","authorship_tag":"ABX9TyN0SsrQ6Q26xfoh0qOyq5fv"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"GZLHDPT7RTqF"},"source":["# Incremental Matrix Factorization on ML-100k using River library"]},{"cell_type":"markdown","metadata":{"id":"m0akdB4nRYML"},"source":["## Setup"]},{"cell_type":"code","metadata":{"id":"It-vcW2kIjMr"},"source":["!pip install -U river numpy"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"XifIn1BoVT7e"},"source":["Restart the session at this point!"]},{"cell_type":"code","metadata":{"id":"sw24c-Z0JpdE"},"source":["import json\n","import river\n","from river.evaluate import progressive_val_score"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jzdwEeHDVYfI"},"source":["## Data"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yWmyT_dDVZ2Y","executionInfo":{"status":"ok","timestamp":1635163478992,"user_tz":-330,"elapsed":1244,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}},"outputId":"53639655-0df0-4119-d8be-4d9b70892bc0"},"source":["!wget -q --show-progress https://github.com/sparsh-ai/model-retraining/raw/main/data/bronze/ml_100k.csv"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\rml_100k.csv           0%[                    ]       0  --.-KB/s               \rml_100k.csv         100%[===================>]  10.54M  --.-KB/s    in 0.1s    \n"]}]},{"cell_type":"code","metadata":{"id":"3J9ke37XjTh-"},"source":["def get_data_stream():\n","    data_stream = river.stream.iter_csv('ml_100k.csv',\n","                                        target=\"rating\",\n","                                        delimiter=\"\\t\",\n","                                        converters={\n","                                            \"timestamp\": int,\n","                                            \"release_date\": int,\n","                                            \"age\": float,\n","                                            \"rating\": float,\n","                                        })\n","    return data_stream "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CMmCuzL2jw3I","executionInfo":{"status":"ok","timestamp":1635163502404,"user_tz":-330,"elapsed":781,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}},"outputId":"e8b9180a-4866-45bd-9272-e96c43a6d652"},"source":["for x, y in get_data_stream():\n","    print(f'x = {json.dumps(x, indent=4)}\\ny = {y}')\n","    break"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["x = {\n","    \"user\": \"259\",\n","    \"item\": \"255\",\n","    \"timestamp\": 874731910000000000,\n","    \"title\": \"My Best Friend's Wedding (1997)\",\n","    \"release_date\": 866764800000000000,\n","    \"genres\": \"comedy, romance\",\n","    \"age\": 21.0,\n","    \"gender\": \"M\",\n","    \"occupation\": \"student\",\n","    \"zip_code\": \"48823\"\n","}\n","y = 4.0\n"]}]},{"cell_type":"markdown","metadata":{"id":"SGeFxD0Nj39U"},"source":["Let's define a routine to evaluate our different models on MovieLens 100K. Mean Absolute Error and Root Mean Squared Error will be our metrics printed alongside model's computation time and memory usage:"]},{"cell_type":"code","metadata":{"id":"Komj40c3j5rF"},"source":["def evaluate(model):\n","    X_y = get_data_stream()\n","    metric = river.metrics.MAE() + river.metrics.RMSE()\n","    _ = progressive_val_score(X_y, model, metric, print_every=25_000, show_time=True, show_memory=True)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NlElS4wAod4w"},"source":["## Naive prediction"]},{"cell_type":"markdown","metadata":{"id":"b-B8KiIQogjf"},"source":["It's good practice in machine learning to start with a naive baseline and then iterate from simple things to complex ones observing progress incrementally. Let's start by predicing the target running mean as a first shot:\n","\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LE1p044Loq9S","executionInfo":{"status":"ok","timestamp":1635163509164,"user_tz":-330,"elapsed":2100,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}},"outputId":"648b01c1-6fcf-47d8-b8af-508a3b24e89b"},"source":["mean = river.stats.Mean()\n","metric = river.metrics.MAE() + river.metrics.RMSE()\n","\n","for i, x_y in enumerate(get_data_stream(), start=1):\n","    _, y = x_y\n","    metric.update(y, mean.get())\n","    mean.update(y)\n","\n","    if not i % 25_000:\n","        print(f'[{i:,d}] {metric}')"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[25,000] MAE: 0.934259, RMSE: 1.124469\n","[50,000] MAE: 0.923893, RMSE: 1.105\n","[75,000] MAE: 0.937359, RMSE: 1.123696\n","[100,000] MAE: 0.942162, RMSE: 1.125783\n"]}]},{"cell_type":"markdown","metadata":{"id":"1ZMTNzTotngT"},"source":["## Parameters"]},{"cell_type":"markdown","metadata":{"id":"bl8vnJkFto1C"},"source":["let's review the important parameters to tune when dealing with this family of methods:\n","\n","- n_factors: the number of latent factors. The more you set, the more items aspects and users preferences you are going to learn. Too many will cause overfitting, l2 regularization could help.\n","- *_optimizer: the optimizers. Classic stochastic gradient descent performs well, finding the good learning rate will make the difference.\n","- initializer: the latent weights initialization. Latent vectors have to be initialized with non-constant values. We generally sample them from a zero-mean normal distribution with small standard deviation."]},{"cell_type":"markdown","metadata":{"id":"n7dSQ7vqptqI"},"source":["## Baseline model"]},{"cell_type":"markdown","metadata":{"id":"eK4MEPR0qCr1"},"source":["Now we can do machine learning and explore available models in river.reco module starting with the baseline model. It extends our naive prediction by adding to the global running mean two bias terms characterizing the user and the item discrepancy from the general tendency. This baseline model can be viewed as a linear regression where the intercept is replaced by the target running mean with the users and the items one hot encoded.\n","\n","All machine learning models in river expect dicts as input with feature names as keys and feature values as values. Specifically, models from river.reco expect a 'user' and an 'item' entries without any type constraint on their values (i.e. can be strings or numbers). Other entries, if exist, are simply ignored. This is quite useful as we don't need to spend time and storage doing one hot encoding."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aqGrK2mUqFSZ","executionInfo":{"status":"ok","timestamp":1630505280987,"user_tz":-330,"elapsed":5870,"user":{"displayName":"Sparsh Agarwal","photoUrl":"","userId":"13037694610922482904"}},"outputId":"6a25accf-a33d-4932-8b40-035505ff96f7"},"source":["baseline_params = {\n","    'optimizer': river.optim.SGD(0.025),\n","    'l2': 0.,\n","    'initializer': river.optim.initializers.Zeros()\n","}\n","\n","model = river.meta.PredClipper(\n","    regressor=river.reco.Baseline(**baseline_params),\n","    y_min=1,\n","    y_max=5\n",")\n","\n","evaluate(model)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[25,000] MAE: 0.761844, RMSE: 0.960972 – 0:00:01.316880 – 170.36 KB\n","[50,000] MAE: 0.753292, RMSE: 0.951223 – 0:00:02.651206 – 239 KB\n","[75,000] MAE: 0.754177, RMSE: 0.953376 – 0:00:03.961798 – 282.81 KB\n","[100,000] MAE: 0.754651, RMSE: 0.954148 – 0:00:05.297710 – 306.41 KB\n"]}]},{"cell_type":"markdown","metadata":{"id":"ZPwZh_-2vDTl"},"source":["## Matrix Factorization"]},{"cell_type":"markdown","metadata":{"id":"bTRwnZ4cq2Ak"},"source":["### Funk Matrix Factorization (FunkMF)"]},{"cell_type":"markdown","metadata":{"id":"u1J08n5xsWQm"},"source":["It's the pure form of matrix factorization consisting of only learning the users and items latent representations. Simon Funk popularized its stochastic gradient descent optimization in 2006 during the Netflix Prize. FunkMF is sometimes referred as Probabilistic Matrix Factorization which is an extended probabilistic version."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uVM_icBosiQp","executionInfo":{"status":"ok","timestamp":1630505461379,"user_tz":-330,"elapsed":9948,"user":{"displayName":"Sparsh Agarwal","photoUrl":"","userId":"13037694610922482904"}},"outputId":"431f7e59-283d-4bb4-9fa2-f8b118decb21"},"source":["funk_mf_params = {\n","    'n_factors': 10,\n","    'optimizer': river.optim.SGD(0.05),\n","    'l2': 0.1,\n","    'initializer': river.optim.initializers.Normal(mu=0., sigma=0.1, seed=73)\n","}\n","\n","model = river.meta.PredClipper(\n","    regressor = river.reco.FunkMF(**funk_mf_params),\n","    y_min=1,\n","    y_max=5\n",")\n","\n","evaluate(model)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[25,000] MAE: 1.070136, RMSE: 1.397014 – 0:00:02.411592 – 938.37 KB\n","[50,000] MAE: 0.99174, RMSE: 1.290666 – 0:00:04.800103 – 1.13 MB\n","[75,000] MAE: 0.961072, RMSE: 1.250842 – 0:00:07.118442 – 1.33 MB\n","[100,000] MAE: 0.944883, RMSE: 1.227688 – 0:00:09.463518 – 1.5 MB\n"]}]},{"cell_type":"markdown","metadata":{"id":"lo6llniWsrv2"},"source":["Results are equivalent to our naive prediction (0.9448 vs 0.9421). By only focusing on the users preferences and the items characteristics, the model is limited in his ability to capture different views of the problem. Despite its poor performance alone, this algorithm is quite useful combined in other models or when we need to build dense representations for other tasks."]},{"cell_type":"markdown","metadata":{"id":"YeGHoup8tFDz"},"source":["### Biased Matrix Factorization (BiasedMF)\n","It's the combination of the Baseline model and FunkMF. Biased Matrix Factorization name is used by some people but some others refer to it by SVD or Funk SVD. It's the case of Yehuda Koren and Robert Bell in Recommender Systems Handbook (Chapter 5 Advances in Collaborative Filtering) and of surprise library. Nevertheless, SVD could be confused with the original Singular Value Decomposition from which it's derived from, and Funk SVD could also be misleading because of the biased part of the model equation which doesn't come from Simon Funk's work. For those reasons, we chose to side with Biased Matrix Factorization which fits more naturally to it."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fjYfzdJctT0s","executionInfo":{"status":"ok","timestamp":1630505661712,"user_tz":-330,"elapsed":10811,"user":{"displayName":"Sparsh Agarwal","photoUrl":"","userId":"13037694610922482904"}},"outputId":"d4524c7e-8308-4698-ebd2-d900bebe579b"},"source":["biased_mf_params = {\n","    'n_factors': 10,\n","    'bias_optimizer': river.optim.SGD(0.025),\n","    'latent_optimizer': river.optim.SGD(0.05),\n","    'weight_initializer': river.optim.initializers.Zeros(),\n","    'latent_initializer': river.optim.initializers.Normal(mu=0., sigma=0.1, seed=73),\n","    'l2_bias': 0.,\n","    'l2_latent': 0.\n","}\n","\n","model = river.meta.PredClipper(\n","    regressor = river.reco.BiasedMF(**biased_mf_params),\n","    y_min=1,\n","    y_max=5\n",")\n","\n","evaluate(model)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[25,000] MAE: 0.761818, RMSE: 0.961057 – 0:00:02.622680 – 1.01 MB\n","[50,000] MAE: 0.751667, RMSE: 0.949443 – 0:00:05.232414 – 1.28 MB\n","[75,000] MAE: 0.749653, RMSE: 0.948723 – 0:00:07.802655 – 1.51 MB\n","[100,000] MAE: 0.748559, RMSE: 0.947854 – 0:00:10.396654 – 1.69 MB\n"]}]},{"cell_type":"markdown","metadata":{"id":"FI7KDXJytcdW"},"source":["Results improved (0.7485 vs 0.7546) demonstrating that users and items latent representations bring additional information."]},{"cell_type":"markdown","metadata":{"id":"lKUG4EHRuZO5"},"source":["## Factorization Machines\n","Steffen Rendel came up in 2010 with Factorization Machines, an algorithm able to handle any real valued feature vector, combining the advantages of general predictors with factorization models. It became quite popular in the field of online advertising, notably after winning several Kaggle competitions. The modeling technique starts with a linear regression to capture the effects of each variable individually. \n","\n","Then are added interaction terms to learn features relations. Instead of learning a single and specific weight per interaction (as in polynomial regression), a set of latent factors is learnt per feature (as in MF). An interaction is calculated by multiplying involved features product with their latent vectors dot product. The degree of factorization — or model order — represents the maximum number of features per interaction considered.\n","\n","Strong emphasis must be placed on feature engineering as it allows FM to mimic most factorization models and significantly impact its performance. High cardinality categorical variables one hot encoding is the most frequent step before feeding the model with data. For more efficiency, river FM implementation considers string values as categorical variables and automatically one hot encode them. FM models have their own module ```river.facto```."]},{"cell_type":"markdown","metadata":{"id":"IInnUCXFvrDT"},"source":["### Mimic Biased Matrix Factorization (BiasedMF)"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EWh1rRqtu6zE","executionInfo":{"status":"ok","timestamp":1630506217489,"user_tz":-330,"elapsed":28155,"user":{"displayName":"Sparsh Agarwal","photoUrl":"","userId":"13037694610922482904"}},"outputId":"6acae318-dc2d-4d7d-eab4-988a730b0b17"},"source":["fm_params = {\n","    'n_factors': 10,\n","    'weight_optimizer': river.optim.SGD(0.025),\n","    'latent_optimizer': river.optim.SGD(0.05),\n","    'sample_normalization': False,\n","    'l1_weight': 0.,\n","    'l2_weight': 0.,\n","    'l1_latent': 0.,\n","    'l2_latent': 0.,\n","    'intercept': 3,\n","    'intercept_lr': .01,\n","    'weight_initializer': river.optim.initializers.Zeros(),\n","    'latent_initializer': river.optim.initializers.Normal(mu=0., sigma=0.1, seed=73),\n","}\n","\n","regressor = river.compose.Select('user', 'item')\n","regressor |= river.facto.FMRegressor(**fm_params)\n","\n","model = river.meta.PredClipper(\n","    regressor=regressor,\n","    y_min=1,\n","    y_max=5\n",")\n","\n","evaluate(model)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[25,000] MAE: 0.761778, RMSE: 0.960803 – 0:00:07.251881 – 1.16 MB\n","[50,000] MAE: 0.751986, RMSE: 0.949941 – 0:00:13.721247 – 1.36 MB\n","[75,000] MAE: 0.750044, RMSE: 0.948911 – 0:00:20.654001 – 1.58 MB\n","[100,000] MAE: 0.748609, RMSE: 0.947994 – 0:00:27.407404 – 1.77 MB\n"]}]},{"cell_type":"markdown","metadata":{"id":"Xkuh5fLJvdph"},"source":["Both MAE are very close to each other (0.7486 vs 0.7485) showing that we almost reproduced reco.BiasedMF algorithm. The cost is a naturally slower running time as FM implementation offers more flexibility."]},{"cell_type":"markdown","metadata":{"id":"wAw-3iwEvz2x"},"source":["### Feature engineering for FM models"]},{"cell_type":"code","metadata":{"id":"8u2s-zO4zd_T"},"source":["def split_genres(x):\n","    genres = x['genres'].split(', ')\n","    return {f'genre_{genre}': 1 / len(genres) for genre in genres}\n","    \n","\n","def bin_age(x):\n","    if x['age'] <= 18:\n","        return {'age_0-18': 1}\n","    elif x['age'] <= 32:\n","        return {'age_19-32': 1}\n","    elif x['age'] < 55:\n","        return {'age_33-54': 1}\n","    else:\n","        return {'age_55-100': 1}"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CcWdtd4Pv2ME","executionInfo":{"status":"ok","timestamp":1630507305520,"user_tz":-330,"elapsed":61569,"user":{"displayName":"Sparsh Agarwal","photoUrl":"","userId":"13037694610922482904"}},"outputId":"03855c42-e7e7-4136-ce55-20d54c7d45a7"},"source":["fm_params = {\n","    'n_factors': 14,\n","    'weight_optimizer': river.optim.SGD(0.01),\n","    'latent_optimizer': river.optim.SGD(0.025),\n","    'intercept': 3,\n","    'latent_initializer': river.optim.initializers.Normal(mu=0., sigma=0.05, seed=73),\n","}\n","\n","regressor = river.compose.Select('user', 'item')\n","regressor += (\n","    river.compose.Select('genres') |\n","    river.compose.FuncTransformer(split_genres)\n",")\n","regressor += (\n","    river.compose.Select('age') |\n","    river.compose.FuncTransformer(bin_age)\n",")\n","regressor |= river.facto.FMRegressor(**fm_params)\n","\n","model = river.meta.PredClipper(\n","    regressor=regressor,\n","    y_min=1,\n","    y_max=5\n",")\n","\n","evaluate(model)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[25,000] MAE: 0.759838, RMSE: 0.961281 – 0:00:15.857599 – 1.43 MB\n","[50,000] MAE: 0.751307, RMSE: 0.951391 – 0:00:33.467707 – 1.68 MB\n","[75,000] MAE: 0.750361, RMSE: 0.951393 – 0:00:47.431751 – 1.95 MB\n","[100,000] MAE: 0.749994, RMSE: 0.951435 – 0:01:01.239416 – 2.2 MB\n"]}]},{"cell_type":"markdown","metadata":{"id":"xKlpR-0izg-m"},"source":["### Higher-Order Factorization Machines (HOFM)"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jPAmK0pbzyJC","executionInfo":{"status":"ok","timestamp":1630507708525,"user_tz":-330,"elapsed":275003,"user":{"displayName":"Sparsh Agarwal","photoUrl":"","userId":"13037694610922482904"}},"outputId":"83cada53-2c99-4391-f58e-9aea8d07ee94"},"source":["hofm_params = {\n","    'degree': 3,\n","    'n_factors': 12,\n","    'weight_optimizer': river.optim.SGD(0.01),\n","    'latent_optimizer': river.optim.SGD(0.025),\n","    'intercept': 3,\n","    'latent_initializer': river.optim.initializers.Normal(mu=0., sigma=0.05, seed=73),\n","}\n","\n","regressor = river.compose.Select('user', 'item')\n","regressor += (\n","    river.compose.Select('genres') |\n","    river.compose.FuncTransformer(split_genres)\n",")\n","regressor += (\n","    river.compose.Select('age') |\n","    river.compose.FuncTransformer(bin_age)\n",")\n","regressor |= river.facto.HOFMRegressor(**hofm_params)\n","\n","model = river.meta.PredClipper(\n","    regressor=regressor,\n","    y_min=1,\n","    y_max=5\n",")\n","\n","evaluate(model)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[25,000] MAE: 0.761297, RMSE: 0.962054 – 0:01:09.293899 – 2.64 MB\n","[50,000] MAE: 0.751865, RMSE: 0.951499 – 0:02:18.506399 – 3.12 MB\n","[75,000] MAE: 0.750853, RMSE: 0.951526 – 0:03:26.322158 – 3.64 MB\n","[100,000] MAE: 0.750607, RMSE: 0.951982 – 0:04:34.358663 – 4.12 MB\n"]}]},{"cell_type":"markdown","metadata":{"id":"I6p052um0Prv"},"source":["High-order interactions are often hard to estimate due to too much sparsity, that's why we won't spend too much time here."]},{"cell_type":"markdown","metadata":{"id":"k4iZ6JzG0Z89"},"source":["### Field-aware Factorization Machines (FFM)\n","Field-aware variant of FM (FFM) improved the original method by adding the notion of \"fields\". A \"field\" is a group of features that belong to a specific domain (e.g. the \"users\" field, the \"items\" field, or the \"movie genres\" field).\n","\n","FFM restricts itself to pairwise interactions and factorizes separated latent spaces — one per combination of fields (e.g. users/items, users/movie genres, or items/movie genres) — instead of a common one shared by all fields. Therefore, each feature has one latent vector per field it can interact with — so that it can learn the specific effect with each different field."]},{"cell_type":"markdown","metadata":{"id":"X32Q02Vo1iSy"},"source":["Note that FFM usually needs to learn smaller number of latent factors than FM as each latent vector only deals with one field."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pNY_JYWG0rZM","executionInfo":{"status":"ok","timestamp":1630507857712,"user_tz":-330,"elapsed":85809,"user":{"displayName":"Sparsh Agarwal","photoUrl":"","userId":"13037694610922482904"}},"outputId":"a7d91a45-3af6-456d-ef99-198aecb7d945"},"source":["ffm_params = {\n","    'n_factors': 8,\n","    'weight_optimizer': river.optim.SGD(0.01),\n","    'latent_optimizer': river.optim.SGD(0.025),\n","    'intercept': 3,\n","    'latent_initializer': river.optim.initializers.Normal(mu=0., sigma=0.05, seed=73),\n","}\n","\n","regressor = river.compose.Select('user', 'item')\n","regressor += (\n","    river.compose.Select('genres') |\n","    river.compose.FuncTransformer(split_genres)\n",")\n","regressor += (\n","    river.compose.Select('age') |\n","    river.compose.FuncTransformer(bin_age)\n",")\n","regressor |= river.facto.FFMRegressor(**ffm_params)\n","\n","model = river.meta.PredClipper(\n","    regressor=regressor,\n","    y_min=1,\n","    y_max=5\n",")\n","\n","evaluate(model)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[25,000] MAE: 0.757718, RMSE: 0.958158 – 0:00:21.345140 – 3.06 MB\n","[50,000] MAE: 0.749502, RMSE: 0.948065 – 0:00:42.577609 – 3.62 MB\n","[75,000] MAE: 0.749275, RMSE: 0.948918 – 0:01:03.900952 – 4.23 MB\n","[100,000] MAE: 0.749542, RMSE: 0.949769 – 0:01:25.155978 – 4.79 MB\n"]}]},{"cell_type":"markdown","metadata":{"id":"m1Sj4JNo1yH4"},"source":["### Field-weighted Factorization Machines (FwFM)\n","Field-weighted Factorization Machines (FwFM) address FFM memory issues caused by its large number of parameters, which is in the order of feature number times field number. As FFM, FwFM is an extension of FM restricted to pairwise interactions, but instead of factorizing separated latent spaces, it learns a specific weight for each field combination modelling the interaction strength."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"eU2b7vvc2D95","executionInfo":{"status":"ok","timestamp":1630508080551,"user_tz":-330,"elapsed":104684,"user":{"displayName":"Sparsh Agarwal","photoUrl":"","userId":"13037694610922482904"}},"outputId":"8b0dbbcd-c134-4f2f-944b-fab60572af6f"},"source":["fwfm_params = {\n","    'n_factors': 10,\n","    'weight_optimizer': river.optim.SGD(0.01),\n","    'latent_optimizer': river.optim.SGD(0.025),\n","    'intercept': 3,\n","    'seed': 73,\n","}\n","\n","regressor = river.compose.Select('user', 'item')\n","regressor += (\n","    river.compose.Select('genres') |\n","    river.compose.FuncTransformer(split_genres)\n",")\n","regressor += (\n","    river.compose.Select('age') |\n","    river.compose.FuncTransformer(bin_age)\n",")\n","regressor |= river.facto.FwFMRegressor(**fwfm_params)\n","\n","model = river.meta.PredClipper(\n","    regressor=regressor,\n","    y_min=1,\n","    y_max=5\n",")\n","\n","evaluate(model)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[25,000] MAE: 0.761539, RMSE: 0.962241 – 0:00:25.952167 – 1.18 MB\n","[50,000] MAE: 0.754089, RMSE: 0.953181 – 0:00:52.040548 – 1.38 MB\n","[75,000] MAE: 0.754806, RMSE: 0.954979 – 0:01:18.107421 – 1.6 MB\n","[100,000] MAE: 0.755404, RMSE: 0.95604 – 0:01:44.092305 – 1.8 MB\n"]}]}]}