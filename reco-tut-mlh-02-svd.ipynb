{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "project_name = \"reco-tut-mlh\"; branch = \"main\"; account = \"sparsh-ai\"\n",
    "project_path = os.path.join('/content', project_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(project_path):\n",
    "    !cp /content/drive/MyDrive/mykeys.py /content\n",
    "    import mykeys\n",
    "    !rm /content/mykeys.py\n",
    "    path = \"/content/\" + project_name; \n",
    "    !mkdir \"{path}\"\n",
    "    %cd \"{path}\"\n",
    "    import sys; sys.path.append(path)\n",
    "    !git config --global user.email \"recotut@recohut.com\"\n",
    "    !git config --global user.name  \"reco-tut\"\n",
    "    !git init\n",
    "    !git remote add origin https://\"{mykeys.git_token}\":x-oauth-basic@github.com/\"{account}\"/\"{project_name}\".git\n",
    "    !git pull origin \"{branch}\"\n",
    "    !git checkout main\n",
    "else:\n",
    "    %cd \"{project_path}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git add . && git commit -m 'commit' && git push origin \"{branch}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, './code')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Singular Value Decomposition (SVD & SVD++)\n",
    "\n",
    "SVD was heavily used in Netflix's Prize Competition in 2009. The grand prize of $1,000,000 was won by BellKor's Pragmatic Chaos. SVD utilizes stochastic gradient descent to attempt to decompose the original sparse matrices into lower ranking user and item factors (matrix factorization). These two matrices are then multiplied together to predict unknown values in the original sparse martix.\n",
    "\n",
    "SVD++ adds a new  factor, the effect of implicit information instead of just the explicit information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q surprise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import surprise\n",
    "\n",
    "from utils import stratified_split\n",
    "import metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fp = os.path.join('./data/bronze', 'u.data')\n",
    "raw_data = pd.read_csv(fp, sep='\\t', names=['userId', 'movieId', 'rating', 'timestamp'])\n",
    "print(f'Shape: {raw_data.shape}')\n",
    "raw_data.sample(10, random_state=123)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = 0.75\n",
    "train, test = stratified_split(raw_data, 'userId', train_size)\n",
    "\n",
    "print(f'Train Shape: {train.shape}')\n",
    "print(f'Test Shape: {test.shape}')\n",
    "print(f'Do they have the same users?: {set(train.userId) == set(test.userId)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVD and SVD++"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop 'Timestamp' because surprise only takes dataframes with 3 columns in this order: userid, itemid, rating.\n",
    "surprise_train = surprise.Dataset.load_from_df(train.drop('timestamp', axis=1), reader=surprise.Reader('ml-100k')).build_full_trainset()\n",
    "\n",
    "# Instantiate models.\n",
    "svd = surprise.SVD(random_state=0, n_factors=64, n_epochs=10, verbose=True)\n",
    "svdpp = surprise.SVDpp(random_state=0, n_factors=64, n_epochs=10, verbose=True)\n",
    "models = [svd, svdpp]\n",
    "\n",
    "# Fit.\n",
    "for model in models:\n",
    "    model.fit(surprise_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recommend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_preds = []\n",
    "for model in models:\n",
    "    # Predict ratings for ALL movies for all users\n",
    "    predictions = []\n",
    "    users = train['userId'].unique()\n",
    "    items = train['movieId'].unique()\n",
    "\n",
    "    for user in users:\n",
    "            for item in items:\n",
    "                predictions.append([user, item, model.predict(user, item).est])\n",
    "    \n",
    "    predictions = pd.DataFrame(predictions, columns=['userId', 'movieId', 'prediction'])\n",
    "    \n",
    "    # Remove movies already seen by users\n",
    "    # Create column of all 1s\n",
    "    temp = train[['userId', 'movieId']].copy()\n",
    "    temp['seen'] = 1\n",
    "\n",
    "    # Outer join and remove movies that have alread been seen (seen=1)\n",
    "    merged = pd.merge(temp, predictions, on=['userId', 'movieId'], how=\"outer\")\n",
    "    merged = merged[merged['seen'].isnull()].drop('seen', axis=1)\n",
    "    \n",
    "    all_preds.append(merged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recommendations = []\n",
    "for predictions in all_preds:\n",
    "    # Create filter for users that appear in both the train and test set\n",
    "    common_users = set(test['userId']).intersection(set(predictions['userId']))\n",
    "    \n",
    "    # Filter the test and predictions so they have the same users between them\n",
    "    test_common = test[test['userId'].isin(common_users)]\n",
    "    svd_pred_common = predictions[predictions['userId'].isin(common_users)]\n",
    "    \n",
    "    if len(set(predictions['userId'])) != len(set(test['userId'])):\n",
    "        print('Number of users in train and test are NOT equal')\n",
    "        print(f\"# of users in train and test respectively: {len(set(predictions['userId']))}, {len(set(test['userId']))}\")\n",
    "        print(f\"# of users in BOTH train and test: {len(set(svd_pred_common['userId']))}\")\n",
    "        continue\n",
    "        \n",
    "    # From the predictions, we want only the top k for each user,\n",
    "    # not all the recommendations.\n",
    "    # Extract the top k recommendations from the predictions\n",
    "    top_movies = svd_pred_common.groupby('userId', as_index=False).apply(lambda x: x.nlargest(10, 'prediction')).reset_index(drop=True)\n",
    "    top_movies['rank'] = top_movies.groupby('userId', sort=False).cumcount() + 1\n",
    "    \n",
    "    recommendations.append(top_movies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation metrics\n",
    "\n",
    "We see how SVD++ performs better than normal SVD in all metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_metrics = {'svd':{}, 'svd++':{}}\n",
    "for recommendation, model in zip(recommendations, model_metrics):\n",
    "    # Create column with the predicted movie's rank for each user.\n",
    "    top_k = recommendation.copy()\n",
    "    top_k['rank'] = recommendation.groupby('userId', sort=False).cumcount() + 1  # For each user, only include movies recommendations that are also in the test set\n",
    "    \n",
    "    # Metrics.\n",
    "    precision_at_k = metrics.precision_at_k(top_k, test, 'userId', 'movieId', 'rank')\n",
    "    recall_at_k = metrics.recall_at_k(top_k, test, 'userId', 'movieId', 'rank')\n",
    "    mean_average_precision = metrics.mean_average_precision(top_k, test, 'userId', 'movieId', 'rank')\n",
    "    ndcg = metrics.ndcg(top_k, test, 'userId', 'movieId', 'rank')\n",
    "\n",
    "    model_metrics[model]['precision'] = precision_at_k\n",
    "    model_metrics[model]['recall'] = recall_at_k\n",
    "    model_metrics[model]['MAP'] = mean_average_precision\n",
    "    model_metrics[model]['NDCG'] = ndcg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model, values in model_metrics.items():\n",
    "    print(f'------ {model} -------',\n",
    "          f'Precision: {values[\"precision\"]:.6f}',\n",
    "          f'Recall: {values[\"recall\"]:.6f}',\n",
    "          f'MAP: {values[\"MAP\"]:.6f} ',\n",
    "          f'NDCG: {values[\"NDCG\"]:.6f}',\n",
    "          '', sep='\\n')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMklsIvMbdBGCUlx5fl9z7R",
   "collapsed_sections": [],
   "mount_file_id": "1WgX5gqwulFU6zMuRI-sXbyIlSPj58LBh",
   "name": "reco-tut-mlh-02-svd.ipynb",
   "provenance": [
    {
     "file_id": "1AqVI1vgtpjVbcx5XdPgFnkJCmWXMAg7G",
     "timestamp": 1628676023810
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
