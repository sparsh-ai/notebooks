{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Social Network Graph EgoNet Link Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Social media represents, nowadays, one of the most interesting and rich sources of information. Every day, thousands of new connections arise, new users join communities, and billions of posts are shared. Graphs mathematically represent all those interactions, helping to make order of all such spontaneous and unstructured traffic.\n",
    "\n",
    "When dealing with social graphs, there are many interesting problems that can be addressed using machine learning. Under the correct settings, it is possible to extract useful insights from this huge amount of data, for improving your marketing strategy, identifying users with dangerous behaviors (for example, terrorist networks), and predicting the likelihood that a user will read your new post.\n",
    "\n",
    "Specifically, link prediction is one of the most interesting and important research topics in this field. Depending on what a connection in your social graph represents, by predicting future edges, you will be able to predict your next suggested friend, the next suggested movie, and which product you are likely to buy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Tip: Link prediction task aims at forecasting the likelihood of a future connection between two nodes and it can be solved using several machine learning algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget http://snap.stanford.edu/data/facebook_combined.txt.gz\n",
    "!wget http://snap.stanford.edu/data/facebook.tar.gz\n",
    "!gzip -d facebook_combined.txt.gz\n",
    "!tar -xf facebook.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install community\n",
    "!pip install stellargraph\n",
    "!pip install node2vec==0.3.3\n",
    "!pip install git+https://github.com/palash1992/GEM.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier \n",
    "\n",
    "import community\n",
    "from community import community_louvain\n",
    "import networkx as nx\n",
    "import networkx.algorithms.community as nx_comm\n",
    "from node2vec import Node2Vec\n",
    "from node2vec.edges import HadamardEmbedder \n",
    "from stellargraph.data import EdgeSplitter\n",
    "from stellargraph import StellarGraph\n",
    "from stellargraph.mapper import GraphSAGELinkGenerator\n",
    "from stellargraph.layer import GraphSAGE, link_classification\n",
    "from tensorflow import keras\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "default_edge_color = 'gray'\n",
    "default_node_color = '#407cc9'\n",
    "enhanced_node_color = '#f5b042'\n",
    "enhanced_edge_color = '#cc2f04'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our goal is to predict the probability that the given 2 nodes are connected. It comes under the *Link Prediction* task. For this, we will first preprocess the data in the right format and then split it into train/test. Then, we will try 3 methods to train a binary classifier that will take 2 nodes as input and outputs the probability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parse edge features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_file_name = \"feature_map.txt\"\n",
    "feature_index = {}  #numeric index to name\n",
    "inverted_feature_index = {} #name to numeric index\n",
    "network = nx.Graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = nx.read_edgelist(\"facebook_combined.txt\", create_using=nx.Graph(), nodetype=int)\n",
    "print(nx.info(G))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's first create a list of participant ids - we call it ego nodes in literature it seems\n",
    "ego_nodes = set([int(name.split('.')[0]) for name in os.listdir(\"./facebook/\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_featname_line(line):\n",
    "  \"\"\" used to parse each line of the files containing feature names \"\"\"\n",
    "  line = line[(line.find(' '))+1:]  # chop first field\n",
    "  split = line.split(';')\n",
    "  name = ';'.join(split[:-1]) # feature name\n",
    "  index = int(split[-1].split(\" \")[-1]) #feature index\n",
    "  return index, name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_features():\n",
    "  \"\"\" \n",
    "  parse each ego-network and creates two dictionaries:\n",
    "      - feature_index: maps numeric indices to names\n",
    "      - inverted_feature_index: maps names to numeric indices\n",
    "  \"\"\"\n",
    "  feat_file_name = 'tmp.txt'\n",
    "  # may need to build the index first\n",
    "  if not os.path.exists(feat_file_name):\n",
    "      feat_index = {}\n",
    "      # build the index from data/*.featnames files\n",
    "      featname_files = glob.iglob(\"facebook/*.featnames\")\n",
    "      for featname_file_name in featname_files:\n",
    "          featname_file = open(featname_file_name, 'r')\n",
    "          for line in featname_file:\n",
    "              # example line:\n",
    "              # 0 birthday;anonymized feature 376\n",
    "              index, name = parse_featname_line(line)\n",
    "              feat_index[index] = name\n",
    "          featname_file.close()\n",
    "      keys = feat_index.keys()\n",
    "      keys = sorted(keys)\n",
    "      out = open(feat_file_name,'w')\n",
    "      for key in keys:\n",
    "          out.write(\"%d %s\\n\" % (key, feat_index[key]))\n",
    "      out.close()\n",
    "\n",
    "  index_file = open(feat_file_name,'r')\n",
    "  for line in index_file:\n",
    "      split = line.strip().split(' ')\n",
    "      key = int(split[0])\n",
    "      val = split[1]\n",
    "      feature_index[key] = val\n",
    "  index_file.close()\n",
    "\n",
    "  for key in feature_index.keys():\n",
    "      val = feature_index[key]\n",
    "      inverted_feature_index[val] = key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_features()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add parsed feature to the node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_nodes(network, ego_nodes):\n",
    "  \"\"\"\n",
    "  for each nodes in the network assign the corresponding features \n",
    "  previously loaded using the load_features function\n",
    "  \"\"\"\n",
    "  # parse each node\n",
    "  for node_id in ego_nodes:\n",
    "      featname_file = open(f'facebook/{node_id}.featnames','r')\n",
    "      feat_file     = open(f'facebook/{node_id}.feat','r')\n",
    "      egofeat_file  = open(f'facebook/{node_id}.egofeat','r')\n",
    "      edge_file     = open(f'facebook/{node_id}.edges','r')\n",
    "\n",
    "      ego_features = [int(x) for x in egofeat_file.readline().split(' ')]\n",
    "\n",
    "      # Add ego node features\n",
    "      network.nodes[node_id]['features'] = np.zeros(len(feature_index))\n",
    "      \n",
    "      # parse ego node\n",
    "      i = 0\n",
    "      for line in featname_file:\n",
    "          key, val = parse_featname_line(line)\n",
    "          # Update feature value if necessary\n",
    "          if ego_features[i] + 1 > network.nodes[node_id]['features'][key]:\n",
    "              network.nodes[node_id]['features'][key] = ego_features[i] + 1\n",
    "          i += 1\n",
    "\n",
    "      # parse neighboring nodes\n",
    "      for line in feat_file:\n",
    "          featname_file.seek(0)\n",
    "          split = [int(x) for x in line.split(' ')]\n",
    "          node_id = split[0]\n",
    "          features = split[1:]\n",
    "\n",
    "          # Add node features\n",
    "          network.nodes[node_id]['features'] = np.zeros(len(feature_index))\n",
    "\n",
    "          i = 0\n",
    "          for line in featname_file:\n",
    "              key, val = parse_featname_line(line)\n",
    "              # Update feature value if necessary\n",
    "              if features[i] + 1 > network.nodes[node_id]['features'][key]:\n",
    "                  network.nodes[node_id]['features'][key] = features[i] + 1\n",
    "              i += 1\n",
    "          \n",
    "      featname_file.close()\n",
    "      feat_file.close()\n",
    "      egofeat_file.close()\n",
    "      edge_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add the parsed features to the networkx nodes\n",
    "parse_nodes(G, ego_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check features has been correctly assigned\n",
    "G.nodes[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we aim to cast this problem as a supervised learning task, we need to create a training and testing dataset. We will therefore create two new subgraphs with the same numbers of nodes but different numbers of edges (as some edges will be removed and treated as positive samples for training/testing the algorithm).\n",
    "\n",
    "We are using the EdgeSplitter class to extract a fraction (p=10%) of all the edges in G, as well as the same number of negative edges, in order to obtain a reduced graph, graph_test. The train_test_split method also returns a list of node pairs, samples_test (where each pair corresponds to an existing or not existing edge in the graph), and a list of binary targets (labels_test) of the same length of the samples_test list. Then, from such a reduced graph, we are repeating the operation to obtain another reduced graph, graph_train, as well as the corresponding samples_train and labels_train lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edgeSplitter = EdgeSplitter(G) \n",
    "graph_test, samples_test, labels_test = edgeSplitter.train_test_split(p=0.1, method=\"global\", seed=24)\n",
    "\n",
    "edgeSplitter = EdgeSplitter(graph_test, G) \n",
    "graph_train, samples_train, labels_train = edgeSplitter.train_test_split(p=0.1, method=\"global\", seed=24)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be comparing three different methods for predicting missing edges:\n",
    "- Method1: node2vec will be used to learn a node embedding. Such embeddings will be used to train a Random Forest classifier in a supervised manner\n",
    "- Method2: graphSAGE (with and without features) will be used for link prediction\n",
    "- Method3: hand-crafted features will be extracted and used to train a Random Forest classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Node2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node2vec = Node2Vec(graph_train) \n",
    "model = node2vec.fit() \n",
    "\n",
    "edges_embs = HadamardEmbedder(keyed_vectors=model.wv) \n",
    "train_embeddings = [edges_embs[str(x[0]),str(x[1])] for x in samples_train]\n",
    "\n",
    "edges_embs = HadamardEmbedder(keyed_vectors=model.wv) \n",
    "test_embeddings = [edges_embs[str(x[0]),str(x[1])] for x in samples_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(n_estimators=10) \n",
    "rf.fit(train_embeddings, labels_train); \n",
    " \n",
    "y_pred = rf.predict(test_embeddings) \n",
    "print('Precision:', metrics.precision_score(labels_test, y_pred)) \n",
    "print('Recall:', metrics.recall_score(labels_test, y_pred)) \n",
    "print('F1-Score:', metrics.f1_score(labels_test, y_pred)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GraphSage with no features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will build a two-layer GraphSAGE architecture that, given labeled pairs of nodes, outputs a pair of node embeddings. Then, a fully connected neural network will be used to process these embeddings and produce link predictions. Notice that the GraphSAGE model and the fully connected network will be concatenated and trained end to end so that the embeddings learning stage is influenced by the predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eye = np.eye(graph_train.number_of_nodes())\n",
    "fake_features = {n:eye[n] for n in G.nodes()}\n",
    "nx.set_node_attributes(graph_train, fake_features, \"fake\")\n",
    "\n",
    "eye = np.eye(graph_test.number_of_nodes())\n",
    "fake_features = {n:eye[n] for n in G.nodes()}\n",
    "nx.set_node_attributes(graph_test, fake_features, \"fake\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_train.nodes[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "num_samples = [4, 4]\n",
    "\n",
    "sg_graph_train = StellarGraph.from_networkx(graph_train, node_features=\"fake\")\n",
    "sg_graph_test = StellarGraph.from_networkx(graph_test, node_features=\"fake\")\n",
    "\n",
    "train_gen = GraphSAGELinkGenerator(sg_graph_train, batch_size, num_samples)\n",
    "train_flow = train_gen.flow(samples_train, labels_train, shuffle=True, seed=24)\n",
    "\n",
    "test_gen = GraphSAGELinkGenerator(sg_graph_test, batch_size, num_samples)\n",
    "test_flow = test_gen.flow(samples_test, labels_test, seed=24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_sizes = [20, 20]\n",
    "graphsage = GraphSAGE(\n",
    "    layer_sizes=layer_sizes, generator=train_gen, bias=True, dropout=0.3\n",
    ")\n",
    "\n",
    "x_inp, x_out = graphsage.in_out_tensors()\n",
    "\n",
    "prediction = link_classification(\n",
    "    output_dim=1, output_act=\"sigmoid\", edge_embedding_method=\"ip\"\n",
    ")(x_out)\n",
    "\n",
    "model = keras.Model(inputs=x_inp, outputs=prediction)\n",
    "\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=1e-3),\n",
    "    loss=keras.losses.mse,\n",
    "    metrics=[\"acc\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "history = model.fit(train_flow, epochs=epochs, validation_data=test_flow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = np.round(model.predict(train_flow)).flatten()\n",
    "print('Precision:', metrics.precision_score(labels_train, y_pred)) \n",
    "print('Recall:', metrics.recall_score(labels_train, y_pred)) \n",
    "print('F1-Score:', metrics.f1_score(labels_train, y_pred)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = np.round(model.predict(test_flow)).flatten()\n",
    "print('Precision:', metrics.precision_score(labels_test, y_pred)) \n",
    "print('Recall:', metrics.recall_score(labels_test, y_pred)) \n",
    "print('F1-Score:', metrics.f1_score(labels_test, y_pred)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GraphSage with features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sg_graph_train = StellarGraph.from_networkx(graph_train, node_features=\"features\")\n",
    "sg_graph_test = StellarGraph.from_networkx(graph_test, node_features=\"features\")\n",
    "\n",
    "train_gen = GraphSAGELinkGenerator(sg_graph_train, batch_size, num_samples)\n",
    "train_flow = train_gen.flow(samples_train, labels_train, shuffle=True, seed=24)\n",
    "\n",
    "test_gen = GraphSAGELinkGenerator(sg_graph_test, batch_size, num_samples)\n",
    "test_flow = test_gen.flow(samples_test, labels_test, seed=24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_sizes = [20, 20]\n",
    "graphsage = GraphSAGE(\n",
    "    layer_sizes=layer_sizes, generator=train_gen, bias=True, dropout=0.3\n",
    ")\n",
    "\n",
    "x_inp, x_out = graphsage.in_out_tensors()\n",
    "\n",
    "prediction = link_classification(\n",
    "    output_dim=1, output_act=\"sigmoid\", edge_embedding_method=\"ip\"\n",
    ")(x_out)\n",
    "\n",
    "model = keras.Model(inputs=x_inp, outputs=prediction)\n",
    "\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=1e-3),\n",
    "    loss=keras.losses.mse,\n",
    "    metrics=[\"acc\"],\n",
    ")\n",
    "\n",
    "epochs = 10\n",
    "history = model.fit(train_flow, epochs=epochs, validation_data=test_flow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = np.round(model.predict(train_flow)).flatten()\n",
    "print('Precision:', metrics.precision_score(labels_train, y_pred)) \n",
    "print('Recall:', metrics.recall_score(labels_train, y_pred)) \n",
    "print('F1-Score:', metrics.f1_score(labels_train, y_pred)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = np.round(model.predict(test_flow)).flatten()\n",
    "print('Precision:', metrics.precision_score(labels_test, y_pred)) \n",
    "print('Recall:', metrics.recall_score(labels_test, y_pred)) \n",
    "print('F1-Score:', metrics.f1_score(labels_test, y_pred)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hand-crafted features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_shortest_path(G,u,v):\n",
    "  \"\"\" return the shortest path length between u,v \n",
    "      in the graph without the edge (u,v) \"\"\"\n",
    "  removed = False\n",
    "  if G.has_edge(u,v):\n",
    "    removed = True\n",
    "    G.remove_edge(u,v) # temporary remove edge\n",
    "  \n",
    "  try:\n",
    "    sp = len(nx.shortest_path(G, u, v))\n",
    "  except:\n",
    "    sp = 0\n",
    "\n",
    "  if removed:\n",
    "    G.add_edge(u,v) # add back the edge if it was removed\n",
    "\n",
    "  return sp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hc_features(G, samples_edges, labels):\n",
    "  # precompute metrics\n",
    "  centralities = nx.degree_centrality(G)\n",
    "  parts = community_louvain.best_partition(G)\n",
    "  \n",
    "  feats = []\n",
    "  for (u,v),l in zip(samples_edges, labels):\n",
    "    shortest_path = get_shortest_path(G, u, v)\n",
    "    j_coefficient = next(nx.jaccard_coefficient(G, ebunch=[(u, v)]))[-1]\n",
    "    u_centrality = centralities[u]\n",
    "    v_centrality = centralities[v]\n",
    "    u_community = parts.get(u)\n",
    "    v_community = parts.get(v)\n",
    "    # add the feature vector\n",
    "    feats += [[shortest_path, j_coefficient, u_centrality, v_centrality]]\n",
    "  return feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_train = get_hc_features(graph_train, samples_train, labels_train)\n",
    "feat_test = get_hc_features(graph_test, samples_test, labels_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(n_estimators=10) \n",
    "rf.fit(feat_train, labels_train); \n",
    " \n",
    "y_pred = rf.predict(feat_test) \n",
    "print('Precision:', metrics.precision_score(labels_test, y_pred)) \n",
    "print('Recall:', metrics.recall_score(labels_test, y_pred)) \n",
    "print('F1-Score:', metrics.f1_score(labels_test, y_pred)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary of results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame(columns=['Algorithm','Embedding','Node Features',\n",
    "                                'Precision','Recall','F1-Score'])\n",
    "\n",
    "idx = 0\n",
    "while True:\n",
    "    for col in results.columns:\n",
    "        _col = input(f\"{col}: \")\n",
    "        if _col=='\\stop': break\n",
    "        results.loc[idx,col] = _col\n",
    "    print('\\n{}\\n'.format('='*100))\n",
    "    if _col=='\\stop': break\n",
    "    idx+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The node2vec-based method is already able to achieve a high level of performance without supervision and per-node information. Such high results might be related to the particular structure of the combined ego network. Due to the high sub-modularity of the network (since it is composed of several ego networks), predicting whether two users will be connected or not might be highly related to the way the two candidate nodes are connected inside the network. For example, there might be a systematic situation in which two users, both connected to several users in the same ego network, have a high chance of being connected as well. On the other hand, two users belonging to different ego networks, or very far from each other, are likely to not be connected, making the prediction task easier. This is also confirmed by the high results achieved using the shallow method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Such a situation might be confusing, instead, for more complicated algorithms like GraphSAGE, especially when node features are involved. For example, two users might share similar interests, making them very similar. However, they might belong to different ego networks, where the corresponding ego users live in two very different parts of the world. So, similar users, which in principle should be connected, are not. However, it is also possible that such algorithms are predicting something further in the future. Recall that the combined ego network is a timestamp of a particular situation in a given period of time. Who knows how it might have evolved right now!\n",
    "\n",
    "Interpreting machine learning algorithms is probably the most interesting challenge of machine learning itself. For this reason, we should always interpret results with care. Our suggestion is always to dig into the dataset and try to give an explanation of your results.\n",
    "\n",
    "Finally, it is important to remark that each of the algorithms was not tuned for the purpose of this demonstration. Different results can be obtained by properly tuning each hyperparameter."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyNM/KygfizW+jiXDxC8eGGH",
   "collapsed_sections": [],
   "name": "rec-tut-gml-06-sng-egonet-part-2-link-prediction.ipynb",
   "provenance": [
    {
     "file_id": "1f05nHjML9TqPNTz_NWt_6VVUvYlPDGM6",
     "timestamp": 1628078002102
    },
    {
     "file_id": "18rsHbAXudxz_EspXEEFhfLLd-MT2jbhK",
     "timestamp": 1628066780292
    },
    {
     "file_id": "1sAKOySokSkK8dTp6GYjmIh3AjBOT1R0J",
     "timestamp": 1627993731574
    },
    {
     "file_id": "1FlR0Nt00zRzrjciEpl46j51IIomB0K_p",
     "timestamp": 1627989061002
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
