{"cells":[{"cell_type":"markdown","metadata":{"id":"tb_GkOpT86gH"},"source":["# Training MPM Recommendation Model on ML-1m in PyTorch"]},{"cell_type":"markdown","source":["| | |\n","| --- | --- |\n","| Problem | In the implicit feedback recommendation, incorporating short-term preference into recommender systems has attracted increasing attention in recent years. However, unexpected behaviors in historical interactions like clicking some items by accident don’t well reflect users’ inherent preferences. Existing studies fail to model the effects of unexpected behaviors thus achieve inferior recommendation performance |\n","| Solution | Multi-Preferences Model (MPM) tries to eliminate the effects of unexpected behaviors by first extracting the users’ instant preferences from their recent historical interactions by a fine-grained preferences module. Then an unexpected-behaviors detector is trained to judge whether these instant preferences are biased by unexpected behaviors. we also integrate user’s general preference in MPM. Finally, an output module is performed to eliminates the effects of unexpected behaviors and integrates all the information to make a final recommendation. |\n","| Dataset | ML-1m |\n","| Preprocessing | We evaluate the performance of our proposed model by the leave-one-out evaluation. For each dataset, we hold out the last one item that each user has interacted with and sample 99 items that unobserved interactions to form the test set, a validation set is also created like the test set and remaining data as a training set. For each positive user-item interaction pair in the training set, we conducted the negative sampling strategy to pair it with four negative items. |\n","| Metrics | HR@10, NDCG@10 |\n","| Models | MPM (Multi-Preferences Model) |\n","| Platform | PyTorch 1.10.0+cpu, Ubuntu 18.0 Google Colab instnce (VM) |\n","| Links | [Paper](https://arxiv.org/pdf/2112.11023v1.pdf), [Code](https://github.com/chenjie04/MPM) |"],"metadata":{"id":"9bdc_KAbmRRM"}},{"cell_type":"markdown","metadata":{"id":"42lNJbTq89Jh"},"source":["## Setup"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4442,"status":"ok","timestamp":1640861410731,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"},"user_tz":-330},"id":"0TQOKHhszdpq","outputId":"77728d1c-266b-4bf7-8571-943d0ea40153"},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting mlperf_compliance\n","  Downloading mlperf_compliance-0.0.10-py3-none-any.whl (24 kB)\n","Installing collected packages: mlperf-compliance\n","Successfully installed mlperf-compliance-0.0.10\n"]}],"source":["!pip install mlperf_compliance"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":843,"status":"ok","timestamp":1640861412178,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"},"user_tz":-330},"id":"3gil-2exz-Sn","outputId":"e15ab3a9-2007-4248-d846-687fed313b20"},"outputs":[{"name":"stdout","output_type":"stream","text":["/content/data\n","--2021-12-30 10:50:10--  https://files.grouplens.org/datasets/movielens/ml-1m.zip\n","Resolving files.grouplens.org (files.grouplens.org)... 128.101.65.152\n","Connecting to files.grouplens.org (files.grouplens.org)|128.101.65.152|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 5917549 (5.6M) [application/zip]\n","Saving to: ‘ml-1m.zip’\n","\n","ml-1m.zip           100%[===================>]   5.64M  34.8MB/s    in 0.2s    \n","\n","2021-12-30 10:50:11 (34.8 MB/s) - ‘ml-1m.zip’ saved [5917549/5917549]\n","\n","Archive:  ml-1m.zip\n","   creating: ml-1m/\n","  inflating: ml-1m/movies.dat        \n","  inflating: ml-1m/ratings.dat       \n","  inflating: ml-1m/README            \n","  inflating: ml-1m/users.dat         \n","/content\n"]}],"source":["!mkdir /content/data\n","%cd /content/data\n","!wget https://files.grouplens.org/datasets/movielens/ml-1m.zip\n","!unzip ml-1m.zip\n","%cd /content"]},{"cell_type":"markdown","metadata":{"id":"RcpXGxRS8-vS"},"source":["## Datasets"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0G2kNVoGzYNO"},"outputs":[],"source":["from collections import namedtuple\n","\n","import pandas as pd\n","\n","\n","RatingData = namedtuple('RatingData',\n","                        ['items', 'users', 'ratings', 'min_date', 'max_date'])\n","\n","\n","def describe_ratings(ratings):\n","    info = RatingData(items=len(ratings['item_id'].unique()),\n","                      users=len(ratings['user_id'].unique()),\n","                      ratings=len(ratings),\n","                      min_date=ratings['timestamp'].min(),\n","                      max_date=ratings['timestamp'].max())\n","    print(\"{ratings} ratings on {items} items from {users} users\"\n","          \" from {min_date} to {max_date}\"\n","          .format(**(info._asdict())))\n","    return info\n","\n","\n","def process_movielens(ratings, sort=True):\n","    ratings['timestamp'] = pd.to_datetime(ratings['timestamp'], unit='s')\n","    if sort:\n","        ratings.sort_values(by='timestamp', inplace=True)\n","    describe_ratings(ratings)\n","    return ratings\n","\n","def process_taobao(ratings,sort=True):\n","    ratings['timestamp'] = pd.to_datetime(ratings['timestamp'],unit='s')\n","    if sort:\n","        ratings.sort_values(by='timestamp', inplace=True)\n","    describe_ratings(ratings)\n","    return ratings\n","\n","\n","def load_ml_100k(filename, sort=True):\n","    names = ['user_id', 'item_id', 'rating', 'timestamp']\n","    ratings = pd.read_csv(filename, sep='\\t', names=names)\n","    return process_movielens(ratings, sort=sort)\n","\n","\n","def load_ml_1m(filename, sort=True):\n","    names = ['user_id', 'item_id', 'rating', 'timestamp']\n","    ratings = pd.read_csv(filename, sep='::', names=names, engine='python')\n","    return process_movielens(ratings, sort=sort)\n","\n","\n","def load_ml_10m(filename, sort=True):\n","    names = ['user_id', 'item_id', 'rating', 'timestamp']\n","    ratings = pd.read_csv(filename, sep='::', names=names, engine='python')\n","    return process_movielens(ratings, sort=sort)\n","\n","\n","def load_ml_20m(filename, sort=True):\n","    ratings = pd.read_csv(filename)\n","    ratings['timestamp'] = pd.to_datetime(ratings['timestamp'], unit='s')\n","    names = {'userId': 'user_id', 'movieId': 'item_id'}\n","    ratings.rename(columns=names, inplace=True)\n","    return process_movielens(ratings, sort=sort)\n","\n","\n","\n","def load_taobao(filename,sort=True):\n","    names = ['user_id','item_id','category_id','behavior_type','timestamp']\n","    ratings = pd.read_csv(filename, names=names)\n","    return process_taobao(ratings,sort=sort)\n","\n","\n","\n","DATASETS = [k.replace('load_', '') for k in locals().keys() if \"load_\" in k]\n","\n","\n","def get_dataset_name(filename):\n","    for dataset in DATASETS:\n","        if dataset in filename.replace('-', '_').lower():\n","            return dataset\n","    raise NotImplementedError\n","\n","\n","def implicit_load(filename, sort=True):\n","\n","    func = globals()[\"load_\" + get_dataset_name(filename)]\n","    return func(filename, sort=sort)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LI6gcxXOzCz6"},"outputs":[],"source":["import os\n","from argparse import ArgumentParser\n","from collections import defaultdict\n","import matplotlib.pyplot as plt\n","\n","import numpy as np\n","import pandas as pd\n","from tqdm import tqdm\n","import random\n","from collections import namedtuple\n","\n","\n","from mlperf_compliance import mlperf_log\n","\n","\n","MIN_RATINGS = 20\n","\n","\n","USER_COLUMN = 'user_id'\n","ITEM_COLUMN = 'item_id'\n","\n","\n","TRAIN_RATINGS_FILENAME = 'train_ratings.csv'\n","TEST_RATINGS_FILENAME = 'test_ratings.csv'\n","TEST_NEG_FILENAME = 'test_negative.csv'\n","DATA_SUMMARY_FILENAME = \"data_summary.csv\"\n","\n","# PATH = 'data/taobao-1m'\n","# OUTPUT = 'data/taobao-1m'\n","PATH = 'data/ml-1m'\n","OUTPUT = 'data/ml-1m'\n","NEGATIVES = 99\n","HISTORY_SIZE = 9\n","RANDOM_SEED = 0\n","\n","def parse_args():\n","    parser = ArgumentParser()\n","\n","    # parser.add_argument('--file',type=str,default=(os.path.join(PATH,'UserBehavior01.csv')),\n","    #                     help='Path to reviews CSV file from dataset')\n","    parser.add_argument('--file',type=str,default=(os.path.join(PATH,'ratings.dat')),\n","                        help='Path to reviews CSV file from dataset')\n","    parser.add_argument('--output', type=str, default=OUTPUT,\n","                        help='Output directory for train and test CSV files')\n","    parser.add_argument('-n', '--negatives', type=int, default=NEGATIVES,\n","                        help='Number of negative samples for each positive'\n","                             'test example')\n","    parser.add_argument('--history_size',type=int,default=HISTORY_SIZE,\n","                        help='The size of history')\n","    parser.add_argument('-s', '--seed', type=int, default=RANDOM_SEED,\n","                        help='Random seed to reproduce same negative samples')\n","    return parser.parse_args({})\n","\n","\n","def main():\n","    args = parse_args()\n","    np.random.seed(args.seed)\n","\n","    print(\"Loading raw data from {}\".format(args.file))\n","    #-------------- MovieLens dataset ------------------------------\n","    df = implicit_load(args.file, sort=False)\n","    #---------------------------------------------------------------\n","\n","    #------ retailrocket-recommender-system-dataset --------------------\n","    # df = pd.read_csv(args.file, sep=',', header=0)\n","    # df.columns = ['timestamp', 'user_id', 'event', 'item_id', 'transaction_id']\n","    # df['timestamp'] = pd.to_datetime(df['timestamp'], unit='ms')\n","    #\n","    #\n","    # RatingData = namedtuple('RatingData',\n","    #                         ['items', 'users', 'ratings', 'min_date', 'max_date'])\n","    # info = RatingData(items=len(df['item_id'].unique()),\n","    #                   users=len(df['user_id'].unique()),\n","    #                   ratings=len(df),\n","    #                   min_date=df['timestamp'].min(),\n","    #                   max_date=df['timestamp'].max())\n","    # print(\"{ratings} ratings on {items} items from {users} users\"\n","    #           \" from {min_date} to {max_date}\"\n","    #           .format(**(info._asdict())))\n","    # #--------------------------------------------------------------------\n","\n","    #-------------------amazon dataset------------------------\n","    # df = pd.read_csv(args.file, sep=',', header=None)\n","    # df.columns = ['user_id', 'item_id', 'rating', 'timestamp']\n","    # df['timestamp'] = pd.to_datetime(df['timestamp'], unit='s')\n","    #\n","    # RatingData = namedtuple('RatingData',\n","    #                         ['items', 'users', 'ratings', 'min_date', 'max_date'])\n","    # info = RatingData(items=len(df['item_id'].unique()),\n","    #                   users=len(df['user_id'].unique()),\n","    #                   ratings=len(df),\n","    #                   min_date=df['timestamp'].min(),\n","    #                   max_date=df['timestamp'].max())\n","    # print(\"{ratings} ratings on {items} items from {users} users\"\n","    #           \" from {min_date} to {max_date}\"\n","    #           .format(**(info._asdict())))\n","\n","\n","    #-------------------------------------------------------------------------\n","\n","    #------------------- hetrec2011 dataset------------------------\n","    # df = pd.read_csv(args.file, sep='\\t', header=0)\n","    # df.columns = ['user_id', 'item_id', 'tag_id', 'timestamp']\n","    # df['timestamp'] = pd.to_datetime(df['timestamp'], unit='ms')\n","    #\n","    # RatingData = namedtuple('RatingData',\n","    #                         ['items', 'users', 'ratings', 'min_date', 'max_date'])\n","    # info = RatingData(items=len(df['item_id'].unique()),\n","    #                   users=len(df['user_id'].unique()),\n","    #                   ratings=len(df),\n","    #                   min_date=df['timestamp'].min(),\n","    #                   max_date=df['timestamp'].max())\n","    # print(\"{ratings} ratings on {items} items from {users} users\"\n","    #           \" from {min_date} to {max_date}\"\n","    #           .format(**(info._asdict())))\n","    #\n","\n","    #-------------------------------------------------------------------------\n","\n","    #------------------- taobao UserBehavior dataset------------------------\n","    # df = pd.read_csv(args.file, sep=',', header=None)\n","    # df.columns = ['user_id', 'item_id', 'category_id', 'behavior_type', 'timestamp']\n","    # df['timestamp'] = pd.to_datetime(df['timestamp'], unit='s')\n","\n","    # RatingData = namedtuple('RatingData',\n","    #                         ['items', 'users', 'ratings', 'min_date', 'max_date'])\n","    # info = RatingData(items=len(df['item_id'].unique()),\n","    #                   users=len(df['user_id'].unique()),\n","    #                   ratings=len(df),\n","    #                   min_date=df['timestamp'].min(),\n","    #                   max_date=df['timestamp'].max())\n","    # print(\"{ratings} ratings on {items} items from {users} users\"\n","    #           \" from {min_date} to {max_date}\"\n","    #           .format(**(info._asdict())))\n","\n","\n","    #-------------------------------------------------------------------------\n","\n","    print(\"Filtering out users with less than {} ratings\".format(MIN_RATINGS))\n","    grouped = df.groupby(USER_COLUMN)\n","    mlperf_log.ncf_print(key=mlperf_log.PREPROC_HP_MIN_RATINGS, value=MIN_RATINGS)\n","    df = grouped.filter(lambda x: len(x) >= MIN_RATINGS)\n","\n","    print(\"Mapping original user and item IDs to new sequential IDs\")\n","    original_users = df[USER_COLUMN].unique()\n","    original_items = df[ITEM_COLUMN].unique()\n","\n","    nb_users = len(original_users)\n","    nb_items = len(original_items)\n","\n","    user_map = {user: index for index, user in enumerate(original_users)}\n","    item_map = {item: index for index, item in enumerate(original_items)}\n","\n","    df[USER_COLUMN] = df[USER_COLUMN].apply(lambda user: user_map[user])\n","    df[ITEM_COLUMN] = df[ITEM_COLUMN].apply(lambda item: item_map[item])\n","\n","    # print(df)\n","\n","\n","    assert df[USER_COLUMN].max() == len(original_users) - 1\n","    assert df[ITEM_COLUMN].max() == len(original_items) - 1\n","\n","    print(\"Creating list of items for each user\")\n","    # Need to sort before popping to get last item\n","    df['timestamp'] = pd.to_datetime(df['timestamp'], unit='s')\n","    df.sort_values(by='timestamp', inplace=True)\n","    all_ratings = set(zip(df[USER_COLUMN], df[ITEM_COLUMN]))\n","    user_to_items = defaultdict(list)\n","    for row in tqdm(df.itertuples(), desc='Ratings', total=len(df)):\n","        user_to_items[getattr(row, USER_COLUMN)].append(getattr(row, ITEM_COLUMN))  # noqa: E501\n","\n","    print(len(user_to_items[0]))\n","    print(user_to_items[0])\n","    print(user_to_items[0][-args.history_size:])\n","\n","\n","\n","    print(\"Generating {} negative samples for each user and creating training set\"\n","          .format(args.negatives))\n","    mlperf_log.ncf_print(key=mlperf_log.PREPROC_HP_NUM_EVAL, value=args.negatives)\n","\n","    train_ratings = []\n","    test_ratings = []\n","    test_negs = []\n","    all_items = set(range(len(original_items)))\n","\n","    for key, value in tqdm(user_to_items.items(), total=len(user_to_items)):\n","        all_negs = all_items - set(value)\n","        all_negs = sorted(list(all_negs))\n","        negs = random.sample(all_negs, args.negatives)\n","\n","        test_item = value.pop()\n","\n","        tmp = [key, test_item]\n","        tmp.extend(negs)\n","        test_negs.append(tmp)\n","\n","        tmp = [key, test_item]\n","        tmp.extend(value[-args.history_size:])\n","        test_ratings.append(tmp)\n","\n","        while len(value) > args.history_size:\n","            tgItem = value.pop()\n","            tmp = [key,tgItem]\n","            tmp.extend(value[-args.history_size:])\n","            train_ratings.append(tmp)\n","\n","\n","\n","    print(\"\\nSaving train and test CSV files to {}\".format(args.output))\n","\n","\n","\n","    df_train_ratings = pd.DataFrame(list(train_ratings))\n","    df_test_ratings = pd.DataFrame(list(test_ratings))\n","    df_test_negs = pd.DataFrame(list(test_negs))\n","\n","\n","    print('Saving data description ...')\n","    data_summary = pd.DataFrame(\n","        {'users': nb_users, 'items': nb_items, 'history_size': HISTORY_SIZE, 'train_entries': len(df_train_ratings), 'test': len(df_test_ratings)},\n","        index=[0])\n","    data_summary.to_csv(os.path.join(args.output, DATA_SUMMARY_FILENAME), header=True, index=False, sep=',')\n","\n","    df_train_ratings['fake_rating'] = 1\n","    df_train_ratings.to_csv(os.path.join(args.output, TRAIN_RATINGS_FILENAME),\n","                            index=False, header=False, sep='\\t')\n","\n","    mlperf_log.ncf_print(key=mlperf_log.INPUT_SIZE, value=len(df_train_ratings))\n","\n","\n","    df_test_ratings['fake_rating'] = 1\n","    df_test_ratings.to_csv(os.path.join(args.output, TEST_RATINGS_FILENAME),\n","                           index=False, header=False, sep='\\t')\n","\n","\n","    df_test_negs.to_csv(os.path.join(args.output, TEST_NEG_FILENAME),\n","                        index=False, header=False, sep='\\t')\n","\n","\n","# if __name__ == '__main__':\n","    # main()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hK-ITnsw0saV"},"outputs":[],"source":["import numpy as np\n","import scipy\n","import scipy.sparse\n","import torch\n","import torch.utils.data\n","import pandas as pd\n","\n","from mlperf_compliance import mlperf_log\n","\n","\n","class CFTrainDataset(torch.utils.data.dataset.Dataset):\n","    def __init__(self, train_fname, data_summary_fname, nb_neg):\n","        data_summary = pd.read_csv(data_summary_fname, sep=',', header=0)\n","        self.nb_users = data_summary.loc[0,'users']\n","        self.nb_items = data_summary.loc[0,'items']\n","        self._load_train_matrix(train_fname)\n","        self.nb_neg = nb_neg\n","\n","        mlperf_log.ncf_print(key=mlperf_log.INPUT_STEP_TRAIN_NEG_GEN, value=nb_neg)\n","        mlperf_log.ncf_print(key=mlperf_log.INPUT_HP_SAMPLE_TRAIN_REPLACEMENT)\n","\n","    def _load_train_matrix(self, train_fname):\n","        def process_line(line):\n","            line = line.strip().split('\\t')\n","            tmp = []\n","            tmp.extend(np.array(line[0:-1]).astype(int))\n","            tmp.extend([float(line[-1]) > 0])\n","\n","            return tmp\n","\n","        with open(train_fname, 'r') as file:\n","            data = list(map(process_line, file))\n","        # self.nb_users = max(data, key=lambda x: x[0])[0] + 1\n","        # self.nb_items = max(data, key=lambda x: x[1])[1] + 1\n","\n","        length = len(data)\n","\n","        self.data = list(filter(lambda x: x[-1], data))\n","        self.mat = scipy.sparse.dok_matrix(\n","                (self.nb_users, self.nb_items), dtype=np.float32)\n","        for i in range(length):\n","            user = self.data[i][0]\n","            item = self.data[i][1]\n","            self.mat[user, item] = 1.\n","\n","    def __len__(self):\n","        return (self.nb_neg + 1) * len(self.data)\n","\n","    def __getitem__(self, idx):\n","        if idx % (self.nb_neg + 1) == 0:\n","            idx = idx // (self.nb_neg + 1)\n","            return self.data[idx][0], self.data[idx][1], torch.LongTensor(self.data[idx][2:-1]), np.ones(1, dtype=np.float32)  # noqa: E501\n","        else:\n","            idx = idx // (self.nb_neg + 1)\n","            u = self.data[idx][0]\n","            j = torch.LongTensor(1).random_(0, int(self.nb_items)).item()\n","            while (u, j) in self.mat:\n","                j = torch.LongTensor(1).random_(0, int(self.nb_items)).item()\n","            return u, j, torch.LongTensor(self.data[idx][2:-1]), np.zeros(1, dtype=np.float32)\n","\n","\n","def load_test_ratings(fname):\n","    def process_line(line):\n","        tmp = map(int, line.strip().split('\\t')[:-1])\n","        return list(tmp)\n","    ratings = map(process_line, open(fname, 'r'))\n","    return list(ratings)\n","\n","\n","def load_test_negs(fname):\n","    def process_line(line):\n","        tmp = map(int, line.strip().split('\\t')[2:])\n","        return list(tmp)\n","    negs = map(process_line, open(fname, 'r'))\n","    return list(negs)"]},{"cell_type":"markdown","metadata":{"id":"OGfBzM1P9Eyr"},"source":["## Utils"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rQXI4k4r01Gs"},"outputs":[],"source":["import os\n","import json\n","from functools import reduce\n","\n","\n","class AverageMeter(object):\n","    \"\"\"Computes and stores the average and current value\"\"\"\n","    def __init__(self):\n","        self.reset()\n","\n","    def reset(self):\n","        self.val = 0\n","        self.avg = 0\n","        self.sum = 0\n","        self.count = 0\n","\n","    def update(self, val, n=1):\n","        self.val = val\n","        self.sum += val * n\n","        self.count += n\n","        self.avg = self.sum / self.count\n","\n","\n","def count_parameters(model):\n","    c = map(lambda p: reduce(lambda x, y: x * y, p.size()), model.parameters())\n","    return sum(c)\n","\n","\n","def save_config(config, run_dir):\n","    path = os.path.join(run_dir, \"config_{}.json\".format(config['timestamp']))\n","    with open(path, 'w') as config_file:\n","        json.dump(config, config_file)\n","        config_file.write('\\n')\n","\n","\n","def save_result(result, path):\n","    write_heading = not os.path.exists(path)\n","    with open(path, mode='a') as out:\n","        if write_heading:\n","            out.write(\",\".join([str(k) for k, v in result.items()]) + '\\n')\n","        out.write(\",\".join([str(v) for k, v in result.items()]) + '\\n')"]},{"cell_type":"markdown","metadata":{"id":"o6BXm5oj9GPw"},"source":["## Layers"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YbTWj0x30-oa"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","from torch.nn.utils import weight_norm\n","\n","\n","class Chomp1d(nn.Module):\n","    def __init__(self, chomp_size):\n","        super(Chomp1d, self).__init__()\n","        self.chomp_size = chomp_size\n","\n","    def forward(self, x):\n","        return x[:, :, :-self.chomp_size].contiguous()\n","\n","\n","class TemporalBlock(nn.Module):\n","    def __init__(self, n_inputs, n_outputs, kernel_size, stride, dilation, padding, dropout=0.2):\n","        super(TemporalBlock, self).__init__()\n","        self.conv1 = weight_norm(nn.Conv1d(n_inputs, n_outputs, kernel_size,\n","                                           stride=stride, padding=padding, dilation=dilation))\n","        self.chomp1 = Chomp1d(padding)\n","        self.relu1 = nn.ReLU()\n","        self.dropout1 = nn.Dropout(dropout)\n","\n","        self.conv2 = weight_norm(nn.Conv1d(n_outputs, n_outputs, kernel_size,\n","                                           stride=stride, padding=padding, dilation=dilation))\n","        self.chomp2 = Chomp1d(padding)\n","        self.relu2 = nn.ReLU()\n","        self.dropout2 = nn.Dropout(dropout)\n","\n","        self.net = nn.Sequential(self.conv1, self.chomp1, self.relu1, self.dropout1,\n","                                 self.conv2, self.chomp2, self.relu2, self.dropout2)\n","        self.downsample = nn.Conv1d(n_inputs, n_outputs, 1) if n_inputs != n_outputs else None\n","        self.relu = nn.ReLU()\n","        self.init_weights()\n","\n","    def init_weights(self):\n","        self.conv1.weight.data.normal_(0, 0.01)\n","        self.conv2.weight.data.normal_(0, 0.01)\n","        if self.downsample is not None:\n","            self.downsample.weight.data.normal_(0, 0.01)\n","\n","    def forward(self, x):\n","        out = self.net(x)\n","        res = x if self.downsample is None else self.downsample(x)\n","        return self.relu(out + res)\n","\n","\n","class TemporalConvNet(nn.Module):\n","    def __init__(self, num_inputs, num_channels, kernel_size=2, dropout=0.2):\n","        super(TemporalConvNet, self).__init__()\n","        layers = []\n","        num_levels = len(num_channels)\n","        for i in range(num_levels):\n","            dilation_size = 2 ** i\n","            in_channels = num_inputs if i == 0 else num_channels[i-1]\n","            out_channels = num_channels[i]\n","            layers += [TemporalBlock(in_channels, out_channels, kernel_size, stride=1, dilation=dilation_size,\n","                                     padding=(kernel_size-1) * dilation_size, dropout=dropout)]\n","\n","        self.network = nn.Sequential(*layers)\n","\n","    def forward(self, x):\n","        return self.network(x)"]},{"cell_type":"markdown","metadata":{"id":"kagIn0Ws9IHd"},"source":["## MPM Model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MB7CKp_w1B7h"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import numpy as np\n","\n","\n","class Multi_Preference_Model(nn.Module):\n","    def __init__(self, nb_users, nb_items, embed_dim, history_size):\n","        super(Multi_Preference_Model, self).__init__()\n","\n","        self.nb_users = nb_users\n","        self.nb_items = nb_items\n","        self.embed_dim = embed_dim\n","        self.history_size = history_size\n","\n","        #user and item embedding\n","        self.user_embed = nn.Embedding(self.nb_users, self.embed_dim)\n","        self.item_embed = nn.Embedding(self.nb_items, self.embed_dim)\n","        self.user_embed.weight.data.normal_(0., 0.01)\n","        self.item_embed.weight.data.normal_(0., 0.01)\n","\n","        #TCN\n","        nhid = self.embed_dim\n","        level = 5\n","        num_channels = [nhid] * (level - 1) + [embed_dim]\n","        self.tcn = TemporalConvNet(num_inputs=self.embed_dim, num_channels=num_channels, kernel_size=3, dropout=0.25)\n","\n","        #MLP\n","        mlp_layer_sizes = [self.embed_dim * 2, 128, 64, 32]\n","        nb_mlp_layers = len(mlp_layer_sizes)\n","        self.mlp = nn.ModuleList()\n","        for i in range(1, nb_mlp_layers):\n","            self.mlp.extend([nn.Linear(mlp_layer_sizes[i-1], mlp_layer_sizes[i])])\n","\n","        #Output Module\n","        self.output_1 = nn.Linear(mlp_layer_sizes[-1] * (self.history_size + 1),128,bias=True)\n","        self.output_2 = nn.Linear(128,64,bias=True)\n","        self.output_3 = nn.Linear(64,32,bias=True)\n","        self.output_4 = nn.Linear(32,1,bias=True)\n","\n","        def golorot_uniform(layer):\n","            fan_in, fan_out = layer.in_features, layer.out_features\n","            limit = np.sqrt(6. / (fan_in + fan_out))\n","            layer.weight.data.uniform_(-limit, limit)\n","\n","        def lecunn_uniform(layer):\n","            fan_in, fan_out = layer.in_features, layer.out_features  # noqa: F841, E501\n","            limit = np.sqrt(3. / fan_in)\n","            layer.weight.data.uniform_(-limit, limit)\n","\n","        for layer in self.mlp:\n","            if type(layer) != nn.Linear:\n","                continue\n","            golorot_uniform(layer)\n","\n","        lecunn_uniform(self.output_1)\n","        lecunn_uniform(self.output_2)\n","        lecunn_uniform(self.output_3)\n","        lecunn_uniform(self.output_4)\n","\n","    def forward(self, user, item, history,sigmoid=False):\n","\n","        item = self.item_embed(item)\n","\n","        #multi granularity preference module\n","        xhistory = self.item_embed(history)\n","\n","        output_TCN = self.tcn(xhistory.transpose(1,2)).transpose(1,2)\n","\n","        predict_vectors = list()\n","\n","        for i in range(self.history_size):\n","            preference = output_TCN[:, i, :]\n","            output_mlp = torch.cat((preference,item),dim=1)\n","            for j, layer in enumerate(self.mlp):\n","                output_mlp = layer(output_mlp)\n","                output_mlp = F.relu(output_mlp)\n","\n","            output_mlp = output_mlp.view(-1, 1, output_mlp.size()[-1])\n","            predict_vectors.append(output_mlp)\n","\n","        predict_vectors_sum = torch.cat(predict_vectors, dim=1)\n","\n","        # general preference module\n","        user = self.user_embed(user)\n","        xmlp = torch.cat((user, item), dim=1)\n","        for i, layer in enumerate(self.mlp):\n","            xmlp = layer(xmlp)\n","            xmlp = F.relu(xmlp)\n","\n","        #output module\n","        xmlp = xmlp.view(-1,1,xmlp.size()[-1])\n","        x = torch.cat((predict_vectors_sum,xmlp),dim=1)\n","        x = x.view(x.size()[0],-1)\n","        x = self.output_1(x)\n","        x = F.relu(x)\n","        x = self.output_2(x)\n","        x = F.relu(x)\n","        x = self.output_3(x)\n","        x = F.relu(x)\n","        x = self.output_4(x)\n","\n","        if sigmoid:\n","            x = torch.sigmoid(x)\n","        return x"]},{"cell_type":"markdown","metadata":{"id":"HZ9Egjd-9KO4"},"source":["## Training and Evaluation"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1441496,"status":"ok","timestamp":1640864863496,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"},"user_tz":-330},"id":"b1JUvaOc1GKa","outputId":"e7c32729-76b1-4c40-aa60-e306d4099c81"},"outputs":[{"name":"stdout","output_type":"stream","text":["Using seed = 3\n","Saving config and results to ./run/MGPM/ml-1m/1640859555\n","Using CPU ...\n","Loading data\n","data/ml-1m/train_ratings.csv\n","\n",":::MLPv0.5.0 ncf 1640859578.866290331 (<ipython-input-13-9639997e32b4>:19) input_step_train_neg_gen: 4\n","\n",":::MLPv0.5.0 ncf 1640859578.917798042 (<ipython-input-13-9639997e32b4>:20) input_hp_sample_train_replacement\n","\n",":::MLPv0.5.0 ncf 1640859578.947994947 (<ipython-input-15-12fac4022899>:192) input_batch_size: 2048\n","\n",":::MLPv0.5.0 ncf 1640859578.977500916 (<ipython-input-15-12fac4022899>:193) input_order\n","Load data done [24.7 s]. #user=6040, #item=3706, #train=939809, #test=6040\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"name":"stdout","output_type":"stream","text":["Multi_Preference_Model(\n","  (user_embed): Embedding(6040, 32)\n","  (item_embed): Embedding(3706, 32)\n","  (tcn): TemporalConvNet(\n","    (network): Sequential(\n","      (0): TemporalBlock(\n","        (conv1): Conv1d(32, 32, kernel_size=(3,), stride=(1,), padding=(2,))\n","        (chomp1): Chomp1d()\n","        (relu1): ReLU()\n","        (dropout1): Dropout(p=0.25, inplace=False)\n","        (conv2): Conv1d(32, 32, kernel_size=(3,), stride=(1,), padding=(2,))\n","        (chomp2): Chomp1d()\n","        (relu2): ReLU()\n","        (dropout2): Dropout(p=0.25, inplace=False)\n","        (net): Sequential(\n","          (0): Conv1d(32, 32, kernel_size=(3,), stride=(1,), padding=(2,))\n","          (1): Chomp1d()\n","          (2): ReLU()\n","          (3): Dropout(p=0.25, inplace=False)\n","          (4): Conv1d(32, 32, kernel_size=(3,), stride=(1,), padding=(2,))\n","          (5): Chomp1d()\n","          (6): ReLU()\n","          (7): Dropout(p=0.25, inplace=False)\n","        )\n","        (relu): ReLU()\n","      )\n","      (1): TemporalBlock(\n","        (conv1): Conv1d(32, 32, kernel_size=(3,), stride=(1,), padding=(4,), dilation=(2,))\n","        (chomp1): Chomp1d()\n","        (relu1): ReLU()\n","        (dropout1): Dropout(p=0.25, inplace=False)\n","        (conv2): Conv1d(32, 32, kernel_size=(3,), stride=(1,), padding=(4,), dilation=(2,))\n","        (chomp2): Chomp1d()\n","        (relu2): ReLU()\n","        (dropout2): Dropout(p=0.25, inplace=False)\n","        (net): Sequential(\n","          (0): Conv1d(32, 32, kernel_size=(3,), stride=(1,), padding=(4,), dilation=(2,))\n","          (1): Chomp1d()\n","          (2): ReLU()\n","          (3): Dropout(p=0.25, inplace=False)\n","          (4): Conv1d(32, 32, kernel_size=(3,), stride=(1,), padding=(4,), dilation=(2,))\n","          (5): Chomp1d()\n","          (6): ReLU()\n","          (7): Dropout(p=0.25, inplace=False)\n","        )\n","        (relu): ReLU()\n","      )\n","      (2): TemporalBlock(\n","        (conv1): Conv1d(32, 32, kernel_size=(3,), stride=(1,), padding=(8,), dilation=(4,))\n","        (chomp1): Chomp1d()\n","        (relu1): ReLU()\n","        (dropout1): Dropout(p=0.25, inplace=False)\n","        (conv2): Conv1d(32, 32, kernel_size=(3,), stride=(1,), padding=(8,), dilation=(4,))\n","        (chomp2): Chomp1d()\n","        (relu2): ReLU()\n","        (dropout2): Dropout(p=0.25, inplace=False)\n","        (net): Sequential(\n","          (0): Conv1d(32, 32, kernel_size=(3,), stride=(1,), padding=(8,), dilation=(4,))\n","          (1): Chomp1d()\n","          (2): ReLU()\n","          (3): Dropout(p=0.25, inplace=False)\n","          (4): Conv1d(32, 32, kernel_size=(3,), stride=(1,), padding=(8,), dilation=(4,))\n","          (5): Chomp1d()\n","          (6): ReLU()\n","          (7): Dropout(p=0.25, inplace=False)\n","        )\n","        (relu): ReLU()\n","      )\n","      (3): TemporalBlock(\n","        (conv1): Conv1d(32, 32, kernel_size=(3,), stride=(1,), padding=(16,), dilation=(8,))\n","        (chomp1): Chomp1d()\n","        (relu1): ReLU()\n","        (dropout1): Dropout(p=0.25, inplace=False)\n","        (conv2): Conv1d(32, 32, kernel_size=(3,), stride=(1,), padding=(16,), dilation=(8,))\n","        (chomp2): Chomp1d()\n","        (relu2): ReLU()\n","        (dropout2): Dropout(p=0.25, inplace=False)\n","        (net): Sequential(\n","          (0): Conv1d(32, 32, kernel_size=(3,), stride=(1,), padding=(16,), dilation=(8,))\n","          (1): Chomp1d()\n","          (2): ReLU()\n","          (3): Dropout(p=0.25, inplace=False)\n","          (4): Conv1d(32, 32, kernel_size=(3,), stride=(1,), padding=(16,), dilation=(8,))\n","          (5): Chomp1d()\n","          (6): ReLU()\n","          (7): Dropout(p=0.25, inplace=False)\n","        )\n","        (relu): ReLU()\n","      )\n","      (4): TemporalBlock(\n","        (conv1): Conv1d(32, 32, kernel_size=(3,), stride=(1,), padding=(32,), dilation=(16,))\n","        (chomp1): Chomp1d()\n","        (relu1): ReLU()\n","        (dropout1): Dropout(p=0.25, inplace=False)\n","        (conv2): Conv1d(32, 32, kernel_size=(3,), stride=(1,), padding=(32,), dilation=(16,))\n","        (chomp2): Chomp1d()\n","        (relu2): ReLU()\n","        (dropout2): Dropout(p=0.25, inplace=False)\n","        (net): Sequential(\n","          (0): Conv1d(32, 32, kernel_size=(3,), stride=(1,), padding=(32,), dilation=(16,))\n","          (1): Chomp1d()\n","          (2): ReLU()\n","          (3): Dropout(p=0.25, inplace=False)\n","          (4): Conv1d(32, 32, kernel_size=(3,), stride=(1,), padding=(32,), dilation=(16,))\n","          (5): Chomp1d()\n","          (6): ReLU()\n","          (7): Dropout(p=0.25, inplace=False)\n","        )\n","        (relu): ReLU()\n","      )\n","    )\n","  )\n","  (mlp): ModuleList(\n","    (0): Linear(in_features=64, out_features=128, bias=True)\n","    (1): Linear(in_features=128, out_features=64, bias=True)\n","    (2): Linear(in_features=64, out_features=32, bias=True)\n","  )\n","  (output_1): Linear(in_features=320, out_features=128, bias=True)\n","  (output_2): Linear(in_features=128, out_features=64, bias=True)\n","  (output_3): Linear(in_features=64, out_features=32, bias=True)\n","  (output_4): Linear(in_features=32, out_features=1, bias=True)\n",")\n","413345 parameters\n","\n",":::MLPv0.5.0 ncf 1640859579.342340708 (<ipython-input-15-12fac4022899>:215) opt_learning_rate: 0.001\n","\n",":::MLPv0.5.0 ncf 1640859579.373433352 (<ipython-input-15-12fac4022899>:217) opt_name: \"Adam\"\n","\n",":::MLPv0.5.0 ncf 1640859579.403172970 (<ipython-input-15-12fac4022899>:218) opt_hp_Adam_beta1: 0.9\n","\n",":::MLPv0.5.0 ncf 1640859579.434629202 (<ipython-input-15-12fac4022899>:219) opt_hp_Adam_beta2: 0.999\n","\n",":::MLPv0.5.0 ncf 1640859579.472294331 (<ipython-input-15-12fac4022899>:220) opt_hp_Adam_epsilon: 1e-08\n","\n",":::MLPv0.5.0 ncf 1640859579.528450012 (<ipython-input-15-12fac4022899>:224) model_hp_loss_fn: \"binary_cross_entropy\"\n","Initial evaluation\n","\n",":::MLPv0.5.0 ncf 1640859579.571680069 (<ipython-input-15-12fac4022899>:114) eval_start\n","\n",":::MLPv0.5.0 ncf 1640859669.393068314 (<ipython-input-15-12fac4022899>:136) eval_size: {\"epoch\": null, \"value\": 604000}\n","\n",":::MLPv0.5.0 ncf 1640859669.430445910 (<ipython-input-15-12fac4022899>:137) eval_hp_num_users: 6040\n","\n",":::MLPv0.5.0 ncf 1640859669.471741915 (<ipython-input-15-12fac4022899>:138) eval_hp_num_neg: 99\n","Initial HR@10 = 0.1028, NDCG@10 = 0.0464\n","\n",":::MLPv0.5.0 ncf 1640859669.506453514 (<ipython-input-15-12fac4022899>:256) train_loop\n","\n",":::MLPv0.5.0 ncf 1640859669.539647579 (<ipython-input-15-12fac4022899>:258) train_epoch: 0\n","\n",":::MLPv0.5.0 ncf 1640859669.571592093 (<ipython-input-15-12fac4022899>:262) input_hp_num_neg: 4\n","\n",":::MLPv0.5.0 ncf 1640859669.601731777 (<ipython-input-15-12fac4022899>:263) input_step_train_neg_gen\n"]},{"name":"stderr","output_type":"stream","text":["Epoch 0 Loss 0.2374 (0.2826): 100%|██████████| 2295/2295 [26:25<00:00,  1.45it/s]"]},{"name":"stdout","output_type":"stream","text":["Epoch 0 evaluation\n","\n",":::MLPv0.5.0 ncf 1640861255.056983471 (<ipython-input-15-12fac4022899>:114) eval_start: 0\n"]},{"name":"stderr","output_type":"stream","text":["\n"]},{"name":"stdout","output_type":"stream","text":["\n",":::MLPv0.5.0 ncf 1640861349.036993027 (<ipython-input-15-12fac4022899>:136) eval_size: {\"epoch\": 0, \"value\": 610040}\n","\n",":::MLPv0.5.0 ncf 1640861349.082477808 (<ipython-input-15-12fac4022899>:137) eval_hp_num_users: 6040\n","\n",":::MLPv0.5.0 ncf 1640861349.121448755 (<ipython-input-15-12fac4022899>:138) eval_hp_num_neg: 100\n","\n",":::MLPv0.5.0 ncf 1640861349.155122280 (<ipython-input-15-12fac4022899>:296) eval_accuracy: {\"epoch\": 0, \"value\": 0.7293046116828918}\n","\n",":::MLPv0.5.0 ncf 1640861349.186644793 (<ipython-input-15-12fac4022899>:297) eval_stop\n","Epoch 0: HR@10 = 0.7293, NDCG@10 = 0.4740, train_time = 1585.40, val_time = 94.19\n","Saving checkpoint..\n","\n",":::MLPv0.5.0 ncf 1640861349.240695238 (<ipython-input-15-12fac4022899>:258) train_epoch: 1\n","\n",":::MLPv0.5.0 ncf 1640861349.282543659 (<ipython-input-15-12fac4022899>:262) input_hp_num_neg: 4\n","\n",":::MLPv0.5.0 ncf 1640861349.318470001 (<ipython-input-15-12fac4022899>:263) input_step_train_neg_gen\n"]},{"name":"stderr","output_type":"stream","text":["Epoch 1 Loss 0.1842 (0.2037): 100%|██████████| 2295/2295 [26:20<00:00,  1.45it/s]"]},{"name":"stdout","output_type":"stream","text":["Epoch 1 evaluation\n","\n",":::MLPv0.5.0 ncf 1640862929.946528673 (<ipython-input-15-12fac4022899>:114) eval_start: 1\n"]},{"name":"stderr","output_type":"stream","text":["\n"]},{"name":"stdout","output_type":"stream","text":["\n",":::MLPv0.5.0 ncf 1640863025.670297623 (<ipython-input-15-12fac4022899>:136) eval_size: {\"epoch\": 1, \"value\": 616080}\n","\n",":::MLPv0.5.0 ncf 1640863025.707294464 (<ipython-input-15-12fac4022899>:137) eval_hp_num_users: 6040\n","\n",":::MLPv0.5.0 ncf 1640863025.745534420 (<ipython-input-15-12fac4022899>:138) eval_hp_num_neg: 101\n","\n",":::MLPv0.5.0 ncf 1640863025.777830362 (<ipython-input-15-12fac4022899>:296) eval_accuracy: {\"epoch\": 1, \"value\": 0.7749999761581421}\n","\n",":::MLPv0.5.0 ncf 1640863025.808667183 (<ipython-input-15-12fac4022899>:297) eval_stop\n","Epoch 1: HR@10 = 0.7750, NDCG@10 = 0.5323, train_time = 1580.58, val_time = 95.91\n","Saving checkpoint..\n","\n",":::MLPv0.5.0 ncf 1640863025.864041567 (<ipython-input-15-12fac4022899>:258) train_epoch: 2\n","\n",":::MLPv0.5.0 ncf 1640863025.894696951 (<ipython-input-15-12fac4022899>:262) input_hp_num_neg: 4\n","\n",":::MLPv0.5.0 ncf 1640863025.925774336 (<ipython-input-15-12fac4022899>:263) input_step_train_neg_gen\n"]},{"name":"stderr","output_type":"stream","text":["Epoch 2 Loss 0.1913 (0.1822): 100%|██████████| 2295/2295 [26:58<00:00,  1.42it/s]"]},{"name":"stdout","output_type":"stream","text":["Epoch 2 evaluation\n","\n",":::MLPv0.5.0 ncf 1640864644.619865417 (<ipython-input-15-12fac4022899>:114) eval_start: 2\n"]},{"name":"stderr","output_type":"stream","text":["\n"]},{"name":"stdout","output_type":"stream","text":["\n",":::MLPv0.5.0 ncf 1640864747.343717098 (<ipython-input-15-12fac4022899>:136) eval_size: {\"epoch\": 2, \"value\": 622120}\n","\n",":::MLPv0.5.0 ncf 1640864747.386755466 (<ipython-input-15-12fac4022899>:137) eval_hp_num_users: 6040\n","\n",":::MLPv0.5.0 ncf 1640864747.428090811 (<ipython-input-15-12fac4022899>:138) eval_hp_num_neg: 102\n","\n",":::MLPv0.5.0 ncf 1640864747.462394714 (<ipython-input-15-12fac4022899>:296) eval_accuracy: {\"epoch\": 2, \"value\": 0.7903973460197449}\n","\n",":::MLPv0.5.0 ncf 1640864747.506796598 (<ipython-input-15-12fac4022899>:297) eval_stop\n","Epoch 2: HR@10 = 0.7904, NDCG@10 = 0.5532, train_time = 1618.65, val_time = 102.93\n","Saving checkpoint..\n","\n",":::MLPv0.5.0 ncf 1640864747.572065830 (<ipython-input-15-12fac4022899>:258) train_epoch: 3\n","\n",":::MLPv0.5.0 ncf 1640864747.607133150 (<ipython-input-15-12fac4022899>:262) input_hp_num_neg: 4\n","\n",":::MLPv0.5.0 ncf 1640864747.644344091 (<ipython-input-15-12fac4022899>:263) input_step_train_neg_gen\n"]},{"name":"stderr","output_type":"stream","text":["Epoch 3 Loss 0.1644 (0.1707): 100%|██████████| 2295/2295 [27:37<00:00,  1.38it/s]"]},{"name":"stdout","output_type":"stream","text":["Epoch 3 evaluation\n","\n",":::MLPv0.5.0 ncf 1640866405.631515026 (<ipython-input-15-12fac4022899>:114) eval_start: 3\n"]},{"name":"stderr","output_type":"stream","text":["\n"]},{"name":"stdout","output_type":"stream","text":["\n",":::MLPv0.5.0 ncf 1640866518.570442915 (<ipython-input-15-12fac4022899>:136) eval_size: {\"epoch\": 3, \"value\": 628160}\n","\n",":::MLPv0.5.0 ncf 1640866518.605526447 (<ipython-input-15-12fac4022899>:137) eval_hp_num_users: 6040\n","\n",":::MLPv0.5.0 ncf 1640866518.641601324 (<ipython-input-15-12fac4022899>:138) eval_hp_num_neg: 103\n","\n",":::MLPv0.5.0 ncf 1640866518.670853138 (<ipython-input-15-12fac4022899>:296) eval_accuracy: {\"epoch\": 3, \"value\": 0.7995033264160156}\n","\n",":::MLPv0.5.0 ncf 1640866518.699945927 (<ipython-input-15-12fac4022899>:297) eval_stop\n","Epoch 3: HR@10 = 0.7995, NDCG@10 = 0.5674, train_time = 1657.94, val_time = 113.12\n","Saving checkpoint..\n","\n",":::MLPv0.5.0 ncf 1640866518.759350777 (<ipython-input-15-12fac4022899>:258) train_epoch: 4\n","\n",":::MLPv0.5.0 ncf 1640866518.789437532 (<ipython-input-15-12fac4022899>:262) input_hp_num_neg: 4\n","\n",":::MLPv0.5.0 ncf 1640866518.818580627 (<ipython-input-15-12fac4022899>:263) input_step_train_neg_gen\n"]},{"name":"stderr","output_type":"stream","text":["Epoch 4 Loss 0.1665 (0.1631): 100%|██████████| 2295/2295 [28:24<00:00,  1.35it/s]"]},{"name":"stdout","output_type":"stream","text":["Epoch 4 evaluation\n","\n",":::MLPv0.5.0 ncf 1640868223.097388268 (<ipython-input-15-12fac4022899>:114) eval_start: 4\n"]},{"name":"stderr","output_type":"stream","text":["\n"]},{"name":"stdout","output_type":"stream","text":["\n",":::MLPv0.5.0 ncf 1640868339.778037786 (<ipython-input-15-12fac4022899>:136) eval_size: {\"epoch\": 4, \"value\": 634200}\n","\n",":::MLPv0.5.0 ncf 1640868339.817111492 (<ipython-input-15-12fac4022899>:137) eval_hp_num_users: 6040\n","\n",":::MLPv0.5.0 ncf 1640868339.855721712 (<ipython-input-15-12fac4022899>:138) eval_hp_num_neg: 104\n","\n",":::MLPv0.5.0 ncf 1640868339.887498140 (<ipython-input-15-12fac4022899>:296) eval_accuracy: {\"epoch\": 4, \"value\": 0.8074503540992737}\n","\n",":::MLPv0.5.0 ncf 1640868339.916709185 (<ipython-input-15-12fac4022899>:297) eval_stop\n","Epoch 4: HR@10 = 0.8075, NDCG@10 = 0.5802, train_time = 1704.22, val_time = 116.87\n","Saving checkpoint..\n","\n",":::MLPv0.5.0 ncf 1640868339.975879908 (<ipython-input-15-12fac4022899>:258) train_epoch: 5\n","\n",":::MLPv0.5.0 ncf 1640868340.008387327 (<ipython-input-15-12fac4022899>:262) input_hp_num_neg: 4\n","\n",":::MLPv0.5.0 ncf 1640868340.038704634 (<ipython-input-15-12fac4022899>:263) input_step_train_neg_gen\n"]},{"name":"stderr","output_type":"stream","text":["Epoch 5 Loss 0.1519 (0.1573): 100%|██████████| 2295/2295 [28:54<00:00,  1.32it/s]"]},{"name":"stdout","output_type":"stream","text":["Epoch 5 evaluation\n","\n",":::MLPv0.5.0 ncf 1640870074.719893694 (<ipython-input-15-12fac4022899>:114) eval_start: 5\n"]},{"name":"stderr","output_type":"stream","text":["\n"]},{"name":"stdout","output_type":"stream","text":["\n",":::MLPv0.5.0 ncf 1640870197.294473886 (<ipython-input-15-12fac4022899>:136) eval_size: {\"epoch\": 5, \"value\": 640240}\n","\n",":::MLPv0.5.0 ncf 1640870197.326944113 (<ipython-input-15-12fac4022899>:137) eval_hp_num_users: 6040\n","\n",":::MLPv0.5.0 ncf 1640870197.360785246 (<ipython-input-15-12fac4022899>:138) eval_hp_num_neg: 105\n","\n",":::MLPv0.5.0 ncf 1640870197.388211012 (<ipython-input-15-12fac4022899>:296) eval_accuracy: {\"epoch\": 5, \"value\": 0.810927152633667}\n","\n",":::MLPv0.5.0 ncf 1640870197.414030552 (<ipython-input-15-12fac4022899>:297) eval_stop\n","Epoch 5: HR@10 = 0.8109, NDCG@10 = 0.5857, train_time = 1734.64, val_time = 122.74\n","Saving checkpoint..\n","\n",":::MLPv0.5.0 ncf 1640870197.469210863 (<ipython-input-15-12fac4022899>:258) train_epoch: 6\n","\n",":::MLPv0.5.0 ncf 1640870197.495621204 (<ipython-input-15-12fac4022899>:262) input_hp_num_neg: 4\n","\n",":::MLPv0.5.0 ncf 1640870197.527369499 (<ipython-input-15-12fac4022899>:263) input_step_train_neg_gen\n"]},{"name":"stderr","output_type":"stream","text":["Epoch 6 Loss 0.1276 (0.1526): 100%|██████████| 2295/2295 [29:29<00:00,  1.30it/s]"]},{"name":"stdout","output_type":"stream","text":["Epoch 6 evaluation\n","\n",":::MLPv0.5.0 ncf 1640871966.699860334 (<ipython-input-15-12fac4022899>:114) eval_start: 6\n"]},{"name":"stderr","output_type":"stream","text":["\n"]},{"name":"stdout","output_type":"stream","text":["\n",":::MLPv0.5.0 ncf 1640872091.283187628 (<ipython-input-15-12fac4022899>:136) eval_size: {\"epoch\": 6, \"value\": 646280}\n","\n",":::MLPv0.5.0 ncf 1640872091.319001198 (<ipython-input-15-12fac4022899>:137) eval_hp_num_users: 6040\n","\n",":::MLPv0.5.0 ncf 1640872091.352862120 (<ipython-input-15-12fac4022899>:138) eval_hp_num_neg: 106\n","\n",":::MLPv0.5.0 ncf 1640872091.382253408 (<ipython-input-15-12fac4022899>:296) eval_accuracy: {\"epoch\": 6, \"value\": 0.8145695328712463}\n","\n",":::MLPv0.5.0 ncf 1640872091.409046412 (<ipython-input-15-12fac4022899>:297) eval_stop\n","Epoch 6: HR@10 = 0.8146, NDCG@10 = 0.5908, train_time = 1769.05, val_time = 124.83\n","Saving checkpoint..\n","\n",":::MLPv0.5.0 ncf 1640872091.464409828 (<ipython-input-15-12fac4022899>:258) train_epoch: 7\n","\n",":::MLPv0.5.0 ncf 1640872091.491896629 (<ipython-input-15-12fac4022899>:262) input_hp_num_neg: 4\n","\n",":::MLPv0.5.0 ncf 1640872091.518489122 (<ipython-input-15-12fac4022899>:263) input_step_train_neg_gen\n"]},{"name":"stderr","output_type":"stream","text":["Epoch 7 Loss 0.1427 (0.1453):   3%|▎         | 78/2295 [01:03<28:48,  1.28it/s]"]}],"source":["import heapq\n","import math\n","import time\n","from functools import partial\n","from datetime import datetime\n","from collections import OrderedDict\n","from argparse import ArgumentParser\n","\n","import random\n","import tqdm\n","import numpy as np\n","import pandas as pd\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","import torch\n","import torch.nn as nn\n","from torch import multiprocessing as mp\n","\n","from mlperf_compliance import mlperf_log\n","\n","import os\n","os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2\"\n","\n","\n","def parse_args():\n","    parser = ArgumentParser(description=\"Train a Nerual Collaborative\"\n","                                        \" Filtering model\")\n","    parser.add_argument('--data', type=str, default='data/ml-1m',\n","                        help='path to test and training data files')\n","    parser.add_argument('-e', '--epochs', type=int, default=2,\n","                        help='number of epochs for training')\n","    parser.add_argument('-b', '--batch-size', type=int, default=2048,\n","                        help='number of examples for each iteration')\n","    parser.add_argument('-n', '--negative-samples', type=int, default=4,\n","                        help='number of negative examples per interaction')\n","    parser.add_argument('-l', '--learning-rate', type=float, default=0.001,\n","                        help='learning rate for optimizer')\n","    parser.add_argument('-k', '--topk', type=int, default=10,\n","                        help='rank for test examples to be considered a hit')\n","    parser.add_argument('--no-cuda', action='store_true',default=False,\n","                        help='use available GPUs')\n","    parser.add_argument('--seed', '-s', type=int,default=3,\n","                        help='manually set random seed for torch')\n","    parser.add_argument('--processes', '-p', type=int, default=1,\n","                        help='Number of processes for evaluating model')\n","    parser.add_argument('--workers', '-w', type=int, default=4,\n","                        help='Number of workers for training DataLoader')\n","    parser.add_argument('--resume', '-r',action='store_true', default=False,\n","                        help='resume from checkpoint')\n","    return parser.parse_args({})\n","\n","\n","def predict(model, users, items, history, batch_size=1024, use_cuda=True):\n","    batches = [(users[i:i + batch_size], items[i:i + batch_size],history[i:i + batch_size])\n","               for i in range(0, len(users), batch_size)]\n","    preds = []\n","    for user, item, _history in batches:\n","        def proc(x):\n","            x = np.array(x,dtype=int)\n","            x = torch.from_numpy(x)\n","            if use_cuda:\n","                x = x.cuda()\n","            return torch.autograd.Variable(x)\n","\n","        # outp, _ = model(proc(user), proc(item), proc(_history), sigmoid=True)\n","        outp = model(proc(user), proc(item), proc(_history), sigmoid=True)\n","\n","        outp = outp.data.cpu().numpy()\n","        preds += list(outp.flatten())\n","    return preds\n","\n","\n","def _calculate_hit(ranked, test_item):\n","    return int(test_item in ranked)\n","\n","\n","def _calculate_ndcg(ranked, test_item):\n","    for i, item in enumerate(ranked):\n","        if item == test_item:\n","            return math.log(2) / math.log(i + 2)\n","    return 0.\n","\n","\n","def eval_one(rating, items, model, K, use_cuda=True):\n","\n","    user = rating[0]\n","    test_item = rating[1]\n","    items.append(test_item)\n","    users = [user] * len(items)\n","    history = []\n","    _history = rating[2:]\n","    for i in range(len(items)):\n","        history.append(_history)\n","\n","    assert len(users) == len(items) == len(history)\n","\n","    predictions = predict(model, users, items, history, use_cuda=use_cuda)\n","\n","    map_item_score = {item: pred for item, pred in zip(items, predictions)}\n","    ranked = heapq.nlargest(K, map_item_score, key=map_item_score.get)\n","\n","    hit = _calculate_hit(ranked, test_item)\n","    ndcg = _calculate_ndcg(ranked, test_item)\n","    return hit, ndcg, len(predictions)\n","\n","\n","def val_epoch(model, ratings, negs, K, use_cuda=True, output=None, epoch=None,\n","              processes=1):\n","    if epoch is None:\n","        print(\"Initial evaluation\")\n","    else:\n","        print(\"Epoch {} evaluation\".format(epoch))\n","\n","    mlperf_log.ncf_print(key=mlperf_log.EVAL_START, value=epoch)\n","    start = datetime.now()\n","    model.eval()\n","    if processes > 1:\n","        context = mp.get_context('spawn')\n","        _eval_one = partial(eval_one, model=model, K=K, use_cuda=use_cuda)\n","        with context.Pool(processes=processes) as workers:\n","            hits_ndcg_numpred = workers.starmap(_eval_one, zip(ratings, negs))\n","        hits, ndcgs, num_preds = zip(*hits_ndcg_numpred)\n","    else:\n","        hits, ndcgs, num_preds = [], [], []\n","        for rating, items in zip(ratings, negs):\n","            hit, ndcg, num_pred = eval_one(rating, items, model, K, use_cuda=use_cuda)\n","            hits.append(hit)\n","            ndcgs.append(ndcg)\n","            num_preds.append(num_pred)\n","\n","    hits = np.array(hits, dtype=np.float32)\n","    ndcgs = np.array(ndcgs, dtype=np.float32)\n","\n","    assert len(set(num_preds)) == 1\n","    num_neg = num_preds[0] - 1  # one true positive, many negatives\n","    mlperf_log.ncf_print(key=mlperf_log.EVAL_SIZE, value={\"epoch\": epoch, \"value\": len(hits) * (1 + num_neg)})\n","    mlperf_log.ncf_print(key=mlperf_log.EVAL_HP_NUM_USERS, value=len(hits))\n","    mlperf_log.ncf_print(key=mlperf_log.EVAL_HP_NUM_NEG, value=num_neg)\n","\n","    end = datetime.now()\n","    if output is not None:\n","        result = OrderedDict()\n","        result['timestamp'] = datetime.now()\n","        result['duration'] = end - start\n","        result['epoch'] = epoch\n","        result['K'] = K\n","        result['hit_rate'] = np.mean(hits)\n","        result['NDCG'] = np.mean(ndcgs)\n","        save_result(result, output)\n","\n","    return hits, ndcgs\n","\n","\n","def main():\n","    # Note: The run start is in data_preprocess.py\n","\n","    args = parse_args()\n","    if args.seed is not None:\n","        print(\"Using seed = {}\".format(args.seed))\n","        torch.manual_seed(args.seed)\n","        np.random.seed(seed=args.seed)\n","\n","    # Save configuration to file\n","    config = {k: v for k, v in args.__dict__.items()}\n","    config['timestamp'] = \"{:.0f}\".format(datetime.utcnow().timestamp())\n","    config['local_timestamp'] = str(datetime.now())\n","    run_dir = \"./run/MGPM/{}/{}\".format(os.path.basename(os.path.normpath(args.data)),config['timestamp'])\n","    print(\"Saving config and results to {}\".format(run_dir))\n","    if not os.path.exists(run_dir) and run_dir != '':\n","        os.makedirs(run_dir)\n","    save_config(config, run_dir)\n","\n","    # Check that GPUs are actually available\n","    use_cuda = not args.no_cuda and torch.cuda.is_available()\n","\n","    if use_cuda:\n","        print(\"Using cuda ...\")\n","    else:\n","        print(\"Using CPU ...\")\n","\n","    t1 = time.time()\n","\n","    best_hit, best_ndcg = 0., 0.\n","    start_epoch = 0  # start from epoch 0 or last checkpoint epoch\n","\n","    # Load Data\n","    print('Loading data')\n","    print(os.path.join(args.data, TRAIN_RATINGS_FILENAME))\n","    train_dataset = CFTrainDataset(\n","        os.path.join(args.data, TRAIN_RATINGS_FILENAME),os.path.join(args.data, DATA_SUMMARY_FILENAME), args.negative_samples)\n","\n","    mlperf_log.ncf_print(key=mlperf_log.INPUT_BATCH_SIZE, value=args.batch_size)\n","    mlperf_log.ncf_print(key=mlperf_log.INPUT_ORDER)  # set shuffle=True in DataLoader\n","    train_dataloader = torch.utils.data.DataLoader(\n","            dataset=train_dataset, batch_size=args.batch_size, shuffle=True,\n","            num_workers=args.workers, pin_memory=True)\n","    test_ratings = load_test_ratings(os.path.join(args.data, TEST_RATINGS_FILENAME))  # noqa: E501\n","    test_negs = load_test_negs(os.path.join(args.data, TEST_NEG_FILENAME))\n","    nb_users, nb_items = train_dataset.nb_users, train_dataset.nb_items\n","    print('Load data done [%.1f s]. #user=%d, #item=%d, #train=%d, #test=%d'\n","          % (time.time()-t1, nb_users, nb_items, train_dataset.mat.nnz,\n","             len(test_ratings)))\n","\n","    # Create model\n","    model = Multi_Preference_Model(nb_users=nb_users, nb_items=nb_items,\n","                      embed_dim=32,history_size=9)\n","    print(model)\n","    print(\"{} parameters\".format(count_parameters(model)))\n","\n","    # Save model text description\n","    with open(os.path.join(run_dir, 'model.txt'), 'w') as file:\n","        file.write(str(model))\n","\n","    # Add optimizer and loss to graph\n","    mlperf_log.ncf_print(key=mlperf_log.OPT_LR, value=args.learning_rate)\n","    beta1, beta2, epsilon = 0.9, 0.999, 1e-8\n","    mlperf_log.ncf_print(key=mlperf_log.OPT_NAME, value=\"Adam\")\n","    mlperf_log.ncf_print(key=mlperf_log.OPT_HP_ADAM_BETA1, value=beta1)\n","    mlperf_log.ncf_print(key=mlperf_log.OPT_HP_ADAM_BETA2, value=beta2)\n","    mlperf_log.ncf_print(key=mlperf_log.OPT_HP_ADAM_EPSILON, value=epsilon)\n","    optimizer = torch.optim.Adam(model.parameters(), betas=(beta1, beta2),\n","                                 lr=args.learning_rate, eps=epsilon)\n","\n","    mlperf_log.ncf_print(key=mlperf_log.MODEL_HP_LOSS_FN, value=mlperf_log.BCE)\n","    # optimizer = torch.optim.SGD(model.parameters(),lr=args.learning_rate,momentum=0.9)\n","    criterion = nn.BCEWithLogitsLoss()\n","\n","\n","    if use_cuda:\n","        # Move model and loss to GPU\n","        model = model.cuda()\n","        criterion = criterion.cuda()\n","\n","    if args.resume:\n","        # Load checkpoint.\n","        print('==> Resuming from checkpoint..')\n","        assert os.path.isdir('checkpoint'), 'Error: no checkpoint directory found!'\n","        checkpoint = torch.load('./checkpoint/' + model._get_name() + '.pd')\n","        model.load_state_dict(checkpoint['model_state_dict'])\n","        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n","        start_epoch = checkpoint['epoch']\n","        best_hit = checkpoint['hit']\n","        best_ndcg = checkpoint['ndcg']\n","\n","\n","    # Create files for tracking training\n","    valid_results_file = os.path.join(run_dir, 'valid_results.csv')\n","\n","    # Calculate initial Hit Ratio and NDCG\n","    if start_epoch == 0:\n","        hits, ndcgs = val_epoch(model, test_ratings, test_negs, args.topk,\n","                                use_cuda=use_cuda, processes=args.processes)\n","        print('Initial HR@{K} = {hit_rate:.4f}, NDCG@{K} = {ndcg:.4f}'\n","              .format(K=args.topk, hit_rate=np.mean(hits), ndcg=np.mean(ndcgs)))\n","\n","    mlperf_log.ncf_print(key=mlperf_log.TRAIN_LOOP)\n","    for epoch in range(start_epoch,args.epochs):\n","        mlperf_log.ncf_print(key=mlperf_log.TRAIN_EPOCH, value=epoch)\n","        model.train()\n","        losses = AverageMeter()\n","\n","        mlperf_log.ncf_print(key=mlperf_log.INPUT_HP_NUM_NEG, value=train_dataset.nb_neg)\n","        mlperf_log.ncf_print(key=mlperf_log.INPUT_STEP_TRAIN_NEG_GEN)\n","        begin = time.time()\n","        loader = tqdm.tqdm(train_dataloader)\n","        for batch_index, (user, item, history, label) in enumerate(loader):\n","            user = torch.autograd.Variable(user, requires_grad=False)\n","            item = torch.autograd.Variable(item, requires_grad=False)\n","            history = torch.autograd.Variable(history, requires_grad=False)\n","            label = torch.autograd.Variable(label, requires_grad=False)\n","            if use_cuda:\n","                user = user.cuda()\n","                item = item.cuda()\n","                history = history.cuda()\n","                label = label.cuda()\n","\n","            # outputs, _ = model(user, item,history)\n","            outputs = model(user, item, history)\n","            loss = criterion(outputs, label)\n","            losses.update(loss.data.item(), user.size(0))\n","\n","            optimizer.zero_grad()\n","            loss.backward()\n","            optimizer.step()\n","\n","            # Save stats to file\n","            description = ('Epoch {} Loss {loss.val:.4f} ({loss.avg:.4f})'\n","                           .format(epoch, loss=losses))\n","            loader.set_description(description)\n","\n","        train_time = time.time() - begin\n","        begin = time.time()\n","        hits, ndcgs = val_epoch(model, test_ratings, test_negs, args.topk,\n","                                use_cuda=use_cuda, output=valid_results_file,\n","                                epoch=epoch, processes=args.processes)\n","        mlperf_log.ncf_print(key=mlperf_log.EVAL_ACCURACY, value={\"epoch\": epoch, \"value\": float(np.mean(hits))})\n","        mlperf_log.ncf_print(key=mlperf_log.EVAL_STOP)\n","        val_time = time.time() - begin\n","        print('Epoch {epoch}: HR@{K} = {hit_rate:.4f}, NDCG@{K} = {ndcg:.4f},'\n","              ' train_time = {train_time:.2f}, val_time = {val_time:.2f}'\n","              .format(epoch=epoch, K=args.topk, hit_rate=np.mean(hits),\n","                      ndcg=np.mean(ndcgs), train_time=train_time,\n","                      val_time=val_time))\n","        if np.mean(hits) >= best_hit or np.mean(ndcgs) >= best_ndcg:\n","            best_hit = np.mean(hits)\n","            best_ndcg = np.mean(ndcgs)\n","            # Save checkpoint.\n","            print('Saving checkpoint..')\n","            state = {\n","                'epoch': epoch,\n","                'model_state_dict': model.state_dict(),\n","                'optimizer_state_dict': optimizer.state_dict(),\n","                'hit':best_hit,\n","                'ndcg':best_ndcg,\n","            }\n","            if not os.path.isdir('checkpoint'):\n","                os.mkdir('checkpoint')\n","            torch.save(state, './checkpoint/' + model._get_name()  + '.pd')\n","\n","    print(\"Best hit: \",best_hit)\n","    print(\"Best_ndcg: \", best_ndcg)\n","\n","    mlperf_log.ncf_print(key=mlperf_log.RUN_STOP)\n","    mlperf_log.ncf_print(key=mlperf_log.RUN_FINAL)\n","\n","\n","if __name__ == '__main__':\n","    main()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EIdOGHLM1mRe"},"outputs":[],"source":[""]}],"metadata":{"accelerator":"GPU","colab":{"name":"2022-01-14-mpm.ipynb","provenance":[{"file_id":"https://github.com/recohut/nbs/blob/main/raw/P868586%20%7C%20Training%20MPM%20Recommendation%20Model%20on%20ML-1m%20in%20PyTorch.ipynb","timestamp":1644614171793}],"collapsed_sections":[],"mount_file_id":"1QRcalee_iiogs06rPU5bWAh10jB-9Hup","authorship_tag":"ABX9TyPcpnDN/8WlaiVp8M3tzXgl"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}