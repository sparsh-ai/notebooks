{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"T611861 | Training RL Agent in Mountain Car Environment with  Policy gradient method","provenance":[{"file_id":"1FHEZUM4FJITO7l33fUAD1g5p3na-E9hu","timestamp":1638444875277},{"file_id":"1wECh43qHPURwZPott73zUQezhFf6PhvO","timestamp":1638444457608},{"file_id":"1_JpMHYPz7gqO_rnMgYUsc3ldONqoUY5d","timestamp":1638444159593},{"file_id":"1e26KmuIxWXVIrgFmcM5LCagOaWWYkr1J","timestamp":1638442144348}],"collapsed_sections":[],"authorship_tag":"ABX9TyOso7lDtOQpFQSpWTJ5jQ+J"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"bmIy05Cqwt3X"},"source":["# Training RL Agent in Mountain Car Environment with  Policy gradient method"]},{"cell_type":"code","metadata":{"id":"qCl7x_aFuhxe","executionInfo":{"status":"ok","timestamp":1638445319722,"user_tz":-330,"elapsed":2432,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}}},"source":["import tensorflow as tf\n","import tensorflow_probability as tfp\n","from tensorflow import keras\n","from tensorflow.keras import layers\n","import numpy as np\n","import gym"],"execution_count":1,"outputs":[]},{"cell_type":"code","metadata":{"id":"2dlmJ4BAu76v","executionInfo":{"status":"ok","timestamp":1638445391601,"user_tz":-330,"elapsed":767,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}}},"source":["class PolicyNet(keras.Model):\n","    def __init__(self, action_dim=1):\n","        super(PolicyNet, self).__init__()\n","        self.fc1 = layers.Dense(24, activation=\"relu\")\n","        self.fc2 = layers.Dense(36, activation=\"relu\")\n","        self.fc3 = layers.Dense(action_dim, activation=\"softmax\")\n","\n","    def call(self, x):\n","        x = self.fc1(x)\n","        x = self.fc2(x)\n","        x = self.fc3(x)\n","        return x\n","\n","    def process(self, observations):\n","        # Process batch observations using `call(x)` behind-the-scenes\n","        action_probabilities = self.predict_on_batch(observations)\n","        return action_probabilities"],"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"id":"6ecXS55wu-r6","executionInfo":{"status":"ok","timestamp":1638445392328,"user_tz":-330,"elapsed":10,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}}},"source":["class Agent(object):\n","    def __init__(self, action_dim=1):\n","        \"\"\"Agent with a neural-network brain powered policy\n","\n","        Args:\n","            action_dim (int): Action dimension\n","        \"\"\"\n","        self.policy_net = PolicyNet(action_dim=action_dim)\n","        self.optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)\n","        self.gamma = 0.99\n","\n","    def policy(self, observation):\n","        observation = observation.reshape(1, -1)\n","        observation = tf.convert_to_tensor(observation, dtype=tf.float32)\n","        action_logits = self.policy_net(observation)\n","        action = tf.random.categorical(tf.math.log(action_logits), num_samples=1)\n","        return action\n","\n","    def get_action(self, observation):\n","        action = self.policy(observation).numpy()\n","        return action.squeeze()\n","\n","    def learn(self, states, rewards, actions):\n","        discounted_reward = 0\n","        discounted_rewards = []\n","        rewards.reverse()\n","        for r in rewards:\n","            discounted_reward = r + self.gamma * discounted_reward\n","            discounted_rewards.append(discounted_reward)\n","        discounted_rewards.reverse()\n","\n","        for state, reward, action in zip(states, discounted_rewards, actions):\n","            with tf.GradientTape() as tape:\n","                action_probabilities = self.policy_net(np.array([state]), training=True)\n","                loss = self.loss(action_probabilities, action, reward)\n","            grads = tape.gradient(loss, self.policy_net.trainable_variables)\n","            self.optimizer.apply_gradients(\n","                zip(grads, self.policy_net.trainable_variables)\n","            )\n","\n","    def loss(self, action_probabilities, action, reward):\n","        dist = tfp.distributions.Categorical(\n","            probs=action_probabilities, dtype=tf.float32\n","        )\n","        log_prob = dist.log_prob(action)\n","        loss = -log_prob * reward\n","        return loss"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"id":"YIynMUi9xY_S","executionInfo":{"status":"ok","timestamp":1638445402431,"user_tz":-330,"elapsed":487,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}}},"source":["def train(agent: Agent, env: gym.Env, episodes: int, render=True):\n","    \"\"\"Train `agent` in `env` for `episodes`\n","\n","    Args:\n","        agent (Agent): Agent to train\n","        env (gym.Env): Environment to train the agent\n","        episodes (int): Number of episodes to train\n","        render (bool): True=Enable/False=Disable rendering; Default=True\n","    \"\"\"\n","\n","    for episode in range(episodes):\n","        done = False\n","        state = env.reset()\n","        total_reward = 0\n","        rewards = []\n","        states = []\n","        actions = []\n","        while not done:\n","            action = agent.get_action(state)\n","            next_state, reward, done, _ = env.step(action)\n","            rewards.append(reward)\n","            states.append(state)\n","            actions.append(action)\n","            state = next_state\n","            total_reward += reward\n","            if render:\n","                env.render()\n","            if done:\n","                agent.learn(states, rewards, actions)\n","                print(\"\\n\")\n","            print(f\"Episode#:{episode} ep_reward:{total_reward}\", end=\"\\r\")"],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gp4KDquT9OAt","executionInfo":{"status":"ok","timestamp":1638445434255,"user_tz":-330,"elapsed":4454,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}},"outputId":"0e09fb27-6011-48a0-b616-677eb8bb8e92"},"source":["if __name__ == \"__main__\":\n","    agent = Agent()\n","    episodes = 2  #  Increase number of episodes to train\n","    env = gym.make(\"MountainCar-v0\")\n","    # Set render=True to visualize Agent's actions in the env\n","    train(agent, env, episodes, render=False)\n","    env.close()"],"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","\n","\n","\n"]}]},{"cell_type":"markdown","metadata":{"id":"6F1CvviNvf0U"},"source":["---"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"eY5_ri7ovC5d","executionInfo":{"status":"ok","timestamp":1638445444675,"user_tz":-330,"elapsed":3603,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}},"outputId":"6f833b45-d2b2-4a03-9d14-9d779006793a"},"source":["!pip install -q watermark\n","%reload_ext watermark\n","%watermark -a \"Sparsh A.\" -m -iv -u -t -d"],"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["Author: Sparsh A.\n","\n","Last updated: 2021-12-02 11:44:06\n","\n","Compiler    : GCC 7.5.0\n","OS          : Linux\n","Release     : 5.4.104+\n","Machine     : x86_64\n","Processor   : x86_64\n","CPU cores   : 2\n","Architecture: 64bit\n","\n","tensorflow            : 2.7.0\n","IPython               : 5.5.0\n","gym                   : 0.17.3\n","tensorflow_probability: 0.15.0\n","numpy                 : 1.19.5\n","keras                 : 2.7.0\n","\n"]}]},{"cell_type":"markdown","metadata":{"id":"eQZmkNqovgsp"},"source":["---"]},{"cell_type":"markdown","metadata":{"id":"Jpw59lINviPG"},"source":["**END**"]}]}