{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"2022-01-09-personalize-deployment.ipynb","provenance":[{"file_id":"https://github.com/recohut/nbs/blob/main/raw/P117351%20%7C%20Operationalize%20end-to-end%20Amazon%20Personalize%20model%20deployment%20process%20using%20AWS%20Step%20Functions%20Data%20Science%20SDK.ipynb","timestamp":1644598246384}],"collapsed_sections":[],"mount_file_id":"1aaqwwE8cKeFTJ_kX-lNDvOWP0Zr6Ykam","authorship_tag":"ABX9TyMeQpy3h6veQnLlocsVXxQd"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"K81MlTZ4J2Cx"},"source":["# Operationalize end-to-end Amazon Personalize model deployment process using AWS Step Functions Data Science SDK\n","\n","1. [Introduction](#Introduction)\n","2. [Setup](#Setup)\n","3. [Task-States](#Task-States)\n","4. [Wait-States](#Wait-States)\n","5. [Choice-States](#Choice-States)\n","6. [Workflow](#Workflow)\n","7. [Generate-Recommendations](#Generate-Recommendations)\n","\n"]},{"cell_type":"markdown","metadata":{"id":"qAClB9a8J2C5"},"source":["## Introduction\n","\n","This notebook describes using the AWS Step Functions Data Science SDK to create and manage an Amazon Personalize workflow. The Step Functions SDK is an open source library that allows data scientists to easily create and execute machine learning workflows using AWS Step Functions. For more information on Step Functions SDK, see the following.\n","* [AWS Step Functions](https://aws.amazon.com/step-functions/)\n","* [AWS Step Functions Developer Guide](https://docs.aws.amazon.com/step-functions/latest/dg/welcome.html)\n","* [AWS Step Functions Data Science SDK](https://aws-step-functions-data-science-sdk.readthedocs.io)\n","\n","In this notebook we will use the SDK to create steps to create Personalize resources, link them together to create a workflow, and execute the workflow in AWS Step Functions. \n","\n","For more information, on Amazon Personalize see the following.\n","\n","* [Amazon Personalize](https://aws.amazon.com/personalize/)\n"]},{"cell_type":"markdown","metadata":{"id":"wGyw9o-NJ2C8"},"source":["## Setup"]},{"cell_type":"markdown","metadata":{"id":"HKpgQxE7J2C-"},"source":["### Import required modules from the SDK"]},{"cell_type":"code","metadata":{"id":"ccBDjF7vIvvv"},"source":["!mkdir -p ~/.aws && cp /content/drive/MyDrive/AWS/d01_admin/* ~/.aws"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"glGX0q2SGqV9"},"source":["!pip install -q boto3"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"iLG-4lmeJihL"},"source":["!pip install -q stepfunctions"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"cuz-quS0KTtE"},"source":["import boto3\n","import json\n","import numpy as np\n","import pandas as pd\n","import time\n","import logging\n","\n","import stepfunctions\n","from stepfunctions.steps import *\n","from stepfunctions.workflow import Workflow"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MzYEDVPkJl51"},"source":["personalize = boto3.client('personalize')\n","personalize_runtime = boto3.client('personalize-runtime')\n","stepfunctions.set_stream_logger(level=logging.INFO)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nj8sUJyEqjRV","executionInfo":{"status":"ok","timestamp":1629783598934,"user_tz":-330,"elapsed":523,"user":{"displayName":"Sparsh Agarwal","photoUrl":"","userId":"13037694610922482904"}},"outputId":"82c0bd55-f04e-4e06-a60c-3b9e2151d421"},"source":["!git clone https://github.com/aws-samples/personalize-data-science-sdk-workflow.git\n","%cd personalize-data-science-sdk-workflow"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/content/personalize-data-science-sdk-workflow\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"HVarZRLNJ2DD"},"source":["### Setup S3 location and filename\n","create an Amazon S3 bucket to store the training dataset and provide the Amazon S3 bucket name and file name in the walkthrough notebook  step Setup S3 location and filename below:"]},{"cell_type":"code","metadata":{"id":"WfrLHb5Rntt7"},"source":["bucket = \"reco-tut-aps\"       # replace with the name of your S3 bucket\n","filename = \"namemovie-lens-100k.csv\"  # replace with a name that you want to save the dataset under"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WhYT5n4WJ2DH"},"source":["### Setup IAM Roles\n","\n","#### Create an execution role for Step Functions\n","\n","You need an execution role so that you can create and execute workflows in Step Functions.\n","\n","1. Go to the [IAM console](https://console.aws.amazon.com/iam/)\n","2. Select **Roles** and then **Create role**.\n","3. Under **Choose the service that will use this role** select **Step Functions**\n","4. Choose **Next** until you can enter a **Role name**\n","5. Enter a name such as `StepFunctionsWorkflowExecutionRole` and then select **Create role**\n","\n","\n","Attach a policy to the role you created. The following steps attach a policy that provides full access to Step Functions, however as a good practice you should only provide access to the resources you need.  \n","\n","1. Under the **Permissions** tab, click **Add inline policy**\n","2. Enter the following in the **JSON** tab\n","\n","```json\n","{\n","    \"Version\": \"2012-10-17\",\n","    \"Statement\": [\n","    \n","        {\n","            \"Effect\": \"Allow\",\n","            \"Action\": [\n","                \"personalize:*\"\n","            ],\n","            \"Resource\": \"*\"\n","        },   \n","\n","        {\n","            \"Effect\": \"Allow\",\n","            \"Action\": [\n","                \"lambda:InvokeFunction\"\n","            ],\n","            \"Resource\": \"*\"\n","        },\n","        {\n","            \"Effect\": \"Allow\",\n","            \"Action\": [\n","                \"iam:PassRole\"\n","            ],\n","            \"Resource\": \"*\",\n","        },\n","        {\n","            \"Effect\": \"Allow\",\n","            \"Action\": [\n","                \"events:PutTargets\",\n","                \"events:PutRule\",\n","                \"events:DescribeRule\"\n","            ],\n","            \"Resource\": \"*\"\n","        }\n","    ]\n","}\n","```\n","\n","3. Choose **Review policy** and give the policy a name such as `StepFunctionsWorkflowExecutionPolicy`\n","4. Choose **Create policy**. You will be redirected to the details page for the role.\n","5. Copy the **Role ARN** at the top of the **Summary**\n","\n"]},{"cell_type":"code","metadata":{"id":"I1pQH4QupYOR"},"source":["workflow_execution_role = \"arn:aws:iam::746888961694:role/StepFunctionsWorkflowExecutionRole\" # paste the StepFunctionsWorkflowExecutionRole ARN from above"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"sCRfs1XrpeXn"},"source":["lambda_state_role = LambdaStep(\n","    state_id=\"create bucket and role\",\n","    parameters={  \n","        \"FunctionName\": \"stepfunction_create_personalize_role\", #replace with the name of the function you created\n","        \"Payload\": {  \n","           \"bucket\": bucket\n","        }\n","    },\n","    result_path='$'\n"," \n",")\n","\n","lambda_state_role.add_retry(Retry(\n","    error_equals=[\"States.TaskFailed\"],\n","    interval_seconds=5,\n","    max_attempts=1,\n","    backoff_rate=4.0\n","))\n","\n","lambda_state_role.add_catch(Catch(\n","    error_equals=[\"States.TaskFailed\"],\n","    next_step=Fail(\"CreateRoleTaskFailed\")\n","))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"XyKX8VzwJ2DO"},"source":["#### Attach Policy to S3 Bucket"]},{"cell_type":"code","metadata":{"id":"Z_doAlx5J2DO","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1629783731289,"user_tz":-330,"elapsed":2003,"user":{"displayName":"Sparsh Agarwal","photoUrl":"","userId":"13037694610922482904"}},"outputId":"64c63124-ad2c-43fa-f76f-3afc18ca099b"},"source":["s3 = boto3.client(\"s3\")\n","\n","policy = {\n","    \"Version\": \"2012-10-17\",\n","    \"Id\": \"PersonalizeS3BucketAccessPolicy\",\n","    \"Statement\": [\n","        {\n","            \"Sid\": \"PersonalizeS3BucketAccessPolicy\",\n","            \"Effect\": \"Allow\",\n","            \"Principal\": {\n","                \"Service\": \"personalize.amazonaws.com\"\n","            },\n","            \"Action\": [\n","                \"s3:GetObject\",\n","                \"s3:ListBucket\"\n","            ],\n","            \"Resource\": [\n","                \"arn:aws:s3:::{}\".format(bucket),\n","                \"arn:aws:s3:::{}/*\".format(bucket)\n","                \n","            ]\n","        }\n","    ]\n","}\n","\n","s3.put_bucket_policy(Bucket=bucket, Policy=json.dumps(policy))\n","\n","# AmazonPersonalizeFullAccess provides access to any S3 bucket with a name that includes \"personalize\" or \"Personalize\" \n","# if you would like to use a bucket with a different name, please consider creating and attaching a new policy\n","# that provides read access to your bucket or attaching the AmazonS3ReadOnlyAccess policy to the role"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'ResponseMetadata': {'HTTPHeaders': {'date': 'Tue, 24 Aug 2021 05:42:17 GMT',\n","   'server': 'AmazonS3',\n","   'x-amz-id-2': 'ufh+JaJboYdCWQ1JZGoa6yJlmF8y1g/UKO/pc8ApXmSO3FTaLFVtpqNVy7GX/iWn0DocpqKiZUA=',\n","   'x-amz-request-id': 'WBPMB91YF2AF6E1P'},\n","  'HTTPStatusCode': 204,\n","  'HostId': 'ufh+JaJboYdCWQ1JZGoa6yJlmF8y1g/UKO/pc8ApXmSO3FTaLFVtpqNVy7GX/iWn0DocpqKiZUA=',\n","  'RequestId': 'WBPMB91YF2AF6E1P',\n","  'RetryAttempts': 1}}"]},"metadata":{},"execution_count":14}]},{"cell_type":"markdown","metadata":{"id":"jZl3hpHIJ2DR"},"source":["#### Create Personalize Role\n"]},{"cell_type":"code","metadata":{"id":"vsRt2r1jJ2DS","colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"status":"ok","timestamp":1629783896480,"user_tz":-330,"elapsed":742,"user":{"displayName":"Sparsh Agarwal","photoUrl":"","userId":"13037694610922482904"}},"outputId":"6e23a954-e80e-4ce0-b736-4963c6cca740"},"source":["iam = boto3.client(\"iam\")\n","\n","role_name = \"personalize-role\" # Create a personalize role\n","\n","assume_role_policy_document = {\n","    \"Version\": \"2012-10-17\",\n","    \"Statement\": [\n","        {\n","          \"Effect\": \"Allow\",\n","          \"Principal\": {\n","            \"Service\": \"personalize.amazonaws.com\"\n","          },\n","          \"Action\": \"sts:AssumeRole\"\n","        }\n","    ]\n","}\n","\n","create_role_response = iam.create_role(\n","    RoleName = role_name,\n","    AssumeRolePolicyDocument = json.dumps(assume_role_policy_document)\n",")\n","\n","policy_arn = \"arn:aws:iam::aws:policy/service-role/AmazonPersonalizeFullAccess\"\n","iam.attach_role_policy(\n","    RoleName = role_name,\n","    PolicyArn = policy_arn\n",")\n","\n","time.sleep(60) # wait for a minute to allow IAM role policy attachment to propagate\n","\n","role_arn = create_role_response[\"Role\"][\"Arn\"]\n","role_arn"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'arn:aws:iam::746888961694:role/personalize-role'"]},"metadata":{},"execution_count":16}]},{"cell_type":"code","metadata":{"id":"u1hGPMbbJ2DT"},"source":["role_arn = \"arn:aws:iam::746888961694:role/personalize-role\""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jCV-PvhcJ2DT"},"source":["## Data-Preparation"]},{"cell_type":"markdown","metadata":{"id":"XWtmxyFdJ2DT"},"source":["### Download, Prepare, and Upload Training Data"]},{"cell_type":"code","metadata":{"id":"5vmvfKd-J2DU","colab":{"base_uri":"https://localhost:8080/","height":816},"executionInfo":{"status":"ok","timestamp":1629783985307,"user_tz":-330,"elapsed":1844,"user":{"displayName":"Sparsh Agarwal","photoUrl":"","userId":"13037694610922482904"}},"outputId":"1a35ed69-0252-4c2f-ad17-369080c5da69"},"source":["!wget -N http://files.grouplens.org/datasets/movielens/ml-100k.zip\n","!unzip -o ml-100k.zip\n","data = pd.read_csv('./ml-100k/u.data', sep='\\t', names=['USER_ID', 'ITEM_ID', 'RATING', 'TIMESTAMP'])\n","pd.set_option('display.max_rows', 5)\n","data.head()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["--2021-08-24 05:46:29--  http://files.grouplens.org/datasets/movielens/ml-100k.zip\n","Resolving files.grouplens.org (files.grouplens.org)... 128.101.65.152\n","Connecting to files.grouplens.org (files.grouplens.org)|128.101.65.152|:80... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 4924029 (4.7M) [application/zip]\n","Saving to: ‘ml-100k.zip’\n","\n","ml-100k.zip         100%[===================>]   4.70M  9.89MB/s    in 0.5s    \n","\n","2021-08-24 05:46:30 (9.89 MB/s) - ‘ml-100k.zip’ saved [4924029/4924029]\n","\n","Archive:  ml-100k.zip\n","   creating: ml-100k/\n","  inflating: ml-100k/allbut.pl       \n","  inflating: ml-100k/mku.sh          \n","  inflating: ml-100k/README          \n","  inflating: ml-100k/u.data          \n","  inflating: ml-100k/u.genre         \n","  inflating: ml-100k/u.info          \n","  inflating: ml-100k/u.item          \n","  inflating: ml-100k/u.occupation    \n","  inflating: ml-100k/u.user          \n","  inflating: ml-100k/u1.base         \n","  inflating: ml-100k/u1.test         \n","  inflating: ml-100k/u2.base         \n","  inflating: ml-100k/u2.test         \n","  inflating: ml-100k/u3.base         \n","  inflating: ml-100k/u3.test         \n","  inflating: ml-100k/u4.base         \n","  inflating: ml-100k/u4.test         \n","  inflating: ml-100k/u5.base         \n","  inflating: ml-100k/u5.test         \n","  inflating: ml-100k/ua.base         \n","  inflating: ml-100k/ua.test         \n","  inflating: ml-100k/ub.base         \n","  inflating: ml-100k/ub.test         \n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>USER_ID</th>\n","      <th>ITEM_ID</th>\n","      <th>RATING</th>\n","      <th>TIMESTAMP</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>196</td>\n","      <td>242</td>\n","      <td>3</td>\n","      <td>881250949</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>186</td>\n","      <td>302</td>\n","      <td>3</td>\n","      <td>891717742</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>22</td>\n","      <td>377</td>\n","      <td>1</td>\n","      <td>878887116</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>244</td>\n","      <td>51</td>\n","      <td>2</td>\n","      <td>880606923</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>166</td>\n","      <td>346</td>\n","      <td>1</td>\n","      <td>886397596</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   USER_ID  ITEM_ID  RATING  TIMESTAMP\n","0      196      242       3  881250949\n","1      186      302       3  891717742\n","2       22      377       1  878887116\n","3      244       51       2  880606923\n","4      166      346       1  886397596"]},"metadata":{},"execution_count":18}]},{"cell_type":"code","metadata":{"id":"OLo1rRrcJ2DV"},"source":["data = data[data['RATING'] > 2]                # keep only movies rated 2 and above\n","data2 = data[['USER_ID', 'ITEM_ID', 'TIMESTAMP']] \n","data2.to_csv(filename, index=False)\n","\n","boto3.Session().resource('s3').Bucket(bucket).Object(filename).upload_file(filename)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"yBpHHwSRJ2DV"},"source":["## Task-States"]},{"cell_type":"markdown","metadata":{"id":"O200GRvTJ2DW"},"source":["### Lambda Task state\n","\n","A `Task` State in Step Functions represents a single unit of work performed by a workflow. Tasks can call Lambda functions and orchestrate other AWS services. See [AWS Service Integrations](https://docs.aws.amazon.com/step-functions/latest/dg/concepts-service-integrations.html) in the *AWS Step Functions Developer Guide*.\n","\n","The following creates a [LambdaStep](https://aws-step-functions-data-science-sdk.readthedocs.io/en/latest/compute.html#stepfunctions.steps.compute.LambdaStep) called `lambda_state`, and then configures the options to [Retry](https://docs.aws.amazon.com/step-functions/latest/dg/concepts-error-handling.html#error-handling-retrying-after-an-error) if the Lambda function fails.\n","\n","#### Create a Lambda functions\n","\n","The Lambda task states in this workflow uses Lambda function **(Python 3.x)** that returns a Personalize resources such as Schema, Datasetgroup, Dataset, Solution, SolutionVersion, etc. Create the following functions in the [Lambda console](https://console.aws.amazon.com/lambda/).\n","\n","1. stepfunction-create-schema\n","2. stepfunctioncreatedatagroup\n","3. stepfunctioncreatedataset\n","4. stepfunction-createdatasetimportjob\n","5. stepfunction_select-recipe_create-solution\n","6. stepfunction_create_solution_version\n","7. stepfunction_getsolution_metric_create_campaign\n","\n","Copy/Paste the corresponding lambda function code from ./Lambda/ folder in the repo\n"]},{"cell_type":"markdown","metadata":{"id":"elVncKGOJ2DX"},"source":["#### Create Schema\n","\n","Before you add a dataset to Amazon Personalize, you must define a schema for that dataset. Once you define the schema and create the dataset, you can't make changes to the schema.for more information refer this documentation."]},{"cell_type":"code","metadata":{"id":"UMqA0zdUJ2DY"},"source":["lambda_state_schema = LambdaStep(\n","    state_id=\"create schema\",\n","    parameters={  \n","        \"FunctionName\": \"stepfunction-create-schema\", #replace with the name of the function you created\n","        \"Payload\": {  \n","           \"input\": \"personalize-stepfunction-schema3484\"\n","        }\n","    },\n","    result_path='$'    \n",")\n","\n","lambda_state_schema.add_retry(Retry(\n","    error_equals=[\"States.TaskFailed\"],\n","    interval_seconds=5,\n","    max_attempts=1,\n","    backoff_rate=4.0\n","))\n","\n","lambda_state_schema.add_catch(Catch(\n","    error_equals=[\"States.TaskFailed\"],\n","    next_step=Fail(\"CreateSchemaTaskFailed\")\n","))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"oAjTD8KEJ2DZ"},"source":["#### Create Datasetgroup\n","\n","Craete Datasetgroup: Creates an empty dataset group. A dataset group contains related datasets that supply data for training a model. A dataset group can contain at most three datasets, one for each type of dataset:\n","•\tInteractions\n","•\tItems\n","•\tUsers\n","To train a model (create a solution), a dataset group that contains an Interactions dataset is required. Call CreateDataset to add a dataset to the group.\n","\n","After you have created a schema , we will create another Stepfunction state based on this lambda function stepfunctioncreatedatagroup.py  below in github lambdas folder by running the Create Datasetgroup¶ step of the notebook. We are using python boto3 APIs to create_dataset_group."]},{"cell_type":"code","metadata":{"id":"j4_o-p77J2Da"},"source":["lambda_state_datasetgroup = LambdaStep(\n","    state_id=\"create dataset Group\",\n","    parameters={  \n","        \"FunctionName\": \"stepfunctioncreatedatagroup\", #replace with the name of the function you created\n","        \"Payload\": {  \n","           \"input\": \"personalize-stepfunction-dataset-group\", \n","           \"schemaArn.$\": '$.Payload.schemaArn'\n","        }\n","    },\n","\n","    result_path='$'\n",")\n","\n","\n","\n","lambda_state_datasetgroup.add_retry(Retry(\n","    error_equals=[\"States.TaskFailed\"],\n","    interval_seconds=5,\n","    max_attempts=1,\n","    backoff_rate=4.0\n","))\n","\n","\n","lambda_state_datasetgroup.add_catch(Catch(\n","    error_equals=[\"States.TaskFailed\"],\n","    next_step=Fail(\"CreateDataSetGroupTaskFailed\")\n","))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"XQ1PtIUyJ2Db"},"source":["#### Create Dataset\n","\n","Creates an empty dataset and adds it to the specified dataset group. Use CreateDatasetImportJob to import your training data to a dataset.\n","\n","There are three types of datasets:\n","\n","Interactions\n","\n","Items\n","\n","Users\n","\n","Each dataset type has an associated schema with required field types. Only the Interactions dataset is required in order to train a model (also referred to as creating a solution)."]},{"cell_type":"code","metadata":{"id":"UWLRhD7IJ2Dc"},"source":["lambda_state_createdataset = LambdaStep(\n","    state_id=\"create dataset\",\n","    parameters={  \n","        \"FunctionName\": \"stepfunctioncreatedataset\", #replace with the name of the function you created\n","#        \"Payload\": {  \n","#           \"schemaArn.$\": '$.Payload.schemaArn',\n","#           \"datasetGroupArn.$\": '$.Payload.datasetGroupArn',\n","            \n","            \n","#        }\n","        \n","        \"Payload\": {  \n","           \"schemaArn.$\": '$.schemaArn',\n","           \"datasetGroupArn.$\": '$.datasetGroupArn',        \n","        } \n","        \n","        \n","    },\n","    result_path = '$'\n",")\n","\n","lambda_state_createdataset.add_retry(Retry(\n","    error_equals=[\"States.TaskFailed\"],\n","    interval_seconds=5,\n","    max_attempts=1,\n","    backoff_rate=4.0\n","))\n","\n","lambda_state_createdataset.add_catch(Catch(\n","    error_equals=[\"States.TaskFailed\"],\n","    next_step=Fail(\"CreateDataSetTaskFailed\")\n","))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SMetj_NqJ2Dc"},"source":["#### Create Dataset Import Job\n","\n","When you have completed Step 1: Creating a Dataset Group and Step 2: Creating a Dataset and a Schema, you are ready to import your training data into Amazon Personalize. When you import data, you can choose to import records in bulk, import records individually, or both, depending on your business requirements and the amount of historical data you have collected. If you have a large amount of historical records, \n","we recommend you first import data in bulk and then add data incrementally as necessary."]},{"cell_type":"code","metadata":{"id":"DP4zbP-WJ2Dd"},"source":["lambda_state_datasetimportjob = LambdaStep(\n","    state_id=\"create dataset import job\",\n","    parameters={  \n","        \"FunctionName\": \"stepfunction-createdatasetimportjob\", #replace with the name of the function you created\n","        \"Payload\": {  \n","           \"datasetimportjob\": \"stepfunction-createdatasetimportjob\",\n","           \"dataset_arn.$\": '$.Payload.dataset_arn',\n","           \"datasetGroupArn.$\": '$.Payload.datasetGroupArn',\n","           \"bucket_name\": bucket,\n","           \"file_name\": filename,\n","           \"role_arn\": role_arn\n","            \n","        }\n","    },\n","\n","    result_path = '$'\n",")\n","\n","lambda_state_datasetimportjob.add_retry(Retry(\n","    error_equals=[\"States.TaskFailed\"],\n","    interval_seconds=5,\n","    max_attempts=1,\n","    backoff_rate=4.0\n","))\n","\n","lambda_state_datasetimportjob.add_catch(Catch(\n","    error_equals=[\"States.TaskFailed\"],\n","    next_step=Fail(\"DatasetImportJobTaskFailed\")\n","))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3qpJV8laJ2Dd"},"source":["#### Create Solution\n","\n","Once you have finished Preparing and Importing Data, you are ready to create a Solution. A Solution refers to the combination of an Amazon Personalize recipe, customized parameters, and one or more solution versions (trained models). Once you create a solution with a solution version, you can create a campaign to deploy the solution version and get recommendations.\n","\n","To create a solution in Amazon Personalize, you do the following:\n","\n","Choose a recipe – A recipe is an Amazon Personalize term specifying an appropriate algorithm to train for a given use case. See Step 1: Choosing a Recipe.\n","\n","Configure a solution – Customize solution parameters and recipe-specific hyperparameters so the model meets your specific business needs. See Step 2: Configuring a Solution.\n","\n","Create a solution version (train a model) – Train the machine learning model Amazon Personalize will use to generate recommendations for your customers. See Step 3: Creating a Solution Version.\n","\n","Evaluate the solution version – Use the metrics Amazon Personalize generates from the new solution version to evaluate the performance of the model. See Step 4: Evaluating the Solution Version.\n"]},{"cell_type":"markdown","metadata":{"id":"jFdpKpBCJ2De"},"source":["#### Choosing a Recipe and Configuring a Solution\n","\n","A recipe is an Amazon Personalize term specifying an appropriate algorithm to train for a given use case. "]},{"cell_type":"code","metadata":{"id":"ZfJ2o9hXJ2De"},"source":["lambda_state_select_receipe_create_solution = LambdaStep(\n","    state_id=\"select receipe and create solution\",\n","    parameters={  \n","        \"FunctionName\": \"stepfunction_select-recipe_create-solution\", #replace with the name of the function you created\n","        \"Payload\": {  \n","           #\"dataset_group_arn.$\": '$.Payload.datasetGroupArn' \n","            \"dataset_group_arn.$\": '$.datasetGroupArn'\n","        }\n","    },\n","    result_path = '$'\n",")\n","\n","lambda_state_select_receipe_create_solution.add_retry(Retry(\n","    error_equals=[\"States.TaskFailed\"],\n","    interval_seconds=5,\n","    max_attempts=1,\n","    backoff_rate=4.0\n","))\n","\n","lambda_state_select_receipe_create_solution.add_catch(Catch(\n","    error_equals=[\"States.TaskFailed\"],\n","    next_step=Fail(\"DatasetReceiptCreateSolutionTaskFailed\")\n","))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"QOf9vztIJ2Df"},"source":["#### Create Solution Version\n","\n","Once you have completed Choosing a Recipe and Configuring a Solution, you are ready to create a Solution Version. A Solution Version refers to a trained machine learning model you can deploy to get recommendations for customers. You can create a solution version using the console, AWS Command Line Interface (AWS CLI), or AWS SDK."]},{"cell_type":"code","metadata":{"id":"fHluaQ2GJ2Df"},"source":["lambda_create_solution_version = LambdaStep(\n","    state_id=\"create solution version\",\n","    parameters={  \n","        \"FunctionName\": \"stepfunction_create_solution_version\", \n","        \"Payload\": {  \n","           \"solution_arn.$\": '$.Payload.solution_arn'           \n","        }\n","    },\n","    result_path = '$'\n",")\n","\n","lambda_create_solution_version.add_retry(Retry(\n","    error_equals=[\"States.TaskFailed\"],\n","    interval_seconds=5,\n","    max_attempts=1,\n","    backoff_rate=4.0\n","))\n","\n","lambda_create_solution_version.add_catch(Catch(\n","    error_equals=[\"States.TaskFailed\"],\n","    next_step=Fail(\"CreateSolutionVersionTaskFailed\")\n","))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1UX-hOvUJ2Df"},"source":["#### Create Campaign\n","\n","A campaign is used to make recommendations for your users. You create a campaign by deploying a solution version"]},{"cell_type":"code","metadata":{"id":"WCh3ylXEJ2Dg"},"source":["lambda_create_campaign = LambdaStep(\n","    state_id=\"create campaign\",\n","    parameters={  \n","        \"FunctionName\": \"stepfunction_getsolution_metric_create_campaign\", \n","        \"Payload\": {  \n","            #\"solution_version_arn.$\": '$.Payload.solution_version_arn'  \n","            \"solution_version_arn.$\": '$.solution_version_arn'\n","        }\n","    },\n","    result_path = '$'\n",")\n","\n","lambda_create_campaign.add_retry(Retry(\n","    error_equals=[\"States.TaskFailed\"],\n","    interval_seconds=5,\n","    max_attempts=1,\n","    backoff_rate=4.0\n","))\n","\n","lambda_create_campaign.add_catch(Catch(\n","    error_equals=[\"States.TaskFailed\"],\n","    next_step=Fail(\"CreateCampaignTaskFailed\")\n","))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"k5G5abaGJ2Dg"},"source":["## Wait-States\n","\n","#### A `Wait` state in Step Functions waits a specific amount of time. See [Wait](https://aws-step-functions-data-science-sdk.readthedocs.io/en/latest/states.html#stepfunctions.steps.states.Wait) in the AWS Step Functions Data Science SDK documentation."]},{"cell_type":"markdown","metadata":{"id":"f9-_p09KJ2Dg"},"source":["#### Wait for Schema to be ready"]},{"cell_type":"code","metadata":{"id":"asINKENUJ2Dh"},"source":["wait_state_schema = Wait(\n","    state_id=\"Wait for create schema - 5 secs\",\n","    seconds=5\n",")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"aQaJYp7mJ2Dj"},"source":["#### Wait for Datasetgroup to be ready"]},{"cell_type":"code","metadata":{"id":"QSWOitkzJ2Dk"},"source":["wait_state_datasetgroup = Wait(\n","    state_id=\"Wait for datasetgroup - 30 secs\",\n","    seconds=30\n",")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VygShtL1J2Dk"},"source":["#### Wait for Dataset to be ready"]},{"cell_type":"code","metadata":{"id":"rp0AXddVJ2Dk"},"source":["wait_state_dataset = Wait(\n","    state_id=\"wait for dataset - 30 secs\",\n","    seconds=30\n",")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fjTGtNNZJ2Dl"},"source":["#### Wait for Dataset Import Job to be ACTIVE"]},{"cell_type":"code","metadata":{"id":"2Ptw09z8J2Dl"},"source":["wait_state_datasetimportjob = Wait(\n","    state_id=\"Wait for datasetimportjob - 30 secs\",\n","    seconds=30\n",")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"UR3BtVUGJ2Dl"},"source":["#### Wait for Receipe to ready"]},{"cell_type":"code","metadata":{"id":"5wgQMQwPJ2Dl"},"source":["wait_state_receipe = Wait(\n","    state_id=\"Wait for receipe - 30 secs\",\n","    seconds=30\n",")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"FX5m4LZQJ2Dl"},"source":["#### Wait for Solution Version to be ACTIVE"]},{"cell_type":"code","metadata":{"id":"TTtvsSW3J2Dm"},"source":["wait_state_solutionversion = Wait(\n","    state_id=\"Wait for solution version - 60 secs\",\n","    seconds=60\n",")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"aE9j-RvgJ2Dm"},"source":["#### Wait for Campaign to be ACTIVE"]},{"cell_type":"code","metadata":{"id":"WpkBzy5RJ2Dm"},"source":["wait_state_campaign = Wait(\n","    state_id=\"Wait for Campaign - 30 secs\",\n","    seconds=30\n",")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_uPinnogJ2Dn"},"source":["\n","\n","### Check status of the lambda task and take action accordingly"]},{"cell_type":"markdown","metadata":{"id":"b9F_mwJiJ2Dn"},"source":["#### If a state fails, move it to `Fail` state. See [Fail](https://aws-step-functions-data-science-sdk.readthedocs.io/en/latest/states.html#stepfunctions.steps.states.Fail) in the AWS Step Functions Data Science SDK documentation."]},{"cell_type":"markdown","metadata":{"id":"TDBopEIcJ2Dn"},"source":["### check datasetgroup status"]},{"cell_type":"code","metadata":{"id":"h93BkFW0J2Do"},"source":["lambda_state_datasetgroupstatus = LambdaStep(\n","    state_id=\"check dataset Group status\",\n","    parameters={  \n","        \"FunctionName\": \"stepfunction_waitforDatasetGroup\", #replace with the name of the function you created\n","        \"Payload\": {  \n","           \"input.$\": '$.Payload.datasetGroupArn',\n","           \"schemaArn.$\": '$.Payload.schemaArn'\n","        }\n","    },\n","    result_path = '$'\n",")\n","\n","lambda_state_datasetgroupstatus.add_retry(Retry(\n","    error_equals=[\"States.TaskFailed\"],\n","    interval_seconds=5,\n","    max_attempts=1,\n","    backoff_rate=4.0\n","))\n","\n","lambda_state_datasetgroupstatus.add_catch(Catch(\n","    error_equals=[\"States.TaskFailed\"],\n","    next_step=Fail(\"DatasetGroupStatusTaskFailed\")\n","))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HKoSh3sXJ2Do"},"source":["### check dataset import job status"]},{"cell_type":"code","metadata":{"id":"NuD_ceTSJ2Do"},"source":["lambda_state_datasetimportjob_status = LambdaStep(\n","    state_id=\"check dataset import job status\",\n","    parameters={  \n","        \"FunctionName\": \"stepfunction_waitfordatasetimportjob\", #replace with the name of the function you created\n","        \"Payload\": {  \n","           \"dataset_import_job_arn.$\": '$.Payload.dataset_import_job_arn',\n","           \"datasetGroupArn.$\": '$.Payload.datasetGroupArn'\n","        }\n","    },\n","    result_path = '$'\n",")\n","\n","lambda_state_datasetimportjob_status.add_retry(Retry(\n","    error_equals=[\"States.TaskFailed\"],\n","    interval_seconds=5,\n","    max_attempts=1,\n","    backoff_rate=4.0\n","))\n","\n","lambda_state_datasetimportjob_status.add_catch(Catch(\n","    error_equals=[\"States.TaskFailed\"],\n","    next_step=Fail(\"DatasetImportJobStatusTaskFailed\")\n","))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ceFrym6bJ2Do"},"source":["### check solution version status"]},{"cell_type":"code","metadata":{"id":"Y7Em6gevJ2Dp"},"source":["\n","solutionversion_succeed_state = Succeed(\n","    state_id=\"The Solution Version ready?\"\n",")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"rN3K8w3UJ2Dp"},"source":["lambda_state_solutionversion_status = LambdaStep(\n","    state_id=\"check solution version status\",\n","    parameters={  \n","        \"FunctionName\": \"stepfunction_waitforSolutionVersion\", #replace with the name of the function you created\n","        \"Payload\": {  \n","           \"solution_version_arn.$\": '$.Payload.solution_version_arn'           \n","        }\n","    },\n","    result_path = '$'\n",")\n","\n","lambda_state_solutionversion_status.add_retry(Retry(\n","    error_equals=[\"States.TaskFailed\"],\n","    interval_seconds=5,\n","    max_attempts=1,\n","    backoff_rate=4.0\n","))\n","\n","lambda_state_solutionversion_status.add_catch(Catch(\n","    error_equals=[\"States.TaskFailed\"],\n","    next_step=Fail(\"SolutionVersionStatusTaskFailed\")\n","))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BZXi905gJ2Dp"},"source":["### check campaign status"]},{"cell_type":"code","metadata":{"id":"L6JKukjJJ2Dp"},"source":["lambda_state_campaign_status = LambdaStep(\n","    state_id=\"check campaign status\",\n","    parameters={  \n","        \"FunctionName\": \"stepfunction_waitforCampaign\", #replace with the name of the function you created\n","        \"Payload\": {  \n","           \"campaign_arn.$\": '$.Payload.campaign_arn'           \n","        }\n","    },\n","    result_path = '$'\n",")\n","\n","lambda_state_campaign_status.add_retry(Retry(\n","    error_equals=[\"States.TaskFailed\"],\n","    interval_seconds=5,\n","    max_attempts=1,\n","    backoff_rate=4.0\n","))\n","\n","lambda_state_campaign_status.add_catch(Catch(\n","    error_equals=[\"States.TaskFailed\"],\n","    next_step=Fail(\"CampaignStatusTaskFailed\")\n","))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"B-Epm1VwJ2Dq"},"source":["## Choice-States\n","\n","Now, attach branches to the Choice state you created earlier. See *Choice Rules* in the [AWS Step Functions Data Science SDK documentation](https://aws-step-functions-data-science-sdk.readthedocs.io) ."]},{"cell_type":"markdown","metadata":{"id":"0tmx9V9SJ2Dq"},"source":["#### Chain together steps for the define the workflow path\n","\n","The following cell links together the steps you've created above into a sequential group. The new path sequentially includes the Lambda state, Wait state, and the Succeed state that you created earlier.\n","\n","#### After chaining together the steps for the workflow path, we will define and visualize the workflow."]},{"cell_type":"code","metadata":{"id":"0AX1lwVVJ2Dq"},"source":["create_campaign_choice_state = Choice(\n","    state_id=\"Is the Campaign ready?\"\n",")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"S03MmV_NJ2Dq"},"source":["create_campaign_choice_state.add_choice(\n","    rule=ChoiceRule.StringEquals(variable=lambda_state_campaign_status.output()['Payload']['status'], value='ACTIVE'),\n","    next_step=Succeed(\"CampaignCreatedSuccessfully\")     \n",")\n","create_campaign_choice_state.add_choice(\n","    ChoiceRule.StringEquals(variable=lambda_state_campaign_status.output()['Payload']['status'], value='CREATE PENDING'),\n","    next_step=wait_state_campaign\n",")\n","create_campaign_choice_state.add_choice(\n","    ChoiceRule.StringEquals(variable=lambda_state_campaign_status.output()['Payload']['status'], value='CREATE IN_PROGRESS'),\n","    next_step=wait_state_campaign\n",")\n","\n","create_campaign_choice_state.default_choice(next_step=Fail(\"CreateCampaignFailed\"))\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8_WGDhgvJ2Dr"},"source":["solutionversion_choice_state = Choice(\n","    state_id=\"Is the Solution Version ready?\"\n",")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"XhLvfSiLJ2Dr"},"source":["solutionversion_succeed_state = Succeed(\n","    state_id=\"The Solution Version ready?\"\n",")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wPly7Zk0J2Dr"},"source":["solutionversion_choice_state.add_choice(\n","    rule=ChoiceRule.StringEquals(variable=lambda_state_solutionversion_status.output()['Payload']['status'], value='ACTIVE'),\n","    next_step=solutionversion_succeed_state   \n",")\n","solutionversion_choice_state.add_choice(\n","    ChoiceRule.StringEquals(variable=lambda_state_solutionversion_status.output()['Payload']['status'], value='CREATE PENDING'),\n","    next_step=wait_state_solutionversion\n",")\n","solutionversion_choice_state.add_choice(\n","    ChoiceRule.StringEquals(variable=lambda_state_solutionversion_status.output()['Payload']['status'], value='CREATE IN_PROGRESS'),\n","    next_step=wait_state_solutionversion\n",")\n","\n","solutionversion_choice_state.default_choice(next_step=Fail(\"create_solution_version_failed\"))\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"D8RHrx4xJ2Dr"},"source":["datasetimportjob_succeed_state = Succeed(\n","    state_id=\"The Solution Version ready?\"\n",")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Zncia-QOJ2Dr"},"source":["datasetimportjob_choice_state = Choice(\n","    state_id=\"Is the DataSet Import Job ready?\"\n",")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZW6uELggJ2Ds"},"source":["datasetimportjob_choice_state.add_choice(\n","    rule=ChoiceRule.StringEquals(variable=lambda_state_datasetimportjob_status.output()['Payload']['status'], value='ACTIVE'),\n","    next_step=datasetimportjob_succeed_state   \n",")\n","datasetimportjob_choice_state.add_choice(\n","    ChoiceRule.StringEquals(variable=lambda_state_datasetimportjob_status.output()['Payload']['status'], value='CREATE PENDING'),\n","    next_step=wait_state_datasetimportjob\n",")\n","datasetimportjob_choice_state.add_choice(\n","    ChoiceRule.StringEquals(variable=lambda_state_datasetimportjob_status.output()['Payload']['status'], value='CREATE IN_PROGRESS'),\n","    next_step=wait_state_datasetimportjob\n",")\n","\n","\n","datasetimportjob_choice_state.default_choice(next_step=Fail(\"dataset_import_job_failed\"))\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"vny-Ir9zJ2Ds"},"source":["datasetgroupstatus_choice_state = Choice(\n","    state_id=\"Is the DataSetGroup ready?\"\n",")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WyQP4BX3J2Ds"},"source":["## Workflow"]},{"cell_type":"markdown","metadata":{"id":"sc4tPUYiJ2Ds"},"source":["### Define Workflow\n","\n","In the following cell, you will define the step that you will use in our workflow.  Then you will create, visualize and execute the workflow. \n","\n","Steps relate to states in AWS Step Functions. For more information, see [States](https://docs.aws.amazon.com/step-functions/latest/dg/concepts-states.html) in the *AWS Step Functions Developer Guide*. For more information on the AWS Step Functions Data Science SDK APIs, see: https://aws-step-functions-data-science-sdk.readthedocs.io. \n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"IW5Zs1iFJ2Ds"},"source":["### Dataset workflow"]},{"cell_type":"code","metadata":{"id":"kC-LSu5yJ2Dt"},"source":["Dataset_workflow_definition=Chain([lambda_state_schema,\n","                                   wait_state_schema,\n","                                   lambda_state_datasetgroup,\n","                                   wait_state_datasetgroup,\n","                                   lambda_state_datasetgroupstatus\n","                                  ])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"dj4lFc3DJ2Dt"},"source":["Dataset_workflow = Workflow(\n","    name=\"Dataset-workflow\",\n","    definition=Dataset_workflow_definition,\n","    role=workflow_execution_role\n",")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZIReHZHSJ2Dt"},"source":["Dataset_workflow.render_graph()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Rfy9NZ4KJ2Dt","outputId":"77778fd2-2eda-4dda-ec90-e0ed370fd37c"},"source":["DatasetWorkflowArn = Dataset_workflow.create()"],"execution_count":null,"outputs":[{"name":"stdout","output_type":"stream","text":["\u001b[32m[INFO] Workflow created successfully on AWS Step Functions.\u001b[0m\n"]}]},{"cell_type":"markdown","metadata":{"id":"DAfPk0bOJ2Du"},"source":["### DatasetImportWorkflow"]},{"cell_type":"code","metadata":{"id":"QwwOlnDGJ2Du"},"source":["DatasetImport_workflow_definition=Chain([lambda_state_createdataset,\n","                                   wait_state_dataset,\n","                                   lambda_state_datasetimportjob,\n","                                   wait_state_datasetimportjob,\n","                                   lambda_state_datasetimportjob_status,\n","                                   datasetimportjob_choice_state\n","                                  ])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"JCXHOq0gJ2Du"},"source":["DatasetImport_workflow = Workflow(\n","    name=\"DatasetImport-workflow\",\n","    definition=DatasetImport_workflow_definition,\n","    role=workflow_execution_role\n",")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"WEhqxRT2J2Du"},"source":["DatasetImport_workflow.render_graph()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"eN9ZSS--J2Dv","outputId":"34aa754b-58d4-4716-dfd8-4719b92eb3e7"},"source":["DatasetImportflowArn = DatasetImport_workflow.create()"],"execution_count":null,"outputs":[{"name":"stdout","output_type":"stream","text":["\u001b[32m[INFO] Workflow created successfully on AWS Step Functions.\u001b[0m\n"]}]},{"cell_type":"markdown","metadata":{"id":"ukKZGxmGJ2Dw"},"source":["Recepie and Solution workflow"]},{"cell_type":"code","metadata":{"id":"MF4Ywk3PJ2Dw"},"source":["Create_receipe_sol_workflow_definition=Chain([lambda_state_select_receipe_create_solution,\n","                                   wait_state_receipe,\n","                                   lambda_create_solution_version,\n","                                   wait_state_solutionversion,\n","                                   lambda_state_solutionversion_status,\n","                                   solutionversion_choice_state\n","                                  ])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zADxSkQnJ2Dx"},"source":["Create_receipe_sol_workflow = Workflow(\n","    name=\"Create_receipe_sol-workflow\",\n","    definition=Create_receipe_sol_workflow_definition,\n","    role=workflow_execution_role\n",")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"PdZ-CA_UJ2D0"},"source":["Create_receipe_sol_workflow.render_graph()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zT4gW6YTJ2D1","outputId":"b59ea1d1-2681-4672-ab60-6d3aabbd75e4"},"source":["CreateReceipeArn = Create_receipe_sol_workflow.create()"],"execution_count":null,"outputs":[{"name":"stdout","output_type":"stream","text":["\u001b[32m[INFO] Workflow created successfully on AWS Step Functions.\u001b[0m\n"]}]},{"cell_type":"markdown","metadata":{"id":"MOvlFUOTJ2D2"},"source":["Create Campaign Workflow"]},{"cell_type":"code","metadata":{"id":"gPw3BTGnJ2D3"},"source":["Create_Campaign_workflow_definition=Chain([lambda_create_campaign,\n","                                   wait_state_campaign,\n","                                   lambda_state_campaign_status,\n","                                   wait_state_datasetimportjob,\n","                                   create_campaign_choice_state\n","                                  ])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"RWDpc6MjJ2D4"},"source":["Campaign_workflow = Workflow(\n","    name=\"Campaign-workflow\",\n","    definition=Create_Campaign_workflow_definition,\n","    role=workflow_execution_role\n",")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"LUYkcXRTJ2D4"},"source":["Campaign_workflow.render_graph()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NH6mxXIjJ2D4","outputId":"66119000-5388-47ef-cf0f-eb896b50b529"},"source":["CreateCampaignArn = Campaign_workflow.create()"],"execution_count":null,"outputs":[{"name":"stdout","output_type":"stream","text":["\u001b[32m[INFO] Workflow created successfully on AWS Step Functions.\u001b[0m\n"]}]},{"cell_type":"markdown","metadata":{"id":"G0pfkXRAJ2D5"},"source":["Main workflow"]},{"cell_type":"code","metadata":{"id":"kMD1jsRYJ2D5"},"source":["call_dataset_workflow_state = Task(\n","    state_id=\"DataSetWorkflow\",\n","    resource=\"arn:aws:states:::states:startExecution.sync:2\",\n","    parameters={\n","                                \"Input\": \"true\",\n","                                #\"StateMachineArn\": \"arn:aws:states:us-east-1:444602785259:stateMachine:Dataset-workflow\",\n","                                \"StateMachineArn\": DatasetWorkflowArn\n","                }\n",")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6cWeYi2rJ2D5"},"source":["call_datasetImport_workflow_state = Task(\n","    state_id=\"DataSetImportWorkflow\",\n","    resource=\"arn:aws:states:::states:startExecution.sync:2\",\n","    parameters={\n","                                 \"Input\":{\n","                                    \"schemaArn.$\": \"$.Output.Payload.schemaArn\",\n","                                    \"datasetGroupArn.$\": \"$.Output.Payload.datasetGroupArn\"\n","                                   },\n","                                \"StateMachineArn\": DatasetImportflowArn,\n","                }\n",")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"c0alenEpJ2D6"},"source":["call_receipe_solution_workflow_state = Task(\n","    state_id=\"ReceipeSolutionWorkflow\",\n","    resource=\"arn:aws:states:::states:startExecution.sync:2\",\n","    parameters={\n","                                 \"Input\":{\n","                                    \"datasetGroupArn.$\": \"$.Output.Payload.datasetGroupArn\"\n","\n","                                   },\n","                                \"StateMachineArn\": CreateReceipeArn\n","                }\n",")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"uM_Z_eAkJ2D6"},"source":["call_campaign_solution_workflow_state = Task(\n","    state_id=\"CampaignWorkflow\",\n","    resource=\"arn:aws:states:::states:startExecution.sync:2\",\n","    parameters={\n","                                 \"Input\":{\n","                                    \"solution_version_arn.$\": \"$.Output.Payload.solution_version_arn\"\n","\n","                                   },\n","                                \"StateMachineArn\": CreateCampaignArn\n","                }\n",")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"lnfdK7nnJ2D6"},"source":["Main_workflow_definition=Chain([call_dataset_workflow_state,\n","                                call_datasetImport_workflow_state,\n","                                call_receipe_solution_workflow_state,\n","                                call_campaign_solution_workflow_state\n","                               ])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qOA8ka1ZJ2D7"},"source":["Main_workflow = Workflow(\n","    name=\"Main-workflow\",\n","    definition=Main_workflow_definition,\n","    role=workflow_execution_role\n",")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jmP40EZOJ2D7"},"source":["Main_workflow.render_graph()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hnmL_wd3J2D7","outputId":"0bd7e7c0-2ced-4f22-e232-5161943f02e6"},"source":["Main_workflow.create()"],"execution_count":null,"outputs":[{"name":"stdout","output_type":"stream","text":["\u001b[32m[INFO] Workflow created successfully on AWS Step Functions.\u001b[0m\n"]},{"data":{"text/plain":["'arn:aws:states:us-east-1:261602857181:stateMachine:Main-workflow'"]},"execution_count":950,"metadata":{},"output_type":"execute_result"}]},{"cell_type":"code","metadata":{"id":"5aybLT6IJ2D8","outputId":"1ca77a2f-e382-40ba-ccc5-a00639d4569c"},"source":["Main_workflow_execution = Main_workflow.execute()"],"execution_count":null,"outputs":[{"name":"stdout","output_type":"stream","text":["\u001b[32m[INFO] Workflow execution started successfully on AWS Step Functions.\u001b[0m\n"]}]},{"cell_type":"markdown","metadata":{"id":"psUCkhxRJ2D8"},"source":["Main_workflow_execution = Workflow(\n","    name=\"Campaign_Workflow\",\n","    definition=path1,\n","    role=workflow_execution_role\n",")\n"]},{"cell_type":"code","metadata":{"id":"v58VTqGYJ2D8"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"aJn_ZPAZJ2D8"},"source":["#Main_workflow_execution.render_graph()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jDLfzQa6J2D9"},"source":["### Create and execute the workflow\n","\n","In the next cells, we will create the branching happy workflow in AWS Step Functions with [create](https://aws-step-functions-data-science-sdk.readthedocs.io/en/latest/workflow.html#stepfunctions.workflow.Workflow.create) and execute it with [execute](https://aws-step-functions-data-science-sdk.readthedocs.io/en/latest/workflow.html#stepfunctions.workflow.Workflow.execute).\n"]},{"cell_type":"code","metadata":{"id":"M0mkIbrZJ2D9"},"source":["#personalize_workflow.create()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"WKSKUVduJ2D9"},"source":["#personalize_workflow_execution = happy_workflow.execute()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NRGHAcZAJ2D9"},"source":["###  Review the workflow progress\n","\n","Review the workflow progress with the [render_progress](https://aws-step-functions-data-science-sdk.readthedocs.io/en/latest/workflow.html#stepfunctions.workflow.Execution.render_progress).\n","\n","Review the execution history by calling [list_events](https://aws-step-functions-data-science-sdk.readthedocs.io/en/latest/workflow.html#stepfunctions.workflow.Execution.list_events) to list all events in the workflow execution."]},{"cell_type":"code","metadata":{"scrolled":true,"id":"GL_pv7j5J2D9"},"source":["Main_workflow_execution.render_progress()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"H0Cbo_KcJ2D-"},"source":["Main_workflow_execution.list_events(html=True)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"XcjmxIS9J2D-"},"source":["## Generate-Recommendations"]},{"cell_type":"markdown","metadata":{"id":"MlR8uXCdJ2D-"},"source":["### Now that we have a successful campaign, let's generate recommendations for the campaign"]},{"cell_type":"markdown","metadata":{"id":"j3GGtZRNJ2D-"},"source":["#### Select a User and an Item"]},{"cell_type":"code","metadata":{"id":"DNqNpKWgJ2D_","outputId":"56e00557-43eb-4dc4-cbf0-de2ddc13a95b"},"source":["items = pd.read_csv('./ml-100k/u.item', sep='|', usecols=[0,1], encoding='latin-1')\n","items.columns = ['ITEM_ID', 'TITLE']\n","\n","\n","user_id, item_id, rating, timestamp = data.sample().values[0]\n","\n","user_id = int(user_id)\n","item_id = int(item_id)\n","\n","print(\"user_id\",user_id)\n","print(\"items\",items)\n","\n","\n","item_title = items.loc[items['ITEM_ID'] == item_id].values[0][-1]\n","print(\"USER: {}\".format(user_id))\n","print(\"ITEM: {}\".format(item_title))\n","print(\"ITEM ID: {}\".format(item_id))\n"],"execution_count":null,"outputs":[{"name":"stdout","output_type":"stream","text":["user_id 402\n","items       ITEM_ID                                      TITLE\n","0           2                           GoldenEye (1995)\n","1           3                          Four Rooms (1995)\n","...       ...                                        ...\n","1679     1681                        You So Crazy (1994)\n","1680     1682  Scream of Stone (Schrei aus Stein) (1991)\n","\n","[1681 rows x 2 columns]\n","USER: 402\n","ITEM: Aladdin (1992)\n","ITEM ID: 95\n"]}]},{"cell_type":"code","metadata":{"id":"Wup_JzVPJ2D_"},"source":["wait_recommendations = Wait(\n","    state_id=\"Wait for recommendations - 10 secs\",\n","    seconds=10\n",")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GhmMBRCvJ2D_"},"source":["#### Lambda Task"]},{"cell_type":"code","metadata":{"id":"f_FBoyjUJ2D_"},"source":["lambda_state_get_recommendations = LambdaStep(\n","    state_id=\"get recommendations\",\n","    parameters={  \n","        \"FunctionName\": \"stepfunction_getRecommendations\", \n","        \"Payload\": {  \n","           \"campaign_arn\": 'arn:aws:personalize:us-east-1:261602857181:campaign/stepfunction-campaign',            \n","           \"user_id\": user_id,  \n","           \"item_id\": item_id             \n","        }\n","    },\n","    result_path = '$'\n",")\n","\n","lambda_state_get_recommendations.add_retry(Retry(\n","    error_equals=[\"States.TaskFailed\"],\n","    interval_seconds=5,\n","    max_attempts=1,\n","    backoff_rate=4.0\n","))\n","\n","lambda_state_get_recommendations.add_catch(Catch(\n","    error_equals=[\"States.TaskFailed\"],\n","    next_step=Fail(\"GetRecommendationTaskFailed\")\n","    #next_step=recommendation_path   \n","))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wCHTwoYOJ2EA"},"source":["#### Create a Succeed State"]},{"cell_type":"code","metadata":{"id":"p9oPY3XwJ2EA"},"source":["workflow_complete = Succeed(\"WorkflowComplete\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"lXKaj9ySJ2EA"},"source":["recommendation_path = Chain([ \n","lambda_state_get_recommendations,\n","wait_recommendations,\n","workflow_complete\n","])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"UnFfflboJ2EA"},"source":["### Define, Create, Render, and Execute Recommendation Workflow\n","\n","In the next cells, we will create a workflow in AWS Step Functions with [create](https://aws-step-functions-data-science-sdk.readthedocs.io/en/latest/workflow.html#stepfunctions.workflow.Workflow.create) and execute it with [execute](https://aws-step-functions-data-science-sdk.readthedocs.io/en/latest/workflow.html#stepfunctions.workflow.Workflow.execute)."]},{"cell_type":"code","metadata":{"id":"FfZgDTMrJ2EB"},"source":["recommendation_workflow = Workflow(\n","    name=\"Recommendation_Workflow4\",\n","    definition=recommendation_path,\n","    role=workflow_execution_role\n",")\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1dYso8YqJ2EB"},"source":["recommendation_workflow.render_graph()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Oj3jJUC5J2EB","outputId":"b8fd34a3-d0fc-45b5-901a-f14806474501"},"source":["recommendation_workflow.create()"],"execution_count":null,"outputs":[{"name":"stdout","output_type":"stream","text":["\u001b[32m[INFO] Workflow created successfully on AWS Step Functions.\u001b[0m\n"]},{"data":{"text/plain":["'arn:aws:states:us-east-1:261602857181:stateMachine:Recommendation_Workflow4'"]},"execution_count":971,"metadata":{},"output_type":"execute_result"}]},{"cell_type":"code","metadata":{"id":"zRSuIQwxJ2EB","outputId":"c9c43a18-30d3-408b-f3ba-03c8c76172e4"},"source":["recommendation_workflow_execution = recommendation_workflow.execute()"],"execution_count":null,"outputs":[{"name":"stdout","output_type":"stream","text":["\u001b[32m[INFO] Workflow execution started successfully on AWS Step Functions.\u001b[0m\n"]}]},{"cell_type":"code","metadata":{"id":"zwsdMiVVJ2EC"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ZC4j4ZDLJ2EE"},"source":["### Review progress\n","\n","Review workflow progress with the [render_progress](https://aws-step-functions-data-science-sdk.readthedocs.io/en/latest/workflow.html#stepfunctions.workflow.Execution.render_progress).\n","\n","Review execution history by calling [list_events](https://aws-step-functions-data-science-sdk.readthedocs.io/en/latest/workflow.html#stepfunctions.workflow.Execution.list_events) to list all events in the workflow execution."]},{"cell_type":"code","metadata":{"id":"9LeIbX-WJ2EE"},"source":["recommendation_workflow_execution.render_progress()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"scrolled":false,"id":"rdf-Qk5WJ2EF"},"source":["recommendation_workflow_execution.list_events(html=True)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1JxN8KAvJ2EF"},"source":["item_list = recommendation_workflow_execution.get_output()['Payload']['item_list']"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Ru-i3odAJ2EF"},"source":["### Get Recommendations"]},{"cell_type":"code","metadata":{"id":"TtHJWsX2J2EF","outputId":"8980d474-18fe-4ca8-ec1b-56d33e0d3696"},"source":["item_list = recommendation_workflow_execution.get_output()['Payload']['item_list']\n","\n","print(\"Recommendations:\")\n","for item in item_list:\n","    np.int(item['itemId'])\n","    item_title = items.loc[items['ITEM_ID'] == np.int(item['itemId'])].values[0][-1]\n","    print(item_title)\n"],"execution_count":null,"outputs":[{"name":"stdout","output_type":"stream","text":["Recommendations:\n","Sound of Music, The (1965)\n","Raiders of the Lost Ark (1981)\n","Beauty and the Beast (1991)\n","Snow White and the Seven Dwarfs (1937)\n","Indiana Jones and the Last Crusade (1989)\n","Fantasia (1940)\n","Princess Bride, The (1987)\n","Empire Strikes Back, The (1980)\n","Terminator, The (1984)\n","One Flew Over the Cuckoo's Nest (1975)\n","Casablanca (1942)\n","Dead Poets Society (1989)\n","Fugitive, The (1993)\n","Graduate, The (1967)\n","M*A*S*H (1970)\n","Monty Python and the Holy Grail (1974)\n","Lion King, The (1994)\n","Apollo 13 (1995)\n","When Harry Met Sally... (1989)\n","My Fair Lady (1964)\n","Patton (1970)\n","Winnie the Pooh and the Blustery Day (1968)\n","Vertigo (1958)\n","Aladdin (1992)\n","Schindler's List (1993)\n"]}]},{"cell_type":"markdown","metadata":{"id":"ulmWlmDtJ2EG"},"source":["## Clean up Amazon Personalize resources"]},{"cell_type":"markdown","metadata":{"id":"0r-vw0cdJ2EG"},"source":["Make sure to clean up the Amazon Personalize and the state machines created blog. Login to Amazon Personalize console and delete resources such as Dataset Groups, Dataset, Solutions, Receipts, and Campaign. "]},{"cell_type":"markdown","metadata":{"id":"0_XJaJhvJ2EG"},"source":["## Clean up State Machine resources"]},{"cell_type":"code","metadata":{"id":"FXwtOF8rJ2EG","outputId":"b96b903c-975f-4eb3-e842-9400fc6961fb"},"source":["Campaign_workflow.delete()\n","\n","recommendation_workflow.delete()\n","\n","Main_workflow.delete()\n","\n","Create_receipe_sol_workflow.delete()\n","\n","DatasetImport_workflow.delete()\n","\n","Dataset_workflow.delete()\n"],"execution_count":null,"outputs":[{"name":"stdout","output_type":"stream","text":["\u001b[32m[INFO] Workflow has been marked for deletion. If the workflow has running executions, it will be deleted when all executions are stopped.\u001b[0m\n","\u001b[32m[INFO] Workflow has been marked for deletion. If the workflow has running executions, it will be deleted when all executions are stopped.\u001b[0m\n","\u001b[32m[INFO] Workflow has been marked for deletion. If the workflow has running executions, it will be deleted when all executions are stopped.\u001b[0m\n","\u001b[32m[INFO] Workflow has been marked for deletion. If the workflow has running executions, it will be deleted when all executions are stopped.\u001b[0m\n","\u001b[32m[INFO] Workflow has been marked for deletion. If the workflow has running executions, it will be deleted when all executions are stopped.\u001b[0m\n","\u001b[32m[INFO] Workflow has been marked for deletion. If the workflow has running executions, it will be deleted when all executions are stopped.\u001b[0m\n"]}]}]}