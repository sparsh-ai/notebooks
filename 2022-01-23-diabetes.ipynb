{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"2022-01-23-diabetes.ipynb","provenance":[{"file_id":"https://github.com/recohut/nbs/blob/main/raw/T239452%20%7C%20Building%20a%20Simple%20Classifier%20in%20Keras%20to%20Detect%20Diabetes%20in%20Patients.ipynb","timestamp":1644662659774},{"file_id":"1LWU_OWDdYB6A4bctZBZ1s4y8P908Fz4C","timestamp":1637682021193}],"collapsed_sections":[],"toc_visible":true,"mount_file_id":"1LWU_OWDdYB6A4bctZBZ1s4y8P908Fz4C","authorship_tag":"ABX9TyM9J0C6xZAMtptu/S/Jqpuh"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Building a Simple Classifier in Keras to Detect Diabetes in Patients"],"metadata":{"id":"_VHQRn5uihp7"}},{"cell_type":"markdown","metadata":{"id":"qPxZLdMIheJ_"},"source":["## Setup"]},{"cell_type":"code","metadata":{"id":"IaUvh0t2a-PQ"},"source":["import keras\n","from keras.models import Sequential\n","from keras.layers import Dense\n","from keras.callbacks import ModelCheckpoint\n","import matplotlib.pyplot as plt\n","from sklearn.model_selection import train_test_split\n","from sklearn.model_selection import StratifiedKFold\n","import numpy\n","numpy.random.seed(seed)\n","import warnings\n","\n","warnings.filterwarnings(\"ignore\", category=FutureWarning)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1N7_NLdna_9s"},"source":["seed = 7\n","numpy.random.seed(seed)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OoYWcddCaz4z","executionInfo":{"status":"ok","timestamp":1637683201271,"user_tz":-330,"elapsed":27,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}},"outputId":"90663a38-0df8-4983-bad1-ec683f043563"},"source":["dataset = numpy.loadtxt(\"https://github.com/sparsh-ai/general-ml/raw/S142234/Hand-On%2BKeras%2B-%2BCase%2BStudy%2BPima%2BIndians%2Bdataset/pima.csv\", delimiter=\",\")\n","dataset.shape"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(768, 9)"]},"metadata":{},"execution_count":3}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8wEdobs2bGmJ","executionInfo":{"status":"ok","timestamp":1637683201272,"user_tz":-330,"elapsed":15,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}},"outputId":"e80dc038-3fa1-414c-ce44-44f4ed7c8e64"},"source":["dataset[:10,:]"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[6.000e+00, 1.480e+02, 7.200e+01, 3.500e+01, 0.000e+00, 3.360e+01,\n","        6.270e-01, 5.000e+01, 1.000e+00],\n","       [1.000e+00, 8.500e+01, 6.600e+01, 2.900e+01, 0.000e+00, 2.660e+01,\n","        3.510e-01, 3.100e+01, 0.000e+00],\n","       [8.000e+00, 1.830e+02, 6.400e+01, 0.000e+00, 0.000e+00, 2.330e+01,\n","        6.720e-01, 3.200e+01, 1.000e+00],\n","       [1.000e+00, 8.900e+01, 6.600e+01, 2.300e+01, 9.400e+01, 2.810e+01,\n","        1.670e-01, 2.100e+01, 0.000e+00],\n","       [0.000e+00, 1.370e+02, 4.000e+01, 3.500e+01, 1.680e+02, 4.310e+01,\n","        2.288e+00, 3.300e+01, 1.000e+00],\n","       [5.000e+00, 1.160e+02, 7.400e+01, 0.000e+00, 0.000e+00, 2.560e+01,\n","        2.010e-01, 3.000e+01, 0.000e+00],\n","       [3.000e+00, 7.800e+01, 5.000e+01, 3.200e+01, 8.800e+01, 3.100e+01,\n","        2.480e-01, 2.600e+01, 1.000e+00],\n","       [1.000e+01, 1.150e+02, 0.000e+00, 0.000e+00, 0.000e+00, 3.530e+01,\n","        1.340e-01, 2.900e+01, 0.000e+00],\n","       [2.000e+00, 1.970e+02, 7.000e+01, 4.500e+01, 5.430e+02, 3.050e+01,\n","        1.580e-01, 5.300e+01, 1.000e+00],\n","       [8.000e+00, 1.250e+02, 9.600e+01, 0.000e+00, 0.000e+00, 0.000e+00,\n","        2.320e-01, 5.400e+01, 1.000e+00]])"]},"metadata":{},"execution_count":4}]},{"cell_type":"markdown","metadata":{"id":"J-Hb0D9LhjwB"},"source":["## Simple Classifier"]},{"cell_type":"code","metadata":{"id":"aeIr6H22bDAy"},"source":["X = dataset[:,0:8]\n","Y = dataset[:,8]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HcAAOsDObODK","executionInfo":{"status":"ok","timestamp":1637681624228,"user_tz":-330,"elapsed":5,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}},"outputId":"214f2b64-470c-4256-b105-01f3fde676d3"},"source":["model = Sequential()\n","model.add(Dense(12, input_dim=8, kernel_initializer='uniform', activation='relu'))\n","model.add(Dense(8, kernel_initializer='uniform', activation='relu'))\n","model.add(Dense(1, kernel_initializer='uniform', activation='sigmoid'))\n","\n","model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n","model.summary()"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"sequential_5\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," dense_3 (Dense)             (None, 12)                108       \n","                                                                 \n"," dense_4 (Dense)             (None, 8)                 104       \n","                                                                 \n"," dense_5 (Dense)             (None, 1)                 9         \n","                                                                 \n","=================================================================\n","Total params: 221\n","Trainable params: 221\n","Non-trainable params: 0\n","_________________________________________________________________\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MRUGuJFSbRip","executionInfo":{"status":"ok","timestamp":1637681693561,"user_tz":-330,"elapsed":42102,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}},"outputId":"b0bdb8cd-cc44-46c0-90c7-a903636c7f24","collapsed":true},"source":["#collapse-hide\n","model.fit(X, Y, epochs=150, batch_size=10)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/150\n","77/77 [==============================] - 1s 2ms/step - loss: 0.6794 - accuracy: 0.6510\n","Epoch 2/150\n","77/77 [==============================] - 0s 2ms/step - loss: 0.6616 - accuracy: 0.6510\n","Epoch 3/150\n","77/77 [==============================] - 0s 1ms/step - loss: 0.6574 - accuracy: 0.6510\n","Epoch 4/150\n","77/77 [==============================] - 0s 2ms/step - loss: 0.6474 - accuracy: 0.6510\n","Epoch 5/150\n","77/77 [==============================] - 0s 2ms/step - loss: 0.6354 - accuracy: 0.6510\n","Epoch 6/150\n","77/77 [==============================] - 0s 2ms/step - loss: 0.6299 - accuracy: 0.6510\n","Epoch 7/150\n","77/77 [==============================] - 0s 2ms/step - loss: 0.6174 - accuracy: 0.6510\n","Epoch 8/150\n","77/77 [==============================] - 0s 2ms/step - loss: 0.6147 - accuracy: 0.6510\n","Epoch 9/150\n","77/77 [==============================] - 0s 2ms/step - loss: 0.6087 - accuracy: 0.6510\n","Epoch 10/150\n","77/77 [==============================] - 0s 2ms/step - loss: 0.6089 - accuracy: 0.6510\n","Epoch 11/150\n","77/77 [==============================] - 0s 2ms/step - loss: 0.6079 - accuracy: 0.6510\n","Epoch 12/150\n","77/77 [==============================] - 0s 2ms/step - loss: 0.6057 - accuracy: 0.6510\n","Epoch 13/150\n","77/77 [==============================] - 0s 2ms/step - loss: 0.6007 - accuracy: 0.6510\n","Epoch 14/150\n","77/77 [==============================] - 0s 2ms/step - loss: 0.6023 - accuracy: 0.6510\n","Epoch 15/150\n","77/77 [==============================] - 0s 2ms/step - loss: 0.6028 - accuracy: 0.6510\n","Epoch 16/150\n","77/77 [==============================] - 0s 2ms/step - loss: 0.5980 - accuracy: 0.6510\n","Epoch 17/150\n","77/77 [==============================] - 0s 2ms/step - loss: 0.6018 - accuracy: 0.6510\n","Epoch 18/150\n","77/77 [==============================] - 0s 2ms/step - loss: 0.5972 - accuracy: 0.6510\n","Epoch 19/150\n","77/77 [==============================] - 0s 2ms/step - loss: 0.5943 - accuracy: 0.6510\n","Epoch 20/150\n","77/77 [==============================] - 0s 2ms/step - loss: 0.5927 - accuracy: 0.6510\n","Epoch 21/150\n","77/77 [==============================] - 0s 2ms/step - loss: 0.5932 - accuracy: 0.6510\n","Epoch 22/150\n","77/77 [==============================] - 0s 2ms/step - loss: 0.5930 - accuracy: 0.6510\n","Epoch 23/150\n","77/77 [==============================] - 0s 2ms/step - loss: 0.5944 - accuracy: 0.6510\n","Epoch 24/150\n","77/77 [==============================] - 0s 2ms/step - loss: 0.5942 - accuracy: 0.6510\n","Epoch 25/150\n","77/77 [==============================] - 0s 2ms/step - loss: 0.5906 - accuracy: 0.6510\n","Epoch 26/150\n","77/77 [==============================] - 0s 2ms/step - loss: 0.5882 - accuracy: 0.6510\n","Epoch 27/150\n","77/77 [==============================] - 0s 2ms/step - loss: 0.5897 - accuracy: 0.6510\n","Epoch 28/150\n","77/77 [==============================] - 0s 1ms/step - loss: 0.5925 - accuracy: 0.6510\n","Epoch 29/150\n","77/77 [==============================] - 0s 2ms/step - loss: 0.5897 - accuracy: 0.6510\n","Epoch 30/150\n","77/77 [==============================] - 0s 2ms/step - loss: 0.5876 - accuracy: 0.6510\n","Epoch 31/150\n","77/77 [==============================] - 0s 2ms/step - loss: 0.5898 - accuracy: 0.6589\n","Epoch 32/150\n","77/77 [==============================] - 0s 2ms/step - loss: 0.5907 - accuracy: 0.6836\n","Epoch 33/150\n","77/77 [==============================] - 0s 2ms/step - loss: 0.5871 - accuracy: 0.6966\n","Epoch 34/150\n","77/77 [==============================] - 0s 2ms/step - loss: 0.5893 - accuracy: 0.7096\n","Epoch 35/150\n","77/77 [==============================] - 0s 2ms/step - loss: 0.5864 - accuracy: 0.7005\n","Epoch 36/150\n","77/77 [==============================] - 0s 2ms/step - loss: 0.5853 - accuracy: 0.7031\n","Epoch 37/150\n","77/77 [==============================] - 0s 2ms/step - loss: 0.5841 - accuracy: 0.7018\n","Epoch 38/150\n","77/77 [==============================] - 0s 2ms/step - loss: 0.5870 - accuracy: 0.6992\n","Epoch 39/150\n","77/77 [==============================] - 0s 2ms/step - loss: 0.5826 - accuracy: 0.7148\n","Epoch 40/150\n","77/77 [==============================] - 0s 2ms/step - loss: 0.5838 - accuracy: 0.7109\n","Epoch 41/150\n","77/77 [==============================] - 0s 2ms/step - loss: 0.5892 - accuracy: 0.6979\n","Epoch 42/150\n","77/77 [==============================] - 0s 2ms/step - loss: 0.5837 - accuracy: 0.7005\n","Epoch 43/150\n","77/77 [==============================] - 0s 2ms/step - loss: 0.5806 - accuracy: 0.7070\n","Epoch 44/150\n","77/77 [==============================] - 0s 2ms/step - loss: 0.5801 - accuracy: 0.7161\n","Epoch 45/150\n","77/77 [==============================] - 0s 2ms/step - loss: 0.5795 - accuracy: 0.7070\n","Epoch 46/150\n","77/77 [==============================] - 0s 2ms/step - loss: 0.5781 - accuracy: 0.7070\n","Epoch 47/150\n","77/77 [==============================] - 0s 2ms/step - loss: 0.5770 - accuracy: 0.7057\n","Epoch 48/150\n","77/77 [==============================] - 0s 2ms/step - loss: 0.5767 - accuracy: 0.7227\n","Epoch 49/150\n","77/77 [==============================] - 0s 2ms/step - loss: 0.5775 - accuracy: 0.7044\n","Epoch 50/150\n","77/77 [==============================] - 0s 2ms/step - loss: 0.5759 - accuracy: 0.7174\n","Epoch 51/150\n","77/77 [==============================] - 0s 2ms/step - loss: 0.5766 - accuracy: 0.7070\n","Epoch 52/150\n","77/77 [==============================] - 0s 2ms/step - loss: 0.5743 - accuracy: 0.7109\n","Epoch 53/150\n","77/77 [==============================] - 0s 2ms/step - loss: 0.5730 - accuracy: 0.7148\n","Epoch 54/150\n","77/77 [==============================] - 0s 2ms/step - loss: 0.5693 - accuracy: 0.7070\n","Epoch 55/150\n","77/77 [==============================] - 0s 2ms/step - loss: 0.5724 - accuracy: 0.7148\n","Epoch 56/150\n","77/77 [==============================] - 0s 2ms/step - loss: 0.5644 - accuracy: 0.7161\n","Epoch 57/150\n","77/77 [==============================] - 0s 2ms/step - loss: 0.5697 - accuracy: 0.7148\n","Epoch 58/150\n","77/77 [==============================] - 0s 2ms/step - loss: 0.5638 - accuracy: 0.7161\n","Epoch 59/150\n","77/77 [==============================] - 0s 2ms/step - loss: 0.5663 - accuracy: 0.7174\n","Epoch 60/150\n","77/77 [==============================] - 0s 2ms/step - loss: 0.5665 - accuracy: 0.7109\n","Epoch 61/150\n","77/77 [==============================] - 0s 2ms/step - loss: 0.5639 - accuracy: 0.7135\n","Epoch 62/150\n","77/77 [==============================] - 0s 2ms/step - loss: 0.5718 - accuracy: 0.6992\n","Epoch 63/150\n","77/77 [==============================] - 0s 2ms/step - loss: 0.5636 - accuracy: 0.7161\n","Epoch 64/150\n","77/77 [==============================] - 0s 2ms/step - loss: 0.5662 - accuracy: 0.7214\n","Epoch 65/150\n","77/77 [==============================] - 0s 2ms/step - loss: 0.5650 - accuracy: 0.7083\n","Epoch 66/150\n","77/77 [==============================] - 0s 2ms/step - loss: 0.5648 - accuracy: 0.7096\n","Epoch 67/150\n","77/77 [==============================] - 0s 2ms/step - loss: 0.5630 - accuracy: 0.7161\n","Epoch 68/150\n","77/77 [==============================] - 0s 2ms/step - loss: 0.5620 - accuracy: 0.7253\n","Epoch 69/150\n","77/77 [==============================] - 0s 2ms/step - loss: 0.5569 - accuracy: 0.7227\n","Epoch 70/150\n","77/77 [==============================] - 0s 2ms/step - loss: 0.5603 - accuracy: 0.7227\n","Epoch 71/150\n","77/77 [==============================] - 0s 2ms/step - loss: 0.5569 - accuracy: 0.7253\n","Epoch 72/150\n","77/77 [==============================] - 0s 2ms/step - loss: 0.5512 - accuracy: 0.7318\n","Epoch 73/150\n","77/77 [==============================] - 0s 2ms/step - loss: 0.5509 - accuracy: 0.7396\n","Epoch 74/150\n","77/77 [==============================] - 0s 2ms/step - loss: 0.5502 - accuracy: 0.7357\n","Epoch 75/150\n","77/77 [==============================] - 0s 2ms/step - loss: 0.5420 - accuracy: 0.7474\n","Epoch 76/150\n","77/77 [==============================] - 0s 2ms/step - loss: 0.5485 - accuracy: 0.7396\n","Epoch 77/150\n","77/77 [==============================] - 0s 2ms/step - loss: 0.5440 - accuracy: 0.7474\n","Epoch 78/150\n","77/77 [==============================] - 0s 2ms/step - loss: 0.5413 - accuracy: 0.7370\n","Epoch 79/150\n","77/77 [==============================] - 0s 2ms/step - loss: 0.5451 - accuracy: 0.7383\n","Epoch 80/150\n","77/77 [==============================] - 0s 2ms/step - loss: 0.5419 - accuracy: 0.7474\n","Epoch 81/150\n","77/77 [==============================] - 0s 2ms/step - loss: 0.5366 - accuracy: 0.7500\n","Epoch 82/150\n","77/77 [==============================] - 0s 2ms/step - loss: 0.5396 - accuracy: 0.7552\n","Epoch 83/150\n","77/77 [==============================] - 0s 2ms/step - loss: 0.5499 - accuracy: 0.7305\n","Epoch 84/150\n","77/77 [==============================] - 0s 2ms/step - loss: 0.5356 - accuracy: 0.7617\n","Epoch 85/150\n","77/77 [==============================] - 0s 2ms/step - loss: 0.5377 - accuracy: 0.7448\n","Epoch 86/150\n","77/77 [==============================] - 0s 2ms/step - loss: 0.5346 - accuracy: 0.7539\n","Epoch 87/150\n","77/77 [==============================] - 0s 2ms/step - loss: 0.5320 - accuracy: 0.7487\n","Epoch 88/150\n","77/77 [==============================] - 0s 2ms/step - loss: 0.5340 - accuracy: 0.7539\n","Epoch 89/150\n","77/77 [==============================] - 0s 2ms/step - loss: 0.5303 - accuracy: 0.7578\n","Epoch 90/150\n","77/77 [==============================] - 0s 2ms/step - loss: 0.5270 - accuracy: 0.7565\n","Epoch 91/150\n","77/77 [==============================] - 0s 2ms/step - loss: 0.5283 - accuracy: 0.7604\n","Epoch 92/150\n","77/77 [==============================] - 0s 2ms/step - loss: 0.5322 - accuracy: 0.7409\n","Epoch 93/150\n","77/77 [==============================] - 0s 2ms/step - loss: 0.5285 - accuracy: 0.7552\n","Epoch 94/150\n","77/77 [==============================] - 0s 2ms/step - loss: 0.5273 - accuracy: 0.7513\n","Epoch 95/150\n","77/77 [==============================] - 0s 2ms/step - loss: 0.5231 - accuracy: 0.7604\n","Epoch 96/150\n","77/77 [==============================] - 0s 2ms/step - loss: 0.5262 - accuracy: 0.7630\n","Epoch 97/150\n","77/77 [==============================] - 0s 2ms/step - loss: 0.5190 - accuracy: 0.7643\n","Epoch 98/150\n","77/77 [==============================] - 0s 2ms/step - loss: 0.5209 - accuracy: 0.7591\n","Epoch 99/150\n","77/77 [==============================] - 0s 2ms/step - loss: 0.5257 - accuracy: 0.7526\n","Epoch 100/150\n","77/77 [==============================] - 0s 2ms/step - loss: 0.5196 - accuracy: 0.7604\n","Epoch 101/150\n","77/77 [==============================] - 0s 2ms/step - loss: 0.5211 - accuracy: 0.7643\n","Epoch 102/150\n","77/77 [==============================] - 0s 2ms/step - loss: 0.5190 - accuracy: 0.7617\n","Epoch 103/150\n","77/77 [==============================] - 0s 2ms/step - loss: 0.5142 - accuracy: 0.7461\n","Epoch 104/150\n","77/77 [==============================] - 0s 2ms/step - loss: 0.5169 - accuracy: 0.7578\n","Epoch 105/150\n","77/77 [==============================] - 0s 2ms/step - loss: 0.5125 - accuracy: 0.7643\n","Epoch 106/150\n","77/77 [==============================] - 0s 2ms/step - loss: 0.5191 - accuracy: 0.7461\n","Epoch 107/150\n","77/77 [==============================] - 0s 2ms/step - loss: 0.5139 - accuracy: 0.7630\n","Epoch 108/150\n","77/77 [==============================] - 0s 2ms/step - loss: 0.5111 - accuracy: 0.7682\n","Epoch 109/150\n","77/77 [==============================] - 0s 2ms/step - loss: 0.5065 - accuracy: 0.7682\n","Epoch 110/150\n","77/77 [==============================] - 0s 2ms/step - loss: 0.5102 - accuracy: 0.7604\n","Epoch 111/150\n","77/77 [==============================] - 0s 2ms/step - loss: 0.5032 - accuracy: 0.7682\n","Epoch 112/150\n","77/77 [==============================] - 0s 2ms/step - loss: 0.5108 - accuracy: 0.7617\n","Epoch 113/150\n","77/77 [==============================] - 0s 2ms/step - loss: 0.5014 - accuracy: 0.7669\n","Epoch 114/150\n","77/77 [==============================] - 0s 2ms/step - loss: 0.4979 - accuracy: 0.7682\n","Epoch 115/150\n","77/77 [==============================] - 0s 2ms/step - loss: 0.5061 - accuracy: 0.7591\n","Epoch 116/150\n","77/77 [==============================] - 0s 2ms/step - loss: 0.5010 - accuracy: 0.7578\n","Epoch 117/150\n","77/77 [==============================] - 0s 2ms/step - loss: 0.5023 - accuracy: 0.7617\n","Epoch 118/150\n","77/77 [==============================] - 0s 2ms/step - loss: 0.4942 - accuracy: 0.7695\n","Epoch 119/150\n","77/77 [==============================] - 0s 2ms/step - loss: 0.5028 - accuracy: 0.7617\n","Epoch 120/150\n","77/77 [==============================] - 0s 2ms/step - loss: 0.4969 - accuracy: 0.7604\n","Epoch 121/150\n","77/77 [==============================] - 0s 2ms/step - loss: 0.4982 - accuracy: 0.7734\n","Epoch 122/150\n","77/77 [==============================] - 0s 2ms/step - loss: 0.4967 - accuracy: 0.7695\n","Epoch 123/150\n","77/77 [==============================] - 0s 2ms/step - loss: 0.5012 - accuracy: 0.7669\n","Epoch 124/150\n","77/77 [==============================] - 0s 2ms/step - loss: 0.4915 - accuracy: 0.7734\n","Epoch 125/150\n","77/77 [==============================] - 0s 2ms/step - loss: 0.4928 - accuracy: 0.7617\n","Epoch 126/150\n","77/77 [==============================] - 0s 2ms/step - loss: 0.4920 - accuracy: 0.7721\n","Epoch 127/150\n","77/77 [==============================] - 0s 2ms/step - loss: 0.4916 - accuracy: 0.7591\n","Epoch 128/150\n","77/77 [==============================] - 0s 2ms/step - loss: 0.4951 - accuracy: 0.7786\n","Epoch 129/150\n","77/77 [==============================] - 0s 2ms/step - loss: 0.4933 - accuracy: 0.7656\n","Epoch 130/150\n","77/77 [==============================] - 0s 2ms/step - loss: 0.4974 - accuracy: 0.7708\n","Epoch 131/150\n","77/77 [==============================] - 0s 2ms/step - loss: 0.4918 - accuracy: 0.7591\n","Epoch 132/150\n","77/77 [==============================] - 0s 2ms/step - loss: 0.4898 - accuracy: 0.7669\n","Epoch 133/150\n","77/77 [==============================] - 0s 2ms/step - loss: 0.4912 - accuracy: 0.7656\n","Epoch 134/150\n","77/77 [==============================] - 0s 2ms/step - loss: 0.4850 - accuracy: 0.7682\n","Epoch 135/150\n","77/77 [==============================] - 0s 2ms/step - loss: 0.4877 - accuracy: 0.7839\n","Epoch 136/150\n","77/77 [==============================] - 0s 2ms/step - loss: 0.4875 - accuracy: 0.7630\n","Epoch 137/150\n","77/77 [==============================] - 0s 2ms/step - loss: 0.4835 - accuracy: 0.7760\n","Epoch 138/150\n","77/77 [==============================] - 0s 2ms/step - loss: 0.4924 - accuracy: 0.7656\n","Epoch 139/150\n","77/77 [==============================] - 0s 2ms/step - loss: 0.4855 - accuracy: 0.7695\n","Epoch 140/150\n","77/77 [==============================] - 0s 2ms/step - loss: 0.4866 - accuracy: 0.7865\n","Epoch 141/150\n","77/77 [==============================] - 0s 2ms/step - loss: 0.4942 - accuracy: 0.7721\n","Epoch 142/150\n","77/77 [==============================] - 0s 2ms/step - loss: 0.4881 - accuracy: 0.7734\n","Epoch 143/150\n","77/77 [==============================] - 0s 2ms/step - loss: 0.4769 - accuracy: 0.7904\n","Epoch 144/150\n","77/77 [==============================] - 0s 2ms/step - loss: 0.4932 - accuracy: 0.7786\n","Epoch 145/150\n","77/77 [==============================] - 0s 2ms/step - loss: 0.4852 - accuracy: 0.7799\n","Epoch 146/150\n","77/77 [==============================] - 0s 2ms/step - loss: 0.4855 - accuracy: 0.7786\n","Epoch 147/150\n","77/77 [==============================] - 0s 2ms/step - loss: 0.5025 - accuracy: 0.7552\n","Epoch 148/150\n","77/77 [==============================] - 0s 2ms/step - loss: 0.4836 - accuracy: 0.7786\n","Epoch 149/150\n","77/77 [==============================] - 0s 2ms/step - loss: 0.4784 - accuracy: 0.7747\n","Epoch 150/150\n","77/77 [==============================] - 0s 2ms/step - loss: 0.4823 - accuracy: 0.7682\n"]},{"output_type":"execute_result","data":{"text/plain":["<keras.callbacks.History at 0x7f27b4b2b250>"]},"metadata":{},"execution_count":14}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dlGVgNT5bpeM","executionInfo":{"status":"ok","timestamp":1637681697635,"user_tz":-330,"elapsed":565,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}},"outputId":"c5f6d37d-a6bf-4555-a8e8-b608cb8fe0ec"},"source":["scores = model.evaluate(X, Y)\n","print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["24/24 [==============================] - 0s 1ms/step - loss: 0.4730 - accuracy: 0.7734\n","accuracy: 77.34%\n"]}]},{"cell_type":"markdown","metadata":{"id":"uOqagZeWpb-u"},"source":["## Classifier with Train/Test Split"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aPr-_Svspj5Y","executionInfo":{"status":"ok","timestamp":1637685373223,"user_tz":-330,"elapsed":24573,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}},"outputId":"9ceb0471-7906-46d4-8c74-d9412d76dd39","collapsed":true},"source":["#collapse-hide\n","X = dataset[:,0:8]\n","Y = dataset[:,8]\n","\n","# split into 67% for train and 33% for test\n","X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.33, random_state=seed)\n","\n","model = Sequential()\n","model.add(Dense(12, input_dim=8, kernel_initializer='uniform', activation='relu'))\n","model.add(Dense(8, kernel_initializer='uniform', activation='relu'))\n","model.add(Dense(1, kernel_initializer='uniform', activation='sigmoid'))\n","\n","model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n","\n","model.fit(X_train, y_train, validation_data=(X_test,y_test), epochs=150, batch_size=10)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/150\n","52/52 [==============================] - 1s 6ms/step - loss: 0.6880 - accuracy: 0.6401 - val_loss: 0.6790 - val_accuracy: 0.6378\n","Epoch 2/150\n","52/52 [==============================] - 0s 2ms/step - loss: 0.6690 - accuracy: 0.6576 - val_loss: 0.6673 - val_accuracy: 0.6378\n","Epoch 3/150\n","52/52 [==============================] - 0s 2ms/step - loss: 0.6569 - accuracy: 0.6576 - val_loss: 0.6598 - val_accuracy: 0.6378\n","Epoch 4/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.6529 - accuracy: 0.6595 - val_loss: 0.6512 - val_accuracy: 0.6378\n","Epoch 5/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.6440 - accuracy: 0.6693 - val_loss: 0.6426 - val_accuracy: 0.6299\n","Epoch 6/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.6330 - accuracy: 0.6790 - val_loss: 0.6302 - val_accuracy: 0.6614\n","Epoch 7/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.6278 - accuracy: 0.6693 - val_loss: 0.6225 - val_accuracy: 0.6575\n","Epoch 8/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.6170 - accuracy: 0.6654 - val_loss: 0.6121 - val_accuracy: 0.6850\n","Epoch 9/150\n","52/52 [==============================] - 0s 2ms/step - loss: 0.6067 - accuracy: 0.6693 - val_loss: 0.6200 - val_accuracy: 0.6575\n","Epoch 10/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.6017 - accuracy: 0.6790 - val_loss: 0.6013 - val_accuracy: 0.6969\n","Epoch 11/150\n","52/52 [==============================] - 0s 2ms/step - loss: 0.5988 - accuracy: 0.6926 - val_loss: 0.6076 - val_accuracy: 0.6654\n","Epoch 12/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.5905 - accuracy: 0.7004 - val_loss: 0.5969 - val_accuracy: 0.6811\n","Epoch 13/150\n","52/52 [==============================] - 0s 2ms/step - loss: 0.5915 - accuracy: 0.6868 - val_loss: 0.5925 - val_accuracy: 0.7087\n","Epoch 14/150\n","52/52 [==============================] - 0s 2ms/step - loss: 0.5936 - accuracy: 0.7023 - val_loss: 0.5952 - val_accuracy: 0.6850\n","Epoch 15/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.5880 - accuracy: 0.6848 - val_loss: 0.5888 - val_accuracy: 0.7047\n","Epoch 16/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.5869 - accuracy: 0.6868 - val_loss: 0.5910 - val_accuracy: 0.6850\n","Epoch 17/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.5880 - accuracy: 0.7062 - val_loss: 0.6157 - val_accuracy: 0.6654\n","Epoch 18/150\n","52/52 [==============================] - 0s 2ms/step - loss: 0.5876 - accuracy: 0.6907 - val_loss: 0.5857 - val_accuracy: 0.6969\n","Epoch 19/150\n","52/52 [==============================] - 0s 2ms/step - loss: 0.5781 - accuracy: 0.7101 - val_loss: 0.5861 - val_accuracy: 0.6850\n","Epoch 20/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.5879 - accuracy: 0.7140 - val_loss: 0.5836 - val_accuracy: 0.7087\n","Epoch 21/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.5804 - accuracy: 0.7023 - val_loss: 0.5820 - val_accuracy: 0.7047\n","Epoch 22/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.5787 - accuracy: 0.6984 - val_loss: 0.5839 - val_accuracy: 0.6969\n","Epoch 23/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.5761 - accuracy: 0.6926 - val_loss: 0.5820 - val_accuracy: 0.6969\n","Epoch 24/150\n","52/52 [==============================] - 0s 2ms/step - loss: 0.5826 - accuracy: 0.7004 - val_loss: 0.5997 - val_accuracy: 0.6614\n","Epoch 25/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.5780 - accuracy: 0.7023 - val_loss: 0.5786 - val_accuracy: 0.7126\n","Epoch 26/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.5737 - accuracy: 0.6946 - val_loss: 0.5774 - val_accuracy: 0.6850\n","Epoch 27/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.5786 - accuracy: 0.7082 - val_loss: 0.5772 - val_accuracy: 0.7008\n","Epoch 28/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.5767 - accuracy: 0.6946 - val_loss: 0.5782 - val_accuracy: 0.6969\n","Epoch 29/150\n","52/52 [==============================] - 0s 2ms/step - loss: 0.5681 - accuracy: 0.7121 - val_loss: 0.5753 - val_accuracy: 0.7087\n","Epoch 30/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.5771 - accuracy: 0.7043 - val_loss: 0.5774 - val_accuracy: 0.7205\n","Epoch 31/150\n","52/52 [==============================] - 0s 2ms/step - loss: 0.5707 - accuracy: 0.7140 - val_loss: 0.5779 - val_accuracy: 0.6969\n","Epoch 32/150\n","52/52 [==============================] - 0s 2ms/step - loss: 0.5672 - accuracy: 0.7101 - val_loss: 0.5739 - val_accuracy: 0.7047\n","Epoch 33/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.5727 - accuracy: 0.7101 - val_loss: 0.5752 - val_accuracy: 0.7126\n","Epoch 34/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.5664 - accuracy: 0.7023 - val_loss: 0.5767 - val_accuracy: 0.7165\n","Epoch 35/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.5604 - accuracy: 0.7043 - val_loss: 0.5889 - val_accuracy: 0.6929\n","Epoch 36/150\n","52/52 [==============================] - 0s 2ms/step - loss: 0.5639 - accuracy: 0.7140 - val_loss: 0.5798 - val_accuracy: 0.7087\n","Epoch 37/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.5647 - accuracy: 0.7121 - val_loss: 0.5823 - val_accuracy: 0.6929\n","Epoch 38/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.5635 - accuracy: 0.7160 - val_loss: 0.5843 - val_accuracy: 0.6929\n","Epoch 39/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.5679 - accuracy: 0.6907 - val_loss: 0.5692 - val_accuracy: 0.7008\n","Epoch 40/150\n","52/52 [==============================] - 0s 2ms/step - loss: 0.5647 - accuracy: 0.7101 - val_loss: 0.5892 - val_accuracy: 0.6732\n","Epoch 41/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.5669 - accuracy: 0.7179 - val_loss: 0.5721 - val_accuracy: 0.7165\n","Epoch 42/150\n","52/52 [==============================] - 0s 2ms/step - loss: 0.5617 - accuracy: 0.7082 - val_loss: 0.5723 - val_accuracy: 0.7047\n","Epoch 43/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.5626 - accuracy: 0.7198 - val_loss: 0.5693 - val_accuracy: 0.7047\n","Epoch 44/150\n","52/52 [==============================] - 0s 2ms/step - loss: 0.5478 - accuracy: 0.7354 - val_loss: 0.5924 - val_accuracy: 0.6772\n","Epoch 45/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.5624 - accuracy: 0.7179 - val_loss: 0.5664 - val_accuracy: 0.7087\n","Epoch 46/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.5529 - accuracy: 0.7198 - val_loss: 0.5771 - val_accuracy: 0.6969\n","Epoch 47/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.5558 - accuracy: 0.7179 - val_loss: 0.5703 - val_accuracy: 0.7244\n","Epoch 48/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.5527 - accuracy: 0.7140 - val_loss: 0.5639 - val_accuracy: 0.7244\n","Epoch 49/150\n","52/52 [==============================] - 0s 2ms/step - loss: 0.5568 - accuracy: 0.7237 - val_loss: 0.5670 - val_accuracy: 0.7008\n","Epoch 50/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.5535 - accuracy: 0.7121 - val_loss: 0.5644 - val_accuracy: 0.7283\n","Epoch 51/150\n","52/52 [==============================] - 0s 2ms/step - loss: 0.5482 - accuracy: 0.7257 - val_loss: 0.5624 - val_accuracy: 0.7165\n","Epoch 52/150\n","52/52 [==============================] - 0s 2ms/step - loss: 0.5509 - accuracy: 0.7198 - val_loss: 0.5680 - val_accuracy: 0.7126\n","Epoch 53/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.5456 - accuracy: 0.7237 - val_loss: 0.5624 - val_accuracy: 0.7283\n","Epoch 54/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.5469 - accuracy: 0.7160 - val_loss: 0.5732 - val_accuracy: 0.7165\n","Epoch 55/150\n","52/52 [==============================] - 0s 2ms/step - loss: 0.5472 - accuracy: 0.7140 - val_loss: 0.5636 - val_accuracy: 0.7244\n","Epoch 56/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.5482 - accuracy: 0.7296 - val_loss: 0.5606 - val_accuracy: 0.7323\n","Epoch 57/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.5482 - accuracy: 0.7121 - val_loss: 0.5768 - val_accuracy: 0.7047\n","Epoch 58/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.5450 - accuracy: 0.7160 - val_loss: 0.5591 - val_accuracy: 0.7205\n","Epoch 59/150\n","52/52 [==============================] - 0s 2ms/step - loss: 0.5479 - accuracy: 0.7315 - val_loss: 0.5601 - val_accuracy: 0.7126\n","Epoch 60/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.5428 - accuracy: 0.7335 - val_loss: 0.5575 - val_accuracy: 0.7244\n","Epoch 61/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.5434 - accuracy: 0.7257 - val_loss: 0.5604 - val_accuracy: 0.7283\n","Epoch 62/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.5359 - accuracy: 0.7296 - val_loss: 0.5606 - val_accuracy: 0.7244\n","Epoch 63/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.5350 - accuracy: 0.7374 - val_loss: 0.5562 - val_accuracy: 0.7244\n","Epoch 64/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.5372 - accuracy: 0.7257 - val_loss: 0.5704 - val_accuracy: 0.7244\n","Epoch 65/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.5360 - accuracy: 0.7257 - val_loss: 0.5585 - val_accuracy: 0.7165\n","Epoch 66/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.5365 - accuracy: 0.7354 - val_loss: 0.5559 - val_accuracy: 0.7244\n","Epoch 67/150\n","52/52 [==============================] - 0s 2ms/step - loss: 0.5344 - accuracy: 0.7043 - val_loss: 0.5519 - val_accuracy: 0.7402\n","Epoch 68/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.5352 - accuracy: 0.7082 - val_loss: 0.5766 - val_accuracy: 0.7087\n","Epoch 69/150\n","52/52 [==============================] - 0s 2ms/step - loss: 0.5406 - accuracy: 0.7412 - val_loss: 0.5722 - val_accuracy: 0.7047\n","Epoch 70/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.5337 - accuracy: 0.7237 - val_loss: 0.5527 - val_accuracy: 0.7283\n","Epoch 71/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.5364 - accuracy: 0.7276 - val_loss: 0.5506 - val_accuracy: 0.7126\n","Epoch 72/150\n","52/52 [==============================] - 0s 2ms/step - loss: 0.5337 - accuracy: 0.7257 - val_loss: 0.5585 - val_accuracy: 0.7165\n","Epoch 73/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.5348 - accuracy: 0.7393 - val_loss: 0.5510 - val_accuracy: 0.7362\n","Epoch 74/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.5254 - accuracy: 0.7393 - val_loss: 0.5490 - val_accuracy: 0.7362\n","Epoch 75/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.5248 - accuracy: 0.7451 - val_loss: 0.5510 - val_accuracy: 0.7283\n","Epoch 76/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.5256 - accuracy: 0.7374 - val_loss: 0.5496 - val_accuracy: 0.7441\n","Epoch 77/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.5257 - accuracy: 0.7315 - val_loss: 0.5545 - val_accuracy: 0.7323\n","Epoch 78/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.5253 - accuracy: 0.7315 - val_loss: 0.5710 - val_accuracy: 0.7165\n","Epoch 79/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.5242 - accuracy: 0.7451 - val_loss: 0.5467 - val_accuracy: 0.7323\n","Epoch 80/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.5207 - accuracy: 0.7374 - val_loss: 0.5454 - val_accuracy: 0.7323\n","Epoch 81/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.5246 - accuracy: 0.7393 - val_loss: 0.5442 - val_accuracy: 0.7402\n","Epoch 82/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.5180 - accuracy: 0.7451 - val_loss: 0.5435 - val_accuracy: 0.7402\n","Epoch 83/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.5194 - accuracy: 0.7374 - val_loss: 0.5421 - val_accuracy: 0.7362\n","Epoch 84/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.5243 - accuracy: 0.7471 - val_loss: 0.5420 - val_accuracy: 0.7402\n","Epoch 85/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.5198 - accuracy: 0.7490 - val_loss: 0.5424 - val_accuracy: 0.7362\n","Epoch 86/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.5152 - accuracy: 0.7549 - val_loss: 0.5438 - val_accuracy: 0.7480\n","Epoch 87/150\n","52/52 [==============================] - 0s 2ms/step - loss: 0.5178 - accuracy: 0.7354 - val_loss: 0.5553 - val_accuracy: 0.7244\n","Epoch 88/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.5281 - accuracy: 0.7354 - val_loss: 0.5671 - val_accuracy: 0.7165\n","Epoch 89/150\n","52/52 [==============================] - 0s 2ms/step - loss: 0.5206 - accuracy: 0.7374 - val_loss: 0.5428 - val_accuracy: 0.7362\n","Epoch 90/150\n","52/52 [==============================] - 0s 2ms/step - loss: 0.5126 - accuracy: 0.7451 - val_loss: 0.5393 - val_accuracy: 0.7402\n","Epoch 91/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.5148 - accuracy: 0.7471 - val_loss: 0.5538 - val_accuracy: 0.7362\n","Epoch 92/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.5207 - accuracy: 0.7510 - val_loss: 0.5415 - val_accuracy: 0.7283\n","Epoch 93/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.5109 - accuracy: 0.7412 - val_loss: 0.5382 - val_accuracy: 0.7520\n","Epoch 94/150\n","52/52 [==============================] - 0s 2ms/step - loss: 0.5108 - accuracy: 0.7412 - val_loss: 0.5419 - val_accuracy: 0.7402\n","Epoch 95/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.5190 - accuracy: 0.7276 - val_loss: 0.5477 - val_accuracy: 0.7402\n","Epoch 96/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.5140 - accuracy: 0.7529 - val_loss: 0.5463 - val_accuracy: 0.7165\n","Epoch 97/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.5163 - accuracy: 0.7432 - val_loss: 0.5416 - val_accuracy: 0.7283\n","Epoch 98/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.5094 - accuracy: 0.7412 - val_loss: 0.5352 - val_accuracy: 0.7441\n","Epoch 99/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.5077 - accuracy: 0.7412 - val_loss: 0.5349 - val_accuracy: 0.7480\n","Epoch 100/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.5104 - accuracy: 0.7549 - val_loss: 0.5410 - val_accuracy: 0.7362\n","Epoch 101/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.5164 - accuracy: 0.7374 - val_loss: 0.5360 - val_accuracy: 0.7480\n","Epoch 102/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.5075 - accuracy: 0.7412 - val_loss: 0.5357 - val_accuracy: 0.7441\n","Epoch 103/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.5023 - accuracy: 0.7432 - val_loss: 0.5477 - val_accuracy: 0.7362\n","Epoch 104/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.5038 - accuracy: 0.7490 - val_loss: 0.5332 - val_accuracy: 0.7244\n","Epoch 105/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.5196 - accuracy: 0.7354 - val_loss: 0.5421 - val_accuracy: 0.7165\n","Epoch 106/150\n","52/52 [==============================] - 0s 2ms/step - loss: 0.5048 - accuracy: 0.7451 - val_loss: 0.5308 - val_accuracy: 0.7362\n","Epoch 107/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.5063 - accuracy: 0.7510 - val_loss: 0.5271 - val_accuracy: 0.7441\n","Epoch 108/150\n","52/52 [==============================] - 0s 2ms/step - loss: 0.5048 - accuracy: 0.7646 - val_loss: 0.5293 - val_accuracy: 0.7402\n","Epoch 109/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.4996 - accuracy: 0.7549 - val_loss: 0.5364 - val_accuracy: 0.7244\n","Epoch 110/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.5060 - accuracy: 0.7412 - val_loss: 0.5283 - val_accuracy: 0.7362\n","Epoch 111/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.4963 - accuracy: 0.7588 - val_loss: 0.5288 - val_accuracy: 0.7283\n","Epoch 112/150\n","52/52 [==============================] - 0s 2ms/step - loss: 0.4981 - accuracy: 0.7490 - val_loss: 0.5366 - val_accuracy: 0.7480\n","Epoch 113/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.5043 - accuracy: 0.7471 - val_loss: 0.5346 - val_accuracy: 0.7480\n","Epoch 114/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.4930 - accuracy: 0.7490 - val_loss: 0.5204 - val_accuracy: 0.7323\n","Epoch 115/150\n","52/52 [==============================] - 0s 2ms/step - loss: 0.4916 - accuracy: 0.7646 - val_loss: 0.5562 - val_accuracy: 0.7244\n","Epoch 116/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.4996 - accuracy: 0.7588 - val_loss: 0.5243 - val_accuracy: 0.7441\n","Epoch 117/150\n","52/52 [==============================] - 0s 2ms/step - loss: 0.5004 - accuracy: 0.7451 - val_loss: 0.5226 - val_accuracy: 0.7559\n","Epoch 118/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.4904 - accuracy: 0.7626 - val_loss: 0.5208 - val_accuracy: 0.7520\n","Epoch 119/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.4945 - accuracy: 0.7529 - val_loss: 0.5178 - val_accuracy: 0.7402\n","Epoch 120/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.4936 - accuracy: 0.7471 - val_loss: 0.5320 - val_accuracy: 0.7559\n","Epoch 121/150\n","52/52 [==============================] - 0s 2ms/step - loss: 0.4989 - accuracy: 0.7490 - val_loss: 0.5332 - val_accuracy: 0.7402\n","Epoch 122/150\n","52/52 [==============================] - 0s 2ms/step - loss: 0.5016 - accuracy: 0.7588 - val_loss: 0.5206 - val_accuracy: 0.7520\n","Epoch 123/150\n","52/52 [==============================] - 0s 2ms/step - loss: 0.4918 - accuracy: 0.7646 - val_loss: 0.5309 - val_accuracy: 0.7362\n","Epoch 124/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.5061 - accuracy: 0.7549 - val_loss: 0.5175 - val_accuracy: 0.7559\n","Epoch 125/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.4858 - accuracy: 0.7626 - val_loss: 0.5166 - val_accuracy: 0.7480\n","Epoch 126/150\n","52/52 [==============================] - 0s 2ms/step - loss: 0.4973 - accuracy: 0.7568 - val_loss: 0.5193 - val_accuracy: 0.7559\n","Epoch 127/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.4825 - accuracy: 0.7588 - val_loss: 0.5173 - val_accuracy: 0.7402\n","Epoch 128/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.4850 - accuracy: 0.7646 - val_loss: 0.5210 - val_accuracy: 0.7441\n","Epoch 129/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.4871 - accuracy: 0.7588 - val_loss: 0.5252 - val_accuracy: 0.7441\n","Epoch 130/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.4905 - accuracy: 0.7588 - val_loss: 0.5159 - val_accuracy: 0.7402\n","Epoch 131/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.4811 - accuracy: 0.7607 - val_loss: 0.5404 - val_accuracy: 0.7520\n","Epoch 132/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.4821 - accuracy: 0.7549 - val_loss: 0.5171 - val_accuracy: 0.7205\n","Epoch 133/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.4815 - accuracy: 0.7665 - val_loss: 0.5176 - val_accuracy: 0.7362\n","Epoch 134/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.4855 - accuracy: 0.7724 - val_loss: 0.5196 - val_accuracy: 0.7480\n","Epoch 135/150\n","52/52 [==============================] - 0s 2ms/step - loss: 0.4825 - accuracy: 0.7510 - val_loss: 0.5173 - val_accuracy: 0.7323\n","Epoch 136/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.4778 - accuracy: 0.7704 - val_loss: 0.5134 - val_accuracy: 0.7520\n","Epoch 137/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.4717 - accuracy: 0.7704 - val_loss: 0.5547 - val_accuracy: 0.7283\n","Epoch 138/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.4936 - accuracy: 0.7646 - val_loss: 0.5196 - val_accuracy: 0.7283\n","Epoch 139/150\n","52/52 [==============================] - 0s 2ms/step - loss: 0.4793 - accuracy: 0.7529 - val_loss: 0.5135 - val_accuracy: 0.7480\n","Epoch 140/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.4737 - accuracy: 0.7782 - val_loss: 0.5189 - val_accuracy: 0.7598\n","Epoch 141/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.4777 - accuracy: 0.7724 - val_loss: 0.5258 - val_accuracy: 0.7480\n","Epoch 142/150\n","52/52 [==============================] - 0s 2ms/step - loss: 0.4769 - accuracy: 0.7549 - val_loss: 0.5180 - val_accuracy: 0.7559\n","Epoch 143/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.4791 - accuracy: 0.7646 - val_loss: 0.5196 - val_accuracy: 0.7677\n","Epoch 144/150\n","52/52 [==============================] - 0s 2ms/step - loss: 0.4770 - accuracy: 0.7685 - val_loss: 0.5160 - val_accuracy: 0.7559\n","Epoch 145/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.4691 - accuracy: 0.7549 - val_loss: 0.5215 - val_accuracy: 0.7402\n","Epoch 146/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.4685 - accuracy: 0.7588 - val_loss: 0.5306 - val_accuracy: 0.7677\n","Epoch 147/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.4756 - accuracy: 0.7626 - val_loss: 0.5146 - val_accuracy: 0.7756\n","Epoch 148/150\n","52/52 [==============================] - 0s 2ms/step - loss: 0.4659 - accuracy: 0.7724 - val_loss: 0.5301 - val_accuracy: 0.7244\n","Epoch 149/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.4793 - accuracy: 0.7646 - val_loss: 0.5351 - val_accuracy: 0.7244\n","Epoch 150/150\n","52/52 [==============================] - 0s 3ms/step - loss: 0.4663 - accuracy: 0.7724 - val_loss: 0.5169 - val_accuracy: 0.7598\n"]},{"output_type":"execute_result","data":{"text/plain":["<keras.callbacks.History at 0x7f649f0c1dd0>"]},"metadata":{},"execution_count":11}]},{"cell_type":"markdown","metadata":{"id":"K2cRrx0yp5sN"},"source":["## Classifier with K-fold Cross-validation"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PSbbFfHSqC3j","executionInfo":{"status":"ok","timestamp":1637685591758,"user_tz":-330,"elapsed":159416,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}},"outputId":"8afb50b5-85e2-4266-d5e3-cd539ade5d0e"},"source":["X = dataset[:,0:8]\n","Y = dataset[:,8]\n","\n","# define 10-fold cross validation test harness\n","kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=seed)\n","cvscores = []\n","\n","for train, test in kfold.split(X, Y):\n","  # create model\n","\tmodel = Sequential()\n","\tmodel.add(Dense(12, input_dim=8, kernel_initializer='uniform', activation='relu'))\n","\tmodel.add(Dense(8, kernel_initializer='uniform', activation='relu'))\n","\tmodel.add(Dense(1, kernel_initializer='uniform', activation='sigmoid'))\n","\t# Compile model\n","\tmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n","\t# Fit the model\n","\tmodel.fit(X[train], Y[train], epochs=150, batch_size=10, verbose=0)\n","\t# evaluate the model\n","\tscores = model.evaluate(X[test], Y[test], verbose=0)\n","\tprint(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n","\tcvscores.append(scores[1] * 100)\n","\n","print(\"%.2f%% (+/- %.2f%%)\" % (numpy.mean(cvscores), numpy.std(cvscores)))"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["accuracy: 68.83%\n","accuracy: 77.92%\n","accuracy: 75.32%\n","WARNING:tensorflow:5 out of the last 3910 calls to <function Model.make_test_function.<locals>.test_function at 0x7f64a22c55f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n","accuracy: 81.82%\n","WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_test_function.<locals>.test_function at 0x7f649e0bdc20> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n","accuracy: 70.13%\n","accuracy: 63.64%\n","accuracy: 77.92%\n","accuracy: 76.62%\n","accuracy: 75.00%\n","accuracy: 78.95%\n","74.62% (+/- 5.21%)\n"]}]},{"cell_type":"markdown","metadata":{"id":"MrpICfmphqw1"},"source":["## Training with checkpoints"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ftQUWdgshqur","executionInfo":{"status":"ok","timestamp":1637683422679,"user_tz":-330,"elapsed":20313,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}},"outputId":"cadafae4-d4fc-4d3d-f897-2f338c510c4b"},"source":["X = dataset[:,0:8]\n","Y = dataset[:,8]\n","\n","model = Sequential()\n","model.add(Dense(12, input_dim=8, activation='relu'))\n","model.add(Dense(8, activation='relu'))\n","model.add(Dense(1, activation='sigmoid'))\n","\n","model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n","\n","filepath=\"weights.best.hdf5\"\n","checkpoint = ModelCheckpoint(filepath, monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')\n","callbacks_list = [checkpoint]\n","\n","model.fit(X, Y, validation_split=0.33, epochs=150, batch_size=10, callbacks=callbacks_list, verbose=0)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Epoch 00001: val_accuracy improved from -inf to 0.46063, saving model to weights.best.hdf5\n","\n","Epoch 00002: val_accuracy improved from 0.46063 to 0.66142, saving model to weights.best.hdf5\n","\n","Epoch 00003: val_accuracy did not improve from 0.66142\n","\n","Epoch 00004: val_accuracy did not improve from 0.66142\n","\n","Epoch 00005: val_accuracy improved from 0.66142 to 0.69291, saving model to weights.best.hdf5\n","\n","Epoch 00006: val_accuracy did not improve from 0.69291\n","\n","Epoch 00007: val_accuracy did not improve from 0.69291\n","\n","Epoch 00008: val_accuracy did not improve from 0.69291\n","\n","Epoch 00009: val_accuracy did not improve from 0.69291\n","\n","Epoch 00010: val_accuracy did not improve from 0.69291\n","\n","Epoch 00011: val_accuracy improved from 0.69291 to 0.70472, saving model to weights.best.hdf5\n","\n","Epoch 00012: val_accuracy did not improve from 0.70472\n","\n","Epoch 00013: val_accuracy did not improve from 0.70472\n","\n","Epoch 00014: val_accuracy did not improve from 0.70472\n","\n","Epoch 00015: val_accuracy improved from 0.70472 to 0.72835, saving model to weights.best.hdf5\n","\n","Epoch 00016: val_accuracy did not improve from 0.72835\n","\n","Epoch 00017: val_accuracy did not improve from 0.72835\n","\n","Epoch 00018: val_accuracy did not improve from 0.72835\n","\n","Epoch 00019: val_accuracy did not improve from 0.72835\n","\n","Epoch 00020: val_accuracy did not improve from 0.72835\n","\n","Epoch 00021: val_accuracy did not improve from 0.72835\n","\n","Epoch 00022: val_accuracy did not improve from 0.72835\n","\n","Epoch 00023: val_accuracy did not improve from 0.72835\n","\n","Epoch 00024: val_accuracy did not improve from 0.72835\n","\n","Epoch 00025: val_accuracy did not improve from 0.72835\n","\n","Epoch 00026: val_accuracy did not improve from 0.72835\n","\n","Epoch 00027: val_accuracy did not improve from 0.72835\n","\n","Epoch 00028: val_accuracy did not improve from 0.72835\n","\n","Epoch 00029: val_accuracy did not improve from 0.72835\n","\n","Epoch 00030: val_accuracy did not improve from 0.72835\n","\n","Epoch 00031: val_accuracy improved from 0.72835 to 0.73228, saving model to weights.best.hdf5\n","\n","Epoch 00032: val_accuracy did not improve from 0.73228\n","\n","Epoch 00033: val_accuracy did not improve from 0.73228\n","\n","Epoch 00034: val_accuracy did not improve from 0.73228\n","\n","Epoch 00035: val_accuracy did not improve from 0.73228\n","\n","Epoch 00036: val_accuracy did not improve from 0.73228\n","\n","Epoch 00037: val_accuracy did not improve from 0.73228\n","\n","Epoch 00038: val_accuracy did not improve from 0.73228\n","\n","Epoch 00039: val_accuracy did not improve from 0.73228\n","\n","Epoch 00040: val_accuracy did not improve from 0.73228\n","\n","Epoch 00041: val_accuracy did not improve from 0.73228\n","\n","Epoch 00042: val_accuracy did not improve from 0.73228\n","\n","Epoch 00043: val_accuracy did not improve from 0.73228\n","\n","Epoch 00044: val_accuracy did not improve from 0.73228\n","\n","Epoch 00045: val_accuracy did not improve from 0.73228\n","\n","Epoch 00046: val_accuracy did not improve from 0.73228\n","\n","Epoch 00047: val_accuracy did not improve from 0.73228\n","\n","Epoch 00048: val_accuracy did not improve from 0.73228\n","\n","Epoch 00049: val_accuracy did not improve from 0.73228\n","\n","Epoch 00050: val_accuracy did not improve from 0.73228\n","\n","Epoch 00051: val_accuracy improved from 0.73228 to 0.74803, saving model to weights.best.hdf5\n","\n","Epoch 00052: val_accuracy did not improve from 0.74803\n","\n","Epoch 00053: val_accuracy did not improve from 0.74803\n","\n","Epoch 00054: val_accuracy did not improve from 0.74803\n","\n","Epoch 00055: val_accuracy did not improve from 0.74803\n","\n","Epoch 00056: val_accuracy did not improve from 0.74803\n","\n","Epoch 00057: val_accuracy did not improve from 0.74803\n","\n","Epoch 00058: val_accuracy did not improve from 0.74803\n","\n","Epoch 00059: val_accuracy did not improve from 0.74803\n","\n","Epoch 00060: val_accuracy did not improve from 0.74803\n","\n","Epoch 00061: val_accuracy did not improve from 0.74803\n","\n","Epoch 00062: val_accuracy did not improve from 0.74803\n","\n","Epoch 00063: val_accuracy did not improve from 0.74803\n","\n","Epoch 00064: val_accuracy did not improve from 0.74803\n","\n","Epoch 00065: val_accuracy did not improve from 0.74803\n","\n","Epoch 00066: val_accuracy did not improve from 0.74803\n","\n","Epoch 00067: val_accuracy did not improve from 0.74803\n","\n","Epoch 00068: val_accuracy did not improve from 0.74803\n","\n","Epoch 00069: val_accuracy did not improve from 0.74803\n","\n","Epoch 00070: val_accuracy did not improve from 0.74803\n","\n","Epoch 00071: val_accuracy did not improve from 0.74803\n","\n","Epoch 00072: val_accuracy did not improve from 0.74803\n","\n","Epoch 00073: val_accuracy did not improve from 0.74803\n","\n","Epoch 00074: val_accuracy did not improve from 0.74803\n","\n","Epoch 00075: val_accuracy did not improve from 0.74803\n","\n","Epoch 00076: val_accuracy did not improve from 0.74803\n","\n","Epoch 00077: val_accuracy did not improve from 0.74803\n","\n","Epoch 00078: val_accuracy did not improve from 0.74803\n","\n","Epoch 00079: val_accuracy did not improve from 0.74803\n","\n","Epoch 00080: val_accuracy did not improve from 0.74803\n","\n","Epoch 00081: val_accuracy did not improve from 0.74803\n","\n","Epoch 00082: val_accuracy did not improve from 0.74803\n","\n","Epoch 00083: val_accuracy did not improve from 0.74803\n","\n","Epoch 00084: val_accuracy did not improve from 0.74803\n","\n","Epoch 00085: val_accuracy did not improve from 0.74803\n","\n","Epoch 00086: val_accuracy did not improve from 0.74803\n","\n","Epoch 00087: val_accuracy did not improve from 0.74803\n","\n","Epoch 00088: val_accuracy did not improve from 0.74803\n","\n","Epoch 00089: val_accuracy did not improve from 0.74803\n","\n","Epoch 00090: val_accuracy did not improve from 0.74803\n","\n","Epoch 00091: val_accuracy did not improve from 0.74803\n","\n","Epoch 00092: val_accuracy did not improve from 0.74803\n","\n","Epoch 00093: val_accuracy did not improve from 0.74803\n","\n","Epoch 00094: val_accuracy did not improve from 0.74803\n","\n","Epoch 00095: val_accuracy did not improve from 0.74803\n","\n","Epoch 00096: val_accuracy did not improve from 0.74803\n","\n","Epoch 00097: val_accuracy did not improve from 0.74803\n","\n","Epoch 00098: val_accuracy did not improve from 0.74803\n","\n","Epoch 00099: val_accuracy improved from 0.74803 to 0.75197, saving model to weights.best.hdf5\n","\n","Epoch 00100: val_accuracy did not improve from 0.75197\n","\n","Epoch 00101: val_accuracy did not improve from 0.75197\n","\n","Epoch 00102: val_accuracy did not improve from 0.75197\n","\n","Epoch 00103: val_accuracy did not improve from 0.75197\n","\n","Epoch 00104: val_accuracy did not improve from 0.75197\n","\n","Epoch 00105: val_accuracy did not improve from 0.75197\n","\n","Epoch 00106: val_accuracy did not improve from 0.75197\n","\n","Epoch 00107: val_accuracy did not improve from 0.75197\n","\n","Epoch 00108: val_accuracy did not improve from 0.75197\n","\n","Epoch 00109: val_accuracy did not improve from 0.75197\n","\n","Epoch 00110: val_accuracy did not improve from 0.75197\n","\n","Epoch 00111: val_accuracy did not improve from 0.75197\n","\n","Epoch 00112: val_accuracy did not improve from 0.75197\n","\n","Epoch 00113: val_accuracy did not improve from 0.75197\n","\n","Epoch 00114: val_accuracy did not improve from 0.75197\n","\n","Epoch 00115: val_accuracy did not improve from 0.75197\n","\n","Epoch 00116: val_accuracy did not improve from 0.75197\n","\n","Epoch 00117: val_accuracy did not improve from 0.75197\n","\n","Epoch 00118: val_accuracy did not improve from 0.75197\n","\n","Epoch 00119: val_accuracy did not improve from 0.75197\n","\n","Epoch 00120: val_accuracy did not improve from 0.75197\n","\n","Epoch 00121: val_accuracy did not improve from 0.75197\n","\n","Epoch 00122: val_accuracy did not improve from 0.75197\n","\n","Epoch 00123: val_accuracy did not improve from 0.75197\n","\n","Epoch 00124: val_accuracy did not improve from 0.75197\n","\n","Epoch 00125: val_accuracy did not improve from 0.75197\n","\n","Epoch 00126: val_accuracy did not improve from 0.75197\n","\n","Epoch 00127: val_accuracy did not improve from 0.75197\n","\n","Epoch 00128: val_accuracy did not improve from 0.75197\n","\n","Epoch 00129: val_accuracy did not improve from 0.75197\n","\n","Epoch 00130: val_accuracy did not improve from 0.75197\n","\n","Epoch 00131: val_accuracy did not improve from 0.75197\n","\n","Epoch 00132: val_accuracy did not improve from 0.75197\n","\n","Epoch 00133: val_accuracy did not improve from 0.75197\n","\n","Epoch 00134: val_accuracy did not improve from 0.75197\n","\n","Epoch 00135: val_accuracy did not improve from 0.75197\n","\n","Epoch 00136: val_accuracy did not improve from 0.75197\n","\n","Epoch 00137: val_accuracy did not improve from 0.75197\n","\n","Epoch 00138: val_accuracy did not improve from 0.75197\n","\n","Epoch 00139: val_accuracy did not improve from 0.75197\n","\n","Epoch 00140: val_accuracy did not improve from 0.75197\n","\n","Epoch 00141: val_accuracy did not improve from 0.75197\n","\n","Epoch 00142: val_accuracy did not improve from 0.75197\n","\n","Epoch 00143: val_accuracy did not improve from 0.75197\n","\n","Epoch 00144: val_accuracy did not improve from 0.75197\n","\n","Epoch 00145: val_accuracy did not improve from 0.75197\n","\n","Epoch 00146: val_accuracy did not improve from 0.75197\n","\n","Epoch 00147: val_accuracy did not improve from 0.75197\n","\n","Epoch 00148: val_accuracy did not improve from 0.75197\n","\n","Epoch 00149: val_accuracy did not improve from 0.75197\n","\n","Epoch 00150: val_accuracy did not improve from 0.75197\n"]},{"output_type":"execute_result","data":{"text/plain":["<keras.callbacks.History at 0x7f64a2359810>"]},"metadata":{},"execution_count":6}]},{"cell_type":"markdown","metadata":{"id":"RV-EQh42ioFh"},"source":["## Loading the best model from disk"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1ifAjxgDhqsP","executionInfo":{"status":"ok","timestamp":1637683484266,"user_tz":-330,"elapsed":897,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}},"outputId":"3350e788-85b0-405a-81f6-fad404755ed3"},"source":["model = Sequential()\n","model.add(Dense(12, input_dim=8, activation='relu'))\n","model.add(Dense(8, activation='relu'))\n","model.add(Dense(1, activation='sigmoid'))\n","\n","model.load_weights(\"weights.best.hdf5\")\n","\n","model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n","print(\"Created model and loaded weights from file\")\n","\n","scores = model.evaluate(X, Y, verbose=0)\n","print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Created model and loaded weights from file\n","accuracy: 74.35%\n"]}]},{"cell_type":"markdown","metadata":{"id":"ANs2N2QrbxBl"},"source":["---"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bZNZkzeubxBn","executionInfo":{"status":"ok","timestamp":1637681742834,"user_tz":-330,"elapsed":3309,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}},"outputId":"f165e6d7-77af-4af4-b827-18e0d18738ae"},"source":["!pip install -q watermark\n","%reload_ext watermark\n","%watermark -a \"Sparsh A.\" -m -iv -u -t -d"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Author: Sparsh A.\n","\n","Last updated: 2021-11-23 15:35:41\n","\n","Compiler    : GCC 7.5.0\n","OS          : Linux\n","Release     : 5.4.104+\n","Machine     : x86_64\n","Processor   : x86_64\n","CPU cores   : 2\n","Architecture: 64bit\n","\n","IPython: 5.5.0\n","keras  : 2.7.0\n","numpy  : 1.19.5\n","\n"]}]},{"cell_type":"markdown","metadata":{"id":"NBMq1V3AbxBn"},"source":["---"]},{"cell_type":"markdown","metadata":{"id":"ebCJvoi9bxBo"},"source":["**END**"]}]}