{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"2022-01-11-gmf-yelp.ipynb","provenance":[{"file_id":"https://github.com/recohut/nbs/blob/main/raw/P254192%20%7C%20Training%20GMF%20with%20Truncated%20and%20Reweighted%20Denoising%20Losses%20on%20Yelp%20Dataset.ipynb","timestamp":1644607236067}],"collapsed_sections":[],"authorship_tag":"ABX9TyOW73ZiRmfpoXImxYpevmen"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# Training GMF with Truncated and Reweighted Denoising Losses on Yelp Dataset"],"metadata":{"id":"T5kY-7DqauRg"}},{"cell_type":"markdown","source":["## Executive summary\n","\n","| | |\n","| --- | --- |\n","| Problem | It is of importance to account for the inevitable noises in implicit feedback. However, little work on recommendation has taken the noisy nature of implicit feedback into consideration. |\n","| Prblm Stmt. | We formulate a denoising recommender training task as $Θ^∗ = \\text{argmin}_Θ \\mathcal{L}(\\textit{denoise}(\\bar{D}))$ aiming to learn a reliable recommender model with parameters $Θ^∗$ by denoising implicit feedback $\\bar{D}$. Formally, by assuming the existence of inconsistency between $y_{ui}^∗$ and $\\bar{y}_{ui}$, we define noisy interactions (a.k.a. false-positive interactions) as $\\{(u, i)|y_{ui}^∗ = 0 ∧ \\bar{y}_{ui} = 1\\}$. |\n","| Solution | Adaptive Denoising Training (ADT), which adaptively prunes the noisy interactions by two paradigms - Truncated Loss and Reweighted Loss. Furthermore, we consider extra feedback (e.g., rating) as auxiliary signal and employ three strategies to incorporate extra feedback into ADT: fine-tuning, warm-up training, and colliding inference. |\n","| Dataset | Adressa, Amazon-books, Yelp2018. |\n","| Preprocessing | We split the dataset into training, validation, and testing sets, and explored two experimental settings: 1) Extra feedback is unavailable during training. To evaluate the performance of denoising implicit feedback, we kept all interactions, including the false-positive ones, in training and validation, and tested the models only on true-positive interactions. 2) Sparse extra feedback is available during training. We assume that partial true-positive interactions have already been known, which will be used to verify the performance of the proposed three strategies: fine-tuning, warm-up training, and colliding inference. |\n","| Metrics | Recall, NDCG |\n","| Hyperparams | For GMF and NeuMF, the factor numbers of users and items are both 32. As to CDAE, the hidden size of MLP is set as 200. In addition, Adam is applied to optimize all the parameters with the learning rate initialized as 0.001 and he batch size set as 1,024. As to the ADT strategies, they have three hyper-parameters in total: α and max in the T-CE loss, and β in the R-CE loss. In detail, max is searched in {0.05, 0.1, 0.2} and β is tuned in {0.05, 0.1, ..., 0.25, 0.5, 1.0}. As for α, we controlled its range by adjusting the iteration number N to the maximum drop rate max, and N is adjusted in {1k, 5k, 10k, 20k, 30k}. In colliding inference, the number of neighbors Nu is tuned in {1, 3, 5, 10, 20, 50, 100}, wj is set as 1/|Nu|, and λ is searched in {0, 0.1, 0.2, ..., 1}. We used the validation set to tune the hyper-parameters and reported the performance on the testing set. |\n","| Models | GMF, NMF, CDAE, {GMF, NMF, CDAE}+T_CE, {GMF, NMF, CDAE}+R_CE |\n","| Cluster | Python 3.6+, PyTorch |\n","| Tags | `LossReweighting`, `TruncatedLoss`, `MatrixFactorization`, `Denoising` |\n","| Credits | Wenjie Wang |"],"metadata":{"id":"wVfZ0xpJb4nI"}},{"cell_type":"markdown","source":["## Process flow\n","\n","![](https://github.com/RecoHut-Stanzas/S063707/raw/main/images/process_flow.svg)"],"metadata":{"id":"qHcxWaZPqFyE"}},{"cell_type":"markdown","source":["## Setup"],"metadata":{"id":"009XG-W-b4is"}},{"cell_type":"code","source":["import numpy as np \n","import pandas as pd \n","import scipy.sparse as sp\n","from copy import deepcopy\n","import random\n","import math\n","import os\n","import time\n","import argparse\n","\n","import torch\n","import torch.nn as nn\n","import torch.utils.data as data\n","import torch.nn.functional as F \n","from torch.autograd import Variable\n","import torch.optim as optim\n","import torch.backends.cudnn as cudnn\n","from torch.utils.tensorboard import SummaryWriter"],"metadata":{"id":"NhKMYe3Mb4gO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["parser = argparse.ArgumentParser()\n","parser.add_argument('--dataset', \n","\ttype = str,\n","\thelp = 'dataset used for training, options: amazon_book, yelp, adressa',\n","\tdefault = 'yelp')\n","parser.add_argument('--model', \n","\ttype = str,\n","\thelp = 'model used for training. options: GMF, NeuMF-end',\n","\tdefault = 'GMF')\n","parser.add_argument('--alpha', \n","\ttype = float, \n","\tdefault = 0.2, \n","\thelp='hyperparameter in loss function')\n","parser.add_argument('--drop_rate', \n","\ttype = float,\n","\thelp = 'drop rate',\n","\tdefault = 0.2)\n","parser.add_argument('--num_gradual', \n","\ttype = int, \n","\tdefault = 30000,\n","\thelp='how many epochs to linearly increase drop_rate')\n","parser.add_argument('--exponent', \n","\ttype = float, \n","\tdefault = 1, \n","\thelp='exponent of the drop rate {0.5, 1, 2}')\n","parser.add_argument(\"--lr\", \n","\ttype=float, \n","\tdefault=0.001, \n","\thelp=\"learning rate\")\n","parser.add_argument(\"--dropout\", \n","\ttype=float,\n","\tdefault=0.0,  \n","\thelp=\"dropout rate\")\n","parser.add_argument(\"--batch_size\", \n","\ttype=int, \n","\tdefault=1024, \n","\thelp=\"batch size for training\")\n","parser.add_argument(\"--epochs\", \n","\ttype=int,\n","\tdefault=10,\n","\thelp=\"training epoches\")\n","parser.add_argument(\"--eval_freq\", \n","\ttype=int,\n","\tdefault=2000,\n","\thelp=\"the freq of eval\")\n","parser.add_argument(\"--top_k\", \n","\ttype=list, \n","\tdefault=[50, 100],\n","\thelp=\"compute metrics@top_k\")\n","parser.add_argument(\"--factor_num\", \n","\ttype=int,\n","\tdefault=32, \n","\thelp=\"predictive factors numbers in the model\")\n","parser.add_argument(\"--num_layers\", \n","\ttype=int,\n","\tdefault=3, \n","\thelp=\"number of layers in MLP model\")\n","parser.add_argument(\"--num_ng\", \n","\ttype=int,\n","\tdefault=1, \n","\thelp=\"sample negative items for training\")\n","parser.add_argument(\"--out\", \n","\tdefault=True,\n","\thelp=\"save model or not\")\n","parser.add_argument(\"--gpu\", \n","\ttype=str,\n","\tdefault=\"0\",\n","\thelp=\"gpu card ID\")\n","args = parser.parse_args([])\n","\n","os.environ[\"CUDA_VISIBLE_DEVICES\"] = args.gpu\n","cudnn.benchmark = True\n","\n","torch.manual_seed(2019) # cpu\n","torch.cuda.manual_seed(2019) #gpu\n","np.random.seed(2019) #numpy\n","random.seed(2019) #random and transforms\n","torch.backends.cudnn.deterministic=True # cudnn\n","\n","def worker_init_fn(worker_id):\n","    np.random.seed(2019 + worker_id)\n","\n","data_path = '{}/'.format(args.dataset)\n","model_path = './models/{}/'.format(args.dataset)\n","os.makedirs('./models', exist_ok=True)\n","print(\"arguments: %s \" %(args))\n","print(\"config model\", args.model)\n","print(\"config data path\", data_path)\n","print(\"config model path\", model_path)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"K4L82dHCeb1H","executionInfo":{"status":"ok","timestamp":1639311517343,"user_tz":-330,"elapsed":722,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}},"outputId":"32c39b27-4240-478a-da19-11af8f6d2dc4"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["arguments: Namespace(alpha=0.2, batch_size=1024, dataset='yelp', drop_rate=0.2, dropout=0.0, epochs=10, eval_freq=2000, exponent=1, factor_num=32, gpu='0', lr=0.001, model='GMF', num_gradual=30000, num_layers=3, num_ng=1, out=True, top_k=[50, 100]) \n","config model GMF\n","config data path yelp/\n","config model path ./models/yelp/\n"]}]},{"cell_type":"markdown","source":["## Data"],"metadata":{"id":"GXK73SDob4dR"}},{"cell_type":"code","source":["!git clone --branch v4 https://github.com/RecoHut-Datasets/yelp.git"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"D6y9RjVob4ZS","executionInfo":{"status":"ok","timestamp":1639310015219,"user_tz":-330,"elapsed":24510,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}},"outputId":"a74f0442-81ed-4ad8-80fd-2ec961f58e1c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Cloning into 'yelp'...\n","remote: Enumerating objects: 57, done.\u001b[K\n","remote: Counting objects: 100% (57/57), done.\u001b[K\n","remote: Compressing objects: 100% (50/50), done.\u001b[K\n","remote: Total 57 (delta 5), reused 52 (delta 3), pack-reused 0\u001b[K\n","Unpacking objects: 100% (57/57), done.\n"]}]},{"cell_type":"code","source":["def load_all(dataset, data_path):\n","\n","\ttrain_rating = data_path + '{}.train.rating'.format(dataset)\n","\tvalid_rating = data_path + '{}.valid.rating'.format(dataset)\n","\ttest_negative = data_path + '{}.test.negative'.format(dataset)\n","\n","\t################# load training data #################\t\n","\ttrain_data = pd.read_csv(\n","\t\ttrain_rating, \n","\t\tsep='\\t', header=None, names=['user', 'item', 'noisy'], \n","\t\tusecols=[0, 1, 2], dtype={0: np.int32, 1: np.int32, 2: np.int32})\n","\n","\tif dataset == \"adressa\":\n","\t\tuser_num = 212231\n","\t\titem_num = 6596\n","\telse:\n","\t\tuser_num = train_data['user'].max() + 1\n","\t\titem_num = train_data['item'].max() + 1\n","\tprint(\"user, item num\")\n","\tprint(user_num, item_num)\n","\ttrain_data = train_data.values.tolist()\n","\n","\t# load ratings as a dok matrix\n","\ttrain_mat = sp.dok_matrix((user_num, item_num), dtype=np.float32)\n","\ttrain_data_list = []\n","\ttrain_data_noisy = []\n","\tfor x in train_data:\n","\t\ttrain_mat[x[0], x[1]] = 1.0\n","\t\ttrain_data_list.append([x[0], x[1]])\n","\t\ttrain_data_noisy.append(x[2])\n","\n","\t################# load validation data #################\n","\tvalid_data = pd.read_csv(\n","\t\tvalid_rating, \n","\t\tsep='\\t', header=None, names=['user', 'item', 'noisy'], \n","\t\tusecols=[0, 1, 2], dtype={0: np.int32, 1: np.int32, 2: np.int32})\n","\tvalid_data = valid_data.values.tolist()\n","\tvalid_data_list = []\n","\tfor x in valid_data:\n","\t\tvalid_data_list.append([x[0], x[1]])\n","\t\n","\tuser_pos = {}\n","\tfor x in train_data_list:\n","\t\tif x[0] in user_pos:\n","\t\t\tuser_pos[x[0]].append(x[1])\n","\t\telse:\n","\t\t\tuser_pos[x[0]] = [x[1]]\n","\tfor x in valid_data_list:\n","\t\tif x[0] in user_pos:\n","\t\t\tuser_pos[x[0]].append(x[1])\n","\t\telse:\n","\t\t\tuser_pos[x[0]] = [x[1]]\n","\n","\n","\t################# load testing data #################\n","\ttest_mat = sp.dok_matrix((user_num, item_num), dtype=np.float32)\n","\n","\ttest_data_pos = {}\n","\twith open(test_negative, 'r') as fd:\n","\t\tline = fd.readline()\n","\t\twhile line != None and line != '':\n","\t\t\tarr = line.split('\\t')\n","\t\t\tif dataset == \"adressa\":\n","\t\t\t\tu = eval(arr[0])[0]\n","\t\t\t\ti = eval(arr[0])[1]\n","\t\t\telse:\n","\t\t\t\tu = int(arr[0])\n","\t\t\t\ti = int(arr[1])\n","\t\t\tif u in test_data_pos:\n","\t\t\t\ttest_data_pos[u].append(i)\n","\t\t\telse:\n","\t\t\t\ttest_data_pos[u] = [i]\n","\t\t\ttest_mat[u, i] = 1.0\n","\t\t\tline = fd.readline()\n","\n","\n","\treturn train_data_list, valid_data_list, test_data_pos, user_pos, user_num, item_num, train_mat, train_data_noisy"],"metadata":{"id":"mpIf7UVqb7ln"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class NCFData(data.Dataset):\n","\tdef __init__(self, features,\n","\t\t\t\tnum_item, train_mat=None, num_ng=0, is_training=0, noisy_or_not=None):\n","\t\tsuper(NCFData, self).__init__()\n","\t\t\"\"\" Note that the labels are only useful when training, we thus \n","\t\t\tadd them in the ng_sample() function.\n","\t\t\"\"\"\n","\t\tself.features_ps = features\n","\t\tif is_training == 0:\n","\t\t\tself.noisy_or_not = noisy_or_not\n","\t\telse:\n","\t\t\tself.noisy_or_not = [0 for _ in range(len(features))]\n","\t\tself.num_item = num_item\n","\t\tself.train_mat = train_mat\n","\t\tself.num_ng = num_ng\n","\t\tself.is_training = is_training\n","\t\tself.labels = [0 for _ in range(len(features))]\n","\n","\tdef ng_sample(self):\n","\t\tassert self.is_training  != 2, 'no need to sampling when testing'\n","\n","\t\tself.features_ng = []\n","\t\tfor x in self.features_ps:\n","\t\t\tu = x[0]\n","\t\t\tfor t in range(self.num_ng):\n","\t\t\t\tj = np.random.randint(self.num_item)\n","\t\t\t\twhile (u, j) in self.train_mat:\n","\t\t\t\t\tj = np.random.randint(self.num_item)\n","\t\t\t\tself.features_ng.append([u, j])\n","\n","\t\tlabels_ps = [1 for _ in range(len(self.features_ps))]\n","\t\tlabels_ng = [0 for _ in range(len(self.features_ng))]\n","\t\tself.noisy_or_not_fill = self.noisy_or_not + [1 for _ in range(len(self.features_ng))]\n","\t\tself.features_fill = self.features_ps + self.features_ng\n","\t\tassert len(self.noisy_or_not_fill) == len(self.features_fill)\n","\t\tself.labels_fill = labels_ps + labels_ng\n","\n","\tdef __len__(self):\n","\t\treturn (self.num_ng + 1) * len(self.labels)\n","\n","\tdef __getitem__(self, idx):\n","\t\tfeatures = self.features_fill if self.is_training != 2 \\\n","\t\t\t\t\telse self.features_ps\n","\t\tlabels = self.labels_fill if self.is_training != 2 \\\n","\t\t\t\t\telse self.labels\n","\t\tnoisy_or_not = self.noisy_or_not_fill if self.is_training != 2 \\\n","\t\t\t\t\telse self.noisy_or_not\n","\n","\t\tuser = features[idx][0]\n","\t\titem = features[idx][1]\n","\t\tlabel = labels[idx]\n","\t\tnoisy_label = noisy_or_not[idx]\n","\n","\t\treturn user, item, label, noisy_label"],"metadata":{"id":"Jv8T8msTcSU1"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Model"],"metadata":{"id":"YutDztVXb7i2"}},{"cell_type":"code","source":["class NCF(nn.Module):\n","\tdef __init__(self, user_num, item_num, factor_num, num_layers,\n","\t\t\t\t\tdropout, model, GMF_model=None, MLP_model=None):\n","\t\tsuper(NCF, self).__init__()\n","\t\t\"\"\"\n","\t\tuser_num: number of users;\n","\t\titem_num: number of items;\n","\t\tfactor_num: number of predictive factors;\n","\t\tnum_layers: the number of layers in MLP model;\n","\t\tdropout: dropout rate between fully connected layers;\n","\t\tmodel: 'MLP', 'GMF', 'NeuMF-end', and 'NeuMF-pre';\n","\t\tGMF_model: pre-trained GMF weights;\n","\t\tMLP_model: pre-trained MLP weights.\n","\t\t\"\"\"\t\t\n","\t\tself.dropout = dropout\n","\t\tself.model = model\n","\t\tself.GMF_model = GMF_model\n","\t\tself.MLP_model = MLP_model\n","\n","\t\tself.embed_user_GMF = nn.Embedding(user_num, factor_num)\n","\t\tself.embed_item_GMF = nn.Embedding(item_num, factor_num)\n","\t\tself.embed_user_MLP = nn.Embedding(\n","\t\t\t\tuser_num, factor_num * (2 ** (num_layers - 1)))\n","\t\tself.embed_item_MLP = nn.Embedding(\n","\t\t\t\titem_num, factor_num * (2 ** (num_layers - 1)))\n","\n","\t\tMLP_modules = []\n","\t\tfor i in range(num_layers):\n","\t\t\tinput_size = factor_num * (2 ** (num_layers - i))\n","\t\t\tMLP_modules.append(nn.Dropout(p=self.dropout))\n","\t\t\tMLP_modules.append(nn.Linear(input_size, input_size//2))\n","\t\t\tMLP_modules.append(nn.ReLU())\n","\t\tself.MLP_layers = nn.Sequential(*MLP_modules)\n","\n","\t\tif self.model in ['MLP', 'GMF']:\n","\t\t\tpredict_size = factor_num \n","\t\telse:\n","\t\t\tpredict_size = factor_num * 2\n","\t\tself.predict_layer = nn.Linear(predict_size, 1)\n","\n","\t\tself._init_weight_()\n","\n","\tdef _init_weight_(self):\n","\t\t\"\"\" We leave the weights initialization here. \"\"\"\n","\t\tif not self.model == 'NeuMF-pre':\n","\t\t\tnn.init.normal_(self.embed_user_GMF.weight, std=0.01)\n","\t\t\tnn.init.normal_(self.embed_user_MLP.weight, std=0.01)\n","\t\t\tnn.init.normal_(self.embed_item_GMF.weight, std=0.01)\n","\t\t\tnn.init.normal_(self.embed_item_MLP.weight, std=0.01)\n","\n","\t\t\tfor m in self.MLP_layers:\n","\t\t\t\tif isinstance(m, nn.Linear):\n","\t\t\t\t\tnn.init.xavier_uniform_(m.weight)\n","\t\t\tnn.init.kaiming_uniform_(self.predict_layer.weight, \n","\t\t\t\t\t\t\t\t\ta=1, nonlinearity='sigmoid')\n","\n","\t\t\tfor m in self.modules():\n","\t\t\t\tif isinstance(m, nn.Linear) and m.bias is not None:\n","\t\t\t\t\tm.bias.data.zero_()\n","\t\telse:\n","\t\t\t# embedding layers\n","\t\t\tself.embed_user_GMF.weight.data.copy_(\n","\t\t\t\t\t\t\tself.GMF_model.embed_user_GMF.weight)\n","\t\t\tself.embed_item_GMF.weight.data.copy_(\n","\t\t\t\t\t\t\tself.GMF_model.embed_item_GMF.weight)\n","\t\t\tself.embed_user_MLP.weight.data.copy_(\n","\t\t\t\t\t\t\tself.MLP_model.embed_user_MLP.weight)\n","\t\t\tself.embed_item_MLP.weight.data.copy_(\n","\t\t\t\t\t\t\tself.MLP_model.embed_item_MLP.weight)\n","\n","\t\t\t# mlp layers\n","\t\t\tfor (m1, m2) in zip(\n","\t\t\t\tself.MLP_layers, self.MLP_model.MLP_layers):\n","\t\t\t\tif isinstance(m1, nn.Linear) and isinstance(m2, nn.Linear):\n","\t\t\t\t\tm1.weight.data.copy_(m2.weight)\n","\t\t\t\t\tm1.bias.data.copy_(m2.bias)\n","\n","\t\t\t# predict layers\n","\t\t\tpredict_weight = torch.cat([\n","\t\t\t\tself.GMF_model.predict_layer.weight, \n","\t\t\t\tself.MLP_model.predict_layer.weight], dim=1)\n","\t\t\tprecit_bias = self.GMF_model.predict_layer.bias + \\\n","\t\t\t\t\t\tself.MLP_model.predict_layer.bias\n","\n","\t\t\tself.predict_layer.weight.data.copy_(0.5 * predict_weight)\n","\t\t\tself.predict_layer.bias.data.copy_(0.5 * precit_bias)\n","\n","\tdef forward(self, user, item):\n","\t\tif not self.model == 'MLP':\n","\t\t\tembed_user_GMF = self.embed_user_GMF(user)\n","\t\t\tembed_item_GMF = self.embed_item_GMF(item)\n","\t\t\toutput_GMF = embed_user_GMF * embed_item_GMF\n","\t\tif not self.model == 'GMF':\n","\t\t\tembed_user_MLP = self.embed_user_MLP(user)\n","\t\t\tembed_item_MLP = self.embed_item_MLP(item)\n","\t\t\tinteraction = torch.cat((embed_user_MLP, embed_item_MLP), -1)\n","\t\t\toutput_MLP = self.MLP_layers(interaction)\n","\n","\t\tif self.model == 'GMF':\n","\t\t\tconcat = output_GMF\n","\t\telif self.model == 'MLP':\n","\t\t\tconcat = output_MLP\n","\t\telse:\n","\t\t\tconcat = torch.cat((output_GMF, output_MLP), -1)\n","\n","\t\tprediction = self.predict_layer(concat)\n","\t\treturn prediction.view(-1)"],"metadata":{"id":"eD0tBH5ub7gH"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Evaluate"],"metadata":{"id":"ADEh-IsUb7Xm"}},{"cell_type":"code","source":["def test_all_users(model, batch_size, item_num, test_data_pos, user_pos, top_k):\n","    \n","    predictedIndices = []\n","    GroundTruth = []\n","    for u in test_data_pos:\n","        batch_num = item_num // batch_size\n","        batch_user = torch.Tensor([u]*batch_size).long().cuda()\n","        st, ed = 0, batch_size\n","        for i in range(batch_num):\n","            batch_item = torch.Tensor([i for i in range(st, ed)]).long().cuda()\n","            pred = model(batch_user, batch_item)\n","            if i == 0:\n","                predictions = pred\n","            else:\n","                predictions = torch.cat([predictions, pred], 0)\n","            st, ed = st+batch_size, ed+batch_size\n","        ed = ed - batch_size\n","        batch_item = torch.Tensor([i for i in range(ed, item_num)]).long().cuda()\n","        batch_user = torch.Tensor([u]*(item_num-ed)).long().cuda()\n","        pred = model(batch_user, batch_item)\n","        predictions = torch.cat([predictions, pred], 0)\n","        test_data_mask = [0] * item_num\n","        if u in user_pos:\n","            for i in user_pos[u]:\n","                test_data_mask[i] = -9999\n","        predictions = predictions + torch.Tensor(test_data_mask).float().cuda()\n","        _, indices = torch.topk(predictions, top_k[-1])\n","        indices = indices.cpu().numpy().tolist()\n","        predictedIndices.append(indices)\n","        GroundTruth.append(test_data_pos[u])\n","    precision, recall, NDCG, MRR = compute_acc(GroundTruth, predictedIndices, top_k)\n","    return precision, recall, NDCG, MRR"],"metadata":{"id":"I5oF1IG7dSFb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def compute_acc(GroundTruth, predictedIndices, topN):\n","    precision = [] \n","    recall = [] \n","    NDCG = [] \n","    MRR = []\n","    \n","    for index in range(len(topN)):\n","        sumForPrecision = 0\n","        sumForRecall = 0\n","        sumForNdcg = 0\n","        sumForMRR = 0\n","        for i in range(len(predictedIndices)):  # for a user,\n","            if len(GroundTruth[i]) != 0:\n","                mrrFlag = True\n","                userHit = 0\n","                userMRR = 0\n","                dcg = 0\n","                idcg = 0\n","                idcgCount = len(GroundTruth[i])\n","                ndcg = 0\n","                hit = []\n","                for j in range(topN[index]):\n","                    if predictedIndices[i][j] in GroundTruth[i]:\n","                        # if Hit!\n","                        dcg += 1.0/math.log2(j + 2)\n","                        if mrrFlag:\n","                            userMRR = (1.0/(j+1.0))\n","                            mrrFlag = False\n","                        userHit += 1\n","                \n","                    if idcgCount > 0:\n","                        idcg += 1.0/math.log2(j + 2)\n","                        idcgCount = idcgCount-1\n","                            \n","                if(idcg != 0):\n","                    ndcg += (dcg/idcg)\n","                    \n","                sumForPrecision += userHit / topN[index]\n","                sumForRecall += userHit / len(GroundTruth[i])               \n","                sumForNdcg += ndcg\n","                sumForMRR += userMRR\n","        \n","        precision.append(sumForPrecision / len(predictedIndices))\n","        recall.append(sumForRecall / len(predictedIndices))\n","        NDCG.append(sumForNdcg / len(predictedIndices))\n","        MRR.append(sumForMRR / len(predictedIndices))\n","        \n","    return precision, recall, NDCG, MRR"],"metadata":{"id":"gBInZ4_jdS8X"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## T_CE"],"metadata":{"id":"K1tu6vgIfkyj"}},{"cell_type":"markdown","source":["### Loss function"],"metadata":{"id":"lfi9YaIBb7dY"}},{"cell_type":"code","source":["def loss_function(y, t, drop_rate):\n","    loss = F.binary_cross_entropy_with_logits(y, t, reduce = False)\n","\n","    loss_mul = loss * t\n","    ind_sorted = np.argsort(loss_mul.cpu().data).cuda()\n","    loss_sorted = loss[ind_sorted]\n","\n","    remember_rate = 1 - drop_rate\n","    num_remember = int(remember_rate * len(loss_sorted))\n","\n","    ind_update = ind_sorted[:num_remember]\n","\n","    loss_update = F.binary_cross_entropy_with_logits(y[ind_update], t[ind_update])\n","\n","    return loss_update"],"metadata":{"id":"SaBvZ5eVb7aw"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Training & Evaluation"],"metadata":{"id":"et6OGWMBeJiE"}},{"cell_type":"code","source":["############################## PREPARE DATASET ##########################\n","\n","train_data, valid_data, test_data_pos, user_pos, user_num ,item_num, train_mat, train_data_noisy = load_all(args.dataset, data_path)\n","\n","# construct the train and test datasets\n","train_dataset = NCFData(\n","\t\ttrain_data, item_num, train_mat, args.num_ng, 0, train_data_noisy)\n","valid_dataset = NCFData(\n","\t\tvalid_data, item_num, train_mat, args.num_ng, 1)\n","\n","train_loader = data.DataLoader(train_dataset,\n","\t\tbatch_size=args.batch_size, shuffle=True, num_workers=2, pin_memory=True, worker_init_fn=worker_init_fn)\n","valid_loader = data.DataLoader(valid_dataset,\n","\t\tbatch_size=args.batch_size, shuffle=True, num_workers=2, pin_memory=True, worker_init_fn=worker_init_fn)\n","\n","print(\"data loaded! user_num:{}, item_num:{} train_data_len:{} test_user_num:{}\".format(user_num, item_num, len(train_data), len(test_data_pos)))\n","\n","########################### CREATE MODEL #################################\n","if args.model == 'NeuMF-pre': # pre-training. Not used in our work.\n","\tGMF_model_path = model_path + 'GMF.pth'\n","\tMLP_model_path = model_path + 'MLP.pth'\n","\tNeuMF_model_path = model_path + 'NeuMF.pth'\n","\tassert os.path.exists(GMF_model_path), 'lack of GMF model'\n","\tassert os.path.exists(MLP_model_path), 'lack of MLP model'\n","\tGMF_model = torch.load(GMF_model_path)\n","\tMLP_model = torch.load(MLP_model_path)\n","else:\n","\tGMF_model = None\n","\tMLP_model = None\n","\n","model = NCF(user_num, item_num, args.factor_num, args.num_layers, \n","\t\t\t\t\t\targs.dropout, args.model, GMF_model, MLP_model)\n","\n","model.cuda()\n","BCE_loss = nn.BCEWithLogitsLoss()\n","\n","if args.model == 'NeuMF-pre':\n","\toptimizer = optim.SGD(model.parameters(), lr=args.lr)\n","else:\n","\toptimizer = optim.Adam(model.parameters(), lr=args.lr)\n","\n","# writer = SummaryWriter() # for visualization\n","\n","# define drop rate schedule\n","def drop_rate_schedule(iteration):\n","\n","\tdrop_rate = np.linspace(0, args.drop_rate**args.exponent, args.num_gradual)\n","\tif iteration < args.num_gradual:\n","\t\treturn drop_rate[iteration]\n","\telse:\n","\t\treturn args.drop_rate\n","\n","\n","########################### Eval #####################################\n","def eval(model, valid_loader, best_loss, count):\n","\t\n","\tmodel.eval()\n","\tepoch_loss = 0\n","\tvalid_loader.dataset.ng_sample() # negative sampling\n","\tfor user, item, label, noisy_or_not in valid_loader:\n","\t\tuser = user.cuda()\n","\t\titem = item.cuda()\n","\t\tlabel = label.float().cuda()\n","\n","\t\tprediction = model(user, item)\n","\t\tloss = loss_function(prediction, label, drop_rate_schedule(count))\n","\t\tepoch_loss += loss.detach()\n","\tprint(\"################### EVAL ######################\")\n","\tprint(\"Eval loss:{}\".format(epoch_loss))\n","\tif epoch_loss < best_loss:\n","\t\tbest_loss = epoch_loss\n","\t\tif args.out:\n","\t\t\tif not os.path.exists(model_path):\n","\t\t\t\tos.mkdir(model_path)\n","\t\t\ttorch.save(model, '{}{}_{}-{}.pth'.format(model_path, args.model, args.drop_rate, args.num_gradual))\n","\treturn best_loss\n","\n","########################### Test #####################################\n","def test(model, test_data_pos, user_pos):\n","\ttop_k = args.top_k\n","\tmodel.eval()\n","\t_, recall, NDCG, _ = test_all_users(model, 4096, item_num, test_data_pos, user_pos, top_k)\n","\n","\tprint(\"################### TEST ######################\")\n","\tprint(\"Recall {:.4f}-{:.4f}\".format(recall[0], recall[1]))\n","\tprint(\"NDCG {:.4f}-{:.4f}\".format(NDCG[0], NDCG[1]))\n","\n","########################### TRAINING #####################################\n","count, best_hr = 0, 0\n","best_loss = 1e9\n","\n","for epoch in range(args.epochs):\n","\tmodel.train() # Enable dropout (if have).\n","\n","\tstart_time = time.time()\n","\ttrain_loader.dataset.ng_sample()\n","\n","\tfor user, item, label, noisy_or_not in train_loader:\n","\t\tuser = user.cuda()\n","\t\titem = item.cuda()\n","\t\tlabel = label.float().cuda()\n","\n","\t\tmodel.zero_grad()\n","\t\tprediction = model(user, item)\n","\t\tloss = loss_function(prediction, label, drop_rate_schedule(count))\n","\t\tloss.backward()\n","\t\toptimizer.step()\n","\n","\t\tif count % args.eval_freq == 0 and count != 0:\n","\t\t\tprint(\"epoch: {}, iter: {}, loss:{}\".format(epoch, count, loss))\n","\t\t\tbest_loss = eval(model, valid_loader, best_loss, count)\n","\t\t\tmodel.train()\n","\n","\t\tcount += 1\n","\n","print(\"############################## Training End. ##############################\")\n","test_model = torch.load('{}{}_{}-{}.pth'.format(model_path, args.model, args.drop_rate, args.num_gradual))\n","test_model.cuda()\n","test(test_model, test_data_pos, user_pos)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vK6RkgUteJdS","executionInfo":{"status":"ok","timestamp":1639311150350,"user_tz":-330,"elapsed":1134452,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}},"outputId":"27de75fc-3cec-4162-e83a-1fcaf217206d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["user, item num\n","45548 57396\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["data loaded! user_num:45548, item_num:57396 train_data_len:1672520 test_user_num:45525\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.\n","  warnings.warn(warning.format(ret))\n"]},{"output_type":"stream","name":"stdout","text":["epoch: 0, iter: 2000, loss:0.4857825040817261\n","################### EVAL ######################\n","Eval loss:184.8551025390625\n","epoch: 1, iter: 4000, loss:0.24887847900390625\n","################### EVAL ######################\n","Eval loss:115.4634780883789\n","epoch: 1, iter: 6000, loss:0.21611060202121735\n","################### EVAL ######################\n","Eval loss:90.331787109375\n","epoch: 2, iter: 8000, loss:0.18297776579856873\n","################### EVAL ######################\n","Eval loss:77.42632293701172\n","epoch: 3, iter: 10000, loss:0.1536407172679901\n","################### EVAL ######################\n","Eval loss:68.5382080078125\n","epoch: 3, iter: 12000, loss:0.1829090118408203\n","################### EVAL ######################\n","Eval loss:62.065673828125\n","epoch: 4, iter: 14000, loss:0.11369739472866058\n","################### EVAL ######################\n","Eval loss:56.34415817260742\n","epoch: 4, iter: 16000, loss:0.08793307840824127\n","################### EVAL ######################\n","Eval loss:51.920955657958984\n","epoch: 5, iter: 18000, loss:0.1071598008275032\n","################### EVAL ######################\n","Eval loss:48.343868255615234\n","epoch: 6, iter: 20000, loss:0.07359810173511505\n","################### EVAL ######################\n","Eval loss:45.544002532958984\n","epoch: 6, iter: 22000, loss:0.1074337512254715\n","################### EVAL ######################\n","Eval loss:41.1801643371582\n","epoch: 7, iter: 24000, loss:0.07862947136163712\n","################### EVAL ######################\n","Eval loss:39.67675018310547\n","epoch: 7, iter: 26000, loss:0.08733634650707245\n","################### EVAL ######################\n","Eval loss:35.83267593383789\n","epoch: 8, iter: 28000, loss:0.0823584571480751\n","################### EVAL ######################\n","Eval loss:32.80326461791992\n","epoch: 9, iter: 30000, loss:0.029638517647981644\n","################### EVAL ######################\n","Eval loss:31.047956466674805\n","epoch: 9, iter: 32000, loss:0.08437806367874146\n","################### EVAL ######################\n","Eval loss:30.97898292541504\n","############################## Training End. ##############################\n","################### TEST ######################\n","Recall 0.0899-0.1473\n","NDCG 0.0364-0.0494\n"]}]},{"cell_type":"markdown","source":["## R_CE"],"metadata":{"id":"QVTERrOWhwQL"}},{"cell_type":"markdown","source":["### Loss function"],"metadata":{"id":"UgioesbdhwNS"}},{"cell_type":"code","source":["def loss_function(y, t, alpha):\n","\n","    loss = F.binary_cross_entropy_with_logits(y, t, reduce = False)\n","    y_ = torch.sigmoid(y).detach()\n","    weight = torch.pow(y_, alpha) * t + torch.pow((1-y_), alpha) * (1-t)\n","    loss_ = loss * weight\n","    loss_ = torch.mean(loss_)\n","    return loss_"],"metadata":{"id":"xhBZZF99hwJM"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Training & Evaluation"],"metadata":{"id":"VcNPdUPYh6D3"}},{"cell_type":"code","source":["############################## PREPARE DATASET ##########################\n","\n","train_data, valid_data, test_data_pos, user_pos, user_num ,item_num, train_mat, train_data_noisy = load_all(args.dataset, data_path)\n","\n","# construct the train and test datasets\n","train_dataset = NCFData(\n","\t\ttrain_data, item_num, train_mat, args.num_ng, 0, train_data_noisy)\n","valid_dataset = NCFData(\n","\t\tvalid_data, item_num, train_mat, args.num_ng, 1)\n","\n","train_loader = data.DataLoader(train_dataset,\n","\t\tbatch_size=args.batch_size, shuffle=True, num_workers=4, pin_memory=True, worker_init_fn=worker_init_fn)\n","valid_loader = data.DataLoader(valid_dataset,\n","\t\tbatch_size=args.batch_size, shuffle=True, num_workers=4, pin_memory=True, worker_init_fn=worker_init_fn)\n","\n","print(\"data loaded! user_num:{}, item_num:{} train_data_len:{} test_user_num:{}\".format(user_num, item_num, len(train_data), len(test_data_pos)))\n","\n","########################### CREATE MODEL #################################\n","if args.model == 'NeuMF-pre': # pre-training. Not used in our work.\n","\tGMF_model_path = model_path + 'GMF.pth'\n","\tMLP_model_path = model_path + 'MLP.pth'\n","\tNeuMF_model_path = model_path + 'NeuMF.pth'\n","\tassert os.path.exists(GMF_model_path), 'lack of GMF model'\n","\tassert os.path.exists(MLP_model_path), 'lack of MLP model'\n","\tGMF_model = torch.load(GMF_model_path)\n","\tMLP_model = torch.load(MLP_model_path)\n","else:\n","\tGMF_model = None\n","\tMLP_model = None\n","\n","model = NCF(user_num, item_num, args.factor_num, args.num_layers, \n","\t\t\t\t\t\targs.dropout, args.model, GMF_model, MLP_model)\n","\n","model.cuda()\n","BCE_loss = nn.BCEWithLogitsLoss()\n","\n","if args.model == 'NeuMF-pre':\n","\toptimizer = optim.SGD(model.parameters(), lr=args.lr)\n","else:\n","\toptimizer = optim.Adam(model.parameters(), lr=args.lr)\n","\n","# writer = SummaryWriter() # for visualization\n","\n","########################### Eval #####################################\n","def eval(model, valid_loader, best_loss, count):\n","\t\n","\tmodel.eval()\n","\tepoch_loss = 0\n","\tvalid_loader.dataset.ng_sample() # negative sampling\n","\tfor user, item, label, noisy_or_not in valid_loader:\n","\t\tuser = user.cuda()\n","\t\titem = item.cuda()\n","\t\tlabel = label.float().cuda()\n","\n","\t\tprediction = model(user, item)\n","\t\tloss = loss_function(prediction, label, args.alpha)\n","\t\tepoch_loss += loss.detach()\n","\tprint(\"################### EVAL ######################\")\n","\tprint(\"Eval loss:{}\".format(epoch_loss))\n","\tif epoch_loss < best_loss:\n","\t\tbest_loss = epoch_loss\n","\t\tif args.out:\n","\t\t\tif not os.path.exists(model_path):\n","\t\t\t\tos.mkdir(model_path)\n","\t\t\ttorch.save(model, '{}{}_{}.pth'.format(model_path, args.model, args.alpha))\n","\treturn best_loss\n","\n","########################### Test #####################################\n","def test(model, test_data_pos, user_pos):\n","\ttop_k = args.top_k\n","\tmodel.eval()\n","\t_, recall, NDCG, _ = test_all_users(model, 4096, item_num, test_data_pos, user_pos, top_k)\n","\n","\tprint(\"################### TEST ######################\")\n","\tprint(\"Recall {:.4f}-{:.4f}\".format(recall[0], recall[1]))\n","\tprint(\"NDCG {:.4f}-{:.4f}\".format(NDCG[0], NDCG[1]))\n","\n","########################### TRAINING #####################################\n","count, best_hr = 0, 0\n","best_loss = 1e9\n","\n","for epoch in range(args.epochs):\n","\tmodel.train() # Enable dropout (if have).\n","\n","\tstart_time = time.time()\n","\ttrain_loader.dataset.ng_sample()\n","\n","\tfor user, item, label, noisy_or_not in train_loader:\n","\t\tuser = user.cuda()\n","\t\titem = item.cuda()\n","\t\tlabel = label.float().cuda()\n","\n","\t\tmodel.zero_grad()\n","\t\tprediction = model(user, item)\n","\t\tloss = loss_function(prediction, label, args.alpha)\n","\t\tloss.backward()\n","\t\toptimizer.step()\n","\n","\t\tif count % args.eval_freq == 0 and count != 0:\n","\t\t\tprint(\"epoch: {}, iter: {}, loss:{}\".format(epoch, count, loss))\n","\t\t\tbest_loss = eval(model, valid_loader, best_loss, count)\n","\t\t\tmodel.train()\n","\n","\t\tcount += 1\n","\n","print(\"############################## Training End. ##############################\")\n","test_model = torch.load('{}{}_{}.pth'.format(model_path, args.model, args.alpha))\n","test_model.cuda()\n","test(test_model, test_data_pos, user_pos)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7Fr3rITCh6A7","executionInfo":{"status":"ok","timestamp":1639312641090,"user_tz":-330,"elapsed":1118311,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}},"outputId":"9321198d-3a69-4e28-e875-55fc46790edb"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["user, item num\n","45548 57396\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["data loaded! user_num:45548, item_num:57396 train_data_len:1672520 test_user_num:45525\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.\n","  warnings.warn(warning.format(ret))\n"]},{"output_type":"stream","name":"stdout","text":["epoch: 0, iter: 2000, loss:0.41513800621032715\n","################### EVAL ######################\n","Eval loss:161.4785614013672\n","epoch: 1, iter: 4000, loss:0.22459657490253448\n","################### EVAL ######################\n","Eval loss:99.95635223388672\n","epoch: 1, iter: 6000, loss:0.19919627904891968\n","################### EVAL ######################\n","Eval loss:84.1415786743164\n","epoch: 2, iter: 8000, loss:0.17733421921730042\n","################### EVAL ######################\n","Eval loss:77.36993408203125\n","epoch: 3, iter: 10000, loss:0.15221725404262543\n","################### EVAL ######################\n","Eval loss:73.90560150146484\n","epoch: 3, iter: 12000, loss:0.16913804411888123\n","################### EVAL ######################\n","Eval loss:70.4941177368164\n","epoch: 4, iter: 14000, loss:0.14335578680038452\n","################### EVAL ######################\n","Eval loss:69.64494323730469\n","epoch: 4, iter: 16000, loss:0.12221892923116684\n","################### EVAL ######################\n","Eval loss:67.81050109863281\n","epoch: 5, iter: 18000, loss:0.11312472820281982\n","################### EVAL ######################\n","Eval loss:68.0766830444336\n","epoch: 6, iter: 20000, loss:0.09356789290904999\n","################### EVAL ######################\n","Eval loss:69.34413146972656\n","epoch: 6, iter: 22000, loss:0.125381737947464\n","################### EVAL ######################\n","Eval loss:68.00005340576172\n","epoch: 7, iter: 24000, loss:0.10218605399131775\n","################### EVAL ######################\n","Eval loss:69.70828247070312\n","epoch: 7, iter: 26000, loss:0.10325159877538681\n","################### EVAL ######################\n","Eval loss:68.36780548095703\n","epoch: 8, iter: 28000, loss:0.09633355587720871\n","################### EVAL ######################\n","Eval loss:70.18128204345703\n","epoch: 9, iter: 30000, loss:0.0662553533911705\n","################### EVAL ######################\n","Eval loss:72.70501708984375\n","epoch: 9, iter: 32000, loss:0.09527254104614258\n","################### EVAL ######################\n","Eval loss:71.64075469970703\n","############################## Training End. ##############################\n","################### TEST ######################\n","Recall 0.0864-0.1365\n","NDCG 0.0367-0.0481\n"]}]},{"cell_type":"markdown","source":["## References\n","\n","1. [https://github.com/RecoHut-Stanzas/S063707](https://github.com/RecoHut-Stanzas/S063707)\n","2. [https://arxiv.org/pdf/2112.01160v1.pdf](https://arxiv.org/pdf/2112.01160v1.pdf)\n","3. [https://github.com/WenjieWWJ/DenoisingRec](https://github.com/WenjieWWJ/DenoisingRec)"],"metadata":{"id":"wH6KkvQ7xVob"}},{"cell_type":"markdown","source":["---"],"metadata":{"id":"OHHk65wEo8uk"}},{"cell_type":"code","source":["!apt-get -qq install tree\n","!rm -r sample_data"],"metadata":{"id":"v0OE_IPAo8um"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!tree -h --du ."],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_-c-bayco8um","executionInfo":{"status":"ok","timestamp":1639312774688,"user_tz":-330,"elapsed":27,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}},"outputId":"e9f3917f-b46d-41c5-ec0a-14c8a0fc8bf7"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":[".\n","├── [126M]  models\n","│   └── [126M]  yelp\n","│       ├── [ 63M]  GMF_0.2-30000.pth\n","│       └── [ 63M]  GMF_0.2.pth\n","└── [ 26M]  yelp\n","    ├── [ 241]  README.md\n","    ├── [1.8M]  yelp.test.negative\n","    ├── [ 21M]  yelp.train.rating\n","    └── [2.7M]  yelp.valid.rating\n","\n"," 152M used in 3 directories, 6 files\n"]}]},{"cell_type":"code","source":["!pip install -q watermark\n","%reload_ext watermark\n","%watermark -a \"Sparsh A.\" -m -iv -u -t -d"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uHYQT0AMo8un","executionInfo":{"status":"ok","timestamp":1639312785478,"user_tz":-330,"elapsed":4282,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}},"outputId":"666f7d5e-5e1f-48e8-f2dc-656995d46897"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Author: Sparsh A.\n","\n","Last updated: 2021-12-12 12:39:53\n","\n","Compiler    : GCC 7.5.0\n","OS          : Linux\n","Release     : 5.4.104+\n","Machine     : x86_64\n","Processor   : x86_64\n","CPU cores   : 2\n","Architecture: 64bit\n","\n","argparse: 1.1\n","IPython : 5.5.0\n","scipy   : 1.4.1\n","torch   : 1.10.0+cu111\n","numpy   : 1.19.5\n","pandas  : 1.1.5\n","\n"]}]},{"cell_type":"markdown","source":["---"],"metadata":{"id":"CrZCu7YIo8uo"}},{"cell_type":"markdown","source":["**END**"],"metadata":{"id":"YL6VsvtSo8uq"}}]}