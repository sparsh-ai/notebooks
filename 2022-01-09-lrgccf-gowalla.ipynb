{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"2022-01-09-lrgccf-gowalla.ipynb","provenance":[{"file_id":"https://github.com/recohut/notebook/blob/master/_notebooks/2022-01-09-lrgccf-gowalla.ipynb","timestamp":1644607994127},{"file_id":"https://github.com/recohut/nbs/blob/main/raw/P174968%20%7C%20LR-GCCF%20on%20Gowalla.ipynb","timestamp":1644598450989}],"collapsed_sections":[],"authorship_tag":"ABX9TyNgfemhKTG44apSVA7UTAxV"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# LR-GCCF on Gowalla"],"metadata":{"id":"mVyRGyhdtRFw"}},{"cell_type":"markdown","source":["## Executive summary"],"metadata":{"id":"Y3oNohENVHAH"}},{"cell_type":"markdown","source":["| | |\n","| --- | --- |\n","| Problem | GCNs suffer from training difficulty due to non-linear activations, and over-smoothing problem. |\n","| Hypothesis | removing non-linearities would enhance recommendation performance.  |\n","| Solution | Linear model with residual network structure |\n","| Dataset | Gowalla |\n","| Preprocessing | we remove users (items) that have less than 10 interaction records. After that, we randomly select 80% of the records for training, 10% for validation and the remaining 10% for test. |\n","| Metrics | HR, NDCG |\n","| Hyperparams | There are two important parameters: the dimension D of the user and item embedding matrix E, and the regularization parameter λ in the objective function. The embedding size is fixed to 64. We try the regularization parameter λ in the range [0.0001, 0.001, 0.01, 0.1], and find λ = 0.01 reaches the best performance. |\n","| Models | LR-GCCF |\n","| Cluster | PyTorch with GPU |"],"metadata":{"id":"DK7RWly8VKvU"}},{"cell_type":"markdown","source":["## Process flow\n","\n","![](https://github.com/RecoHut-Stanzas/S794944/raw/main/images/process_flow.svg)"],"metadata":{"id":"s4weKl5kcU3v"}},{"cell_type":"markdown","source":["## Setup"],"metadata":{"id":"paRynetXLa6r"}},{"cell_type":"code","source":["import random\n","import torch \n","import time\n","import pdb\n","import math\n","import os\n","import sys\n","from shutil import copyfile\n","from collections import defaultdict\n","import numpy as np\n","import pandas as pd \n","import scipy.sparse as sp \n","\n","import torch\n","import torch.nn as nn \n","from torch.utils.data import DataLoader\n","import torch.nn.functional as F\n","import torch.autograd as autograd\n","from torch.autograd import Variable\n","import torch.utils.data as data"],"metadata":{"id":"zfsN6pxILcQG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["os.environ[\"CUDA_VISIBLE_DEVICES\"] = '0'"],"metadata":{"id":"CkWg0kI9LjgC"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Data"],"metadata":{"id":"VhlaApRdJJDh"}},{"cell_type":"code","source":["# download\n","dataset = 'gowalla'\n","!git clone --branch v1 https://github.com/RecoHut-Datasets/gowalla.git\n","!wget -q --show-progress -O gowalla/val.txt https://github.com/RecoHut-Datasets/gowalla/raw/main/silver/v1/val.txt\n","\n","# set paths\n","training_path='./gowalla/train.txt'\n","testing_path='./gowalla/test.txt'\n","val_path='./gowalla/val.txt'\n","\n","# meta\n","user_num=29858\n","item_num=40981 \n","factor_num=64\n","batch_size=2048*512\n","top_k=20 \n","num_negative_test_val=-1##all\n","\n","#testing\n","start_i_test=3\n","end_i_test=4\n","setp=1\n","\n","path_save_base = './datanpy'\n","if not os.path.exists(path_save_base):\n","    os.makedirs(path_save_base) \n","\n","run_id='0'\n","path_save_log_base='./log/'+dataset+'/newloss'+run_id\n","if not os.path.exists(path_save_log_base):\n","    os.makedirs(path_save_log_base)  \n","\n","result_file=open(path_save_log_base+'/results.txt','w+')\n","\n","path_save_model_base='./newlossModel/'+dataset+'/s'+run_id\n","if not os.path.exists(path_save_model_base):\n","    os.makedirs(path_save_model_base)"],"metadata":{"id":"V5NS5Z5ULdnO"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"iYvFa6MOwiF0"},"source":["data2npy"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1v2H-ZUPwjUi","executionInfo":{"status":"ok","timestamp":1639038539256,"user_tz":-330,"elapsed":4316,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}},"outputId":"6f97fdcd-fb87-481f-d314-dee370d6ae73"},"source":["train_data_user = defaultdict(set)\n","train_data_item = defaultdict(set) \n","links_file = open(training_path)\n","num_u=0\n","num_u_i=0\n","for _, line in enumerate(links_file):\n","    line=line.strip('\\n')\n","    tmp = line.split(' ')\n","    num_u_i+=len(tmp)-1\n","    num_u+=1\n","    u_id=int(tmp[0])\n","    for i_id in tmp[1:]: \n","        train_data_user[u_id].add(int(i_id))\n","        train_data_item[int(i_id)].add(u_id)\n","np.save(os.path.join(path_save_base,'training_set.npy'),[train_data_user,train_data_item,num_u_i]) \n","print(num_u,num_u_i)\n"," \n","test_data_user = defaultdict(set)\n","test_data_item = defaultdict(set) \n","links_file = open(testing_path)\n","num_u=0\n","num_u_i=0\n","for _, line in enumerate(links_file):\n","    line=line.strip('\\n')\n","    tmp = line.split(' ')\n","    num_u_i+=len(tmp)-1\n","    num_u+=1\n","    u_id=int(tmp[0])\n","    for i_id in tmp[1:]: \n","        test_data_user[u_id].add(int(i_id))\n","        test_data_item[int(i_id)].add(u_id)\n","np.save(os.path.join(path_save_base,'testing_set.npy'),[test_data_user,test_data_item,num_u_i]) \n","print(num_u,num_u_i)\n","\n","\n","val_data_user = defaultdict(set)\n","val_data_item = defaultdict(set) \n","links_file = open(val_path)\n","num_u=0\n","num_u_i=0\n","for _, line in enumerate(links_file):\n","    line=line.strip('\\n')\n","    tmp = line.split(' ')\n","    num_u_i+=len(tmp)-1\n","    num_u+=1\n","    u_id=int(tmp[0])\n","    for i_id in tmp[1:]: \n","        val_data_user[u_id].add(int(i_id))\n","        val_data_item[int(i_id)].add(u_id)\n","np.save(os.path.join(path_save_base,'val_set.npy'),[val_data_user,val_data_item,num_u_i]) \n","print(num_u,num_u_i)\n","\n","\n","user_rating_set_all = defaultdict(set)\n","for u in range(num_u):\n","    train_tmp = set()\n","    test_tmp = set() \n","    val_tmp = set() \n","    if u in train_data_user:\n","        train_tmp = train_data_user[u]\n","    if u in test_data_user:\n","        test_tmp = test_data_user[u] \n","    if u in val_data_user:\n","        val_tmp = val_data_user[u] \n","    user_rating_set_all[u]=train_tmp|test_tmp|val_tmp\n","np.save(os.path.join(path_save_base,'user_rating_set_all.npy'),user_rating_set_all) "],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["29858 810128\n","29858 217242\n","29857 108621\n"]}]},{"cell_type":"markdown","metadata":{"id":"0iTyipe9wzfU"},"source":["## Dataset"]},{"cell_type":"code","metadata":{"id":"LBJTWOBOxph7"},"source":["class BPRData(data.Dataset):\n","    def __init__(self,train_dict=None,num_item=0, num_ng=1, is_training=None, data_set_count=0,all_rating=None):\n","        super(BPRData, self).__init__()\n","\n","        self.num_item = num_item\n","        self.train_dict = train_dict\n","        self.num_ng = num_ng\n","        self.is_training = is_training\n","        self.data_set_count = data_set_count\n","        self.all_rating=all_rating\n","        self.set_all_item=set(range(num_item))  \n","\n","    def ng_sample(self):\n","        # assert self.is_training, 'no need to sampling when testing'\n","        # print('ng_sample----is----call-----') \n","        self.features_fill = []\n","        for user_id in self.train_dict:\n","            positive_list=self.train_dict[user_id]#self.train_dict[user_id]\n","            all_positive_list=self.all_rating[user_id]\n","            #item_i: positive item ,,item_j:negative item   \n","            # temp_neg=list(self.set_all_item-all_positive_list)\n","            # random.shuffle(temp_neg)\n","            # count=0\n","            # for item_i in positive_list:\n","            #     for t in range(self.num_ng):   \n","            #         self.features_fill.append([user_id,item_i,temp_neg[count]])\n","            #         count+=1  \n","            for item_i in positive_list:   \n","                for t in range(self.num_ng):\n","                    item_j=np.random.randint(self.num_item)\n","                    while item_j in all_positive_list:\n","                        item_j=np.random.randint(self.num_item)\n","                    self.features_fill.append([user_id,item_i,item_j]) \n","      \n","    def __len__(self):  \n","        return self.num_ng*self.data_set_count#return self.num_ng*len(self.train_dict)\n","         \n","\n","    def __getitem__(self, idx):\n","        features = self.features_fill  \n","        \n","        user = features[idx][0]\n","        item_i = features[idx][1]\n","        item_j = features[idx][2] \n","        return user, item_i, item_j "],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class resData(data.Dataset):\n","    def __init__(self,train_dict=None,batch_size=0,num_item=0,all_pos=None):\n","        super(resData, self).__init__() \n","      \n","        self.train_dict = train_dict \n","        self.batch_size = batch_size\n","        self.all_pos_train=all_pos \n","\n","        self.features_fill = []\n","        for user_id in self.train_dict:\n","            self.features_fill.append(user_id)\n","        self.set_all=set(range(num_item))\n","   \n","    def __len__(self):  \n","        return math.ceil(len(self.train_dict)*1.0/self.batch_size)#self.data_set_count==batch_size\n","         \n","\n","    def __getitem__(self, idx): \n","        \n","        user_test=[]\n","        item_test=[]\n","        split_test=[]\n","        for i in range(self.batch_size):#self.data_set_count==batch_size \n","            index_my=self.batch_size*idx+i \n","            if index_my == len(self.train_dict):\n","                break   \n","            user = self.features_fill[index_my]\n","            item_i_list = list(self.train_dict[user])\n","            item_j_list = list(self.set_all-self.all_pos_train[user])\n","            # pdb.set_trace() \n","            u_i=[user]*(len(item_i_list)+len(item_j_list))\n","            user_test.extend(u_i)\n","            item_test.extend(item_i_list)\n","            item_test.extend(item_j_list)  \n","            split_test.append([(len(item_i_list)+len(item_j_list)),len(item_j_list)]) \n","           \n","        return torch.from_numpy(np.array(user_test)), torch.from_numpy(np.array(item_test)), split_test"],"metadata":{"id":"c3O3h1d8J6Yx"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"acpOF47Ix24z"},"source":["## Evaluate"]},{"cell_type":"code","metadata":{"id":"7p77Vhgxx9D9"},"source":["def metrics_loss(model, test_val_loader_loss, batch_size): \n","    start_time = time.time() \n","    loss_sum=[]\n","    loss_sum2=[]\n","    for user, item_i, item_j in test_val_loader_loss:\n","        user = user.cuda()\n","        item_i = item_i.cuda()\n","        item_j = item_j.cuda() \n","     \n","        prediction_i, prediction_j,loss,loss2 = model(user, item_i, item_j) \n","        loss_sum.append(loss.item())  \n","        loss_sum2.append(loss2.item())\n","\n","        # if np.isnan(loss2.item()).any():\n","        #     pdb.set_trace()\n","    # pdb.set_trace()\n","    elapsed_time = time.time() - start_time\n","    test_val_loss1=round(np.mean(loss_sum),4)\n","    test_val_loss=round(np.mean(loss_sum2),4)\n","    str_print_val_loss=' val loss:'+str(test_val_loss)\n","    # print(round(elapsed_time,3))\n","    # print(test_val_loss1,test_val_loss)\n","    return test_val_loss"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def hr_ndcg(indices_sort_top,index_end_i,top_k): \n","    hr_topK=0\n","    ndcg_topK=0\n","\n","    ndcg_max=[0]*top_k\n","    temp_max_ndcg=0\n","    for i_topK in range(top_k):\n","        temp_max_ndcg+=1.0/math.log(i_topK+2)\n","        ndcg_max[i_topK]=temp_max_ndcg\n","\n","    max_hr=top_k\n","    max_ndcg=ndcg_max[top_k-1]\n","    if index_end_i<top_k:\n","        max_hr=(index_end_i)*1.0\n","        max_ndcg=ndcg_max[index_end_i-1] \n","    count=0\n","    for item_id in indices_sort_top:\n","        if item_id < index_end_i:\n","            hr_topK+=1.0\n","            ndcg_topK+=1.0/math.log(count+2) \n","        count+=1\n","        if count==top_k:\n","            break\n","\n","    hr_t=hr_topK/max_hr\n","    ndcg_t=ndcg_topK/max_ndcg  \n","    # hr_t,ndcg_t,index_end_i,indices_sort_top\n","    # pdb.set_trace() \n","    return hr_t,ndcg_t"],"metadata":{"id":"QgxclVVvKBY0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def metrics(model, test_val_loader, top_k, num_negative_test_val, batch_size):\n","    HR, NDCG = [], [] \n","    test_loss_sum=[]\n","    # pdb.set_trace()  \n"," \n","    test_start_time = time.time()\n","    for user, item_i, item_j in test_val_loader:  \n","        # start_time = time.time()\n","        # pdb.set_trace()\n","        user = user.cuda()\n","        item_i = item_i.cuda()\n","        item_j = item_j #index to split\n","\n","        prediction_i, prediction_j,loss_test,loss2_test = model(user, item_i, torch.cuda.LongTensor([0])) \n","        test_loss_sum.append(loss2_test.item())  \n","        # pdb.set_trace()   \n","        elapsed_time = time.time() - test_start_time\n","        print('time:'+str(round(elapsed_time,2)))\n","        courrent_index=0\n","        courrent_user_index=0\n","        for len_i,len_j in item_j:\n","            index_end_i=(len_i-len_j).item()  \n","            #pre_error=(prediction_i[0][courrent_index:(courrent_index+index_end_i)]- prediction_i[0][(courrent_index+index_end_i):(courrent_index+index_end_j)])#.sum() \n","            #loss_test=nn.MSELoss((pre_error).sum())#-(prediction_i[0][courrent_index:(courrent_index+index_end_i)]- prediction_i[0][(courrent_index+index_end_i):(courrent_index+index_end_j)]).sigmoid().log()#.sum()   \n","            _, indices = torch.topk(prediction_i[0][courrent_index:(courrent_index+len_i)], top_k)   \n","            hr_t,ndcg_t=hr_ndcg(indices.tolist(),index_end_i,top_k)  \n","            # print(hr_t,ndcg_t,indices,index_end_i)\n","            # pdb.set_trace()\n","            HR.append(hr_t)\n","            NDCG.append(ndcg_t) \n","            courrent_index+=len_i \n","            courrent_user_index+=1 \n","    test_loss=round(np.mean(test_loss_sum[:-1]),4)  \n","    return test_loss,round(np.mean(HR),4) , round(np.mean(NDCG),4) "],"metadata":{"id":"SPI2Ho8DKDzY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["os.path.join(path_save_base,'/training_set.npy')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"UlSlchBlUAaI","executionInfo":{"status":"ok","timestamp":1639038587411,"user_tz":-330,"elapsed":550,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}},"outputId":"ccfccbfe-4b2d-4285-92f4-1f22cbe31ded"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'/training_set.npy'"]},"metadata":{},"execution_count":12}]},{"cell_type":"code","metadata":{"id":"ih-TuDTPyY6d"},"source":["training_user_set,training_item_set,training_set_count = np.load(os.path.join(path_save_base,'training_set.npy'),allow_pickle=True)\n","testing_user_set,testing_item_set,testing_set_count = np.load(os.path.join(path_save_base,'testing_set.npy'),allow_pickle=True)  \n","val_user_set,val_item_set,val_set_count = np.load(os.path.join(path_save_base,'val_set.npy'),allow_pickle=True)    \n","user_rating_set_all = np.load(os.path.join(path_save_base,'user_rating_set_all.npy'),allow_pickle=True).item()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fwB0JUa-yWRo"},"source":["def readD(set_matrix,num_):\n","    user_d=[] \n","    for i in range(num_):\n","        len_set=1.0/(len(set_matrix[i])+1)  \n","        user_d.append(len_set)\n","    return user_d"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["u_d=readD(training_user_set,user_num)\n","i_d=readD(training_item_set,item_num)\n","#1/(d_i+1)\n","d_i_train=u_d\n","d_j_train=i_d\n","#1/sqrt((d_i+1)(d_j+1)) \n","# d_i_j_train=np.sqrt(u_d*i_d) "],"metadata":{"id":"rFv-2iRkKs6V"},"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"iyUc-TfcySm-"},"source":["#user-item  to user-item matrix and item-user matrix\n","def readTrainSparseMatrix(set_matrix,is_user):\n","    user_items_matrix_i=[]\n","    user_items_matrix_v=[] \n","    if is_user:\n","        d_i=u_d\n","        d_j=i_d\n","    else:\n","        d_i=i_d\n","        d_j=u_d\n","    for i in set_matrix:\n","        len_set=len(set_matrix[i])  \n","        for j in set_matrix[i]:\n","            user_items_matrix_i.append([i,j])\n","            d_i_j=np.sqrt(d_i[i]*d_j[j])\n","            #1/sqrt((d_i+1)(d_j+1)) \n","            user_items_matrix_v.append(d_i_j)#(1./len_set) \n","    user_items_matrix_i=torch.cuda.LongTensor(user_items_matrix_i)\n","    user_items_matrix_v=torch.cuda.FloatTensor(user_items_matrix_v)\n","    return torch.sparse.FloatTensor(user_items_matrix_i.t(), user_items_matrix_v)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MSoboFoQyRHv"},"source":["sparse_u_i=readTrainSparseMatrix(training_user_set,True)\n","sparse_i_u=readTrainSparseMatrix(training_item_set,False)\n","#user-item  to user-item matrix and item-user matrix\n","# pdb.set_trace()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Model"],"metadata":{"id":"W3fawtyxMs3O"}},{"cell_type":"markdown","source":["![](https://github.com/RecoHut-Stanzas/S794944/raw/main/images/Overall_framework.jpg)"],"metadata":{"id":"7OFu8P4HU2Dy"}},{"cell_type":"code","metadata":{"id":"KpquhpXeyKp3"},"source":["class BPR(nn.Module):\n","    def __init__(self, user_num, item_num, factor_num,user_item_matrix,item_user_matrix,d_i_train,d_j_train):\n","        super(BPR, self).__init__()\n","        \"\"\"\n","        user_num: number of users;\n","        item_num: number of items;\n","        factor_num: number of predictive factors.\n","        \"\"\"     \n","        self.user_item_matrix = user_item_matrix\n","        self.item_user_matrix = item_user_matrix\n","        self.embed_user = nn.Embedding(user_num, factor_num)\n","        self.embed_item = nn.Embedding(item_num, factor_num) \n","\n","        for i in range(len(d_i_train)):\n","            d_i_train[i]=[d_i_train[i]]\n","        for i in range(len(d_j_train)):\n","            d_j_train[i]=[d_j_train[i]]\n","\n","        self.d_i_train=torch.cuda.FloatTensor(d_i_train)\n","        self.d_j_train=torch.cuda.FloatTensor(d_j_train)\n","        self.d_i_train=self.d_i_train.expand(-1,factor_num)\n","        self.d_j_train=self.d_j_train.expand(-1,factor_num)\n","\n","        nn.init.normal_(self.embed_user.weight, std=0.01)\n","        nn.init.normal_(self.embed_item.weight, std=0.01)  \n","\n","    def forward(self, user, item_i, item_j):    \n","\n","        users_embedding=self.embed_user.weight\n","        items_embedding=self.embed_item.weight  \n","\n","        gcn1_users_embedding = (torch.sparse.mm(self.user_item_matrix, items_embedding) + users_embedding.mul(self.d_i_train))#*2. #+ users_embedding\n","        gcn1_items_embedding = (torch.sparse.mm(self.item_user_matrix, users_embedding) + items_embedding.mul(self.d_j_train))#*2. #+ items_embedding\n","   \n","        gcn2_users_embedding = (torch.sparse.mm(self.user_item_matrix, gcn1_items_embedding) + gcn1_users_embedding.mul(self.d_i_train))#*2. + users_embedding\n","        gcn2_items_embedding = (torch.sparse.mm(self.item_user_matrix, gcn1_users_embedding) + gcn1_items_embedding.mul(self.d_j_train))#*2. + items_embedding\n","          \n","        gcn3_users_embedding = (torch.sparse.mm(self.user_item_matrix, gcn2_items_embedding) + gcn2_users_embedding.mul(self.d_i_train))#*2. + gcn1_users_embedding\n","        gcn3_items_embedding = (torch.sparse.mm(self.item_user_matrix, gcn2_users_embedding) + gcn2_items_embedding.mul(self.d_j_train))#*2. + gcn1_items_embedding\n","        \n","        # gcn4_users_embedding = (torch.sparse.mm(self.user_item_matrix, gcn3_items_embedding) + gcn3_users_embedding.mul(self.d_i_train))#*2. + gcn1_users_embedding\n","        # gcn4_items_embedding = (torch.sparse.mm(self.item_user_matrix, gcn3_users_embedding) + gcn3_items_embedding.mul(self.d_j_train))#*2. + gcn1_items_embedding\n","        \n","        gcn_users_embedding= torch.cat((users_embedding,gcn1_users_embedding,gcn2_users_embedding,gcn3_users_embedding),-1)#+gcn4_users_embedding\n","        gcn_items_embedding= torch.cat((items_embedding,gcn1_items_embedding,gcn2_items_embedding,gcn3_items_embedding),-1)#+gcn4_items_embedding#\n","      \n","        \n","        user = F.embedding(user,gcn_users_embedding)\n","        item_i = F.embedding(item_i,gcn_items_embedding)\n","        item_j = F.embedding(item_j,gcn_items_embedding)  \n","        # # pdb.set_trace() \n","        prediction_i = (user * item_i).sum(dim=-1)\n","        prediction_j = (user * item_j).sum(dim=-1) \n","        # loss=-((rediction_i-prediction_j).sigmoid())**2#self.loss(prediction_i,prediction_j)#.sum()\n","        l2_regulization = 0.01*(user**2+item_i**2+item_j**2).sum(dim=-1)\n","        # l2_regulization = 0.01*((gcn1_users_embedding**2).sum(dim=-1).mean()+(gcn1_items_embedding**2).sum(dim=-1).mean())\n","      \n","        loss2= -((prediction_i - prediction_j).sigmoid().log().mean())\n","        # loss= loss2 + l2_regulization\n","        loss= -((prediction_i - prediction_j)).sigmoid().log().mean() +l2_regulization.mean()\n","        # pdb.set_trace()\n","        return prediction_i, prediction_j,loss,loss2"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wAwTA05SyNpP"},"source":["train_dataset = BPRData(\n","        train_dict=training_user_set, num_item=item_num, num_ng=5, is_training=True,\\\n","        data_set_count=training_set_count,all_rating=user_rating_set_all)\n","train_loader = DataLoader(train_dataset,\n","        batch_size=batch_size, shuffle=True, num_workers=2)\n","  \n","testing_dataset_loss = BPRData(\n","        train_dict=testing_user_set, num_item=item_num, num_ng=5, is_training=True,\\\n","        data_set_count=testing_set_count,all_rating=user_rating_set_all)\n","testing_loader_loss = DataLoader(testing_dataset_loss,\n","        batch_size=batch_size, shuffle=False, num_workers=0)\n","\n","val_dataset_loss = BPRData(\n","        train_dict=val_user_set, num_item=item_num, num_ng=5, is_training=True,\\\n","        data_set_count=val_set_count,all_rating=user_rating_set_all)\n","val_loader_loss = DataLoader(val_dataset_loss,\n","        batch_size=batch_size, shuffle=False, num_workers=0)\n","   \n","   \n","model = BPR(user_num, item_num, factor_num,sparse_u_i,sparse_i_u,d_i_train,d_j_train)\n","model=model.to('cuda') \n","\n","optimizer_bpr = torch.optim.Adam(model.parameters(), lr=0.005)#, betas=(0.5, 0.99))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Training"],"metadata":{"id":"SO9SZNWfMuh5"}},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"D9ksZjDmyMBg","executionInfo":{"status":"ok","timestamp":1639038912797,"user_tz":-330,"elapsed":237974,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}},"outputId":"7cbe674b-26d3-407f-ed5a-890bf00428c9"},"source":["########################### TRAINING #####################################\n"," \n","# testing_loader_loss.dataset.ng_sample() \n","\n","print('--------training processing-------')\n","count, best_hr = 0, 0\n","for epoch in range(5):\n","    model.train() \n","    start_time = time.time()\n","    train_loader.dataset.ng_sample()\n","    # pdb.set_trace()\n","    print('train data of ng_sample is  end')\n","    # elapsed_time = time.time() - start_time\n","    # print(' time:'+str(round(elapsed_time,1)))\n","    # start_time = time.time()\n","    \n","    train_loss_sum=[]\n","    train_loss_sum2=[]\n","    for user, item_i, item_j in train_loader:\n","        user = user.cuda()\n","        item_i = item_i.cuda()\n","        item_j = item_j.cuda() \n","\n","        model.zero_grad()\n","        prediction_i, prediction_j,loss,loss2 = model(user, item_i, item_j) \n","        loss.backward()\n","        optimizer_bpr.step() \n","        count += 1  \n","        train_loss_sum.append(loss.item())  \n","        train_loss_sum2.append(loss2.item())  \n","        # print(count)\n","\n","    elapsed_time = time.time() - start_time\n","    train_loss=round(np.mean(train_loss_sum[:-1]),4)\n","    train_loss2=round(np.mean(train_loss_sum2[:-1]),4)\n","    str_print_train=\"epoch:\"+str(epoch)+' time:'+str(round(elapsed_time,1))+'\\t train loss:'+str(train_loss)+\"=\"+str(train_loss2)+\"+\" \n","    print('--train--',elapsed_time)\n","\n","    PATH_model=path_save_model_base+'/epoch'+str(epoch)+'.pt'\n","    torch.save(model.state_dict(), PATH_model)\n","    \n","    model.eval()   \n","    # ######test and val###########   \n","    val_loader_loss.dataset.ng_sample() \n","    val_loss=metrics_loss(model,val_loader_loss,batch_size)  \n","    # str_print_train+=' val loss:'+str(val_loss)\n","\n","    testing_loader_loss.dataset.ng_sample() \n","    test_loss=metrics_loss(model,testing_loader_loss,batch_size) \n","    print(str_print_train+' val loss:'+str(val_loss)+' test loss:'+str(test_loss)) \n","    result_file.write(str_print_train+' val loss:'+str(val_loss)+' test loss:'+str(test_loss)) \n","    result_file.write('\\n') \n","    result_file.flush() "],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["--------training processing-------\n","train data of ng_sample is  end\n","--train-- 36.8933470249176\n","epoch:0 time:36.9\t train loss:0.6929=0.6927+ val loss:0.6856 test loss:0.6863\n","train data of ng_sample is  end\n","--train-- 35.34687113761902\n","epoch:1 time:35.3\t train loss:0.6765=0.6749+ val loss:0.6315 test loss:0.6367\n","train data of ng_sample is  end\n","--train-- 33.52420949935913\n","epoch:2 time:33.5\t train loss:0.6113=0.6035+ val loss:0.5238 test loss:0.5349\n","train data of ng_sample is  end\n","--train-- 35.61563539505005\n","epoch:3 time:35.6\t train loss:0.5038=0.4826+ val loss:0.4007 test loss:0.4129\n","train data of ng_sample is  end\n","--train-- 34.377206325531006\n","epoch:4 time:34.4\t train loss:0.399=0.3562+ val loss:0.3048 test loss:0.3141\n"]}]},{"cell_type":"markdown","metadata":{"id":"jxYefqWOzUSX"},"source":["## Testing"]},{"cell_type":"code","source":["def readD(set_matrix,num_):\n","    user_d=[] \n","    for i in range(num_):\n","        len_set=1.0/(len(set_matrix[i])+1)  \n","        user_d.append(len_set)\n","    return user_d\n","u_d=readD(training_user_set,user_num)\n","i_d=readD(training_item_set,item_num)\n","d_i_train=u_d\n","d_j_train=i_d"],"metadata":{"id":"kWWePLOTVxSg"},"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ONukrcRw2ngS"},"source":["#user-item  to user-item matrix and item-user matrix\n","def readTrainSparseMatrix(set_matrix,is_user):\n","    user_items_matrix_i=[]\n","    user_items_matrix_v=[] \n","    if is_user:\n","        d_i=u_d\n","        d_j=i_d\n","    else:\n","        d_i=i_d\n","        d_j=u_d\n","    for i in set_matrix:\n","        len_set=len(set_matrix[i])  \n","        for j in set_matrix[i]:\n","            user_items_matrix_i.append([i,j])\n","            d_i_j=np.sqrt(d_i[i]*d_j[j])\n","            #1/sqrt((d_i+1)(d_j+1)) \n","            user_items_matrix_v.append(d_i_j)#(1./len_set) \n","    user_items_matrix_i=torch.cuda.LongTensor(user_items_matrix_i)\n","    user_items_matrix_v=torch.cuda.FloatTensor(user_items_matrix_v)\n","    return torch.sparse.FloatTensor(user_items_matrix_i.t(), user_items_matrix_v)\n","\n","sparse_u_i=readTrainSparseMatrix(training_user_set,True)\n","sparse_i_u=readTrainSparseMatrix(training_item_set,False)\n","\n","#user-item  to user-item matrix and item-user matrix\n","# pdb.set_trace()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"37odx_9d2xCS"},"source":["class BPR(nn.Module):\n","    def __init__(self, user_num, item_num, factor_num,user_item_matrix,item_user_matrix,d_i_train,d_j_train):\n","        super(BPR, self).__init__()\n","        \"\"\"\n","        user_num: number of users;\n","        item_num: number of items;\n","        factor_num: number of predictive factors.\n","        \"\"\"     \n","        self.user_item_matrix = user_item_matrix\n","        self.item_user_matrix = item_user_matrix\n","        self.embed_user = nn.Embedding(user_num, factor_num)\n","        self.embed_item = nn.Embedding(item_num, factor_num) \n","\n","        for i in range(len(d_i_train)):\n","            d_i_train[i]=[d_i_train[i]]\n","        for i in range(len(d_j_train)):\n","            d_j_train[i]=[d_j_train[i]]\n","\n","        self.d_i_train=torch.cuda.FloatTensor(d_i_train)\n","        self.d_j_train=torch.cuda.FloatTensor(d_j_train)\n","        self.d_i_train=self.d_i_train.expand(-1,factor_num)\n","        self.d_j_train=self.d_j_train.expand(-1,factor_num)\n","\n","        nn.init.normal_(self.embed_user.weight, std=0.01)\n","        nn.init.normal_(self.embed_item.weight, std=0.01)  \n","\n","    def forward(self, user, item_i, item_j):    \n","\n","        users_embedding=self.embed_user.weight\n","        items_embedding=self.embed_item.weight  \n","\n","        gcn1_users_embedding = (torch.sparse.mm(self.user_item_matrix, items_embedding) + users_embedding.mul(self.d_i_train))#*2. #+ users_embedding\n","        gcn1_items_embedding = (torch.sparse.mm(self.item_user_matrix, users_embedding) + items_embedding.mul(self.d_j_train))#*2. #+ items_embedding\n","   \n","        gcn2_users_embedding = (torch.sparse.mm(self.user_item_matrix, gcn1_items_embedding) + gcn1_users_embedding.mul(self.d_i_train))#*2. + users_embedding\n","        gcn2_items_embedding = (torch.sparse.mm(self.item_user_matrix, gcn1_users_embedding) + gcn1_items_embedding.mul(self.d_j_train))#*2. + items_embedding\n","          \n","        gcn3_users_embedding = (torch.sparse.mm(self.user_item_matrix, gcn2_items_embedding) + gcn2_users_embedding.mul(self.d_i_train))#*2. + gcn1_users_embedding\n","        gcn3_items_embedding = (torch.sparse.mm(self.item_user_matrix, gcn2_users_embedding) + gcn2_items_embedding.mul(self.d_j_train))#*2. + gcn1_items_embedding\n","       \n","        gcn_users_embedding= torch.cat((users_embedding,gcn1_users_embedding,gcn2_users_embedding,gcn3_users_embedding),-1)#+gcn4_users_embedding\n","        gcn_items_embedding= torch.cat((items_embedding,gcn1_items_embedding,gcn2_items_embedding,gcn3_items_embedding),-1)#+gcn4_items_embedding#\n","        \n","        \n","        g0_mean=torch.mean(users_embedding)\n","        g0_var=torch.var(users_embedding)\n","        g1_mean=torch.mean(gcn1_users_embedding)\n","        g1_var=torch.var(gcn1_users_embedding) \n","        g2_mean=torch.mean(gcn2_users_embedding)\n","        g2_var=torch.var(gcn2_users_embedding)\n","        g3_mean=torch.mean(gcn3_users_embedding)\n","        g3_var=torch.var(gcn3_users_embedding)\n","        # g4_mean=torch.mean(gcn4_users_embedding)\n","        # g4_var=torch.var(gcn4_users_embedding)\n","        # g5_mean=torch.mean(gcn5_users_embedding)\n","        # g5_var=torch.var(gcn5_users_embedding)\n","        # g6_mean=torch.mean(gcn6_users_embedding)\n","        # g6_var=torch.var(gcn6_users_embedding)\n","        g_mean=torch.mean(gcn_users_embedding)\n","        g_var=torch.var(gcn_users_embedding)\n","\n","        i0_mean=torch.mean(items_embedding)\n","        i0_var=torch.var(items_embedding)\n","        i1_mean=torch.mean(gcn1_items_embedding)\n","        i1_var=torch.var(gcn1_items_embedding)\n","        i2_mean=torch.mean(gcn2_items_embedding)\n","        i2_var=torch.var(gcn2_items_embedding)\n","        i3_mean=torch.mean(gcn3_items_embedding)\n","        i3_var=torch.var(gcn3_items_embedding)\n","        # i4_mean=torch.mean(gcn4_items_embedding)\n","        # i4_var=torch.var(gcn4_items_embedding) \n","        # i5_mean=torch.mean(gcn5_items_embedding)\n","        # i5_var=torch.var(gcn5_items_embedding)\n","        # i6_mean=torch.mean(gcn6_items_embedding)\n","        # i6_var=torch.var(gcn6_items_embedding)\n","        i_mean=torch.mean(gcn_items_embedding)\n","        i_var=torch.var(gcn_items_embedding)\n","\n","        # pdb.set_trace() \n","\n","        str_user=str(round(g0_mean.item(),7))+' '\n","        str_user+=str(round(g0_var.item(),7))+' '\n","        str_user+=str(round(g1_mean.item(),7))+' '\n","        str_user+=str(round(g1_var.item(),7))+' '\n","        str_user+=str(round(g2_mean.item(),7))+' '\n","        str_user+=str(round(g2_var.item(),7))+' '\n","        str_user+=str(round(g3_mean.item(),7))+' '\n","        str_user+=str(round(g3_var.item(),7))+' '\n","        # str_user+=str(round(g4_mean.item(),7))+' '\n","        # str_user+=str(round(g4_var.item(),7))+' '\n","        # str_user+=str(round(g5_mean.item(),7))+' '\n","        # str_user+=str(round(g5_var.item(),7))+' '\n","        # str_user+=str(round(g6_mean.item(),7))+' '\n","        # str_user+=str(round(g6_var.item(),7))+' '\n","        str_user+=str(round(g_mean.item(),7))+' '\n","        str_user+=str(round(g_var.item(),7))+' '\n","\n","        str_item=str(round(i0_mean.item(),7))+' '\n","        str_item+=str(round(i0_var.item(),7))+' '\n","        str_item+=str(round(i1_mean.item(),7))+' '\n","        str_item+=str(round(i1_var.item(),7))+' '\n","        str_item+=str(round(i2_mean.item(),7))+' '\n","        str_item+=str(round(i2_var.item(),7))+' '\n","        str_item+=str(round(i3_mean.item(),7))+' '\n","        str_item+=str(round(i3_var.item(),7))+' '\n","        # str_item+=str(round(i4_mean.item(),7))+' '\n","        # str_item+=str(round(i4_var.item(),7))+' '\n","        # str_item+=str(round(i5_mean.item(),7))+' '\n","        # str_item+=str(round(i5_var.item(),7))+' '\n","        # str_item+=str(round(i6_mean.item(),7))+' '\n","        # str_item+=str(round(i6_var.item(),7))+' '\n","        str_item+=str(round(i_mean.item(),7))+' '\n","        str_item+=str(round(i_var.item(),7))+' '\n","        print(str_user)\n","        print(str_item)\n","        return gcn_users_embedding, gcn_items_embedding,str_user,str_item \n","\n","test_batch=52#int(batch_size/32) \n","testing_dataset = resData(train_dict=testing_user_set, batch_size=test_batch,num_item=item_num,all_pos=training_user_set)\n","testing_loader = DataLoader(testing_dataset,batch_size=1, shuffle=False, num_workers=0) \n"," \n","model = BPR(user_num, item_num, factor_num,sparse_u_i,sparse_i_u,d_i_train,d_j_train)\n","model=model.to('cuda')\n","   \n","optimizer_bpr = torch.optim.Adam(model.parameters(), lr=0.001)#, betas=(0.5, 0.99))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LbN03HqY2scu","executionInfo":{"status":"ok","timestamp":1639039232397,"user_tz":-330,"elapsed":164076,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}},"outputId":"c20f2291-2639-45ea-a4a9-ad4f148f724a"},"source":["########################### TESTING ##################################### \n","# testing_loader_loss.dataset.ng_sample() \n","\n","def largest_indices(ary, n):\n","    \"\"\"Returns the n largest indices from a numpy array.\"\"\"\n","    flat = ary.flatten()\n","    indices = np.argpartition(flat, -n)[-n:]\n","    indices = indices[np.argsort(-flat[indices])]\n","    return np.unravel_index(indices, ary.shape)\n","\n","print('--------test processing-------')\n","count, best_hr = 0, 0\n","for epoch in range(start_i_test,end_i_test,setp):\n","    model.train()   \n","\n","    PATH_model=path_save_model_base+'/epoch'+str(epoch)+'.pt'\n","    #torch.save(model.state_dict(), PATH_model) \n","    model.load_state_dict(torch.load(PATH_model)) \n","    model.eval()     \n","    # ######test and val###########    \n","    gcn_users_embedding, gcn_items_embedding,gcn_user_emb,gcn_item_emb= model(torch.cuda.LongTensor([0]), torch.cuda.LongTensor([0]), torch.cuda.LongTensor([0])) \n","    user_e=gcn_users_embedding.cpu().detach().numpy()\n","    item_e=gcn_items_embedding.cpu().detach().numpy()\n","    all_pre=np.matmul(user_e,item_e.T) \n","    HR, NDCG = [], [] \n","    set_all=set(range(item_num))  \n","    #spend 461s \n","    test_start_time = time.time()\n","    for u_i in testing_user_set: \n","        item_i_list = list(testing_user_set[u_i])\n","        index_end_i=len(item_i_list)\n","        item_j_list = list(set_all-training_user_set[u_i]-testing_user_set[u_i])\n","        item_i_list.extend(item_j_list) \n","\n","        pre_one=all_pre[u_i][item_i_list] \n","        indices=largest_indices(pre_one, top_k)\n","        indices=list(indices[0])   \n","\n","        hr_t,ndcg_t=hr_ndcg(indices,index_end_i,top_k) \n","        elapsed_time = time.time() - test_start_time \n","        HR.append(hr_t)\n","        NDCG.append(ndcg_t)    \n","    hr_test=round(np.mean(HR),4)\n","    ndcg_test=round(np.mean(NDCG),4)    \n","        \n","    # test_loss,hr_test,ndcg_test = metrics(model,testing_loader,top_k,num_negative_test_val,batch_size)  \n","    str_print_evl=\"epoch:\"+str(epoch)+'time:'+str(round(elapsed_time,2))+\"\\t test\"+\" hit:\"+str(hr_test)+' ndcg:'+str(ndcg_test) \n","    print(str_print_evl)   \n","    result_file.write(gcn_user_emb)\n","    result_file.write('\\n')\n","    result_file.write(gcn_item_emb)\n","    result_file.write('\\n')  \n","\n","    result_file.write(str_print_evl)\n","    result_file.write('\\n')\n","    result_file.flush()"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["--------test processing-------\n","0.0005065 0.0039031 0.0008974 0.0035089 0.0004212 0.0024023 0.0008231 0.0026235 0.000662 0.0031095 \n","0.0012799 0.003691 0.0006682 0.0020425 0.0010818 0.0021988 0.0006408 0.0016047 0.0009177 0.0023843 \n","epoch:3time:151.82\t test hit:0.1021 ndcg:0.0841\n"]}]},{"cell_type":"markdown","metadata":{"id":"TLHw8weh7OQB"},"source":["---"]},{"cell_type":"code","metadata":{"id":"ZYGZlyeY7OQD"},"source":["!apt-get -qq install tree\n","!rm -r sample_data"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oVruV5yt7OQD","executionInfo":{"status":"ok","timestamp":1638797225389,"user_tz":-330,"elapsed":24,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}},"outputId":"8cc165b8-6167-4329-c3f1-5cd2ae01d647"},"source":["!tree -h --du ."],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":[".\n","├── [ 15M]  datanpy\n","│   ├── [2.9M]  testing_set.npy\n","│   ├── [6.3M]  training_set.npy\n","│   ├── [3.6M]  user_rating_set_all.npy\n","│   └── [2.1M]  val_set.npy\n","├── [7.3M]  gowalla\n","│   ├── [495K]  item_list.txt\n","│   ├── [1.0K]  README.md\n","│   ├── [1.3M]  test.txt\n","│   ├── [4.4M]  train.txt\n","│   ├── [343K]  user_list.txt\n","│   └── [752K]  val.txt\n","├── [ 12K]  log\n","│   └── [8.2K]  gowalla\n","│       └── [4.2K]  newlosss0\n","│           ├── [   0]  results_hdcg_hr.txt\n","│           └── [ 245]  results.txt\n","└── [ 86M]  newlossModel\n","    └── [ 86M]  gowalla\n","        └── [ 86M]  ss0\n","            ├── [ 17M]  epoch0.pt\n","            ├── [ 17M]  epoch1.pt\n","            ├── [ 17M]  epoch2.pt\n","            ├── [ 17M]  epoch3.pt\n","            └── [ 17M]  epoch4.pt\n","\n"," 109M used in 8 directories, 17 files\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MrbNlTOW7OQE","executionInfo":{"status":"ok","timestamp":1638797236830,"user_tz":-330,"elapsed":3692,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}},"outputId":"b4f06826-9df0-476d-ecb4-ff603183437b"},"source":["!pip install -q watermark\n","%reload_ext watermark\n","%watermark -a \"Sparsh A.\" -m -iv -u -t -d"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Author: Sparsh A.\n","\n","Last updated: 2021-12-06 13:27:22\n","\n","Compiler    : GCC 7.5.0\n","OS          : Linux\n","Release     : 5.4.104+\n","Machine     : x86_64\n","Processor   : x86_64\n","CPU cores   : 2\n","Architecture: 64bit\n","\n","pandas     : 1.1.5\n","IPython    : 5.5.0\n","scipy      : 1.4.1\n","numpy      : 1.19.5\n","torchvision: 0.11.1+cu111\n","sys        : 3.7.12 (default, Sep 10 2021, 00:21:48) \n","[GCC 7.5.0]\n","argparse   : 1.1\n","torch      : 1.10.0+cu111\n","\n"]}]},{"cell_type":"code","metadata":{"id":"jzgrKrkC7Y-N","executionInfo":{"status":"ok","timestamp":1638797255976,"user_tz":-330,"elapsed":448,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}},"outputId":"672fccb0-588b-49ca-8a77-5ab549796055","colab":{"base_uri":"https://localhost:8080/"}},"source":["!nvidia-smi"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mon Dec  6 13:27:41 2021       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 495.44       Driver Version: 460.32.03    CUDA Version: 11.2     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  Tesla K80           Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   73C    P0    74W / 149W |    761MiB / 11441MiB |      0%      Default |\n","|                               |                      |                  N/A |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","|  No running processes found                                                 |\n","+-----------------------------------------------------------------------------+\n"]}]},{"cell_type":"markdown","metadata":{"id":"9Kw61_pe7OQE"},"source":["---"]},{"cell_type":"markdown","metadata":{"id":"OCasCymq7OQG"},"source":["**END**"]}]}