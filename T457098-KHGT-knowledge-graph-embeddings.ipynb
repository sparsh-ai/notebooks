{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"T457098 | KHGT knowledge graph embeddings","provenance":[],"collapsed_sections":[],"toc_visible":true,"authorship_tag":"ABX9TyOPvo/wZKeKqB2MtARQ/HVT"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"PN6o0n4FLmgR"},"source":["# KHGT knowledge graph embeddings"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YrCCIZJL0RHf","executionInfo":{"status":"ok","timestamp":1634137431404,"user_tz":-330,"elapsed":17526,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}},"outputId":"858d4b73-626e-41e2-82dd-0b55ae2690bf"},"source":["!git clone https://github.com/akaxlh/KHGT.git\n","!cp -r KHGT/Datasets ."],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Cloning into 'KHGT'...\n","remote: Enumerating objects: 103, done.\u001b[K\n","remote: Counting objects: 100% (103/103), done.\u001b[K\n","remote: Compressing objects: 100% (87/87), done.\u001b[K\n","remote: Total 103 (delta 22), reused 0 (delta 0), pack-reused 0\u001b[K\n","Receiving objects: 100% (103/103), 95.64 MiB | 11.47 MiB/s, done.\n","Resolving deltas: 100% (22/22), done.\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Hfxn2KIl0VHV","executionInfo":{"status":"ok","timestamp":1634137438694,"user_tz":-330,"elapsed":7321,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}},"outputId":"94150b40-d43e-4b02-bd9e-b839458b14ed"},"source":["!apt-get install tree"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Reading package lists... Done\n","Building dependency tree       \n","Reading state information... Done\n","The following NEW packages will be installed:\n","  tree\n","0 upgraded, 1 newly installed, 0 to remove and 37 not upgraded.\n","Need to get 40.7 kB of archives.\n","After this operation, 105 kB of additional disk space will be used.\n","Get:1 http://archive.ubuntu.com/ubuntu bionic/universe amd64 tree amd64 1.7.0-5 [40.7 kB]\n","Fetched 40.7 kB in 0s (102 kB/s)\n","Selecting previously unselected package tree.\n","(Reading database ... 155047 files and directories currently installed.)\n","Preparing to unpack .../tree_1.7.0-5_amd64.deb ...\n","Unpacking tree (1.7.0-5) ...\n","Setting up tree (1.7.0-5) ...\n","Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n"]}]},{"cell_type":"markdown","metadata":{"id":"MwvRbg_N594z"},"source":["### Data"]},{"cell_type":"code","metadata":{"id":"gYHG2lwu5_dO"},"source":["!cd Datasets/retail && unrar x -e trn_pv.part01.rar"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XWWTd3-F0bI6","executionInfo":{"status":"ok","timestamp":1634137440493,"user_tz":-330,"elapsed":60,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}},"outputId":"3836699b-cec3-4df0-d771-6bc33b7bccb0"},"source":["!tree --du -h ./Datasets"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["./Datasets\n","├── [ 44M]  MultiInt-ML10M\n","│   ├── [4.6M]  iiMats\n","│   ├── [ 41K]  trn_catDict\n","│   ├── [ 11M]  trn_neg\n","│   ├── [ 12M]  trn_neutral.rar\n","│   ├── [ 16M]  trn_pos.rar\n","│   └── [ 84K]  tst_int\n","├── [ 93M]  retail\n","│   ├── [5.7M]  iiMats\n","│   ├── [ 98K]  itmCat\n","│   ├── [5.5M]  trn_buy\n","│   ├── [5.7M]  trn_cart\n","│   ├── [591K]  trn_catDict\n","│   ├── [2.6M]  trn_fav\n","│   ├── [ 47M]  trn_pv\n","│   ├── [5.0M]  trn_pv.part01.rar\n","│   ├── [5.0M]  trn_pv.part02.rar\n","│   ├── [5.0M]  trn_pv.part03.rar\n","│   ├── [5.0M]  trn_pv.part04.rar\n","│   ├── [5.0M]  trn_pv.part05.rar\n","│   ├── [411K]  trn_pv.part06.rar\n","│   └── [168K]  tst_int\n","└── [ 23M]  Yelp\n","    ├── [5.7M]  iiMats\n","    ├── [428K]  trn_catDict\n","    ├── [2.3M]  trn_neg\n","    ├── [2.8M]  trn_neutral\n","    ├── [7.8M]  trn_pos\n","    ├── [3.3M]  trn_tip\n","    └── [273K]  tst_int\n","\n"," 160M used in 3 directories, 27 files\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"So1K062h1lce","executionInfo":{"status":"ok","timestamp":1634137440497,"user_tz":-330,"elapsed":49,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}},"outputId":"917f04b9-7711-4f6c-e16a-092cdcc50e2d"},"source":["%tensorflow_version 1.x"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["TensorFlow 1.x selected.\n"]}]},{"cell_type":"markdown","metadata":{"id":"SgLNJEgF14Dd"},"source":["### CLI Run"]},{"cell_type":"code","metadata":{"id":"fLU7xmNX1XHy"},"source":["# # To run KHGT on Yelp data\n","# ! python labcode_yelp.py\n","\n","# # For MovieLens data, use the following command to train\n","# ! python labcode_ml10m.py --data ml10m --graphSampleN 1000 --save_path XXX\n","\n","# # test with larger sampled sub-graphs\n","# ! python labcode_ml10m.py --data ml10m --graphSampleN 5000 --epoch 0 --load_model XXX\n","\n","# # For Online Retail data, use this command to train\n","# ! python labcode_retail.py --data retail --graphSampleN 15000 --reg 1e-1 --save_path XXX\n","\n","# # test it with larger sampled sub-graphs\n","# ! python labcode_retail.py --data retail --graphSampleN 30000 --epoch 0 --load_model XXX"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"cZs_LlNf2E-M"},"source":["### NN Layers"]},{"cell_type":"code","metadata":{"id":"jXrWU2Sg2E7u"},"source":["import tensorflow as tf\n","from tensorflow.contrib.layers import xavier_initializer\n","import numpy as np\n","\n","paramId = 0\n","biasDefault = False\n","params = {}\n","regParams = {}\n","ita = 0.2\n","leaky = 0.1\n","\n","def getParamId():\n","\tglobal paramId\n","\tparamId += 1\n","\treturn paramId\n","\n","def setIta(ITA):\n","\tita = ITA\n","\n","def setBiasDefault(val):\n","\tglobal biasDefault\n","\tbiasDefault = val\n","\n","def getParam(name):\n","\treturn params[name]\n","\n","def addReg(name, param):\n","\tglobal regParams\n","\tif name not in regParams:\n","\t\tregParams[name] = param\n","\t# else:\n","\t# \tprint('ERROR: Parameter already exists')\n","\n","def addParam(name, param):\n","\tglobal params\n","\tif name not in params:\n","\t\tparams[name] = param\n","\n","def defineRandomNameParam(shape, dtype=tf.float32, reg=False, initializer='xavier', trainable=True):\n","\tname = 'defaultParamName%d'%getParamId()\n","\treturn defineParam(name, shape, dtype, reg, initializer, trainable)\n","\n","def defineParam(name, shape, dtype=tf.float32, reg=False, initializer='xavier', trainable=True):\n","\tglobal params\n","\tglobal regParams\n","\tassert name not in params, 'name %s already exists' % name\n","\tif initializer == 'xavier':\n","\t\tret = tf.get_variable(name=name, dtype=dtype, shape=shape,\n","\t\t\tinitializer=xavier_initializer(dtype=tf.float32),\n","\t\t\ttrainable=trainable)\n","\telif initializer == 'trunc_normal':\n","\t\tret = tf.get_variable(name=name, initializer=tf.random.truncated_normal(shape=[int(shape[0]), shape[1]], mean=0.0, stddev=0.03, dtype=dtype))\n","\telif initializer == 'zeros':\n","\t\tret = tf.get_variable(name=name, dtype=dtype,\n","\t\t\tinitializer=tf.zeros(shape=shape, dtype=tf.float32),\n","\t\t\ttrainable=trainable)\n","\telif initializer == 'ones':\n","\t\tret = tf.get_variable(name=name, dtype=dtype, initializer=tf.ones(shape=shape, dtype=tf.float32), trainable=trainable)\n","\telif not isinstance(initializer, str):\n","\t\tret = tf.get_variable(name=name, dtype=dtype,\n","\t\t\tinitializer=initializer, trainable=trainable)\n","\telse:\n","\t\tprint('ERROR: Unrecognized initializer')\n","\t\texit()\n","\tparams[name] = ret\n","\tif reg:\n","\t\tregParams[name] = ret\n","\treturn ret\n","\n","def getOrDefineParam(name, shape, dtype=tf.float32, reg=False, initializer='xavier', trainable=True, reuse=False):\n","\tglobal params\n","\tglobal regParams\n","\tif name in params:\n","\t\tassert reuse, 'Reusing Param %s Not Specified' % name\n","\t\tif reg and name not in regParams:\n","\t\t\tregParams[name] = params[name]\n","\t\treturn params[name]\n","\treturn defineParam(name, shape, dtype, reg, initializer, trainable)\n","\n","def BN(inp, name=None):\n","\tglobal ita\n","\tdim = inp.get_shape()[1]\n","\tname = 'defaultParamName%d'%getParamId()\n","\tscale = tf.Variable(tf.ones([dim]))\n","\tshift = tf.Variable(tf.zeros([dim]))\n","\tfcMean, fcVar = tf.nn.moments(inp, axes=[0])\n","\tema = tf.train.ExponentialMovingAverage(decay=0.5)\n","\temaApplyOp = ema.apply([fcMean, fcVar])\n","\twith tf.control_dependencies([emaApplyOp]):\n","\t\tmean = tf.identity(fcMean)\n","\t\tvar = tf.identity(fcVar)\n","\tret = tf.nn.batch_normalization(inp, mean, var, shift,\n","\t\tscale, 1e-8)\n","\treturn ret\n","\n","def FC(inp, outDim, name=None, useBias=False, activation=None, reg=False, useBN=False, dropout=None, initializer='xavier', reuse=False):\n","\tglobal params\n","\tglobal regParams\n","\tglobal leaky\n","\tinDim = inp.get_shape()[1]\n","\ttemName = name if name!=None else 'defaultParamName%d'%getParamId()\n","\tW = getOrDefineParam(temName, [inDim, outDim], reg=reg, initializer=initializer, reuse=reuse)\n","\tif dropout != None:\n","\t\tret = tf.nn.dropout(inp, rate=dropout) @ W\n","\telse:\n","\t\tret = inp @ W\n","\tif useBias:\n","\t\tret = Bias(ret, name=name, reuse=reuse)\n","\tif useBN:\n","\t\tret = BN(ret)\n","\tif activation != None:\n","\t\tret = Activate(ret, activation)\n","\treturn ret\n","\n","def Bias(data, name=None, reg=False, reuse=False):\n","\tinDim = data.get_shape()[-1]\n","\ttemName = name if name!=None else 'defaultParamName%d'%getParamId()\n","\ttemBiasName = temName + 'Bias'\n","\tbias = getOrDefineParam(temBiasName, inDim, reg=False, initializer='zeros', reuse=reuse)\n","\tif reg:\n","\t\tregParams[temBiasName] = bias\n","\treturn data + bias\n","\n","def ActivateHelp(data, method):\n","\tif method == 'relu':\n","\t\tret = tf.nn.relu(data)\n","\telif method == 'sigmoid':\n","\t\tret = tf.nn.sigmoid(data)\n","\telif method == 'tanh':\n","\t\tret = tf.nn.tanh(data)\n","\telif method == 'softmax':\n","\t\tret = tf.nn.softmax(data, axis=-1)\n","\telif method == 'leakyRelu':\n","\t\tret = tf.maximum(leaky*data, data)\n","\telif method == 'twoWayLeakyRelu6':\n","\t\ttemMask = tf.to_float(tf.greater(data, 6.0))\n","\t\tret = temMask * (6 + leaky * (data - 6)) + (1 - temMask) * tf.maximum(leaky * data, data)\n","\telif method == '-1relu':\n","\t\tret = tf.maximum(-1.0, data)\n","\telif method == 'relu6':\n","\t\tret = tf.maximum(0.0, tf.minimum(6.0, data))\n","\telif method == 'relu3':\n","\t\tret = tf.maximum(0.0, tf.minimum(3.0, data))\n","\telse:\n","\t\traise Exception('Error Activation Function')\n","\treturn ret\n","\n","def Activate(data, method, useBN=False):\n","\tglobal leaky\n","\tif useBN:\n","\t\tret = BN(data)\n","\telse:\n","\t\tret = data\n","\tret = ActivateHelp(ret, method)\n","\treturn ret\n","\n","def Regularize(names=None, method='L2'):\n","\tret = 0\n","\tif method == 'L1':\n","\t\tif names != None:\n","\t\t\tfor name in names:\n","\t\t\t\tret += tf.reduce_sum(tf.abs(getParam(name)))\n","\t\telse:\n","\t\t\tfor name in regParams:\n","\t\t\t\tret += tf.reduce_sum(tf.abs(regParams[name]))\n","\telif method == 'L2':\n","\t\tif names != None:\n","\t\t\tfor name in names:\n","\t\t\t\tret += tf.reduce_sum(tf.square(getParam(name)))\n","\t\telse:\n","\t\t\tfor name in regParams:\n","\t\t\t\tret += tf.reduce_sum(tf.square(regParams[name]))\n","\treturn ret\n","\n","def Dropout(data, rate):\n","\tif rate == None:\n","\t\treturn data\n","\telse:\n","\t\treturn tf.nn.dropout(data, rate=rate)\n","\n","def selfAttention(localReps, number, inpDim, numHeads):\n","\tQ = defineRandomNameParam([inpDim, inpDim], reg=True)\n","\tK = defineRandomNameParam([inpDim, inpDim], reg=True)\n","\tV = defineRandomNameParam([inpDim, inpDim], reg=True)\n","\trspReps = tf.reshape(tf.stack(localReps, axis=1), [-1, inpDim])\n","\tq = tf.reshape(rspReps @ Q, [-1, number, 1, numHeads, inpDim//numHeads])\n","\tk = tf.reshape(rspReps @ K, [-1, 1, number, numHeads, inpDim//numHeads])\n","\tv = tf.reshape(rspReps @ V, [-1, 1, number, numHeads, inpDim//numHeads])\n","\tatt = tf.nn.softmax(tf.reduce_sum(q * k, axis=-1, keepdims=True) / tf.sqrt(inpDim/numHeads), axis=2)\n","\tattval = tf.reshape(tf.reduce_sum(att * v, axis=2), [-1, number, inpDim])\n","\trets = [None] * number\n","\tparamId = 'dfltP%d' % getParamId()\n","\tfor i in range(number):\n","\t\ttem1 = tf.reshape(tf.slice(attval, [0, i, 0], [-1, 1, -1]), [-1, inpDim])\n","\t\t# tem2 = FC(tem1, inpDim, useBias=True, name=paramId+'_1', reg=True, activation='relu', reuse=True) + localReps[i]\n","\t\trets[i] = tem1 + localReps[i]\n","\treturn rets\n","\n","def lightSelfAttention(localReps, number, inpDim, numHeads):\n","\tQ = defineRandomNameParam([inpDim, inpDim], reg=False)\n","\trspReps = tf.reshape(tf.stack(localReps, axis=1), [-1, inpDim])\n","\ttem = rspReps @ Q\n","\tq = tf.reshape(tem, [-1, number, 1, numHeads, inpDim//numHeads])\n","\tk = tf.reshape(tem, [-1, 1, number, numHeads, inpDim//numHeads])\n","\tv = tf.reshape(rspReps, [-1, 1, number, numHeads, inpDim//numHeads])\n","\tatt = tf.nn.softmax(tf.reduce_sum(q * k, axis=-1, keepdims=True) / tf.sqrt(inpDim/numHeads), axis=2)\n","\tattval = tf.reshape(tf.reduce_sum(att * v, axis=2), [-1, number, inpDim])\n","\trets = [None] * number\n","\tparamId = 'dfltP%d' % getParamId()\n","\tfor i in range(number):\n","\t\ttem1 = tf.reshape(tf.slice(attval, [0, i, 0], [-1, 1, -1]), [-1, inpDim])\n","\t\trets[i] = tem1 + localReps[i]\n","\treturn rets"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dN6PkpzA2E5l"},"source":["### Logger"]},{"cell_type":"code","metadata":{"id":"mEEETzOA2E2t"},"source":["import datetime\n","\n","logmsg = ''\n","timemark = dict()\n","saveDefault = False\n","\n","def log(msg, save=None, oneline=False):\n","\tglobal logmsg\n","\tglobal saveDefault\n","\ttime = datetime.datetime.now()\n","\ttem = '%s: %s' % (time, msg)\n","\tif save != None:\n","\t\tif save:\n","\t\t\tlogmsg += tem + '\\n'\n","\telif saveDefault:\n","\t\tlogmsg += tem + '\\n'\n","\tif oneline:\n","\t\tprint(tem, end='\\r')\n","\telse:\n","\t\tprint(tem)\n","\n","def marktime(marker):\n","\tglobal timemark\n","\ttimemark[marker] = datetime.datetime.now()\n","\n","def SpentTime(marker):\n","\tglobal timemark\n","\tif marker not in timemark:\n","\t\tmsg = 'LOGGER ERROR, marker', marker, ' not found'\n","\t\ttem = '%s: %s' % (time, msg)\n","\t\tprint(tem)\n","\t\treturn False\n","\treturn datetime.datetime.now() - timemark[marker]\n","\n","def SpentTooLong(marker, day=0, hour=0, minute=0, second=0):\n","\tglobal timemark\n","\tif marker not in timemark:\n","\t\tmsg = 'LOGGER ERROR, marker', marker, ' not found'\n","\t\ttem = '%s: %s' % (time, msg)\n","\t\tprint(tem)\n","\t\treturn False\n","\treturn datetime.datetime.now() - timemark[marker] >= datetime.timedelta(days=day, hours=hour, minutes=minute, seconds=second)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dvjuYIYx2E0Y"},"source":["### Params"]},{"cell_type":"code","metadata":{"id":"FRqvzMDZ2Ewo"},"source":["import argparse\n","\n","def parse_args():\n","\tparser = argparse.ArgumentParser(description='Model Params')\n","\tparser.add_argument('--lr', default=1e-3, type=float, help='learning rate')\n","\tparser.add_argument('--batch', default=32, type=int, help='batch size')\n","\tparser.add_argument('--reg', default=1e-2, type=float, help='weight decay regularizer')\n","\t# parser.add_argument('--epoch', default=120, type=int, help='number of epochs')\n","\tparser.add_argument('--epoch', default=12, type=int, help='number of epochs')\n","\tparser.add_argument('--decay', default=0.96, type=float, help='weight decay rate')\n","\tparser.add_argument('--save_path', default='tem', help='file name to save model and training record')\n","\tparser.add_argument('--latdim', default=16, type=int, help='embedding size')\n","\tparser.add_argument('--memosize', default=2, type=int, help='memory size')\n","\tparser.add_argument('--sampNum', default=40, type=int, help='batch size for sampling')\n","\tparser.add_argument('--att_head', default=2, type=int, help='number of attention heads')\n","\tparser.add_argument('--gnn_layer', default=2, type=int, help='number of gnn layers')\n","\tparser.add_argument('--trnNum', default=10000, type=int, help='number of training instances per epoch')\n","\tparser.add_argument('--load_model', default=None, help='model name to load')\n","\tparser.add_argument('--shoot', default=10, type=int, help='K of top k')\n","\tparser.add_argument('--data', default='yelp', type=str, help='name of dataset')\n","\tparser.add_argument('--target', default='buy', type=str, help='target behavior to predict on')\n","\tparser.add_argument('--deep_layer', default=0, type=int, help='number of deep layers to make the final prediction')\n","\tparser.add_argument('--mult', default=1, type=float, help='multiplier for the result')\n","\tparser.add_argument('--keepRate', default=0.7, type=float, help='rate for dropout')\n","\tparser.add_argument('--iiweight', default=0.3, type=float, help='weight for ii')\n","\tparser.add_argument('--slot', default=5, type=int, help='length of time slots')\n","\tparser.add_argument('--graphSampleN', default=25000, type=int, help='use 25000 for training and 200000 for testing, empirically')\n","\tparser.add_argument('--divSize', default=50, type=int, help='div size for smallTestEpoch')\n","\treturn parser.parse_args(\"\")\n"," \n","args = parse_args()\n","# args.user = 147894\n","# args.item = 99037\n","# ML10M\n","# args.user = 67788\n","# args.item = 8704\n","# yelp\n","args.user = 19800\n","args.item = 22734\n","\n","\n","args.decay_step = args.trnNum//args.batch"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"criRetuA3-No"},"source":["# python labcode_retail.py --data retail --graphSampleN 15000 --reg 1e-1 --save_path XXX\n","args.data = 'yelp'\n","# args.graphSampleN = 15000\n","# args.reg = 1e-1\n","args.save_path = '/content'"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"p6_37HdT2Eut"},"source":["### Utils"]},{"cell_type":"code","metadata":{"id":"7T3lIkbp2Esn"},"source":["import pickle\n","import numpy as np\n","from scipy.sparse import csr_matrix\n","import scipy.sparse as sp\n","\n","if args.data == 'yelp':\n","\tpredir = 'Datasets/Yelp/'\n","\tbehs = ['tip', 'neg', 'neutral', 'pos']\n","elif args.data == 'ml10m':\n","\tpredir = 'Datasets/MultiInt-ML10M/'\n","\tbehs = ['neg', 'neutral', 'pos']\n","elif args.data == 'retail':\n","\tpredir = 'Datasets/retail/'\n","\tbehs = ['pv', 'fav', 'cart', 'buy']\n","\n","trnfile = predir + 'trn_'\n","tstfile = predir + 'tst_'\n","\n","\n","def helpInit(a, b, c):\n","\tret = [[None] * b for i in range(a)]\n","\tfor i in range(a):\n","\t\tfor j in range(b):\n","\t\t\tret[i][j] = [None] * c\n","\treturn ret\n","\n","def timeProcess(trnMats):\n","\tmi = 1e15\n","\tma = 0\n","\tfor i in range(len(trnMats)):\n","\t\tminn = np.min(trnMats[i].data)\n","\t\tmaxx = np.max(trnMats[i].data)\n","\t\tmi = min(mi, minn)\n","\t\tma = max(ma, maxx)\n","\tmaxTime = 0\n","\tfor i in range(len(trnMats)):\n","\t\tnewData = ((trnMats[i].data - mi) / (3600*24*args.slot)).astype(np.int32)\n","\t\tmaxTime = max(np.max(newData), maxTime)\n","\t\ttrnMats[i] = csr_matrix((newData, trnMats[i].indices, trnMats[i].indptr), shape=trnMats[i].shape)\n","\tprint('MAX TIME', maxTime)\n","\treturn trnMats, maxTime + 1\n","\n","# behs = ['buy']\n","\n","def ObtainIIMats(trnMats, predir):\n","\t# # MAKE\n","\t# iiMats = list()\n","\t# for i in range(len(behs)):\n","\t# \tiiMats.append(makeIiMats(trnMats[i]))\n","\t# \tprint('i', i)\n","\t# with open(predir+'trn_catDict', 'rb') as fs:\n","\t# \tcatDict = pickle.load(fs)\n","\t# iiMats.append(makeCatIiMats(catDict, trnMats[0].shape[1]))\n","\n","\t# # DUMP\n","\t# with open(predir+'iiMats_cache', 'wb') as fs:\n","\t# \tpickle.dump(iiMats, fs)\n","\t# exit()\n","\n","\t# READ\n","\twith open(predir+'iiMats', 'rb') as fs:\n","\t\tiiMats = pickle.load(fs)\n","\t# iiMats = iiMats[3:]# + iiMats[2:]\n","\treturn iiMats\n","\n","def LoadData():\n","\ttrnMats = list()\n","\tfor i in range(len(behs)):\n","\t\tbeh = behs[i]\n","\t\tpath = trnfile + beh\n","\t\twith open(path, 'rb') as fs:\n","\t\t\tmat = pickle.load(fs)\n","\t\ttrnMats.append(mat)\n","\t\tif args.target == 'click':\n","\t\t\ttrnLabel = (mat if i==0 else 1 * (trnLabel + mat != 0))\n","\t\telif args.target == 'buy' and i == len(behs) - 1:\n","\t\t\ttrnLabel = 1 * (mat != 0)\n","\ttrnMats, maxTime = timeProcess(trnMats)\n","\t# test set\n","\tpath = tstfile + 'int'\n","\twith open(path, 'rb') as fs:\n","\t\ttstInt = np.array(pickle.load(fs))\n","\ttstStat = (tstInt!=None)\n","\ttstUsrs = np.reshape(np.argwhere(tstStat!=False), [-1])\n","\n","\tiiMats = ObtainIIMats(trnMats, predir)\n","\n","\treturn trnMats, iiMats, tstInt, trnLabel, tstUsrs, len(behs), maxTime\n","\n","# negative sampling using pre-sampled entities (preSamp) for efficiency\n","def negSamp(temLabel, preSamp, sampSize=1000):\n","\tnegset = [None] * sampSize\n","\tcur = 0\n","\tfor temval in preSamp:\n","\t\tif temLabel[temval] == 0:\n","\t\t\tnegset[cur] = temval\n","\t\t\tcur += 1\n","\t\tif cur == sampSize:\n","\t\t\tbreak\n","\tnegset = np.array(negset[:cur])\n","\treturn negset\n","\n","def transpose(mat):\n","\tcoomat = sp.coo_matrix(mat)\n","\treturn csr_matrix(coomat.transpose())\n","\n","def transToLsts(mat, mask=False):\n","\tshape = [mat.shape[0], mat.shape[1]]\n","\tcoomat = sp.coo_matrix(mat)\n","\tindices = np.array(list(map(list, zip(coomat.row, coomat.col))), dtype=np.int32)\n","\tdata = coomat.data.astype(np.int32)\n","\n","\t# half mask\n","\tif mask:\n","\t\tspMask = (np.random.uniform(size=data.shape) > 0.5) * 1.0\n","\t\tdata = data * spMask\n","\n","\tif indices.shape[0] == 0:\n","\t\tindices = np.array([[0, 0]], dtype=np.int32)\n","\t\tdata = np.array([0.0], np.int32)\n","\treturn indices, data, shape\n","\n","def makeCatIiMats(dic, itmnum):\n","\tretInds = []\n","\tfor key in dic:\n","\t\ttemLst = list(dic[key])\n","\t\tfor i in range(len(temLst)):\n","\t\t\tif args.data == 'tmall' and args.target == 'click':\n","\t\t\t\tdiv = 50\n","\t\t\telse:\n","\t\t\t\tdiv = 10\n","\t\t\tif args.data == 'ml10m' or args.data == 'tmall' and args.target == 'click':\n","\t\t\t\tscdTemLst = list(np.random.choice(range(len(temLst)), len(temLst) // div, replace=False))\n","\t\t\telse:\n","\t\t\t\tscdTemLst = range(len(temLst))\n","\t\t\tfor j in scdTemLst:#range(len(temLst)):\n","\t\t\t\t# if args.data == 'ml10m' and np.random.uniform(0.0, 1.0) < 0.1:\n","\t\t\t\t# \tcontinue\n","\t\t\t\tretInds.append([temLst[i], temLst[j]])\n","\tpckLocs = np.random.permutation(len(retInds))[:100000]#:len(retInds)//100]\n","\tretInds = np.array(retInds, dtype=np.int32)[pckLocs]\n","\tretData = np.array([1] * retInds.shape[0], np.int32)\n","\treturn retInds, retData, [itmnum, itmnum]\n","\n","def makeIiMats(mat):\n","\tshape = [mat.shape[0], mat.shape[1]]\n","\tcoomat = sp.coo_matrix(mat)\n","\tindices = list(map(list, zip(coomat.row, coomat.col)))\n","\tuDict = [set() for i in range(shape[0])]\n","\tfor ind in indices:\n","\t\tusr = ind[0]\n","\t\titm = ind[1]\n","\t\tuDict[usr].add(itm)\n","\tretInds = []\n","\tfor usr in range(shape[0]):\n","\t\ttemLst = list(uDict[usr])\n","\t\tfor i in range(len(temLst)):\n","\t\t\tif args.data == 'tmall' and args.target == 'click':\n","\t\t\t\tdiv = 50\n","\t\t\telse:\n","\t\t\t\tdiv = 10\n","\t\t\tif args.data == 'ml10m' or args.data == 'tmall' and args.target == 'click':\n","\t\t\t\tscdTemLst = list(np.random.choice(range(len(temLst)), len(temLst) // div, replace=False))\n","\t\t\telse:\n","\t\t\t\tscdTemLst = range(len(temLst))\n","\t\t\tfor j in scdTemLst:#range(len(temLst)):\n","\t\t\t\t# if args.data == 'ml10m' and np.random.uniform(0.0, 1.0) < 0.1:\n","\t\t\t\t# \tcontinue\n","\t\t\t\tretInds.append([temLst[i], temLst[j]])\n","\tpckLocs = np.random.permutation(len(retInds))[:100000]#[:len(retInds)//100]\n","\tretInds = np.array(retInds, dtype=np.int32)[pckLocs]\n","\tretData = np.array([1] * retInds.shape[0], np.int32)\n","\treturn retInds, retData, [shape[1], shape[1]]\n","\n","def prepareGlobalData(trnMats, trnLabel, iiMats):\n","\tglobal adjs\n","\tglobal adj\n","\tglobal tpadj\n","\tglobal iiAdjs\n","\tadjs = trnMats\n","\tiiAdjs = list()\n","\tfor i in range(len(iiMats)):\n","\t\tiiAdjs.append(csr_matrix((iiMats[i][1], (iiMats[i][0][:,0], iiMats[i][0][:,1])), shape=iiMats[i][2]))\n","\tadj = trnLabel.astype(np.float32)\n","\ttpadj = transpose(adj)\n","\tadjNorm = np.reshape(np.array(np.sum(adj, axis=1)), [-1])\n","\ttpadjNorm = np.reshape(np.array(np.sum(tpadj, axis=1)), [-1])\n","\tfor i in range(adj.shape[0]):\n","\t\tfor j in range(adj.indptr[i], adj.indptr[i+1]):\n","\t\t\tadj.data[j] /= adjNorm[i]\n","\tfor i in range(tpadj.shape[0]):\n","\t\tfor j in range(tpadj.indptr[i], tpadj.indptr[i+1]):\n","\t\t\ttpadj.data[j] /= tpadjNorm[i]\n","\n","def sampleLargeGraph(pckUsrs, pckItms=None, sampDepth=2, sampNum=args.graphSampleN):\n","\tglobal adjs\n","\tglobal adj\n","\tglobal tpadj\n","\tglobal iiAdjs\n","\n","\tdef makeMask(nodes, size):\n","\t\tmask = np.ones(size)\n","\t\tif not nodes is None:\n","\t\t\tmask[nodes] = 0.0\n","\t\treturn mask\n","\n","\tdef updateBdgt(adj, nodes):\n","\t\tif nodes is None:\n","\t\t\treturn 0\n","\t\ttembat = 1000\n","\t\tret = 0\n","\t\tfor i in range(int(np.ceil(len(nodes) / tembat))):\n","\t\t\tst = tembat * i\n","\t\t\ted = min((i+1) * tembat, len(nodes))\n","\t\t\ttemNodes = nodes[st: ed]\n","\t\t\tret += np.sum(adj[temNodes], axis=0)\n","\t\treturn ret\n","\n","\tdef sample(budget, mask, sampNum):\n","\t\tscore = (mask * np.reshape(np.array(budget), [-1])) ** 2\n","\t\tnorm = np.sum(score)\n","\t\tif norm == 0:\n","\t\t\treturn np.random.choice(len(score), 1)\n","\t\tscore = list(score / norm)\n","\t\tarrScore = np.array(score)\n","\t\tposNum = np.sum(np.array(score)!=0)\n","\t\tif posNum < sampNum:\n","\t\t\tpckNodes1 = np.squeeze(np.argwhere(arrScore!=0))\n","\t\t\tpckNodes2 = np.random.choice(np.squeeze(np.argwhere(arrScore==0.0)), min(len(score) - posNum, sampNum - posNum), replace=False)\n","\t\t\tpckNodes = np.concatenate([pckNodes1, pckNodes2], axis=0)\n","\t\telse:\n","\t\t\tpckNodes = np.random.choice(len(score), sampNum, p=score, replace=False)\n","\t\treturn pckNodes\n","\n","\tusrMask = makeMask(pckUsrs, adj.shape[0])\n","\titmMask = makeMask(pckItms, adj.shape[1])\n","\titmBdgt = updateBdgt(adj, pckUsrs)\n","\tif pckItms is None:\n","\t\tpckItms = sample(itmBdgt, itmMask, len(pckUsrs))\n","\t\t# pckItms = sample(itmBdgt, itmMask, sampNum)\n","\t\titmMask = itmMask * makeMask(pckItms, adj.shape[1])\n","\tusrBdgt = updateBdgt(tpadj, pckItms)\n","\tfor i in range(sampDepth):\n","\t\tnewUsrs = sample(usrBdgt, usrMask, sampNum)\n","\t\tusrMask = usrMask * makeMask(newUsrs, adj.shape[0])\n","\t\tnewItms = sample(itmBdgt, itmMask, sampNum)\n","\t\titmMask = itmMask * makeMask(newItms, adj.shape[1])\n","\t\tif i == sampDepth - 1:\n","\t\t\tbreak\n","\t\tusrBdgt += updateBdgt(tpadj, newItms)\n","\t\titmBdgt += updateBdgt(adj, newUsrs)\n","\tusrs = np.reshape(np.argwhere(usrMask==0), [-1])\n","\titms = np.reshape(np.argwhere(itmMask==0), [-1])\n","\tpckAdjs = []\n","\tpckTpAdjs = []\n","\tpckIiAdjs = []\n","\tfor i in range(len(adjs)):\n","\t\tpckU = adjs[i][usrs]\n","\t\ttpPckI = transpose(pckU)[itms]\n","\t\tpckTpAdjs.append(tpPckI)\n","\t\tpckAdjs.append(transpose(tpPckI))\n","\tfor i in range(len(iiAdjs)):\n","\t\tpckI = iiAdjs[i][itms]\n","\t\ttpPckI = transpose(pckI)[itms]\n","\t\tpckIiAdjs.append(tpPckI)\n","\treturn pckAdjs, pckTpAdjs, pckIiAdjs, usrs, itms"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"q55PIcHK2EqT"},"source":["### Run"]},{"cell_type":"code","metadata":{"id":"va31ijFN2Elx"},"source":["!mkdir -p History Models"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tQMAToyz1ZLa","executionInfo":{"status":"ok","timestamp":1634141291431,"user_tz":-330,"elapsed":3843860,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}},"outputId":"677ddbbe-a690-4c44-f6c5-c51f9f6993e5"},"source":["import numpy as np\n","import tensorflow as tf\n","from tensorflow.core.protobuf import config_pb2\n","import pickle\n","\n","\n","class Recommender:\n","\tdef __init__(self, sess, datas):\n","\t\tself.sess = sess\n","\t\tself.trnMats, self.iiMats, self.tstInt, self.label, self.tstUsrs, args.intTypes, self.maxTime = datas\n","\t\tprepareGlobalData(self.trnMats, self.label, self.iiMats)\n","\t\targs.user, args.item = self.trnMats[0].shape\n","\t\tprint('USER', args.user, 'ITEM', args.item)\n","\t\tself.metrics = dict()\n","\t\tmets = ['Loss', 'preLoss', 'HR', 'NDCG']\n","\t\tfor met in mets:\n","\t\t\tself.metrics['Train'+met] = list()\n","\t\t\tself.metrics['Test'+met] = list()\n","\n","\tdef makePrint(self, name, ep, reses, save):\n","\t\tret = 'Epoch %d/%d, %s: ' % (ep, args.epoch, name)\n","\t\tfor metric in reses:\n","\t\t\tval = reses[metric]\n","\t\t\tret += '%s = %.4f, ' % (metric, val)\n","\t\t\ttem = name + metric\n","\t\t\tif save and tem in self.metrics:\n","\t\t\t\tself.metrics[tem].append(val)\n","\t\tret = ret[:-2] + '  '\n","\t\treturn ret\n","\n","\tdef run(self):\n","\t\tself.prepareModel()\n","\t\tlog('Model Prepared')\n","\t\tif args.load_model != None:\n","\t\t\tself.loadModel()\n","\t\t\tstloc = len(self.metrics['TrainLoss']) * 3\n","\t\telse:\n","\t\t\tstloc = 0\n","\t\t\tinit = tf.global_variables_initializer()\n","\t\t\tself.sess.run(init)\n","\t\t\tlog('Varaibles Inited')\n","\t\tfor ep in range(stloc, args.epoch):\n","\t\t\ttest = (ep % 3 == 0)\n","\t\t\treses = self.trainEpoch()\n","\t\t\tlog(self.makePrint('Train', ep, reses, test))\n","\t\t\tif test:\n","\t\t\t\treses = self.testEpoch()\n","\t\t\t\tlog(self.makePrint('Test', ep, reses, test))\n","\t\t\tif ep % 5 == 0:\n","\t\t\t\tself.saveHistory()\n","\t\t\tprint()\n","\t\treses = self.testEpoch()\n","\t\tlog(self.makePrint('Test', args.epoch, reses, True))\n","\t\tself.saveHistory()\n","\n","\tdef GAT(self, srcEmbeds, tgtEmbeds, tgtNodes, maxNum, Qs, Ks, Vs):\n","\t\tQWeight = tf.nn.softmax(defineRandomNameParam([args.memosize, 1, 1], reg=True), axis=1)\n","\t\tKWeight = tf.nn.softmax(defineRandomNameParam([args.memosize, 1, 1], reg=True), axis=1)\n","\t\tVWeight = tf.nn.softmax(defineRandomNameParam([args.memosize, 1, 1], reg=True), axis=1)\n","\t\tQ = tf.reduce_sum(Qs * QWeight, axis=0)\n","\t\tK = tf.reduce_sum(Ks * KWeight, axis=0)\n","\t\tV = tf.reduce_sum(Vs * VWeight, axis=0)\n","\n","\t\tq = tf.reshape(tgtEmbeds @ Q, [-1, args.att_head, args.latdim//args.att_head])\n","\t\tk = tf.reshape(srcEmbeds @ K, [-1, args.att_head, args.latdim//args.att_head])\n","\t\tv = tf.reshape(srcEmbeds @ V, [-1, args.att_head, args.latdim//args.att_head])\n","\t\tlogits = tf.math.exp(tf.reduce_sum(q * k, axis=-1, keepdims=True) / tf.sqrt(args.latdim/args.att_head))\n","\t\tattNorm = tf.nn.embedding_lookup(tf.math.segment_sum(logits, tgtNodes), tgtNodes) + 1e-6\n","\t\tatt = logits / attNorm\n","\t\tpadAttval = tf.pad(att * v, [[0, 1], [0, 0], [0, 0]])\n","\t\tpadTgtNodes = tf.concat([tgtNodes, tf.reshape(maxNum-1, [1])], axis=-1)\n","\t\tattval = tf.reshape(tf.math.segment_sum(padAttval, padTgtNodes), [-1, args.latdim])\n","\t\treturn attval\n","\n","\tdef messagePropagate(self, srclats, tgtlats, mats, maxNum, wTime=True):\n","\t\tunAct = []\n","\t\tlats1 = []\n","\t\tparamId = 'dfltP%d' % getParamId()\n","\t\tQs = defineRandomNameParam([args.memosize, args.latdim, args.latdim], reg=True)\n","\t\tKs = defineRandomNameParam([args.memosize, args.latdim, args.latdim], reg=True)\n","\t\tVs = defineRandomNameParam([args.memosize, args.latdim, args.latdim], reg=True)\n","\t\tfor mat in mats:\n","\t\t\ttimeEmbed = FC(self.timeEmbed, args.latdim, reg=True)\n","\t\t\tsrcNodes = tf.squeeze(tf.slice(mat.indices, [0, 1], [-1, 1]))\n","\t\t\ttgtNodes = tf.squeeze(tf.slice(mat.indices, [0, 0], [-1, 1]))\n","\t\t\tedgeVals = mat.values\n","\t\t\tsrcEmbeds = (tf.nn.embedding_lookup(srclats, srcNodes) + (tf.nn.embedding_lookup(timeEmbed, edgeVals) if wTime else 0))\n","\t\t\ttgtEmbeds = tf.nn.embedding_lookup(tgtlats, tgtNodes)\n","\n","\t\t\tnewTgtEmbeds = self.GAT(srcEmbeds, tgtEmbeds, tgtNodes, maxNum, Qs, Ks, Vs)\n","\n","\t\t\tunAct.append(newTgtEmbeds)\n","\t\t\tlats1.append(Activate(newTgtEmbeds, self.actFunc))\n","\n","\t\tlats2 = lightSelfAttention(lats1, number=len(mats), inpDim=args.latdim, numHeads=args.att_head)\n","\n","\t\t# aggregation gate\n","\t\tglobalQuery = Activate(tf.add_n(unAct), self.actFunc)\n","\t\tweights = []\n","\t\tparamId = 'dfltP%d' % getParamId()\n","\t\tfor lat in lats2:\n","\t\t\ttemlat = FC(tf.concat([lat, globalQuery], axis=-1) , args.latdim//2, useBias=False, reg=False, activation=self.actFunc, name=paramId+'_1', reuse=True)\n","\t\t\tweight = FC(temlat, 1, useBias=False, reg=False, name=paramId+'_2', reuse=True)\n","\t\t\tweights.append(weight)\n","\t\tstkWeight = tf.concat(weights, axis=1)\n","\t\tsftWeight = tf.reshape(tf.nn.softmax(stkWeight, axis=1), [-1, len(mats), 1]) * 8\n","\t\tstkLat = tf.stack(lats2, axis=1)\n","\t\tlat = tf.reshape(tf.reduce_sum(sftWeight * stkLat, axis=1), [-1, args.latdim])\n","\t\treturn lat\n","\n","\tdef makeTimeEmbed(self):\n","\t\tdivTerm = 1 / (10000 ** (tf.range(0, args.latdim * 2, 2, dtype=tf.float32) / args.latdim))\n","\t\tpos = tf.expand_dims(tf.range(0, self.maxTime, dtype=tf.float32), axis=-1)\n","\t\tsine = tf.expand_dims(tf.math.sin(pos * divTerm) / np.sqrt(args.latdim), axis=-1)\n","\t\tcosine = tf.expand_dims(tf.math.cos(pos * divTerm) / np.sqrt(args.latdim), axis=-1)\n","\t\ttimeEmbed = tf.reshape(tf.concat([sine, cosine], axis=-1), [self.maxTime, args.latdim*2])/4.0\n","\t\treturn timeEmbed\n","\n","\tdef ours(self):\n","\t\tall_uEmbed0 = defineParam('uEmbed0', [args.user, args.latdim], reg=True)\n","\t\tall_iEmbed0 = defineParam('iEmbed0', [args.item, args.latdim], reg=True)\n","\t\tuEmbed0 = tf.nn.embedding_lookup(all_uEmbed0, self.all_usrs)\n","\t\tiEmbed0 = tf.nn.embedding_lookup(all_iEmbed0, self.all_itms)\n","\t\tself.timeEmbed = tf.Variable(initial_value=self.makeTimeEmbed(), shape=[self.maxTime, args.latdim*2], name='timeEmbed', trainable=True)\n","\t\taddReg('timeEmbed', self.timeEmbed)\n","\t\tulats = [uEmbed0]\n","\t\tilats = [iEmbed0]\n","\t\tfor i in range(args.gnn_layer):\n","\t\t\tulat = self.messagePropagate(ilats[-1], ulats[-1], self.adjs, self.usrNum)\n","\t\t\tilat1 = self.messagePropagate(ulats[-1], ilats[-1], self.tpAdjs, self.itmNum)\n","\t\t\tilat2 = self.messagePropagate(ilats[-1], ilats[-1], self.iiAdjs, self.itmNum, wTime=False)\n","\t\t\tilat = args.iiweight * ilat2 + (1.0 - args.iiweight) * ilat1\n","\t\t\tulats.append(ulat + ulats[-1])\n","\t\t\tilats.append(ilat + ilats[-1])\n","\n","\t\tUEmbedPred = defineParam('UEmbedPred', shape=[args.user, args.latdim], dtype=tf.float32, reg=False)\n","\t\tIEmbedPred = defineParam('IEmbedPred', shape=[args.item, args.latdim], dtype=tf.float32, reg=False)\n","\t\tulats[0] = tf.nn.embedding_lookup(UEmbedPred, self.all_usrs)\n","\t\tilats[0] = tf.nn.embedding_lookup(IEmbedPred, self.all_itms)\n","\n","\t\tulat = tf.add_n(ulats)\n","\t\tilat = tf.add_n(ilats)\n","\t\tpckULat = tf.nn.embedding_lookup(ulat, self.uids)\n","\t\tpckILat = tf.nn.embedding_lookup(ilat, self.iids)\n","\n","\t\tpredLat = pckULat * pckILat * args.mult\n","\n","\t\tfor i in range(args.deep_layer):\n","\t\t\tpredLat = FC(predLat, args.latdim, reg=True, useBias=True, activation=self.actFunc) + predLat\n","\t\tpred = tf.squeeze(FC(predLat, 1, reg=True, useBias=True))\n","\t\treturn pred\n","\n","\tdef prepareModel(self):\n","\t\tself.keepRate = tf.placeholder(name='keepRate', dtype=tf.float32, shape=[])\n","\t\tself.actFunc = 'twoWayLeakyRelu6'\n","\t\tself.adjs = []\n","\t\tself.tpAdjs = []\n","\t\tself.iiAdjs = []\n","\t\tfor i in range(args.intTypes):\n","\t\t\tself.adjs.append(tf.sparse_placeholder(dtype=tf.int32))\n","\t\t\tself.tpAdjs.append(tf.sparse_placeholder(dtype=tf.int32))\n","\t\tfor i in range(len(self.iiMats)):\n","\t\t\tself.iiAdjs.append(tf.sparse_placeholder(dtype=tf.int32))\n","\n","\t\tself.all_usrs = tf.placeholder(name='all_usrs', dtype=tf.int32, shape=[None])\n","\t\tself.all_itms = tf.placeholder(name='all_itms', dtype=tf.int32, shape=[None])\n","\t\tself.usrNum = tf.placeholder(name='usrNum', dtype=tf.int64, shape=[])\n","\t\tself.itmNum = tf.placeholder(name='itmNum', dtype=tf.int64, shape=[])\n","\t\tself.uids = tf.placeholder(name='uids', dtype=tf.int32, shape=[None])\n","\t\tself.iids = tf.placeholder(name='iids', dtype=tf.int32, shape=[None])\n","\n","\t\tself.pred = self.ours()\n","\t\tsampNum = tf.shape(self.iids)[0] // 2\n","\t\tposPred = tf.slice(self.pred, [0], [sampNum])\n","\t\tnegPred = tf.slice(self.pred, [sampNum], [-1])\n","\t\tself.preLoss = tf.reduce_sum(tf.maximum(0.0, 1.0 - (posPred - negPred))) / args.batch\n","\t\tself.regLoss = args.reg * Regularize()\n","\t\tself.loss = self.preLoss + self.regLoss\n","\n","\t\tglobalStep = tf.Variable(0, trainable=False)\n","\t\tlearningRate = tf.train.exponential_decay(args.lr, globalStep, args.decay_step, args.decay, staircase=True)\n","\t\tself.optimizer = tf.train.AdamOptimizer(learningRate).minimize(self.loss, global_step=globalStep)\n","\n","\tdef sampleTrainBatch(self, batchIds, itmnum, label):\n","\t\tpreSamp = list(np.random.permutation(itmnum))\n","\t\ttemLabel = label[batchIds].toarray()\n","\t\tbatch = len(batchIds)\n","\t\ttemlen = batch * 2 * args.sampNum\n","\t\tuIntLoc = [None] * temlen\n","\t\tiIntLoc = [None] * temlen\n","\t\tcur = 0\n","\t\tfor i in range(batch):\n","\t\t\tposset = np.reshape(np.argwhere(temLabel[i]!=0), [-1])\n","\t\t\tnegset = negSamp(temLabel[i], preSamp)\n","\t\t\tposlocs = np.random.choice(posset, args.sampNum)\n","\t\t\tneglocs = np.random.choice(negset, args.sampNum)\n","\t\t\tfor j in range(args.sampNum):\n","\t\t\t\tuIntLoc[cur] = uIntLoc[cur+temlen//2] = batchIds[i]\n","\t\t\t\tiIntLoc[cur] = poslocs[j]\n","\t\t\t\tiIntLoc[cur+temlen//2] = neglocs[j]\n","\t\t\t\tcur += 1\n","\t\treturn uIntLoc, iIntLoc\n","\n","\tdef trainEpoch(self):\n","\t\tnum = args.user\n","\t\tsfIds = np.random.permutation(num)[:args.trnNum]\n","\t\tepochLoss, epochPreLoss = [0] * 2\n","\t\tnum = len(sfIds)\n","\t\tsteps = int(np.ceil(num / args.batch))\n","\n","\t\tpckAdjs, pckTpAdjs, pckIiAdjs, usrs, itms = sampleLargeGraph(sfIds)\n","\t\tpckLabel = transpose(transpose(self.label[usrs])[itms])\n","\t\tusrIdMap = dict(map(lambda x: (usrs[x], x), range(len(usrs))))\n","\t\tsfIds = list(map(lambda x: usrIdMap[x], sfIds))\n","\t\tfeeddict = {self.all_usrs: usrs, self.all_itms: itms, self.usrNum: len(usrs), self.itmNum: len(itms)}\n","\t\tfor i in range(args.intTypes):\n","\t\t\tfeeddict[self.adjs[i]] = transToLsts(pckAdjs[i])\n","\t\t\tfeeddict[self.tpAdjs[i]] = transToLsts(pckTpAdjs[i])\n","\t\tfor i in range(len(pckIiAdjs)):\n","\t\t\tfeeddict[self.iiAdjs[i]] = transToLsts(pckIiAdjs[i])\n","\n","\t\tfor i in range(steps):\n","\t\t\tst = i * args.batch\n","\t\t\ted = min((i+1) * args.batch, num)\n","\t\t\tbatIds = sfIds[st: ed]\n","\n","\t\t\tuLocs, iLocs = self.sampleTrainBatch(batIds, pckAdjs[0].shape[1], pckLabel)\n","\n","\t\t\ttarget = [self.optimizer, self.preLoss, self.regLoss, self.loss]\n","\t\t\tfeeddict[self.uids] = uLocs\n","\t\t\tfeeddict[self.iids] = iLocs\n","\t\t\tres = self.sess.run(target, feed_dict=feeddict, options=config_pb2.RunOptions(report_tensor_allocations_upon_oom=True))\n","\n","\t\t\tpreLoss, regLoss, loss = res[1:]\n","\n","\t\t\tepochLoss += loss\n","\t\t\tepochPreLoss += preLoss\n","\t\t\tlog('Step %d/%d: loss = %.2f, regLoss = %.2f         ' % (i, steps, loss, regLoss), save=False, oneline=True)\n","\t\tret = dict()\n","\t\tret['Loss'] = epochLoss / steps\n","\t\tret['preLoss'] = epochPreLoss / steps\n","\t\treturn ret\n","\n","\tdef sampleTestBatch(self, batchIds, label, tstInt):\n","\t\tbatch = len(batchIds)\n","\t\ttemTst = tstInt[batchIds]\n","\t\ttemLabel = label[batchIds].toarray()\n","\t\ttemlen = batch * 100\n","\t\tuIntLoc = [None] * temlen\n","\t\tiIntLoc = [None] * temlen\n","\t\ttstLocs = [None] * batch\n","\t\tcur = 0\n","\t\tfor i in range(batch):\n","\t\t\tposloc = temTst[i]\n","\t\t\tnegset = np.reshape(np.argwhere(temLabel[i]==0), [-1])\n","\t\t\trdnNegSet = np.random.permutation(negset)[:99]\n","\t\t\tlocset = np.concatenate((rdnNegSet, np.array([posloc])))\n","\t\t\ttstLocs[i] = locset\n","\t\t\tfor j in range(100):\n","\t\t\t\tuIntLoc[cur] = batchIds[i]\n","\t\t\t\tiIntLoc[cur] = locset[j]\n","\t\t\t\tcur += 1\n","\t\treturn uIntLoc, iIntLoc, temTst, tstLocs\n","\n","\tdef testEpoch(self):\n","\t\tepochHit, epochNdcg = [0] * 2\n","\t\tids = self.tstUsrs\n","\t\tnum = len(ids)\n","\t\ttstBat = np.maximum(1, args.batch * args.sampNum // 100)\n","\t\tsteps = int(np.ceil(num / tstBat))\n","\n","\t\tposItms = self.tstInt[ids]\n","\t\tpckAdjs, pckTpAdjs, pckIiAdjs, usrs, itms = sampleLargeGraph(ids, list(set(posItms)))\n","\t\tpckLabel = transpose(transpose(self.label[usrs])[itms])\n","\t\tusrIdMap = dict(map(lambda x: (usrs[x], x), range(len(usrs))))\n","\t\titmIdMap = dict(map(lambda x: (itms[x], x), range(len(itms))))\n","\t\tids = list(map(lambda x: usrIdMap[x], ids))\n","\t\titmMapping = (lambda x: None if (x is None) else itmIdMap[x])\n","\t\tpckTstInt = np.array(list(map(lambda x: itmMapping(self.tstInt[usrs[x]]), range(len(usrs)))))\n","\t\tfeeddict = {self.all_usrs: usrs, self.all_itms: itms, self.usrNum: len(usrs), self.itmNum: len(itms)}\n","\t\tfor i in range(args.intTypes):\n","\t\t\tfeeddict[self.adjs[i]] = transToLsts(pckAdjs[i])\n","\t\t\tfeeddict[self.tpAdjs[i]] = transToLsts(pckTpAdjs[i])\n","\t\tfor i in range(len(pckIiAdjs)):\n","\t\t\tfeeddict[self.iiAdjs[i]] = transToLsts(pckIiAdjs[i])\n","\n","\t\tfor i in range(steps):\n","\t\t\tst = i * tstBat\n","\t\t\ted = min((i+1) * tstBat, num)\n","\t\t\tbatIds = ids[st: ed]\n","\t\t\tuLocs, iLocs, temTst, tstLocs = self.sampleTestBatch(batIds, pckLabel, pckTstInt)\n","\t\t\tfeeddict[self.uids] = uLocs\n","\t\t\tfeeddict[self.iids] = iLocs\n","\t\t\tpreds = self.sess.run(self.pred, feed_dict=feeddict, options=config_pb2.RunOptions(report_tensor_allocations_upon_oom=True))\n","\t\t\thit, ndcg = self.calcRes(np.reshape(preds, [ed-st, 100]), temTst, tstLocs)\n","\t\t\tepochHit += hit\n","\t\t\tepochNdcg += ndcg\n","\t\t\tlog('Steps %d/%d: hit = %d, ndcg = %d          ' % (i, steps, hit, ndcg), save=False, oneline=True)\n","\t\tret = dict()\n","\t\tret['HR'] = epochHit / num\n","\t\tret['NDCG'] = epochNdcg / num\n","\t\treturn ret\n","\n","\tdef calcRes(self, preds, temTst, tstLocs):\n","\t\thit = 0\n","\t\tndcg = 0\n","\t\tfor j in range(preds.shape[0]):\n","\t\t\tpredvals = list(zip(preds[j], tstLocs[j]))\n","\t\t\tpredvals.sort(key=lambda x: x[0], reverse=True)\n","\t\t\tshoot = list(map(lambda x: x[1], predvals[:args.shoot]))\n","\t\t\tif temTst[j] in shoot:\n","\t\t\t\thit += 1\n","\t\t\t\tndcg += np.reciprocal(np.log2(shoot.index(temTst[j])+2))\n","\t\treturn hit, ndcg\n","\t\n","\tdef saveHistory(self):\n","\t\tif args.epoch == 0:\n","\t\t\treturn\n","\t\twith open('History/' + args.save_path + '.his', 'wb') as fs:\n","\t\t\tpickle.dump(self.metrics, fs)\n","\n","\t\tsaver = tf.train.Saver()\n","\t\tsaver.save(self.sess, 'Models/' + args.save_path)\n","\t\tlog('Model Saved: %s' % args.save_path)\n","\n","\tdef loadModel(self):\n","\t\tsaver = tf.train.Saver()\n","\t\tsaver.restore(sess, 'Models/' + args.load_model)\n","\t\twith open('History/' + args.load_model + '.his', 'rb') as fs:\n","\t\t\tself.metrics = pickle.load(fs)\n","\t\tlog('Model Loaded')\t\n","\n","if __name__ == '__main__':\n","\tsaveDefault = True\n","\tconfig = tf.ConfigProto()\n","\tconfig.gpu_options.allow_growth = True\n","\n","\tlog('Start')\n","\tdatas = LoadData()\n","\tlog('Load Data')\n","\n","\twith tf.Session(config=config) as sess:\n","\t\trecom = Recommender(sess, datas)\n","\t\trecom.run()"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["2021-10-13 15:04:10.039967: Start\n","MAX TIME 1094\n","2021-10-13 15:04:10.096106: Load Data\n","USER 19800 ITEM 22734\n","WARNING:tensorflow:From <ipython-input-7-ecd2f04a1ae1>:136: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use `tf.cast` instead.\n","WARNING:tensorflow:From /tensorflow-1.15.2/python3.7/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use tf.where in 2.0, which has the same broadcast rule as np.where\n"]},{"output_type":"stream","name":"stderr","text":["/tensorflow-1.15.2/python3.7/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n","  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n","/tensorflow-1.15.2/python3.7/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n","  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n","/tensorflow-1.15.2/python3.7/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n","  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n","/tensorflow-1.15.2/python3.7/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n","  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n","/tensorflow-1.15.2/python3.7/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n","  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n","/tensorflow-1.15.2/python3.7/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n","  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n","/tensorflow-1.15.2/python3.7/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n","  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"]},{"output_type":"stream","name":"stdout","text":["2021-10-13 15:04:34.029657: Model Prepared\n","2021-10-13 15:04:41.483301: Varaibles Inited\n","2021-10-13 15:08:38.798118: Epoch 0/12, Train: Loss = 18.5635, preLoss = 11.4253  \n","2021-10-13 15:12:19.264704: Epoch 0/12, Test: HR = 0.7615, NDCG = 0.4565  \n","2021-10-13 15:12:23.154377: Model Saved: /content\n","\n","2021-10-13 15:16:07.419656: Epoch 1/12, Train: Loss = 10.5176, preLoss = 5.8621  \n","\n","2021-10-13 15:19:52.159093: Epoch 2/12, Train: Loss = 8.7574, preLoss = 4.9491  \n","\n","2021-10-13 15:23:36.753966: Epoch 3/12, Train: Loss = 7.8547, preLoss = 4.5318  \n","2021-10-13 15:27:14.745525: Epoch 3/12, Test: HR = 0.8325, NDCG = 0.5306  \n","\n","2021-10-13 15:30:59.128221: Epoch 4/12, Train: Loss = 7.0885, preLoss = 4.0844  \n","\n","2021-10-13 15:34:43.731039: Epoch 5/12, Train: Loss = 6.6603, preLoss = 3.8928  \n","2021-10-13 15:34:47.116829: Model Saved: /content\n","\n","2021-10-13 15:38:31.044468: Epoch 6/12, Train: Loss = 6.2040, preLoss = 3.6275  \n","2021-10-13 15:42:09.733204: Epoch 6/12, Test: HR = 0.8523, NDCG = 0.5521  \n","\n","2021-10-13 15:45:53.872000: Epoch 7/12, Train: Loss = 5.9576, preLoss = 3.5394  \n","\n","2021-10-13 15:49:37.597534: Epoch 8/12, Train: Loss = 5.7873, preLoss = 3.4780  \n","\n","2021-10-13 15:53:19.679767: Epoch 9/12, Train: Loss = 5.6110, preLoss = 3.4042  \n","2021-10-13 15:56:56.951197: Epoch 9/12, Test: HR = 0.8489, NDCG = 0.5583  \n","\n","2021-10-13 16:00:41.228502: Epoch 10/12, Train: Loss = 5.4236, preLoss = 3.3165  \n","2021-10-13 16:00:44.503677: Model Saved: /content\n","\n","2021-10-13 16:04:27.882297: Epoch 11/12, Train: Loss = 5.2494, preLoss = 3.2395  \n","\n","2021-10-13 16:08:07.839769: Epoch 12/12, Test: HR = 0.8541, NDCG = 0.5591  \n","2021-10-13 16:08:11.216854: Model Saved: /content\n"]}]}]}