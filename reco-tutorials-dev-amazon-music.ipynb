{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"reco-tutorials-dev-amazon-music.ipynb","provenance":[],"collapsed_sections":[],"mount_file_id":"1CUoG8eX4oI2_9qOWQYIj4g-N_49fPGB5","authorship_tag":"ABX9TyOpXqyjTAUdPa8g1ZApURP6"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zaRyPR2aPsNL","executionInfo":{"status":"ok","timestamp":1622487537613,"user_tz":-330,"elapsed":412,"user":{"displayName":"Sparsh Agarwal","photoUrl":"","userId":"13037694610922482904"}},"outputId":"e8cb03ea-e5d0-40c1-b3e1-aa6002e21355"},"source":["# !git clone https://github.com/MrRezaeiUofT/Amazon-Product-Recommender.git\n","# !cd Amazon-Product-Recommender && zip -s- Dataset.zip -O /content/AmazonDataset\n","# !unzip AmazonDataset.zip\n","# !mkdir AmazonMusicRatings && cd AmazonMusicRatings && \\\n","# cp /content/train.json/train.json . && \\\n","# cp /content/test.json/test.json . && \\\n","# cp /content/rating_pairs.csv .\n","# %cd AmazonMusicRatings"],"execution_count":43,"outputs":[{"output_type":"stream","text":["/content/AmazonMusicRatings\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"r-_QIw7Fq0hR","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1622493335680,"user_tz":-330,"elapsed":436,"user":{"displayName":"Sparsh Agarwal","photoUrl":"","userId":"13037694610922482904"}},"outputId":"ad5f1cee-d9c4-4671-e973-9a317176edbe"},"source":["from tqdm import tqdm\n","from tensorflow.keras.preprocessing.text import Tokenizer, text_to_word_sequence\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","import pandas as pd\n","import tensorflow.keras.backend as K\n","import tensorflow as tf\n","import string\n","import snowballstemmer\n","import nltk\n","import string\n","import numpy as np\n","import pandas as pd\n","from nltk.corpus import stopwords\n","import re\n","from sklearn.manifold import TSNE\n","import numpy as np\n","from collections import defaultdict\n","import json\n","from collections import defaultdict\n","import json\n","import pandas as pd\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","import numpy as np\n","from tqdm import tqdm\n","from tensorflow.keras.preprocessing.text import Tokenizer, text_to_word_sequence\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from sklearn.utils import resample\n","import matplotlib.pyplot as plt\n","from tensorflow.keras.layers import Convolution2D,MaxPooling2D,Dropout,Flatten,Dense,TimeDistributed,Input, LSTM\n","from tensorflow.keras.models import Model\n","from tensorflow.keras.optimizers import Adam, RMSprop\n","np.random.seed(0)\n","from sklearn.feature_extraction.text import CountVectorizer\n","from sklearn.model_selection import train_test_split\n","from collections import defaultdict\n","import json\n","import pandas as pd\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","import numpy as np\n","from tqdm import tqdm\n","from tensorflow.keras.preprocessing.text import Tokenizer, text_to_word_sequence\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from sklearn.utils import resample\n","import matplotlib.pyplot as plt\n","from tensorflow.keras.layers import Convolution2D,MaxPooling2D,Dropout,Flatten,Dense,TimeDistributed,Input, LSTM\n","from tensorflow.keras.models import Model\n","from tensorflow.keras.optimizers import Adam, RMSprop\n","np.random.seed(0)\n","from sklearn.feature_extraction.text import CountVectorizer\n","from sklearn.model_selection import train_test_split\n","from collections import defaultdict\n","import json\n","import pandas as pd\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","import numpy as np\n","from tqdm import tqdm\n","from tensorflow.keras.preprocessing.text import Tokenizer, text_to_word_sequence\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","\n","import matplotlib.pyplot as plt\n","from tensorflow.keras.layers import Convolution2D,MaxPooling2D,Dropout,Flatten,Dense,TimeDistributed,Input, LSTM\n","from tensorflow.keras.models import Model\n","from tensorflow.keras.optimizers import Adam, RMSprop\n","np.random.seed(0)\n","from sklearn.feature_extraction.text import CountVectorizer\n","from sklearn.model_selection import train_test_split\n","import nltk\n","nltk.download('stopwords')"],"execution_count":96,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{"tags":[]},"execution_count":96}]},{"cell_type":"code","metadata":{"id":"Tws-ZYtnq3ur"},"source":["MAX_WORD_TO_USE = 1000 # how many words to use in training\n","MAX_WORD_TO_USE_other=100\n","MAX_LEN = 100 # number of time-steps."],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ifpmYdPbq8cG"},"source":["def Text_cleaner(Data):\n","    HoleText = []\n","    separator = ' '\n","    for i in range(Data.shape[0]):\n","        HoleText.append(separator.join(text_to_word_sequence(str(Data[i]))))\n","    x_c = HoleText\n","    return x_c"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Fw6wB4_7rAeH"},"source":["def Text_length(Data):\n","    HoleText = []\n","    for i in range(Data.shape[0]):\n","        HoleText.append(text_to_word_sequence(str(Data[i])))\n","    x_c = HoleText\n","    x_c_l=[len(x_c[i]) for i in range(len(x_c))]\n","    return x_c_l"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"PYJSp4MorJvy"},"source":["def weighted_categorical_crossentropy(weights):\n","    \"\"\"\n","    A weighted version of keras.objectives.categorical_crossentropy\n","\n","    Variables:\n","        weights: numpy array of shape (C,) where C is the number of classes\n","\n","    Usage:\n","        weights = np.array([0.5,2,10]) # Class one at 0.5, class 2 twice the normal weights, class 3 10x.\n","        loss = weighted_categorical_crossentropy(weights)\n","        model.compile(loss=loss,optimizer='adam')\n","    \"\"\"\n","\n","    weights = K.variable(weights)\n","\n","    def loss(y_true, y_pred):\n","        # scale predictions so that the class probas of each sample sum to 1\n","        y_pred /= K.sum(y_pred, axis=-1, keepdims=True)\n","        # clip to prevent NaN's and Inf's\n","        y_pred = K.clip(y_pred, K.epsilon(), 1 - K.epsilon())\n","        # calc\n","        loss = y_true * K.log(y_pred) * weights\n","        loss = -K.sum(loss, -1)\n","        return loss\n","\n","    return loss"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5ZDPDvmirMwu"},"source":["def weighted_mse(weights):\n","    \"\"\"\n","    A weighted version of MSE\n","    Variables:\n","        weights: numpy array of shape (C,) where C is the number of classes\n","\n","    Usage:\n","        weights = np.array([0.5,2,10]) # Class one at 0.5, class 2 twice the normal weights, class 3 10x.\n","\n","        model.compile(loss=loss,optimizer='adam')\n","    \"\"\"\n","\n","    weights = K.variable(weights)\n","\n","    def loss(y_true, y_pred):\n","        # scale predictions so that the class probas of each sample sum to 1\n","        y_pred /= K.sum(y_pred, axis=-1, keepdims=True)\n","        # clip to prevent NaN's and Inf's\n","        # y_pred = K.clip(y_pred, K.epsilon(), 1 - K.epsilon())\n","        # calc\n","        loss = tf.reduce_logsumexp(tf.pow(y_true - y_pred,2) * weights)\n","\n","        return loss\n","\n","    return loss"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"XM1QF8Y3qSPt"},"source":["def clean_text(text):\n","    ## Remove puncuation\n","    text = text.translate(string.punctuation)\n","\n","    ## Convert words to lower case and split them\n","    text = text.lower().split()\n","\n","    ## Remove stop words\n","    stops = set(stopwords.words(\"english\"))\n","    text = [w for w in text if not w in stops and len(w) >= 3]\n","\n","    text = \" \".join(text)\n","    ## Clean the text\n","    text = re.sub(r\"[^A-Za-z0-9^,!.\\/'+-=]\", \" \", text)\n","    text = re.sub(r\"what's\", \"what is \", text)\n","    text = re.sub(r\"\\'s\", \" \", text)\n","    text = re.sub(r\"\\'ve\", \" have \", text)\n","    text = re.sub(r\"n't\", \" not \", text)\n","    text = re.sub(r\"i'm\", \"i am \", text)\n","    text = re.sub(r\"\\'re\", \" are \", text)\n","    text = re.sub(r\"\\'d\", \" would \", text)\n","    text = re.sub(r\"\\'ll\", \" will \", text)\n","    text = re.sub(r\",\", \" \", text)\n","    text = re.sub(r\"\\.\", \" \", text)\n","    text = re.sub(r\"!\", \" ! \", text)\n","    text = re.sub(r\"\\/\", \" \", text)\n","    text = re.sub(r\"\\^\", \" ^ \", text)\n","    text = re.sub(r\"\\+\", \" + \", text)\n","    text = re.sub(r\"\\-\", \" - \", text)\n","    text = re.sub(r\"\\=\", \" = \", text)\n","    text = re.sub(r\"'\", \" \", text)\n","    text = re.sub(r\"(\\d+)(k)\", r\"\\g<1>000\", text)\n","    text = re.sub(r\":\", \" : \", text)\n","    text = re.sub(r\" e g \", \" eg \", text)\n","    text = re.sub(r\" b g \", \" bg \", text)\n","    text = re.sub(r\" u s \", \" american \", text)\n","    text = re.sub(r\"\\0s\", \"0\", text)\n","    text = re.sub(r\" 9 11 \", \"911\", text)\n","    text = re.sub(r\"e - mail\", \"email\", text)\n","    text = re.sub(r\"j k\", \"jk\", text)\n","    text = re.sub(r\"\\s{2,}\", \" \", text)\n","    # Stemming\n","    text = text.split()\n","    stemmer = nltk.stem.SnowballStemmer('english')\n","    stemmed_words = [stemmer.stem(word) for word in text]\n","    text = \" \".join(stemmed_words)\n","\n","    return text"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"EAesjcp3rP0r","executionInfo":{"status":"ok","timestamp":1622487745212,"user_tz":-330,"elapsed":644,"user":{"displayName":"Sparsh Agarwal","photoUrl":"","userId":"13037694610922482904"}}},"source":["# allRatings = []\n","# userRatings = defaultdict(list)\n","\n","# with open('train.json', 'r') as train_file:\n","#     for row in train_file:\n","#         data = json.loads(row)\n","#         r = float(data['overall'])\n","#         allRatings.append(r)\n","#         userRatings[data['reviewerID']].append(r)\n","\n","# globalAverage = sum(allRatings)/len(allRatings)\n","# userAverage = {}\n","# for u in userRatings:\n","#     userAverage[u] = sum(userRatings[u]) / len(userRatings[u])\n","\n","# predictions = open('rating_predictions.csv', 'w')\n","# for l in open('rating_pairs.csv'):\n","#     if l.startswith('userID'):\n","#         #header\n","#         predictions.write(l)\n","#         continue\n","#     u,p = l.strip().split('-')\n","#     if u in userAverage:\n","#         predictions.write(u + '-' + p + ',' + str(userAverage[u]) + '\\n')\n","#     else:\n","#         predictions.write(u + '-' + p + ',' + str(globalAverage) + '\\n')\n","# predictions.close()"],"execution_count":48,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":564},"id":"fv0Wk5Qb5Zbe","executionInfo":{"status":"ok","timestamp":1622492070125,"user_tz":-330,"elapsed":2079,"user":{"displayName":"Sparsh Agarwal","photoUrl":"","userId":"13037694610922482904"}},"outputId":"306a4e89-a0a1-4b09-f82c-2bea5b222d19"},"source":["data = []\n","bad_records = 0\n","with open('train.json') as f:\n","    for line in f:\n","      try:\n","        data.append(json.loads(line))\n","      except:\n","        bad_records+=1\n","        pass\n","print('Bad records skipped: {}'.format(bad_records))\n","\n","df = pd.DataFrame.from_records(data).reset_index()\n","df.head()"],"execution_count":88,"outputs":[{"output_type":"stream","text":["Bad records skipped: 53\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>index</th>\n","      <th>overall</th>\n","      <th>reviewTime</th>\n","      <th>reviewerID</th>\n","      <th>reviewText</th>\n","      <th>summary</th>\n","      <th>unixReviewTime</th>\n","      <th>category</th>\n","      <th>price</th>\n","      <th>itemID</th>\n","      <th>reviewHash</th>\n","      <th>image</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>4.0</td>\n","      <td>08 24, 2010</td>\n","      <td>u04428712</td>\n","      <td>So is Katy Perry's new album \"Teenage Dream\" c...</td>\n","      <td>Amazing that I Actually Bought This...More Ama...</td>\n","      <td>1282608000</td>\n","      <td>Pop</td>\n","      <td>$35.93</td>\n","      <td>p70761125</td>\n","      <td>85559980</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1</td>\n","      <td>5.0</td>\n","      <td>10 31, 2009</td>\n","      <td>u06946603</td>\n","      <td>I got this CD almost 10 years ago, and given t...</td>\n","      <td>Excellent album</td>\n","      <td>1256947200</td>\n","      <td>Alternative Rock</td>\n","      <td>$11.28</td>\n","      <td>p85427891</td>\n","      <td>41699565</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>2</td>\n","      <td>4.0</td>\n","      <td>10 13, 2015</td>\n","      <td>u92735614</td>\n","      <td>I REALLY enjoy this pairing of Anderson and Po...</td>\n","      <td>Love the Music, Hate the Light Show</td>\n","      <td>1444694400</td>\n","      <td>Pop</td>\n","      <td>$89.86</td>\n","      <td>p82172532</td>\n","      <td>24751194</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>3</td>\n","      <td>5.0</td>\n","      <td>06 28, 2017</td>\n","      <td>u35112935</td>\n","      <td>Finally got it . It was everything thought it ...</td>\n","      <td>Great</td>\n","      <td>1498608000</td>\n","      <td>Pop</td>\n","      <td>$11.89</td>\n","      <td>p15255251</td>\n","      <td>22820631</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>4</td>\n","      <td>4.0</td>\n","      <td>10 12, 2015</td>\n","      <td>u07141505</td>\n","      <td>Look at all star cast.  Outstanding record, pl...</td>\n","      <td>Love these guys.</td>\n","      <td>1444608000</td>\n","      <td>Jazz</td>\n","      <td>$15.24</td>\n","      <td>p82618188</td>\n","      <td>53377470</td>\n","      <td>NaN</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   index  overall   reviewTime reviewerID  ...   price     itemID  reviewHash image\n","0      0      4.0  08 24, 2010  u04428712  ...  $35.93  p70761125    85559980   NaN\n","1      1      5.0  10 31, 2009  u06946603  ...  $11.28  p85427891    41699565   NaN\n","2      2      4.0  10 13, 2015  u92735614  ...  $89.86  p82172532    24751194   NaN\n","3      3      5.0  06 28, 2017  u35112935  ...  $11.89  p15255251    22820631   NaN\n","4      4      4.0  10 12, 2015  u07141505  ...  $15.24  p82618188    53377470   NaN\n","\n","[5 rows x 12 columns]"]},"metadata":{"tags":[]},"execution_count":88}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":408},"id":"VvAmdI0GEDxB","executionInfo":{"status":"ok","timestamp":1622492099068,"user_tz":-330,"elapsed":430,"user":{"displayName":"Sparsh Agarwal","photoUrl":"","userId":"13037694610922482904"}},"outputId":"e2011e93-e6a9-43ad-f71f-d2a286769dd0"},"source":["rate_pair_df = pd.read_json('test.json', lines=True)\n","rate_pair_df.head()"],"execution_count":89,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>reviewTime</th>\n","      <th>reviewerID</th>\n","      <th>reviewText</th>\n","      <th>summary</th>\n","      <th>unixReviewTime</th>\n","      <th>category</th>\n","      <th>price</th>\n","      <th>itemID</th>\n","      <th>reviewHash</th>\n","      <th>image</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>03 26, 2015</td>\n","      <td>u32476110</td>\n","      <td>Fantastic mix of \"old school\" with a creative ...</td>\n","      <td>Fantastic mix of \"old school\" with a creative ...</td>\n","      <td>1427328000</td>\n","      <td>Pop</td>\n","      <td>$14.98</td>\n","      <td>p76243483</td>\n","      <td>20167847</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>05 15, 2017</td>\n","      <td>u36732410</td>\n","      <td>Update: Indications\\nThere are various opinion...</td>\n","      <td>Digitally Extracted Stereo (DES) Rules!</td>\n","      <td>1494806400</td>\n","      <td>Pop</td>\n","      <td>$15.16</td>\n","      <td>p92485419</td>\n","      <td>30527605</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>06 4, 2015</td>\n","      <td>u85385007</td>\n","      <td>This album provides a new twist on old Sammy H...</td>\n","      <td>Excellent unplugged album</td>\n","      <td>1433376000</td>\n","      <td>Pop</td>\n","      <td>$7.37</td>\n","      <td>p40031588</td>\n","      <td>12169432</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>04 23, 2009</td>\n","      <td>u30715529</td>\n","      <td>(Symbol) can be considered as another masterpi...</td>\n","      <td>another masterpiece</td>\n","      <td>1240444800</td>\n","      <td>Pop</td>\n","      <td>$12.45</td>\n","      <td>p88719785</td>\n","      <td>55648615</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>09 15, 2000</td>\n","      <td>u95909892</td>\n","      <td>Many would think this album is good only becau...</td>\n","      <td>True Classic Rock</td>\n","      <td>968976000</td>\n","      <td>Alternative Rock</td>\n","      <td>$2.07</td>\n","      <td>p59188380</td>\n","      <td>9520938</td>\n","      <td>NaN</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["    reviewTime reviewerID  ... reviewHash image\n","0  03 26, 2015  u32476110  ...   20167847   NaN\n","1  05 15, 2017  u36732410  ...   30527605   NaN\n","2   06 4, 2015  u85385007  ...   12169432   NaN\n","3  04 23, 2009  u30715529  ...   55648615   NaN\n","4  09 15, 2000  u95909892  ...    9520938   NaN\n","\n","[5 rows x 10 columns]"]},"metadata":{"tags":[]},"execution_count":89}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":204},"id":"FjIi3RRYEOes","executionInfo":{"status":"ok","timestamp":1622492131060,"user_tz":-330,"elapsed":395,"user":{"displayName":"Sparsh Agarwal","photoUrl":"","userId":"13037694610922482904"}},"outputId":"ab2e2dc4-6b94-4810-aa42-8f5d6d09791b"},"source":["rate_pair_id = pd.read_csv('rating_pairs.csv')\n","rate_pair_id.head()"],"execution_count":90,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>userID-itemID</th>\n","      <th>prediction</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>u32476110-p76243483</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>u36732410-p92485419</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>u85385007-p40031588</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>u30715529-p88719785</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>u95909892-p59188380</td>\n","      <td>NaN</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["         userID-itemID  prediction\n","0  u32476110-p76243483         NaN\n","1  u36732410-p92485419         NaN\n","2  u85385007-p40031588         NaN\n","3  u30715529-p88719785         NaN\n","4  u95909892-p59188380         NaN"]},"metadata":{"tags":[]},"execution_count":90}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8hkH3HuIEUsQ","executionInfo":{"status":"ok","timestamp":1622492158622,"user_tz":-330,"elapsed":425,"user":{"displayName":"Sparsh Agarwal","photoUrl":"","userId":"13037694610922482904"}},"outputId":"8522039d-8dea-4dcf-d6bf-280bd87fa264"},"source":["compensate_number = df.overall.value_counts()[5]\n","compensate_number"],"execution_count":91,"outputs":[{"output_type":"execute_result","data":{"text/plain":["81710"]},"metadata":{"tags":[]},"execution_count":91}]},{"cell_type":"code","metadata":{"id":"C9Ewm3-lzxNz","executionInfo":{"status":"ok","timestamp":1622492164479,"user_tz":-330,"elapsed":415,"user":{"displayName":"Sparsh Agarwal","photoUrl":"","userId":"13037694610922482904"}}},"source":["# MaxWordLength=50\n","# Batch_Size = 20000\n","# Dp_rate=0.2\n","# Epochs=200\n","# vocabulary_size = 2000"],"execution_count":92,"outputs":[]},{"cell_type":"code","metadata":{"id":"RkrDMxFLF2SI","executionInfo":{"status":"ok","timestamp":1622494855997,"user_tz":-330,"elapsed":658,"user":{"displayName":"Sparsh Agarwal","photoUrl":"","userId":"13037694610922482904"}}},"source":["def preprocess(train, test, vocabulary_size):\n","\n","  train.reset_index(drop=True, inplace=True)\n","  test.reset_index(drop=True, inplace=True)\n","\n","  # combine summary and review text columns\n","  text = []\n","  for i in range(train.shape[0]):\n","    text.append(clean_text(str(train['summary'][i]) + str(train['reviewText'][i])))\n","  for i in range(test.shape[0]):\n","    text.append(clean_text(str(test['summary'][i]) + str(test['reviewText'][i])))\n","\n","  # text tokenization\n","  tokenizer = Tokenizer(num_words=vocabulary_size)\n","  tokenizer.fit_on_texts(text)\n","  F_text = tokenizer.texts_to_matrix(text, mode='freq')\n","\n","  F_text = F_text[:train.shape[0],:]\n","  F_text_te = F_text[train.shape[0]:,:]\n","\n","  # time normalization\n","  F_time = (train.unixReviewTime.values - train.unixReviewTime.min())/(train.unixReviewTime.max() - train.unixReviewTime.min())\n","  F_time = F_time.reshape([-1,1])\n","\n","  # dummy encoding of category column\n","  F_cat = pd.get_dummies(train.category).values\n","\n","  newD = pd.DataFrame(np.concatenate([train['itemID'].values, test['itemID'].values], axis=0), columns=['itemID'])\n","  newD['reviewerID'] = np.concatenate([train['reviewerID'].values, test['reviewerID'].values], axis=0)\n","  newD['itemID'] = newD['itemID'].groupby(newD['itemID']).transform('count')\n","  newD['reviewerID'] = newD['reviewerID'].groupby(newD['reviewerID']).transform('count')\n","\n","  RID_temp = pd.get_dummies(newD['reviewerID']).values\n","  F_newD_reviewerID = RID_temp[:train.shape[0],:]\n","  F_newD_reviewerID_te = RID_temp[train.shape[0]:,:]\n","\n","  IID_temp = pd.get_dummies(newD['itemID']).values\n","  F_newD_itemID = IID_temp[:train.shape[0],:]\n","  F_newD_itemID_te = IID_temp[train.shape[0]:,:]\n","\n","  vocab_size_RID=RID_temp.max()+1\n","  vocab_size_IID=IID_temp.max()+1\n","\n","  return F_text, F_time, F_cat, F_newD_reviewerID, F_newD_itemID, F_newD_itemID_te, vocab_size_RID, vocab_size_IID"],"execution_count":121,"outputs":[]},{"cell_type":"code","metadata":{"id":"Sg4aFxqn0QBX","executionInfo":{"status":"ok","timestamp":1622494342961,"user_tz":-330,"elapsed":2287,"user":{"displayName":"Sparsh Agarwal","photoUrl":"","userId":"13037694610922482904"}}},"source":["temp_train = df.sample(1000).reset_index(drop=True)\n","temp_test = rate_pair_df.sample(100).reset_index(drop=True)\n","\n","F_text, F_time, F_cat, F_newD_reviewerID, F_newD_itemID, _, _, _ = preprocess(temp_train, temp_test, vocabulary_size)\n","\n","y = temp_train.overall.to_numpy()\n","X = np.concatenate([F_text, F_time, F_cat, F_newD_reviewerID, F_newD_itemID], axis=-1)\n","\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=101)"],"execution_count":112,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1vw5WjcFKYhc","executionInfo":{"status":"ok","timestamp":1622494344640,"user_tz":-330,"elapsed":10,"user":{"displayName":"Sparsh Agarwal","photoUrl":"","userId":"13037694610922482904"}},"outputId":"c75d569e-438b-4ade-a9e0-34759a9b067f"},"source":["########### naive_bayes\n","from sklearn.naive_bayes import MultinomialNB\n","from sklearn import metrics\n","nb = MultinomialNB()\n","nb.fit(X_train, y_train)\n","nb_preds = nb.predict(X_test)\n","print('Naive_bayesian RMS=%f'%(metrics.mean_squared_error(y_test, nb_preds)))"],"execution_count":113,"outputs":[{"output_type":"stream","text":["Naive_bayesian RMS=1.356667\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TUtszP_JKbWx","executionInfo":{"status":"ok","timestamp":1622494354692,"user_tz":-330,"elapsed":4467,"user":{"displayName":"Sparsh Agarwal","photoUrl":"","userId":"13037694610922482904"}},"outputId":"3ce3a0c0-45ab-447d-e905-ed160e6b6248"},"source":["########### SVM\n","from sklearn.linear_model import LogisticRegression\n","logreg = LogisticRegression(C=1e5)\n","# Create an instance of Logistic Regression Classifier and fit the data.\n","logreg.fit(X_train, y_train)\n","rl_preds=logreg.predict(X_test)\n","print('Logistic Regression RMS=%f'%(metrics.mean_squared_error(y_test,rl_preds)))"],"execution_count":114,"outputs":[{"output_type":"stream","text":["Logistic Regression RMS=1.116667\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n","STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n","\n","Increase the number of iterations (max_iter) or scale the data as shown in:\n","    https://scikit-learn.org/stable/modules/preprocessing.html\n","Please also refer to the documentation for alternative solver options:\n","    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n","  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bU0sjMisOyoh","executionInfo":{"status":"ok","timestamp":1622494904340,"user_tz":-330,"elapsed":539,"user":{"displayName":"Sparsh Agarwal","photoUrl":"","userId":"13037694610922482904"}},"outputId":"ca1a85f9-343e-4f46-fe7b-70eebc676ea4"},"source":["# x = preprocess(temp_train, temp_test, vocabulary_size)\n"],"execution_count":125,"outputs":[{"output_type":"execute_result","data":{"text/plain":["8"]},"metadata":{"tags":[]},"execution_count":125}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GZ8geHQ3LawE","outputId":"1cc54bcd-d769-48c4-f779-78903c0752d7"},"source":["MaxWordLength=50\n","Batch_Size = 10000\n","Dp_rate=0.5\n","Epochs=10\n","vocabulary_size = 5000\n","\n","temp_train = df.sample(1000).reset_index(drop=True)\n","temp_test = rate_pair_df.sample(100).reset_index(drop=True)\n","\n","F_text, F_time, F_cat, F_newD_reviewerID, F_newD_itemID, F_newD_itemID_te, vocab_size_RID, vocab_size_IID = preprocess(temp_train, temp_test, vocabulary_size)\n","\n","''' Model Selection'''\n","x_text = Input(shape=(F_text.shape[1]),name='x_text')\n","x_time = Input(shape=(F_time.shape[1]),name='x_time')\n","x_cat = Input(shape=(F_cat.shape[1]),name='x_cat')\n","x_RID = Input(shape=(F_newD_reviewerID.shape[1]),name='x_RID')\n","x_IID = Input(shape=(F_newD_itemID_te.shape[1]),name='x_IID')\n","\n","H_t=tf.keras.layers.Embedding(vocabulary_size, 100)(x_text)\n","H_t=LSTM(100, dropout=Dp_rate, recurrent_dropout=Dp_rate)(H_t)\n","\n","H_RID=tf.keras.layers.Embedding(vocab_size_RID, 20)(x_RID)\n","H_RID=tf.reshape(H_RID, (-1,20))\n","\n","H_IID=tf.keras.layers.Embedding(vocab_size_IID, 20)(x_IID)\n","H_IID=tf.reshape(H_IID, (-1,20))\n","\n","H_c=Dense(5,activation='relu')(x_cat)\n","H_c=Dropout(Dp_rate)(H_c)\n","\n","H=tf.concat([H_t,H_RID,H_IID,x_cat,x_time],axis=-1)\n","\n","# Final predictions and model.\n","prediction = Dense(1, activation='sigmoid')(H)\n","model = Model(inputs=[x_text,x_time,x_cat,x_RID,x_IID], outputs= prediction)\n","optimizer = Adam(lr=0.01, beta_1=0.9, beta_2=0.999)\n","model.compile(loss=tf.keras.losses.MSE,\n","              optimizer=optimizer,metrics=['mse'])\n","model.summary()\n","\n","''' Train mode'''\n","history=model.fit([F_text,F_time,F_cat,F_newD_reviewerID,F_newD_itemID], y/5,\n","          batch_size=Batch_Size,\n","          epochs=Epochs,\n","          verbose=1,\n","          validation_split=.3, shuffle=True)\n","plt.figure(figsize=(12,5))\n","\n","plt.plot(history.history['loss'])\n","plt.plot(history.history['val_loss'])\n","plt.title('model loss')\n","plt.ylabel('loss')\n","plt.xlabel('epoch')\n","plt.legend(['train', 'valid'], loc='lower right')\n","plt.show()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py:375: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n","  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"],"name":"stderr"},{"output_type":"stream","text":["Model: \"model\"\n","__________________________________________________________________________________________________\n","Layer (type)                    Output Shape         Param #     Connected to                     \n","==================================================================================================\n","x_text (InputLayer)             [(None, 5000)]       0                                            \n","__________________________________________________________________________________________________\n","x_RID (InputLayer)              [(None, 5)]          0                                            \n","__________________________________________________________________________________________________\n","x_IID (InputLayer)              [(None, 4)]          0                                            \n","__________________________________________________________________________________________________\n","embedding_3 (Embedding)         (None, 5000, 100)    500000      x_text[0][0]                     \n","__________________________________________________________________________________________________\n","embedding_4 (Embedding)         (None, 5, 20)        40          x_RID[0][0]                      \n","__________________________________________________________________________________________________\n","embedding_5 (Embedding)         (None, 4, 20)        40          x_IID[0][0]                      \n","__________________________________________________________________________________________________\n","lstm_2 (LSTM)                   (None, 100)          80400       embedding_3[0][0]                \n","__________________________________________________________________________________________________\n","tf.reshape_1 (TFOpLambda)       (None, 20)           0           embedding_4[0][0]                \n","__________________________________________________________________________________________________\n","tf.reshape_2 (TFOpLambda)       (None, 20)           0           embedding_5[0][0]                \n","__________________________________________________________________________________________________\n","x_cat (InputLayer)              [(None, 5)]          0                                            \n","__________________________________________________________________________________________________\n","x_time (InputLayer)             [(None, 1)]          0                                            \n","__________________________________________________________________________________________________\n","tf.concat (TFOpLambda)          (None, 146)          0           lstm_2[0][0]                     \n","                                                                 tf.reshape_1[0][0]               \n","                                                                 tf.reshape_2[0][0]               \n","                                                                 x_cat[0][0]                      \n","                                                                 x_time[0][0]                     \n","__________________________________________________________________________________________________\n","dense_1 (Dense)                 (None, 1)            147         tf.concat[0][0]                  \n","==================================================================================================\n","Total params: 580,627\n","Trainable params: 580,627\n","Non-trainable params: 0\n","__________________________________________________________________________________________________\n","Epoch 1/10\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"BW2-8ZmozN-z"},"source":["''' Model Selection'''\n","x_text = Input(shape=(F_text.shape[1]),name='x_text')\n","x_time = Input(shape=(F_time.shape[1]),name='x_time')\n","x_cat = Input(shape=(F_cat.shape[1]),name='x_cat')\n","x_RID = Input(shape=(F_newD_reviewerID.shape[1]),name='x_RID')\n","x_IID = Input(shape=(F_newD_itemID_te.shape[1]),name='x_IID')\n","\n","H_t=tf.keras.layers.Embedding(vocabulary_size, 100)(x_text)\n","H_t=LSTM(100, dropout=Dp_rate, recurrent_dropout=Dp_rate)(H_t)\n","\n","\n","H_RID=tf.keras.layers.Embedding(vocab_size_RID, 20)(x_RID)\n","H_RID=tf.reshape(H_RID, (-1,20))\n","\n","\n","H_IID=tf.keras.layers.Embedding(vocab_size_IID, 20)(x_IID)\n","H_IID=tf.reshape(H_IID, (-1,20))\n","\n","H_c=Dense(5,activation='relu')(x_cat)\n","H_c=Dropout(Dp_rate)(H_c)\n","\n","H=tf.concat([H_t,H_RID,H_IID,x_cat,x_time],axis=-1)\n","\n","# Final predictions and model.\n","prediction = Dense(1, activation='sigmoid')(H)\n","model = Model(inputs=[x_text,x_time,x_cat,x_RID,x_IID], outputs= prediction)\n","optimizer = Adam(lr=0.01, beta_1=0.9, beta_2=0.999)\n","model.compile(loss=tf.keras.losses.MSE,\n","              optimizer=optimizer,metrics=['mse'])\n","model.summary()\n","\n","''' Train mode'''\n","history=model.fit([F_text,F_time,F_cat,F_newD_reviewerID,F_newD_itemID], y/5,\n","          batch_size=Batch_Size,\n","          epochs=Epochs,\n","          verbose=1,\n","          validation_split=.3, shuffle=True)\n","plt.figure(figsize=(12,5))\n","\n","plt.plot(history.history['loss'])\n","plt.plot(history.history['val_loss'])\n","plt.title('model loss')\n","plt.ylabel('loss')\n","plt.xlabel('epoch')\n","plt.legend(['train', 'valid'], loc='lower right')\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7BmOi1ihzN8j"},"source":["Text=[]\n","for i in range(Data.shape[0]):\n","    # print(i)\n","    Text.append(clean_text(str(Data['summary'][i]) + str(Data['reviewText'][i]) ))\n","for i in range(RatePairData.shape[0]):\n","\n","    Text.append(clean_text(str(RatePairData['summary'][i]) + str(RatePairData['reviewText'][i]) ))\n","\n","tokenizer = Tokenizer(num_words= vocabulary_size)\n","tokenizer.fit_on_texts(Text)\n","F_textD = tokenizer.texts_to_matrix(Text, mode='freq')\n","# F_textD = pad_sequences(sequences, maxlen=MaxWordLength)\n","F_text=F_textD[:Data.shape[0],:]\n","F_text_te=F_textD[Data.shape[0]:,:]\n","'''P2--> Normalize unixReviewTime'''\n","F_time=( Data.unixReviewTime.values  - Data.unixReviewTime.min())/(Data.unixReviewTime.max() - Data.unixReviewTime.min())\n","F_time=F_time.reshape([-1,1])\n","'''P3--> one-hot encoding of categories'''\n","F_cat=pd.get_dummies(Data.category).values\n","'''P4 --> for later'''\n","''' Part 5 --> add popularity of an product as feature. I used histigram method'''\n","newD=pd.DataFrame(np.concatenate([Data['itemID'].values,RatePairData['itemID'].values],axis=0),columns=['itemID'])\n","newD['reviewerID']=np.concatenate([Data['reviewerID'].values,RatePairData['reviewerID'].values],axis=0)\n","newD['itemID']=newD['itemID'].groupby(newD['itemID']).transform('count')\n","newD['itemID']=(newD['itemID'].values-newD['itemID'].min())/(newD['itemID'].max()-newD['itemID'].min())\n","\n","newD['reviewerID']=newD['reviewerID'].groupby(newD['reviewerID']).transform('count')\n","newD['reviewerID']=(newD['reviewerID'].values-newD['reviewerID'].min())/(newD['reviewerID'].max()-newD['itemID'].min())\n","newD=newD.to_numpy()\n","F_newD=newD[:Data.shape[0]]\n","F_newD_te=newD[Data.shape[0]:]\n","\n","'''test-train split '''\n","y = Data['overall']\n","y=Data.overall.to_numpy()\n","\n","''' Model Selection'''\n","x_text = Input(shape=(F_text.shape[1]),name='x_text')\n","x_time = Input(shape=(F_time.shape[1]),name='x_time')\n","x_cat = Input(shape=(F_cat.shape[1]),name='x_cat')\n","x_newD = Input(shape=(F_newD.shape[1]),name='x_newD')\n","\n","\n","H=Dense(MaxWordLength,activation='relu')(x_text)\n","H=Dropout(Dp_rate)(H)\n","H=Dense(MaxWordLength,activation='relu')(H)\n","H=Dropout(Dp_rate)(H)\n","H=tf.concat([H,x_time,x_cat,x_newD],axis=-1)\n","H=Dense(MaxWordLength, activation='relu')(H)\n","H=Dense(MaxWordLength, activation='relu')(H)\n","# Final predictions and model.\n","prediction = Dense(1, activation='sigmoid')(H)\n","# kernel_initializer='ones',\n","#     kernel_regularizer=tf.keras.regularizers.L1(0.01),\n","#     activity_regularizer=tf.keras.regularizers.L2(0.01)\n","model = Model(inputs=[x_text,x_time,x_cat,x_newD], outputs= prediction)\n","model.compile(loss=tf.keras.losses.MSE,\n","              optimizer='adam')\n","model.summary()\n","\n","''' Train modek'''\n","history=model.fit([F_text,F_time,F_cat,F_newD], y/5,\n","          batch_size=Batch_Size,\n","          epochs=Epochs,\n","          verbose=1,\n","          validation_split=0.0,shuffle=True)\n","plt.figure(figsize=(12,5))\n","\n","plt.plot(history.history['loss'])\n","plt.plot(history.history['val_loss'])\n","plt.title('model loss')\n","plt.ylabel('loss')\n","plt.xlabel('epoch')\n","plt.legend(['train', 'valid'], loc='lower right')\n","plt.show()\n","##################### prediction\n","'''P2--> Normalize unixReviewTime'''\n","F_time_te=( RatePairData.unixReviewTime.values  - Data.unixReviewTime.min())/(Data.unixReviewTime.max() - Data.unixReviewTime.min())\n","F_time_te=F_time_te.reshape([-1,1])\n","'''P3--> one-hot encoding of categories'''\n","F_cat_te=pd.get_dummies(RatePairData.category).values\n","\n","y_pred = model([F_text_te,F_time_te,F_cat_te,F_newD_te])\n","RatePairID.prediction=y_pred.numpy()*5\n","\n","\n","RatePairID.to_csv('rating_predictions.csv',index=False)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"AM61Lsn2zN5z"},"source":[""],"execution_count":null,"outputs":[]}]}