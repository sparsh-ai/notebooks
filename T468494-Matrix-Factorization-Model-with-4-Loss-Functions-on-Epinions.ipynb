{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"NnG6ZPo6ctof"},"outputs":[],"source":["!pip install torchsnooper\n","!pip install guppy3"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":17386,"status":"ok","timestamp":1637851739495,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"},"user_tz":-330},"id":"LFLhMq57fHng","outputId":"03f43a57-be72-4216-d020-08ea9f4abe9a"},"outputs":[{"name":"stdout","output_type":"stream","text":["/content/data/Epinions/th_0/fold_0\n","Downloading...\n","From: https://drive.google.com/uc?id=1wE8sH6rhWGCnmiIQW8p_VBDSO1kEKDsx\n","To: /content/data/Epinions/th_0/fold_0/epinions_preprocessing.ipynb\n","100% 18.8k/18.8k [00:00\u003c00:00, 16.0MB/s]\n","Collecting lenskit\n","  Downloading lenskit-0.13.1-py3-none-any.whl (87 kB)\n","\u001b[K     |████████████████████████████████| 87 kB 3.3 MB/s \n","\u001b[?25hRequirement already satisfied: psutil\u003e=5 in /usr/local/lib/python3.7/dist-packages (from lenskit) (5.4.8)\n","Collecting binpickle\u003e=0.3.2\n","  Downloading binpickle-0.3.4-py3-none-any.whl (13 kB)\n","Requirement already satisfied: numpy\u003e=1.17 in /usr/local/lib/python3.7/dist-packages (from lenskit) (1.19.5)\n","Requirement already satisfied: pyarrow\u003e=0.15 in /usr/local/lib/python3.7/dist-packages (from lenskit) (3.0.0)\n","Requirement already satisfied: scipy\u003e=1.2 in /usr/local/lib/python3.7/dist-packages (from lenskit) (1.4.1)\n","Requirement already satisfied: cffi\u003e=1.12.2 in /usr/local/lib/python3.7/dist-packages (from lenskit) (1.15.0)\n","Collecting seedbank\u003e=0.1.0\n","  Downloading seedbank-0.1.1-py3-none-any.whl (7.4 kB)\n","Collecting csr\u003e=0.3.1\n","  Downloading csr-0.4.0-py3-none-any.whl (23 kB)\n","Requirement already satisfied: pandas\u003e=0.24 in /usr/local/lib/python3.7/dist-packages (from lenskit) (1.1.5)\n","Requirement already satisfied: numba\u003c0.54,\u003e=0.51 in /usr/local/lib/python3.7/dist-packages (from lenskit) (0.51.2)\n","Collecting pickle5\n","  Downloading pickle5-0.0.12-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (256 kB)\n","\u001b[K     |████████████████████████████████| 256 kB 17.2 MB/s \n","\u001b[?25hRequirement already satisfied: msgpack\u003e=1.0 in /usr/local/lib/python3.7/dist-packages (from binpickle\u003e=0.3.2-\u003elenskit) (1.0.2)\n","Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi\u003e=1.12.2-\u003elenskit) (2.21)\n","Requirement already satisfied: llvmlite\u003c0.35,\u003e=0.34.0.dev0 in /usr/local/lib/python3.7/dist-packages (from numba\u003c0.54,\u003e=0.51-\u003elenskit) (0.34.0)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from numba\u003c0.54,\u003e=0.51-\u003elenskit) (57.4.0)\n","Requirement already satisfied: python-dateutil\u003e=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas\u003e=0.24-\u003elenskit) (2.8.2)\n","Requirement already satisfied: pytz\u003e=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas\u003e=0.24-\u003elenskit) (2018.9)\n","Requirement already satisfied: six\u003e=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil\u003e=2.7.3-\u003epandas\u003e=0.24-\u003elenskit) (1.15.0)\n","Installing collected packages: pickle5, seedbank, csr, binpickle, lenskit\n","Successfully installed binpickle-0.3.4 csr-0.4.0 lenskit-0.13.1 pickle5-0.0.12 seedbank-0.1.1\n","trust_data.txt      100%[===================\u003e]   6.06M  --.-KB/s    in 0.09s   \n","    user   item  rating\n","0  22605  42915       1\n","1  22605   5052       1\n","2  22605  42913       1\n","3  22605  18420       1\n","4  22605  42914       1\n","Num_of_users: 33960\n","Num_of_items: 49288\n","\u001b[4mCount after only keeping users with at least 25 relevant interactions\u001b[0m\n","Num_of_users: 883\n","Num_of_items: 24777\n","Total_interactions: 161171\n","   user  item  rating\n","0  2362   170       1\n","1  2362   968       1\n","2  2362  3123       1\n","3  2362   161       1\n","4  2362  2691       1\n","   user  item  rating  user_id  item_id\n","0  2362   170       1        0        0\n","1  2362   968       1        0        1\n","2  2362  3123       1        0        2\n","3  2362   161       1        0        3\n","4  2362  2691       1        0        4\n","883 24777\n","143511\n","17660\n","143511\n","17660\n","143511\n","17660\n","143511\n","17660\n","143511\n","17660\n","\u003cclass 'pandas.core.frame.DataFrame'\u003e\n","Int64Index: 17660 entries, 39052 to 148320\n","Data columns (total 5 columns):\n"," #   Column   Non-Null Count  Dtype\n","---  ------   --------------  -----\n"," 0   user     17660 non-null  int64\n"," 1   item     17660 non-null  int64\n"," 2   rating   17660 non-null  int64\n"," 3   user_id  17660 non-null  int64\n"," 4   item_id  17660 non-null  int64\n","dtypes: int64(5)\n","memory usage: 827.8 KB\n","total 10400\n","drwxr-xr-x 2 root root    4096 Nov 25 14:48 .\n","drwxr-xr-x 3 root root    4096 Nov 25 14:48 ..\n","-rw-r--r-- 1 root root      62 Nov 25 14:48 dataset_meta_info.json\n","-rw-r--r-- 1 root root   18805 Nov 25 14:48 epinions_preprocessing.ipynb\n","-rw-r--r-- 1 root root 3787655 Nov 25 14:48 test.csv\n","-rw-r--r-- 1 root root  465147 Nov 25 14:48 train.csv\n","-rw-r--r-- 1 root root 6357397 Nov 25 14:48 trust_data.txt\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","markdown 3.3.6 requires importlib-metadata\u003e=4.4; python_version \u003c \"3.10\", but you have importlib-metadata 2.1.2 which is incompatible.\u001b[0m\n","Author: Sparsh A.\n","\n","Last updated: 2021-11-25 14:48:57\n","\n","Compiler    : GCC 7.5.0\n","OS          : Linux\n","Release     : 5.4.104+\n","Machine     : x86_64\n","Processor   : x86_64\n","CPU cores   : 2\n","Architecture: 64bit\n","\n","numpy  : 1.19.5\n","lenskit: 0.13.1\n","IPython: 5.5.0\n","pandas : 1.1.5\n","json   : 2.0.9\n","\n"]}],"source":["!mkdir -p /content/data/Epinions/th_0/fold_0\n","%cd /content/data/Epinions/th_0/fold_0\n","!gdown --id 1wE8sH6rhWGCnmiIQW8p_VBDSO1kEKDsx -O epinions_preprocessing.ipynb\n","%run epinions_preprocessing.ipynb"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":6625,"status":"ok","timestamp":1637851746112,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"},"user_tz":-330},"id":"3CiwpYWBclgz"},"outputs":[],"source":["import torch\n","import numpy as numpy\n","import torch.nn as nn\n","import torchsnooper\n","\n","import os\n","import pandas as pd\n","\n","import math\n","import argparse\n","import sys\n","\n","import numpy as np\n","import pandas as pd\n","from scipy.sparse import csr_matrix\n","import matplotlib\n","matplotlib.use('Agg')\n","from matplotlib import pyplot as plt\n","\n","from torch.utils.data import TensorDataset, DataLoader\n","\n","from tqdm import tqdm\n","import pickle\n","\n","from guppy import hpy\n","\n","import torch\n","import numpy as np\n","import pandas as pd\n","import os\n","from sklearn.model_selection import train_test_split\n","from scipy.sparse import csr_matrix\n","import json"]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":6,"status":"ok","timestamp":1637851746113,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"},"user_tz":-330},"id":"gd2Kl01Iim_f"},"outputs":[],"source":["class LambdaMF(nn.Module):\n","    def __init__(self, n_user, n_item, init_range, emb_size, \n","    \t\t\tweight_user=None, \n","    \t\t\tweight_item=None):\n","\n","        super(LambdaMF, self).__init__()\n","        self.user_emb = nn.Embedding(n_user, emb_size)\n","        self.item_emb = nn.Embedding(n_item, emb_size)\n","          # initlializing weights\n","        if weight_user == None:\n","            self.user_emb.weight.data.uniform_(-init_range, init_range)\n","        else:\n","            self.user_emb = weight_user\n","           \n","        if weight_item == None:\n","            self.item_emb.weight.data.uniform_(-init_range, init_range)\n","        else:\n","            self.item_emb = weight_item\n","        \n","    def forward(self, userID, itemID, rels, mode):\n","        # score = torch.empty(size=(userID.size()[0], items.size()[1]))\n","        user = self.user_emb(userID)\n","        items = self.item_emb(itemID)\n","\n","        if mode == 'train':\n","            pred = (user * items).sum(-1)\n","            idx_pad = (rels == 20).nonzero()\n","            pred[idx_pad[:, 0], idx_pad[:, 1]] = -100\n","            # rels[idx_pad[:, 0], idx_pad[:, 1]] = 0\n","            return pred, rels\n","        \n","        return (user * items).sum(-1)"]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":1829,"status":"ok","timestamp":1637851747936,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"},"user_tz":-330},"id":"TFqE_-hWcleg"},"outputs":[],"source":["class lambda_loss:\n","\tdef __init__(self, device, rank_list, target_list, t, b, p, num_pos, num_neg):              \n","\t\t# self.rank_list = rank_list.unsqueeze(1)\n","\t\t# self.target_list = target_list.unsqueeze(1)\n","\t\tself.rank_list = rank_list\n","\t\tself.target_list = target_list\n","\t\tself.num_pos = num_pos\n","\t\tself.num_neg = num_neg\n","\t\tself.device = device\n","\t\tself.p = p\n","\n","\t\n","\tdef dcg(self, rank_list, target_list):\n","\t\treturn torch.sum((torch.pow(2, target_list) - 1)/ torch.log2(2 + rank_list.float()), dim=1)\n","\t\n","\tdef rbp(self, rank_list, target_list, p):\n","\t\treturn torch.sum(target_list * torch.pow(p, rank_list), dim=1)\n","\n","\tdef rr(self, rank_list, target_list):\n","\t\trank_list = rank_list + 1\n","\t\trr_all = target_list / rank_list\n","\t\tvalues, _ = torch.max(rr_all, dim=1)\n","\t\t# print(values)\n","\t\treturn values\n","\t\n","\tdef smart_sort(self, x, permutation):\n","\t\td1, d2 = x.size()\n","\t\tret = x[\n","\t\t\ttorch.arange(d1).unsqueeze(1).repeat((1, d2)).flatten(),\n","\t\t\tpermutation.flatten()\n","\t\t].view(d1, d2)\n","\t\treturn ret\n","\t\n","\n","\tdef ap(self, rank_list, target_list):\n","\t\tvalue, idxs = torch.sort(rank_list)        \n","\t\ttarget_reorder = self.smart_sort(target_list, idxs)\n","\t\trank_list = value + 1\n","\t\tap_ind = target_reorder * target_reorder.cumsum(dim=1) / rank_list\n","\t\t# print((ap_ind != 0).sum(dim=1))\n","\t\tap = ap_ind.sum(1) / (ap_ind != 0).sum(dim=1)\n","\t\treturn ap\n","\n","\t# @torchsnooper.snoop()\n","\tdef lambda_dcg(self):\n","\t\tdevice = self.device\n","\t\trank_list = self.rank_list\n","\t\ttarget_list = self.target_list\n","\n","\t\tnum_doc, n_docs = rank_list.size()\n","\n","\t\tn_rel = (1.0 * (target_list == 1)).sum(-1).squeeze(-1).int()\n","\t\tn_val = (1.0 * (target_list != 20)).sum(-1).squeeze(-1).int()\n","\n","\t\tif not n_rel.size():\n","\t\t\tn_rel = n_rel.unsqueeze(0)\n","\t\t\tn_val = n_val.unsqueeze(0)\n","\n","\t\trank_list = rank_list.unsqueeze(1)\n","\t\t\n","\t\t(sorted_scores, sorted_idxs) = rank_list.permute(0, 2, 1).sort(dim=1, descending=True)\n","\t\t# print(sorted_idxs)\n","\t\tdoc_ranks = torch.zeros(num_doc, n_docs).to(device)   \n","\n","\t\tfor i in torch.arange(num_doc):\n","\t\t\tdoc_ranks[i, sorted_idxs[i]] = 1 + torch.arange(n_docs).view((n_docs, 1)).float().to(device)\n","\t\t\n","\t\tdoc_ranks = doc_ranks.unsqueeze(1)\n","\t\tdoc_rank_ori = (doc_ranks - 1).squeeze(1)\n","\t\t# doc_ranks = doc_ranks.permute(0, 2, 1)\n","\n","\t\t# print(rank_list[:, :n_rel].size())\n","\t\t# print(rank_list[:, n_rel:].size())\n","\n","\t\texped = torch.zeros([num_doc, n_docs, n_docs]).to(device)\n","\t\t\n","\t\n","\t\tfor i in range(num_doc):\n","\t\t\trel = n_rel[i]\n","\t\t\tval = n_val[i]\n","\t\t\t# print(n_docs, rel, val)\n","\t\t\t# print(rank_list[i, :, :rel].shape)\n","\t\t\t# print(rank_list[i, :, rel:val])\n","\t\t\trank_new = rank_list[i, :, :rel].permute(1, 0) - rank_list[i, :, rel:val] \n","\t\t\t# print(rank_new.shape)\n","\t\t\tscore_diffs = rank_new.exp()\n","\t\t\t# print(exped.shape)\n","\t\t\texped[i] = nn.ZeroPad2d((0, n_docs+rel-val, 0, n_docs-rel))(score_diffs) \n","\n","\t\tN = 1\n"," \n","\t\tdcg_diffs = torch.zeros([num_doc, n_docs, n_docs]).to(device) \n","\n","\t\tfor i in range(num_doc):\n","\t\t\trel = n_rel[i]\n","\t\t\tval = n_val[i]\n","\n","\t\t\tdiff_new = 1 / (1 + doc_ranks[i, :, :rel]).log2().permute(1, 0) - (1 / (1 + doc_ranks[i, :, rel:val]).log2())\n","\t\t\tnorm = (1 / (2 + torch.arange(rel).float()).log2()).sum().to(device)\n","\t\t\tdiff_new = diff_new / norm\n","\t\t\t# print(n_docs-rel)\n","\t\t\t# print(n_docs+rel-val)\n","\t\t\tdcg_diffs[i] = nn.ZeroPad2d((0, n_docs+rel-val, 0, n_docs-rel))(diff_new)\n","\n","\t\tlamb_updates = 1 / (1 + exped) * N * dcg_diffs.abs()\n","\t\tloss = lamb_updates.sum()\n","\n","\t   \n","\t\treturn loss\n","\n","\n","\tdef lambda_ap(self):\n","\t\trank_list = self.rank_list\n","\t\ttarget_list = self.target_list\n","\n","\t\t# n_docs = len(rank_list)\n","\t\t# num_doc, _, n_docs = rank_list.size()\n","\t\tnum_doc, n_docs = rank_list.size()\n","\n","\t\t# n_rel = target_list.sum(dim=2)\n","\t\t# n_rel = self.num_pos\n","\t\tn_rel = (1.0 * (target_list == 1)).sum(-1).squeeze(-1).int()\n","\t\tn_val = (1.0 * (target_list != 20)).sum(-1).squeeze(-1).int()\n","\n","\t\tif not n_rel.size():\n","\t\t\tn_rel = n_rel.unsqueeze(0)\n","\t\t\tn_val = n_val.unsqueeze(0)\n","\t\t\t# print(n_rel)\n","\t\t\t# print(n_val)\n","\t\t# print(n_rel.shape)\n","\n","\t\t# rank_list = rank_list.permute(0, 2, 1)\n","\t\trank_list = rank_list.unsqueeze(1)\n","\n","\t\t(sorted_scores, sorted_idxs) = rank_list.permute(0, 2, 1).sort(dim=1, descending=True)\n","\t\t# print(sorted_idxs)\n","\t\tdoc_ranks = torch.zeros(num_doc, n_docs).to(device)   \n","\n","\t\tfor i in torch.arange(num_doc):\n","\t\t\tdoc_ranks[i, sorted_idxs[i]] = 1 + torch.arange(n_docs).view((n_docs, 1)).float().to(device)\n","\n","\t\tdoc_ranks = doc_ranks.unsqueeze(1)\n","\t\tdoc_rank_ori = (doc_ranks - 1).squeeze(1)\n","\n","\t\texped = torch.zeros([num_doc, n_docs, n_docs]).to(device)\n","\n","\t\tfor i in range(num_doc):\n","\t\t\trel = n_rel[i]\n","\t\t\tval = n_val[i]\n","\t\t\t# print(n_docs, rel, val)\n","\t\t\t# print(rank_list[i, :, :rel].shape)\n","\t\t\t# print(rank_list[i, :, rel:val])\n","\t\t\trank_new = rank_list[i, :, :rel].permute(1, 0) - rank_list[i, :, rel:val] \n","\t\t\t# print(rank_new.shape)\n","\t\t\tscore_diffs = rank_new.exp()\n","\t\t\t# print(exped.shape)\n","\t\t\texped[i] = nn.ZeroPad2d((0, n_docs+rel-val, 0, n_docs-rel))(score_diffs) \n","\t\n","\n","\t\tN = 1\n","\t\tap_diffs = torch.zeros([num_doc, n_docs, n_docs]).to(device) \n","\n","\t\tfor i in range(num_doc):\n","\t\t\trel = n_rel[i]\n","\t\t\tval = n_val[i]\n","\n","\t\t\t# print(n_docs, rel, val)\n","\t\t\t# print(rank_list[i, :, :rel].shape)\n","\t\t\t# print(rank_list[i, :, rel:val])\n","\t\t\t# print(rel, val)\n","\n","\t\t\trank_new = torch.zeros([rel, val-rel]).to(device)\n","\n","\n","\t\t\tfor j in range(rel):\n","\t\t\t\trank_p = doc_ranks[i, :, j].item()\n","\t\t\t\t\n","\t\t\t\t# print(1.0 * (doc_ranks[i] \u003c= rank_p) * 1.0 * (target_list[i] == 1))\n","\t\t\t\t# target_list[i, :val] == 1\n","\t\t\t\t\n","\t\t\t\tm = (1.0 * (doc_ranks[i] \u003c= rank_p) * 1.0 * (target_list[i] == 1)).sum(-1)\n","\n","\t\t\t\t# m = (1.0 * ((target_list[i, :val] == 1) and (rank_list[i] \u003c= rank_p))).sum(-1)\n","\t\t\t\tterm_2 = (m / rank_p).item()\n","\n","\t\t\t\tfor k in range(val-rel):\n","\t\t\t\t\trank_n = (doc_ranks[i, :, val-rel+k]).item()\n","\t\t\t\t\t\n","\t\t\t\t\tif rank_p \u003c rank_n:\n","\t\t\t\t\t\trank_new[j, k] = 0\n","\t\t\t\t\telse:\n","\t\t\t\t\t\tn = (1.0 * (doc_ranks[i] \u003c= rank_n) * 1.0 * (target_list[i] == 1)).sum(-1)\n","\t\t\t\t\t\tterm_1 = (n + 1) / rank_n\n","\t\t\t\t\t\tterm_1 = term_1.item()\n","\n","\t\t\t\t\t\tprec = target_list[i, : val] * doc_ranks[i, :, :val]\n","\t\t\t\t\t\tprec = prec.squeeze(0).double()\n","\t\t\t\t\t\t# print(prec)\n","\t\t\t\t\t\t# print(rank_p, rank_n)\n","\t\t\t\n","\t\t\t\t\t\t\n","\t\t\t\t\t\n","\t\t\t\t\t\tprec = torch.where(prec \u003e rank_n, prec, 0.)\n","\t\t\t\t\t\tprec = torch.where(prec \u003c rank_p, prec, 0.)\n","\n","\t\t\t\t\t\tprec = prec[prec.nonzero()]\n","\n","\t\t\t\t\t\tif prec == torch.Size([]):\n","\t\t\t\t\t\t\tterm_3 = 0\n","\t\t\t\t\t\telse:\n","\t\t\t\t\t\t\tterm_3 = (1.0 / prec).sum()\n","\t\t\t\t\t\t# print(term_3)\n","\t\t\t\t\t\t\n","\n","\t\t\t\t\t\trank_new[j, k] = (term_1 - term_2 + term_3) / rel\n","\t\t\t\tap_diffs[i] = nn.ZeroPad2d((0, n_docs+rel-val, 0, n_docs-rel))(rank_new) \n","\n","\t\tlamb_updates = 1 / (1 + exped) * N * ap_diffs.abs()\n","\t\tloss = lamb_updates.sum()\n","\t\treturn loss\n","\n","\n","\tdef lambda_rr(self):\n","\t\t# device = self.device\n","\t\trank_list = self.rank_list\n","\t\ttarget_list = self.target_list\n","\t\t# p = self.p\n","\n","\t\t# n_docs = len(rank_list)\n","\t\t# num_doc, _, n_docs = rank_list.size()\n","\t\tnum_doc, n_docs = rank_list.size()\n","\n","\t\t# n_rel = target_list.sum(dim=2)\n","\t\t# n_rel = self.num_pos\n","\t\tn_rel = (1.0 * (target_list == 1)).sum(-1).squeeze(-1).int()\n","\t\tn_val = (1.0 * (target_list != 20)).sum(-1).squeeze(-1).int()\n","\n","\t\tif not n_rel.size():\n","\t\t\tn_rel = n_rel.unsqueeze(0)\n","\t\t\tn_val = n_val.unsqueeze(0)\n","\t\t\t\n","\n","\t\t# rank_list = rank_list.permute(0, 2, 1)\n","\t\trank_list = rank_list.unsqueeze(1)\n","\t\t\n","\t\t(sorted_scores, sorted_idxs) = rank_list.permute(0, 2, 1).sort(dim=1, descending=True)\n","\t\t# print(sorted_idxs)\n","\t\tdoc_ranks = torch.zeros(num_doc, n_docs).to(device)   \n","\n","\t\tfor i in torch.arange(num_doc):\n","\t\t\tdoc_ranks[i, sorted_idxs[i]] = 1 + torch.arange(n_docs).view((n_docs, 1)).float().to(device)\n","\t\t\n","\t\tdoc_ranks = doc_ranks.unsqueeze(1)\n","\t\tdoc_rank_ori = (doc_ranks - 1).squeeze(1)\n","\n","\t\texped = torch.zeros([num_doc, n_docs, n_docs]).to(device)\n","\t\t\n","\t\n","\t\tfor i in range(num_doc):\n","\t\t\trel = n_rel[i]\n","\t\t\tval = n_val[i]\n","\t\t\trank_new = rank_list[i, :, :rel].permute(1, 0) - rank_list[i, :, rel:val] \n","\t\t\t# print(rank_new.shape)\n","\t\t\tscore_diffs = rank_new.exp()\n","\t\t\t# print(exped.shape)\n","\t\t\texped[i] = nn.ZeroPad2d((0, n_docs+rel-val, 0, n_docs-rel))(score_diffs) \n","\n","\t\trr_diffs = torch.zeros([num_doc, n_docs, n_docs]).to(device) \n","\n","\t\tfor i in range(num_doc):\n","\t\t\trel = n_rel[i]\n","\t\t\tval = n_val[i]\n","\n","\t\t\trank_new = torch.zeros([rel, val-rel]).to(device)\n","\n","\t\t\tdiff_new_ = 1 / doc_ranks[i, :, :rel].permute(1, 0) - 1 / doc_ranks[i, :, rel:val]\n","\t\t\tdiff_new_ = torch.clamp(diff_new_, max=0)\n","\t\t\ttop_rel = torch.argmin(doc_ranks[i, :, :rel])\n","\t\t\tdiff_new = torch.zeros_like(diff_new_)\n","\t\t\tdiff_new[top_rel] = diff_new_[top_rel]\n","\n","\t\t\trr_diffs[i] = nn.ZeroPad2d((0, n_docs+rel-val, 0, n_docs-rel))(diff_new)\n","\t\t\n","\t\tN = 1\n","\t\tlamb_updates = 1 / (1 + exped) * N * rr_diffs.abs()\n","\t\tloss = lamb_updates.sum()\n","\t\treturn loss\n","\n","\n","\tdef lambda_rbp(self):\n","\t\tdevice = self.device\n","\t\trank_list = self.rank_list\n","\t\ttarget_list = self.target_list\n","\t\tp = self.p\n","\n","\t\t# n_docs = len(rank_list)\n","\t\t# num_doc, _, n_docs = rank_list.size()\n","\t\tnum_doc, n_docs = rank_list.size()\n","\n","\t\t# n_rel = target_list.sum(dim=2)\n","\t\t# n_rel = self.num_pos\n","\t\tn_rel = (1.0 * (target_list == 1)).sum(-1).squeeze(-1).int()\n","\t\tn_val = (1.0 * (target_list != 20)).sum(-1).squeeze(-1).int()\n","\n","\t\tif not n_rel.size():\n","\t\t\tn_rel = n_rel.unsqueeze(0)\n","\t\t\tn_val = n_val.unsqueeze(0)\n","\t\t\t# print(n_rel)\n","\t\t\t# print(n_val)\n","\t\t# print(n_rel.shape)\n","\n","\t\t# rank_list = rank_list.permute(0, 2, 1)\n","\t\trank_list = rank_list.unsqueeze(1)\n","\t\t\n","\t\t(sorted_scores, sorted_idxs) = rank_list.permute(0, 2, 1).sort(dim=1, descending=True)\n","\t\t# print(sorted_idxs)\n","\t\tdoc_ranks = torch.zeros(num_doc, n_docs).to(device)   \n","\n","\t\tfor i in torch.arange(num_doc):\n","\t\t\tdoc_ranks[i, sorted_idxs[i]] = 1 + torch.arange(n_docs).view((n_docs, 1)).float().to(device)\n","\t\t\n","\t\tdoc_ranks = doc_ranks.unsqueeze(1)\n","\t\tdoc_rank_ori = (doc_ranks - 1).squeeze(1)\n","\t\t# doc_ranks = doc_ranks.permute(0, 2, 1)\n","\n","\t\t# print(rank_list[:, :n_rel].size())\n","\t\t# print(rank_list[:, n_rel:].size())\n","\n","\t\texped = torch.zeros([num_doc, n_docs, n_docs]).to(device)\n","\n","\t\tfor i in range(num_doc):\n","\t\t\trel = n_rel[i]\n","\t\t\tval = n_val[i]\n","\t\t\t\n","\t\t\trank_new = rank_list[i, :, :rel].permute(1, 0) - rank_list[i, :, rel:val] \n","\t\t\tscore_diffs = rank_new.exp()\n","\t\t\texped[i] = nn.ZeroPad2d((0, n_docs+rel-val, 0, n_docs-rel))(score_diffs) \n","\n","\t\tN = 1\n"," \n","\t\trbp_diffs = torch.zeros([num_doc, n_docs, n_docs]).to(device) \n","\n","\t\tfor i in range(num_doc):\n","\t\t\trel = n_rel[i]\n","\t\t\tval = n_val[i]\n","\t\t\tnorm = 1.0 / (1 - torch.pow(p, rel))\n","\t\t\tdiff_new = torch.pow(p, doc_ranks[i, :, :rel]).permute(1, 0) - torch.pow(p, doc_ranks[i, :, rel:val])\n","\t\t\tdiff_new = (1 - p) * diff_new / norm\n","\t\t\trbp_diffs[i] = nn.ZeroPad2d((0, n_docs+rel-val, 0, n_docs-rel))(diff_new)\n","\n","\t\tlamb_updates = 1 / (1 + exped) * N * rbp_diffs.abs()\n","\t\tloss = lamb_updates.sum()\n","\t   \n","\t\treturn loss"]},{"cell_type":"code","execution_count":6,"metadata":{"executionInfo":{"elapsed":7,"status":"ok","timestamp":1637851747937,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"},"user_tz":-330},"id":"BLq0koQSclcZ"},"outputs":[],"source":["def hit(gt):\n","    for gt_item in gt:\n","        if gt_item == 1:\n","            return 1\n","    return 0\n","\n","\n","def dcg_at_k(gt, k):\n","    return np.sum((np.power(2, gt[: k]) - 1) / np.log2(np.arange(2, k + 2)))\n","\n","def dcg(gt):\n","    return np.sum((np.power(2, gt) - 1) / np.log2(np.arange(2, len(gt) + 2)))\n","\n","def evaluation(model, test_loader, max_rating, device, k, p, n_item):\n","    model.eval()\n","    NDCG_at_5, NDCG, RR, AP, RBP_80, RBP_90, RBP_95 = [], [], [], [], [], [], []\n","    \n","    for user, items, binary_rels, scale_rels in test_loader:\n","        user, items, binary_rels, scale_rels = user.to(device), items.to(device), binary_rels.to(device), scale_rels.to(device)\n","        for i in range(len(user)):\n","            gt_items = []\n","            u = user[i]\n","            item = items[i]\n","            binary_rel = binary_rels[i]\n","            scale_rel = scale_rels[i]\n","\n","            prediction_i = model(u, item, -1, mode='test')\n","\n","            # ratings, indices = torch.topk(prediction_i, k)\n","            ratings, indices = torch.topk(prediction_i, len(item))\n","            \n","            recommends = torch.take(item, indices).cpu().numpy().tolist()\n","#             print(u, recommends)\n","            binary_gt = binary_rel[indices].cpu().numpy()\n","            scale_gt = scale_rel[indices].cpu().numpy()\n","#             gt = recommends      \n","#             gt = [gt[j].cpu().numpy().tolist() for j in range(len(gt))]\n","            \n","            recommends = list(filter(lambda x: x != n_item, recommends))\n","            binary_gt = list(filter(lambda x: x != 20, binary_gt))\n","            scale_gt = list(filter(lambda x: x != 20, scale_gt))\n","            if len(scale_gt) \u003c 5:\n","                scale_gt = scale_gt + [0] * (5 - len(scale_gt))\n","\n","\n","            non_zero = np.asarray(binary_gt).nonzero()[0]\n","\n","            # # with cutoff\n","            # rr = 1. / (non_zero[0] + 1) if non_zero.size else 0.\n","            # ap = (binary_gt * np.cumsum(binary_gt) / (1 + np.arange(k))).mean()\n","            # rbp = (1 - p) * (binary_gt * np.power(p, range(k))).sum()\n","\n","            # no cutoff\n","            rr = 1. / (non_zero[0] + 1) if non_zero.size else 0.\n","            ap = (binary_gt * np.cumsum(binary_gt) / (1 + np.arange(len(binary_gt))))\n","            ap = ap[np.nonzero(ap)].mean()\n","            rbp_80 = (1 - 0.8) * (binary_gt * np.power(0.8, range(len(binary_gt)))).sum()\n","            rbp_90 = (1 - 0.9) * (binary_gt * np.power(0.9, range(len(binary_gt)))).sum()\n","            rbp_95 = (1 - 0.95) * (binary_gt * np.power(0.95, range(len(binary_gt)))).sum()\n","        \n","        ###################################################### \n","        # dcg with cutoff\n","            # full_mark = [max_rating] * k\n","        # dcg without cutoff\n","            idcg_gt = np.sort(scale_gt)[::-1]\n","            # score = 0\n","            # for j in range(len(item)):\n","            #     score = score + (np.power(2, idcg_gt[j]) - 1) / np.log2(j+2)\n","        #######################################################\n","#             print(hit(gt))\n","#             print(dcg(gt))\n","            # HR.append(hit(binary_gt))\n","            # NDCG.append(dcg(scale_gt) / score)\n","            NDCG_at_5.append(dcg_at_k(scale_gt, k) / dcg_at_k(idcg_gt, k))\n","            NDCG.append(dcg(scale_gt) / dcg(idcg_gt))\n","            RR.append(rr)\n","            AP.append(ap)\n","            RBP_80.append(rbp_80)\n","            RBP_90.append(rbp_90)\n","            RBP_95.append(rbp_95)\n","\n","#             print(len(NDCG))\n","    \n","    # print(\"HR = %.4f\" % np.mean(HR))\n","    print(\"NDCG@5 = %.4f\" % np.mean(NDCG_at_5))\n","    print(\"NDCG = %.4f\" % np.mean(NDCG))\n","    print(\"MRR = %.4f\" % np.mean(RR))\n","    print(\"MAP = %.4f\" % np.mean(AP))\n","    print(\"RBP_80 = %.4f\" % np.mean(RBP_80))\n","    print(\"RBP_90 = %.4f\" % np.mean(RBP_90))\n","    print(\"RBP_95 = %.4f\" % np.mean(RBP_95))\n","        \n","    return NDCG_at_5, NDCG, RR, AP, RBP_80, RBP_90, RBP_95, np.mean(NDCG_at_5), np.mean(NDCG), np.mean(RR), np.mean(AP), np.mean(RBP_80), np.mean(RBP_90), np.mean(RBP_95)"]},{"cell_type":"code","execution_count":7,"metadata":{"executionInfo":{"elapsed":7,"status":"ok","timestamp":1637851747938,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"},"user_tz":-330},"id":"liSYZe_IclaD"},"outputs":[],"source":["# make folders for data, models and results\n","def dir_exists(path):\n","\tif not os.path.exists(path):\n","\t\tos.makedirs(path)\n","\n","def save_model(epoch, model, best_result, optimizer, save_path):\n","\ttorch.save({\n","\t\t'epoch': epoch + 1,\n","\t\t'state_dict': model.state_dict(),\n","\t\t'best_performance': best_result,\n","\t\t'optimizer': optimizer.state_dict(),\n","\t\t}, save_path)\n","\n","# ---------------------------Amazon Dataset Preprocessing Functions---------------\n","\n","def get_unique_id(data_pd: pd.DataFrame, column: str) -\u003e (dict, pd.DataFrame):\n","\t\"\"\"\n","\tclear the ids\n","\t:param data_pd: pd.DataFrame \n","\t:param column: specified col\n","\t:return: dict: {value: id}\n","\t\"\"\"\n","\tnew_column = '{}_id'.format(column)\n","\tassert new_column not in data_pd.columns\n","\ttemp = data_pd.loc[:, [column]].drop_duplicates().reset_index(drop=True)\n","\ttemp[new_column] = temp.index\n","\ttemp.index = temp[column]\n","\tdel temp[column]\n","\t# data_pd.merge()\n","\tdata_pd = pd.merge(left=data_pd,\n","\t\tright=temp,\n","\t\tleft_on=column,\n","\t\tright_index=True,\n","\t\thow='left')\n","\n","\treturn temp[new_column].to_dict(), data_pd\n","\n","\n","def load_data_amazon(df):\n","\tuser_ids, data_pd = get_unique_id(df, 'user')\n","\titem_ids, data_pd = get_unique_id(df, 'item')\n","\n","\n","\tdata_pd = data_pd.loc[:, ['user', 'user_id', 'item', 'item_id', 'rating', 'timestamp']]\n","\tdata_pd = data_pd.drop(['user', 'item'], axis=1)\n","\n","\treturn data_pd\n","\n","\n","def write_result(res_path, dataset, loss_type, num_neg, lr, temp, threshold, tradeoff, k, hr, ndcg, mrr, mAP, rbp):\n","\tdir_exists(res_path)\n","\tres_path = os.path.join(res_path, str(dataset) + '.txt')\n","\tf = open(res_path, 'a+')\n","\tf.write('loss type: ' + loss_type + ', num_neg: ' + str(num_neg) + ', learning rate: ' + str(lr) + ', temperature: ' + str(temp) + ', threshold: ' + str(threshold) + ', tradeoff:' + str(tradeoff) + ', k:' + str(k) + '\\n')\n","\tf.write('HR = ' + str(hr) + ', NDCG = ' + str(ndcg) + ', MRR = ' + str(mrr) + ', MAP = ' + str(mAP) + ', RBP = ' + str(rbp) + '\\n\\n' )\n","\tf.close()\n","\n","\n","def choose_loss(loss_type, device, prediction, rel, t, b, temp, p, f_rbp, num_pos, num_neg):\n","\tif loss_type == 'dcg':\n","\t\tloss_value = ndcg_loss(device, prediction, rel, t, b, num_pos, num_neg, temp).to(device)\n","\telif loss_type == 'rr':\n","\t\tloss_value = rr_loss(device, prediction, rel, temp).to(device)\n","\telif loss_type == 'ap':\n","\t\tloss_value = ap_loss(device, prediction, rel, temp).to(device)\n","\telif loss_type == 'rbp':\n","\t\tloss_value = rbp_loss(device, prediction, rel, temp, p, f_rbp).to(device)\n","\telif loss_type == 'nrbp':\n","\t\tloss_value = nrbp_loss(device, prediction, rel, temp, p, f_rbp).to(device)\n","\telif loss_type == 'nrbp_1':\n","\t\tloss_value = nrbp_loss_1(device, prediction, rel, temp, p, f_rbp).to(device)\n","\telif loss_type == 'lambda_dcg':\n","\t\tloss_value = lambda_loss(device, prediction, rel, t, b, p, num_pos, num_neg).lambda_dcg().to(device)\n","\telif loss_type == 'lambda_rr':\n","\t\tloss_value = lambda_loss(device, prediction, rel, t, b, p, num_pos, num_neg).lambda_rr().to(device)\n","\telif loss_type == 'lambda_ap':\n","\t\tloss_value = lambda_loss(device, prediction, rel, t, b, p, num_pos, num_neg).lambda_ap().to(device)\n","\telif loss_type == 'lambda_rbp':\n","\t\tloss_value = lambda_loss(device, prediction, rel, t, b, p, num_pos, num_neg).lambda_rbp().to(device)\n","\n","\telse: print('The loss function of your choice is not available.')\n","\n","\treturn loss_value\n","\n","def loss_train_test(model, optimizer, dataloader, loss_type, device, t, b, temp, p, f_rbp, num_pos, num_neg):\n","\tloss = 0\n","\t# if mode == 'train':\n","\tfor user, items, rels in dataloader:\n","\t\tbatch = len(user)\n","\t\tuser, items, rel = user.to(device), items.to(device), rels.type(torch.FloatTensor).to(device)       \n","\t\tuser = torch.unsqueeze(user, 1)  \n","\n","\t\tprediction, rel = model(user, items, rel, mode='train')\n","\t\t# idx_pad = (rel == 20).nonzero()\n","\t\t# print(prediction.size())\n","\t\t# print(rels.size())\n","\t\tloss_value = choose_loss(loss_type, device, \\\n","\t\t\tprediction, rel, t, b, temp, p, f_rbp, num_pos, num_neg)\n","\t\t# for param in model.parameters():\n","\t\t# \tregularization_loss += torch.sum(torch.abs(param))\n","\t\t# loss_value += 0.0001 * regularization_loss\n","\t\t# print(loss_value)\n","\t\t# print(loss_value / user.size(0))\n","\t\toptimizer.zero_grad()\n","\t\tloss_value.sum().backward()\n","\t\toptimizer.step()\n","\t\tloss += loss_value.sum()\n","\treturn loss"]},{"cell_type":"code","execution_count":8,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1637851748708,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"},"user_tz":-330},"id":"MasDcDxJd6rS"},"outputs":[],"source":["def load_data(path, dataset, threshold, fold):\n","\tread_path = os.path.join(path, dataset, 'th_'+str(threshold))\n","\tdata_path = os.path.join(read_path, 'fold_'+str(fold), 'train.csv')\n","\tstat_path = os.path.join(read_path, 'dataset_meta_info_'+str(threshold)+'.json')\n","\ttest_path = os.path.join(read_path, 'fold_'+str(fold), 'test.csv')\n","\n","\tdf = pd.read_csv(data_path, header=0)\n","\tdf_test = pd.read_csv(test_path, header=0)\n","\tprint(df.shape)\n","\tprint(df_test.shape)\n","\n","\twith open(os.path.join(stat_path), 'r') as f:\n","\t\tdataset_meta_info = json.load(f)\n","\n","\n","\tn_user = dataset_meta_info['user_size']\n","\tn_item = dataset_meta_info['item_size']\n","\t\n","\ttrain_row = []\n","\ttrain_col = []\n","\ttrain_rating = []\n","\n","\tfor line in df.itertuples():\n","\t\t# print(line)\n","\t\tif line[3] \u003e= threshold:\n","\t\t\tu = line[4]\n","\t\t\ti = line[5]\n","\t\t\ttrain_row.append(u)\n","\t\t\ttrain_col.append(i)\n","\t\t\ttrain_rating.append(1)\n","\ttrain_matrix = csr_matrix((train_rating, (train_row, train_col)), shape=(n_user, n_item))\n","\n","\ttest_row = []\n","\ttest_col = []\n","\ttest_rating = []\n","\n","\tfor line in df_test.itertuples():\n","\t\tif line[3] \u003e= threshold:\n","\t\t\tu = line[4]\n","\t\t\ti = line[5]\n","\t\t\ttest_row.append(u)\n","\t\t\ttest_col.append(i)\n","\t\t\ttest_rating.append(1)\n","\ttest_matrix = csr_matrix((test_rating, (test_row, test_col)), shape=(n_user, n_item))\n","\n","\ttest_row = []\n","\ttest_col = []\n","\ttest_rel = []\n","\tfor line in df_test.itertuples():\n","\t\tu = line[4]\n","\t\ti = line[5]\n","\t\trel = line[3]\n","\t\ttest_row.append(u)\n","\t\ttest_col.append(i)\n","\t\ttest_rel.append(rel)\n","\ttest_rel_matrix = csr_matrix((test_rel, (test_row, test_col)), shape=(n_user, n_item))\n","\treturn n_user, n_item, train_matrix, test_matrix, test_rel_matrix\n","\n","\n","def train_preparation(n_user, n_item, matrix, frac):\n","\tmat = []\n","\trels = []\n","\tusers = []\n","\n","\tall_items = set(np.arange(n_item))\n","\tfor user in range(n_user):\n","\t\tpos_items = list(matrix.getrow(user).nonzero()[1])\n","\t\tneg_pool = list(all_items - set(matrix.getrow(user).nonzero()[1]))\n","\t\tlen_pos = len(pos_items)\n","\t\tnum_neg = int(frac * len_pos)\n","\t\ttrain_rel = [1] * len_pos  + [0] * num_neg\n","\t\tneg_i = list(np.random.choice(neg_pool, size=num_neg, replace=False))\n","\t\titems = pos_items + neg_i\n","\t\tmat.append(items)\n","\t\trels.append(train_rel)\n","\t\tusers.append(user)\n","\n","\tmax_cols = max([len(item) for item in mat])\n","\tfor line in mat:\n","\t\tline += [n_item] * (max_cols - len(line))\n","\tfor line in rels:\n","\t\tline += [20] * (max_cols - len(line))\n","\treturn mat, rels, users\n","\n","\n","def test_preparation(n_user, n_item, train_matrix, test_matrix, frac):\n","\tmat = []\n","\trels = []\n","\tusers = []\n","\tall_items = set(np.arange(n_item))\n","\n","\tfor user in range(n_user):\n","\t\tpos_items = list(test_matrix.getrow(user).nonzero()[1])\n","\t\tneg_pool = list(all_items - set(train_matrix.getrow(user).nonzero()[1]) - set(test_matrix.getrow(user).nonzero()[1]))\n","\t\tlen_pos = len(pos_items)\n","\t\tnum_neg = int(frac * len_pos)\n","\t\ttest_rel = [1] * len_pos + [0] * num_neg\n","\t\tneg_i = list(np.random.choice(neg_pool, size=num_neg, replace=False))\n","\t\titems = pos_items + neg_i\n","\t\tmat.append(items)\n","\t\trels.append(test_rel)\n","\t\tusers.append(user)\n","\n","\tmax_cols = max([len(item) for item in mat])\n","\tfor line in mat:\n","\t\tline += [n_item] * (max_cols - len(line))\n","\tfor line in rels:\n","\t\tline += [20] * (max_cols - len(line))\n","\treturn mat, rels, users"]},{"cell_type":"code","execution_count":9,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1637851750125,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"},"user_tz":-330},"id":"dCW2Gaiqex8j"},"outputs":[],"source":["!mv /content/data/Epinions/th_0/fold_0/dataset_meta_info.json /content/data/Epinions/th_0\n","# alson rename it -- add _0"]},{"cell_type":"code","execution_count":10,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":513,"status":"ok","timestamp":1637851767980,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"},"user_tz":-330},"id":"qmXmbOXbhh5P","outputId":"f2947102-d0d1-484e-c6ff-b10ee1145b27"},"outputs":[{"name":"stdout","output_type":"stream","text":["/content\n"]}],"source":["%cd /content"]},{"cell_type":"code","execution_count":13,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":11568,"status":"ok","timestamp":1637851976307,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"},"user_tz":-330},"id":"4vepnGO4IRYF","outputId":"9ad50594-a68e-4065-9f8c-3155b743ef90"},"outputs":[{"name":"stdout","output_type":"stream","text":["(17660, 6)\n","(143511, 6)\n"]}],"source":["sys.path.append('.')\n","\n","# make possible new folders for data, models, and results\n","dir_exists('lambda_models')\n","dir_exists('logs')\n","dir_exists('lambda_results')\n","\n","parser = argparse.ArgumentParser(description='Parameter settings')\n","parser.add_argument('--data_path', nargs='?', default='./data/',\n","\t\t\t\t\t\thelp='Input data path.')\n","parser.add_argument('--save_path', nargs='?', default='./lambda_models/',\n","                        help='Save data path.')\n","parser.add_argument('--res_path', nargs='?', default='./lambda_results/',\n","                        help='Save data path.')\n","parser.add_argument('--dataset', type=str, default='Epinions',\n","\t\t\t\t\tchoices=['Epinions', 'citeulike', 'ml-10m', 'Clothing_Shoes_and_Jewelry', 'Home_and_Kitchen', 'Sports_and_Outdoors'])  \n","parser.add_argument('--threshold', type=int, default=0,\n","\t\t\t\t\thelp='binary threshold for pos/neg') \n","parser.add_argument('--fold', type=int, default=0,\n","\t\t\t\t\tchoices = [0, 1, 2, 3, 4],\n","\t\t\t\t\thelp='fold ID for experiments')\n","parser.add_argument('--num_pos', type=int, default=20,\n","\t\t\t\t\thelp='number of negative items sampled')\n","parser.add_argument('--num_neg', type=int, default=200,\n","\t\t\t\t\thelp='number of negative items sampled')\n","parser.add_argument('--batch_size', type=int, default=16, \n","\t\t\t\t\thelp='input batch size for training (default: 128)')\n","parser.add_argument('--random_range', type=float, default=0.01,\n","\t\t\t\t\thelp='[-random_range, random_range] for initialization')\n","parser.add_argument('--emb_size', type=int, default=32,\n","\t\t\t\t\thelp='latent factor embedding size (default: 32)')\n","parser.add_argument('--no-cuda', action='store_true', default=True,\n","\t\t\t\t\thelp='enables CUDA training')\n","parser.add_argument('--loss_type', type=str, default='lambda_dcg', \n","\t\t\t\t\tchoices=['lambda_dcg', 'lambda_rr', 'lambda_ap', 'lambda_rbp' ],\n","\t\t\t\t\thelp='listwise loss function selection')\n","parser.add_argument('--reg', type=float, default=0,\n","\t\t\t\t\thelp='l2 regularization')   \n","parser.add_argument('--lr', type=float, default=0.1, \n","\t\t\t\t\thelp='learning rate')  \n","parser.add_argument('--epochs', type=int, default=5,\n","\t\t\t\t\thelp='number of epochs to train (default: 1000)') \n","parser.add_argument('--p', type=float, default=0.95, \n","\t\t\t\t\thelp='probability value for RBP')\n","parser.add_argument('--t', type=int, default=2, \n","\t\t\t\t\thelp='power base for DCG')\n","parser.add_argument('--b', type=int, default=2, \n","\t\t\t\t\thelp='log base for DCG')\n","parser.add_argument('--f_rbp', type=float, default=0.01,\n","\t\t\t\t\thelp='the value to make rankings smaller for RBP training')\n","parser.add_argument('--temp', type=float, default=1.0,\n","\t\t\t\t\thelp='temperature value for training acceleration')\n","parser.add_argument('--max_rating', type=float, default=1.0,\n","\t\t\t\t\thelp='max rating scale')\n","parser.add_argument('--k', type=int, default=5,\n","\t\t\t\t\thelp='cutoff')\n","parser.add_argument('--frac', type=float, default=1.0,\n","\t\t\t\t\thelp='negative sampling ratio')\n","\n","args = parser.parse_args(args={})  \n","args.cuda = not args.no_cuda and torch.cuda.is_available()\n","os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n","\n","torch.manual_seed(2020)\n","torch.cuda.manual_seed(2020)\n","np.random.seed(2020)\n","# print(args.cuda)\n","\n","device = torch.device(\"cuda\" if args.cuda else \"cpu\")\n","kwargs = {'num_workers': 1, 'pin_memory': True} if args.cuda else {}\n","\n","\n","data_path = args.data_path\n","save_path = args.save_path\n","res_path = args.res_path\n","dataset = args.dataset\n","\n","dir_exists(res_path)\n","dir_exists(os.path.join(res_path, dataset))\n","\n","\n","threshold = args.threshold\n","fold = args.fold\n","p = args.p\n","t = args.t\n","b = args.b \n","f_rbp = args.f_rbp \n","temp = args.temp\n","batch_size = args.batch_size\n","\n","num_pos = args.num_pos\n","num_neg = args.num_neg \n","reg = args.reg\n","emb_size = args.emb_size\n","random_range = args.random_range\n","lr = args.lr\n","k = args.k\n","\n","epochs = args.epochs\n","loss_type = args.loss_type \n","max_rating = args.max_rating\n","\n","frac = args.frac                       \n","\n","\n","n_user, n_item, train_matrix, test_matrix, test_rel_matrix = load_data(data_path, dataset, threshold, fold)\n","\n","# print(n_user, n_item)\n","# print(train_matrix)\n","\n","\n","# train_neg_items = get_neg_items(n_user, n_item, train_matrix, num_neg)\n","# print(np.array(train_neg_items).shape)\n","\n","train_mat, train_rels, train_user = train_preparation(n_user, n_item, train_matrix, frac)\n","test_mat, test_rels_binary, test_user = test_preparation(n_user, n_item, train_matrix, test_matrix, frac)\n","_, test_rels_scale, _ = test_preparation(n_user, n_item, train_matrix, test_rel_matrix, frac)\n","# print(np.array(train_mat).shape)\n","\n","train_tensor = TensorDataset(torch.from_numpy(np.array(train_user)),\n","\t\t\t\t\t\t\ttorch.from_numpy(np.array(train_mat)),\n","\t\t\t\t\t\t   torch.from_numpy(np.array(train_rels)))\n","train_loader = DataLoader(train_tensor, batch_size=batch_size, shuffle=True)\n","\n","\n","test_tensor = TensorDataset(torch.from_numpy(np.array(test_user)),\n","\t\t\t\t\t\t\ttorch.from_numpy(np.array(test_mat)),\n","\t\t\t\t\t\t\ttorch.from_numpy(np.array(test_rels_binary)),\n","\t\t\t\t\t\t\ttorch.from_numpy(np.array(test_rels_scale)))\n","test_loader = DataLoader(test_tensor, batch_size=batch_size, shuffle=False)"]},{"cell_type":"code","execution_count":14,"metadata":{"executionInfo":{"elapsed":19,"status":"ok","timestamp":1637851976308,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"},"user_tz":-330},"id":"CI9nTAbElQgQ"},"outputs":[],"source":["import warnings\n","warnings.filterwarnings('ignore')"]},{"cell_type":"code","execution_count":15,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":526334,"status":"ok","timestamp":1637852502625,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"},"user_tz":-330},"id":"06xQn_pqiV8Z"},"outputs":[{"name":"stderr","output_type":"stream","text":["  0%|          | 0/120 [00:00\u003c?, ?it/s]/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:56: RuntimeWarning: Mean of empty slice.\n","/usr/local/lib/python3.7/dist-packages/numpy/core/_methods.py:170: RuntimeWarning: invalid value encountered in double_scalars\n","  ret = ret.dtype.type(ret / rcount)\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:74: RuntimeWarning: invalid value encountered in double_scalars\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:75: RuntimeWarning: invalid value encountered in double_scalars\n","  1%|          | 1/120 [02:15\u003c4:28:50, 135.55s/it]"]},{"name":"stdout","output_type":"stream","text":["NDCG@5 = nan\n","NDCG = nan\n","MRR = 0.0011\n","MAP = nan\n","RBP_80 = 0.0010\n","RBP_90 = 0.0008\n","RBP_95 = 0.0007\n"]},{"name":"stderr","output_type":"stream","text":["  4%|▍         | 5/120 [11:21\u003c4:19:50, 135.57s/it]"]},{"name":"stdout","output_type":"stream","text":["NDCG@5 = nan\n","NDCG = nan\n","MRR = 0.0011\n","MAP = nan\n","RBP_80 = 0.0011\n","RBP_90 = 0.0011\n","RBP_95 = 0.0011\n"]},{"name":"stderr","output_type":"stream","text":["  8%|▊         | 10/120 [22:38\u003c4:07:44, 135.13s/it]"]},{"name":"stdout","output_type":"stream","text":["NDCG@5 = nan\n","NDCG = nan\n","MRR = 0.0011\n","MAP = nan\n","RBP_80 = 0.0011\n","RBP_90 = 0.0011\n","RBP_95 = 0.0011\n"]},{"name":"stderr","output_type":"stream","text":[" 12%|█▎        | 15/120 [33:37\u003c3:50:00, 131.44s/it]"]},{"name":"stdout","output_type":"stream","text":["NDCG@5 = nan\n","NDCG = nan\n","MRR = 0.0011\n","MAP = nan\n","RBP_80 = 0.0011\n","RBP_90 = 0.0011\n","RBP_95 = 0.0011\n"]},{"name":"stderr","output_type":"stream","text":[" 17%|█▋        | 20/120 [45:24\u003c3:54:31, 140.71s/it]"]},{"name":"stdout","output_type":"stream","text":["NDCG@5 = nan\n","NDCG = nan\n","MRR = 0.0011\n","MAP = nan\n","RBP_80 = 0.0011\n","RBP_90 = 0.0011\n","RBP_95 = 0.0011\n"]},{"name":"stderr","output_type":"stream","text":[" 21%|██        | 25/120 [57:11\u003c3:45:08, 142.20s/it]"]},{"name":"stdout","output_type":"stream","text":["NDCG@5 = nan\n","NDCG = nan\n","MRR = 0.0011\n","MAP = nan\n","RBP_80 = 0.0011\n","RBP_90 = 0.0011\n","RBP_95 = 0.0011\n"]},{"name":"stderr","output_type":"stream","text":["\r 22%|██▏       | 26/120 [59:35\u003c3:43:17, 142.53s/it]"]}],"source":["# print(torch.cuda.memory_summary())\n","\n","# --------------------------------MODEL---------------------------------\n","model = LambdaMF(n_user, n_item+1, \n","\t\t\t   init_range=random_range, emb_size=emb_size).to(device)\n","\n","# ---------------------------Train and test---------------------------------------\n","train_loss_all = []\n","test_loss_all = []\n","epoch_all = [] \n","# leave an interface for the epochID incase we save the loss value larger than 1\n","Best_ndcg = Best_ndcg_at_5 = Best_ap = Best_rr = Best_rbp_80 = Best_rbp_90 = Best_rbp_95 = 0\n","columns = ['loss_type', 'lr', 'threshold', 'reg', 'fold', 'frac', 'emb_size', 'NDCG@5', 'NDCG', 'RR', 'AP', 'RBP_80', 'RBP_90', 'RBP_95']\n","columns_indi = ['NDCG@5', 'NDCG', 'AP', 'RR', 'RBP_80', 'RBP_90', 'RBP_95']\n","\n","# --------------------------Define optimizer----------------------------------\n","best_result = 0\n","weight_decay = reg\n","\n","optimizer = torch.optim.SGD(model.parameters(), lr=lr, weight_decay=weight_decay)\n"," \n","# model_file = \"model_\" + dataset + '_' + loss_type + '_' + str(lr) + '_' + str(p) + '_' + str(threshold) + '_' + str(reg) +  '_' + str(num_neg) + '_' + str(emb_size) + '_' + str(fold) + \".pth.tar\"\n","# save_path = os.path.join(args.save_path, model_file)\n","# checkpoint = torch.load(save_path)\n","# model.load_state_dict(checkpoint['state_dict'])\n","# optimizer.load_state_dict(checkpoint['optimizer'])\n","# best_result = checkpoint['best_performance']\n","# epoch = check\n","\n","for i in tqdm(range(epochs)):\n","\n","\ttrain_loss = loss_train_test(model, optimizer, train_loader, loss_type, device, \\\n","\t\t\t\t\t\t\t\tt, b, temp, p, f_rbp, num_pos, num_neg)\n","\t# print(train_loss)\n","\t## ------------Evaluation------------------------\n","\tif i == 0 or (i + 1) % 5 == 0:\n","\t\tNDCG_at_5, NDCG, RR, AP, RBP_80, RBP_90, RBP_95, ndcg_at_5, ndcg, mrr, mAP, rbp_80, rbp_90, rbp_95 = evaluation(model, test_loader, max_rating, device, k, p, n_item)\n","\t\tif loss_type == 'lambda_dcg':\n","\t\t\tresult_current = ndcg\n","\t\telif loss_type == 'lambda_rr':\n","\t\t\tresult_current = mrr\n","\t\telif loss_type == 'lambda_ap':\n","\t\t\tresult_current = mAP\n","\t\telif loss_type == 'lambda_rbp' and p == 0.80:\n","\t\t\tresult_current = rbp_80\n","\t\telif loss_type == 'lambda_rbp' and p == 0.9:\n","\t\t\tresult_current = rbp_90\n","\t\telif loss_type == 'lambda_rbp' and p == 0.95:\n","\t\t\tresult_current = rbp_95\n","# \t\telse:\n","# \t\t\tprint('loss function does not exist.')\n","\n","\t\tif result_current \u003e best_result:\n","\t\t\tepoch = i\n","\t\t\tbest_result = result_current\n","\t\t\t# best_hr = hr\n","\t\t\tbest_ndcg = ndcg\n","\t\t\tbest_ndcg_at_5 = ndcg_at_5\n","\t\t\tbest_mrr = mrr\n","\t\t\tbest_mAP = mAP\n","\t\t\tbest_rbp_80 = rbp_80\n","\t\t\tbest_rbp_90 = rbp_90\n","\t\t\tbest_rbp_95 = rbp_95\n","\t\t\tbest_NDCG = NDCG\n","\t\t\tbest_RR = RR\n","\t\t\tbest_AP = AP\n","\t\t\tbest_RBP_80 = RBP_80\n","\t\t\tbest_RBP_90 = RBP_90\n","\t\t\tbest_RBP_95 = RBP_95\n","\t\t\tmodel_file = \"model_\" + dataset + '_' + loss_type + '_' + str(lr) + '_' + str(p) + '_' + str(threshold) + '_' + str(reg) +  '_' + str(num_neg) + '_' + str(emb_size) + '_' + str(fold) + \".pth.tar\"\n","\t\t\tsave_path = os.path.join(args.save_path, model_file)\n","\t\t\tprint(\"Best\" + args.loss_type.upper() + \": %.4f\" % best_result)\n","\t\t\t# save_model(epoch, model, best_result, optimizer, save_path) \n","\t\t\tloss_type = loss_type\n","\t\t\t# if loss_type == 'lambda_rbp':\n","\t\t\t# \tif p == 0.80:\n","\t\t\t# \t\tloss_type == 'lambda_rbp_80'\n","\t\t\t# \telif p == 0.90:\n","\t\t\t# \t\tloss_typer == 'lambda_rbp_90'\n","\t\t\t# \telif p == 0.95:\n","\t\t\t# \t\tloss_typer == 'lambda_rbp_95'\n","\n","\t\t\tresult = pd.DataFrame([[loss_type, lr, threshold, reg, fold, frac, emb_size, best_ndcg_at_5, best_ndcg, best_mrr, best_mAP, best_rbp_80, best_rbp_90, best_rbp_95]], columns=columns).round(4)\n","\t\t\tresult_indi = list(zip(*[NDCG_at_5, NDCG, AP, RR, RBP_80, RBP_90, RBP_95]))\n","\t\t\tresult_indi = pd.DataFrame(result_indi, columns=columns_indi).round(4)\n","\t\t\tif loss_type == 'lambda_rbp':\n","\t\t\t\tname = 'loss_type_' + loss_type + '_' + str(p) + '_lr_' + str(lr) + '_th_' + str(threshold) + '_reg_' + str(reg) + '_fold_' + str(fold) + '_frac_' + str(frac) + '_emb_size_' + str(emb_size)\n","\t\t\telse:\n","\t\t\t\tname = 'loss_type_' + loss_type + '_lr_' + str(lr) + '_th_' + str(threshold) + '_reg_' + str(reg) + '_fold_' + str(fold) + '_frac_' + str(frac) + '_emb_size_' + str(emb_size)\n","\t\t\tres_path = os.path.join('./results/', dataset, 'overall')\n","\t\t\tres_path_indi = os.path.join('./results/', dataset, 'individual')\n","\n","\t\t\tdir_exists(res_path)\n","\t\t\tdir_exists(res_path_indi)\n","\t\t\tresult.to_csv(os.path.join(res_path, name+'.csv'), index=False)\n","\t\t\tresult_indi.to_csv(os.path.join(res_path_indi,  name+'.csv'), index=False)"]}],"metadata":{"accelerator":"GPU","colab":{"authorship_tag":"ABX9TyMSIyxMK95O6860YEb/rgPw","collapsed_sections":[],"name":"T468494 | Matrix Factorization Model with 4 Loss Functions on Epinions","version":""},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}