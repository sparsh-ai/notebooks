{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"reco-tut-ml-from-scratch-deep-learning.ipynb","provenance":[],"collapsed_sections":["pG6ocw7ivJD7","UX7ARKMewy_w","Lh4rL_8i0qgq","Z1Rs-eqv2oai","chufsACo234O","rWWNEeuq3WWN","qI2-Nc1R5EeB","7jdbcMHF8jJH","aYXh1248Abmt","JyekfPAoBRrX","zU7M17ypDPH8","04ZchhgUDgX5"],"mount_file_id":"1JjenVRGYD_Y8o3NFEVOvQOrG_nh0ttmM","authorship_tag":"ABX9TyOwidQMSjHefScS9wGi6AC8"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"G6pRZRSrvnrg"},"source":["## Supporting components"]},{"cell_type":"markdown","metadata":{"id":"YSA1acYJvq0V"},"source":["### Sigmoid function"]},{"cell_type":"code","metadata":{"id":"AjVnoVenvvZA"},"source":["import math\n","\n","def sigmoid(t: float) -> float:\n","    return 1 / (1 + math.exp(-t))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"EpEBiaKAxvq0"},"source":["### Inverse Normal CDF"]},{"cell_type":"code","metadata":{"id":"5Z5gA2lOxyAW"},"source":["import math\n","\n","def normal_cdf(x: float, mu: float = 0, sigma: float = 1) -> float:\n","    return (1 + math.erf((x - mu) / math.sqrt(2) / sigma)) / 2\n","\n","def inverse_normal_cdf(p: float,\n","                       mu: float = 0,\n","                       sigma: float = 1,\n","                       tolerance: float = 0.00001) -> float:\n","    \"\"\"Find approximate inverse using binary search\"\"\"\n","\n","    # if not standard, compute standard and rescale\n","    if mu != 0 or sigma != 1:\n","        return mu + sigma * inverse_normal_cdf(p, tolerance=tolerance)\n","\n","    low_z = -10.0                      # normal_cdf(-10) is (very close to) 0\n","    hi_z  =  10.0                      # normal_cdf(10)  is (very close to) 1\n","    while hi_z - low_z > tolerance:\n","        mid_z = (low_z + hi_z) / 2     # Consider the midpoint\n","        mid_p = normal_cdf(mid_z)      # and the cdf's value there\n","        if mid_p < p:\n","            low_z = mid_z              # Midpoint too low, search above it\n","        else:\n","            hi_z = mid_z               # Midpoint too high, search below it\n","\n","    return mid_z"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kCWYAcPay9V3"},"source":["### Dot product"]},{"cell_type":"code","metadata":{"id":"75RqxDUdy_Bl"},"source":["from typing import List\n","\n","Vector = List[float]\n","\n","def dot(v: Vector, w: Vector) -> float:\n","    \"\"\"Computes v_1 * w_1 + ... + v_n * w_n\"\"\"\n","    assert len(v) == len(w), \"vectors must be same length\"\n","\n","    return sum(v_i * w_i for v_i, w_i in zip(v, w))\n","\n","assert dot([1, 2, 3], [4, 5, 6]) == 32  # 1 * 4 + 2 * 5 + 3 * 6"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"w9uD95sx4XrG"},"source":["### Assignment operator"]},{"cell_type":"code","metadata":{"id":"gL_3-pwa4csg"},"source":["tensor = [[1, 2], [3, 4]]\n","\n","for row in tensor:\n","    row = [0, 0]\n","assert tensor == [[1, 2], [3, 4]], \"assignment doesn't update a list\"\n","\n","for row in tensor:\n","    row[:] = [0, 0]\n","assert tensor == [[0, 0], [0, 0]], \"but slice assignment does\""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"XpWk0z2eFOVB"},"source":["### One-hot encode"]},{"cell_type":"code","metadata":{"id":"QmYSkHmtFCC3"},"source":["def one_hot_encode(i: int, num_labels: int = 10) -> List[float]:\n","    return [1.0 if j == i else 0.0 for j in range(num_labels)]\n","\n","assert one_hot_encode(3) == [0, 0, 0, 1, 0, 0, 0, 0, 0, 0]\n","assert one_hot_encode(2, num_labels=5) == [0, 0, 1, 0, 0]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0aWx1fLUvLrN"},"source":["## Tensor"]},{"cell_type":"code","metadata":{"id":"094dTb7HvXjP"},"source":["Tensor = list"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"yItneJdjv-bK"},"source":["from typing import List\n","\n","def shape(tensor: Tensor) -> List[int]:\n","    sizes: List[int] = []\n","    while isinstance(tensor, list):\n","        sizes.append(len(tensor))\n","        tensor = tensor[0]\n","    return sizes\n","\n","assert shape([1, 2, 3]) == [3]\n","assert shape([[1, 2], [3, 4], [5, 6]]) == [3, 2]\n","\n","def is_1d(tensor: Tensor) -> bool:\n","    \"\"\"\n","    If tensor[0] is a list, it's a higher-order tensor.\n","    Otherwise, tensor is 1-dimensonal (that is, a vector).\n","    \"\"\"\n","    return not isinstance(tensor[0], list)\n","\n","assert is_1d([1, 2, 3])\n","assert not is_1d([[1, 2], [3, 4]])\n","\n","def tensor_sum(tensor: Tensor) -> float:\n","    \"\"\"Sums up all the values in the tensor\"\"\"\n","    if is_1d(tensor):\n","        return sum(tensor)  # just a list of floats, use Python sum\n","    else:\n","        return sum(tensor_sum(tensor_i)      # Call tensor_sum on each row\n","                   for tensor_i in tensor)   # and sum up those results.\n","\n","assert tensor_sum([1, 2, 3]) == 6\n","assert tensor_sum([[1, 2], [3, 4]]) == 10\n","\n","from typing import Callable\n","\n","def tensor_apply(f: Callable[[float], float], tensor: Tensor) -> Tensor:\n","    \"\"\"Applies f elementwise\"\"\"\n","    if is_1d(tensor):\n","        return [f(x) for x in tensor]\n","    else:\n","        return [tensor_apply(f, tensor_i) for tensor_i in tensor]\n","\n","assert tensor_apply(lambda x: x + 1, [1, 2, 3]) == [2, 3, 4]\n","assert tensor_apply(lambda x: 2 * x, [[1, 2], [3, 4]]) == [[2, 4], [6, 8]]\n","\n","def zeros_like(tensor: Tensor) -> Tensor:\n","    return tensor_apply(lambda _: 0.0, tensor)\n","\n","assert zeros_like([1, 2, 3]) == [0, 0, 0]\n","assert zeros_like([[1, 2], [3, 4]]) == [[0, 0], [0, 0]]\n","\n","def tensor_combine(f: Callable[[float, float], float],\n","                   t1: Tensor,\n","                   t2: Tensor) -> Tensor:\n","    \"\"\"Applies f to corresponding elements of t1 and t2\"\"\"\n","    if is_1d(t1):\n","        return [f(x, y) for x, y in zip(t1, t2)]\n","    else:\n","        return [tensor_combine(f, t1_i, t2_i)\n","                for t1_i, t2_i in zip(t1, t2)]\n","\n","import operator\n","assert tensor_combine(operator.add, [1, 2, 3], [4, 5, 6]) == [5, 7, 9]\n","assert tensor_combine(operator.mul, [1, 2, 3], [4, 5, 6]) == [4, 10, 18]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"m7rxN-uWu4OF"},"source":["## The Layer Abstraction"]},{"cell_type":"code","metadata":{"id":"foF7FLKlu6gb"},"source":["from typing import Iterable, Tuple\n","\n","class Layer:\n","    \"\"\"\n","    Our neural networks will be composed of Layers, each of which\n","    knows how to do some computation on its inputs in the \"forward\"\n","    direction and propagate gradients in the \"backward\" direction.\n","    \"\"\"\n","    def forward(self, input):\n","        \"\"\"\n","        Note the lack of types. We're not going to be prescriptive\n","        about what kinds of inputs layers can take and what kinds\n","        of outputs they can return.\n","        \"\"\"\n","        raise NotImplementedError\n","\n","    def backward(self, gradient):\n","        \"\"\"\n","        Similarly, we're not going to be prescriptive about what the\n","        gradient looks like. It's up to you the user to make sure\n","        that you're doing things sensibly.\n","        \"\"\"\n","        raise NotImplementedError\n","\n","    def params(self) -> Iterable[Tensor]:\n","        \"\"\"\n","        Returns the parameters of this layer. The default implementation\n","        returns nothing, so that if you have a layer with no parameters\n","        you don't have to implement this.\n","        \"\"\"\n","        return ()\n","\n","    def grads(self) -> Iterable[Tensor]:\n","        \"\"\"\n","        Returns the gradients, in the same order as params().\n","        \"\"\"\n","        return ()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"pG6ocw7ivJD7"},"source":["## Sigmoid Layer"]},{"cell_type":"markdown","metadata":{"id":"eYlYbcHGwNjq"},"source":["Some layers (for example, a layer that applies sigmoid to each of its inputs) have no parameters to update, so we provide a default implementation that handles that case."]},{"cell_type":"code","metadata":{"id":"EvxlEk2wwP_d"},"source":["class Sigmoid(Layer):\n","    def forward(self, input: Tensor) -> Tensor:\n","        \"\"\"\n","        Apply sigmoid to each element of the input tensor,\n","        and save the results to use in backpropagation.\n","        \"\"\"\n","        self.sigmoids = tensor_apply(sigmoid, input)\n","        return self.sigmoids\n","\n","    def backward(self, gradient: Tensor) -> Tensor:\n","        return tensor_combine(lambda sig, grad: sig * (1 - sig) * grad,\n","                              self.sigmoids,\n","                              gradient)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"UX7ARKMewy_w"},"source":["## Linear Layer"]},{"cell_type":"markdown","metadata":{"id":"uUajCJzzxf2N"},"source":["This layer will have parameters, which we’d like to initialize with random values.\n","\n","It turns out that the initial parameter values can make a huge difference in how quickly (and sometimes whether) the network trains. If weights are too big, they may produce large outputs in a range where the activation function has near-zero gradients. And parts of the network that have zero gradients necessarily can’t learn anything via gradient descent.\n","\n","Accordingly, we’ll implement three different schemes for randomly generating our weight tensors. The first is to choose each value from the random uniform distribution on [0, 1]—that is, as a random.random(). The second (and default) is to choose each value randomly from a standard normal distribution. And the third is to use Xavier initialization, where each weight is initialized with a random draw from a normal distribution with mean 0 and variance 2 / (num_inputs + num_outputs). It turns out this often works nicely for neural network weights. We’ll implement these with a random_uniform function and a random_normal function:"]},{"cell_type":"code","metadata":{"id":"sJS9wywqyUXO"},"source":["import random\n","\n","def random_uniform(*dims: int) -> Tensor:\n","    if len(dims) == 1:\n","        return [random.random() for _ in range(dims[0])]\n","    else:\n","        return [random_uniform(*dims[1:]) for _ in range(dims[0])]\n","\n","def random_normal(*dims: int,\n","                  mean: float = 0.0,\n","                  variance: float = 1.0) -> Tensor:\n","    if len(dims) == 1:\n","        return [mean + variance * inverse_normal_cdf(random.random())\n","                for _ in range(dims[0])]\n","    else:\n","        return [random_normal(*dims[1:], mean=mean, variance=variance)\n","                for _ in range(dims[0])]\n","\n","assert shape(random_uniform(2, 3, 4)) == [2, 3, 4]\n","assert shape(random_normal(5, 6, mean=10)) == [5, 6]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"63Kzr3-UyWaE"},"source":["And then wrap them all in a random_tensor function:"]},{"cell_type":"code","metadata":{"id":"l6YN2XrvycsU"},"source":["def random_tensor(*dims: int, init: str = 'normal') -> Tensor:\n","    if init == 'normal':\n","        return random_normal(*dims)\n","    elif init == 'uniform':\n","        return random_uniform(*dims)\n","    elif init == 'xavier':\n","        variance = len(dims) / sum(dims)\n","        return random_normal(*dims, variance=variance)\n","    else:\n","        raise ValueError(f\"unknown init: {init}\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kFkCPiU1yi_e"},"source":["Now we can define our linear layer. We need to initialize it with the dimension of the inputs (which tells us how many weights each neuron needs), the dimension of the outputs (which tells us how many neurons we should have), and the initialization scheme we want:"]},{"cell_type":"code","metadata":{"id":"miezLf8bzXxN"},"source":["class Linear(Layer):\n","    def __init__(self, input_dim: int, output_dim: int, init: str = 'xavier') -> None:\n","        \"\"\"\n","        A layer of output_dim neurons, each with input_dim weights\n","        (and a bias).\n","        \"\"\"\n","        self.input_dim = input_dim\n","        self.output_dim = output_dim\n","\n","        # self.w[o] is the weights for the o-th neuron\n","        self.w = random_tensor(output_dim, input_dim, init=init)\n","\n","        # self.b[o] is the bias term for the o-th neuron\n","        self.b = random_tensor(output_dim, init=init)\n","\n","    def forward(self, input: Tensor) -> Tensor:\n","        # Save the input to use in the backward pass.\n","        self.input = input\n","\n","        # Return the vector of neuron outputs.\n","        return [dot(input, self.w[o]) + self.b[o]\n","                for o in range(self.output_dim)]\n","\n","    def backward(self, gradient: Tensor) -> Tensor:\n","        # Each b[o] gets added to output[o], which means\n","        # the gradient of b is the same as the output gradient.\n","        self.b_grad = gradient\n","\n","        # Each w[o][i] multiplies input[i] and gets added to output[o].\n","        # So its gradient is input[i] * gradient[o].\n","        self.w_grad = [[self.input[i] * gradient[o]\n","                        for i in range(self.input_dim)]\n","                       for o in range(self.output_dim)]\n","\n","        # Each input[i] multiplies every w[o][i] and gets added to every\n","        # output[o]. So its gradient is the sum of w[o][i] * gradient[o]\n","        # across all the outputs.\n","        return [sum(self.w[o][i] * gradient[o] for o in range(self.output_dim))\n","                for i in range(self.input_dim)]\n","\n","    def params(self) -> Iterable[Tensor]:\n","        return [self.w, self.b]\n","\n","    def grads(self) -> Iterable[Tensor]:\n","        return [self.w_grad, self.b_grad]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Lh4rL_8i0qgq"},"source":["## Neural Networks as a Sequence of Layers\n","We’d like to think of neural networks as sequences of layers, so let’s come up with a way to combine multiple layers into one. The resulting neural network is itself a layer, and it implements the Layer methods in the obvious ways:"]},{"cell_type":"code","metadata":{"id":"hBlbwv4P2JAD"},"source":["class Sequential(Layer):\n","    \"\"\"\n","    A layer consisting of a sequence of other layers.\n","    It's up to you to make sure that the output of each layer\n","    makes sense as the input to the next layer.\n","    \"\"\"\n","    def __init__(self, layers: List[Layer]) -> None:\n","        self.layers = layers\n","\n","    def forward(self, input):\n","        \"\"\"Just forward the input through the layers in order.\"\"\"\n","        for layer in self.layers:\n","            input = layer.forward(input)\n","        return input\n","\n","    def backward(self, gradient):\n","        \"\"\"Just backpropagate the gradient through the layers in reverse.\"\"\"\n","        for layer in reversed(self.layers):\n","            gradient = layer.backward(gradient)\n","        return gradient\n","\n","    def params(self) -> Iterable[Tensor]:\n","        \"\"\"Just return the params from each layer.\"\"\"\n","        return (param for layer in self.layers for param in layer.params())\n","\n","    def grads(self) -> Iterable[Tensor]:\n","        \"\"\"Just return the grads from each layer.\"\"\"\n","        return (grad for layer in self.layers for grad in layer.grads())"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"n-bqb4zT2egt"},"source":["So we could represent the neural network we used for XOR as:"]},{"cell_type":"code","metadata":{"id":"Nqdap0u02j6w"},"source":["xor_net = Sequential([\n","    Linear(input_dim=2, output_dim=2),\n","    Sigmoid(),\n","    Linear(input_dim=2, output_dim=1),\n","    Sigmoid()\n","])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6vwBC3FQ2oIe"},"source":["But we still need a little more machinery to train it."]},{"cell_type":"markdown","metadata":{"id":"Z1Rs-eqv2oai"},"source":["## Loss\n","\n","Here we’ll want to experiment with different loss functions, so (as usual) we’ll introduce a new Loss abstraction that encapsulates both the loss computation and the gradient computation:"]},{"cell_type":"code","metadata":{"id":"RkEte1Hi2ySQ"},"source":["class Loss:\n","    def loss(self, predicted: Tensor, actual: Tensor) -> float:\n","        \"\"\"How good are our predictions? (Larger numbers are worse.)\"\"\"\n","        raise NotImplementedError\n","\n","    def gradient(self, predicted: Tensor, actual: Tensor) -> Tensor:\n","        \"\"\"How does the loss change as the predictions change?\"\"\"\n","        raise NotImplementedError"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"chufsACo234O"},"source":["### Sum of Squared Loss Function"]},{"cell_type":"code","metadata":{"id":"tSeMhuz33Qca"},"source":["class SSE(Loss):\n","    \"\"\"Loss function that computes the sum of the squared errors.\"\"\"\n","    def loss(self, predicted: Tensor, actual: Tensor) -> float:\n","        # Compute the tensor of squared differences\n","        squared_errors = tensor_combine(\n","            lambda predicted, actual: (predicted - actual) ** 2,\n","            predicted,\n","            actual)\n","\n","        # And just add them up\n","        return tensor_sum(squared_errors)\n","\n","    def gradient(self, predicted: Tensor, actual: Tensor) -> Tensor:\n","        return tensor_combine(\n","            lambda predicted, actual: 2 * (predicted - actual),\n","            predicted,\n","            actual)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"W-Eq6nOm3U_k"},"source":["sse_loss = SSE()\n","assert sse_loss.loss([1, 2, 3], [10, 20, 30]) == 9 ** 2 + 18 ** 2 + 27 ** 2\n","assert sse_loss.gradient([1, 2, 3], [10, 20, 30]) == [-18, -36, -54]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rWWNEeuq3WWN"},"source":["## Optimization\n","we’ll build a Optimizer abstraction, of which gradient descent will be a specific instance:"]},{"cell_type":"code","metadata":{"id":"s3OXf5qy3saZ"},"source":["class Optimizer:\n","    \"\"\"\n","    An optimizer updates the weights of a layer (in place) using information\n","    known by either the layer or the optimizer (or by both).\n","    \"\"\"\n","    def step(self, layer: Layer) -> None:\n","        raise NotImplementedError"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DeeyHONy30Bn"},"source":["After that it’s easy to implement gradient descent, again using tensor_combine:"]},{"cell_type":"code","metadata":{"id":"sQ36BEhF32B1"},"source":["class GradientDescent(Optimizer):\n","    def __init__(self, learning_rate: float = 0.1) -> None:\n","        self.lr = learning_rate\n","\n","    def step(self, layer: Layer) -> None:\n","        for param, grad in zip(layer.params(), layer.grads()):\n","            # Update param using a gradient step\n","            param[:] = tensor_combine(\n","                lambda param, grad: param - grad * self.lr,\n","                param,\n","                grad)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hPQ9uTfF4xXu"},"source":["To demonstrate the value of this abstraction, let’s implement another optimizer that uses momentum. The idea is that we don’t want to overreact to each new gradient, and so we maintain a running average of the gradients we’ve seen, updating it with each new gradient and taking a step in the direction of the average:"]},{"cell_type":"code","metadata":{"id":"jALUKECW4xzv"},"source":["class Momentum(Optimizer):\n","    def __init__(self,\n","                 learning_rate: float,\n","                 momentum: float = 0.9) -> None:\n","        self.lr = learning_rate\n","        self.mo = momentum\n","        self.updates: List[Tensor] = []  # running average\n","\n","    def step(self, layer: Layer) -> None:\n","        # If we have no previous updates, start with all zeros.\n","        if not self.updates:\n","            self.updates = [zeros_like(grad) for grad in layer.grads()]\n","\n","        for update, param, grad in zip(self.updates,\n","                                       layer.params(),\n","                                       layer.grads()):\n","            # Apply momentum\n","            update[:] = tensor_combine(\n","                lambda u, g: self.mo * u + (1 - self.mo) * g,\n","                update,\n","                grad)\n","\n","            # Then take a gradient step\n","            param[:] = tensor_combine(\n","                lambda p, u: p - self.lr * u,\n","                param,\n","                update)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qI2-Nc1R5EeB"},"source":["## XOR Model"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0cYP7mHm5PVc","executionInfo":{"status":"ok","timestamp":1630995509405,"user_tz":-330,"elapsed":11228,"user":{"displayName":"Sparsh Agarwal","photoUrl":"","userId":"13037694610922482904"}},"outputId":"798b8d7a-c751-4db3-be4c-806ecec715b6"},"source":["import tqdm\n","\n","# training data\n","xs = [[0., 0], [0., 1], [1., 0], [1., 1]]\n","ys = [[0.], [1.], [1.], [0.]]\n","\n","random.seed(0)\n","\n","net = Sequential([\n","    Linear(input_dim=2, output_dim=2),\n","    Sigmoid(),\n","    Linear(input_dim=2, output_dim=1)\n","])\n","\n","optimizer = GradientDescent(learning_rate=0.1)\n","loss = SSE()\n","\n","with tqdm.trange(3000) as t:\n","    for epoch in t:\n","        epoch_loss = 0.0\n","\n","        for x, y in zip(xs, ys):\n","            predicted = net.forward(x)\n","            epoch_loss += loss.loss(predicted, y)\n","            gradient = loss.gradient(predicted, y)\n","            net.backward(gradient)\n","\n","            optimizer.step(net)\n","\n","        t.set_description(f\"xor loss {epoch_loss:.3f}\")"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["xor loss 0.000: 100%|██████████| 3000/3000 [00:07<00:00, 403.38it/s]\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xXyK9SzZ6BkF","executionInfo":{"status":"ok","timestamp":1630995519928,"user_tz":-330,"elapsed":493,"user":{"displayName":"Sparsh Agarwal","photoUrl":"","userId":"13037694610922482904"}},"outputId":"121d262a-2702-4ba7-e7f8-a94caf67a792"},"source":["for param in net.params():\n","    print(param)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[[-1.6425160695224101, -1.4948117798303153], [-4.567646572029667, -3.3649176350731924]]\n","[1.7673716823255166, 0.38727014379473046]\n","[[3.198620479170406, -3.5018030621426175]]\n","[-0.6462765963362207]\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"efPf5YAD6JSA","executionInfo":{"status":"ok","timestamp":1630996112747,"user_tz":-330,"elapsed":18,"user":{"displayName":"Sparsh Agarwal","photoUrl":"","userId":"13037694610922482904"}},"outputId":"91987929-2f23-4229-e4c3-23af4f9fa76b"},"source":["x1 = 0\n","x2 = 0\n","\n","w_h1x1 = -1.64\n","w_h1x2 = -1.49\n","w_h2x1 = -4.57\n","w_h2x2 = -3.36\n","\n","w_h1o = 1.77\n","w_h2o = 0.39\n","\n","b_h1 = 3.2\n","b_h2 = -3.5\n","\n","b_o = -0.65\n","\n","h1 = w_h1x1 * x1 + w_h1x2 * x2 + b_h1\n","h2 = w_h2x1 * x1 + w_h2x2 * x2 + b_h2\n","\n","output = w_h1o * h1 + w_h2o * h2 + b_o\n","output"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["3.6490000000000005"]},"metadata":{},"execution_count":27}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"r_MbLaNT8XpG","executionInfo":{"status":"ok","timestamp":1630996149923,"user_tz":-330,"elapsed":427,"user":{"displayName":"Sparsh Agarwal","photoUrl":"","userId":"13037694610922482904"}},"outputId":"e39e614e-3509-4a54-c97a-883801a6a4eb"},"source":["h1 = -2.6 * x1 + -2.7 * x2 + 0.2  # NOR\n","h2 =  2.1 * x1 +  2.1 * x2 - 3.4  # AND\n","output =  -3.1 * h1 + -2.6 * h2 + 1.8  # NOR\n","output"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["10.02"]},"metadata":{},"execution_count":28}]},{"cell_type":"markdown","metadata":{"id":"7jdbcMHF8jJH"},"source":["## Other Activation Functions\n","The sigmoid function has fallen out of favor for a couple of reasons. One reason is that sigmoid(0) equals 1/2, which means that a neuron whose inputs sum to 0 has a positive output. Another is that its gradient is very close to 0 for very large and very small inputs, which means that its gradients can get “saturated” and its weights can get stuck.\n","\n","One popular replacement is tanh (“hyperbolic tangent”), which is a different sigmoid-shaped function that ranges from –1 to 1 and outputs 0 if its input is 0. The derivative of tanh(x) is just 1 - tanh(x) ** 2, which makes the layer easy to write:"]},{"cell_type":"code","metadata":{"id":"nvCRusqIALYB"},"source":["import math\n","\n","def tanh(x: float) -> float:\n","    # If x is very large or very small, tanh is (essentially) 1 or -1.\n","    # We check for this because e.g. math.exp(1000) raises an error.\n","    if x < -100:  return -1\n","    elif x > 100: return 1\n","\n","    em2x = math.exp(-2 * x)\n","    return (1 - em2x) / (1 + em2x)\n","\n","class Tanh(Layer):\n","    def forward(self, input: Tensor) -> Tensor:\n","        # Save tanh output to use in backward pass.\n","        self.tanh = tensor_apply(tanh, input)\n","        return self.tanh\n","\n","    def backward(self, gradient: Tensor) -> Tensor:\n","        return tensor_combine(\n","            lambda tanh, grad: (1 - tanh ** 2) * grad,\n","            self.tanh,\n","            gradient)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Nb3ApyvqATDK"},"source":["In larger networks another popular replacement is Relu, which is 0 for negative inputs and the identity for positive inputs:"]},{"cell_type":"code","metadata":{"id":"ejTfUu2-AVvW"},"source":["class Relu(Layer):\n","    def forward(self, input: Tensor) -> Tensor:\n","        self.input = input\n","        return tensor_apply(lambda x: max(x, 0), input)\n","\n","    def backward(self, gradient: Tensor) -> Tensor:\n","        return tensor_combine(lambda x, grad: grad if x > 0 else 0,\n","                              self.input,\n","                              gradient)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"aYXh1248Abmt"},"source":["## FizzBuzz"]},{"cell_type":"code","metadata":{"id":"9uDHd_cEAgPU"},"source":["def binary_encode(x: int) -> Vector:\n","    binary: List[float] = []\n","\n","    for i in range(10):\n","        binary.append(x % 2)\n","        x = x // 2\n","\n","    return binary\n","\n","#                             1  2  4  8 16 32 64 128 256 512\n","assert binary_encode(0)   == [0, 0, 0, 0, 0, 0, 0, 0,  0,  0]\n","assert binary_encode(1)   == [1, 0, 0, 0, 0, 0, 0, 0,  0,  0]\n","assert binary_encode(10)  == [0, 1, 0, 1, 0, 0, 0, 0,  0,  0]\n","assert binary_encode(101) == [1, 0, 1, 0, 0, 1, 1, 0,  0,  0]\n","assert binary_encode(999) == [1, 1, 1, 0, 0, 1, 1, 1,  1,  1]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"aUDKX8y1A0_N"},"source":["def argmax(xs: list) -> int:\n","    \"\"\"Returns the index of the largest value\"\"\"\n","    return max(range(len(xs)), key=lambda i: xs[i])\n","\n","assert argmax([0, -1]) == 0               # items[0] is largest\n","assert argmax([-1, 0]) == 1               # items[1] is largest\n","assert argmax([-1, 10, 5, 20, -3]) == 3   # items[3] is largest"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NDbR5vkqA26W"},"source":["def fizz_buzz_encode(x: int) -> Vector:\n","    if x % 15 == 0:\n","        return [0, 0, 0, 1]\n","    elif x % 5 == 0:\n","        return [0, 0, 1, 0]\n","    elif x % 3 == 0:\n","        return [0, 1, 0, 0]\n","    else:\n","        return [1, 0, 0, 0]\n","\n","assert fizz_buzz_encode(2) == [1, 0, 0, 0]\n","assert fizz_buzz_encode(6) == [0, 1, 0, 0]\n","assert fizz_buzz_encode(10) == [0, 0, 1, 0]\n","assert fizz_buzz_encode(30) == [0, 0, 0, 1]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HJxZVHV2A5JW","executionInfo":{"status":"ok","timestamp":1630997995647,"user_tz":-330,"elapsed":606773,"user":{"displayName":"Sparsh Agarwal","photoUrl":"","userId":"13037694610922482904"}},"outputId":"dc3b5f74-485e-4b61-af22-ce0a5ddbb7b9"},"source":["xs = [binary_encode(n) for n in range(101, 1024)]\n","ys = [fizz_buzz_encode(n) for n in range(101, 1024)]\n","\n","NUM_HIDDEN = 25\n","\n","random.seed(0)\n","\n","net = Sequential([\n","    Linear(input_dim=10, output_dim=NUM_HIDDEN, init='uniform'),\n","    Tanh(),\n","    Linear(input_dim=NUM_HIDDEN, output_dim=4, init='uniform'),\n","    Sigmoid()\n","])\n","\n","def fizzbuzz_accuracy(low: int, hi: int, net: Layer) -> float:\n","    num_correct = 0\n","    for n in range(low, hi):\n","        x = binary_encode(n)\n","        predicted = argmax(net.forward(x))\n","        actual = argmax(fizz_buzz_encode(n))\n","        if predicted == actual:\n","            num_correct += 1\n","\n","    return num_correct / (hi - low)\n","\n","optimizer = Momentum(learning_rate=0.1, momentum=0.9)\n","loss = SSE()\n","\n","with tqdm.trange(1000) as t:\n","    for epoch in t:\n","        epoch_loss = 0.0\n","\n","        for x, y in zip(xs, ys):\n","            predicted = net.forward(x)\n","            epoch_loss += loss.loss(predicted, y)\n","            gradient = loss.gradient(predicted, y)\n","            net.backward(gradient)\n","\n","            optimizer.step(net)\n","\n","        accuracy = fizzbuzz_accuracy(101, 1024, net)\n","        t.set_description(f\"fb loss: {epoch_loss:.2f} acc: {accuracy:.2f}\")\n","\n","# Now check results on the test set\n","print(\"test results\", fizzbuzz_accuracy(1, 101, net))"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["fb loss: 64.54 acc: 0.95: 100%|██████████| 1000/1000 [10:06<00:00,  1.65it/s]"]},{"output_type":"stream","name":"stdout","text":["test results 0.9\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}]},{"cell_type":"markdown","metadata":{"id":"JyekfPAoBRrX"},"source":["## Softmaxes and Cross-Entropy\n","The neural net we used in the previous section ended in a Sigmoid layer, which means that its output was a vector of numbers between 0 and 1. In particular, it could output a vector that was entirely 0s, or it could output a vector that was entirely 1s. Yet when we’re doing classification problems, we’d like to output a 1 for the correct class and a 0 for all the incorrect classes. Generally our predictions will not be so perfect, but we’d at least like to predict an actual probability distribution over the classes.\n","\n","For example, if we have two classes, and our model outputs [0, 0], it’s hard to make much sense of that. It doesn’t think the output belongs in either class?\n","\n","But if our model outputs [0.4, 0.6], we can interpret it as a prediction that there’s a probability of 0.4 that our input belongs to the first class and 0.6 that our input belongs to the second class.\n","\n","In order to accomplish this, we typically forgo the final Sigmoid layer and instead use the softmax function, which converts a vector of real numbers to a vector of probabilities. We compute exp(x) for each number in the vector, which results in a vector of positive numbers. After that, we just divide each of those positive numbers by the sum, which gives us a bunch of positive numbers that add up to 1—that is, a vector of probabilities."]},{"cell_type":"code","metadata":{"id":"ncIY84WrCZqj"},"source":["def softmax(tensor: Tensor) -> Tensor:\n","    \"\"\"Softmax along the last dimension\"\"\"\n","    if is_1d(tensor):\n","        # Subtract largest value for numerical stability.\n","        largest = max(tensor)\n","        exps = [math.exp(x - largest) for x in tensor]\n","\n","        sum_of_exps = sum(exps)                 # This is the total \"weight.\"\n","        return [exp_i / sum_of_exps             # Probability is the fraction\n","                for exp_i in exps]              # of the total weight.\n","    else:\n","        return [softmax(tensor_i) for tensor_i in tensor]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9JZiuSGQCbwK"},"source":["Once our network produces probabilities, we often use a different loss function called cross-entropy (or sometimes “negative log likelihood”).\n","\n","If our network outputs are probabilities, the cross-entropy loss represents the negative log likelihood of the observed data, which means that minimizing that loss is the same as maximizing the log likelihood (and hence the likelihood) of the training data.\n","\n","Typically we won’t include the softmax function as part of the neural network itself. This is because it turns out that if softmax is part of your loss function but not part of the network itself, the gradients of the loss with respect to the network outputs are very easy to compute."]},{"cell_type":"code","metadata":{"id":"znZHCTe8CfyE"},"source":["class SoftmaxCrossEntropy(Loss):\n","    \"\"\"\n","    This is the negative-log-likelihood of the observed values, given the\n","    neural net model. So if we choose weights to minimize it, our model will\n","    be maximizing the likelihood of the observed data.\n","    \"\"\"\n","    def loss(self, predicted: Tensor, actual: Tensor) -> float:\n","        # Apply softmax to get probabilities\n","        probabilities = softmax(predicted)\n","\n","        # This will be log p_i for the actual class i and 0 for the other\n","        # classes. We add a tiny amount to p to avoid taking log(0).\n","        likelihoods = tensor_combine(lambda p, act: math.log(p + 1e-30) * act,\n","                                     probabilities,\n","                                     actual)\n","\n","        # And then we just sum up the negatives.\n","        return -tensor_sum(likelihoods)\n","\n","    def gradient(self, predicted: Tensor, actual: Tensor) -> Tensor:\n","        probabilities = softmax(predicted)\n","\n","        # Isn't this a pleasant equation?\n","        return tensor_combine(lambda p, actual: p - actual,\n","                              probabilities,\n","                              actual)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zU7M17ypDPH8"},"source":["## FizzBuzz Revisited\n","\n","If I now train the same Fizz Buzz network using SoftmaxCrossEntropy loss, I find that it typically trains much faster (that is, in many fewer epochs). Presumably this is because it is much easier to find weights that softmax to a given distribution than it is to find weights that sigmoid to a given distribution.\n","\n","That is, if I need to predict class 0 (a vector with a 1 in the first position and 0s in the remaining positions), in the linear + sigmoid case I need the first output to be a large positive number and the remaining outputs to be large negative numbers. In the softmax case, however, I just need the first output to be larger than the remaining outputs. Clearly there are a lot more ways for the second case to happen, which suggests that it should be easier to find weights that make it so:"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kIcU2I6YDSfP","executionInfo":{"status":"ok","timestamp":1630998060037,"user_tz":-330,"elapsed":62657,"user":{"displayName":"Sparsh Agarwal","photoUrl":"","userId":"13037694610922482904"}},"outputId":"2ab3f971-18a5-4cb9-9ab9-31445e069f86"},"source":["random.seed(0)\n","\n","net = Sequential([\n","    Linear(input_dim=10, output_dim=NUM_HIDDEN, init='uniform'),\n","    Tanh(),\n","    Linear(input_dim=NUM_HIDDEN, output_dim=4, init='uniform')\n","    # No final sigmoid layer now\n","])\n","\n","optimizer = Momentum(learning_rate=0.1, momentum=0.9)\n","loss = SoftmaxCrossEntropy()\n","\n","with tqdm.trange(100) as t:\n","    for epoch in t:\n","        epoch_loss = 0.0\n","\n","        for x, y in zip(xs, ys):\n","            predicted = net.forward(x)\n","            epoch_loss += loss.loss(predicted, y)\n","            gradient = loss.gradient(predicted, y)\n","            net.backward(gradient)\n","\n","            optimizer.step(net)\n","\n","        accuracy = fizzbuzz_accuracy(101, 1024, net)\n","        t.set_description(f\"fb loss: {epoch_loss:.3f} acc: {accuracy:.2f}\")\n","\n","# Again check results on the test set\n","print(\"test results\", fizzbuzz_accuracy(1, 101, net))"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["fb loss: 6.116 acc: 1.00: 100%|██████████| 100/100 [01:02<00:00,  1.60it/s]"]},{"output_type":"stream","name":"stdout","text":["test results 0.97\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}]},{"cell_type":"markdown","metadata":{"id":"04ZchhgUDgX5"},"source":["## Dropout\n","A common way of regularizing neural networks is using dropout. At training time, we randomly turn off each neuron (that is, replace its output with 0) with some fixed probability. This means that the network can’t learn to depend on any individual neuron, which seems to help with overfitting.\n","\n","At evaluation time, we don’t want to dropout any neurons, so a Dropout layer will need to know whether it’s training or not. In addition, at training time a Dropout layer only passes on some random fraction of its input. To make its output comparable during evaluation, we’ll scale down the outputs (uniformly) using that same fraction:"]},{"cell_type":"code","metadata":{"id":"vFV8sfRZDt-S"},"source":["class Dropout(Layer):\n","    def __init__(self, p: float) -> None:\n","        self.p = p\n","        self.train = True\n","\n","    def forward(self, input: Tensor) -> Tensor:\n","        if self.train:\n","            # Create a mask of 0s and 1s shaped like the input\n","            # using the specified probability.\n","            self.mask = tensor_apply(\n","                lambda _: 0 if random.random() < self.p else 1,\n","                input)\n","            # Multiply by the mask to dropout inputs.\n","            return tensor_combine(operator.mul, input, self.mask)\n","        else:\n","            # During evaluation just scale down the outputs uniformly.\n","            return tensor_apply(lambda x: x * (1 - self.p), input)\n","\n","    def backward(self, gradient: Tensor) -> Tensor:\n","        if self.train:\n","            # Only propagate the gradients where mask == 1\n","            return tensor_combine(operator.mul, gradient, self.mask)\n","        else:\n","            raise RuntimeError(\"don't call backward when not in train mode\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3ixnJz77D8zk"},"source":["## MNIST"]},{"cell_type":"markdown","metadata":{"id":"ULgeH-3nEFvU"},"source":["MNIST is a dataset of handwritten digits that everyone uses to learn deep learning.\n","\n","It is available in a somewhat tricky binary format, so we’ll install the mnist library to work with it. (Yes, this part is technically not “from scratch.”)"]},{"cell_type":"code","metadata":{"id":"2mYpMrJJELpj"},"source":["!pip install -q mnist"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"njOqdb-jEOvP"},"source":["import mnist\n","\n","# This will download the data; change this to where you want it.\n","# (Yes, it's a 0-argument function, that's what the library expects.)\n","# (Yes, I'm assigning a lambda to a variable, like I said never to do.)\n","mnist.temporary_dir = lambda: '/tmp'\n","\n","# Each of these functions first downloads the data and returns a numpy array.\n","# We call .tolist() because our \"tensors\" are just lists.\n","train_images = mnist.train_images().tolist()\n","train_labels = mnist.train_labels().tolist()\n","\n","assert shape(train_images) == [60000, 28, 28]\n","assert shape(train_labels) == [60000]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":252},"id":"ROabOZgsEUzd","executionInfo":{"status":"ok","timestamp":1630998196843,"user_tz":-330,"elapsed":3711,"user":{"displayName":"Sparsh Agarwal","photoUrl":"","userId":"13037694610922482904"}},"outputId":"9e166d89-06d0-4c9b-97eb-ae27145d1b83"},"source":["import matplotlib.pyplot as plt\n","\n","fig, ax = plt.subplots(10, 10)\n","\n","for i in range(10):\n","    for j in range(10):\n","        # Plot each image in black and white and hide the axes.\n","        ax[i][j].imshow(train_images[10 * i + j], cmap='Greys')\n","        ax[i][j].xaxis.set_visible(False)\n","        ax[i][j].yaxis.set_visible(False)\n","\n","plt.show()"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 100 Axes>"]},"metadata":{"needs_background":"light"}}]},{"cell_type":"markdown","metadata":{"id":"yQ-dlzj4Eb-W"},"source":["We also need to load the test images:"]},{"cell_type":"code","metadata":{"id":"_McDmHFpEVCb"},"source":["test_images = mnist.test_images().tolist()\n","test_labels = mnist.test_labels().tolist()\n","\n","assert shape(test_images) == [10000, 28, 28]\n","assert shape(test_labels) == [10000]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"GxYdBp44Ebqd"},"source":["# Recenter the images\n","\n","# Compute the average pixel value\n","avg = tensor_sum(train_images) / 60000 / 28 / 28\n","\n","# Recenter, rescale, and flatten\n","train_images = [[(pixel - avg) / 256 for row in image for pixel in row]\n","                for image in train_images]\n","test_images = [[(pixel - avg) / 256 for row in image for pixel in row]\n","                for image in test_images]\n","\n","assert shape(train_images) == [60000, 784], \"images should be flattened\"\n","assert shape(test_images) == [10000, 784], \"images should be flattened\"\n","\n","# After centering, average pixel should be very close to 0\n","assert -0.0001 < tensor_sum(train_images) < 0.0001\n","\n","\n","# One-hot encode the test data\n","\n","train_labels = [one_hot_encode(label) for label in train_labels]\n","test_labels = [one_hot_encode(label) for label in test_labels]\n","\n","assert shape(train_labels) == [60000, 10]\n","assert shape(test_labels) == [10000, 10]\n","\n","\n","# Training loop\n","\n","import tqdm\n","\n","def loop(model: Layer,\n","            images: List[Tensor],\n","            labels: List[Tensor],\n","            loss: Loss,\n","            optimizer: Optimizer = None) -> None:\n","    correct = 0         # Track number of correct predictions.\n","    total_loss = 0.0    # Track total loss.\n","\n","    with tqdm.trange(len(images)) as t:\n","        for i in t:\n","            predicted = model.forward(images[i])             # Predict.\n","            if argmax(predicted) == argmax(labels[i]):       # Check for\n","                correct += 1                                 # correctness.\n","            total_loss += loss.loss(predicted, labels[i])    # Compute loss.\n","\n","            # If we're training, backpropagate gradient and update weights.\n","            if optimizer is not None:\n","                gradient = loss.gradient(predicted, labels[i])\n","                model.backward(gradient)\n","                optimizer.step(model)\n","\n","            # And update our metrics in the progress bar.\n","            avg_loss = total_loss / (i + 1)\n","            acc = correct / (i + 1)\n","            t.set_description(f\"mnist loss: {avg_loss:.3f} acc: {acc:.3f}\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kMGFrU2CE1hr"},"source":["### The logistic regression model for MNIST"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9PO0QKLwE3vd","executionInfo":{"status":"ok","timestamp":1630999307088,"user_tz":-330,"elapsed":840827,"user":{"displayName":"Sparsh Agarwal","photoUrl":"","userId":"13037694610922482904"}},"outputId":"7a3c5ba9-ee02-4da3-ea9f-cb1a6065bc48"},"source":["random.seed(0)\n","\n","# Logistic regression is just a linear layer followed by softmax\n","model = Linear(784, 10)\n","loss = SoftmaxCrossEntropy()\n","\n","# This optimizer seems to work\n","optimizer = Momentum(learning_rate=0.01, momentum=0.99)\n","\n","# Train on the training data\n","loop(model, train_images, train_labels, loss, optimizer)\n","\n","# Test on the test data (no optimizer means just evaluate)\n","loop(model, test_images, test_labels, loss)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["mnist loss: 0.357 acc: 0.898: 100%|██████████| 60000/60000 [13:19<00:00, 75.09it/s]\n","mnist loss: 0.361 acc: 0.891: 100%|██████████| 10000/10000 [00:41<00:00, 241.92it/s]\n"]}]},{"cell_type":"markdown","metadata":{"id":"st_YKd0FFYsO"},"source":["### A deep neural network for MNIST"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"L9urdRbVFbB8","executionInfo":{"status":"ok","timestamp":1631001597245,"user_tz":-330,"elapsed":2290203,"user":{"displayName":"Sparsh Agarwal","photoUrl":"","userId":"13037694610922482904"}},"outputId":"5ed624ad-2718-4b65-c99d-487bb465334e"},"source":["random.seed(0)\n","\n","# Name them so we can turn train on and off\n","dropout1 = Dropout(0.1)\n","dropout2 = Dropout(0.1)\n","\n","model = Sequential([\n","    Linear(784, 30),  # Hidden layer 1: size 30\n","    dropout1,\n","    Tanh(),\n","    Linear(30, 10),   # Hidden layer 2: size 10\n","    dropout2,\n","    Tanh(),\n","    Linear(10, 10)    # Output layer: size 10\n","])\n","\n","\n","# Training the deep model for MNIST\n","\n","optimizer = Momentum(learning_rate=0.01, momentum=0.99)\n","loss = SoftmaxCrossEntropy()\n","\n","# Enable dropout and train (takes > 20 minutes on my laptop!)\n","dropout1.train = dropout2.train = True\n","loop(model, train_images, train_labels, loss, optimizer)\n","\n","# Disable dropout and evaluate\n","dropout1.train = dropout2.train = False\n","loop(model, test_images, test_labels, loss)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["mnist loss: 0.486 acc: 0.857: 100%|██████████| 60000/60000 [36:57<00:00, 27.06it/s]\n","mnist loss: 0.313 acc: 0.908: 100%|██████████| 10000/10000 [01:13<00:00, 136.94it/s]\n"]}]},{"cell_type":"code","metadata":{"id":"bIKNizVBFfAd"},"source":[""],"execution_count":null,"outputs":[]}]}