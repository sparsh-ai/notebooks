{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"2022-01-04-mot.ipynb","provenance":[{"file_id":"https://github.com/recohut/nbs/blob/main/raw/6c80309f-ce1e-4029-b551-5bdef599ca33.ipynb","timestamp":1644432107258}],"toc_visible":true,"collapsed_sections":[],"authorship_tag":"ABX9TyMR+69zRzJw56qVRlHwSrGn"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","source":["# Multiple Object Tracking with PyTorch"],"metadata":{"id":"Mk6lds2Tyoj_"}},{"cell_type":"markdown","metadata":{"id":"QX1esx9JMpFR"},"source":["---\n","**Reference**\n","\n","\n","*   Object Detection with Faster R-CNN: https://www.learnopencv.com/faster-r-cnn-object-detection-with-pytorch/ \n","*   Simple Online and Realtime Tracking (SORT) algorithm for object ID tracking: https://arxiv.org/abs/1602.00763\n","\n","\n","**Question**\n",": What is Multiple Object Tracking?\n","- Object tracking is one of the tasks in computer vision, which is detecting an object and searching for that object in a video or a series of images (actually both meaning the same thing). \n","- Surveillance cameras in public places for spotting suspicious activities or crimes, and a computer system called 'Hawk-eye' for tracking the trajectory of the ball in various sports are typical examples of applying object tracking in a real life.\n","\n","\n","**Goals**\n","\n","\n","1. We will use MOT17Det Dataset\n","2. First part: Object Detection with **Faster R-CNN**\n","3. Second part: Multiple Object(ID) Tracking with  **Simple Online and Realtime Tracking (SORT)** algorithm\n","\n","\n","\n","---\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"hzmQhgb43MMg"},"source":["**0. Preparation**\n","\n","\n","*   For your convenience, it is recommended to mount your Google Drive first.\n","*   Then create extra space for this tutorial in there.\n","\n","\n","---\n","\n","\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"cnS6Q7M7C2VP"},"source":["from google.colab import drive\n","root = '/content/drive/'\n","drive.mount(root)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"gf8jnmft4v7Q"},"source":["# Making Directory\n","\n","import os \n","from os.path import join\n","\n","mot = \"My Drive/Colab Notebooks/MOT/\"   # a custom path. you can change if you want to\n","MOT_PATH = join(root,mot)\n","!mkdir \"{MOT_PATH}\""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1zHl8FW6p1DI"},"source":["\n","\n","---\n","\n","\n","**1. Dataset**\n","\n","\n","*   https://motchallenge.net/ : MOT17Det Dataset for Pedestrian Detection Challenge\n","*   We will only use MOT17-09 dataset for our task.\n","\n","\n","---\n","\n","\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"O9lkwFuo2CK3"},"source":["# Download MOT17Det Dataset\n","\n","!wget -P \"{MOT_PATH}\" https://motchallenge.net/data/MOT17Det.zip\n","!cd \"{MOT_PATH}\";unzip MOT17Det.zip"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9oHlCPGp6xZx"},"source":["# Remove unwanted data for drive volume issue (optional)\n","\n","!cd \"{MOT_PATH}\";rm -rf test\n","!cd \"{MOT_PATH}\";rm -rf train/MOT17-02;rm -rf train/MOT17-04;rm -rf train/MOT17-05\n","!cd \"{MOT_PATH}\";rm -rf train/MOT17-10;rm -rf train/MOT17-11;rm -rf train/MOT17-13"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Ct_3R8Nf-GlT"},"source":["import sys\n","\n","motdata = join(MOT_PATH,'train/MOT17-09/img1/')\n","sys.path.append(motdata)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_evT8k1S9GQU"},"source":["# Example: Original picture before detection\n","\n","import matplotlib.pylab as plt\n","import cv2\n","\n","list_motdata = os.listdir(motdata)  \n","list_motdata.sort()\n","\n","img_ex_path = motdata + list_motdata[0]\n","img_ex_origin = cv2.imread(img_ex_path)\n","img_ex = cv2.cvtColor(img_ex_origin, cv2.COLOR_BGR2RGB)\n","\n","plt.imshow(img_ex)\n","plt.axis('off')\n","plt.show()\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"nvuU_xZjqrJd"},"source":["\n","\n","---\n","\n","\n","**2. Object Detection with Faster R-CNN**\n","\n","*  We will use a pretrained Faster R-CNN model using ResNet50 as a backbone with FPN.\n","\n","\n","\n","\n","\n","---\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"7IEdaeDV6xfZ"},"source":["# Import required packages/modules first\n","\n","from PIL import Image\n","import numpy as np\n","import torch\n","import torchvision\n","from torchvision import transforms as T"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"vaFru4rA6xrr"},"source":["# Download the pretrained Faster R-CNN model from torchvision\n","\n","model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n","model.eval()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"KBcUWr2W9Tfe"},"source":["# Define the class names given by PyTorch's official Docs\n","\n","COCO_INSTANCE_CATEGORY_NAMES = [\n","    '__background__', 'person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus',\n","    'train', 'truck', 'boat', 'traffic light', 'fire hydrant', 'N/A', 'stop sign',\n","    'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse', 'sheep', 'cow',\n","    'elephant', 'bear', 'zebra', 'giraffe', 'N/A', 'backpack', 'umbrella', 'N/A', 'N/A',\n","    'handbag', 'tie', 'suitcase', 'frisbee', 'skis', 'snowboard', 'sports ball',\n","    'kite', 'baseball bat', 'baseball glove', 'skateboard', 'surfboard', 'tennis racket',\n","    'bottle', 'N/A', 'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl',\n","    'banana', 'apple', 'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza',\n","    'donut', 'cake', 'chair', 'couch', 'potted plant', 'bed', 'N/A', 'dining table',\n","    'N/A', 'N/A', 'toilet', 'N/A', 'tv', 'laptop', 'mouse', 'remote', 'keyboard', 'cell phone',\n","    'microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'N/A', 'book',\n","    'clock', 'vase', 'scissors', 'teddy bear', 'hair drier', 'toothbrush'\n","]\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZAbS5uwE9vd0"},"source":["# Defining a function for get a prediction result from the model\n","\n","def get_prediction(img_path, threshold):\n","  img = Image.open(img_path) # Load the image\n","  transform = T.Compose([T.ToTensor()]) # Defing PyTorch Transform\n","  img = transform(img) # Apply the transform to the image\n","  pred = model([img]) # Pass the image to the model\n","  pred_class = [COCO_INSTANCE_CATEGORY_NAMES[i] for i in list(pred[0]['labels'].numpy())] # Get the Prediction Score\n","  pred_boxes = [[(i[0], i[1]), (i[2], i[3])] for i in list(pred[0]['boxes'].detach().numpy())] # Bounding boxes\n","  pred_score = list(pred[0]['scores'].detach().numpy())\n","  pred_t = [pred_score.index(x) for x in pred_score if x > threshold][-1] # Get list of index with score greater than threshold.\n","  pred_boxes = pred_boxes[:pred_t+1]\n","  pred_class = pred_class[:pred_t+1]\n","  return pred_boxes, pred_class"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Sa4XIuLH6xoa"},"source":["# Defining a api function for object detection\n","\n","def object_detection_api(img_path, threshold=0.5, rect_th=3, text_size=1.5, text_th=3):\n"," \n","  boxes, pred_cls = get_prediction(img_path, threshold) # Get predictions\n","  img = cv2.imread(img_path) # Read image with cv2\n","  img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB) # Convert to RGB\n","  for i in range(len(boxes)):\n","    cv2.rectangle(img, boxes[i][0], boxes[i][1],color=(0, 255, 0), thickness=rect_th) # Draw Rectangle with the coordinates\n","    cv2.putText(img,pred_cls[i], boxes[i][0],  cv2.FONT_HERSHEY_SIMPLEX, text_size, (0,255,0),thickness=text_th) # Write the prediction class\n","  plt.figure(figsize=(15,20)) # display the output image\n","  plt.imshow(img)\n","  plt.xticks([])\n","  plt.yticks([])\n","  plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"cDlyeXSXHMLo"},"source":["# Example: After detection\n","object_detection_api(img_ex_path,threshold=0.8)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6X8gXp0eInzV"},"source":["\n","\n","\n","\n","*   The picture above is an example of applying Detection Network (in our case, Faster R-CNN).\n","*   Since the purpose of dataset we are using is 'tracking', you can see that most of the detected classes are 'person'.\n","*   We need a prediction result (bbs offset, class label, pred scores) for all the images.\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"iCelLE4jq8ye"},"source":["\n","\n","---\n","\n","\n","**3. Object ID Tracking with SORT**\n","\n","\n","*   Simple Online and Realtime Tracking (SORT) algorithm for object ID tracking \n","\n","---\n","\n"]},{"cell_type":"code","metadata":{"id":"j1WuiRXsHMPG"},"source":["# Git clone: SORT Algorithm\n","\n","!cd \"{MOT_PATH}\";git clone https://github.com/abewley/sort.git\n","  \n","sort = join(MOT_PATH,'sort/')\n","sys.path.append(sort)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"byjLd9LkKTux"},"source":["# requirement for sort\n","!cd \"{sort}\";pip install -r requirements.txt"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Rk2pt2SyuH9s"},"source":["# Optional: if error occurs, you might need to re-install scikit-image and imgaug\n","\n","!pip uninstall scikit-image\n","!pip uninstall imgaug\n","!pip install imgaug\n","!pip install -U scikit-image\n","\n","import skimage\n","print(skimage.__version__)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9nfV0kTkMmsd"},"source":["# Detection information on all the images is well-refined as a json file, which is available at our course git repo\n","\n","!cd \"{MOT_PATH}\";git clone https://github.com/mlvlab/COSE474.git"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zdErbmxk96w1"},"source":["import json\n","import collections\n","from pprint import pprint\n","from sort import *\n","\n","jsonpath = join(MOT_PATH,'COSE474/3_MOT_detinfo.json')\n","\n","with open(jsonpath) as data_file:    \n","   data = json.load(data_file)\n","odata = collections.OrderedDict(sorted(data.items()))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"eONi2h58yKjD"},"source":["# Let's check out downloaded json file\n","\n","pprint(odata)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WnsyoUpmz42L"},"source":["\n","\n","---\n","\n","\n","\n","*   For each image, bbs offset, class label, pred scores are all annotated.\n","*   Labels are annotated as a number - not a word, and for further information, go to the website below.\n","* https://tech.amikelive.com/node-718/what-object-categories-labels-are-in-coco-dataset/ \n","\n","\n","\n","---\n","\n"]},{"cell_type":"code","metadata":{"id":"vUB25un4yJsX"},"source":["img_path = motdata    # img root path\n","\n","# Making new directory for saving results\n","save_path = join(MOT_PATH,'save/')\n","!mkdir \"{save_path}\""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_k9vAOQQ960r"},"source":["mot_tracker = Sort()      # Tracker using SORT Algorithm"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MLvfa1Ls964o"},"source":["for key in odata.keys():   \n","    arrlist = []\n","    det_img = cv2.imread(os.path.join(img_path, key))\n","    overlay = det_img.copy()\n","    det_result = data[key] \n","    \n","    for info in det_result:\n","        bbox = info['bbox']\n","        labels = info['labels']\n","        scores = info['scores']\n","        templist = bbox+[scores]\n","        \n","        if labels == 1: # label 1 is a person in MS COCO Dataset\n","            arrlist.append(templist)\n","            \n","    track_bbs_ids = mot_tracker.update(np.array(arrlist))\n","    \n","    mot_imgid = key.replace('.jpg','')\n","    newname = save_path + mot_imgid + '_mot.jpg'\n","    print(mot_imgid)\n","    \n","    for j in range(track_bbs_ids.shape[0]):  \n","        ele = track_bbs_ids[j, :]\n","        x = int(ele[0])\n","        y = int(ele[1])\n","        x2 = int(ele[2])\n","        y2 = int(ele[3])\n","        track_label = str(int(ele[4])) \n","        cv2.rectangle(det_img, (x, y), (x2, y2), (0, 255, 255), 4)\n","        cv2.putText(det_img, '#'+track_label, (x+5, y-10), 0,0.6,(0,255,255),thickness=2)\n","        \n","    cv2.imwrite(newname,det_img)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kyVRJJLMnNGo"},"source":["\n","---\n","It's all done!\n","\n","\n","*   Finally, you can get a sequence of image with each Tracking ID for every detected person. \n","*   Check '3_MOT_result.gif' for whole demo experience.\n","\n","\n","\n","---\n","\n","\n"]}]}