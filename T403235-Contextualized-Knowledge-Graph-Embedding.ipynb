{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"T403235 | Contextualized Knowledge Graph Embedding","provenance":[],"collapsed_sections":[],"toc_visible":true,"authorship_tag":"ABX9TyMWdustc5fXcESpIu6rLPSb"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"QjaoZMXuzYyl"},"source":["# Contextualized Knowledge Graph Embedding"]},{"cell_type":"code","metadata":{"id":"gEhj7V1vk0D5"},"source":["!apt-get -qq install tree"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PXiCrlpojkDs","executionInfo":{"status":"ok","timestamp":1633959331424,"user_tz":-330,"elapsed":436,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}},"outputId":"bbb7ea29-aa2b-49ce-c94a-964ba2ab31be"},"source":["%%writefile wget_dataset.sh\n","#!/bin/bash\n","mkdir data\n","cd data\n","##downloads the 4 widely used KBC dataset\n","wget -q --show-progress --no-check-certificate https://everest.hds.utc.fr/lib/exe/fetch.php?media=en:fb15k.tgz -O fb15k.tgz\n","wget -q --show-progress --no-check-certificate https://everest.hds.utc.fr/lib/exe/fetch.php?media=en:wordnet-mlj12.tar.gz -O wordnet-mlj12.tar.gz\n","wget -q --show-progress --no-check-certificat https://download.microsoft.com/download/8/7/0/8700516A-AB3D-4850-B4BB-805C515AECE1/FB15K-237.2.zip -O FB15K-237.2.zip\n","wget -q --show-progress --no-check-certificat https://raw.githubusercontent.com/TimDettmers/ConvE/master/WN18RR.tar.gz -O WN18RR.tar.gz \n","\n","##downloads the path query dataset\n","wget -q --show-progress --no-check-certificate https://worksheets.codalab.org/rest/bundles/0xdb6b691c2907435b974850e8eb9a5fc2/contents/blob/ -O freebase_paths.tar.gz\n","wget -q --show-progress --no-check-certificate https://worksheets.codalab.org/rest/bundles/0xf91669f6c6d74987808aeb79bf716bd0/contents/blob/ -O wordnet_paths.tar.gz\n","\n","## organize the train/valid/test files by renaming\n","#fb15k\n","tar -xvf fb15k.tgz \n","mv FB15k fb15k\n","mv ./fb15k/freebase_mtr100_mte100-train.txt ./fb15k/train.txt\n","mv ./fb15k/freebase_mtr100_mte100-test.txt ./fb15k/test.txt\n","mv ./fb15k/freebase_mtr100_mte100-valid.txt ./fb15k/valid.txt\n","\n","#wn18\n","tar -zxvf wordnet-mlj12.tar.gz && mv wordnet-mlj12 wn18\n","mv wn18/wordnet-mlj12-train.txt wn18/train.txt\n","mv wn18/wordnet-mlj12-test.txt wn18/test.txt\n","mv wn18/wordnet-mlj12-valid.txt wn18/valid.txt\n","\n","\n","#fb15k237\n","unzip FB15K-237.2.zip && mv Release fb15k237\n","\n","#wn18rr\n","mkdir wn18rr && tar -zxvf WN18RR.tar.gz -C wn18rr\n","\n","#pathqueryWN\n","mkdir pathqueryWN && tar -zxvf wordnet_paths.tar.gz -C pathqueryWN\n","\n","#pathqueryFB\n","mkdir pathqueryFB && tar -zxvf freebase_paths.tar.gz -C pathqueryFB\n","\n","##rm tmp zip files\n","rm ./*.gz\n","rm ./*.tgz\n","rm ./*.zip"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Overwriting wget_dataset.sh\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Ui_FPRbyjtH5","executionInfo":{"status":"ok","timestamp":1633959384850,"user_tz":-330,"elapsed":52994,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}},"outputId":"8ba6da11-7f91-4e76-a5e3-703f71077db6"},"source":["!sh wget_dataset.sh"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["mkdir: cannot create directory ‘data’: File exists\n","wget_dataset.sh: 4: wget_dataset.sh: pushd: not found\n","fb15k.tgz           100%[===================>]   7.31M  6.45MB/s    in 1.1s    \n","wordnet-mlj12.tar.g 100%[===================>]   3.14M  3.64MB/s    in 0.9s    \n","FB15K-237.2.zip     100%[===================>] 139.45M  17.2MB/s    in 8.0s    \n","WN18RR.tar.gz       100%[===================>] 871.93K  --.-KB/s    in 0.03s   \n","freebase_paths.tar.     [          <=>       ] 118.39M  6.33MB/s    in 19s     \n","wordnet_paths.tar.g     [ <=>                ]  36.34M  4.99MB/s    in 7.2s    \n","FB15k/\n","FB15k/freebase_mtr100_mte100-test.txt\n","FB15k/freebase_mtr100_mte100-train.txt\n","FB15k/freebase_mtr100_mte100-valid.txt\n","FB15k/README\n","./._wordnet-mlj12\n","wordnet-mlj12/\n","wordnet-mlj12/README\n","wordnet-mlj12/._wordnet-mlj12-definitions.txt\n","wordnet-mlj12/wordnet-mlj12-definitions.txt\n","wordnet-mlj12/._wordnet-mlj12-test.txt\n","wordnet-mlj12/wordnet-mlj12-test.txt\n","wordnet-mlj12/._wordnet-mlj12-train.txt\n","wordnet-mlj12/wordnet-mlj12-train.txt\n","wordnet-mlj12/._wordnet-mlj12-valid.txt\n","wordnet-mlj12/wordnet-mlj12-valid.txt\n","wordnet-mlj12/Wordnet3.0-LICENSE\n","Archive:  FB15K-237.2.zip\n","  inflating: Release/MSR-LA_Data_Full Rights_FB15K-237 Knowledge Base Completion Dataset (2650).docx  \n","  inflating: Release/README.txt      \n","  inflating: Release/test.txt        \n","  inflating: Release/text_cvsc.txt   \n","  inflating: Release/text_emnlp.txt  \n","  inflating: Release/train.txt       \n","  inflating: Release/valid.txt       \n","valid.txt\n","train.txt\n","test.txt\n","./\n","./test_approx\n","./test_gen\n","./train\n","./dev_approx\n","./dev\n","./test\n","./dev_gen\n","./\n","./test_approx\n","./test_gen\n","./train\n","./dev_approx\n","./dev\n","./test\n","./dev_gen\n","wget_dataset.sh: 47: wget_dataset.sh: popd: not found\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tgXup_B7kD-f","executionInfo":{"status":"ok","timestamp":1633959615986,"user_tz":-330,"elapsed":459,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}},"outputId":"56b2b559-6420-42a5-96f3-e58a20fff1a4"},"source":["!tree /content/data"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/data\n","├── fb15k\n","│   ├── README\n","│   ├── test.txt\n","│   ├── train.txt\n","│   └── valid.txt\n","├── fb15k237\n","│   ├── MSR-LA_Data_Full Rights_FB15K-237 Knowledge Base Completion Dataset (2650).docx\n","│   ├── README.txt\n","│   ├── test.txt\n","│   ├── text_cvsc.txt\n","│   ├── text_emnlp.txt\n","│   ├── train.txt\n","│   └── valid.txt\n","├── pathqueryFB\n","│   ├── dev\n","│   ├── dev_approx\n","│   ├── dev_gen\n","│   ├── test\n","│   ├── test_approx\n","│   ├── test_gen\n","│   └── train\n","├── pathqueryWN\n","│   ├── dev\n","│   ├── dev_approx\n","│   ├── dev_gen\n","│   ├── test\n","│   ├── test_approx\n","│   ├── test_gen\n","│   └── train\n","├── wn18\n","│   ├── README\n","│   ├── test.txt\n","│   ├── train.txt\n","│   ├── valid.txt\n","│   ├── Wordnet3.0-LICENSE\n","│   └── wordnet-mlj12-definitions.txt\n","└── wn18rr\n","    ├── test.txt\n","    ├── train.txt\n","    └── valid.txt\n","\n","6 directories, 34 files\n"]}]},{"cell_type":"code","metadata":{"id":"iBTzRb35k7o8"},"source":["# Attention! Python 2.7.14  and python3 gives different vocabulary order. We use Python 2.7.14 to preprocess files.\n","\n","# input files: train.txt valid.txt test.txt  \n","# (these are default filenames, change files name with the following arguments:  --train $trainname --valid $validname --test $testname)\n","# output files: vocab.txt train.coke.txt valid.coke.txt test.coke.txt\n","python ./bin/kbc_data_preprocess.py --task fb15k --dir ./data/fb15k\n","python ./bin/kbc_data_preprocess.py --task wn18 --dir ./data/wn18\n","python ./bin/kbc_data_preprocess.py --task fb15k237 --dir ./data/fb15k237\n","python ./bin/kbc_data_preprocess.py --task wn18rr --dir ./data/wn18rr\n","\n","# input files: train dev test\n","# (these are default filenames, change files name with the following arguments: --train $trainname --valid $validname --test $testname)\n","# output files: vocab.txt train.coke.txt valid.coke.txt test.coke.txt sen_candli.txt trivial_sen.txt\n","python ./bin/pathquery_data_preprocess.py --task pathqueryFB --dir ./data/pathqueryFB \n","python ./bin/pathquery_data_preprocess.py --task pathqueryWN --dir ./data/pathqueryWN"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ned-nTDelA5W"},"source":["\"\"\"\n","data preprocess for KBC datasets\n","\"\"\"\n","\n","\n","def get_unique_entities_relations(train_file, dev_file, test_file):\n","    entity_lst = dict()\n","    relation_lst = dict()\n","    all_files = [train_file, dev_file, test_file]\n","    for input_file in all_files:\n","        print(\"dealing %s\" % train_file)\n","        with open(input_file, \"r\") as f:\n","            for line in f.readlines():\n","                tokens = line.strip().split(\"\\t\")\n","                assert len(tokens) == 3\n","                entity_lst[tokens[0]] = len(entity_lst)\n","                entity_lst[tokens[2]] = len(entity_lst)\n","                relation_lst[tokens[1]] = len(relation_lst)\n","    print(\">> Number of unique entities: %s\" % len(entity_lst))\n","    print(\">> Number of unique relations: %s\" % len(relation_lst))\n","    return entity_lst, relation_lst\n","\n","\n","def write_vocab(output_file, entity_lst, relation_lst):\n","    fout = open(output_file, \"w\")\n","    fout.write(\"[PAD]\" + \"\\n\")\n","    for i in range(95):\n","        fout.write(\"[unused{}]\\n\".format(i))\n","    fout.write(\"[UNK]\" + \"\\n\")\n","    fout.write(\"[CLS]\" + \"\\n\")\n","    fout.write(\"[SEP]\" + \"\\n\")\n","    fout.write(\"[MASK]\" + \"\\n\")\n","    for e in entity_lst.keys():\n","        fout.write(e + \"\\n\")\n","    for r in relation_lst.keys():\n","        fout.write(r + \"\\n\")\n","    vocab_size = 100 + len(entity_lst) + len(relation_lst)\n","    print(\">> vocab_size: %s\" % vocab_size)\n","    fout.close()\n","\n","\n","def load_vocab(vocab_file):\n","    \"\"\"Loads a vocabulary file into a dictionary.\"\"\"\n","    vocab = collections.OrderedDict()\n","    fin = open(vocab_file)\n","    for num, line in enumerate(fin):\n","        items = line.strip().split(\"\\t\")\n","        if len(items) > 2:\n","            break\n","        token = items[0]\n","        index = items[1] if len(items) == 2 else num\n","        token = token.strip()\n","        vocab[token] = int(index)\n","    return vocab\n","\n","\n","def write_true_triples(train_file, dev_file, test_file, vocab, output_file):\n","    true_triples = []\n","    all_files = [train_file, dev_file, test_file]\n","    for input_file in all_files:\n","        with open(input_file, \"r\") as f:\n","            for line in f.readlines():\n","                h, r, t = line.strip('\\r \\n').split('\\t')\n","                assert (h in vocab) and (r in vocab) and (t in vocab)\n","                hpos = vocab[h]\n","                rpos = vocab[r]\n","                tpos = vocab[t]\n","                true_triples.append((hpos, rpos, tpos))\n","\n","    print(\">> Number of true triples: %d\" % len(true_triples))\n","    fout = open(output_file, \"w\")\n","    for hpos, rpos, tpos in true_triples:\n","        fout.write(str(hpos) + \"\\t\" + str(rpos) + \"\\t\" + str(tpos) + \"\\n\")\n","    fout.close()\n","\n","\n","def generate_mask_type(input_file, output_file):\n","    with open(output_file, \"w\") as fw:\n","        with open(input_file, \"r\") as fr:\n","            for line in fr.readlines():\n","                fw.write(line.strip('\\r \\n') + \"\\tMASK_HEAD\\n\")\n","                fw.write(line.strip('\\r \\n') + \"\\tMASK_TAIL\\n\")\n","\n","\n","def kbc_data_preprocess(train_file, dev_file, test_file, vocab_path,\n","                        true_triple_path, new_train_file, new_dev_file,\n","                        new_test_file):\n","    entity_lst, relation_lst = get_unique_entities_relations(\n","        train_file, dev_file, test_file)\n","    write_vocab(vocab_path, entity_lst, relation_lst)\n","    vocab = load_vocab(vocab_path)\n","    write_true_triples(train_file, dev_file, test_file, vocab,\n","                       true_triple_path)\n","\n","    generate_mask_type(train_file, new_train_file)\n","    generate_mask_type(dev_file, new_dev_file)\n","    generate_mask_type(test_file, new_test_file)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"n3GTzXRQrNv4"},"source":["TASK_NAME = 'fb15k'\n","TASK_DATA_PATH = '/content/data/fb15k'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Xehn5e9qqw_Y"},"source":["def get_args():\n","    parser = argparse.ArgumentParser()\n","    parser.add_argument(\n","        \"--task\",\n","        type=str,\n","        default=TASK_NAME,\n","        help=\"task name: fb15k, fb15k237, wn18rr, wn18, pathqueryFB, pathqueryWN\"\n","    )\n","    parser.add_argument(\n","        \"--dir\",\n","        type=str,\n","        default=TASK_DATA_PATH,\n","        help=\"task data directory\")\n","    parser.add_argument(\n","        \"--train\",\n","        type=str,\n","        required=False,\n","        default=\"train.txt\",\n","        help=\"train file name, default train.txt\")\n","    parser.add_argument(\n","        \"--valid\",\n","        type=str,\n","        required=False,\n","        default=\"valid.txt\",\n","        help=\"valid file name, default valid.txt\")\n","    parser.add_argument(\n","        \"--test\",\n","        type=str,\n","        required=False,\n","        default=\"test.txt\",\n","        help=\"test file name, default test.txt\")\n","    args = parser.parse_args(args={})\n","    return args"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Lzi7TzUOq2h2","executionInfo":{"status":"ok","timestamp":1633961222193,"user_tz":-330,"elapsed":5617,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}},"outputId":"4fd8556e-efee-4bb1-9ce1-996ba8572238"},"source":["args = get_args()\n","task = args.task.lower()\n","assert task in [\"fb15k\", \"wn18\", \"fb15k237\", \"wn18rr\"]\n","\n","raw_train_file = os.path.join(args.dir, args.train)\n","raw_dev_file = os.path.join(args.dir, args.valid)\n","raw_test_file = os.path.join(args.dir, args.test)\n","\n","vocab_file = os.path.join(args.dir, \"vocab.txt\")\n","true_triple_file = os.path.join(args.dir, \"all.txt\")\n","new_train_file = os.path.join(args.dir, \"train.coke.txt\")\n","new_test_file = os.path.join(args.dir, \"test.coke.txt\")\n","new_dev_file = os.path.join(args.dir, \"valid.coke.txt\")\n","\n","kbc_data_preprocess(raw_train_file, raw_dev_file, raw_test_file,\n","                    vocab_file, true_triple_file, new_train_file,\n","                    new_dev_file, new_test_file)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["dealing /content/data/fb15k/train.txt\n","dealing /content/data/fb15k/train.txt\n","dealing /content/data/fb15k/train.txt\n",">> Number of unique entities: 14951\n",">> Number of unique relations: 1345\n",">> vocab_size: 16396\n",">> Number of true triples: 592213\n"]}]},{"cell_type":"code","metadata":{"id":"Q81zKOWlswv3"},"source":["class Args:\n","    TASK='fb15k'\n","    NUM_VOCAB=16396  #NUM_VOCAB and NUM_RELATIONS must be consistent with vocab.txt file \n","    NUM_RELATIONS=1345\n","\n","    # training hyper-paramters\n","    BATCH_SIZE=512\n","    LEARNING_RATE=5e-4\n","    EPOCH=400\n","    SOFT_LABEL=0.8\n","    SKIP_STEPS=1000\n","    MAX_SEQ_LEN=3\n","    HIDDEN_DROPOUT_PROB=0.1\n","    ATTENTION_PROBS_DROPOUT_PROB=0.1\n","\n","    # file paths for training and evaluation \n","    DATA=\"./data\"\n","    OUTPUT=\"./output_fb15k\"\n","    TRAIN_FILE=\"./data/fb15k/train.coke.txt\"\n","    VALID_FILE=\"./data/fb15k/valid.coke.txt\"\n","    TEST_FILE=\"./data/fb15k/test.coke.txt\"\n","    VOCAB_PATH=\"./data/fb15k/vocab.txt\"\n","    TRUE_TRIPLE_PATH=\"./data/fb15k/all.txt\"\n","    CHECKPOINTS=\"./output_fb15k/models\"\n","    INIT_CHECKPOINTS=CHECKPOINTS\n","    LOG_FILE=\"./output_fb15k/train.log\"\n","    LOG_EVAL_FILE=\"./output_fb15k/test.log\"\n","\n","    # transformer net config the follwoing are default configs for all tasks\n","    HIDDEN_SIZE=256\n","    NUM_HIDDEN_LAYERS=12\n","    NUM_ATTENTION_HEADS=4\n","    MAX_POSITION_EMBEDDINS=40\n","\n","    hidden_size=256\n","    num_hidden_layers=6\n","    num_attention_heads=4\n","    vocab_size=-1\n","    num_relations=None\n","    max_position_embeddings=10\n","    hidden_act=\"gelu\"\n","    hidden_dropout_prob=0.1\n","    attention_probs_dropout_prob=0.1\n","    initializer_range=0.02\n","    intermediate_size=512\n","    init_checkpoint=None\n","    init_pretraining_params=None\n","    checkpoints=\"checkpoints\"\n","    weight_sharing=True\n","    epoch=100\n","    learning_rate=5e-5\n","    lr_scheduler=\"linear_warmup_decay\"\n","    soft_label=0.9\n","    weight_decay=0.01\n","    warmup_proportion=0.1\n","    use_ema=True\n","    ema_decay=0.9999\n","    use_fp16=False\n","    loss_scaling=1.0\n","    skip_steps=1000\n","    verbose=False\n","    dataset=\"\"\n","    train_file=None\n","    sen_candli_file=None\n","    sen_trivial_file=None\n","    predict_file=None\n","    vocab_path=None\n","    true_triple_path=None\n","    max_seq_len=3\n","    batch_size=12\n","    in_tokens=False\n","    do_train=False\n","    do_predict=False\n","    use_cuda=False\n","    use_fast_executor=False\n","    num_iteration_per_drop_scope=1\n","\n","args =Args()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"V96vxiictavi"},"source":["!pip install -q paddlepaddle"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"SKiV0ivEuVAG"},"source":["from __future__ import absolute_import\n","from __future__ import division\n","from __future__ import print_function\n","\n","import argparse\n","import collections\n","import multiprocessing\n","import os\n","import time\n","import logging\n","import json\n","import random\n","import six\n","\n","import numpy as np\n","import paddle\n","import paddle.fluid as fluid\n","\n","from functools import partial, reduce\n","import numpy as np\n","\n","import paddle.fluid as fluid\n","import paddle.fluid.layers as layers\n","from paddle.fluid.layer_helper import LayerHelper\n","\n","import os\n","import six\n","import ast\n","import copy\n","import logging\n","\n","import numpy as np\n","import paddle.fluid as fluid"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"sBVRcq3eu2Y5"},"source":["logging.basicConfig(\n","    format='%(asctime)s - %(levelname)s - %(name)s - %(message)s',\n","    datefmt='%m/%d/%Y %H:%M:%S',\n","    level=logging.INFO)\n","logging.getLogger().setLevel(logging.INFO)\n","logger = logging.getLogger(__name__)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Vbm2jUuXuj6x"},"source":["def str2bool(v):\n","    # because argparse does not support to parse \"true, False\" as python\n","    # boolean directly\n","    return v.lower() in (\"true\", \"t\", \"1\")\n","\n","\n","class ArgumentGroup(object):\n","    def __init__(self, parser, title, des):\n","        self._group = parser.add_argument_group(title=title, description=des)\n","\n","    def add_arg(self, name, type, default, help, **kwargs):\n","        type = str2bool if type == bool else type\n","        self._group.add_argument(\n","            \"--\" + name,\n","            default=default,\n","            type=type,\n","            help=help + ' Default: %(default)s.',\n","            **kwargs)\n","\n","\n","def print_arguments(args):\n","    logger.info('-----------  Configuration Arguments -----------')\n","    for arg, value in sorted(six.iteritems(vars(args))):\n","        logger.info('%s: %s' % (arg, value))\n","    logger.info('------------------------------------------------')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"GsXRDbDsumZf"},"source":["def cast_fp16_to_fp32(i, o, prog):\n","    prog.global_block().append_op(\n","        type=\"cast\",\n","        inputs={\"X\": i},\n","        outputs={\"Out\": o},\n","        attrs={\n","            \"in_dtype\": fluid.core.VarDesc.VarType.FP16,\n","            \"out_dtype\": fluid.core.VarDesc.VarType.FP32\n","        })\n","\n","\n","def cast_fp32_to_fp16(i, o, prog):\n","    prog.global_block().append_op(\n","        type=\"cast\",\n","        inputs={\"X\": i},\n","        outputs={\"Out\": o},\n","        attrs={\n","            \"in_dtype\": fluid.core.VarDesc.VarType.FP32,\n","            \"out_dtype\": fluid.core.VarDesc.VarType.FP16\n","        })\n","\n","\n","def copy_to_master_param(p, block):\n","    v = block.vars.get(p.name, None)\n","    if v is None:\n","        raise ValueError(\"no param name %s found!\" % p.name)\n","    new_p = fluid.framework.Parameter(\n","        block=block,\n","        shape=v.shape,\n","        dtype=fluid.core.VarDesc.VarType.FP32,\n","        type=v.type,\n","        lod_level=v.lod_level,\n","        stop_gradient=p.stop_gradient,\n","        trainable=p.trainable,\n","        optimize_attr=p.optimize_attr,\n","        regularizer=p.regularizer,\n","        gradient_clip_attr=p.gradient_clip_attr,\n","        error_clip=p.error_clip,\n","        name=v.name + \".master\")\n","    return new_p\n","\n","\n","def create_master_params_grads(params_grads, main_prog, startup_prog,\n","                               loss_scaling):\n","    master_params_grads = []\n","    tmp_role = main_prog._current_role\n","    OpRole = fluid.core.op_proto_and_checker_maker.OpRole\n","    main_prog._current_role = OpRole.Backward\n","    for p, g in params_grads:\n","        # create master parameters\n","        master_param = copy_to_master_param(p, main_prog.global_block())\n","        startup_master_param = startup_prog.global_block()._clone_variable(\n","            master_param)\n","        startup_p = startup_prog.global_block().var(p.name)\n","        cast_fp16_to_fp32(startup_p, startup_master_param, startup_prog)\n","        # cast fp16 gradients to fp32 before apply gradients\n","        if g.name.find(\"layer_norm\") > -1:\n","            if loss_scaling > 1:\n","                scaled_g = g / float(loss_scaling)\n","            else:\n","                scaled_g = g\n","            master_params_grads.append([p, scaled_g])\n","            continue\n","        master_grad = fluid.layers.cast(g, \"float32\")\n","        if loss_scaling > 1:\n","            master_grad = master_grad / float(loss_scaling)\n","        master_params_grads.append([master_param, master_grad])\n","    main_prog._current_role = tmp_role\n","    return master_params_grads\n","\n","\n","def master_param_to_train_param(master_params_grads, params_grads, main_prog):\n","    for idx, m_p_g in enumerate(master_params_grads):\n","        train_p, _ = params_grads[idx]\n","        if train_p.name.find(\"layer_norm\") > -1:\n","            continue\n","        with main_prog._optimized_guard([m_p_g[0], m_p_g[1]]):\n","            cast_fp32_to_fp16(m_p_g[0], train_p, main_prog)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0TY649WNug58"},"source":["def layer_norm(x,\n","               begin_norm_axis=1,\n","               epsilon=1e-12,\n","               param_attr=None,\n","               bias_attr=None):\n","    \"\"\"\n","    Replace build-in layer_norm op with this function\n","    \"\"\"\n","    helper = LayerHelper('layer_norm', **locals())\n","    mean = layers.reduce_mean(x, dim=begin_norm_axis, keep_dim=True)\n","    shift_x = layers.elementwise_sub(x=x, y=mean, axis=0)\n","    variance = layers.reduce_mean(\n","        layers.square(shift_x), dim=begin_norm_axis, keep_dim=True)\n","    r_stdev = layers.rsqrt(variance + epsilon)\n","    norm_x = layers.elementwise_mul(x=shift_x, y=r_stdev, axis=0)\n","\n","    param_shape = [reduce(lambda x, y: x * y, norm_x.shape[begin_norm_axis:])]\n","    param_dtype = norm_x.dtype\n","    scale = helper.create_parameter(\n","        attr=param_attr,\n","        shape=param_shape,\n","        dtype=param_dtype,\n","        default_initializer=fluid.initializer.Constant(1.))\n","    bias = helper.create_parameter(\n","        attr=bias_attr,\n","        shape=param_shape,\n","        dtype=param_dtype,\n","        is_bias=True,\n","        default_initializer=fluid.initializer.Constant(0.))\n","\n","    out = layers.elementwise_mul(x=norm_x, y=scale, axis=-1)\n","    out = layers.elementwise_add(x=out, y=bias, axis=-1)\n","\n","    return out\n","\n","\n","def multi_head_attention(queries,\n","                         keys,\n","                         values,\n","                         attn_bias,\n","                         d_key,\n","                         d_value,\n","                         d_model,\n","                         n_head=1,\n","                         dropout_rate=0.,\n","                         cache=None,\n","                         param_initializer=None,\n","                         name='multi_head_att'):\n","    \"\"\"\n","    Multi-Head Attention. Note that attn_bias is added to the logit before\n","    computing softmax activiation to mask certain selected positions so that\n","    they will not considered in attention weights.\n","    \"\"\"\n","    keys = queries if keys is None else keys\n","    values = keys if values is None else values\n","\n","    if not (len(queries.shape) == len(keys.shape) == len(values.shape) == 3):\n","        raise ValueError(\n","            \"Inputs: quries, keys and values should all be 3-D tensors.\")\n","\n","    def __compute_qkv(queries, keys, values, n_head, d_key, d_value):\n","        \"\"\"\n","        Add linear projection to queries, keys, and values.\n","        \"\"\"\n","        q = layers.fc(input=queries,\n","                      size=d_key * n_head,\n","                      num_flatten_dims=2,\n","                      param_attr=fluid.ParamAttr(\n","                          name=name + '_query_fc.w_0',\n","                          initializer=param_initializer),\n","                      bias_attr=name + '_query_fc.b_0')\n","        k = layers.fc(input=keys,\n","                      size=d_key * n_head,\n","                      num_flatten_dims=2,\n","                      param_attr=fluid.ParamAttr(\n","                          name=name + '_key_fc.w_0',\n","                          initializer=param_initializer),\n","                      bias_attr=name + '_key_fc.b_0')\n","        v = layers.fc(input=values,\n","                      size=d_value * n_head,\n","                      num_flatten_dims=2,\n","                      param_attr=fluid.ParamAttr(\n","                          name=name + '_value_fc.w_0',\n","                          initializer=param_initializer),\n","                      bias_attr=name + '_value_fc.b_0')\n","        return q, k, v\n","\n","    def __split_heads(x, n_head):\n","        \"\"\"\n","        Reshape the last dimension of inpunt tensor x so that it becomes two\n","        dimensions and then transpose. Specifically, input a tensor with shape\n","        [bs, max_sequence_length, n_head * hidden_dim] then output a tensor\n","        with shape [bs, n_head, max_sequence_length, hidden_dim].\n","        \"\"\"\n","        hidden_size = x.shape[-1]\n","        # The value 0 in shape attr means copying the corresponding dimension\n","        # size of the input as the output dimension size.\n","        reshaped = layers.reshape(\n","            x=x, shape=[0, 0, n_head, hidden_size // n_head], inplace=True)\n","\n","        # permuate the dimensions into:\n","        # [batch_size, n_head, max_sequence_len, hidden_size_per_head]\n","        return layers.transpose(x=reshaped, perm=[0, 2, 1, 3])\n","\n","    def __combine_heads(x):\n","        \"\"\"\n","        Transpose and then reshape the last two dimensions of inpunt tensor x\n","        so that it becomes one dimension, which is reverse to __split_heads.\n","        \"\"\"\n","        if len(x.shape) == 3: return x\n","        if len(x.shape) != 4:\n","            raise ValueError(\"Input(x) should be a 4-D Tensor.\")\n","\n","        trans_x = layers.transpose(x, perm=[0, 2, 1, 3])\n","        # The value 0 in shape attr means copying the corresponding dimension\n","        # size of the input as the output dimension size.\n","        return layers.reshape(\n","            x=trans_x,\n","            shape=[0, 0, trans_x.shape[2] * trans_x.shape[3]],\n","            inplace=True)\n","\n","    def scaled_dot_product_attention(q, k, v, attn_bias, d_key, dropout_rate):\n","        \"\"\"\n","        Scaled Dot-Product Attention\n","        \"\"\"\n","        scaled_q = layers.scale(x=q, scale=d_key**-0.5)\n","        product = layers.matmul(x=scaled_q, y=k, transpose_y=True)\n","        if attn_bias:\n","            product += attn_bias\n","        weights = layers.softmax(product)\n","        if dropout_rate:\n","            weights = layers.dropout(\n","                weights,\n","                dropout_prob=dropout_rate,\n","                dropout_implementation=\"upscale_in_train\",\n","                is_test=False)\n","        out = layers.matmul(weights, v)\n","        return out\n","\n","    q, k, v = __compute_qkv(queries, keys, values, n_head, d_key, d_value)\n","\n","    if cache is not None:  # use cache and concat time steps\n","        # Since the inplace reshape in __split_heads changes the shape of k and\n","        # v, which is the cache input for next time step, reshape the cache\n","        # input from the previous time step first.\n","        k = cache[\"k\"] = layers.concat(\n","            [layers.reshape(\n","                cache[\"k\"], shape=[0, 0, d_model]), k], axis=1)\n","        v = cache[\"v\"] = layers.concat(\n","            [layers.reshape(\n","                cache[\"v\"], shape=[0, 0, d_model]), v], axis=1)\n","\n","    q = __split_heads(q, n_head)\n","    k = __split_heads(k, n_head)\n","    v = __split_heads(v, n_head)\n","\n","    ctx_multiheads = scaled_dot_product_attention(q, k, v, attn_bias, d_key,\n","                                                  dropout_rate)\n","\n","    out = __combine_heads(ctx_multiheads)\n","\n","    # Project back to the model size.\n","    proj_out = layers.fc(input=out,\n","                         size=d_model,\n","                         num_flatten_dims=2,\n","                         param_attr=fluid.ParamAttr(\n","                             name=name + '_output_fc.w_0',\n","                             initializer=param_initializer),\n","                         bias_attr=name + '_output_fc.b_0')\n","    return proj_out\n","\n","\n","def positionwise_feed_forward(x,\n","                              d_inner_hid,\n","                              d_hid,\n","                              dropout_rate,\n","                              hidden_act,\n","                              param_initializer=None,\n","                              name='ffn'):\n","    \"\"\"\n","    Position-wise Feed-Forward Networks.\n","    This module consists of two linear transformations with a ReLU activation\n","    in between, which is applied to each position separately and identically.\n","    \"\"\"\n","    hidden = layers.fc(input=x,\n","                       size=d_inner_hid,\n","                       num_flatten_dims=2,\n","                       act=hidden_act,\n","                       param_attr=fluid.ParamAttr(\n","                           name=name + '_fc_0.w_0',\n","                           initializer=param_initializer),\n","                       bias_attr=name + '_fc_0.b_0')\n","    if dropout_rate:\n","        hidden = layers.dropout(\n","            hidden,\n","            dropout_prob=dropout_rate,\n","            dropout_implementation=\"upscale_in_train\",\n","            is_test=False)\n","    out = layers.fc(input=hidden,\n","                    size=d_hid,\n","                    num_flatten_dims=2,\n","                    param_attr=fluid.ParamAttr(\n","                        name=name + '_fc_1.w_0',\n","                        initializer=param_initializer),\n","                    bias_attr=name + '_fc_1.b_0')\n","    return out\n","\n","\n","def pre_post_process_layer(prev_out,\n","                           out,\n","                           process_cmd,\n","                           dropout_rate=0.,\n","                           name=''):\n","    \"\"\"\n","    Add residual connection, layer normalization and droput to the out tensor\n","    optionally according to the value of process_cmd.\n","    This will be used before or after multi-head attention and position-wise\n","    feed-forward networks.\n","    \"\"\"\n","    for cmd in process_cmd:\n","        if cmd == \"a\":  # add residual connection\n","            out = out + prev_out if prev_out else out\n","        elif cmd == \"n\":  # add layer normalization\n","            out_dtype = out.dtype\n","            if out_dtype == fluid.core.VarDesc.VarType.FP16:\n","                out = layers.cast(x=out, dtype=\"float32\")\n","            out = layer_norm(\n","                out,\n","                begin_norm_axis=len(out.shape) - 1,\n","                param_attr=fluid.ParamAttr(\n","                    name=name + '_layer_norm_scale',\n","                    initializer=fluid.initializer.Constant(1.)),\n","                bias_attr=fluid.ParamAttr(\n","                    name=name + '_layer_norm_bias',\n","                    initializer=fluid.initializer.Constant(0.)))\n","            if out_dtype == fluid.core.VarDesc.VarType.FP16:\n","                out = layers.cast(x=out, dtype=\"float16\")\n","        elif cmd == \"d\":  # add dropout\n","            if dropout_rate:\n","                out = layers.dropout(\n","                    out,\n","                    dropout_prob=dropout_rate,\n","                    dropout_implementation=\"upscale_in_train\",\n","                    is_test=False)\n","    return out\n","\n","\n","pre_process_layer = partial(pre_post_process_layer, None)\n","post_process_layer = pre_post_process_layer\n","\n","\n","def encoder_layer(enc_input,\n","                  attn_bias,\n","                  n_head,\n","                  d_key,\n","                  d_value,\n","                  d_model,\n","                  d_inner_hid,\n","                  prepostprocess_dropout,\n","                  attention_dropout,\n","                  relu_dropout,\n","                  hidden_act,\n","                  preprocess_cmd=\"n\",\n","                  postprocess_cmd=\"da\",\n","                  param_initializer=None,\n","                  name=''):\n","    \"\"\"The encoder layers that can be stacked to form a deep encoder.\n","    This module consits of a multi-head (self) attention followed by\n","    position-wise feed-forward networks and both the two components companied\n","    with the post_process_layer to add residual connection, layer normalization\n","    and droput.\n","    \"\"\"\n","    attn_output = multi_head_attention(\n","        pre_process_layer(\n","            enc_input,\n","            preprocess_cmd,\n","            prepostprocess_dropout,\n","            name=name + '_pre_att'),\n","        None,\n","        None,\n","        attn_bias,\n","        d_key,\n","        d_value,\n","        d_model,\n","        n_head,\n","        attention_dropout,\n","        param_initializer=param_initializer,\n","        name=name + '_multi_head_att')\n","    attn_output = post_process_layer(\n","        enc_input,\n","        attn_output,\n","        postprocess_cmd,\n","        prepostprocess_dropout,\n","        name=name + '_post_att')\n","    ffd_output = positionwise_feed_forward(\n","        pre_process_layer(\n","            attn_output,\n","            preprocess_cmd,\n","            prepostprocess_dropout,\n","            name=name + '_pre_ffn'),\n","        d_inner_hid,\n","        d_model,\n","        relu_dropout,\n","        hidden_act,\n","        param_initializer=param_initializer,\n","        name=name + '_ffn')\n","    return post_process_layer(\n","        attn_output,\n","        ffd_output,\n","        postprocess_cmd,\n","        prepostprocess_dropout,\n","        name=name + '_post_ffn')\n","\n","\n","def encoder(enc_input,\n","            attn_bias,\n","            n_layer,\n","            n_head,\n","            d_key,\n","            d_value,\n","            d_model,\n","            d_inner_hid,\n","            prepostprocess_dropout,\n","            attention_dropout,\n","            relu_dropout,\n","            hidden_act,\n","            preprocess_cmd=\"n\",\n","            postprocess_cmd=\"da\",\n","            param_initializer=None,\n","            name=''):\n","    \"\"\"\n","    The encoder is composed of a stack of identical layers returned by calling\n","    encoder_layer.\n","    \"\"\"\n","    for i in range(n_layer):\n","        enc_output = encoder_layer(\n","            enc_input,\n","            attn_bias,\n","            n_head,\n","            d_key,\n","            d_value,\n","            d_model,\n","            d_inner_hid,\n","            prepostprocess_dropout,\n","            attention_dropout,\n","            relu_dropout,\n","            hidden_act,\n","            preprocess_cmd,\n","            postprocess_cmd,\n","            param_initializer=param_initializer,\n","            name=name + '_layer_' + str(i))\n","        enc_input = enc_output\n","    enc_output = pre_process_layer(\n","        enc_output,\n","        preprocess_cmd,\n","        prepostprocess_dropout,\n","        name=\"post_encoder\")\n","\n","    return enc_output"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8f-ejWXzt5Rz"},"source":["def mask(input_tokens, input_mask_type, max_len, mask_id):\n","    \"\"\"\n","    Add mask for batch_tokens, return out, mask_label, mask_pos;\n","    Note: mask_pos responding the batch_tokens after padded;\n","    \"\"\"\n","    output_tokens = []\n","    mask_label = []\n","    mask_pos = []\n","    for sent_index, sent in enumerate(input_tokens):\n","        mask_type = input_mask_type[sent_index]\n","        if mask_type == \"MASK_HEAD\":\n","            token_index = 0\n","            mask_label.append(sent[token_index])\n","            mask_pos.append(sent_index * max_len + token_index)\n","            sent_out = sent[:]\n","            sent_out[token_index] = mask_id\n","            output_tokens.append(sent_out)\n","        elif mask_type == \"MASK_TAIL\":\n","            token_index = len(sent) - 1\n","            mask_label.append(sent[token_index])\n","            mask_pos.append(sent_index * max_len + token_index)\n","            sent_out = sent[:]\n","            sent_out[token_index] = mask_id\n","            output_tokens.append(sent_out)\n","        else:\n","            raise ValueError(\n","                \"Unknown mask type, which should be in ['MASK_HEAD', 'MASK_TAIL'].\"\n","            )\n","    mask_label = np.array(mask_label).astype(\"int64\").reshape([-1, 1])\n","    mask_pos = np.array(mask_pos).astype(\"int64\").reshape([-1, 1])\n","    return output_tokens, mask_label, mask_pos\n","\n","\n","def pad_batch_data(insts,\n","                   max_len,\n","                   pad_idx=0,\n","                   return_pos=False,\n","                   return_input_mask=False):\n","    \"\"\"\n","    Pad the instances to the max sequence length in batch, and generate the\n","    corresponding position data and input mask.\n","    \"\"\"\n","    return_list = []\n","\n","    # Any token included in dict can be used to pad, since the paddings' loss\n","    # will be masked out by weights and make no effect on parameter gradients.\n","\n","    inst_data = np.array([\n","        list(inst) + list([pad_idx] * (max_len - len(inst))) for inst in insts\n","    ])\n","    return_list += [inst_data.astype(\"int64\").reshape([-1, max_len, 1])]\n","\n","    # position data\n","    if return_pos:\n","        inst_pos = np.array([\n","            list(range(0, len(inst))) + [pad_idx] * (max_len - len(inst))\n","            for inst in insts\n","        ])\n","\n","        return_list += [inst_pos.astype(\"int64\").reshape([-1, max_len, 1])]\n","\n","    if return_input_mask:\n","        # This is used to avoid attention on paddings.\n","        input_mask_data = np.array([[1] * len(inst) + [0] *\n","                                    (max_len - len(inst)) for inst in insts])\n","        input_mask_data = np.expand_dims(input_mask_data, axis=-1)\n","        return_list += [input_mask_data.astype(\"float32\")]\n","\n","    return return_list if len(return_list) > 1 else return_list[0]\n","\n","\n","def prepare_batch_data(insts, max_len, pad_id=None, mask_id=None):\n","    \"\"\" masking, padding, turn list data into numpy arrays, for batch examples\n","    \"\"\"\n","    batch_src_ids = [inst[0] for inst in insts]\n","    batch_mask_type = [inst[1] for inst in insts]\n","\n","    # First step: do mask without padding\n","    if mask_id >= 0:\n","        out, mask_label, mask_pos = mask(\n","            input_tokens=batch_src_ids,\n","            input_mask_type=batch_mask_type,\n","            max_len=max_len,\n","            mask_id=mask_id)\n","    else:\n","        out = batch_src_ids\n","\n","    # Second step: padding and turn into numpy arrays\n","    src_id, pos_id, input_mask = pad_batch_data(\n","        out,\n","        max_len=max_len,\n","        pad_idx=pad_id,\n","        return_pos=True,\n","        return_input_mask=True)\n","\n","    if mask_id >= 0:\n","        return_list = [src_id, pos_id, input_mask, mask_label, mask_pos]\n","    else:\n","        return_list = [src_id, pos_id, input_mask]\n","\n","    return return_list if len(return_list) > 1 else return_list[0]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"s0TpSytNt6JQ"},"source":["RawExample = collections.namedtuple(\"RawExample\", [\"token_ids\", \"mask_type\"])\n","\n","\n","def convert_to_unicode(text):\n","    \"\"\"Converts `text` to Unicode (if it's not already), assuming utf-8 input.\"\"\"\n","    if six.PY3:\n","        if isinstance(text, str):\n","            return text\n","        elif isinstance(text, bytes):\n","            return text.decode(\"utf-8\", \"ignore\")\n","        else:\n","            raise ValueError(\"Unsupported string type: %s\" % (type(text)))\n","    elif six.PY2:\n","        if isinstance(text, str):\n","            return text.decode(\"utf-8\", \"ignore\")\n","        elif isinstance(text, unicode):\n","            return text\n","        else:\n","            raise ValueError(\"Unsupported string type: %s\" % (type(text)))\n","    else:\n","        raise ValueError(\"Not running on Python2 or Python 3?\")\n","\n","\n","#def printable_text(text):\n","#    \"\"\"Returns text encoded in a way suitable for print or `tf.logging`.\"\"\"\n","#\n","#    # These functions want `str` for both Python2 and Python3, but in one case\n","#    # it's a Unicode string and in the other it's a byte string.\n","#    if six.PY3:\n","#        if isinstance(text, str):\n","#            return text\n","#        elif isinstance(text, bytes):\n","#            return text.decode(\"utf-8\", \"ignore\")\n","#        else:\n","#            raise ValueError(\"Unsupported string type: %s\" % (type(text)))\n","#    elif six.PY2:\n","#        if isinstance(text, str):\n","#            return text\n","#        elif isinstance(text, unicode):\n","#            return text.encode(\"utf-8\")\n","#        else:\n","#            raise ValueError(\"Unsupported string type: %s\" % (type(text)))\n","#    else:\n","#        raise ValueError(\"Not running on Python2 or Python 3?\")\n","\n","\n","def load_vocab(vocab_file):\n","    \"\"\"Loads a vocabulary file into a dictionary.\"\"\"\n","    vocab = collections.OrderedDict()\n","    fin = open(vocab_file)\n","    for num, line in enumerate(fin):\n","        items = line.strip().split(\"\\t\")\n","        if len(items) > 2:\n","            break\n","        token = items[0]\n","        index = items[1] if len(items) == 2 else num\n","        token = token.strip()\n","        vocab[token] = int(index)\n","    return vocab\n","\n","\n","#def convert_by_vocab(vocab, items):\n","#    \"\"\"Converts a sequence of [tokens|ids] using the vocab.\"\"\"\n","#    output = []\n","#    for item in items:\n","#        output.append(vocab[item])\n","#    return output\n","\n","\n","def convert_tokens_to_ids(vocab, tokens):\n","    \"\"\"Converts a sequence of tokens into ids using the vocab.\"\"\"\n","    output = []\n","    for item in tokens:\n","        output.append(vocab[item])\n","    return output\n","\n","\n","class KBCDataReader(object):\n","    \"\"\" DataReader\n","    \"\"\"\n","\n","    def __init__(self,\n","                 vocab_path,\n","                 data_path,\n","                 max_seq_len=3,\n","                 batch_size=4096,\n","                 is_training=True,\n","                 shuffle=True,\n","                 dev_count=1,\n","                 epoch=10,\n","                 vocab_size=-1):\n","        self.vocab = load_vocab(vocab_path)\n","        if vocab_size > 0:\n","            assert len(self.vocab) == vocab_size, \\\n","                \"Assert Error! Input vocab_size(%d) is not consistant with voab_file(%d)\" % \\\n","                (vocab_size, len(self.vocab))\n","        self.pad_id = self.vocab[\"[PAD]\"]\n","        self.mask_id = self.vocab[\"[MASK]\"]\n","\n","        self.max_seq_len = max_seq_len\n","        self.batch_size = batch_size\n","\n","        self.is_training = is_training\n","        self.shuffle = shuffle\n","        self.dev_count = dev_count\n","        self.epoch = epoch\n","        if not is_training:\n","            self.shuffle = False\n","            self.dev_count = 1\n","            self.epoch = 1\n","\n","        self.examples = self.read_example(data_path)\n","        self.total_instance = len(self.examples)\n","\n","        self.current_epoch = -1\n","        self.current_instance_index = -1\n","\n","    def get_progress(self):\n","        \"\"\"return current progress of traning data\n","        \"\"\"\n","        return self.current_instance_index, self.current_epoch\n","\n","    def line2tokens(self, line):\n","        tokens = line.split(\"\\t\")\n","        return tokens\n","\n","    def read_example(self, input_file):\n","        \"\"\"Reads the input file into a list of examples.\"\"\"\n","        examples = []\n","        with open(input_file, \"r\") as f:\n","            for line in f.readlines():\n","                line = convert_to_unicode(line.strip())\n","                tokens = self.line2tokens(line)\n","                assert len(tokens) <= (self.max_seq_len + 1), \\\n","                    \"Expecting at most [max_seq_len + 1]=%d tokens each line, current tokens %d\" \\\n","                    % (self.max_seq_len + 1, len(tokens))\n","                token_ids = convert_tokens_to_ids(self.vocab, tokens[:-1])\n","                if len(token_ids) <= 0:\n","                    continue\n","                examples.append(\n","                    RawExample(\n","                        token_ids=token_ids, mask_type=tokens[-1]))\n","                # if len(examples) <= 10:\n","                #     logger.info(\"*** Example ***\")\n","                #     logger.info(\"tokens: %s\" % \" \".join([printable_text(x) for x in tokens]))\n","                #     logger.info(\"token_ids: %s\" % \" \".join([str(x) for x in token_ids]))\n","        return examples\n","\n","    def data_generator(self):\n","        \"\"\" wrap the batch data generator\n","        \"\"\"\n","        range_list = [i for i in range(self.total_instance)]\n","\n","        def wrapper():\n","            \"\"\" wrapper batch data\n","            \"\"\"\n","\n","            def reader():\n","                for epoch_index in range(self.epoch):\n","                    self.current_epoch = epoch_index\n","                    if self.shuffle is True:\n","                        np.random.shuffle(range_list)\n","                    for idx, sample in enumerate(range_list):\n","                        self.current_instance_index = idx\n","                        yield self.examples[sample]\n","\n","            def batch_reader(reader, batch_size):\n","                \"\"\"reader generator for batches of examples\n","                :param reader: reader generator for one example\n","                :param batch_size: int batch size\n","                :return: a list of examples for batch data\n","                \"\"\"\n","                batch = []\n","                for example in reader():\n","                    token_ids = example.token_ids\n","                    mask_type = example.mask_type\n","                    example_out = [token_ids] + [mask_type]\n","                    to_append = len(batch) < batch_size\n","                    if to_append is False:\n","                        yield batch\n","                        batch = [example_out]\n","                    else:\n","                        batch.append(example_out)\n","                if len(batch) > 0:\n","                    yield batch\n","\n","            all_device_batches = []\n","            for batch_data in batch_reader(reader, self.batch_size):\n","                batch_data = prepare_batch_data(\n","                    batch_data,\n","                    max_len=self.max_seq_len,\n","                    pad_id=self.pad_id,\n","                    mask_id=self.mask_id)\n","                if len(all_device_batches) < self.dev_count:\n","                    all_device_batches.append(batch_data)\n","\n","                if len(all_device_batches) == self.dev_count:\n","                    for batch in all_device_batches:\n","                        yield batch\n","                    all_device_batches = []\n","\n","        return wrapper\n","\n","\n","class PathqueryDataReader(KBCDataReader):\n","    def __init__(self,\n","                 vocab_path,\n","                 data_path,\n","                 max_seq_len=3,\n","                 batch_size=4096,\n","                 is_training=True,\n","                 shuffle=True,\n","                 dev_count=1,\n","                 epoch=10,\n","                 vocab_size=-1):\n","\n","        KBCDataReader.__init__(self, vocab_path, data_path, max_seq_len,\n","                               batch_size, is_training, shuffle, dev_count,\n","                               epoch, vocab_size)\n","\n","    def line2tokens(self, line):\n","        tokens = []\n","        s, path, o, mask_type = line.split(\"\\t\")\n","        path_tokens = path.split(\",\")\n","        tokens.append(s)\n","        tokens.extend(path_tokens)\n","        tokens.append(o)\n","        tokens.append(mask_type)\n","        return tokens"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Wqa-q13Yt6G1"},"source":["def cast_fp32_to_fp16(exe, main_program):\n","    logger.info(\"Cast parameters to float16 data format.\")\n","    for param in main_program.global_block().all_parameters():\n","        if not param.name.endswith(\".master\"):\n","            param_t = fluid.global_scope().find_var(param.name).get_tensor()\n","            data = np.array(param_t)\n","            if param.name.find(\"layer_norm\") == -1:\n","                param_t.set(np.float16(data).view(np.uint16), exe.place)\n","            master_param_var = fluid.global_scope().find_var(param.name +\n","                                                             \".master\")\n","            if master_param_var is not None:\n","                master_param_var.get_tensor().set(data, exe.place)\n","\n","\n","def init_checkpoint(exe,\n","                    init_checkpoint_path,\n","                    main_program,\n","                    use_fp16=False,\n","                    print_var_verbose=False):\n","    assert os.path.exists(\n","        init_checkpoint_path), \"[%s] cann't be found.\" % init_checkpoint_path\n","\n","    def existed_persitables(var):\n","        if not fluid.io.is_persistable(var):\n","            return False\n","        return os.path.exists(os.path.join(init_checkpoint_path, var.name))\n","\n","    fluid.io.load_vars(\n","        exe,\n","        init_checkpoint_path,\n","        main_program=main_program,\n","        predicate=existed_persitables)\n","    logger.info(\"Load model from {}\".format(init_checkpoint_path))\n","\n","    if use_fp16:\n","        cast_fp32_to_fp16(exe, main_program)\n","\n","    # Used for debug on parameters\n","    if print_var_verbose is True:\n","\n","        def params(var):\n","            if not isinstance(var, fluid.framework.Parameter):\n","                return False\n","            return True\n","\n","        existed_vars = list(filter(params, main_program.list_vars()))\n","        existed_vars = sorted(existed_vars, key=lambda x: x.name)\n","        for var in existed_vars:\n","            logger.info(\"var name:{} shape:{}\".format(var.name, var.shape))\n","\n","\n","def init_pretraining_params(exe,\n","                            pretraining_params_path,\n","                            main_program,\n","                            use_fp16=False):\n","    assert os.path.exists(pretraining_params_path\n","                          ), \"[%s] cann't be found.\" % pretraining_params_path\n","\n","    def existed_params(var):\n","        if not isinstance(var, fluid.framework.Parameter):\n","            return False\n","        return os.path.exists(os.path.join(pretraining_params_path, var.name))\n","\n","    fluid.io.load_vars(\n","        exe,\n","        pretraining_params_path,\n","        main_program=main_program,\n","        predicate=existed_params)\n","    logger.info(\"Load pretraining parameters from {}.\".format(\n","        pretraining_params_path))\n","\n","    if use_fp16:\n","        cast_fp32_to_fp16(exe, main_program)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZiE9AuVKt6Co"},"source":["class CoKEModel(object):\n","    def __init__(self,\n","                 src_ids,\n","                 position_ids,\n","                 input_mask,\n","                 config,\n","                 soft_label=0.9,\n","                 weight_sharing=True,\n","                 use_fp16=False):\n","\n","        self._emb_size = config['hidden_size']\n","        self._n_layer = config['num_hidden_layers']\n","        self._n_head = config['num_attention_heads']\n","        self._voc_size = config['vocab_size']\n","        self._n_relation = config['num_relations']\n","        self._max_position_seq_len = config['max_position_embeddings']\n","        self._hidden_act = config['hidden_act']\n","        self._prepostprocess_dropout = config['hidden_dropout_prob']\n","        self._attention_dropout = config['attention_probs_dropout_prob']\n","        self._intermediate_size = config['intermediate_size']\n","        self._soft_label = soft_label\n","        self._weight_sharing = weight_sharing\n","\n","        self._word_emb_name = \"word_embedding\"\n","        self._pos_emb_name = \"pos_embedding\"\n","        self._dtype = \"float16\" if use_fp16 else \"float32\"\n","\n","        # Initialize all weigths by truncated normal initializer, and all biases\n","        # will be initialized by constant zero by default.\n","        self._param_initializer = fluid.initializer.TruncatedNormal(\n","            scale=config['initializer_range'])\n","\n","        self._build_model(src_ids, position_ids, input_mask)\n","\n","    def _build_model(self, src_ids, position_ids, input_mask):\n","        # padding id in vocabulary must be set to 0\n","        emb_out = fluid.layers.embedding(\n","            input=src_ids,\n","            size=[self._voc_size, self._emb_size],\n","            dtype=self._dtype,\n","            param_attr=fluid.ParamAttr(\n","                name=self._word_emb_name, initializer=self._param_initializer),\n","            is_sparse=False)\n","        position_emb_out = fluid.layers.embedding(\n","            input=position_ids,\n","            size=[self._max_position_seq_len, self._emb_size],\n","            dtype=self._dtype,\n","            param_attr=fluid.ParamAttr(\n","                name=self._pos_emb_name, initializer=self._param_initializer))\n","\n","        emb_out = emb_out + position_emb_out\n","\n","        emb_out = pre_process_layer(\n","            emb_out, 'nd', self._prepostprocess_dropout, name='pre_encoder')\n","\n","        if self._dtype == \"float16\":\n","            input_mask = fluid.layers.cast(x=input_mask, dtype=self._dtype)\n","\n","        self_attn_mask = fluid.layers.matmul(\n","            x=input_mask, y=input_mask, transpose_y=True)\n","        self_attn_mask = fluid.layers.scale(\n","            x=self_attn_mask, scale=10000.0, bias=-1.0, bias_after_scale=False)\n","        n_head_self_attn_mask = fluid.layers.stack(\n","            x=[self_attn_mask] * self._n_head, axis=1)\n","        n_head_self_attn_mask.stop_gradient = True\n","\n","        self._enc_out = encoder(\n","            enc_input=emb_out,\n","            attn_bias=n_head_self_attn_mask,\n","            n_layer=self._n_layer,\n","            n_head=self._n_head,\n","            d_key=self._emb_size // self._n_head,\n","            d_value=self._emb_size // self._n_head,\n","            d_model=self._emb_size,\n","            d_inner_hid=self._intermediate_size,\n","            prepostprocess_dropout=self._prepostprocess_dropout,\n","            attention_dropout=self._attention_dropout,\n","            relu_dropout=0,\n","            hidden_act=self._hidden_act,\n","            preprocess_cmd=\"\",\n","            postprocess_cmd=\"dan\",\n","            param_initializer=self._param_initializer,\n","            name='encoder')\n","\n","    #def get_sequence_output(self):\n","    #    return self._enc_out\n","\n","    def get_pretraining_output(self, mask_label, mask_pos):\n","        \"\"\"Get the loss & fc_out for training\"\"\"\n","        mask_pos = fluid.layers.cast(x=mask_pos, dtype='int32')\n","\n","        reshaped_emb_out = fluid.layers.reshape(\n","            x=self._enc_out, shape=[-1, self._emb_size])\n","        # extract masked tokens' feature\n","        mask_feat = fluid.layers.gather(input=reshaped_emb_out, index=mask_pos)\n","\n","        # transform: fc\n","        mask_trans_feat = fluid.layers.fc(\n","            input=mask_feat,\n","            size=self._emb_size,\n","            act=self._hidden_act,\n","            param_attr=fluid.ParamAttr(\n","                name='mask_lm_trans_fc.w_0',\n","                initializer=self._param_initializer),\n","            bias_attr=fluid.ParamAttr(name='mask_lm_trans_fc.b_0'))\n","        # transform: layer norm\n","        mask_trans_feat = pre_process_layer(\n","            mask_trans_feat, 'n', name='mask_lm_trans')\n","\n","        mask_lm_out_bias_attr = fluid.ParamAttr(\n","            name=\"mask_lm_out_fc.b_0\",\n","            initializer=fluid.initializer.Constant(value=0.0))\n","        if self._weight_sharing:\n","            fc_out = fluid.layers.matmul(\n","                x=mask_trans_feat,\n","                y=fluid.default_main_program().global_block().var(\n","                    self._word_emb_name),\n","                transpose_y=True)\n","            fc_out += fluid.layers.create_parameter(\n","                shape=[self._voc_size],\n","                dtype=self._dtype,\n","                attr=mask_lm_out_bias_attr,\n","                is_bias=True)\n","        else:\n","            fc_out = fluid.layers.fc(input=mask_trans_feat,\n","                                     size=self._voc_size,\n","                                     param_attr=fluid.ParamAttr(\n","                                         name=\"mask_lm_out_fc.w_0\",\n","                                         initializer=self._param_initializer),\n","                                     bias_attr=mask_lm_out_bias_attr)\n","        #generate soft labels for loss cross entropy loss\n","        one_hot_labels = fluid.layers.one_hot(\n","            input=mask_label, depth=self._voc_size)\n","        entity_indicator = fluid.layers.fill_constant_batch_size_like(\n","            input=mask_label,\n","            shape=[-1, (self._voc_size - self._n_relation)],\n","            dtype='int64',\n","            value=0)\n","        relation_indicator = fluid.layers.fill_constant_batch_size_like(\n","            input=mask_label,\n","            shape=[-1, self._n_relation],\n","            dtype='int64',\n","            value=1)\n","        is_relation = fluid.layers.concat(\n","            input=[entity_indicator, relation_indicator], axis=-1)\n","        soft_labels = one_hot_labels * self._soft_label \\\n","                      + (1.0 - one_hot_labels - is_relation) \\\n","                      * ((1.0 - self._soft_label) / (self._voc_size - 1 - self._n_relation))\n","        soft_labels.stop_gradient = True\n","\n","        mask_lm_loss = fluid.layers.softmax_with_cross_entropy(\n","            logits=fc_out, label=soft_labels, soft_label=True)\n","        mean_mask_lm_loss = fluid.layers.mean(mask_lm_loss)\n","\n","        return mean_mask_lm_loss, fc_out"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"SqHYnH5YvTTG"},"source":["def kbc_batch_evaluation(eval_i, all_examples, batch_results, tt):\n","    r_hts_idx = collections.defaultdict(list)\n","    scores_head = collections.defaultdict(list)\n","    scores_tail = collections.defaultdict(list)\n","    batch_r_hts_cnt = 0\n","    b_size = len(batch_results)\n","    for j in range(b_size):\n","        result = batch_results[j]\n","        i = eval_i + j\n","        example = all_examples[i]\n","        assert len(example.token_ids\n","                   ) == 3, \"For kbc task each example consists of 3 tokens\"\n","        h, r, t = example.token_ids\n","\n","        _mask_type = example.mask_type\n","        if i % 2 == 0:\n","            r_hts_idx[r].append((h, t))\n","            batch_r_hts_cnt += 1\n","        if _mask_type == \"MASK_HEAD\":\n","            scores_head[(r, t)] = result\n","        elif _mask_type == \"MASK_TAIL\":\n","            scores_tail[(r, h)] = result\n","        else:\n","            raise ValueError(\"Unknown mask type in prediction example:%d\" % i)\n","\n","    rank = {}\n","    f_rank = {}\n","    for r, hts in r_hts_idx.items():\n","        r_rank = {'head': [], 'tail': []}\n","        r_f_rank = {'head': [], 'tail': []}\n","        for h, t in hts:\n","            scores_t = scores_tail[(r, h)][:]\n","            sortidx_t = np.argsort(scores_t)[::-1]\n","            r_rank['tail'].append(np.where(sortidx_t == t)[0][0] + 1)\n","\n","            rm_idx = tt[r]['ts'][h]\n","            rm_idx = [i for i in rm_idx if i != t]\n","            for i in rm_idx:\n","                scores_t[i] = -np.Inf\n","            sortidx_t = np.argsort(scores_t)[::-1]\n","            r_f_rank['tail'].append(np.where(sortidx_t == t)[0][0] + 1)\n","\n","            scores_h = scores_head[(r, t)][:]\n","            sortidx_h = np.argsort(scores_h)[::-1]\n","            r_rank['head'].append(np.where(sortidx_h == h)[0][0] + 1)\n","\n","            rm_idx = tt[r]['hs'][t]\n","            rm_idx = [i for i in rm_idx if i != h]\n","            for i in rm_idx:\n","                scores_h[i] = -np.Inf\n","            sortidx_h = np.argsort(scores_h)[::-1]\n","            r_f_rank['head'].append(np.where(sortidx_h == h)[0][0] + 1)\n","        rank[r] = r_rank\n","        f_rank[r] = r_f_rank\n","\n","    h_pos = [p for k in rank.keys() for p in rank[k]['head']]\n","    t_pos = [p for k in rank.keys() for p in rank[k]['tail']]\n","    f_h_pos = [p for k in f_rank.keys() for p in f_rank[k]['head']]\n","    f_t_pos = [p for k in f_rank.keys() for p in f_rank[k]['tail']]\n","\n","    ranks = np.asarray(h_pos + t_pos)\n","    f_ranks = np.asarray(f_h_pos + f_t_pos)\n","    return ranks, f_ranks\n","\n","\n","def pathquery_batch_evaluation(eval_i, all_examples, batch_results,\n","                               sen_negli_dict, trivial_sen_set):\n","    \"\"\" evaluate the metrics for batch datas for pathquery datasets \"\"\"\n","    mqs = []\n","    ranks = []\n","    for j, result in enumerate(batch_results):\n","        i = eval_i + j\n","        example = all_examples[i]\n","        token_ids, mask_type = example\n","        assert mask_type in [\"MASK_TAIL\", \"MASK_HEAD\"\n","                             ], \" Unknown mask type in pathquery evaluation\"\n","        label = token_ids[-1] if mask_type == \"MASK_TAIL\" else token_ids[0]\n","\n","        sen = \" \".join([str(x) for x in token_ids])\n","        if sen in trivial_sen_set:\n","            mq = rank = -1\n","        else:\n","            # candidate vocab set\n","            cand_set = sen_negli_dict[sen]\n","            assert label in set(\n","                cand_set), \"predict label must be in the candidate set\"\n","\n","            cand_idx = np.sort(np.array(cand_set))\n","            cand_ret = result[\n","                cand_idx]  #logits for candidate words(neg + gold words)\n","            cand_ranks = np.argsort(cand_ret)[::-1]\n","            pred_y = cand_idx[cand_ranks]\n","\n","            rank = (np.argwhere(pred_y == label).ravel().tolist())[0] + 1\n","            mq = (len(cand_set) - rank) / (len(cand_set) - 1.0)\n","        mqs.append(mq)\n","        ranks.append(rank)\n","    return mqs, ranks\n","\n","\n","def compute_kbc_metrics(rank_li, frank_li, output_evaluation_result_file):\n","    \"\"\" combine the kbc rank results from batches into the final metrics \"\"\"\n","    rank_rets = np.array(rank_li).ravel()\n","    frank_rets = np.array(frank_li).ravel()\n","    mrr = np.mean(1.0 / rank_rets)\n","    fmrr = np.mean(1.0 / frank_rets)\n","\n","    hits1 = np.mean(rank_rets <= 1.0)\n","    hits3 = np.mean(rank_rets <= 3.0)\n","    hits10 = np.mean(rank_rets <= 10.0)\n","    # filtered metrics\n","    fhits1 = np.mean(frank_rets <= 1.0)\n","    fhits3 = np.mean(frank_rets <= 3.0)\n","    fhits10 = np.mean(frank_rets <= 10.0)\n","\n","    eval_result = {\n","        'mrr': mrr,\n","        'hits1': hits1,\n","        'hits3': hits3,\n","        'hits10': hits10,\n","        'fmrr': fmrr,\n","        'fhits1': fhits1,\n","        'fhits3': fhits3,\n","        'fhits10': fhits10\n","    }\n","    with open(output_evaluation_result_file, \"w\") as fw:\n","        fw.write(json.dumps(eval_result, indent=4) + \"\\n\")\n","    return eval_result\n","\n","\n","def compute_pathquery_metrics(mq_li, rank_li, output_evaluation_result_file):\n","    \"\"\" combine the pathquery mq, rank results from batches into the final metrics \"\"\"\n","    rank_rets = np.array(rank_li).ravel()\n","    _idx = np.where(rank_rets != -1)\n","\n","    non_trivial_eval_rets = rank_rets[_idx]\n","    non_trivial_mq = np.array(mq_li).ravel()[_idx]\n","    non_trivial_cnt = non_trivial_eval_rets.size\n","\n","    mq = np.mean(non_trivial_mq)\n","    mr = np.mean(non_trivial_eval_rets)\n","    mrr = np.mean(1.0 / non_trivial_eval_rets)\n","    fhits10 = np.mean(non_trivial_eval_rets <= 10.0)\n","\n","    eval_result = {\n","        'fcnt': non_trivial_cnt,\n","        'mq': mq,\n","        'mr': mr,\n","        'fhits10': fhits10\n","    }\n","\n","    with open(output_evaluation_result_file, \"w\") as fw:\n","        fw.write(json.dumps(eval_result, indent=4) + \"\\n\")\n","    return eval_result"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fD5NX0mdvTQK"},"source":["def linear_warmup_decay(learning_rate, warmup_steps, num_train_steps):\n","    \"\"\" Applies linear warmup of learning rate from 0 and decay to 0.\"\"\"\n","    with fluid.default_main_program()._lr_schedule_guard():\n","        lr = fluid.layers.tensor.create_global_var(\n","            shape=[1],\n","            value=0.0,\n","            dtype='float32',\n","            persistable=True,\n","            name=\"scheduled_learning_rate\")\n","\n","        global_step = fluid.layers.learning_rate_scheduler._decay_step_counter(\n","        )\n","\n","        with fluid.layers.control_flow.Switch() as switch:\n","            with switch.case(global_step < num_train_steps * 0.1):\n","                warmup_lr = learning_rate * (global_step /\n","                                             (num_train_steps * 0.1))\n","                fluid.layers.tensor.assign(warmup_lr, lr)\n","            with switch.default():\n","                decayed_lr = fluid.layers.learning_rate_scheduler.polynomial_decay(\n","                    learning_rate=learning_rate,\n","                    decay_steps=num_train_steps,\n","                    end_learning_rate=0.0,\n","                    power=1.0,\n","                    cycle=False)\n","                fluid.layers.tensor.assign(decayed_lr, lr)\n","\n","        return lr\n","\n","\n","def optimization(loss,\n","                 warmup_steps,\n","                 num_train_steps,\n","                 learning_rate,\n","                 train_program,\n","                 startup_prog,\n","                 weight_decay,\n","                 scheduler='linear_warmup_decay',\n","                 use_fp16=False,\n","                 loss_scaling=1.0):\n","    if warmup_steps > 0:\n","        if scheduler == 'noam_decay':\n","            scheduled_lr = fluid.layers.learning_rate_scheduler\\\n","             .noam_decay(1/(warmup_steps *(learning_rate ** 2)),\n","                         warmup_steps)\n","        elif scheduler == 'linear_warmup_decay':\n","            scheduled_lr = linear_warmup_decay(learning_rate, warmup_steps,\n","                                               num_train_steps)\n","        else:\n","            raise ValueError(\"Unkown learning rate scheduler, should be \"\n","                             \"'noam_decay' or 'linear_warmup_decay'\")\n","        optimizer = fluid.optimizer.Adam(\n","            learning_rate=scheduled_lr, epsilon=1e-6)\n","    else:\n","        optimizer = fluid.optimizer.Adam(\n","            learning_rate=learning_rate, epsilon=1e-6)\n","        scheduled_lr = learning_rate\n","\n","    clip_norm_thres = 1.0\n","    # When using mixed precision training, scale the gradient clip threshold\n","    # by loss_scaling\n","    if use_fp16 and loss_scaling > 1.0:\n","        clip_norm_thres *= loss_scaling\n","    fluid.clip.set_gradient_clip(\n","        clip=fluid.clip.GradientClipByGlobalNorm(clip_norm=clip_norm_thres))\n","\n","    def exclude_from_weight_decay(name):\n","        if name.find(\"layer_norm\") > -1:\n","            return True\n","        bias_suffix = [\"_bias\", \"_b\", \".b_0\"]\n","        for suffix in bias_suffix:\n","            if name.endswith(suffix):\n","                return True\n","        return False\n","\n","    param_list = dict()\n","\n","    if use_fp16:\n","        param_grads = optimizer.backward(loss)\n","        master_param_grads = create_master_params_grads(\n","            param_grads, train_program, startup_prog, loss_scaling)\n","\n","        for param, _ in master_param_grads:\n","            param_list[param.name] = param * 1.0\n","            param_list[param.name].stop_gradient = True\n","\n","        optimizer.apply_gradients(master_param_grads)\n","\n","        if weight_decay > 0:\n","            for param, grad in master_param_grads:\n","                # if exclude_from_weight_decay(param.name.rstrip(\".master\")):\n","                #     continue\n","                with param.block.program._optimized_guard(\n","                    [param, grad]), fluid.framework.name_scope(\"weight_decay\"):\n","                    updated_param = param - param_list[\n","                        param.name] * weight_decay * scheduled_lr\n","                    fluid.layers.assign(output=param, input=updated_param)\n","\n","        master_param_to_train_param(master_param_grads, param_grads,\n","                                    train_program)\n","\n","    else:\n","        for param in train_program.global_block().all_parameters():\n","            param_list[param.name] = param * 1.0\n","            param_list[param.name].stop_gradient = True\n","\n","        _, param_grads = optimizer.minimize(loss)\n","\n","        if weight_decay > 0:\n","            for param, grad in param_grads:\n","                # if exclude_from_weight_decay(param.name):\n","                #     continue\n","                with param.block.program._optimized_guard(\n","                    [param, grad]), fluid.framework.name_scope(\"weight_decay\"):\n","                    updated_param = param - param_list[\n","                        param.name] * weight_decay * scheduled_lr\n","                    fluid.layers.assign(output=param, input=updated_param)\n","\n","    return scheduled_lr"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ncwP_RQHtr6f"},"source":["# # yapf: disable\n","# parser = argparse.ArgumentParser()\n","# model_g = ArgumentGroup(parser, \"model\", \"model configuration and paths.\")\n","# model_g.add_arg(\"hidden_size\",              int, 256,            \"CoKE model config: hidden size, default 256\")\n","# model_g.add_arg(\"num_hidden_layers\",        int, 6,              \"CoKE model config: num_hidden_layers, default 6\")\n","# model_g.add_arg(\"num_attention_heads\",      int, 4,              \"CoKE model config: num_attention_heads, default 4\")\n","# model_g.add_arg(\"vocab_size\",               int, -1,           \"CoKE model config: vocab_size\")\n","# model_g.add_arg(\"num_relations\",         int, None,           \"CoKE model config: vocab_size\")\n","# model_g.add_arg(\"max_position_embeddings\",  int, 10,             \"CoKE model config: max_position_embeddings\")\n","# model_g.add_arg(\"hidden_act\",               str, \"gelu\",         \"CoKE model config: hidden_ac, default gelu\")\n","# model_g.add_arg(\"hidden_dropout_prob\",      float, 0.1,          \"CoKE model config: attention_probs_dropout_prob, default 0.1\")\n","# model_g.add_arg(\"attention_probs_dropout_prob\", float, 0.1,      \"CoKE model config: attention_probs_dropout_prob, default 0.1\")\n","# model_g.add_arg(\"initializer_range\",        int, 0.02,           \"CoKE model config: initializer_range\")\n","# model_g.add_arg(\"intermediate_size\",        int, 512,            \"CoKE model config: intermediate_size, default 512\")\n","\n","# model_g.add_arg(\"init_checkpoint\",          str,  None,          \"Init checkpoint to resume training from, or for prediction only\")\n","# model_g.add_arg(\"init_pretraining_params\",  str,  None,          \"Init pre-training params which preforms fine-tuning from. If the \"\n","#                  \"arg 'init_checkpoint' has been set, this argument wouldn't be valid.\")\n","# model_g.add_arg(\"checkpoints\",              str,  \"checkpoints\", \"Path to save checkpoints.\")\n","# model_g.add_arg(\"weight_sharing\",           bool, True,          \"If set, share weights between word embedding and masked lm.\")\n","\n","# train_g = ArgumentGroup(parser, \"training\", \"training options.\")\n","# train_g.add_arg(\"epoch\",             int,    100,                \"Number of epoches for training.\")\n","# train_g.add_arg(\"learning_rate\",     float,  5e-5,               \"Learning rate used to train with warmup.\")\n","# train_g.add_arg(\"lr_scheduler\",     str, \"linear_warmup_decay\",  \"scheduler of learning rate.\",\n","#                 choices=['linear_warmup_decay', 'noam_decay'])\n","# train_g.add_arg(\"soft_label\",               float, 0.9,          \"Value of soft labels for loss computation\")\n","# train_g.add_arg(\"weight_decay\",      float,  0.01,               \"Weight decay rate for L2 regularizer.\")\n","# train_g.add_arg(\"warmup_proportion\", float,  0.1,                \"Proportion of training steps to perform linear learning rate warmup for.\")\n","# train_g.add_arg(\"use_ema\",           bool,   True,               \"Whether to use ema.\")\n","# train_g.add_arg(\"ema_decay\",         float,  0.9999,             \"Decay rate for expoential moving average.\")\n","# train_g.add_arg(\"use_fp16\",          bool,   False,              \"Whether to use fp16 mixed precision training.\")\n","# train_g.add_arg(\"loss_scaling\",      float,  1.0,                \"Loss scaling factor for mixed precision training, only valid when use_fp16 is enabled.\")\n","\n","# log_g = ArgumentGroup(parser, \"logging\", \"logging related.\")\n","# log_g.add_arg(\"skip_steps\",          int,    1000,               \"The steps interval to print loss.\")\n","# log_g.add_arg(\"verbose\",             bool,   False,              \"Whether to output verbose log.\")\n","\n","# data_g = ArgumentGroup(parser, \"data\", \"Data paths, vocab paths and data processing options\")\n","# data_g.add_arg(\"dataset\",                   str,   \"\",    \"dataset name\")\n","# data_g.add_arg(\"train_file\",                str,   None,  \"Data for training.\")\n","# data_g.add_arg(\"sen_candli_file\",           str,   None,  \"sentence_candicate_list file for path query evaluation. Only used for path query datasets\")\n","# data_g.add_arg(\"sen_trivial_file\",           str,   None,  \"trivial sentence file for pathquery evaluation. Only used for path query datasets\")\n","# data_g.add_arg(\"predict_file\",              str,   None,  \"Data for predictions.\")\n","# data_g.add_arg(\"vocab_path\",                str,   None,  \"Path to vocabulary.\")\n","# data_g.add_arg(\"true_triple_path\",          str,   None,  \"Path to all true triples. Only used for KBC evaluation.\")\n","# data_g.add_arg(\"max_seq_len\",               int,   3,     \"Number of tokens of the longest sequence.\")\n","# data_g.add_arg(\"batch_size\",                int,   12,    \"Total examples' number in batch for training. see also --in_tokens.\")\n","# data_g.add_arg(\"in_tokens\",                 bool,  False,\n","#                \"If set, the batch size will be the maximum number of tokens in one batch. \"\n","#                \"Otherwise, it will be the maximum number of examples in one batch.\")\n","\n","# run_type_g = ArgumentGroup(parser, \"run_type\", \"running type options.\")\n","# run_type_g.add_arg(\"do_train\",                     bool,   False,  \"Whether to perform training.\")\n","# run_type_g.add_arg(\"do_predict\",                   bool,   False,  \"Whether to perform prediction.\")\n","# run_type_g.add_arg(\"use_cuda\",                     bool,   True,   \"If set, use GPU for training, default is True.\")\n","# run_type_g.add_arg(\"use_fast_executor\",            bool,   False,  \"If set, use fast parallel executor (in experiment).\")\n","# run_type_g.add_arg(\"num_iteration_per_drop_scope\", int,    1,      \"Ihe iteration intervals to clean up temporary variables.\")\n","\n","# args = parser.parse_args(args={})"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MSWD-fs9vqDR","outputId":"a8a8908a-cfbd-454a-c164-983db362925d"},"source":["def create_model(pyreader_name, coke_config):\n","    pyreader = fluid.layers.py_reader\\\n","            (\n","        capacity=50,\n","        shapes=[[-1, args.max_seq_len, 1],\n","                [-1, args.max_seq_len, 1],\n","                [-1, args.max_seq_len, 1], [-1, 1], [-1, 1]],\n","        dtypes=[\n","            'int64', 'int64', 'float32', 'int64', 'int64'],\n","        lod_levels=[0, 0, 0, 0, 0],\n","        name=pyreader_name,\n","        use_double_buffer=True)\n","    (src_ids, pos_ids, input_mask, mask_labels, mask_positions) = fluid.layers.read_file(pyreader)\n","\n","    coke = CoKEModel(\n","        src_ids=src_ids,\n","        position_ids=pos_ids,\n","        input_mask=input_mask,\n","        config=coke_config,\n","        soft_label=args.soft_label,\n","        weight_sharing=args.weight_sharing,\n","        use_fp16=args.use_fp16)\n","\n","    loss, fc_out = coke.get_pretraining_output(mask_label=mask_labels, mask_pos=mask_positions)\n","    if args.use_fp16 and args.loss_scaling > 1.0:\n","        loss = loss * args.loss_scaling\n","\n","    batch_ones = fluid.layers.fill_constant_batch_size_like(\n","        input=mask_labels, dtype='int64', shape=[1], value=1)\n","    num_seqs = fluid.layers.reduce_sum(input=batch_ones)\n","\n","    return pyreader, loss, fc_out, num_seqs\n","\n","\n","def pathquery_predict(test_exe, test_program, test_pyreader, fetch_list, all_examples,\n","                      sen_negli_dict, trivial_sen_set, eval_result_file):\n","    eval_i = 0\n","    step = 0\n","    batch_mqs = []\n","    batch_ranks = []\n","    test_pyreader.start()\n","    while True:\n","        try:\n","            np_fc_out = test_exe.run(fetch_list=fetch_list, program=test_program)[0]\n","            mqs, ranks = pathquery_batch_evaluation(eval_i, all_examples, np_fc_out,\n","                                                    sen_negli_dict, trivial_sen_set)\n","            batch_mqs.extend(mqs)\n","            batch_ranks.extend(ranks)\n","            step += 1\n","            if step % 10 == 0:\n","                logger.info(\"Processing pathquery_predict step:%d example: %d\" % (step, eval_i))\n","            _batch_len = np_fc_out.shape[0]\n","            eval_i += _batch_len\n","        except fluid.core.EOFException:\n","            test_pyreader.reset()\n","            break\n","\n","    eval_result = compute_pathquery_metrics(batch_mqs, batch_ranks, eval_result_file)\n","    return eval_result\n","\n","\n","def kbc_predict(test_exe, test_program, test_pyreader, fetch_list, all_examples, true_triplets_dict, eval_result_file):\n","    eval_i = 0\n","    step = 0\n","    batch_eval_rets = []\n","    f_batch_eval_rets = []\n","    test_pyreader.start()\n","    while True:\n","        try:\n","            batch_results = []\n","            np_fc_out = test_exe.run(fetch_list=fetch_list, program=test_program)[0]\n","            _batch_len = np_fc_out.shape[0]\n","            for idx in range(np_fc_out.shape[0]):\n","                logits = [float(x) for x in np_fc_out[idx].flat]\n","                batch_results.append(logits)\n","            rank, frank = kbc_batch_evaluation(eval_i, all_examples, batch_results, true_triplets_dict)\n","            batch_eval_rets.extend(rank)\n","            f_batch_eval_rets.extend(frank)\n","            if step % 10 == 0:\n","                logger.info(\"Processing kbc_predict step: %d exmaples:%d\" % (step, eval_i))\n","            step += 1\n","            eval_i += _batch_len\n","        except fluid.core.EOFException:\n","            test_pyreader.reset()\n","            break\n","    eval_result = compute_kbc_metrics(batch_eval_rets, f_batch_eval_rets, eval_result_file)\n","    return eval_result\n","\n","\n","def predict(test_exe, test_program, test_pyreader, fetch_list, all_examples, args):\n","    dataset = args.dataset\n","    if not os.path.exists(args.checkpoints):\n","        os.makedirs(args.checkpoints)\n","    eval_result_file = os.path.join(args.checkpoints, \"eval_result.json\")\n","    logger.info(\">> Evaluation result file: %s\" % eval_result_file)\n","\n","    if dataset.lower() in [\"pathquerywn\", \"pathqueryfb\"]:\n","        sen_candli_dict, trivial_sen_set = _load_pathquery_eval_dict(args.sen_candli_file,\n","                                                                   args.sen_trivial_file)\n","        logger.debug(\">> Load sen_candli_dict size: %d\" % len(sen_candli_dict))\n","        logger.debug(\">> Trivial sen set size: %d\" % len(trivial_sen_set))\n","        logger.debug(\">> Finish load sen_candli set at:{}\".format(time.ctime()))\n","        eval_performance = pathquery_predict(test_exe, test_program, test_pyreader, fetch_list,\n","                                              all_examples, sen_candli_dict, trivial_sen_set,\n","                                              eval_result_file)\n","\n","        outs = \"%s\\t%.3f\\t%.3f\" % (args.dataset, eval_performance['mq'], eval_performance['fhits10'])\n","        logger.info(\"\\n---------- Evaluation Performance --------------\\n%s\\n%s\" %\n","                    (\"\\t\".join([\"TASK\", \"MQ\", \"Hits@10\"]), outs))\n","    else:\n","        true_triplets_dict = _load_kbc_eval_dict(args.true_triple_path)\n","        logger.info(\">> Finish loading true triplets dict %s\" % time.ctime())\n","        eval_performance = kbc_predict(test_exe, test_program, test_pyreader, fetch_list,\n","                                        all_examples, true_triplets_dict, eval_result_file)\n","        outs = \"%s\\t%.3f\\t%.3f\\t%.3f\\t%.3f\" % (args.dataset,\n","                                               eval_performance['fmrr'],\n","                                               eval_performance['fhits1'],\n","                                               eval_performance['fhits3'],\n","                                               eval_performance['fhits10'])\n","        logger.info(\"\\n----------- Evaluation Performance --------------\\n%s\\n%s\" %\n","                    (\"\\t\".join([\"TASK\", \"MRR\", \"Hits@1\", \"Hits@3\", \"Hits@10\"]), outs))\n","    return eval_performance\n","\n","\n","def _load_kbc_eval_dict(true_triple_file):\n","    def load_true_triples(true_triple_file):\n","        true_triples = []\n","        with open(true_triple_file, \"r\") as fr:\n","            for line in fr.readlines():\n","                tokens = line.strip(\"\\r \\n\").split(\"\\t\")\n","                assert len(tokens) == 3\n","                true_triples.append(\n","                    (int(tokens[0]), int(tokens[1]), int(tokens[2])))\n","        logger.debug(\"Finish loading %d true triples\" % len(true_triples))\n","        return true_triples\n","    true_triples = load_true_triples(true_triple_file)\n","    true_triples_dict = collections.defaultdict(lambda: {'hs': collections.defaultdict(list),\n","                                          'ts': collections.defaultdict(list)})\n","    for h, r, t in true_triples:\n","        true_triples_dict[r]['ts'][h].append(t)\n","        true_triples_dict[r]['hs'][t].append(h)\n","    return true_triples_dict\n","\n","\n","def _load_pathquery_eval_dict(sen_candli_file, trivial_sen_file, add_gold_o = True):\n","    sen_candli_dict = dict()\n","    for line in open(sen_candli_file):\n","        line = line.strip()\n","        segs = line.split(\"\\t\")\n","        assert len(segs) == 2, \" Illegal format for sen_candli_dict, expects 2 columns data\"\n","        sen = segs[0]\n","        candset = set(segs[1].split(\" \"))\n","        if add_gold_o is True:\n","            gold_o = sen.split(\" \")[-1]\n","            candset.add(gold_o)\n","        _li = list(candset)\n","        int_li = [int(x) for x in _li]\n","        sen_candli_dict[sen] = int_li\n","    trivial_senset = {x.strip() for x in open(trivial_sen_file)}\n","\n","    return sen_candli_dict, trivial_senset\n","\n","\n","def init_coke_net_config(args, print_config = True):\n","    config = dict()\n","    config[\"hidden_size\"] = args.hidden_size\n","    config[\"num_hidden_layers\"] = args.num_hidden_layers\n","    config[\"num_attention_heads\"] = args.num_attention_heads\n","    config[\"vocab_size\"] = args.vocab_size\n","    config[\"num_relations\"] = args.num_relations\n","    config[\"max_position_embeddings\"] = args.max_position_embeddings\n","    config[\"hidden_act\"] = args.hidden_act\n","    config[\"hidden_dropout_prob\"] = args.hidden_dropout_prob\n","    config[\"attention_probs_dropout_prob\"] = args.attention_probs_dropout_prob\n","    config[\"initializer_range\"] = args.initializer_range\n","    config[\"intermediate_size\"] = args.intermediate_size\n","\n","    if print_config is True:\n","        logger.info('----------- CoKE Network Configuration -------------')\n","        for arg, value in config.items():\n","            logger.info('%s: %s' % (arg, value))\n","        logger.info('------------------------------------------------')\n","    return config\n","\n","\n","def main(args):\n","    if not (args.do_train or args.do_predict):\n","        raise ValueError(\"For args `do_train` and `do_predict`, at \"\n","                         \"least one of them must be True.\")\n","    if args.use_cuda:\n","        place = fluid.CUDAPlace(0)\n","        dev_count = fluid.core.get_cuda_device_count()\n","    else:\n","        place = fluid.CPUPlace()\n","        dev_count = int(os.environ.get('CPU_NUM', multiprocessing.cpu_count()))\n","    exe = fluid.Executor(place)\n","\n","    startup_prog = fluid.Program()\n","\n","    # Init programs\n","    coke_config = init_coke_net_config(args, print_config=True)\n","    if args.do_train:\n","        train_data_reader = get_data_reader(args, args.train_file, is_training=True,\n","                                            epoch=args.epoch, shuffle=True, dev_count=dev_count,\n","                                            vocab_size=args.vocab_size)\n","\n","        num_train_examples = train_data_reader.total_instance\n","        if args.in_tokens:\n","            max_train_steps = args.epoch * num_train_examples // (\n","                    args.batch_size // args.max_seq_len) // dev_count\n","        else:\n","            max_train_steps = args.epoch * num_train_examples // (\n","                args.batch_size) // dev_count\n","        warmup_steps = int(max_train_steps * args.warmup_proportion)\n","        logger.info(\"Device count: %d\" % dev_count)\n","        logger.info(\"Num train examples: %d\" % num_train_examples)\n","        logger.info(\"Max train steps: %d\" % max_train_steps)\n","        logger.info(\"Num warmup steps: %d\" % warmup_steps)\n","\n","        train_program = fluid.Program()\n","\n","        # Create model and set optimization for train\n","        with fluid.program_guard(train_program, startup_prog):\n","            with fluid.unique_name.guard():\n","                train_pyreader, loss, _, num_seqs = create_model(\n","                    pyreader_name='train_reader',\n","                    coke_config=coke_config)\n","\n","                scheduled_lr = optimization(\n","                    loss=loss,\n","                    warmup_steps=warmup_steps,\n","                    num_train_steps=max_train_steps,\n","                    learning_rate=args.learning_rate,\n","                    train_program=train_program,\n","                    startup_prog=startup_prog,\n","                    weight_decay=args.weight_decay,\n","                    scheduler=args.lr_scheduler,\n","                    use_fp16=args.use_fp16,\n","                    loss_scaling=args.loss_scaling)\n","\n","                if args.use_ema:\n","                    ema = fluid.optimizer.ExponentialMovingAverage(args.ema_decay)\n","                    ema.update()\n","\n","                fluid.memory_optimize(train_program, skip_opt_set=[loss.name, num_seqs.name])\n","\n","        if args.verbose:\n","            if args.in_tokens:\n","                lower_mem, upper_mem, unit = fluid.contrib.memory_usage(\n","                    program=train_program,\n","                    batch_size=args.batch_size // args.max_seq_len)\n","            else:\n","                lower_mem, upper_mem, unit = fluid.contrib.memory_usage(\n","                    program=train_program, batch_size=args.batch_size)\n","            logger.info(\"Theoretical memory usage in training:  %.3f - %.3f %s\" %\n","                        (lower_mem, upper_mem, unit))\n","\n","    if args.do_predict:\n","        # Create model for prediction\n","        test_prog = fluid.Program()\n","        with fluid.program_guard(test_prog, startup_prog):\n","            with fluid.unique_name.guard():\n","                test_pyreader, _, fc_out, num_seqs = create_model(\n","                    pyreader_name='test_reader',\n","                    coke_config=coke_config)\n","\n","                if args.use_ema and 'ema' not in dir():\n","                    ema = fluid.optimizer.ExponentialMovingAverage(args.ema_decay)\n","\n","                fluid.memory_optimize(test_prog, skip_opt_set=[fc_out.name, num_seqs.name])\n","\n","        test_prog = test_prog.clone(for_test=True)\n","\n","    exe.run(startup_prog)\n","\n","    # Init checkpoints\n","    if args.do_train:\n","        init_train_checkpoint(args, exe, startup_prog)\n","    elif args.do_predict:\n","        init_predict_checkpoint(args, exe, startup_prog)\n","\n","    # Run training\n","    if args.do_train:\n","        exec_strategy = fluid.ExecutionStrategy()\n","        exec_strategy.use_experimental_executor = args.use_fast_executor\n","        exec_strategy.num_threads = dev_count\n","        exec_strategy.num_iteration_per_drop_scope = args.num_iteration_per_drop_scope\n","\n","        train_exe = fluid.ParallelExecutor(\n","            use_cuda=args.use_cuda,\n","            loss_name=loss.name,\n","            exec_strategy=exec_strategy,\n","            main_program=train_program)\n","\n","        train_pyreader.decorate_tensor_provider(train_data_reader.data_generator())\n","\n","        train_pyreader.start()\n","        steps = 0\n","        total_cost, total_num_seqs = [], []\n","        time_begin = time.time()\n","        while steps < max_train_steps:\n","            try:\n","                steps += 1\n","                if steps % args.skip_steps == 0:\n","                    if warmup_steps <= 0:\n","                        fetch_list = [loss.name, num_seqs.name]\n","                    else:\n","                        fetch_list = [\n","                            loss.name, scheduled_lr.name, num_seqs.name\n","                        ]\n","                else:\n","                    fetch_list = []\n","\n","                outputs = train_exe.run(fetch_list=fetch_list)\n","\n","                if steps % args.skip_steps == 0:\n","                    if warmup_steps <= 0:\n","                        np_loss, np_num_seqs = outputs\n","                    else:\n","                        np_loss, np_lr, np_num_seqs = outputs\n","                    total_cost.extend(np_loss * np_num_seqs)\n","                    total_num_seqs.extend(np_num_seqs)\n","\n","                    if args.verbose:\n","                        verbose = \"train pyreader queue size: %d, \" % train_pyreader.queue.size(\n","                        )\n","                        verbose += \"learning rate: %f\" % (\n","                            np_lr[0]\n","                            if warmup_steps > 0 else args.learning_rate)\n","                        logger.info(verbose)\n","\n","                    time_end = time.time()\n","                    used_time = time_end - time_begin\n","                    current_example, epoch = train_data_reader.get_progress()\n","\n","                    logger.info(\"epoch: %d, progress: %d/%d, step: %d, loss: %f, \"\n","                                \"speed: %f steps/s\" %\n","                                (epoch, current_example, num_train_examples, steps,\n","                                 np.sum(total_cost) / np.sum(total_num_seqs),\n","                                 args.skip_steps / used_time))\n","                    total_cost, total_num_seqs = [], []\n","                    time_begin = time.time()\n","\n","                if steps == max_train_steps:\n","                    save_path = os.path.join(args.checkpoints, \"step_\" + str(steps))\n","                    fluid.io.save_persistables(exe, save_path, train_program)\n","            except fluid.core.EOFException:\n","                logger.warning(\">> EOFException\")\n","                save_path = os.path.join(args.checkpoints, \"step_\" + str(steps) + \"_final\")\n","                fluid.io.save_persistables(exe, save_path, train_program)\n","                train_pyreader.reset()\n","                break\n","        logger.info(\">>Finish training at %s \" % time.ctime())\n","\n","    # Run prediction\n","    if args.do_predict:\n","        assert dev_count == 1, \"During prediction, dev_count expects 1, current is %d\" % dev_count\n","        test_data_reader = get_data_reader(args, args.predict_file, is_training=False,\n","                                           epoch=1, shuffle=False, dev_count=dev_count,\n","                                           vocab_size=args.vocab_size)\n","        test_pyreader.decorate_tensor_provider(test_data_reader.data_generator())\n","\n","        if args.use_ema:\n","            with ema.apply(exe):\n","                eval_performance = predict(exe, test_prog, test_pyreader,\n","                                           [fc_out.name], test_data_reader.examples, args)\n","        else:\n","            eval_performance = predict(exe, test_prog, test_pyreader,\n","                                       [fc_out.name], test_data_reader.examples, args)\n","\n","        logger.info(\">>Finish predicting at %s \" % time.ctime())\n","\n","\n","def init_predict_checkpoint(args, exe, startup_prog):\n","    if args.dataset in [\"pathQueryWN\", \"pathQueryFB\"]:\n","        assert args.sen_candli_file is not None and args.sen_trivial_file is not None, \"during test, pathQuery sen_candli_file and path_trivial_file must be set \"\n","    if not args.init_checkpoint:\n","        raise ValueError(\"args 'init_checkpoint' should be set if\"\n","                         \"only doing prediction!\")\n","    init_checkpoint(\n","        exe,\n","        args.init_checkpoint,\n","        main_program=startup_prog,\n","        use_fp16=args.use_fp16)\n","\n","\n","def init_train_checkpoint(args, exe, startup_prog):\n","    if args.init_checkpoint and args.init_pretraining_params:\n","        logger.info(\n","            \"WARNING: args 'init_checkpoint' and 'init_pretraining_params' \"\n","            \"both are set! Only arg 'init_checkpoint' is made valid.\")\n","    if args.init_checkpoint:\n","        init_checkpoint(\n","            exe,\n","            args.init_checkpoint,\n","            main_program=startup_prog,\n","            use_fp16=args.use_fp16,\n","            print_var_verbose=False)\n","    elif args.init_pretraining_params:\n","        init_pretraining_params(\n","            exe,\n","            args.init_pretraining_params,\n","            main_program=startup_prog,\n","            use_fp16=args.use_fp16)\n","\n","\n","def get_data_reader(args, data_file, epoch, is_training, shuffle, dev_count, vocab_size):\n","    if args.dataset.lower() in [\"pathqueryfb\", \"pathquerywn\"]:\n","        Reader = PathqueryDataReader\n","    else:\n","        Reader = KBCDataReader\n","    data_reader = Reader(\n","        vocab_path=args.vocab_path,\n","        data_path=data_file,\n","        max_seq_len=args.max_seq_len,\n","        batch_size=args.batch_size,\n","        is_training=is_training,\n","        shuffle=shuffle,\n","        dev_count=dev_count,\n","        epoch=epoch,\n","        vocab_size=vocab_size)\n","    return data_reader\n","\n","\n","if __name__ == '__main__':\n","    args.do_train=True\n","    print_arguments(args)\n","    main(args)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["10/11/2021 14:40:08 - INFO - __main__ - -----------  Configuration Arguments -----------\n","10/11/2021 14:40:08 - INFO - __main__ - do_train: True\n"]}]}]}