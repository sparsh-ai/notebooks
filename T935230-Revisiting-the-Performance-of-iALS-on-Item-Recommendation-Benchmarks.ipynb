{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"T935230 | Revisiting the Performance of iALS on Item Recommendation Benchmarks","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyP+094M6IyLIQO6fDuRqsiT"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"id":"a6dr9dm975ah"},"source":["!wget https://github.com/hexiangnan/neural_collaborative_filtering/archive/master.zip\n","!unzip master.zip\n","!mv neural_collaborative_filtering-master/* ./"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"uSqdAibP8wF9","executionInfo":{"status":"ok","timestamp":1635510315611,"user_tz":-330,"elapsed":874,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}}},"source":["import scipy.sparse as sp\n","import numpy as np\n","import concurrent.futures\n","import argparse\n","from collections import defaultdict\n","import math\n","import heapq # for retrieval topK\n","import multiprocessing\n","from time import time"],"execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"id":"2t7yWU929anA","executionInfo":{"status":"ok","timestamp":1635510524082,"user_tz":-330,"elapsed":888,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}}},"source":["# Global variables that are shared across processes\n","_model = None\n","_testRatings = None\n","_testNegatives = None\n","_K = None"],"execution_count":20,"outputs":[]},{"cell_type":"code","metadata":{"id":"1BPDa4kN78OC","executionInfo":{"status":"ok","timestamp":1635510525107,"user_tz":-330,"elapsed":14,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}}},"source":["class Dataset(object):\n","\n","    def __init__(self, path):\n","        self.trainMatrix = self.load_rating_file_as_matrix(path + \".train.rating\")\n","        self.testRatings = self.load_rating_file_as_list(path + \".test.rating\")\n","        self.testNegatives = self.load_negative_file(path + \".test.negative\")\n","        assert len(self.testRatings) == len(self.testNegatives)\n","        \n","        self.num_users, self.num_items = self.trainMatrix.shape\n","        \n","    def load_rating_file_as_list(self, filename):\n","        ratingList = []\n","        with open(filename, \"r\") as f:\n","            line = f.readline()\n","            while line != None and line != \"\":\n","                arr = line.split(\"\\t\")\n","                user, item = int(arr[0]), int(arr[1])\n","                ratingList.append([user, item])\n","                line = f.readline()\n","        return ratingList\n","    \n","    def load_negative_file(self, filename):\n","        negativeList = []\n","        with open(filename, \"r\") as f:\n","            line = f.readline()\n","            while line != None and line != \"\":\n","                arr = line.split(\"\\t\")\n","                negatives = []\n","                for x in arr[1: ]:\n","                    negatives.append(int(x))\n","                negativeList.append(negatives)\n","                line = f.readline()\n","        return negativeList\n","    \n","    def load_rating_file_as_matrix(self, filename):\n","        '''\n","        Read .rating file and Return dok matrix.\n","        The first line of .rating file is: num_users\\t num_items\n","        '''\n","        # Get number of users and items\n","        num_users, num_items = 0, 0\n","        with open(filename, \"r\") as f:\n","            line = f.readline()\n","            while line != None and line != \"\":\n","                arr = line.split(\"\\t\")\n","                u, i = int(arr[0]), int(arr[1])\n","                num_users = max(num_users, u)\n","                num_items = max(num_items, i)\n","                line = f.readline()\n","        # Construct matrix\n","        mat = sp.dok_matrix((num_users+1, num_items+1), dtype=np.float32)\n","        with open(filename, \"r\") as f:\n","            line = f.readline()\n","            while line != None and line != \"\":\n","                arr = line.split(\"\\t\")\n","                user, item, rating = int(arr[0]), int(arr[1]), float(arr[2])\n","                if (rating > 0):\n","                    mat[user, item] = 1.0\n","                line = f.readline()    \n","        return mat"],"execution_count":21,"outputs":[]},{"cell_type":"code","metadata":{"id":"T301-0Aq9BBu","executionInfo":{"status":"ok","timestamp":1635510525108,"user_tz":-330,"elapsed":14,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}}},"source":["class IALSDataset():\n","  \"\"\"A class holding the train and test data.\"\"\"\n","\n","  def __init__(self, train_by_user, train_by_item, test, num_batches):\n","    \"\"\"Creates a DataSet and batches it.\n","    Args:\n","      train_by_user: list of (user, items)\n","      train_by_item: list of (item, users)\n","      test: list of (user, history_items, target_items)\n","      num_batches: partitions each set using this many batches.\n","    \"\"\"\n","    self.train_by_user = train_by_user\n","    self.train_by_item = train_by_item\n","    self.test = test\n","    self.num_users = len(train_by_user)\n","    self.num_items = len(train_by_item)\n","    self.user_batches = self._batch(train_by_user, num_batches)\n","    self.item_batches = self._batch(train_by_item, num_batches)\n","    self.test_batches = self._batch(test, num_batches)\n","\n","  def _batch(self, xs, num_batches):\n","    batches = [[] for _ in range(num_batches)]\n","    for i, x in enumerate(xs):\n","      batches[i % num_batches].append(x)\n","    return batches\n","\n","\n","def map_parallel(fn, xs, *args):\n","  \"\"\"Applies a function to a list, equivalent to [fn(x, *args) for x in xs].\"\"\"\n","  if len(xs) == 1:\n","    return [fn(xs[0], *args)]\n","\n","  num_threads = len(xs)\n","  executor = concurrent.futures.ProcessPoolExecutor(num_threads)\n","  futures = [executor.submit(fn, x, *args) for x in xs]\n","  concurrent.futures.wait(futures)\n","  results = [future.result() for future in futures]\n","  return results\n","\n","\n","class Recommender():\n","  \"\"\"A Recommender class used to evaluate a recommendation algorithm.\n","  Inheriting classes must implement the score() method.\n","  \"\"\"\n","\n","  def _evaluate_user(self, user_history, ground_truth, exclude):\n","    \"\"\"Evaluates one user.\n","    Args:\n","      user_history: list of items to use in the projection.\n","      ground_truth: list of target items.\n","      exclude: list of items to exclude, usually the same as ground_truth.\n","    Returns:\n","      A tuple of (Recall@20, Recall@50 and nDCG@100).\n","    \"\"\"\n","    scores = self.score(user_history)\n","    scores[exclude] = -np.infty\n","    topk = np.argsort(scores)[::-1]\n","\n","    def recall(k, gt_set, topk):\n","      result = 0.0\n","      for i in range(k):\n","        if topk[i] in gt_set:\n","          result += 1\n","      return result / min(k, len(gt_set))\n","\n","    def ndcg(k, gt_set, topk):\n","      result = 0.0\n","      norm = 0.0\n","      for i in range(k):\n","        if topk[i] in gt_set:\n","          result += 1.0/np.log2(i+2)\n","      for i in range(min(k, len(gt_set))):\n","        norm += 1.0/np.log2(i+2)\n","      return result / norm\n","\n","    gt_set = ground_truth\n","    return np.array([\n","        recall(20, gt_set, topk), recall(50, gt_set, topk),\n","        ndcg(100, gt_set, topk)\n","        ])\n","\n","  def _evaluate_users(self, users):\n","    \"\"\"Evaluates a set of users.\n","    Args:\n","      users: a list of users, where each user is a tuple\n","        (id, history, ground truth).\n","    Returns:\n","      A dict mapping user id to a tuple of (Recall@20, Recall@50, nDCG@100).\n","    \"\"\"\n","    metrics = {}\n","    for user_id, ground_truth, history in users:\n","      if set(ground_truth) & set(history):\n","        raise ValueError(\"The history and ground_truth must be disjoint.\")\n","      metrics[user_id] = self._evaluate_user(history, ground_truth, history)\n","    return metrics\n","\n","  def evaluate(self, users_batches):\n","    results = map_parallel(self._evaluate_users, users_batches)\n","    all_metrics = []\n","    for r in results:\n","      all_metrics.extend(list(r.values()))\n","    return np.mean(all_metrics, axis=0)\n","\n","\n","class IALS(Recommender):\n","  \"\"\"iALS solver.\"\"\"\n","\n","  def __init__(self, num_users, num_items, embedding_dim, reg,\n","               unobserved_weight, stddev):\n","    self.embedding_dim = embedding_dim\n","    self.reg = reg\n","    self.unobserved_weight = unobserved_weight\n","    self.user_embedding = np.random.normal(\n","        0, stddev, (num_users, embedding_dim))\n","    self.item_embedding = np.random.normal(\n","        0, stddev, (num_items, embedding_dim))\n","    self._update_user_gramian()\n","    self._update_item_gramian()\n","\n","  def _update_user_gramian(self):\n","    self.user_gramian = np.matmul(self.user_embedding.T, self.user_embedding)\n","\n","  def _update_item_gramian(self):\n","    self.item_gramian = np.matmul(self.item_embedding.T, self.item_embedding)\n","\n","  def score(self, user_history):\n","    user_emb = project(\n","        user_history, self.item_embedding, self.item_gramian, self.reg,\n","        self.unobserved_weight)\n","    result = np.dot(user_emb, self.item_embedding.T)\n","    return result\n","\n","  def train(self, ds):\n","    \"\"\"Runs one iteration of the IALS algorithm.\n","    Args:\n","      ds: a DataSet object.\n","    \"\"\"\n","    # Solve for the user embeddings\n","    self._solve(ds.user_batches, is_user=True)\n","    self._update_user_gramian()\n","    # Solve for the item embeddings\n","    self._solve(ds.item_batches, is_user=False)\n","    self._update_item_gramian()\n","\n","  def _solve(self, batches, is_user):\n","    \"\"\"Solves one side of the matrix.\"\"\"\n","    if is_user:\n","      embedding = self.user_embedding\n","      args = (self.item_embedding, self.item_gramian, self.reg,\n","              self.unobserved_weight)\n","    else:\n","      embedding = self.item_embedding\n","      args = (self.user_embedding, self.user_gramian, self.reg,\n","              self.unobserved_weight)\n","    results = map_parallel(solve, batches, *args)\n","    for r in results:\n","      for user, emb in r.items():\n","        embedding[user, :] = emb\n","\n","\n","def project(user_history, item_embedding, item_gramian, reg, unobserved_weight):\n","  \"\"\"Solves one iteration of the iALS algorithm.\"\"\"\n","  if not user_history:\n","    raise ValueError(\"empty user history in projection\")\n","  emb_dim = np.shape(item_embedding)[1]\n","  lhs = np.zeros([emb_dim, emb_dim])\n","  rhs = np.zeros([emb_dim])\n","  for item in user_history:\n","    item_emb = item_embedding[item]\n","    lhs += np.outer(item_emb, item_emb)\n","    rhs += item_emb\n","\n","  lhs += unobserved_weight * item_gramian\n","  lhs = lhs + np.identity(emb_dim) * reg\n","  return np.linalg.solve(lhs, rhs)\n","\n","\n","def solve(data_by_user, item_embedding, item_gramian, global_reg,\n","          unobserved_weight):\n","  user_embedding = {}\n","  for user, items in data_by_user:\n","    reg = global_reg *(len(items) + unobserved_weight * item_embedding.shape[0])\n","    user_embedding[user] = project(\n","        items, item_embedding, item_gramian, reg, unobserved_weight)\n","  return user_embedding"],"execution_count":22,"outputs":[]},{"cell_type":"code","metadata":{"id":"_zs6-GKG_wRX","executionInfo":{"status":"ok","timestamp":1635510525109,"user_tz":-330,"elapsed":13,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}}},"source":["def evaluate_model(model, testRatings, testNegatives, K, num_thread):\n","    \"\"\"\n","    Evaluate the performance (Hit_Ratio, NDCG) of top-K recommendation\n","    Return: score of each test rating.\n","    \"\"\"\n","    global _model\n","    global _testRatings\n","    global _testNegatives\n","    global _K\n","    _model = model\n","    _testRatings = testRatings\n","    _testNegatives = testNegatives\n","    _K = K\n","        \n","    hits, ndcgs = [],[]\n","    if(num_thread > 1): # Multi-thread\n","        pool = multiprocessing.Pool(processes=num_thread)\n","        res = pool.map(eval_one_rating, range(len(_testRatings)))\n","        pool.close()\n","        pool.join()\n","        hits = [r[0] for r in res]\n","        ndcgs = [r[1] for r in res]\n","        return (hits, ndcgs)\n","    # Single thread\n","    for idx in range(len(_testRatings)):\n","        (hr,ndcg) = eval_one_rating(idx)\n","        hits.append(hr)\n","        ndcgs.append(ndcg)      \n","    return (hits, ndcgs)\n","\n","def eval_one_rating(idx):\n","    rating = _testRatings[idx]\n","    items = _testNegatives[idx]\n","    u = rating[0]\n","    gtItem = rating[1]\n","    items.append(gtItem)\n","    # Get prediction scores\n","    map_item_score = {}\n","    users = np.full(len(items), u, dtype = 'int32')\n","    predictions = _model.predict([users, np.array(items)], \n","                                 batch_size=100, verbose=0)\n","    for i in range(len(items)):\n","        item = items[i]\n","        map_item_score[item] = predictions[i]\n","    items.pop()\n","    \n","    # Evaluate top rank list\n","    ranklist = heapq.nlargest(_K, map_item_score, key=map_item_score.get)\n","    hr = getHitRatio(ranklist, gtItem)\n","    ndcg = getNDCG(ranklist, gtItem)\n","    return (hr, ndcg)\n","\n","def getHitRatio(ranklist, gtItem):\n","    for item in ranklist:\n","        if item == gtItem:\n","            return 1\n","    return 0\n","\n","def getNDCG(ranklist, gtItem):\n","    for i in range(len(ranklist)):\n","        item = ranklist[i]\n","        if item == gtItem:\n","            return math.log(2) / math.log(i+2)\n","    return 0"],"execution_count":23,"outputs":[]},{"cell_type":"code","metadata":{"id":"L7NRk5sm9f9Z","executionInfo":{"status":"ok","timestamp":1635510525111,"user_tz":-330,"elapsed":15,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}}},"source":["class MFModel(IALS):\n","\n","  def _predict_one(self, user, item):\n","    \"\"\"Predicts the score of a user for an item.\"\"\"\n","    return np.dot(self.user_embedding[user],\n","                  self.item_embedding[item])\n","\n","  def predict(self, pairs, batch_size, verbose):\n","    \"\"\"Computes predictions for a given set of user-item pairs.\n","    Args:\n","      pairs: A pair of lists (users, items) of the same length.\n","      batch_size: unused.\n","      verbose: unused.\n","    Returns:\n","      predictions: A list of the same length as users and items, such that\n","      predictions[i] is the models prediction for (users[i], items[i]).\n","    \"\"\"\n","    del batch_size, verbose\n","    num_examples = len(pairs[0])\n","    assert num_examples == len(pairs[1])\n","    predictions = np.empty(num_examples)\n","    for i in range(num_examples):\n","      predictions[i] = self._predict_one(pairs[0][i], pairs[1][i])\n","    return predictions"],"execution_count":24,"outputs":[]},{"cell_type":"code","metadata":{"id":"zTIw3ShbAoAO","executionInfo":{"status":"ok","timestamp":1635510525112,"user_tz":-330,"elapsed":14,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}}},"source":["def evaluate(model, test_ratings, test_negatives, K=10):\n","  \"\"\"Helper that calls evaluate from the NCF libraries.\"\"\"\n","  (hits, ndcgs) = evaluate_model(model, test_ratings, test_negatives, K=K,\n","                                 num_thread=1)\n","  return np.array(hits).mean(), np.array(ndcgs).mean()"],"execution_count":25,"outputs":[]},{"cell_type":"code","metadata":{"id":"vaVNyFOUAmdu","executionInfo":{"status":"ok","timestamp":1635510525530,"user_tz":-330,"elapsed":6,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}}},"source":["class Args:\n","    # Path to the dataset\n","    data = 'Data/ml-1m'\n","    # Number of training epochs\n","    epochs = 128\n","    # Embedding dimensions, the first dimension will be used for the bias\n","    embedding_dim = 8\n","    # L2 regularization for user and item embeddings\n","    regularization = 0.0\n","    # Weight for unobserved pairs\n","    unobserved_weight = 1.0\n","    # Standard deviation for initialization\n","    stddev = 0.1\n","\n","args = Args()"],"execution_count":26,"outputs":[]},{"cell_type":"code","metadata":{"id":"uUFXzrqvA5Im","executionInfo":{"status":"ok","timestamp":1635510525532,"user_tz":-330,"elapsed":7,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}}},"source":["args.epochs = 12\n","args.embedding_dim = 192\n","args.regularization = 0.007\n","args.unobserved_weight = 0.3\n","args.stddev = 0.1"],"execution_count":27,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DefLVhgKAlTh","executionInfo":{"status":"ok","timestamp":1635513740072,"user_tz":-330,"elapsed":3214050,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}},"outputId":"390e34de-fb36-425e-8dfc-6feeb22d5feb"},"source":["# Load the dataset\n","dataset = Dataset(args.data)\n","train_pos_pairs = np.column_stack(dataset.trainMatrix.nonzero())\n","test_ratings, test_negatives = (dataset.testRatings, dataset.testNegatives)\n","print('Dataset: #user=%d, #item=%d, #train_pairs=%d, #test_pairs=%d' % (\n","    dataset.num_users, dataset.num_items, train_pos_pairs.shape[0],\n","    len(test_ratings)))\n","\n","train_by_user = defaultdict(list)\n","train_by_item = defaultdict(list)\n","\n","for u, i in train_pos_pairs:\n","    train_by_user[u].append(i)\n","    train_by_item[i].append(u)\n","\n","train_by_user = list(train_by_user.items())\n","train_by_item = list(train_by_item.items())\n","\n","train_ds = IALSDataset(train_by_user, train_by_item, [], 1)\n","\n","# Initialize the model\n","model = MFModel(dataset.num_users, dataset.num_items,\n","                args.embedding_dim, args.regularization,\n","                args.unobserved_weight,\n","                args.stddev / np.sqrt(args.embedding_dim))\n","\n","# Train and evaluate model\n","hr, ndcg = evaluate(model, test_ratings, test_negatives, K=10)\n","print('Epoch %4d:\\t HR=%.4f, NDCG=%.4f\\t'\n","    % (0, hr, ndcg))\n","for epoch in range(args.epochs):\n","    # Training\n","    _ = model.train(train_ds)\n","\n","# Evaluation\n","hr, ndcg = evaluate(model, test_ratings, test_negatives, K=10)\n","print('Epoch %4d:\\t HR=%.4f, NDCG=%.4f\\t'\n","        % (epoch+1, hr, ndcg))"],"execution_count":28,"outputs":[{"output_type":"stream","name":"stdout","text":["Dataset: #user=6040, #item=3706, #train_pairs=994169, #test_pairs=6040\n","Epoch    0:\t HR=0.1103, NDCG=0.0491\t\n","Epoch   12:\t HR=0.7316, NDCG=0.4535\t\n"]}]}]}