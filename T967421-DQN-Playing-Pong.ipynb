{"cells":[{"cell_type":"markdown","metadata":{"id":"V0U5obweqqYb"},"source":["# DQN Playing Pong"]},{"cell_type":"code","execution_count":1,"metadata":{"executionInfo":{"elapsed":8,"status":"ok","timestamp":1634481018197,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"},"user_tz":-330},"id":"26Gn8vqjn4yi"},"outputs":[],"source":["%load_ext tensorboard"]},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":26181,"status":"ok","timestamp":1634481044372,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"},"user_tz":-330},"id":"QvAH9ZbJnbid"},"outputs":[],"source":["import argparse\n","import time\n","import numpy as np\n","import collections\n","\n","import cv2\n","import gym\n","import gym.spaces\n","\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","\n","from torch.utils.tensorboard import SummaryWriter"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":39,"status":"ok","timestamp":1634481044386,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"},"user_tz":-330},"id":"2yyRzxntnuKU"},"outputs":[],"source":["DEFAULT_ENV_NAME = \"PongNoFrameskip-v4\"\n","MEAN_REWARD_BOUND = 5\n","\n","GAMMA = 0.99\n","BATCH_SIZE = 32\n","REPLAY_SIZE = 10000\n","LEARNING_RATE = 1e-4\n","SYNC_TARGET_FRAMES = 1000\n","REPLAY_START_SIZE = 10000\n","\n","EPSILON_DECAY_LAST_FRAME = 150000\n","EPSILON_START = 1.0\n","EPSILON_FINAL = 0.01\n","\n","\n","Experience = collections.namedtuple(\n","    'Experience', field_names=['state', 'action', 'reward',\n","                               'done', 'new_state'])"]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":38,"status":"ok","timestamp":1634481044388,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"},"user_tz":-330},"id":"H58NI0TVoJ1I"},"outputs":[],"source":["class DQN(nn.Module):\n","    def __init__(self, input_shape, n_actions):\n","        super(DQN, self).__init__()\n","\n","        self.conv = nn.Sequential(\n","            nn.Conv2d(input_shape[0], 32, kernel_size=8, stride=4),\n","            nn.ReLU(),\n","            nn.Conv2d(32, 64, kernel_size=4, stride=2),\n","            nn.ReLU(),\n","            nn.Conv2d(64, 64, kernel_size=3, stride=1),\n","            nn.ReLU()\n","        )\n","\n","        conv_out_size = self._get_conv_out(input_shape)\n","        self.fc = nn.Sequential(\n","            nn.Linear(conv_out_size, 512),\n","            nn.ReLU(),\n","            nn.Linear(512, n_actions)\n","        )\n","\n","    def _get_conv_out(self, shape):\n","        o = self.conv(torch.zeros(1, *shape))\n","        return int(np.prod(o.size()))\n","\n","    def forward(self, x):\n","        conv_out = self.conv(x).view(x.size()[0], -1)\n","        return self.fc(conv_out)"]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":37,"status":"ok","timestamp":1634481044389,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"},"user_tz":-330},"id":"hvKq23PEoJxy"},"outputs":[],"source":["class FireResetEnv(gym.Wrapper):\n","    def __init__(self, env=None):\n","        \"\"\"For environments where the user need to press FIRE for the game to start.\"\"\"\n","        super(FireResetEnv, self).__init__(env)\n","        assert env.unwrapped.get_action_meanings()[1] == 'FIRE'\n","        assert len(env.unwrapped.get_action_meanings()) \u003e= 3\n","\n","    def step(self, action):\n","        return self.env.step(action)\n","\n","    def reset(self):\n","        self.env.reset()\n","        obs, _, done, _ = self.env.step(1)\n","        if done:\n","            self.env.reset()\n","        obs, _, done, _ = self.env.step(2)\n","        if done:\n","            self.env.reset()\n","        return obs\n","\n","\n","class MaxAndSkipEnv(gym.Wrapper):\n","    def __init__(self, env=None, skip=4):\n","        \"\"\"Return only every `skip`-th frame\"\"\"\n","        super(MaxAndSkipEnv, self).__init__(env)\n","        # most recent raw observations (for max pooling across time steps)\n","        self._obs_buffer = collections.deque(maxlen=2)\n","        self._skip = skip\n","\n","    def step(self, action):\n","        total_reward = 0.0\n","        done = None\n","        for _ in range(self._skip):\n","            obs, reward, done, info = self.env.step(action)\n","            self._obs_buffer.append(obs)\n","            total_reward += reward\n","            if done:\n","                break\n","        max_frame = np.max(np.stack(self._obs_buffer), axis=0)\n","        return max_frame, total_reward, done, info\n","\n","    def reset(self):\n","        \"\"\"Clear past frame buffer and init. to first obs. from inner env.\"\"\"\n","        self._obs_buffer.clear()\n","        obs = self.env.reset()\n","        self._obs_buffer.append(obs)\n","        return obs\n","\n","\n","class ProcessFrame84(gym.ObservationWrapper):\n","    def __init__(self, env=None):\n","        super(ProcessFrame84, self).__init__(env)\n","        self.observation_space = gym.spaces.Box(\n","            low=0, high=255, shape=(84, 84, 1), dtype=np.uint8)\n","\n","    def observation(self, obs):\n","        return ProcessFrame84.process(obs)\n","\n","    @staticmethod\n","    def process(frame):\n","        if frame.size == 210 * 160 * 3:\n","            img = np.reshape(frame, [210, 160, 3]).astype(\n","                np.float32)\n","        elif frame.size == 250 * 160 * 3:\n","            img = np.reshape(frame, [250, 160, 3]).astype(\n","                np.float32)\n","        else:\n","            assert False, \"Unknown resolution.\"\n","        img = img[:, :, 0] * 0.299 + img[:, :, 1] * 0.587 + \\\n","              img[:, :, 2] * 0.114\n","        resized_screen = cv2.resize(\n","            img, (84, 110), interpolation=cv2.INTER_AREA)\n","        x_t = resized_screen[18:102, :]\n","        x_t = np.reshape(x_t, [84, 84, 1])\n","        return x_t.astype(np.uint8)\n","\n","\n","class ImageToPyTorch(gym.ObservationWrapper):\n","    def __init__(self, env):\n","        super(ImageToPyTorch, self).__init__(env)\n","        old_shape = self.observation_space.shape\n","        new_shape = (old_shape[-1], old_shape[0], old_shape[1])\n","        self.observation_space = gym.spaces.Box(\n","            low=0.0, high=1.0, shape=new_shape, dtype=np.float32)\n","\n","    def observation(self, observation):\n","        return np.moveaxis(observation, 2, 0)\n","\n","\n","class ScaledFloatFrame(gym.ObservationWrapper):\n","    def observation(self, obs):\n","        return np.array(obs).astype(np.float32) / 255.0\n","\n","\n","class BufferWrapper(gym.ObservationWrapper):\n","    def __init__(self, env, n_steps, dtype=np.float32):\n","        super(BufferWrapper, self).__init__(env)\n","        self.dtype = dtype\n","        old_space = env.observation_space\n","        self.observation_space = gym.spaces.Box(\n","            old_space.low.repeat(n_steps, axis=0),\n","            old_space.high.repeat(n_steps, axis=0), dtype=dtype)\n","\n","    def reset(self):\n","        self.buffer = np.zeros_like(\n","            self.observation_space.low, dtype=self.dtype)\n","        return self.observation(self.env.reset())\n","\n","    def observation(self, observation):\n","        self.buffer[:-1] = self.buffer[1:]\n","        self.buffer[-1] = observation\n","        return self.buffer\n","\n","\n","def make_env(env_name):\n","    env = gym.make(env_name)\n","    env = MaxAndSkipEnv(env)\n","    env = FireResetEnv(env)\n","    env = ProcessFrame84(env)\n","    env = ImageToPyTorch(env)\n","    env = BufferWrapper(env, 4)\n","    return ScaledFloatFrame(env)"]},{"cell_type":"code","execution_count":6,"metadata":{"executionInfo":{"elapsed":37,"status":"ok","timestamp":1634481044391,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"},"user_tz":-330},"id":"Igb0Q4-5nzGC"},"outputs":[],"source":["class ExperienceBuffer:\n","    def __init__(self, capacity):\n","        self.buffer = collections.deque(maxlen=capacity)\n","\n","    def __len__(self):\n","        return len(self.buffer)\n","\n","    def append(self, experience):\n","        self.buffer.append(experience)\n","\n","    def sample(self, batch_size):\n","        indices = np.random.choice(len(self.buffer), batch_size,\n","                                   replace=False)\n","        states, actions, rewards, dones, next_states = \\\n","            zip(*[self.buffer[idx] for idx in indices])\n","        return np.array(states), np.array(actions), \\\n","               np.array(rewards, dtype=np.float32), \\\n","               np.array(dones, dtype=np.uint8), \\\n","               np.array(next_states)\n","\n","\n","class Agent:\n","    def __init__(self, env, exp_buffer):\n","        self.env = env\n","        self.exp_buffer = exp_buffer\n","        self._reset()\n","\n","    def _reset(self):\n","        self.state = env.reset()\n","        self.total_reward = 0.0\n","\n","    @torch.no_grad()\n","    def play_step(self, net, epsilon=0.0, device=\"cpu\"):\n","        done_reward = None\n","\n","        if np.random.random() \u003c epsilon:\n","            action = env.action_space.sample()\n","        else:\n","            state_a = np.array([self.state], copy=False)\n","            state_v = torch.tensor(state_a).to(device)\n","            q_vals_v = net(state_v)\n","            _, act_v = torch.max(q_vals_v, dim=1)\n","            action = int(act_v.item())\n","\n","        # do step in the environment\n","        new_state, reward, is_done, _ = self.env.step(action)\n","        self.total_reward += reward\n","\n","        exp = Experience(self.state, action, reward,\n","                         is_done, new_state)\n","        self.exp_buffer.append(exp)\n","        self.state = new_state\n","        if is_done:\n","            done_reward = self.total_reward\n","            self._reset()\n","        return done_reward\n","\n","\n","def calc_loss(batch, net, tgt_net, device=\"cpu\"):\n","    states, actions, rewards, dones, next_states = batch\n","\n","    states_v = torch.tensor(np.array(\n","        states, copy=False)).to(device)\n","    next_states_v = torch.tensor(np.array(\n","        next_states, copy=False)).to(device)\n","    actions_v = torch.tensor(actions).to(device)\n","    rewards_v = torch.tensor(rewards).to(device)\n","    done_mask = torch.BoolTensor(dones).to(device)\n","\n","    state_action_values = net(states_v).gather(\n","        1, actions_v.unsqueeze(-1)).squeeze(-1)\n","    with torch.no_grad():\n","        next_state_values = tgt_net(next_states_v).max(1)[0]\n","        next_state_values[done_mask] = 0.0\n","        next_state_values = next_state_values.detach()\n","\n","    expected_state_action_values = next_state_values * GAMMA + \\\n","                                   rewards_v\n","    return nn.MSELoss()(state_action_values,\n","                        expected_state_action_values)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"g1TICNe4opwJ"},"outputs":[],"source":["! wget http://www.atarimania.com/roms/Roms.rar\n","! mkdir /content/ROM/\n","! unrar e /content/Roms.rar /content/ROM/\n","! python -m atari_py.import_roms /content/ROM/"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"O38T67a7nvOA"},"outputs":[{"name":"stdout","output_type":"stream","text":["DQN(\n","  (conv): Sequential(\n","    (0): Conv2d(4, 32, kernel_size=(8, 8), stride=(4, 4))\n","    (1): ReLU()\n","    (2): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2))\n","    (3): ReLU()\n","    (4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n","    (5): ReLU()\n","  )\n","  (fc): Sequential(\n","    (0): Linear(in_features=3136, out_features=512, bias=True)\n","    (1): ReLU()\n","    (2): Linear(in_features=512, out_features=6, bias=True)\n","  )\n",")\n","1010: done 1 games, reward -20.000, eps 0.99, speed 593.43 f/s\n","1920: done 2 games, reward -20.500, eps 0.99, speed 584.34 f/s\n","2866: done 3 games, reward -20.667, eps 0.98, speed 575.13 f/s\n","3822: done 4 games, reward -20.500, eps 0.97, speed 561.08 f/s\n","5084: done 5 games, reward -19.800, eps 0.97, speed 519.56 f/s\n","Best reward updated -20.000 -\u003e -19.800\n","5846: done 6 games, reward -20.000, eps 0.96, speed 379.69 f/s\n","6651: done 7 games, reward -20.143, eps 0.96, speed 370.22 f/s\n","7825: done 8 games, reward -19.750, eps 0.95, speed 351.77 f/s\n","Best reward updated -19.800 -\u003e -19.750\n","8667: done 9 games, reward -19.889, eps 0.94, speed 483.48 f/s\n","9448: done 10 games, reward -20.000, eps 0.94, speed 514.79 f/s\n","10272: done 11 games, reward -20.091, eps 0.93, speed 32.54 f/s\n","11318: done 12 games, reward -20.083, eps 0.92, speed 10.43 f/s\n","12415: done 13 games, reward -20.000, eps 0.92, speed 10.20 f/s\n","13239: done 14 games, reward -20.071, eps 0.91, speed 10.29 f/s\n","14098: done 15 games, reward -20.067, eps 0.91, speed 10.23 f/s\n","15077: done 16 games, reward -20.062, eps 0.90, speed 10.27 f/s\n","15963: done 17 games, reward -20.059, eps 0.89, speed 9.81 f/s\n","16849: done 18 games, reward -20.056, eps 0.89, speed 9.61 f/s\n","17690: done 19 games, reward -20.105, eps 0.88, speed 10.05 f/s\n","18491: done 20 games, reward -20.150, eps 0.88, speed 9.78 f/s\n","19454: done 21 games, reward -20.190, eps 0.87, speed 10.22 f/s\n","20504: done 22 games, reward -20.182, eps 0.86, speed 10.13 f/s\n","21266: done 23 games, reward -20.217, eps 0.86, speed 9.75 f/s\n","22319: done 24 games, reward -20.167, eps 0.85, speed 9.75 f/s\n","23184: done 25 games, reward -20.200, eps 0.85, speed 9.75 f/s\n","24261: done 26 games, reward -20.192, eps 0.84, speed 9.64 f/s\n","25239: done 27 games, reward -20.185, eps 0.83, speed 9.59 f/s\n","26214: done 28 games, reward -20.214, eps 0.83, speed 9.74 f/s\n","27283: done 29 games, reward -20.172, eps 0.82, speed 10.04 f/s\n","28488: done 30 games, reward -20.100, eps 0.81, speed 10.11 f/s\n","29505: done 31 games, reward -20.097, eps 0.80, speed 10.00 f/s\n","30323: done 32 games, reward -20.125, eps 0.80, speed 10.01 f/s\n","31318: done 33 games, reward -20.091, eps 0.79, speed 9.74 f/s\n","32295: done 34 games, reward -20.088, eps 0.78, speed 10.06 f/s\n","33085: done 35 games, reward -20.114, eps 0.78, speed 10.02 f/s\n","34059: done 36 games, reward -20.083, eps 0.77, speed 10.00 f/s\n","34929: done 37 games, reward -20.081, eps 0.77, speed 10.00 f/s\n","35899: done 38 games, reward -20.105, eps 0.76, speed 9.93 f/s\n","37059: done 39 games, reward -20.077, eps 0.75, speed 10.02 f/s\n","37849: done 40 games, reward -20.100, eps 0.75, speed 10.02 f/s\n","38787: done 41 games, reward -20.098, eps 0.74, speed 9.98 f/s\n","39842: done 42 games, reward -20.119, eps 0.73, speed 9.94 f/s\n","40794: done 43 games, reward -20.140, eps 0.73, speed 9.91 f/s\n","41721: done 44 games, reward -20.136, eps 0.72, speed 9.88 f/s\n","42815: done 45 games, reward -20.133, eps 0.71, speed 9.90 f/s\n","44087: done 46 games, reward -20.109, eps 0.71, speed 9.90 f/s\n","45243: done 47 games, reward -20.128, eps 0.70, speed 9.90 f/s\n"]}],"source":["if __name__ == \"__main__\":\n","    parser = argparse.ArgumentParser()\n","    parser.add_argument(\"--cuda\", default=True,\n","                        action=\"store_true\", help=\"Enable cuda\")\n","    parser.add_argument(\"--env\", default=DEFAULT_ENV_NAME,\n","                        help=\"Name of the environment, default=\" +\n","                             DEFAULT_ENV_NAME)\n","    args = parser.parse_args(args={})\n","    device = torch.device(\"cuda\" if args.cuda else \"cpu\")\n","\n","    env = make_env(args.env)\n","\n","    net = DQN(env.observation_space.shape,\n","                        env.action_space.n).to(device)\n","    tgt_net = DQN(env.observation_space.shape,\n","                            env.action_space.n).to(device)\n","    writer = SummaryWriter(comment=\"-\" + args.env)\n","    print(net)\n","\n","    buffer = ExperienceBuffer(REPLAY_SIZE)\n","    agent = Agent(env, buffer)\n","    epsilon = EPSILON_START\n","\n","    optimizer = optim.Adam(net.parameters(), lr=LEARNING_RATE)\n","    total_rewards = []\n","    frame_idx = 0\n","    ts_frame = 0\n","    ts = time.time()\n","    best_m_reward = None\n","\n","    while True:\n","        frame_idx += 1\n","        epsilon = max(EPSILON_FINAL, EPSILON_START -\n","                      frame_idx / EPSILON_DECAY_LAST_FRAME)\n","\n","        reward = agent.play_step(net, epsilon, device=device)\n","        if reward is not None:\n","            total_rewards.append(reward)\n","            speed = (frame_idx - ts_frame) / (time.time() - ts)\n","            ts_frame = frame_idx\n","            ts = time.time()\n","            m_reward = np.mean(total_rewards[-100:])\n","            print(\"%d: done %d games, reward %.3f, \"\n","                  \"eps %.2f, speed %.2f f/s\" % (\n","                frame_idx, len(total_rewards), m_reward, epsilon,\n","                speed\n","            ))\n","            writer.add_scalar(\"epsilon\", epsilon, frame_idx)\n","            writer.add_scalar(\"speed\", speed, frame_idx)\n","            writer.add_scalar(\"reward_100\", m_reward, frame_idx)\n","            writer.add_scalar(\"reward\", reward, frame_idx)\n","            if best_m_reward is None or best_m_reward \u003c m_reward:\n","                torch.save(net.state_dict(), args.env +\n","                           \"-best_%.0f.dat\" % m_reward)\n","                if best_m_reward is not None:\n","                    print(\"Best reward updated %.3f -\u003e %.3f\" % (\n","                        best_m_reward, m_reward))\n","                best_m_reward = m_reward\n","            if m_reward \u003e MEAN_REWARD_BOUND:\n","                print(\"Solved in %d frames!\" % frame_idx)\n","                break\n","\n","        if len(buffer) \u003c REPLAY_START_SIZE:\n","            continue\n","\n","        if frame_idx % SYNC_TARGET_FRAMES == 0:\n","            tgt_net.load_state_dict(net.state_dict())\n","\n","        optimizer.zero_grad()\n","        batch = buffer.sample(BATCH_SIZE)\n","        loss_t = calc_loss(batch, net, tgt_net, device=device)\n","        loss_t.backward()\n","        optimizer.step()\n","    writer.close()"]}],"metadata":{"accelerator":"GPU","colab":{"authorship_tag":"ABX9TyO5EyHtLvvu8uCECx/2HcBX","collapsed_sections":[],"name":"T967421 | DQN Playing Pong","version":""},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}