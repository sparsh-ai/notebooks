{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "project_name = \"reco-tut-sjr\"; branch = \"main\"; account = \"sparsh-ai\"\n",
    "project_path = os.path.join('/content', project_name)\n",
    "\n",
    "if not os.path.exists(project_path):\n",
    "    !cp /content/drive/MyDrive/mykeys.py /content\n",
    "    import mykeys\n",
    "    !rm /content/mykeys.py\n",
    "    path = \"/content/\" + project_name; \n",
    "    !mkdir \"{path}\"\n",
    "    %cd \"{path}\"\n",
    "    import sys; sys.path.append(path)\n",
    "    !git config --global user.email \"recotut@recohut.com\"\n",
    "    !git config --global user.name  \"reco-tut\"\n",
    "    !git init\n",
    "    !git remote add origin https://\"{mykeys.git_token}\":x-oauth-basic@github.com/\"{account}\"/\"{project_name}\".git\n",
    "    !git pull origin \"{branch}\"\n",
    "    !git checkout main\n",
    "else:\n",
    "    %cd \"{project_path}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "import string\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('punkt') \n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "stop = stopwords.words('english')\n",
    "stop_words_ = set(stopwords.words('english'))\n",
    "wn = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = sorted(glob.glob('./data/bronze/*.parquet.gz'))\n",
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_exp = pd.read_parquet(files[0])\n",
    "df_exp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_job = pd.read_parquet(files[1])\n",
    "df_job.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_poi = pd.read_parquet(files[2])\n",
    "df_poi.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create job corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_job2 = df_job.merge(df_exp, on='Applicant.ID', how='left')\n",
    "df_job2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_job2['Job.ID'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_job2 = df_job2.drop_duplicates(subset='Job.ID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_job2.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the NA's\n",
    "df_job2.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select only required columns\n",
    "cols = ['Job.ID','Title','Position', 'Company','City_x','Job.Description']\n",
    "df_job2 = df_job2[cols]\n",
    "df_job2.columns = ['Job.ID', 'Title', 'Position', 'Company','City','Job_Description']\n",
    "df_job2.head() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking for the null values again.\n",
    "df_job2.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "str_cols = ['Title', 'Position', 'Company', 'City', 'Job_Description']\n",
    "df_job2.loc[:, str_cols] = df_job2.loc[:, str_cols].fillna('')\n",
    "df_job2.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating the jobs corpus\n",
    "# combining the columns\n",
    "df_job2[\"text\"] = df_job2[\"Position\"].map(str) + \" \" + df_job2[\"Company\"] +\" \"+ df_job2[\"City\"]+\" \"+df_job2['Job_Description'] +\" \"+df_job2['Title']\n",
    "df_all = df_job2[['Job.ID', 'text', 'Title']]\n",
    "df_all = df_all.fillna(\" \")\n",
    "df_all.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def black_txt(token):\n",
    "    return  token not in stop_words_ and token not in list(string.punctuation)  and len(token)>2   \n",
    "  \n",
    "def clean_txt(text):\n",
    "  clean_text = []\n",
    "  clean_text2 = []\n",
    "  text = re.sub(\"'\", \"\",text)\n",
    "  text=re.sub(\"(\\\\d|\\\\W)+\",\" \",text) \n",
    "  text = text.replace(\"nbsp\", \"\")\n",
    "  clean_text = [ wn.lemmatize(word, pos=\"v\") for word in word_tokenize(text.lower()) if black_txt(word)]\n",
    "  clean_text2 = [word for word in clean_text if black_txt(word)]\n",
    "  return \" \".join(clean_text2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleaning the job corpus\n",
    "df_all['text'] = df_all['text'].apply(clean_txt)\n",
    "df_all.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applicant dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's now create the applicant corpus\n",
    "df_job_view = df_job[['Applicant.ID', 'Job.ID', 'Position', 'Company','City']]\n",
    "df_job_view[\"select_pos_com_city\"] = df_job_view[\"Position\"].map(str) + \"  \" + df_job_view[\"Company\"] +\"  \"+ df_job_view[\"City\"]\n",
    "df_job_view['select_pos_com_city'] = df_job_view['select_pos_com_city'].map(str).apply(clean_txt)\n",
    "df_job_view['select_pos_com_city'] = df_job_view['select_pos_com_city'].str.lower()\n",
    "df_job_view = df_job_view[['Applicant.ID','select_pos_com_city']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_job_view.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experience dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_exp.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_experience = df_exp[['Applicant.ID','Position.Name']].copy()\n",
    "df_experience['Position.Name'] = df_experience['Position.Name'].map(str).apply(clean_txt)\n",
    "df_experience =  df_experience.sort_values(by='Applicant.ID')\n",
    "df_experience = df_experience.fillna(\" \")\n",
    "df_experience.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# same applicant has 3 applications 100001 in single line so we need to join them\n",
    "df_experience = df_experience.groupby('Applicant.ID', sort=True)['Position.Name'].apply(' '.join).reset_index()\n",
    "df_experience.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Position of Interest dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_poi = df_poi.sort_values(by='Applicant.ID')\n",
    "df_poi.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_poi = df_poi.drop('Updated.At', 1)\n",
    "df_poi = df_poi.drop('Created.At', 1)\n",
    "\n",
    "#cleaning the text\n",
    "df_poi['Position.Of.Interest']=df_poi['Position.Of.Interest'].map(str).apply(clean_txt)\n",
    "df_poi = df_poi.fillna(\" \")\n",
    "df_poi = df_poi.groupby('Applicant.ID', sort=True)['Position.Of.Interest'].apply(' '.join).reset_index()\n",
    "\n",
    "df_poi.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the final user dataset by merging all the users datasets\n",
    "\n",
    "Merging `df_job_view`, `df_experience`, and `df_poi` datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_jobs_exp = df_job_view.merge(df_experience, left_on='Applicant.ID', right_on='Applicant.ID', how='outer')\n",
    "df_jobs_exp = df_jobs_exp.fillna(' ')\n",
    "df_jobs_exp = df_jobs_exp.sort_values(by='Applicant.ID')\n",
    "df_jobs_exp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_jobs_exp_poi = df_jobs_exp.merge(df_poi, left_on='Applicant.ID', right_on='Applicant.ID', how='outer')\n",
    "df_jobs_exp_poi = df_jobs_exp_poi.fillna(' ')\n",
    "df_jobs_exp_poi = df_jobs_exp_poi.sort_values(by='Applicant.ID')\n",
    "df_jobs_exp_poi.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combining all the columns\n",
    "df_jobs_exp_poi[\"text\"] = df_jobs_exp_poi[\"select_pos_com_city\"].map(str) + df_jobs_exp_poi[\"Position.Name\"] +\" \"+ df_jobs_exp_poi[\"Position.Of.Interest\"]\n",
    "df_final_person = df_jobs_exp_poi[['Applicant.ID','text']]\n",
    "df_final_person.columns = ['Applicant_id','text']\n",
    "df_final_person.loc[:, 'text'] = df_final_person.loc[:, 'text'].apply(clean_txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final_person.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exporting the processed datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir ./data/silver\n",
    "df_all.to_pickle('./data/silver/jobs.p', compression='gzip')\n",
    "df_final_person.to_pickle('./data/silver/applicants.p', compression='gzip')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Versioning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git add . && git commit -m 'commit' && git push origin main"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extras - Wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bunch_text = \" \".join(text for text in df_all.tail(10000).text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = set(STOPWORDS)\n",
    "wordcloud = WordCloud(stopwords=stopwords, background_color=\"white\", colormap= \"magma\").generate(bunch_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=[11,11])\n",
    "plt.imshow(wordcloud, interpolation=\"sinc\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget -q --show-progress -O ./docs/img1.png https://microventures.com/wp-content/uploads/team-1697987_640.png"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = Image.open(\"./docs/img1.png\").convert('RGBA')\n",
    "x = np.array(img)\n",
    "r, g, b, a = np.rollaxis(x, axis = -1)\n",
    "r[a == 0] = 255\n",
    "g[a == 0] = 255\n",
    "b[a == 0] = 255\n",
    "x = np.dstack([r, g, b, a])\n",
    "img = Image.fromarray(x, 'RGBA')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thresh = 200\n",
    "fn = lambda x : 255 if x <= thresh else 0\n",
    "wf_mask = img.convert('L').point(fn, mode='1')\n",
    "wf_mask = np.array(wf_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_format(val):\n",
    "    if val == 0:\n",
    "        return 255\n",
    "    else:\n",
    "        return val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_wf_mask = np.ndarray((wf_mask.shape[0],wf_mask.shape[1]), np.int32)\n",
    "\n",
    "for i in range(len(wf_mask)):\n",
    "    transformed_wf_mask[i] = list(map(transform_format, wf_mask[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wc = WordCloud(background_color=\"white\", mask=transformed_wf_mask,\n",
    "               stopwords=stopwords, contour_width=.1, contour_color='black')\n",
    "\n",
    "# Generate a wordcloud\n",
    "wc.generate(bunch_text)\n",
    "\n",
    "# show\n",
    "plt.figure(figsize=[20,10])\n",
    "plt.imshow(wc, interpolation=\"sinc\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Versioning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git add . && git commit -m 'commit' && git push origin main"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMnlewi+gBkU6sYPVOZPHp8",
   "collapsed_sections": [],
   "mount_file_id": "1s9OrUjRikwAtPJ8e_gME_GEuhAtDRWmt",
   "name": "reco-tut-sjr-02-eda-and-preprocessing.ipynb",
   "provenance": [
    {
     "file_id": "1s_VxICd0YIo8sUPmQhWVL59ykpfBf0OW",
     "timestamp": 1628190019913
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
