{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"2022-01-23-cartpole-drqn.ipynb","provenance":[{"file_id":"https://github.com/recohut/nbs/blob/main/raw/T244614%20%7C%20Training%20RL%20Agent%20in%20CartPole%20Environment%20with%20DRQN%20method.ipynb","timestamp":1644662651622},{"file_id":"1PG1q81afxiJCXPZbQKhgfgSBbDx9HITY","timestamp":1638447916295},{"file_id":"11_5lxJoi19PUPuTzS5RBlSitYErAUY3h","timestamp":1638447606203}],"collapsed_sections":[],"authorship_tag":"ABX9TyP6dNCvIQAXHqPMIFjfiEbw"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"tyMytwWZFUhp"},"source":["# Training RL Agent in CartPole Environment with DRQN method"]},{"cell_type":"code","metadata":{"id":"ysNR-nNZEHYs"},"source":["import tensorflow as tf\n","from datetime import datetime\n","import os\n","from tensorflow.keras.layers import Input, Dense, LSTM\n","from tensorflow.keras.optimizers import Adam\n","import gym\n","import argparse\n","import numpy as np\n","from collections import deque\n","import random"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"doMRsX9OEImC"},"source":["tf.keras.backend.set_floatx(\"float64\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dCLXisl6ELQ0","executionInfo":{"status":"ok","timestamp":1638448200655,"user_tz":-330,"elapsed":443,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}},"outputId":"449e6724-08b8-4a67-c8c1-c47524f14ad5"},"source":["parser = argparse.ArgumentParser()\n","parser.add_argument(\"--env\", default=\"CartPole-v0\")\n","parser.add_argument(\"--lr\", type=float, default=0.005)\n","parser.add_argument(\"--batch_size\", type=int, default=64)\n","parser.add_argument(\"--time_steps\", type=int, default=4)\n","parser.add_argument(\"--gamma\", type=float, default=0.95)\n","parser.add_argument(\"--eps\", type=float, default=1.0)\n","parser.add_argument(\"--eps_decay\", type=float, default=0.995)\n","parser.add_argument(\"--eps_min\", type=float, default=0.01)\n","parser.add_argument(\"--logdir\", default=\"logs\")\n","\n","args = parser.parse_args([])\n","logdir = os.path.join(\n","    args.logdir, parser.prog, args.env, datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",")\n","print(f\"Saving training logs to:{logdir}\")\n","writer = tf.summary.create_file_writer(logdir)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Saving training logs to:logs/ipykernel_launcher.py/CartPole-v0/20211202-123002\n"]}]},{"cell_type":"code","metadata":{"id":"xFoB1mz4EZHC"},"source":["class ReplayBuffer:\n","    def __init__(self, capacity=10000):\n","        self.buffer = deque(maxlen=capacity)\n","\n","    def store(self, state, action, reward, next_state, done):\n","        self.buffer.append([state, action, reward, next_state, done])\n","\n","    def sample(self):\n","        sample = random.sample(self.buffer, args.batch_size)\n","        states, actions, rewards, next_states, done = map(np.asarray, zip(*sample))\n","        states = np.array(states).reshape(args.batch_size, args.time_steps, -1)\n","        next_states = np.array(next_states).reshape(\n","            args.batch_size, args.time_steps, -1\n","        )\n","        return states, actions, rewards, next_states, done\n","\n","    def size(self):\n","        return len(self.buffer)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"c3BmqKjDEatV"},"source":["class DRQN:\n","    def __init__(self, state_dim, action_dim):\n","        self.state_dim = state_dim\n","        self.action_dim = action_dim\n","        self.epsilon = args.eps\n","\n","        self.opt = Adam(args.lr)\n","        self.compute_loss = tf.keras.losses.MeanSquaredError()\n","        self.model = self.nn_model()\n","\n","    def nn_model(self):\n","        return tf.keras.Sequential(\n","            [\n","                Input((args.time_steps, self.state_dim)),\n","                LSTM(32, activation=\"tanh\"),\n","                Dense(16, activation=\"relu\"),\n","                Dense(self.action_dim),\n","            ]\n","        )\n","\n","    def predict(self, state):\n","        return self.model.predict(state)\n","\n","    def get_action(self, state):\n","        state = np.reshape(state, [1, args.time_steps, self.state_dim])\n","        self.epsilon *= args.eps_decay\n","        self.epsilon = max(self.epsilon, args.eps_min)\n","        q_value = self.predict(state)[0]\n","        if np.random.random() < self.epsilon:\n","            return random.randint(0, self.action_dim - 1)\n","        return np.argmax(q_value)\n","\n","    def train(self, states, targets):\n","        targets = tf.stop_gradient(targets)\n","        with tf.GradientTape() as tape:\n","            logits = self.model(states, training=True)\n","            assert targets.shape == logits.shape\n","            loss = self.compute_loss(targets, logits)\n","        grads = tape.gradient(loss, self.model.trainable_variables)\n","        self.opt.apply_gradients(zip(grads, self.model.trainable_variables))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"PQZ6EpazGSub"},"source":["class Agent:\n","    def __init__(self, env):\n","        self.env = env\n","        self.state_dim = self.env.observation_space.shape[0]\n","        self.action_dim = self.env.action_space.n\n","\n","        self.states = np.zeros([args.time_steps, self.state_dim])\n","\n","        self.model = DRQN(self.state_dim, self.action_dim)\n","        self.target_model = DRQN(self.state_dim, self.action_dim)\n","        self.update_target()\n","\n","        self.buffer = ReplayBuffer()\n","\n","    def update_target(self):\n","        weights = self.model.model.get_weights()\n","        self.target_model.model.set_weights(weights)\n","\n","    def replay_experience(self):\n","        for _ in range(10):\n","            states, actions, rewards, next_states, done = self.buffer.sample()\n","            targets = self.model.predict(states)\n","            next_q_values = self.target_model.predict(next_states).max(axis=1)\n","            targets[range(args.batch_size), actions] = (\n","                rewards + (1 - done) * next_q_values * args.gamma\n","            )\n","            self.model.train(states, targets)\n","\n","    def update_states(self, next_state):\n","        self.states = np.roll(self.states, -1, axis=0)\n","        self.states[-1] = next_state\n","\n","    def train(self, max_episodes=1000):\n","        with writer.as_default():\n","            for ep in range(max_episodes):\n","                done, episode_reward = False, 0\n","                self.states = np.zeros([args.time_steps, self.state_dim])\n","                self.update_states(self.env.reset())\n","                while not done:\n","                    action = self.model.get_action(self.states)\n","                    next_state, reward, done, _ = self.env.step(action)\n","                    prev_states = self.states\n","                    self.update_states(next_state)\n","                    self.buffer.store(\n","                        prev_states, action, reward * 0.01, self.states, done\n","                    )\n","                    episode_reward += reward\n","\n","                if self.buffer.size() >= args.batch_size:\n","                    self.replay_experience()\n","                self.update_target()\n","                print(f\"Episode#{ep} Reward:{episode_reward}\")\n","                tf.summary.scalar(\"episode_reward\", episode_reward, step=ep)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sw6rePMLEcxu","executionInfo":{"status":"ok","timestamp":1638448246003,"user_tz":-330,"elapsed":18059,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}},"outputId":"ba33b4f7-851b-4359-f321-f61ccb4ab275"},"source":["if __name__ == \"__main__\":\n","    env = gym.make(\"CartPole-v0\")\n","    agent = Agent(env)\n","    agent.train(max_episodes=10)  # Increase max_episodes value"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Episode#0 Reward:25.0\n","Episode#1 Reward:10.0\n","Episode#2 Reward:15.0\n","Episode#3 Reward:15.0\n","Episode#4 Reward:26.0\n","Episode#5 Reward:14.0\n","Episode#6 Reward:11.0\n","Episode#7 Reward:14.0\n","Episode#8 Reward:13.0\n","Episode#9 Reward:20.0\n"]}]},{"cell_type":"markdown","metadata":{"id":"ixk34zYIEgB_"},"source":["---"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VDwjGZWIE5il","executionInfo":{"status":"ok","timestamp":1638448258528,"user_tz":-330,"elapsed":3221,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}},"outputId":"9331af1c-fceb-4a39-a7d5-d2afe76d9303"},"source":["!pip install -q watermark\n","%reload_ext watermark\n","%watermark -a \"Sparsh A.\" -m -iv -u -t -d"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Author: Sparsh A.\n","\n","Last updated: 2021-12-02 12:31:00\n","\n","Compiler    : GCC 7.5.0\n","OS          : Linux\n","Release     : 5.4.104+\n","Machine     : x86_64\n","Processor   : x86_64\n","CPU cores   : 2\n","Architecture: 64bit\n","\n","argparse  : 1.1\n","tensorflow: 2.7.0\n","IPython   : 5.5.0\n","gym       : 0.17.3\n","numpy     : 1.19.5\n","\n"]}]},{"cell_type":"markdown","metadata":{"id":"haKsOAX2E1XI"},"source":["---"]},{"cell_type":"markdown","metadata":{"id":"4s5DJf8WE2as"},"source":["**END**"]}]}