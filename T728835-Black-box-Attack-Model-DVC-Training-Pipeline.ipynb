{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"T728835 | Black-box Attack Model DVC Training Pipeline","provenance":[],"collapsed_sections":["q-KQcUGsZ6cD","QZRvVywhZoYk"],"mount_file_id":"1setm5s2p9gLn2YIOLhFDlO46_LdPLChv","authorship_tag":"ABX9TyM5GubGymIXMAATwgtrvR8R"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"id":"cwjDbsFozbUv","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1631872911033,"user_tz":-330,"elapsed":37797,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}},"outputId":"222e0e89-63f5-4ceb-dc92-778019221070"},"source":["!pip install -U -q dvc dvc[gdrive]\n","# !dvc get https://github.com/sparsh-ai/reco-data ml1m/v0/ratings.dat"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[K     |████████████████████████████████| 667 kB 5.2 MB/s \n","\u001b[K     |████████████████████████████████| 40 kB 15 kB/s \n","\u001b[K     |████████████████████████████████| 170 kB 59.5 MB/s \n","\u001b[K     |████████████████████████████████| 4.6 MB 47.4 MB/s \n","\u001b[K     |████████████████████████████████| 49 kB 6.1 MB/s \n","\u001b[K     |████████████████████████████████| 296 kB 69.2 MB/s \n","\u001b[K     |████████████████████████████████| 530 kB 66.2 MB/s \n","\u001b[K     |████████████████████████████████| 119 kB 73.8 MB/s \n","\u001b[K     |████████████████████████████████| 211 kB 64.3 MB/s \n","\u001b[K     |████████████████████████████████| 41 kB 685 kB/s \n","\u001b[K     |████████████████████████████████| 109 kB 66.1 MB/s \n","\u001b[K     |████████████████████████████████| 44 kB 2.5 MB/s \n","\u001b[K     |████████████████████████████████| 1.3 MB 50.7 MB/s \n","\u001b[K     |████████████████████████████████| 63 kB 1.7 MB/s \n","\u001b[K     |████████████████████████████████| 2.6 MB 37.9 MB/s \n","\u001b[K     |████████████████████████████████| 64 kB 2.4 MB/s \n","\u001b[K     |████████████████████████████████| 201 kB 54.1 MB/s \n","\u001b[K     |████████████████████████████████| 51 kB 6.5 MB/s \n","\u001b[K     |████████████████████████████████| 546 kB 56.7 MB/s \n","\u001b[K     |████████████████████████████████| 142 kB 58.7 MB/s \n","\u001b[K     |████████████████████████████████| 294 kB 69.4 MB/s \n","\u001b[K     |████████████████████████████████| 54 kB 2.4 MB/s \n","\u001b[K     |████████████████████████████████| 3.0 MB 49.5 MB/s \n","\u001b[?25h  Building wheel for configobj (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Building wheel for flufl.lock (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Building wheel for nanotime (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Building wheel for pygtrie (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Building wheel for atpublic (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Building wheel for ftfy (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Building wheel for mailchecker (setup.py) ... \u001b[?25l\u001b[?25hdone\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","tensorflow 2.6.0 requires typing-extensions~=3.7.4, but you have typing-extensions 3.10.0.2 which is incompatible.\u001b[0m\n"]}]},{"cell_type":"markdown","metadata":{"id":"q-KQcUGsZ6cD"},"source":["## Dataset"]},{"cell_type":"code","metadata":{"id":"d-do78dkFnCl"},"source":["RAW_DATASET_ROOT_FOLDER = '/content/data/bronze'\n","PREP_DATASET_ROOT_FOLDER = '/content/data/silver'\n","FNL_DATASET_ROOT_FOLDER = '/content/data/gold'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"DHp3CLL6FiKd"},"source":["import pickle\n","import shutil\n","import tempfile\n","import os\n","from datetime import date\n","from pathlib import Path\n","import gzip\n","from abc import *\n","\n","import numpy as np\n","import pandas as pd\n","from tqdm import tqdm\n","tqdm.pandas()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"IL4UTwazFGMD"},"source":["class AbstractDataset(metaclass=ABCMeta):\n","    def __init__(self, args):\n","        self.args = args\n","        self.min_rating = args.min_rating\n","        self.min_uc = args.min_uc\n","        self.min_sc = args.min_sc\n","        self.split = args.split\n","\n","        assert self.min_uc >= 2, 'Need at least 2 ratings per user for validation and test'\n","\n","    @classmethod\n","    @abstractmethod\n","    def code(cls):\n","        pass\n","\n","    @classmethod\n","    def raw_code(cls):\n","        return cls.code()\n","\n","    @classmethod\n","    def all_raw_file_names(cls):\n","        return []\n","\n","    @classmethod\n","    @abstractmethod\n","    def url(cls):\n","        pass\n","\n","    @abstractmethod\n","    def preprocess(self):\n","        pass\n","\n","    @abstractmethod\n","    def load_ratings_df(self):\n","        pass\n","\n","    @abstractmethod\n","    def maybe_download_raw_dataset(self):\n","        pass\n","\n","    def load_dataset(self):\n","        self.preprocess()\n","        dataset_path = self._get_preprocessed_dataset_path()\n","        dataset = pickle.load(dataset_path.open('rb'))\n","        return dataset\n","\n","    def filter_triplets(self, df):\n","        print('Filtering triplets')\n","        if self.min_sc > 0:\n","            item_sizes = df.groupby('sid').size()\n","            good_items = item_sizes.index[item_sizes >= self.min_sc]\n","            df = df[df['sid'].isin(good_items)]\n","\n","        if self.min_uc > 0:\n","            user_sizes = df.groupby('uid').size()\n","            good_users = user_sizes.index[user_sizes >= self.min_uc]\n","            df = df[df['uid'].isin(good_users)]\n","        return df\n","\n","    def densify_index(self, df):\n","        print('Densifying index')\n","        umap = {u: i for i, u in enumerate(set(df['uid']), start=1)}\n","        smap = {s: i for i, s in enumerate(set(df['sid']), start=1)}\n","        df['uid'] = df['uid'].map(umap)\n","        df['sid'] = df['sid'].map(smap)\n","        return df, umap, smap\n","\n","    def split_df(self, df, user_count):\n","        if self.args.split == 'leave_one_out':\n","            print('Splitting')\n","            user_group = df.groupby('uid')\n","            user2items = user_group.progress_apply(\n","                lambda d: list(d.sort_values(by=['timestamp', 'sid'])['sid']))\n","            train, val, test = {}, {}, {}\n","            for i in range(user_count):\n","                user = i + 1\n","                items = user2items[user]\n","                train[user], val[user], test[user] = items[:-2], items[-2:-1], items[-1:]\n","            return train, val, test\n","        else:\n","            raise NotImplementedError\n","\n","    def _get_rawdata_root_path(self):\n","        return Path(RAW_DATASET_ROOT_FOLDER)\n","\n","    def _get_rawdata_folder_path(self):\n","        root = self._get_rawdata_root_path()\n","        return root.joinpath(self.raw_code())\n","\n","    def _get_preprocessed_root_path(self):\n","        root = Path(PREP_DATASET_ROOT_FOLDER)\n","        return root.joinpath(self.raw_code())\n","\n","    def _get_preprocessed_folder_path(self):\n","        preprocessed_root = self._get_preprocessed_root_path()\n","        folder_name = '{}_min_rating{}-min_uc{}-min_sc{}-split{}' \\\n","            .format(self.code(), self.min_rating, self.min_uc, self.min_sc, self.split)\n","        return preprocessed_root.joinpath(folder_name)\n","\n","    def _get_preprocessed_dataset_path(self):\n","        folder = self._get_preprocessed_folder_path()\n","        return folder.joinpath('dataset.pkl')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"eBEM6mZcGBpX"},"source":["class ML1MDataset(AbstractDataset):\n","    @classmethod\n","    def code(cls):\n","        return 'ml-1m'\n","\n","    @classmethod\n","    def url(cls):\n","        return {'path':'ml1m/v0',\n","                'repo':'https://github.com/sparsh-ai/reco-data'}\n","\n","    @classmethod\n","    def all_raw_file_names(cls):\n","        return ['ratings.dat']\n","\n","    def maybe_download_raw_dataset(self):\n","        folder_path = self._get_rawdata_folder_path()\n","        if not folder_path.is_dir():\n","            folder_path.mkdir(parents=True)\n","        if all(folder_path.joinpath(filename).is_file() for filename in self.all_raw_file_names()):\n","            print('Raw data already exists. Skip downloading')\n","            return\n","        \n","        print(\"Raw file doesn't exist. Downloading...\")\n","        for filename in self.all_raw_file_names():\n","            with open(os.path.join(folder_path,filename), \"wb\") as f:\n","                with dvc.api.open(\n","                    path=self.url()['path']+'/'+filename,\n","                    repo=self.url()['repo'],\n","                    mode='rb') as scan:\n","                    f.write(scan.read())\n","        print()\n","\n","    def preprocess(self):\n","        dataset_path = self._get_preprocessed_dataset_path()\n","        if dataset_path.is_file():\n","            print('Already preprocessed. Skip preprocessing')\n","            return\n","        if not dataset_path.parent.is_dir():\n","            dataset_path.parent.mkdir(parents=True)\n","        self.maybe_download_raw_dataset()\n","        df = self.load_ratings_df()\n","        df = self.filter_triplets(df)\n","        df, umap, smap = self.densify_index(df)\n","        train, val, test = self.split_df(df, len(umap))\n","        dataset = {'train': train,\n","                   'val': val,\n","                   'test': test,\n","                   'umap': umap,\n","                   'smap': smap}\n","        with dataset_path.open('wb') as f:\n","            pickle.dump(dataset, f)\n","\n","    def load_ratings_df(self):\n","        folder_path = self._get_rawdata_folder_path()\n","        file_path = folder_path.joinpath('ratings.dat')\n","        df = pd.read_csv(file_path, sep='::', header=None)\n","        df.columns = ['uid', 'sid', 'rating', 'timestamp']\n","        return df"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Su5bfrmJguLA"},"source":["DATASETS = {\n","    ML1MDataset.code(): ML1MDataset,\n","    # ML20MDataset.code(): ML20MDataset,\n","    # SteamDataset.code(): SteamDataset,\n","    # GamesDataset.code(): GamesDataset,\n","    # BeautyDataset.code(): BeautyDataset,\n","    # BeautyDenseDataset.code(): BeautyDenseDataset,\n","    # YooChooseDataset.code(): YooChooseDataset\n","}\n","\n","\n","def dataset_factory(args):\n","    dataset = DATASETS[args.dataset_code]\n","    return dataset(args)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"QZRvVywhZoYk"},"source":["## Negative Sampling"]},{"cell_type":"code","metadata":{"id":"CeyMnI3LZqfq"},"source":["from abc import *\n","from pathlib import Path\n","import pickle\n","\n","\n","class AbstractNegativeSampler(metaclass=ABCMeta):\n","    def __init__(self, train, val, test, user_count, item_count, sample_size, seed, flag, save_folder):\n","        self.train = train\n","        self.val = val\n","        self.test = test\n","        self.user_count = user_count\n","        self.item_count = item_count\n","        self.sample_size = sample_size\n","        self.seed = seed\n","        self.flag = flag\n","        self.save_folder = save_folder\n","\n","    @classmethod\n","    @abstractmethod\n","    def code(cls):\n","        pass\n","\n","    @abstractmethod\n","    def generate_negative_samples(self):\n","        pass\n","\n","    def get_negative_samples(self):\n","        savefile_path = self._get_save_path()\n","        if savefile_path.is_file():\n","            print('Negatives samples exist. Loading.')\n","            seen_samples, negative_samples = pickle.load(savefile_path.open('rb'))\n","            return seen_samples, negative_samples\n","        print(\"Negative samples don't exist. Generating.\")\n","        seen_samples, negative_samples = self.generate_negative_samples()\n","        with savefile_path.open('wb') as f:\n","            pickle.dump([seen_samples, negative_samples], f)\n","        return seen_samples, negative_samples\n","\n","    def _get_save_path(self):\n","        folder = Path(self.save_folder)\n","        filename = '{}-sample_size{}-seed{}-{}.pkl'.format(\n","            self.code(), self.sample_size, self.seed, self.flag)\n","        return folder.joinpath(filename)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"m8VYfuAuZtXz"},"source":["from tqdm import trange\n","import numpy as np\n","\n","\n","class RandomNegativeSampler(AbstractNegativeSampler):\n","    @classmethod\n","    def code(cls):\n","        return 'random'\n","\n","    def generate_negative_samples(self):\n","        assert self.seed is not None, 'Specify seed for random sampling'\n","        np.random.seed(self.seed)\n","        num_samples = 2 * self.user_count * self.sample_size\n","        all_samples = np.random.choice(self.item_count, num_samples) + 1\n","\n","        seen_samples = {}\n","        negative_samples = {}\n","        print('Sampling negative items randomly...')\n","        j = 0\n","        for i in trange(self.user_count):\n","            user = i + 1\n","            seen = set(self.train[user])\n","            seen.update(self.val[user])\n","            seen.update(self.test[user])\n","            seen_samples[user] = seen\n","\n","            samples = []\n","            while len(samples) < self.sample_size:\n","                item = all_samples[j % num_samples]\n","                j += 1\n","                if item in seen or item in samples:\n","                    continue\n","                samples.append(item)\n","            negative_samples[user] = samples\n","\n","        return seen_samples, negative_samples"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"SfOLiCHfZtpV"},"source":["from tqdm import trange\n","from collections import Counter\n","import numpy as np\n","\n","\n","class PopularNegativeSampler(AbstractNegativeSampler):\n","    @classmethod\n","    def code(cls):\n","        return 'popular'\n","\n","    def generate_negative_samples(self):\n","        assert self.seed is not None, 'Specify seed for random sampling'\n","        np.random.seed(self.seed)\n","        popularity = self.items_by_popularity()\n","        items = list(popularity.keys())\n","        total = 0\n","        for i in range(len(items)):\n","            total += popularity[items[i]]\n","        for i in range(len(items)):\n","            popularity[items[i]] /= total\n","        probs = list(popularity.values())\n","        num_samples = 2 * self.user_count * self.sample_size\n","        all_samples = np.random.choice(items, num_samples, p=probs)\n","\n","        seen_samples = {}\n","        negative_samples = {}\n","        print('Sampling negative items by popularity...')\n","        j = 0\n","        for i in trange(self.user_count):\n","            user = i + 1\n","            seen = set(self.train[user])\n","            seen.update(self.val[user])\n","            seen.update(self.test[user])\n","            seen_samples[user] = seen\n","\n","            samples = []\n","            while len(samples) < self.sample_size:\n","                item = all_samples[j % num_samples]\n","                j += 1\n","                if item in seen or item in samples:\n","                    continue\n","                samples.append(item)\n","            negative_samples[user] = samples\n","\n","        return seen_samples, negative_samples\n","\n","    def items_by_popularity(self):\n","        popularity = Counter()\n","        self.users = sorted(self.train.keys())\n","        for user in self.users:\n","            popularity.update(self.train[user])\n","            popularity.update(self.val[user])\n","            popularity.update(self.test[user])\n","\n","        popularity = dict(popularity)\n","        popularity = {k: v for k, v in sorted(popularity.items(), key=lambda item: item[1], reverse=True)}\n","        return popularity"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"tPQmZFW3d-v5"},"source":["NEGATIVE_SAMPLERS = {\n","    PopularNegativeSampler.code(): PopularNegativeSampler,\n","    RandomNegativeSampler.code(): RandomNegativeSampler,\n","}\n","\n","\n","def negative_sampler_factory(code, train, val, test, user_count, item_count, sample_size, seed, flag, save_folder):\n","    negative_sampler = NEGATIVE_SAMPLERS[code]\n","    return negative_sampler(train, val, test, user_count, item_count, sample_size, seed, flag, save_folder)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"URvSeRPHZ9uK"},"source":["## Dataloader"]},{"cell_type":"code","metadata":{"id":"yipNZfdnZ-pd"},"source":["from abc import *\n","import random\n","\n","\n","class AbstractDataloader(metaclass=ABCMeta):\n","    def __init__(self, args, dataset):\n","        self.args = args\n","        self.rng = random.Random()\n","        self.save_folder = dataset._get_preprocessed_folder_path()\n","        dataset = dataset.load_dataset()\n","        self.train = dataset['train']\n","        self.val = dataset['val']\n","        self.test = dataset['test']\n","        self.umap = dataset['umap']\n","        self.smap = dataset['smap']\n","        self.user_count = len(self.umap)\n","        self.item_count = len(self.smap)\n","\n","    @classmethod\n","    @abstractmethod\n","    def code(cls):\n","        pass\n","\n","    @abstractmethod\n","    def get_pytorch_dataloaders(self):\n","        pass"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"cGZMtlFZaD2v"},"source":["import torch\n","import random\n","import torch.utils.data as data_utils\n","\n","\n","class RNNDataloader():\n","    def __init__(self, args, dataset):\n","        self.args = args\n","        self.rng = random.Random()\n","        self.save_folder = dataset._get_preprocessed_folder_path()\n","        dataset = dataset.load_dataset()\n","        self.train = dataset['train']\n","        self.val = dataset['val']\n","        self.test = dataset['test']\n","        self.umap = dataset['umap']\n","        self.smap = dataset['smap']\n","        self.user_count = len(self.umap)\n","        self.item_count = len(self.smap)\n","\n","        args.num_items = len(self.smap)\n","        self.max_len = args.bert_max_len\n","\n","        val_negative_sampler = negative_sampler_factory(args.test_negative_sampler_code,\n","                                                        self.train, self.val, self.test,\n","                                                        self.user_count, self.item_count,\n","                                                        args.test_negative_sample_size,\n","                                                        args.test_negative_sampling_seed,\n","                                                        'val', self.save_folder)\n","        test_negative_sampler = negative_sampler_factory(args.test_negative_sampler_code,\n","                                                         self.train, self.val, self.test,\n","                                                         self.user_count, self.item_count,\n","                                                         args.test_negative_sample_size,\n","                                                         args.test_negative_sampling_seed,\n","                                                         'test', self.save_folder)\n","\n","        self.seen_samples, self.val_negative_samples = val_negative_sampler.get_negative_samples()\n","        self.seen_samples, self.test_negative_samples = test_negative_sampler.get_negative_samples()\n","\n","    @classmethod\n","    def code(cls):\n","        return 'rnn'\n","\n","    def get_pytorch_dataloaders(self):\n","        train_loader = self._get_train_loader()\n","        val_loader = self._get_val_loader()\n","        test_loader = self._get_test_loader()\n","        return train_loader, val_loader, test_loader\n","\n","    def _get_train_loader(self):\n","        dataset = self._get_train_dataset()\n","        dataloader = data_utils.DataLoader(dataset, batch_size=self.args.train_batch_size,\n","                                           shuffle=True, pin_memory=True)\n","        return dataloader\n","\n","    def _get_train_dataset(self):\n","        dataset = RNNTrainDataset(\n","            self.train, self.max_len)\n","        return dataset\n","\n","    def _get_val_loader(self):\n","        return self._get_eval_loader(mode='val')\n","\n","    def _get_test_loader(self):\n","        return self._get_eval_loader(mode='test')\n","\n","    def _get_eval_loader(self, mode):\n","        batch_size = self.args.val_batch_size if mode == 'val' else self.args.test_batch_size\n","        dataset = self._get_eval_dataset(mode)\n","        dataloader = data_utils.DataLoader(dataset, batch_size=batch_size, shuffle=False, pin_memory=True)\n","        return dataloader\n","\n","    def _get_eval_dataset(self, mode):\n","        if mode == 'val':\n","            dataset = RNNValidDataset(self.train, self.val, self.max_len, self.val_negative_samples)\n","        elif mode == 'test':\n","            dataset = RNNTestDataset(self.train, self.val, self.test, self.max_len, self.test_negative_samples)\n","        return dataset\n","\n","\n","class RNNTrainDataset(data_utils.Dataset):\n","    def __init__(self, u2seq, max_len):\n","        # self.u2seq = u2seq\n","        # self.users = sorted(self.u2seq.keys())\n","        self.max_len = max_len\n","        self.all_seqs = []\n","        self.all_labels = []\n","        for u in sorted(u2seq.keys()):\n","            seq = u2seq[u]\n","            for i in range(1, len(seq)):\n","                self.all_seqs += [seq[:-i]]\n","                self.all_labels += [seq[-i]]\n","\n","        assert len(self.all_seqs) == len(self.all_labels)\n","\n","    def __len__(self):\n","        return len(self.all_seqs)\n","\n","    def __getitem__(self, index):\n","        tokens = self.all_seqs[index][-self.max_len:]\n","        length = len(tokens)\n","        tokens = tokens + [0] * (self.max_len - length)\n","        \n","        return torch.LongTensor(tokens), torch.LongTensor([length]), torch.LongTensor([self.all_labels[index]])\n","\n","\n","class RNNValidDataset(data_utils.Dataset):\n","    def __init__(self, u2seq, u2answer, max_len, negative_samples, valid_users=None):\n","        self.u2seq = u2seq  # train\n","        if not valid_users:\n","            self.users = sorted(self.u2seq.keys())\n","        else:\n","            self.users = valid_users\n","        self.users = sorted(self.u2seq.keys())\n","        self.u2answer = u2answer\n","        self.max_len = max_len\n","        self.negative_samples = negative_samples\n","        \n","    def __len__(self):\n","        return len(self.users)\n","\n","    def __getitem__(self, index):\n","        user = self.users[index]\n","        tokens = self.u2seq[user][-self.max_len:]\n","        length = len(tokens)\n","        tokens = tokens + [0] * (self.max_len - length)\n","\n","        answer = self.u2answer[user]\n","        negs = self.negative_samples[user]\n","        candidates = answer + negs\n","        labels = [1] * len(answer) + [0] * len(negs)\n","        \n","        return torch.LongTensor(tokens), torch.LongTensor([length]), torch.LongTensor(candidates), torch.LongTensor(labels)\n","\n","\n","class RNNTestDataset(data_utils.Dataset):\n","    def __init__(self, u2seq, u2val, u2answer, max_len, negative_samples, test_users=None):\n","        self.u2seq = u2seq  # train\n","        self.u2val = u2val  # val\n","        if not test_users:\n","            self.users = sorted(self.u2seq.keys())\n","        else:\n","            self.users = test_users\n","        self.users = sorted(self.u2seq.keys())\n","        self.u2answer = u2answer  # test\n","        self.max_len = max_len\n","        self.negative_samples = negative_samples\n","\n","    def __len__(self):\n","        return len(self.users)\n","\n","    def __getitem__(self, index):\n","        user = self.users[index]\n","        tokens = (self.u2seq[user] + self.u2val[user])[-self.max_len:]  # append validation item after train seq\n","        length = len(tokens)\n","        tokens = tokens + [0] * (self.max_len - length)\n","        answer = self.u2answer[user]\n","        negs = self.negative_samples[user]\n","        candidates = answer + negs\n","        labels = [1] * len(answer) + [0] * len(negs)\n","\n","        return torch.LongTensor(tokens), torch.LongTensor([length]), torch.LongTensor(candidates), torch.LongTensor(labels)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"bGaHGNi_sho2"},"source":["import torch\n","import random\n","import torch.utils.data as data_utils\n","\n","\n","class BERTDataloader():\n","    def __init__(self, args, dataset):\n","        self.args = args\n","        self.rng = random.Random()\n","        self.save_folder = dataset._get_preprocessed_folder_path()\n","        dataset = dataset.load_dataset()\n","        self.train = dataset['train']\n","        self.val = dataset['val']\n","        self.test = dataset['test']\n","        self.umap = dataset['umap']\n","        self.smap = dataset['smap']\n","        self.user_count = len(self.umap)\n","        self.item_count = len(self.smap)\n","\n","        args.num_items = self.item_count\n","        self.max_len = args.bert_max_len\n","        self.mask_prob = args.bert_mask_prob\n","        self.max_predictions = args.bert_max_predictions\n","        self.sliding_size = args.sliding_window_size\n","        self.CLOZE_MASK_TOKEN = self.item_count + 1\n","\n","        val_negative_sampler = negative_sampler_factory(args.test_negative_sampler_code,\n","                                                        self.train, self.val, self.test,\n","                                                        self.user_count, self.item_count,\n","                                                        args.test_negative_sample_size,\n","                                                        args.test_negative_sampling_seed,\n","                                                        'val', self.save_folder)\n","        test_negative_sampler = negative_sampler_factory(args.test_negative_sampler_code,\n","                                                         self.train, self.val, self.test,\n","                                                         self.user_count, self.item_count,\n","                                                         args.test_negative_sample_size,\n","                                                         args.test_negative_sampling_seed,\n","                                                         'test', self.save_folder)\n","\n","        self.seen_samples, self.val_negative_samples = val_negative_sampler.get_negative_samples()\n","        self.seen_samples, self.test_negative_samples = test_negative_sampler.get_negative_samples()\n","\n","    @classmethod\n","    def code(cls):\n","        return 'bert'\n","\n","    def get_pytorch_dataloaders(self):\n","        train_loader = self._get_train_loader()\n","        val_loader = self._get_val_loader()\n","        test_loader = self._get_test_loader()\n","        return train_loader, val_loader, test_loader\n","\n","    def _get_train_loader(self):\n","        dataset = self._get_train_dataset()\n","        dataloader = data_utils.DataLoader(dataset, batch_size=self.args.train_batch_size,\n","                                           shuffle=True, pin_memory=True)\n","        return dataloader\n","\n","    def _get_train_dataset(self):\n","        dataset = BERTTrainDataset(\n","            self.train, self.max_len, self.mask_prob, self.max_predictions, self.sliding_size, self.CLOZE_MASK_TOKEN, self.item_count, self.rng)\n","        return dataset\n","\n","    def _get_val_loader(self):\n","        return self._get_eval_loader(mode='val')\n","\n","    def _get_test_loader(self):\n","        return self._get_eval_loader(mode='test')\n","\n","    def _get_eval_loader(self, mode):\n","        batch_size = self.args.val_batch_size if mode == 'val' else self.args.test_batch_size\n","        dataset = self._get_eval_dataset(mode)\n","        dataloader = data_utils.DataLoader(dataset, batch_size=batch_size, shuffle=False, pin_memory=True)\n","        return dataloader\n","\n","    def _get_eval_dataset(self, mode):\n","        if mode == 'val':\n","            dataset = BERTValidDataset(self.train, self.val, self.max_len, self.CLOZE_MASK_TOKEN, self.val_negative_samples)\n","        elif mode == 'test':\n","            dataset = BERTTestDataset(self.train, self.val, self.test, self.max_len, self.CLOZE_MASK_TOKEN, self.test_negative_samples)\n","        return dataset\n","\n","\n","class BERTTrainDataset(data_utils.Dataset):\n","    def __init__(self, u2seq, max_len, mask_prob, max_predictions, sliding_size, mask_token, num_items, rng):\n","        # self.u2seq = u2seq\n","        # self.users = sorted(self.u2seq.keys())\n","        self.max_len = max_len\n","        self.mask_prob = mask_prob\n","        self.max_predictions = max_predictions\n","        self.sliding_step = int(sliding_size * max_len)\n","        self.mask_token = mask_token\n","        self.num_items = num_items\n","        self.rng = rng\n","        \n","        assert self.sliding_step > 0\n","        self.all_seqs = []\n","        for u in sorted(u2seq.keys()):\n","            seq = u2seq[u]\n","            if len(seq) < self.max_len + self.sliding_step:\n","                self.all_seqs.append(seq)\n","            else:\n","                start_idx = range(len(seq) - max_len, -1, -self.sliding_step)\n","                self.all_seqs = self.all_seqs + [seq[i:i + max_len] for i in start_idx]\n","\n","    def __len__(self):\n","        return len(self.all_seqs)\n","        # return len(self.users)\n","\n","    def __getitem__(self, index):\n","        # user = self.users[index]\n","        # seq = self._getseq(user)\n","        seq = self.all_seqs[index]\n","\n","        tokens = []\n","        labels = []\n","        covered_items = set()\n","        for i in range(len(seq)):\n","            s = seq[i]\n","            if (len(covered_items) >= self.max_predictions) or (s in covered_items):\n","                tokens.append(s)\n","                labels.append(0)\n","                continue\n","            \n","            temp_mask_prob = self.mask_prob\n","            if i == (len(seq) - 1):\n","                temp_mask_prob += 0.1 * (1 - self.mask_prob)\n","\n","            prob = self.rng.random()\n","            if prob < temp_mask_prob:\n","                covered_items.add(s)\n","                prob /= temp_mask_prob\n","                if prob < 0.8:\n","                    tokens.append(self.mask_token)\n","                elif prob < 0.9:\n","                    tokens.append(self.rng.randint(1, self.num_items))\n","                else:\n","                    tokens.append(s)\n","\n","                labels.append(s)\n","            else:\n","                tokens.append(s)\n","                labels.append(0)\n","\n","        tokens = tokens[-self.max_len:]\n","        labels = labels[-self.max_len:]\n","\n","        mask_len = self.max_len - len(tokens)\n","\n","        tokens = [0] * mask_len + tokens\n","        labels = [0] * mask_len + labels\n","\n","        return torch.LongTensor(tokens), torch.LongTensor(labels)\n","\n","    def _getseq(self, user):\n","        return self.u2seq[user]\n","\n","\n","class BERTValidDataset(data_utils.Dataset):\n","    def __init__(self, u2seq, u2answer, max_len, mask_token, negative_samples, valid_users=None):\n","        self.u2seq = u2seq  # train\n","        if not valid_users:\n","            self.users = sorted(self.u2seq.keys())\n","        else:\n","            self.users = valid_users\n","        self.u2answer = u2answer\n","        self.max_len = max_len\n","        self.mask_token = mask_token\n","        self.negative_samples = negative_samples\n","\n","    def __len__(self):\n","        return len(self.users)\n","\n","    def __getitem__(self, index):\n","        user = self.users[index]\n","        seq = self.u2seq[user]\n","        answer = self.u2answer[user]\n","        negs = self.negative_samples[user]\n","\n","        candidates = answer + negs\n","        labels = [1] * len(answer) + [0] * len(negs)\n","\n","        seq = seq + [self.mask_token]\n","        seq = seq[-self.max_len:]\n","        padding_len = self.max_len - len(seq)\n","        seq = [0] * padding_len + seq\n","\n","        return torch.LongTensor(seq), torch.LongTensor(candidates), torch.LongTensor(labels)\n","\n","\n","class BERTTestDataset(data_utils.Dataset):\n","    def __init__(self, u2seq, u2val, u2answer, max_len, mask_token, negative_samples, test_users=None):\n","        self.u2seq = u2seq  # train\n","        self.u2val = u2val  # val\n","        if not test_users:\n","            self.users = sorted(self.u2seq.keys())\n","        else:\n","            self.users = test_users\n","        self.users = sorted(self.u2seq.keys())\n","        self.u2answer = u2answer  # test\n","        self.max_len = max_len\n","        self.mask_token = mask_token\n","        self.negative_samples = negative_samples\n","\n","    def __len__(self):\n","        return len(self.users)\n","\n","    def __getitem__(self, index):\n","        user = self.users[index]\n","        seq = self.u2seq[user] + self.u2val[user]  # append validation item after train seq\n","        answer = self.u2answer[user]\n","        negs = self.negative_samples[user]\n","\n","        candidates = answer + negs\n","        labels = [1] * len(answer) + [0] * len(negs)\n","\n","        seq = seq + [self.mask_token]\n","        seq = seq[-self.max_len:]\n","        padding_len = self.max_len - len(seq)\n","        seq = [0] * padding_len + seq\n","\n","        return torch.LongTensor(seq), torch.LongTensor(candidates), torch.LongTensor(labels)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7xSS1QfzeHei"},"source":["def dataloader_factory(args):\n","    dataset = dataset_factory(args)\n","    if args.model_code == 'bert':\n","        dataloader = BERTDataloader(args, dataset)\n","    elif args.model_code == 'sas':\n","        dataloader = SASDataloader(args, dataset)\n","    else:\n","        dataloader = RNNDataloader(args, dataset)\n","    train, val, test = dataloader.get_pytorch_dataloaders()\n","    return train, val, test"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"N2PIoEWWgA8X"},"source":["## Args"]},{"cell_type":"code","metadata":{"id":"_fbBKtg6iOF7"},"source":["import numpy as np\n","import random\n","import torch\n","import argparse"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"dUiLMT_VjAhO"},"source":["def set_template(args):\n","    args.min_uc = 5\n","    args.min_sc = 5\n","    args.split = 'leave_one_out'\n","    dataset_code = {'1': 'ml-1m', '20': 'ml-20m', 'b': 'beauty', 'bd': 'beauty_dense' , 'g': 'games', 's': 'steam', 'y': 'yoochoose'}\n","    args.dataset_code = dataset_code[input('Input 1 / 20 for movielens, b for beauty, bd for dense beauty, g for games, s for steam and y for yoochoose: ')]\n","    if args.dataset_code == 'ml-1m':\n","        args.sliding_window_size = 0.5\n","        args.bert_hidden_units = 64\n","        args.bert_dropout = 0.1\n","        args.bert_attn_dropout = 0.1\n","        args.bert_max_len = 200\n","        args.bert_mask_prob = 0.2\n","        args.bert_max_predictions = 40\n","    elif args.dataset_code == 'ml-20m':\n","        args.sliding_window_size = 0.5\n","        args.bert_hidden_units = 64\n","        args.bert_dropout = 0.1\n","        args.bert_attn_dropout = 0.1\n","        args.bert_max_len = 200\n","        args.bert_mask_prob = 0.2\n","        args.bert_max_predictions = 20\n","    elif args.dataset_code in ['beauty', 'beauty_dense']:\n","        args.sliding_window_size = 0.5\n","        args.bert_hidden_units = 64\n","        args.bert_dropout = 0.5\n","        args.bert_attn_dropout = 0.2\n","        args.bert_max_len = 50\n","        args.bert_mask_prob = 0.6\n","        args.bert_max_predictions = 30\n","    elif args.dataset_code == 'games':\n","        args.sliding_window_size = 0.5\n","        args.bert_hidden_units = 64\n","        args.bert_dropout = 0.5\n","        args.bert_attn_dropout = 0.5\n","        args.bert_max_len = 50\n","        args.bert_mask_prob = 0.5\n","        args.bert_max_predictions = 25\n","    elif args.dataset_code == 'steam':\n","        args.sliding_window_size = 0.5\n","        args.bert_hidden_units = 64\n","        args.bert_dropout = 0.2\n","        args.bert_attn_dropout = 0.2\n","        args.bert_max_len = 50\n","        args.bert_mask_prob = 0.4\n","        args.bert_max_predictions = 20\n","    elif args.dataset_code == 'yoochoose':\n","        args.sliding_window_size = 0.5\n","        args.bert_hidden_units = 256\n","        args.bert_dropout = 0.2\n","        args.bert_attn_dropout = 0.2\n","        args.bert_max_len = 50\n","        args.bert_mask_prob = 0.4\n","        args.bert_max_predictions = 20\n","\n","    batch = 128\n","    args.train_batch_size = batch\n","    args.val_batch_size = batch\n","    args.test_batch_size = batch\n","    args.train_negative_sampler_code = 'random'\n","    args.train_negative_sample_size = 0\n","    args.train_negative_sampling_seed = 0\n","    args.test_negative_sampler_code = 'random'\n","    args.test_negative_sample_size = 100\n","    args.test_negative_sampling_seed = 98765\n","\n","    model_codes = {'b': 'bert', 's':'sas', 'n':'narm'}\n","    args.model_code = model_codes[input('Input model code, b for BERT, s for SASRec and n for NARM: ')]\n","\n","    if torch.cuda.is_available():\n","        args.device = 'cuda:' + input('Input GPU ID: ')\n","    else:\n","        args.device = 'cpu'\n","    args.optimizer = 'AdamW'\n","    args.lr = 0.001\n","    args.weight_decay = 0.01\n","    args.enable_lr_schedule = True\n","    args.decay_step = 10000\n","    args.gamma = 1.\n","    args.enable_lr_warmup = False\n","    args.warmup_steps = 100\n","    args.num_epochs = 1000\n","\n","    args.metric_ks = [1, 5, 10]\n","    args.best_metric = 'NDCG@10'\n","    args.model_init_seed = 98765\n","    args.bert_num_blocks = 2\n","    args.bert_num_heads = 2\n","    args.bert_head_size = None"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1FxRPzNBgChc","executionInfo":{"status":"ok","timestamp":1631873778329,"user_tz":-330,"elapsed":463,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}},"outputId":"3801d542-e661-46f5-cdbb-9ff9c2d1d1c9"},"source":["parser = argparse.ArgumentParser()\n","\n","################\n","# Dataset\n","################\n","parser.add_argument('--dataset_code', type=str, default='ml-1m', choices=DATASETS.keys())\n","parser.add_argument('--min_rating', type=int, default=0)\n","parser.add_argument('--min_uc', type=int, default=5)\n","parser.add_argument('--min_sc', type=int, default=5)\n","parser.add_argument('--split', type=str, default='leave_one_out')\n","parser.add_argument('--dataset_split_seed', type=int, default=0)\n","\n","################\n","# Dataloader\n","################\n","parser.add_argument('--dataloader_random_seed', type=float, default=0)\n","parser.add_argument('--train_batch_size', type=int, default=64)\n","parser.add_argument('--val_batch_size', type=int, default=64)\n","parser.add_argument('--test_batch_size', type=int, default=64)\n","parser.add_argument('--sliding_window_size', type=float, default=0.5)\n","\n","################\n","# NegativeSampler\n","################\n","parser.add_argument('--train_negative_sampler_code', type=str, default='random', choices=['popular', 'random'])\n","parser.add_argument('--train_negative_sample_size', type=int, default=0)\n","parser.add_argument('--train_negative_sampling_seed', type=int, default=0)\n","parser.add_argument('--test_negative_sampler_code', type=str, default='random', choices=['popular', 'random'])\n","parser.add_argument('--test_negative_sample_size', type=int, default=100)\n","parser.add_argument('--test_negative_sampling_seed', type=int, default=0)\n","\n","################\n","# Trainer\n","################\n","# device #\n","parser.add_argument('--device', type=str, default='cpu', choices=['cpu', 'cuda'])\n","parser.add_argument('--num_gpu', type=int, default=1)\n","# optimizer & lr#\n","parser.add_argument('--optimizer', type=str, default='AdamW', choices=['AdamW', 'Adam', 'SGD'])\n","parser.add_argument('--weight_decay', type=float, default=0)\n","parser.add_argument('--adam_epsilon', type=float, default=1e-9)\n","parser.add_argument('--momentum', type=float, default=None)\n","parser.add_argument('--lr', type=float, default=0.001)\n","parser.add_argument('--enable_lr_schedule', type=bool, default=True)\n","parser.add_argument('--decay_step', type=int, default=100)\n","parser.add_argument('--gamma', type=float, default=1)\n","parser.add_argument('--enable_lr_warmup', type=bool, default=True)\n","parser.add_argument('--warmup_steps', type=int, default=100)\n","# epochs #\n","parser.add_argument('--num_epochs', type=int, default=100)\n","# logger #\n","parser.add_argument('--log_period_as_iter', type=int, default=12800)\n","# evaluation #\n","parser.add_argument('--metric_ks', nargs='+', type=int, default=[1, 5, 10, 20])\n","parser.add_argument('--best_metric', type=str, default='NDCG@10')\n","\n","################\n","# Model\n","################\n","parser.add_argument('--model_code', type=str, default='bert', choices=['bert', 'sas', 'narm'])\n","# BERT specs, used for SASRec and NARM as well #\n","parser.add_argument('--bert_max_len', type=int, default=None)\n","parser.add_argument('--bert_hidden_units', type=int, default=64)\n","parser.add_argument('--bert_num_blocks', type=int, default=2)\n","parser.add_argument('--bert_num_heads', type=int, default=2)\n","parser.add_argument('--bert_head_size', type=int, default=32)\n","parser.add_argument('--bert_dropout', type=float, default=0.1)\n","parser.add_argument('--bert_attn_dropout', type=float, default=0.1)\n","parser.add_argument('--bert_mask_prob', type=float, default=0.2)\n","\n","################\n","# Distillation & Retraining\n","################\n","parser.add_argument('--num_generated_seqs', type=int, default=3000)\n","parser.add_argument('--num_original_seqs', type=int, default=0)\n","parser.add_argument('--num_poisoned_seqs', type=int, default=100)\n","parser.add_argument('--num_alter_items', type=int, default=10)\n","\n","################\n","\n","args = parser.parse_args(args={})\n","\n","print('\\n'.join(f'{k}={v}' for k, v in vars(args).items()))"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["dataset_code=ml-1m\n","min_rating=0\n","min_uc=5\n","min_sc=5\n","split=leave_one_out\n","dataset_split_seed=0\n","dataloader_random_seed=0\n","train_batch_size=64\n","val_batch_size=64\n","test_batch_size=64\n","sliding_window_size=0.5\n","train_negative_sampler_code=random\n","train_negative_sample_size=0\n","train_negative_sampling_seed=0\n","test_negative_sampler_code=random\n","test_negative_sample_size=100\n","test_negative_sampling_seed=0\n","device=cpu\n","num_gpu=1\n","optimizer=AdamW\n","weight_decay=0\n","adam_epsilon=1e-09\n","momentum=None\n","lr=0.001\n","enable_lr_schedule=True\n","decay_step=100\n","gamma=1\n","enable_lr_warmup=True\n","warmup_steps=100\n","num_epochs=100\n","log_period_as_iter=12800\n","metric_ks=[1, 5, 10, 20]\n","best_metric=NDCG@10\n","model_code=bert\n","bert_max_len=None\n","bert_hidden_units=64\n","bert_num_blocks=2\n","bert_num_heads=2\n","bert_head_size=32\n","bert_dropout=0.1\n","bert_attn_dropout=0.1\n","bert_mask_prob=0.2\n","num_generated_seqs=3000\n","num_original_seqs=0\n","num_poisoned_seqs=100\n","num_alter_items=10\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pT6EteS8i6M9","executionInfo":{"status":"ok","timestamp":1631873850392,"user_tz":-330,"elapsed":8081,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}},"outputId":"1ff4b2e9-d474-4b5d-98ae-4a54b5070265"},"source":["set_template(args)\n","print('\\n'.join(f'{k}={v}' for k, v in vars(args).items()))"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Input 1 / 20 for movielens, b for beauty, bd for dense beauty, g for games, s for steam and y for yoochoose: 1\n","Input model code, b for BERT, s for SASRec and n for NARM: b\n","dataset_code=ml-1m\n","min_rating=0\n","min_uc=5\n","min_sc=5\n","split=leave_one_out\n","dataset_split_seed=0\n","dataloader_random_seed=0\n","train_batch_size=128\n","val_batch_size=128\n","test_batch_size=128\n","sliding_window_size=0.5\n","train_negative_sampler_code=random\n","train_negative_sample_size=0\n","train_negative_sampling_seed=0\n","test_negative_sampler_code=random\n","test_negative_sample_size=100\n","test_negative_sampling_seed=98765\n","device=cpu\n","num_gpu=1\n","optimizer=AdamW\n","weight_decay=0.01\n","adam_epsilon=1e-09\n","momentum=None\n","lr=0.001\n","enable_lr_schedule=True\n","decay_step=10000\n","gamma=1.0\n","enable_lr_warmup=False\n","warmup_steps=100\n","num_epochs=1000\n","log_period_as_iter=12800\n","metric_ks=[1, 5, 10]\n","best_metric=NDCG@10\n","model_code=bert\n","bert_max_len=200\n","bert_hidden_units=64\n","bert_num_blocks=2\n","bert_num_heads=2\n","bert_head_size=None\n","bert_dropout=0.1\n","bert_attn_dropout=0.1\n","bert_mask_prob=0.2\n","num_generated_seqs=3000\n","num_original_seqs=0\n","num_poisoned_seqs=100\n","num_alter_items=10\n","bert_max_predictions=40\n","model_init_seed=98765\n"]}]},{"cell_type":"code","metadata":{"id":"AknBtJvTiNCV"},"source":["def fix_random_seed_as(random_seed):\n","    random.seed(random_seed)\n","    np.random.seed(random_seed)\n","    torch.manual_seed(random_seed)\n","    torch.backends.cudnn.deterministic = True\n","    torch.backends.cudnn.benchmark = False"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"OeAGQBj7hmwb"},"source":["fix_random_seed_as(args.model_init_seed)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"A9W86oDnQ3cu"},"source":["import dvc.api"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yhhewFrziUnW","executionInfo":{"status":"ok","timestamp":1631874009549,"user_tz":-330,"elapsed":79194,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}},"outputId":"b6df7353-c7ab-422a-85c9-8b98327b4ff0"},"source":["train_loader, val_loader, test_loader = dataloader_factory(args)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Raw file doesn't exist. Downloading...\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/oauth2client/_helpers.py:255: UserWarning: Cannot access /tmp/tmppiqjs4p3dvc-clone/.dvc/tmp/gdrive-user-credentials.json: No such file or directory\n","  warnings.warn(_MISSING_FILE_MESSAGE.format(filename))\n"]},{"output_type":"stream","name":"stdout","text":["Go to the following link in your browser:\n","\n","    https://accounts.google.com/o/oauth2/auth?client_id=710796635688-iivsgbgsb6uv1fap6635dhvuei09o66c.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.appdata&access_type=offline&response_type=code&approval_prompt=force\n","\n","Enter verification code: 4/1AX4XfWiUQd8ajlSqleJFKaAUgwPfrvetEKk_zqej8LrF-UMaTbpcY_xgoAs\n","Authentication successful.\n","\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:55: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n"]},{"output_type":"stream","name":"stdout","text":["Filtering triplets\n","Densifying index\n","Splitting\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 6040/6040 [00:08<00:00, 746.40it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Negative samples don't exist. Generating.\n","Sampling negative items randomly...\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 6040/6040 [00:01<00:00, 4920.24it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Negative samples don't exist. Generating.\n","Sampling negative items randomly...\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 6040/6040 [00:01<00:00, 4912.47it/s]\n"]}]},{"cell_type":"markdown","metadata":{"id":"oe7BpmDxpVls"},"source":["## Model"]},{"cell_type":"code","metadata":{"id":"q1HbzTinr7F2"},"source":["import math\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","\n","class TokenEmbedding(nn.Embedding):\n","    def __init__(self, vocab_size, embed_size=512):\n","        super().__init__(vocab_size, embed_size, padding_idx=0)\n","\n","\n","class PositionalEmbedding(nn.Module):\n","    def __init__(self, max_len, d_model):\n","        super().__init__()\n","        self.d_model = d_model\n","        self.pe = nn.Embedding(max_len+1, d_model)\n","\n","    def forward(self, x):\n","        pose = (x > 0) * (x > 0).sum(dim=-1).unsqueeze(1).repeat(1, x.size(-1))\n","        pose += torch.arange(start=-(x.size(1)-1), end=1, step=1, device=x.device)\n","        pose = pose * (x > 0)\n","\n","        return self.pe(pose)\n","\n","\n","class GELU(nn.Module):\n","    def forward(self, x):\n","        return 0.5 * x * (1 + torch.tanh(math.sqrt(2 / math.pi) * (x + 0.044715 * torch.pow(x, 3))))\n","\n","\n","class PositionwiseFeedForward(nn.Module):\n","    def __init__(self, d_model, d_ff):\n","        super().__init__()\n","        self.w_1 = nn.Linear(d_model, d_ff)\n","        self.w_2 = nn.Linear(d_ff, d_model)\n","        self.activation = GELU()\n","\n","    def forward(self, x):\n","        return self.w_2(self.activation(self.w_1(x)))\n","\n","\n","# layer norm\n","class LayerNorm(nn.Module):\n","    def __init__(self, features, eps=1e-6):\n","        super().__init__()\n","        self.weight = nn.Parameter(torch.ones(features))\n","        self.bias = nn.Parameter(torch.zeros(features))\n","        self.eps = eps\n","\n","    def forward(self, x):\n","        mean = x.mean(-1, keepdim=True)\n","        std = x.std(-1, keepdim=True)\n","        return self.weight * (x - mean) / (std + self.eps) + self.bias\n","\n","\n","# layer norm and dropout (dropout and then layer norm)\n","class SublayerConnection(nn.Module):\n","    def __init__(self, size, dropout):\n","        super().__init__()\n","        self.layer_norm = LayerNorm(size)\n","        self.dropout = nn.Dropout(dropout)\n","\n","    def forward(self, x, sublayer):\n","        # return x + self.dropout(sublayer(self.norm(x)))  # original implementation\n","        return self.layer_norm(x + self.dropout(sublayer(x)))  # BERT4Rec implementation\n","\n","\n","class Attention(nn.Module):\n","    def forward(self, query, key, value, mask=None, dropout=None, sas=False):\n","        scores = torch.matmul(query, key.transpose(-2, -1)) \\\n","            / math.sqrt(query.size(-1))\n","\n","        if mask is not None:\n","            scores = scores.masked_fill(mask == 0, -1e9)\n","\n","        if sas:\n","            direction_mask = torch.ones_like(scores)\n","            direction_mask = torch.tril(direction_mask)\n","            scores = scores.masked_fill(direction_mask == 0, -1e9)\n","\n","        p_attn = F.softmax(scores, dim=-1)\n","\n","        if dropout is not None:\n","            p_attn = dropout(p_attn)\n","\n","        return torch.matmul(p_attn, value), p_attn\n","\n","\n","class MultiHeadedAttention(nn.Module):\n","    def __init__(self, h, d_model, head_size=None, dropout=0.1):\n","        super().__init__()\n","        assert d_model % h == 0\n","\n","        self.h = h\n","        self.d_k = d_model // h\n","        if head_size is not None:\n","            self.head_size = head_size\n","        else:\n","            self.head_size = d_model // h\n","\n","        self.linear_layers = nn.ModuleList(\n","            [nn.Linear(d_model, self.h * self.head_size) for _ in range(3)])\n","        self.attention = Attention()\n","        self.dropout = nn.Dropout(p=dropout)\n","        self.output_linear = nn.Linear(self.h * self.head_size, d_model)\n","\n","    def forward(self, query, key, value, mask=None):\n","        batch_size = query.size(0)\n","\n","        # 1) do all the linear projections in batch from d_model => h x d_k\n","        query, key, value = [l(x).view(batch_size, -1, self.h, self.head_size).transpose(1, 2)\n","                             for l, x in zip(self.linear_layers, (query, key, value))]\n","        \n","        # 2) apply attention on all the projected vectors in batch.\n","        x, attn = self.attention(\n","            query, key, value, mask=mask, dropout=self.dropout)\n","\n","        # 3) \"concat\" using a view and apply a final linear.\n","        x = x.transpose(1, 2).contiguous().view(\n","            batch_size, -1, self.h * self.head_size)\n","        return self.output_linear(x)\n","\n","\n","class TransformerBlock(nn.Module):\n","    def __init__(self, hidden, attn_heads, head_size, feed_forward_hidden, dropout, attn_dropout=0.1):\n","        super().__init__()\n","        self.attention = MultiHeadedAttention(\n","            h=attn_heads, d_model=hidden, head_size=head_size, dropout=attn_dropout)\n","        self.feed_forward = PositionwiseFeedForward(\n","            d_model=hidden, d_ff=feed_forward_hidden)\n","        self.input_sublayer = SublayerConnection(size=hidden, dropout=dropout)\n","        self.output_sublayer = SublayerConnection(size=hidden, dropout=dropout)\n","\n","    def forward(self, x, mask):\n","        x = self.input_sublayer(\n","            x, lambda _x: self.attention.forward(_x, _x, _x, mask=mask))\n","        x = self.output_sublayer(x, self.feed_forward)\n","        return x\n","\n","\n","class SASMultiHeadedAttention(nn.Module):\n","    def __init__(self, h, d_model, head_size=None, dropout=0.1):\n","        super().__init__()\n","        assert d_model % h == 0\n","\n","        self.h = h\n","        self.d_k = d_model // h\n","        if head_size is not None:\n","            self.head_size = head_size\n","        else:\n","            self.head_size = d_model // h\n","\n","        self.linear_layers = nn.ModuleList(\n","            [nn.Linear(d_model, self.h * self.head_size) for _ in range(3)])\n","        self.attention = Attention()\n","        self.dropout = nn.Dropout(p=dropout)\n","        self.layer_norm = LayerNorm(d_model)\n","\n","    def forward(self, query, key, value, mask=None):\n","        batch_size = query.size(0)\n","\n","        # 1) do all the linear projections in batch from d_model => h x d_k\n","        query_, key_, value_ = [l(x).view(batch_size, -1, self.h, self.head_size).transpose(1, 2)\n","                             for l, x in zip(self.linear_layers, (query, key, value))]\n","        \n","        # 2) apply attention on all the projected vectors in batch.\n","        x, attn = self.attention(\n","            query_, key_, value_, mask=mask, dropout=self.dropout, sas=True)\n","\n","        # 3) \"concat\" using a view and apply a final linear.\n","        x = x.transpose(1, 2).contiguous().view(\n","            batch_size, -1, self.h * self.head_size)\n","        \n","        return self.layer_norm(x + query)\n","\n","\n","class SASPositionwiseFeedForward(nn.Module):\n","    def __init__(self, d_model, d_ff, dropout=0.1):\n","        super().__init__()\n","        self.conv1 = nn.Conv1d(in_channels=d_model, out_channels=d_ff, kernel_size=1)\n","        self.activation = nn.ReLU()\n","        self.dropout = nn.Dropout(dropout)\n","        self.conv2 = nn.Conv1d(in_channels=d_ff, out_channels=d_model, kernel_size=1)\n","        self.layer_norm = LayerNorm(d_model)\n","\n","    def forward(self, x):\n","        x_ = self.dropout(self.activation(self.conv1(x.permute(0, 2, 1))))\n","        return self.layer_norm(self.dropout(self.conv2(x_)).permute(0, 2, 1) + x)\n","\n","\n","class SASTransformerBlock(nn.Module):\n","    def __init__(self, hidden, attn_heads, head_size, feed_forward_hidden, dropout, attn_dropout=0.1):\n","        super().__init__()\n","        self.layer_norm = LayerNorm(hidden)\n","        self.attention = SASMultiHeadedAttention(\n","            h=attn_heads, d_model=hidden, head_size=head_size, dropout=attn_dropout)\n","        self.feed_forward = SASPositionwiseFeedForward(\n","            d_model=hidden, d_ff=feed_forward_hidden, dropout=dropout)\n","\n","    def forward(self, x, mask):\n","        x = self.attention(self.layer_norm(x), x, x, mask)\n","        x = self.feed_forward(x)\n","        return x"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"sXgy-q-cr_Db"},"source":["from torch import nn as nn\n","import math\n","\n","\n","class BERT(nn.Module):\n","    def __init__(self, args):\n","        super().__init__()\n","        self.args = args\n","        self.embedding = BERTEmbedding(self.args)\n","        self.model = BERTModel(self.args)\n","        self.truncated_normal_init()\n","\n","    def truncated_normal_init(self, mean=0, std=0.02, lower=-0.04, upper=0.04):\n","        with torch.no_grad():\n","            l = (1. + math.erf(((lower - mean) / std) / math.sqrt(2.))) / 2.\n","            u = (1. + math.erf(((upper - mean) / std) / math.sqrt(2.))) / 2.\n","\n","            for n, p in self.model.named_parameters():\n","                if not 'layer_norm' in n:\n","                    p.uniform_(2 * l - 1, 2 * u - 1)\n","                    p.erfinv_()\n","                    p.mul_(std * math.sqrt(2.))\n","                    p.add_(mean)\n","        \n","    def forward(self, x):\n","        x, mask = self.embedding(x)\n","        scores = self.model(x, self.embedding.token.weight, mask)\n","        return scores\n","\n","\n","class BERTEmbedding(nn.Module):\n","    def __init__(self, args):\n","        super().__init__()\n","        vocab_size = args.num_items + 2\n","        hidden = args.bert_hidden_units\n","        max_len = args.bert_max_len\n","        dropout = args.bert_dropout\n","\n","        self.token = TokenEmbedding(\n","            vocab_size=vocab_size, embed_size=hidden)\n","        self.position = PositionalEmbedding(\n","            max_len=max_len, d_model=hidden)\n","\n","        self.layer_norm = LayerNorm(features=hidden)\n","        self.dropout = nn.Dropout(p=dropout)\n","\n","    def get_mask(self, x):\n","        if len(x.shape) > 2:\n","            x = torch.ones(x.shape[:2]).to(x.device)\n","        return (x > 0).unsqueeze(1).repeat(1, x.size(1), 1).unsqueeze(1)\n","\n","    def forward(self, x):\n","        mask = self.get_mask(x)\n","        if len(x.shape) > 2:\n","            pos = self.position(torch.ones(x.shape[:2]).to(x.device))\n","            x = torch.matmul(x, self.token.weight) + pos\n","        else:\n","            x = self.token(x) + self.position(x)\n","        return self.dropout(self.layer_norm(x)), mask\n","\n","\n","class BERTModel(nn.Module):\n","    def __init__(self, args):\n","        super().__init__()\n","        hidden = args.bert_hidden_units\n","        heads = args.bert_num_heads\n","        head_size = args.bert_head_size\n","        dropout = args.bert_dropout\n","        attn_dropout = args.bert_attn_dropout\n","        layers = args.bert_num_blocks\n","\n","        self.transformer_blocks = nn.ModuleList([TransformerBlock(\n","            hidden, heads, head_size, hidden * 4, dropout, attn_dropout) for _ in range(layers)])\n","        self.linear = nn.Linear(hidden, hidden)\n","        self.bias = torch.nn.Parameter(torch.zeros(args.num_items + 2))\n","        self.bias.requires_grad = True\n","        self.activation = GELU()\n","\n","    def forward(self, x, embedding_weight, mask):\n","        for transformer in self.transformer_blocks:\n","            x = transformer.forward(x, mask)\n","        x = self.activation(self.linear(x))\n","        scores = torch.matmul(x, embedding_weight.permute(1, 0)) + self.bias\n","        return scores"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"uvIV4L_MtNWc"},"source":["## Run"]},{"cell_type":"code","metadata":{"id":"oF_w0ys1sDbU"},"source":["if args.model_code == 'bert':\n","    model = BERT(args)\n","# elif args.model_code == 'sas':\n","#     model = SASRec(args)\n","# elif args.model_code == 'narm':\n","#     model = NARM(args)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ectrw0tJs5S7"},"source":["export_root = 'experiments/' + args.model_code + '/' + args.dataset_code"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Xm13hL0xs7aV"},"source":["resume=False\n","if resume:\n","    try: \n","        model.load_state_dict(torch.load(os.path.join(export_root, 'models', 'best_acc_model.pth'), map_location='cpu').get(STATE_DICT_KEY))\n","    except FileNotFoundError:\n","        print('Failed to load old model, continue training new model...')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"OEnP_ZeetK5w"},"source":["## Trainer"]},{"cell_type":"code","metadata":{"id":"Ic_s_K8Et0F_"},"source":["STATE_DICT_KEY = 'model_state_dict'\n","OPTIMIZER_STATE_DICT_KEY = 'optimizer_state_dict'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"LWZ_v8N5tjq8"},"source":["import os\n","import torch\n","from abc import ABCMeta, abstractmethod\n","\n","\n","def save_state_dict(state_dict, path, filename):\n","    torch.save(state_dict, os.path.join(path, filename))\n","\n","\n","class LoggerService(object):\n","    def __init__(self, train_loggers=None, val_loggers=None):\n","        self.train_loggers = train_loggers if train_loggers else []\n","        self.val_loggers = val_loggers if val_loggers else []\n","\n","    def complete(self, log_data):\n","        for logger in self.train_loggers:\n","            logger.complete(**log_data)\n","        for logger in self.val_loggers:\n","            logger.complete(**log_data)\n","\n","    def log_train(self, log_data):\n","        for logger in self.train_loggers:\n","            logger.log(**log_data)\n","\n","    def log_val(self, log_data):\n","        for logger in self.val_loggers:\n","            logger.log(**log_data)\n","\n","\n","class AbstractBaseLogger(metaclass=ABCMeta):\n","    @abstractmethod\n","    def log(self, *args, **kwargs):\n","        raise NotImplementedError\n","\n","    def complete(self, *args, **kwargs):\n","        pass\n","\n","\n","class RecentModelLogger(AbstractBaseLogger):\n","    def __init__(self, checkpoint_path, filename='checkpoint-recent.pth'):\n","        self.checkpoint_path = checkpoint_path\n","        if not os.path.exists(self.checkpoint_path):\n","            os.mkdir(self.checkpoint_path)\n","        self.recent_epoch = None\n","        self.filename = filename\n","\n","    def log(self, *args, **kwargs):\n","        epoch = kwargs['epoch']\n","\n","        if self.recent_epoch != epoch:\n","            self.recent_epoch = epoch\n","            state_dict = kwargs['state_dict']\n","            state_dict['epoch'] = kwargs['epoch']\n","            save_state_dict(state_dict, self.checkpoint_path, self.filename)\n","\n","    def complete(self, *args, **kwargs):\n","        save_state_dict(kwargs['state_dict'],\n","                        self.checkpoint_path, self.filename + '.final')\n","\n","\n","class BestModelLogger(AbstractBaseLogger):\n","    def __init__(self, checkpoint_path, metric_key='mean_iou', filename='best_acc_model.pth'):\n","        self.checkpoint_path = checkpoint_path\n","        if not os.path.exists(self.checkpoint_path):\n","            os.mkdir(self.checkpoint_path)\n","\n","        self.best_metric = 0.\n","        self.metric_key = metric_key\n","        self.filename = filename\n","\n","    def log(self, *args, **kwargs):\n","        current_metric = kwargs[self.metric_key]\n","        if self.best_metric < current_metric:\n","            print(\"Update Best {} Model at {}\".format(\n","                self.metric_key, kwargs['epoch']))\n","            self.best_metric = current_metric\n","            save_state_dict(kwargs['state_dict'],\n","                            self.checkpoint_path, self.filename)\n","\n","\n","class MetricGraphPrinter(AbstractBaseLogger):\n","    def __init__(self, writer, key='train_loss', graph_name='Train Loss', group_name='metric'):\n","        self.key = key\n","        self.graph_label = graph_name\n","        self.group_name = group_name\n","        self.writer = writer\n","\n","    def log(self, *args, **kwargs):\n","        if self.key in kwargs:\n","            self.writer.add_scalar(\n","                self.group_name + '/' + self.graph_label, kwargs[self.key], kwargs['accum_iter'])\n","        else:\n","            self.writer.add_scalar(\n","                self.group_name + '/' + self.graph_label, 0, kwargs['accum_iter'])\n","\n","    def complete(self, *args, **kwargs):\n","        self.writer.close()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"OCpOZTWQtcc5"},"source":["import json\n","import os\n","import pprint as pp\n","import random\n","from datetime import date\n","from pathlib import Path\n","\n","import numpy as np\n","import torch\n","import torch.nn.functional as F\n","import torch.backends.cudnn as cudnn\n","from torch import optim as optim\n","\n","\n","def ndcg(scores, labels, k):\n","    scores = scores.cpu()\n","    labels = labels.cpu()\n","    rank = (-scores).argsort(dim=1)\n","    cut = rank[:, :k]\n","    hits = labels.gather(1, cut)\n","    position = torch.arange(2, 2+k)\n","    weights = 1 / torch.log2(position.float())\n","    dcg = (hits.float() * weights).sum(1)\n","    idcg = torch.Tensor([weights[:min(int(n), k)].sum()\n","                         for n in labels.sum(1)])\n","    ndcg = dcg / idcg\n","    return ndcg.mean()\n","\n","\n","def recalls_and_ndcgs_for_ks(scores, labels, ks):\n","    metrics = {}\n","\n","    scores = scores\n","    labels = labels\n","    answer_count = labels.sum(1)\n","\n","    labels_float = labels.float()\n","    rank = (-scores).argsort(dim=1)\n","\n","    cut = rank\n","    for k in sorted(ks, reverse=True):\n","        cut = cut[:, :k]\n","        hits = labels_float.gather(1, cut)\n","        metrics['Recall@%d' % k] = \\\n","            (hits.sum(1) / torch.min(torch.Tensor([k]).to(\n","                labels.device), labels.sum(1).float())).mean().cpu().item()\n","\n","        position = torch.arange(2, 2+k)\n","        weights = 1 / torch.log2(position.float())\n","        dcg = (hits * weights.to(hits.device)).sum(1)\n","        idcg = torch.Tensor([weights[:min(int(n), k)].sum()\n","                             for n in answer_count]).to(dcg.device)\n","        ndcg = (dcg / idcg).mean()\n","        metrics['NDCG@%d' % k] = ndcg.cpu().item()\n","    return metrics\n","\n","\n","def em_and_agreement(scores_rank, labels_rank):\n","    em = (scores_rank == labels_rank).float().mean()\n","    temp = np.hstack((scores_rank.numpy(), labels_rank.numpy()))\n","    temp = np.sort(temp, axis=1)\n","    agreement = np.mean(np.sum(temp[:, 1:] == temp[:, :-1], axis=1))\n","    return em, agreement\n","\n","\n","def kl_agreements_and_intersctions_for_ks(scores, soft_labels, ks, k_kl=100):\n","    metrics = {}\n","    scores = scores.cpu()\n","    soft_labels = soft_labels.cpu()\n","    scores_rank = (-scores).argsort(dim=1)\n","    labels_rank = (-soft_labels).argsort(dim=1)\n","\n","    top_kl_scores = F.log_softmax(scores.gather(1, labels_rank[:, :k_kl]), dim=-1)\n","    top_kl_labels = F.softmax(soft_labels.gather(1, labels_rank[:, :k_kl]), dim=-1)\n","    kl = F.kl_div(top_kl_scores, top_kl_labels, reduction='batchmean')\n","    metrics['KL-Div'] = kl.item()\n","    for k in sorted(ks, reverse=True):\n","        em, agreement = em_and_agreement(scores_rank[:, :k], labels_rank[:, :k])\n","        metrics['EM@%d' % k] = em.item()\n","        metrics['Agr@%d' % k] = (agreement / k).item()\n","    return metrics\n","\n","\n","class AverageMeterSet(object):\n","    def __init__(self, meters=None):\n","        self.meters = meters if meters else {}\n","\n","    def __getitem__(self, key):\n","        if key not in self.meters:\n","            meter = AverageMeter()\n","            meter.update(0)\n","            return meter\n","        return self.meters[key]\n","\n","    def update(self, name, value, n=1):\n","        if name not in self.meters:\n","            self.meters[name] = AverageMeter()\n","        self.meters[name].update(value, n)\n","\n","    def reset(self):\n","        for meter in self.meters.values():\n","            meter.reset()\n","\n","    def values(self, format_string='{}'):\n","        return {format_string.format(name): meter.val for name, meter in self.meters.items()}\n","\n","    def averages(self, format_string='{}'):\n","        return {format_string.format(name): meter.avg for name, meter in self.meters.items()}\n","\n","    def sums(self, format_string='{}'):\n","        return {format_string.format(name): meter.sum for name, meter in self.meters.items()}\n","\n","    def counts(self, format_string='{}'):\n","        return {format_string.format(name): meter.count for name, meter in self.meters.items()}\n","\n","\n","class AverageMeter(object):\n","    \"\"\"Computes and stores the average and current value\"\"\"\n","\n","    def __init__(self):\n","        self.val = 0\n","        self.avg = 0\n","        self.sum = 0\n","        self.count = 0\n","\n","    def reset(self):\n","        self.val = 0\n","        self.avg = 0\n","        self.sum = 0\n","        self.count = 0\n","\n","    def update(self, val, n=1):\n","        self.val = val\n","        self.sum += val\n","        self.count += n\n","        self.avg = self.sum / self.count\n","\n","    def __format__(self, format):\n","        return \"{self.val:{format}} ({self.avg:{format}})\".format(self=self, format=format)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"XFZx45xjt8YY","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1631875337607,"user_tz":-330,"elapsed":19238,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}},"outputId":"b6b99e36-75f5-4140-e796-468d6f406500"},"source":["!pip install faiss-cpu --no-cache\n","!apt-get install libomp-dev"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting faiss-cpu\n","  Downloading faiss_cpu-1.7.1.post2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (8.4 MB)\n","     |████████████████████████████████| 8.4 MB 2.0 MB/s \n","\u001b[?25hInstalling collected packages: faiss-cpu\n","Successfully installed faiss-cpu-1.7.1.post2\n","Reading package lists... Done\n","Building dependency tree       \n","Reading state information... Done\n","The following additional packages will be installed:\n","  libomp5\n","Suggested packages:\n","  libomp-doc\n","The following NEW packages will be installed:\n","  libomp-dev libomp5\n","0 upgraded, 2 newly installed, 0 to remove and 40 not upgraded.\n","Need to get 239 kB of archives.\n","After this operation, 804 kB of additional disk space will be used.\n","Get:1 http://archive.ubuntu.com/ubuntu bionic/universe amd64 libomp5 amd64 5.0.1-1 [234 kB]\n","Get:2 http://archive.ubuntu.com/ubuntu bionic/universe amd64 libomp-dev amd64 5.0.1-1 [5,088 B]\n","Fetched 239 kB in 1s (361 kB/s)\n","Selecting previously unselected package libomp5:amd64.\n","(Reading database ... 148492 files and directories currently installed.)\n","Preparing to unpack .../libomp5_5.0.1-1_amd64.deb ...\n","Unpacking libomp5:amd64 (5.0.1-1) ...\n","Selecting previously unselected package libomp-dev.\n","Preparing to unpack .../libomp-dev_5.0.1-1_amd64.deb ...\n","Unpacking libomp-dev (5.0.1-1) ...\n","Setting up libomp5:amd64 (5.0.1-1) ...\n","Setting up libomp-dev (5.0.1-1) ...\n","Processing triggers for libc-bin (2.27-3ubuntu1.2) ...\n","/sbin/ldconfig.real: /usr/local/lib/python3.7/dist-packages/ideep4py/lib/libmkldnn.so.0 is not a symbolic link\n","\n"]}]},{"cell_type":"code","metadata":{"id":"IMszuWPHtQ4T"},"source":["# from config import STATE_DICT_KEY, OPTIMIZER_STATE_DICT_KEY\n","# from .utils import *\n","# from .loggers import *\n","\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.nn.functional as F\n","from torch.optim.lr_scheduler import LambdaLR\n","from torch.utils.tensorboard import SummaryWriter\n","from tqdm import tqdm\n","\n","import json\n","import faiss\n","import numpy as np\n","from abc import *\n","from pathlib import Path\n","\n","\n","class BERTTrainer(metaclass=ABCMeta):\n","    def __init__(self, args, model, train_loader, val_loader, test_loader, export_root):\n","        self.args = args\n","        self.device = args.device\n","        self.model = model.to(self.device)\n","        self.is_parallel = args.num_gpu > 1\n","        if self.is_parallel:\n","            self.model = nn.DataParallel(self.model)\n","\n","        self.num_epochs = args.num_epochs\n","        self.metric_ks = args.metric_ks\n","        self.best_metric = args.best_metric\n","        self.train_loader = train_loader\n","        self.val_loader = val_loader\n","        self.test_loader = test_loader\n","        self.optimizer = self._create_optimizer()\n","        if args.enable_lr_schedule:\n","            if args.enable_lr_warmup:\n","                self.lr_scheduler = self.get_linear_schedule_with_warmup(\n","                    self.optimizer, args.warmup_steps, len(train_loader) * self.num_epochs)\n","            else:\n","                self.lr_scheduler = optim.lr_scheduler.StepLR(\n","                    self.optimizer, step_size=args.decay_step, gamma=args.gamma)\n","            \n","        self.export_root = export_root\n","        self.writer, self.train_loggers, self.val_loggers = self._create_loggers()\n","        self.logger_service = LoggerService(\n","            self.train_loggers, self.val_loggers)\n","        self.log_period_as_iter = args.log_period_as_iter\n","\n","        self.ce = nn.CrossEntropyLoss(ignore_index=0)\n","\n","    def train(self):\n","        accum_iter = 0\n","        self.validate(0, accum_iter)\n","        for epoch in range(self.num_epochs):\n","            accum_iter = self.train_one_epoch(epoch, accum_iter)\n","            self.validate(epoch, accum_iter)\n","        self.logger_service.complete({\n","            'state_dict': (self._create_state_dict()),\n","        })\n","        self.writer.close()\n","\n","    def train_one_epoch(self, epoch, accum_iter):\n","        self.model.train()\n","        average_meter_set = AverageMeterSet()\n","        tqdm_dataloader = tqdm(self.train_loader)\n","\n","        for batch_idx, batch in enumerate(tqdm_dataloader):\n","            batch_size = batch[0].size(0)\n","            batch = [x.to(self.device) for x in batch]\n","\n","            self.optimizer.zero_grad()\n","            loss = self.calculate_loss(batch)\n","            loss.backward()\n","            self.clip_gradients(5)\n","            self.optimizer.step()\n","            if self.args.enable_lr_schedule:\n","                self.lr_scheduler.step()\n","\n","            average_meter_set.update('loss', loss.item())\n","            tqdm_dataloader.set_description(\n","                'Epoch {}, loss {:.3f} '.format(epoch+1, average_meter_set['loss'].avg))\n","\n","            accum_iter += batch_size\n","\n","            if self._needs_to_log(accum_iter):\n","                tqdm_dataloader.set_description('Logging to Tensorboard')\n","                log_data = {\n","                    'state_dict': (self._create_state_dict()),\n","                    'epoch': epoch + 1,\n","                    'accum_iter': accum_iter,\n","                }\n","                log_data.update(average_meter_set.averages())\n","                self.logger_service.log_train(log_data)\n","\n","        return accum_iter\n","\n","    def validate(self, epoch, accum_iter):\n","        self.model.eval()\n","\n","        average_meter_set = AverageMeterSet()\n","\n","        with torch.no_grad():\n","            tqdm_dataloader = tqdm(self.val_loader)\n","            for batch_idx, batch in enumerate(tqdm_dataloader):\n","                batch = [x.to(self.device) for x in batch]\n","\n","                metrics = self.calculate_metrics(batch)\n","                self._update_meter_set(average_meter_set, metrics)\n","                self._update_dataloader_metrics(\n","                    tqdm_dataloader, average_meter_set)\n","\n","            log_data = {\n","                'state_dict': (self._create_state_dict()),\n","                'epoch': epoch+1,\n","                'accum_iter': accum_iter,\n","            }\n","            log_data.update(average_meter_set.averages())\n","            self.logger_service.log_val(log_data)\n","\n","    def test(self):\n","        best_model_dict = torch.load(os.path.join(\n","            self.export_root, 'models', 'best_acc_model.pth')).get(STATE_DICT_KEY)\n","        self.model.load_state_dict(best_model_dict)\n","        self.model.eval()\n","\n","        average_meter_set = AverageMeterSet()\n","\n","        all_scores = []\n","        average_scores = []\n","        with torch.no_grad():\n","            tqdm_dataloader = tqdm(self.test_loader)\n","            for batch_idx, batch in enumerate(tqdm_dataloader):\n","                batch = [x.to(self.device) for x in batch]\n","                metrics = self.calculate_metrics(batch)\n","                \n","                # seqs, candidates, labels = batch\n","                # scores = self.model(seqs)\n","                # scores = scores[:, -1, :]\n","                # scores_sorted, indices = torch.sort(scores, dim=-1, descending=True)\n","                # all_scores += scores_sorted[:, :100].cpu().numpy().tolist()\n","                # average_scores += scores_sorted.cpu().numpy().tolist()\n","                # scores = scores.gather(1, candidates)\n","                # metrics = recalls_and_ndcgs_for_ks(scores, labels, self.metric_ks)\n","\n","                self._update_meter_set(average_meter_set, metrics)\n","                self._update_dataloader_metrics(\n","                    tqdm_dataloader, average_meter_set)\n","\n","            average_metrics = average_meter_set.averages()\n","            with open(os.path.join(self.export_root, 'logs', 'test_metrics.json'), 'w') as f:\n","                json.dump(average_metrics, f, indent=4)\n","        \n","        return average_metrics\n","\n","    def calculate_loss(self, batch):\n","        seqs, labels = batch\n","        logits = self.model(seqs)\n","\n","        logits = logits.view(-1, logits.size(-1))\n","        labels = labels.view(-1)\n","        loss = self.ce(logits, labels)\n","        return loss\n","\n","    def calculate_metrics(self, batch):\n","        seqs, candidates, labels = batch\n","\n","        scores = self.model(seqs)\n","        scores = scores[:, -1, :]\n","        scores = scores.gather(1, candidates)\n","\n","        metrics = recalls_and_ndcgs_for_ks(scores, labels, self.metric_ks)\n","        return metrics\n","\n","    def clip_gradients(self, limit=5):\n","        for p in self.model.parameters():\n","            nn.utils.clip_grad_norm_(p, 5)\n","\n","    def _update_meter_set(self, meter_set, metrics):\n","        for k, v in metrics.items():\n","            meter_set.update(k, v)\n","\n","    def _update_dataloader_metrics(self, tqdm_dataloader, meter_set):\n","        description_metrics = ['NDCG@%d' % k for k in self.metric_ks[:3]\n","                               ] + ['Recall@%d' % k for k in self.metric_ks[:3]]\n","        description = 'Eval: ' + \\\n","            ', '.join(s + ' {:.3f}' for s in description_metrics)\n","        description = description.replace('NDCG', 'N').replace('Recall', 'R')\n","        description = description.format(\n","            *(meter_set[k].avg for k in description_metrics))\n","        tqdm_dataloader.set_description(description)\n","\n","    def _create_optimizer(self):\n","        args = self.args\n","        param_optimizer = list(self.model.named_parameters())\n","        no_decay = ['bias', 'layer_norm']\n","        optimizer_grouped_parameters = [\n","            {\n","                'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n","                'weight_decay': args.weight_decay,\n","            },\n","            {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0},\n","        ]\n","        if args.optimizer.lower() == 'adamw':\n","            return optim.AdamW(optimizer_grouped_parameters, lr=args.lr, eps=args.adam_epsilon)\n","        elif args.optimizer.lower() == 'adam':\n","            return optim.Adam(optimizer_grouped_parameters, lr=args.lr, weight_decay=args.weight_decay)\n","        elif args.optimizer.lower() == 'sgd':\n","            return optim.SGD(optimizer_grouped_parameters, lr=args.lr, weight_decay=args.weight_decay, momentum=args.momentum)\n","        else:\n","            raise ValueError\n","\n","    def get_linear_schedule_with_warmup(self, optimizer, num_warmup_steps, num_training_steps, last_epoch=-1):\n","        # based on hugging face get_linear_schedule_with_warmup\n","        def lr_lambda(current_step: int):\n","            if current_step < num_warmup_steps:\n","                return float(current_step) / float(max(1, num_warmup_steps))\n","            return max(\n","                0.0, float(num_training_steps - current_step) / float(max(1, num_training_steps - num_warmup_steps))\n","            )\n","\n","        return LambdaLR(optimizer, lr_lambda, last_epoch)\n","\n","    def _create_loggers(self):\n","        root = Path(self.export_root)\n","        writer = SummaryWriter(root.joinpath('logs'))\n","        model_checkpoint = root.joinpath('models')\n","\n","        train_loggers = [\n","            MetricGraphPrinter(writer, key='epoch',\n","                               graph_name='Epoch', group_name='Train'),\n","            MetricGraphPrinter(writer, key='loss',\n","                               graph_name='Loss', group_name='Train'),\n","        ]\n","\n","        val_loggers = []\n","        for k in self.metric_ks:\n","            val_loggers.append(\n","                MetricGraphPrinter(writer, key='NDCG@%d' % k, graph_name='NDCG@%d' % k, group_name='Validation'))\n","            val_loggers.append(\n","                MetricGraphPrinter(writer, key='Recall@%d' % k, graph_name='Recall@%d' % k, group_name='Validation'))\n","        val_loggers.append(RecentModelLogger(model_checkpoint))\n","        val_loggers.append(BestModelLogger(\n","            model_checkpoint, metric_key=self.best_metric))\n","        return writer, train_loggers, val_loggers\n","\n","    def _create_state_dict(self):\n","        return {\n","            STATE_DICT_KEY: self.model.module.state_dict() if self.is_parallel else self.model.state_dict(),\n","            OPTIMIZER_STATE_DICT_KEY: self.optimizer.state_dict(),\n","        }\n","\n","    def _needs_to_log(self, accum_iter):\n","        return accum_iter % self.log_period_as_iter < self.args.train_batch_size and accum_iter != 0"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ImYTqhiQtMEV"},"source":["## Run"]},{"cell_type":"code","metadata":{"id":"LMcGCoJbtGrh"},"source":["if args.model_code == 'bert':\n","    trainer = BERTTrainer(args, model, train_loader, val_loader, test_loader, export_root)\n","if args.model_code == 'sas':\n","    trainer = SASTrainer(args, model, train_loader, val_loader, test_loader, export_root)\n","eli f args.model_code == 'narm':\n","    args.num_epochs = 100\n","    trainer = RNNTrainer(args, model, train_loader, val_loader, test_loader, export_root)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MIwZM-5GtS5f","outputId":"7503f208-7976-409c-8813-91e9b07286d5"},"source":["trainer.train()"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Eval: N@1 0.009, N@5 0.025, N@10 0.040, R@1 0.009, R@5 0.043, R@10 0.091: 100%|██████████| 48/48 [00:51<00:00,  1.07s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Update Best NDCG@10 Model at 1\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 1, loss 8.044 : 100%|██████████| 68/68 [03:56<00:00,  3.47s/it]\n","Eval: N@1 0.023, N@5 0.066, N@10 0.095, R@1 0.023, R@5 0.110, R@10 0.199: 100%|██████████| 48/48 [00:51<00:00,  1.08s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Update Best NDCG@10 Model at 1\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 2, loss 7.722 : 100%|██████████| 68/68 [03:52<00:00,  3.42s/it]\n","Eval: N@1 0.042, N@5 0.105, N@10 0.143, R@1 0.042, R@5 0.169, R@10 0.287: 100%|██████████| 48/48 [00:51<00:00,  1.08s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Update Best NDCG@10 Model at 2\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 3, loss 7.426 : 100%|██████████| 68/68 [03:52<00:00,  3.42s/it]\n","Eval: N@1 0.056, N@5 0.125, N@10 0.165, R@1 0.056, R@5 0.194, R@10 0.319: 100%|██████████| 48/48 [00:51<00:00,  1.08s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Update Best NDCG@10 Model at 3\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 4, loss 7.237 : 100%|██████████| 68/68 [03:52<00:00,  3.42s/it]\n","Eval: N@1 0.064, N@5 0.148, N@10 0.191, R@1 0.064, R@5 0.231, R@10 0.364: 100%|██████████| 48/48 [00:51<00:00,  1.07s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Update Best NDCG@10 Model at 4\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 5, loss 7.127 : 100%|██████████| 68/68 [03:52<00:00,  3.42s/it]\n","Eval: N@1 0.076, N@5 0.165, N@10 0.211, R@1 0.076, R@5 0.254, R@10 0.396: 100%|██████████| 48/48 [00:51<00:00,  1.07s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Update Best NDCG@10 Model at 5\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 6, loss 7.088 :  85%|████████▌ | 58/68 [03:23<00:34,  3.49s/it]"]}]},{"cell_type":"code","metadata":{"id":"NzxzWNf4v8sk"},"source":["trainer.test()"],"execution_count":null,"outputs":[]}]}