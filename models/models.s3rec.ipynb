{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp models.s3rec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# S3Rec\n",
    "> Self-Supervised Learning for Sequential Recommendation with Mutual Information Maximization.\n",
    "\n",
    "S3Rec is the first work to incorporate self-supervised learning in sequential recommendation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev.showdoc import *\n",
    "from fastcore.nb_imports import *\n",
    "from fastcore.test import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from recohut.models.layers.encoding import Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class S3Rec(nn.Module):\n",
    "    \"\"\"\n",
    "    References:\n",
    "        1. https://github.com/RecoHut-Stanzas/STOSA/blob/ee14e2eabcc60922eb52cc7d3231df4954d9ff16/models.py#L7\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 item_size,\n",
    "                 hidden_size,\n",
    "                 attribute_size,\n",
    "                 max_seq_length,\n",
    "                 mask_id,\n",
    "                 num_attention_heads,\n",
    "                 num_hidden_layers=2,\n",
    "                 hidden_dropout_prob=0.2,\n",
    "                 attention_probs_dropout_prob=0.2,\n",
    "                 hidden_act='gelu',\n",
    "                 initializer_range=0.02):\n",
    "        super().__init__()\n",
    "        self.item_size = item_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.attribute_size = attribute_size\n",
    "        self.max_seq_length = max_seq_length\n",
    "        self.mask_id = mask_id\n",
    "        self.num_attention_heads = num_attention_heads\n",
    "        self.num_hidden_layers = num_hidden_layers\n",
    "        self.hidden_dropout_prob = hidden_dropout_prob\n",
    "        self.attention_probs_dropout_prob = attention_probs_dropout_prob\n",
    "        self.hidden_act = hidden_act\n",
    "        self.initializer_range = initializer_range\n",
    "\n",
    "        self.item_embeddings = nn.Embedding(item_size, hidden_size, padding_idx=0)\n",
    "        self.attribute_embeddings = nn.Embedding(attribute_size, hidden_size, padding_idx=0)\n",
    "        self.position_embeddings = nn.Embedding(max_seq_length, hidden_size)\n",
    "        self.item_encoder = Encoder(hidden_size, hidden_act, num_attention_heads, \n",
    "                                    hidden_dropout_prob, attention_probs_dropout_prob, \n",
    "                                    num_hidden_layers)\n",
    "        self.layernorm = nn.LayerNorm(hidden_size, eps=1e-12)\n",
    "        self.dropout = nn.Dropout(hidden_dropout_prob)\n",
    "\n",
    "        # add unique dense layer for 4 losses respectively\n",
    "        self.aap_norm = nn.Linear(hidden_size, hidden_size)\n",
    "        self.mip_norm = nn.Linear(hidden_size, hidden_size)\n",
    "        self.map_norm = nn.Linear(hidden_size, hidden_size)\n",
    "        self.sp_norm = nn.Linear(hidden_size, hidden_size)\n",
    "        self.criterion = nn.BCELoss(reduction='none')\n",
    "        self.apply(self.init_weights)\n",
    "\n",
    "    # AAP\n",
    "    def associated_attribute_prediction(self, sequence_output, attribute_embedding):\n",
    "        '''\n",
    "        :param sequence_output: [B L H]\n",
    "        :param attribute_embedding: [arribute_num H]\n",
    "        :return: scores [B*L tag_num]\n",
    "        '''\n",
    "        sequence_output = self.aap_norm(sequence_output) # [B L H]\n",
    "        sequence_output = sequence_output.view([-1, self.hidden_size, 1]) # [B*L H 1]\n",
    "        # [tag_num H] [B*L H 1] -> [B*L tag_num 1]\n",
    "        score = torch.matmul(attribute_embedding, sequence_output)\n",
    "        return torch.sigmoid(score.squeeze(-1)) # [B*L tag_num]\n",
    "\n",
    "    # MIP sample neg items\n",
    "    def masked_item_prediction(self, sequence_output, target_item):\n",
    "        '''\n",
    "        :param sequence_output: [B L H]\n",
    "        :param target_item: [B L H]\n",
    "        :return: scores [B*L]\n",
    "        '''\n",
    "        sequence_output = self.mip_norm(sequence_output.view([-1,self.hidden_size])) # [B*L H]\n",
    "        target_item = target_item.view([-1,self.hidden_size]) # [B*L H]\n",
    "        score = torch.mul(sequence_output, target_item) # [B*L H]\n",
    "        return torch.sigmoid(torch.sum(score, -1)) # [B*L]\n",
    "\n",
    "    # MAP\n",
    "    def masked_attribute_prediction(self, sequence_output, attribute_embedding):\n",
    "        sequence_output = self.map_norm(sequence_output)  # [B L H]\n",
    "        sequence_output = sequence_output.view([-1, self.hidden_size, 1])  # [B*L H 1]\n",
    "        # [tag_num H] [B*L H 1] -> [B*L tag_num 1]\n",
    "        score = torch.matmul(attribute_embedding, sequence_output)\n",
    "        return torch.sigmoid(score.squeeze(-1)) # [B*L tag_num]\n",
    "\n",
    "    # SP sample neg segment\n",
    "    def segment_prediction(self, context, segment):\n",
    "        '''\n",
    "        :param context: [B H]\n",
    "        :param segment: [B H]\n",
    "        :return:\n",
    "        '''\n",
    "        context = self.sp_norm(context)\n",
    "        score = torch.mul(context, segment) # [B H]\n",
    "        return torch.sigmoid(torch.sum(score, dim=-1)) # [B]\n",
    "\n",
    "    #\n",
    "    def add_position_embedding(self, sequence):\n",
    "\n",
    "        seq_length = sequence.size(1)\n",
    "        position_ids = torch.arange(seq_length, dtype=torch.long, device=sequence.device)\n",
    "        position_ids = position_ids.unsqueeze(0).expand_as(sequence)\n",
    "        item_embeddings = self.item_embeddings(sequence)\n",
    "        position_embeddings = self.position_embeddings(position_ids)\n",
    "        sequence_emb = item_embeddings + position_embeddings\n",
    "        sequence_emb = self.layernorm(sequence_emb)\n",
    "        sequence_emb = self.dropout(sequence_emb)\n",
    "\n",
    "        return sequence_emb\n",
    "\n",
    "    def pretrain(self, attributes, masked_item_sequence, pos_items, neg_items,\n",
    "                  masked_segment_sequence, pos_segment, neg_segment):\n",
    "\n",
    "        # Encode masked sequence\n",
    "        sequence_emb = self.add_position_embedding(masked_item_sequence)\n",
    "        sequence_mask = (masked_item_sequence == 0).float() * -1e8\n",
    "        sequence_mask = torch.unsqueeze(torch.unsqueeze(sequence_mask, 1), 1)\n",
    "\n",
    "        encoded_layers = self.item_encoder(sequence_emb,\n",
    "                                          sequence_mask,\n",
    "                                          output_all_encoded_layers=True)\n",
    "        # [B L H]\n",
    "        sequence_output = encoded_layers[-1]\n",
    "\n",
    "        attribute_embeddings = self.attribute_embeddings.weight\n",
    "        # AAP\n",
    "        aap_score = self.associated_attribute_prediction(sequence_output, attribute_embeddings)\n",
    "        aap_loss = self.criterion(aap_score, attributes.view(-1, self.attribute_size).float())\n",
    "        # only compute loss at non-masked position\n",
    "        aap_mask = (masked_item_sequence != self.mask_id).float() * \\\n",
    "                         (masked_item_sequence != 0).float()\n",
    "        aap_loss = torch.sum(aap_loss * aap_mask.flatten().unsqueeze(-1))\n",
    "\n",
    "        # MIP\n",
    "        pos_item_embs = self.item_embeddings(pos_items)\n",
    "        neg_item_embs = self.item_embeddings(neg_items)\n",
    "        pos_score = self.masked_item_prediction(sequence_output, pos_item_embs)\n",
    "        neg_score = self.masked_item_prediction(sequence_output, neg_item_embs)\n",
    "        mip_distance = torch.sigmoid(pos_score - neg_score)\n",
    "        mip_loss = self.criterion(mip_distance, torch.ones_like(mip_distance, dtype=torch.float32))\n",
    "        mip_mask = (masked_item_sequence == self.mask_id).float()\n",
    "        mip_loss = torch.sum(mip_loss * mip_mask.flatten())\n",
    "\n",
    "        # MAP\n",
    "        map_score = self.masked_attribute_prediction(sequence_output, attribute_embeddings)\n",
    "        map_loss = self.criterion(map_score, attributes.view(-1, self.attribute_size).float())\n",
    "        map_mask = (masked_item_sequence == self.mask_id).float()\n",
    "        map_loss = torch.sum(map_loss * map_mask.flatten().unsqueeze(-1))\n",
    "\n",
    "        # SP\n",
    "        # segment context\n",
    "        segment_context = self.add_position_embedding(masked_segment_sequence)\n",
    "        segment_mask = (masked_segment_sequence == 0).float() * -1e8\n",
    "        segment_mask = torch.unsqueeze(torch.unsqueeze(segment_mask, 1), 1)\n",
    "        segment_encoded_layers = self.item_encoder(segment_context,\n",
    "                                               segment_mask,\n",
    "                                               output_all_encoded_layers=True)\n",
    "\n",
    "        # take the last position hidden as the context\n",
    "        segment_context = segment_encoded_layers[-1][:, -1, :]# [B H]\n",
    "        # pos_segment\n",
    "        pos_segment_emb = self.add_position_embedding(pos_segment)\n",
    "        pos_segment_mask = (pos_segment == 0).float() * -1e8\n",
    "        pos_segment_mask = torch.unsqueeze(torch.unsqueeze(pos_segment_mask, 1), 1)\n",
    "        pos_segment_encoded_layers = self.item_encoder(pos_segment_emb,\n",
    "                                                   pos_segment_mask,\n",
    "                                                   output_all_encoded_layers=True)\n",
    "        pos_segment_emb = pos_segment_encoded_layers[-1][:, -1, :]\n",
    "\n",
    "        # neg_segment\n",
    "        neg_segment_emb = self.add_position_embedding(neg_segment)\n",
    "        neg_segment_mask = (neg_segment == 0).float() * -1e8\n",
    "        neg_segment_mask = torch.unsqueeze(torch.unsqueeze(neg_segment_mask, 1), 1)\n",
    "        neg_segment_encoded_layers = self.item_encoder(neg_segment_emb,\n",
    "                                                       neg_segment_mask,\n",
    "                                                       output_all_encoded_layers=True)\n",
    "        neg_segment_emb = neg_segment_encoded_layers[-1][:, -1, :] # [B H]\n",
    "\n",
    "        pos_segment_score = self.segment_prediction(segment_context, pos_segment_emb)\n",
    "        neg_segment_score = self.segment_prediction(segment_context, neg_segment_emb)\n",
    "\n",
    "        sp_distance = torch.sigmoid(pos_segment_score - neg_segment_score)\n",
    "\n",
    "        sp_loss = torch.sum(self.criterion(sp_distance,\n",
    "                                           torch.ones_like(sp_distance, dtype=torch.float32)))\n",
    "\n",
    "        return aap_loss, mip_loss, map_loss, sp_loss\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "\n",
    "        attention_mask = (input_ids > 0).long()\n",
    "        extended_attention_mask = attention_mask.unsqueeze(1).unsqueeze(2) # torch.int64\n",
    "        max_len = attention_mask.size(-1)\n",
    "        attn_shape = (1, max_len, max_len)\n",
    "        subsequent_mask = torch.triu(torch.ones(attn_shape), diagonal=1) # torch.uint8\n",
    "        subsequent_mask = (subsequent_mask == 0).unsqueeze(1)\n",
    "        subsequent_mask = subsequent_mask.long()\n",
    "\n",
    "        extended_attention_mask = extended_attention_mask * subsequent_mask\n",
    "        extended_attention_mask = extended_attention_mask.to(dtype=next(self.parameters()).dtype) # fp16 compatibility\n",
    "        extended_attention_mask = (1.0 - extended_attention_mask) * -10000.0\n",
    "\n",
    "        sequence_emb = self.add_position_embedding(input_ids)\n",
    "\n",
    "        item_encoded_layers = self.item_encoder(sequence_emb,\n",
    "                                                extended_attention_mask,\n",
    "                                                output_all_encoded_layers=True)\n",
    "\n",
    "        sequence_output = item_encoded_layers[-1]\n",
    "        return sequence_output\n",
    "\n",
    "    def init_weights(self, module):\n",
    "        \"\"\" Initialize the weights.\n",
    "        \"\"\"\n",
    "        if isinstance(module, (nn.Linear, nn.Embedding)):\n",
    "            module.weight.data.normal_(mean=0.0, std=self.initializer_range)\n",
    "        elif isinstance(module, nn.LayerNorm):\n",
    "            module.bias.data.zero_()\n",
    "            module.weight.data.fill_(1.0)\n",
    "        if isinstance(module, nn.Linear) and module.bias is not None:\n",
    "            module.bias.data.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_size = 10\n",
    "hidden_size = 10\n",
    "attribute_size = 5\n",
    "max_seq_length = 20\n",
    "hidden_dropout_prob = 0.2\n",
    "mask_id = 20\n",
    "initializer_range = 0.02\n",
    "num_hidden_layers = 2\n",
    "attention_probs_dropout_prob = 0.2\n",
    "hidden_act = 'gelu'\n",
    "num_attention_heads = 2\n",
    "\n",
    "model = S3RecModel(item_size, hidden_size, attribute_size, max_seq_length, mask_id, \n",
    "                   num_attention_heads, num_hidden_layers, hidden_dropout_prob, \n",
    "                   attention_probs_dropout_prob, hidden_act, initializer_range)\n",
    "\n",
    "x = torch.randint(0, 5, (item_size, hidden_size))\n",
    "\n",
    "output = model.forward(x)\n",
    "output_shapes = [list(x.shape) for x in [j for sub in output for j in sub]]\n",
    "\n",
    "test_eq(output_shapes[0], [10, 10])\n",
    "test_eq(output_shapes[11], [2, 10, 10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **References**\n",
    "> - Kun Zhou, Hui Wang, Wayne Xin Zhao, Yutao Zhu, Sirui Wang, Fuzheng Zhang, Zhongyuan Wang, Ji-Rong Wen (2020). S^3-Rec: Self-Supervised Learning for Sequential Recommendation with Mutual Information Maximization. [arXiv:2008.07873 cs.IR](https://arxiv.org/abs/2008.07873)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Author: Sparsh A.\n",
      "\n",
      "Last updated: 2022-01-22 16:49:16\n",
      "\n",
      "recohut: 0.0.11\n",
      "\n",
      "Compiler    : GCC 7.5.0\n",
      "OS          : Linux\n",
      "Release     : 5.4.144+\n",
      "Machine     : x86_64\n",
      "Processor   : x86_64\n",
      "CPU cores   : 2\n",
      "Architecture: 64bit\n",
      "\n",
      "torch    : 1.10.0+cu111\n",
      "IPython  : 5.5.0\n",
      "watermark: 2.3.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "%reload_ext watermark\n",
    "%watermark -a \"Sparsh A.\" -m -iv -u -t -d -p recohut"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
