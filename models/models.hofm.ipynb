{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp models.hofm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HOFM\n",
    "> A pytorch implementation of Higher-Order Factorization Machines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev.showdoc import *\n",
    "from fastcore.nb_imports import *\n",
    "from fastcore.test import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "from itertools import combinations\n",
    "\n",
    "from recohut.models.layers.embedding import EmbeddingLayer\n",
    "from recohut.models.layers.interaction import InnerProductLayer\n",
    "from recohut.models.layers.common import LR_Layer\n",
    "\n",
    "from recohut.models.bases.ctr import CTRModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class HOFM(CTRModel):\n",
    "    def __init__(self, \n",
    "                 feature_map, \n",
    "                 model_id=\"HOFM\",\n",
    "                 task=\"binary_classification\",\n",
    "                 learning_rate=1e-3, \n",
    "                 embedding_initializer=\"torch.nn.init.normal_(std=1e-4)\",\n",
    "                 order=3,\n",
    "                 embedding_dim=10,\n",
    "                 reuse_embedding=False,\n",
    "                 **kwargs):\n",
    "        super(HOFM, self).__init__(feature_map, \n",
    "                                           model_id=model_id,\n",
    "                                           **kwargs)\n",
    "        self.order = order\n",
    "        assert order >= 2, \"order >= 2 is required in HOFM!\"\n",
    "        self.reuse_embedding = reuse_embedding\n",
    "        if reuse_embedding:\n",
    "            assert isinstance(embedding_dim, int), \"embedding_dim should be an integer when reuse_embedding=True.\"\n",
    "            self.embedding_layer = EmbeddingLayer(feature_map, embedding_dim)\n",
    "        else:\n",
    "            if not isinstance(embedding_dim, list):\n",
    "                embedding_dim = [embedding_dim] * (order - 1)\n",
    "            self.embedding_layers = nn.ModuleList([EmbeddingLayer(feature_map, embedding_dim[i]) \\\n",
    "                                                   for i in range(order - 1)])\n",
    "        self.inner_product_layer = InnerProductLayer(feature_map.num_fields)\n",
    "        self.lr_layer = LR_Layer(feature_map, use_bias=True)\n",
    "        self.output_activation = self.get_final_activation(task)\n",
    "        self.field_conjunction_dict = dict()\n",
    "        for order_i in range(3, self.order + 1):\n",
    "            order_i_conjunction = zip(*list(combinations(range(feature_map.num_fields), order_i)))\n",
    "            for k, field_index in enumerate(order_i_conjunction):\n",
    "                self.field_conjunction_dict[(order_i, k)] = torch.LongTensor(field_index)\n",
    "        self.init_weights(embedding_initializer=embedding_initializer)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        y_pred = self.lr_layer(inputs)\n",
    "        if self.reuse_embedding:\n",
    "            feature_emb = self.embedding_layer(inputs)\n",
    "        for i in range(2, self.order + 1):\n",
    "            order_i_out = self.high_order_interaction(feature_emb if self.reuse_embedding \\\n",
    "                                                      else self.embedding_layers[i - 2](inputs), order_i=i)\n",
    "            y_pred += order_i_out\n",
    "        if self.output_activation is not None:\n",
    "            y_pred = self.output_activation(y_pred)\n",
    "        return y_pred\n",
    "\n",
    "    def high_order_interaction(self, feature_emb, order_i):\n",
    "        if order_i == 2:\n",
    "            interaction_out = self.inner_product_layer(feature_emb)\n",
    "        elif order_i > 2:\n",
    "            index = self.field_conjunction_dict[(order_i, 0)].to(self.device)\n",
    "            hadamard_product = torch.index_select(feature_emb, 1, index)\n",
    "            for k in range(1, order_i):\n",
    "                index = self.field_conjunction_dict[(order_i, k)].to(self.device)\n",
    "                hadamard_product = hadamard_product * torch.index_select(feature_emb, 1, index)\n",
    "            interaction_out = hadamard_product.sum((1, 2)).view(-1, 1)\n",
    "        return interaction_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'model_id': 'HOFM',\n",
    "              'data_dir': '/content/data',\n",
    "              'model_root': './checkpoints/',\n",
    "              'learning_rate': 1e-3,\n",
    "              'optimizer': 'adamw',\n",
    "              'task': 'binary_classification',\n",
    "              'loss': 'binary_crossentropy',\n",
    "              'metrics': ['logloss', 'AUC'],\n",
    "              'regularizer': 0,\n",
    "              'order': 4,\n",
    "              'embedding_dim': 10,\n",
    "              'reuse_embedding': False,\n",
    "              'batch_size': 64,\n",
    "              'epochs': 3,\n",
    "              'shuffle': True,\n",
    "              'seed': 2019,\n",
    "              'use_hdf5': True,\n",
    "              'workers': 1,\n",
    "              'verbose': 0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = HOFM(ds.dataset.feature_map, **params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "\n",
      "  | Name                | Type              | Params\n",
      "----------------------------------------------------------\n",
      "0 | embedding_layers    | ModuleList        | 14.3 K\n",
      "1 | inner_product_layer | InnerProductLayer | 378   \n",
      "2 | lr_layer            | LR_Layer          | 477   \n",
      "3 | output_activation   | Sigmoid           | 0     \n",
      "----------------------------------------------------------\n",
      "14.8 K    Trainable params\n",
      "378       Non-trainable params\n",
      "15.1 K    Total params\n",
      "0.061     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0314fac5b1547d593f8a1929d560359",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "386fc95a2ae9440eaa453221df66f7be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3af172c4631482ca8ae164deb3a2c32",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{'Test Metrics': {'AUC': tensor(1.), 'logloss': tensor(0.5169)}}\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'Test Metrics': {'AUC': tensor(1.), 'logloss': tensor(0.5169)}}]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pl_trainer(model, ds, max_epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import torch\n",
    "\n",
    "from recohut.models.layers.common import FeaturesEmbedding, FeaturesLinear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class FactorizationMachine(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, reduce_sum=True):\n",
    "        super().__init__()\n",
    "        self.reduce_sum = reduce_sum\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        :param x: Float tensor of size ``(batch_size, num_fields, embed_dim)``\n",
    "        \"\"\"\n",
    "        square_of_sum = torch.sum(x, dim=1) ** 2\n",
    "        sum_of_square = torch.sum(x ** 2, dim=1)\n",
    "        ix = square_of_sum - sum_of_square\n",
    "        if self.reduce_sum:\n",
    "            ix = torch.sum(ix, dim=1, keepdim=True)\n",
    "        return 0.5 * ix\n",
    "\n",
    "class AnovaKernel(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, order, reduce_sum=True):\n",
    "        super().__init__()\n",
    "        self.order = order\n",
    "        self.reduce_sum = reduce_sum\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        :param x: Float tensor of size ``(batch_size, num_fields, embed_dim)``\n",
    "        \"\"\"\n",
    "        batch_size, num_fields, embed_dim = x.shape\n",
    "        a_prev = torch.ones((batch_size, num_fields + 1, embed_dim), dtype=torch.float).to(x.device)\n",
    "        for t in range(self.order):\n",
    "            a = torch.zeros((batch_size, num_fields + 1, embed_dim), dtype=torch.float).to(x.device)\n",
    "            a[:, t+1:, :] += x[:, t:, :] * a_prev[:, t:-1, :]\n",
    "            a = torch.cumsum(a, dim=1)\n",
    "            a_prev = a\n",
    "        if self.reduce_sum:\n",
    "            return torch.sum(a[:, -1, :], dim=-1, keepdim=True)\n",
    "        else:\n",
    "            return a[:, -1, :]\n",
    "\n",
    "class HOFM_v2(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    A pytorch implementation of Higher-Order Factorization Machines.\n",
    "    Reference:\n",
    "        M Blondel, et al. Higher-Order Factorization Machines, 2016.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, field_dims, order, embed_dim):\n",
    "        super().__init__()\n",
    "        if order < 1:\n",
    "            raise ValueError(f'invalid order: {order}')\n",
    "        self.order = order\n",
    "        self.embed_dim = embed_dim\n",
    "        self.linear = FeaturesLinear(field_dims)\n",
    "        if order >= 2:\n",
    "            self.embedding = FeaturesEmbedding(field_dims, embed_dim * (order - 1))\n",
    "            self.fm = FactorizationMachine(reduce_sum=True)\n",
    "        if order >= 3:\n",
    "            self.kernels = torch.nn.ModuleList([\n",
    "                AnovaKernel(order=i, reduce_sum=True) for i in range(3, order + 1)\n",
    "            ])\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        :param x: Long tensor of size ``(batch_size, num_fields)``\n",
    "        \"\"\"\n",
    "        y = self.linear(x).squeeze(1)\n",
    "        if self.order >= 2:\n",
    "            x = self.embedding(x)\n",
    "            x_part = x[:, :, :self.embed_dim]\n",
    "            y += self.fm(x_part).squeeze(1)\n",
    "            for i in range(self.order - 2):\n",
    "                x_part = x[:, :, (i + 1) * self.embed_dim: (i + 2) * self.embed_dim]\n",
    "                y += self.kernels[i](x_part).squeeze(1)\n",
    "        return torch.sigmoid(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **References:-**\n",
    "- M Blondel, et al. Higher-Order Factorization Machines, 2016.\n",
    "- https://github.com/rixwew/pytorch-fm/blob/master/torchfm/model/hofm.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "%reload_ext watermark\n",
    "%watermark -a \"Sparsh A.\" -m -iv -u -t -d -p recohut"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
