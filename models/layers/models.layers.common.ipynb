{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp models.layers.common"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Common Layers\n",
    "> Common layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev.showdoc import *\n",
    "from fastcore.nb_imports import *\n",
    "from fastcore.test import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#exporti\n",
    "def get_activation(activation):\n",
    "    if isinstance(activation, str):\n",
    "        if activation.lower() == \"relu\":\n",
    "            return nn.ReLU()\n",
    "        elif activation.lower() == \"sigmoid\":\n",
    "            return nn.Sigmoid()\n",
    "        elif activation.lower() == \"tanh\":\n",
    "            return nn.Tanh()\n",
    "        else:\n",
    "            return getattr(nn, activation)()\n",
    "    else:\n",
    "        return activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class FeaturesLinear(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    References:\n",
    "        1. https://github.com/rixwew/pytorch-fm/blob/master/torchfm/layer.py\n",
    "    \"\"\"\n",
    "    def __init__(self, field_dims, output_dim=1):\n",
    "        super().__init__()\n",
    "        self.fc = torch.nn.Embedding(sum(field_dims), output_dim)\n",
    "        self.bias = torch.nn.Parameter(torch.zeros((output_dim,)))\n",
    "        self.offsets = np.array((0, *np.cumsum(field_dims)[:-1]), dtype=np.long)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        :param x: Long tensor of size ``(batch_size, num_fields)``\n",
    "        \"\"\"\n",
    "        x = x + x.new_tensor(self.offsets).unsqueeze(0)\n",
    "        return torch.sum(self.fc(x), dim=1) + self.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class FeaturesEmbedding(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    References:\n",
    "        1. https://github.com/rixwew/pytorch-fm/blob/master/torchfm/layer.py\n",
    "    \"\"\"\n",
    "    def __init__(self, field_dims, embed_dim):\n",
    "        super().__init__()\n",
    "        self.embedding = torch.nn.Embedding(sum(field_dims), embed_dim)\n",
    "        self.offsets = np.array((0, *np.cumsum(field_dims)[:-1]), dtype=np.long)\n",
    "        torch.nn.init.xavier_uniform_(self.embedding.weight.data)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        :param x: Long tensor of size ``(batch_size, num_fields)``\n",
    "        \"\"\"\n",
    "        x = x + x.new_tensor(self.offsets).unsqueeze(0)\n",
    "        return self.embedding(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class MultiLayerPerceptron(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    References:\n",
    "        1. https://github.com/rixwew/pytorch-fm/blob/master/torchfm/layer.py\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim, embed_dims, dropout, output_layer=True):\n",
    "        super().__init__()\n",
    "        layers = list()\n",
    "        for embed_dim in embed_dims:\n",
    "            layers.append(torch.nn.Linear(input_dim, embed_dim))\n",
    "            layers.append(torch.nn.BatchNorm1d(embed_dim))\n",
    "            layers.append(torch.nn.ReLU())\n",
    "            layers.append(torch.nn.Dropout(p=dropout))\n",
    "            input_dim = embed_dim\n",
    "        if output_layer:\n",
    "            layers.append(torch.nn.Linear(input_dim, 1))\n",
    "        self.mlp = torch.nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        :param x: Float tensor of size ``(batch_size, embed_dim)``\n",
    "        \"\"\"\n",
    "        return self.mlp(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class MLP_Layer(nn.Module):\n",
    "    def __init__(self, \n",
    "                 input_dim, \n",
    "                 output_dim=None, \n",
    "                 hidden_units=[], \n",
    "                 hidden_activations=\"ReLU\",\n",
    "                 output_activation=None, \n",
    "                 dropout_rates=[], \n",
    "                 batch_norm=False, \n",
    "                 use_bias=True):\n",
    "        super(MLP_Layer, self).__init__()\n",
    "        dense_layers = []\n",
    "        if not isinstance(dropout_rates, list):\n",
    "            dropout_rates = [dropout_rates] * len(hidden_units)\n",
    "        if not isinstance(hidden_activations, list):\n",
    "            hidden_activations = [hidden_activations] * len(hidden_units)\n",
    "        hidden_activations = [get_activation(x) for x in hidden_activations]\n",
    "        hidden_units = [input_dim] + hidden_units\n",
    "        for idx in range(len(hidden_units) - 1):\n",
    "            dense_layers.append(nn.Linear(hidden_units[idx], hidden_units[idx + 1], bias=use_bias))\n",
    "            if batch_norm:\n",
    "                dense_layers.append(nn.BatchNorm1d(hidden_units[idx + 1]))\n",
    "            if hidden_activations[idx]:\n",
    "                dense_layers.append(hidden_activations[idx])\n",
    "            if dropout_rates[idx] > 0:\n",
    "                dense_layers.append(nn.Dropout(p=dropout_rates[idx]))\n",
    "        if output_dim is not None:\n",
    "            dense_layers.append(nn.Linear(hidden_units[-1], output_dim, bias=use_bias))\n",
    "        if output_activation is not None:\n",
    "            dense_layers.append(get_activation(output_activation))\n",
    "        self.dnn = nn.Sequential(*dense_layers) # * used to unpack list\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        return self.dnn(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class MaskedAveragePooling(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MaskedAveragePooling, self).__init__()\n",
    "\n",
    "    def forward(self, embedding_matrix):\n",
    "        sum_pooling_matrix = torch.sum(embedding_matrix, dim=1)\n",
    "        non_padding_length = (embedding_matrix != 0).sum(dim=1)\n",
    "        embedding_vec = sum_pooling_matrix / (non_padding_length.float() + 1e-16)\n",
    "        return embedding_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class MaskedSumPooling(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MaskedSumPooling, self).__init__()\n",
    "\n",
    "    def forward(self, embedding_matrix):\n",
    "        # mask by zeros\n",
    "        return torch.sum(embedding_matrix, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class KMaxPooling(nn.Module):\n",
    "    def __init__(self, k, dim):\n",
    "        super(KMaxPooling, self).__init__()\n",
    "        self.k = k\n",
    "        self.dim = dim\n",
    "\n",
    "    def forward(self, X):\n",
    "        index = X.topk(self.k, dim=self.dim)[1].sort(dim=self.dim)[0]\n",
    "        output = X.gather(self.dim, index)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shallow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "from itertools import combinations\n",
    "\n",
    "from recohut.models.layers.embedding import EmbeddingLayer\n",
    "from recohut.models.layers.interaction import InnerProductLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class LR_Layer(nn.Module):\n",
    "    def __init__(self, feature_map, output_activation=None, use_bias=True):\n",
    "        super(LR_Layer, self).__init__()\n",
    "        self.bias = nn.Parameter(torch.zeros(1), requires_grad=True) if use_bias else None\n",
    "        self.output_activation = output_activation\n",
    "        # A trick for quick one-hot encoding in LR\n",
    "        self.embedding_layer = EmbeddingLayer(feature_map, 1, use_pretrain=False)\n",
    "\n",
    "    def forward(self, X):\n",
    "        embed_weights = self.embedding_layer(X)\n",
    "        output = embed_weights.sum(dim=1)\n",
    "        if self.bias is not None:\n",
    "            output += self.bias\n",
    "        if self.output_activation is not None:\n",
    "            output = self.output_activation(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class FM_Layer(nn.Module):\n",
    "    def __init__(self, feature_map, output_activation=None, use_bias=True):\n",
    "        super(FM_Layer, self).__init__()\n",
    "        self.inner_product_layer = InnerProductLayer(feature_map.num_fields, output=\"product_sum_pooling\")\n",
    "        self.lr_layer = LR_Layer(feature_map, output_activation=None, use_bias=use_bias)\n",
    "        self.output_activation = output_activation\n",
    "\n",
    "    def forward(self, X, feature_emb):\n",
    "        lr_out = self.lr_layer(X)\n",
    "        dot_sum = self.inner_product_layer(feature_emb)\n",
    "        output = dot_sum + lr_out\n",
    "        if self.output_activation is not None:\n",
    "            output = self.output_activation(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, hidden_size, eps=1e-12):\n",
    "        \"\"\"Construct a layernorm module in the TF style (epsilon inside the square root).\n",
    "        \"\"\"\n",
    "        super(LayerNorm, self).__init__()\n",
    "        self.weight = nn.Parameter(torch.ones(hidden_size))\n",
    "        self.bias = nn.Parameter(torch.zeros(hidden_size))\n",
    "        self.variance_epsilon = eps\n",
    "\n",
    "    def forward(self, x):\n",
    "        u = x.mean(-1, keepdim=True)\n",
    "        s = (x - u).pow(2).mean(-1, keepdim=True)\n",
    "        x = (x - u) / torch.sqrt(s + self.variance_epsilon)\n",
    "        return self.weight * x + self.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = (2,3)\n",
    "layer = LayerNorm(hidden_size=hidden_size)\n",
    "\n",
    "torch.manual_seed(0)\n",
    "x = torch.rand(hidden_size)\n",
    "\n",
    "output = torch.round(layer.forward(x).detach()*1e4)/1e4\n",
    "\n",
    "expected =  torch.tensor([[0.1621,  1.1356, -1.2977],\n",
    "                          [-1.0854, -0.2424,  1.3278]])\n",
    "\n",
    "test_eq(output, expected)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **References**\n",
    "> - https://github.com/xue-pai/FuxiCTR/blob/main/fuxictr/pytorch/layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Author: Sparsh A.\n",
      "\n",
      "Last updated: 2022-01-11 12:02:41\n",
      "\n",
      "Compiler    : GCC 7.5.0\n",
      "OS          : Linux\n",
      "Release     : 5.4.144+\n",
      "Machine     : x86_64\n",
      "Processor   : x86_64\n",
      "CPU cores   : 2\n",
      "Architecture: 64bit\n",
      "\n",
      "torch     : 1.10.0+cu111\n",
      "numpy     : 1.19.5\n",
      "PIL       : 7.1.2\n",
      "IPython   : 5.5.0\n",
      "matplotlib: 3.2.2\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "%reload_ext watermark\n",
    "%watermark -a \"Sparsh A.\" -m -iv -u -t -d"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
