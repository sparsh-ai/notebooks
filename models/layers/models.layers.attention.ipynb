{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp models.layers.attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention Layers\n",
    "> Implementation of Attention modules including Multihead attention etc.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev.showdoc import *\n",
    "from fastcore.nb_imports import *\n",
    "from fastcore.test import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from recohut.utils.distances import wasserstein_distance_matmul"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TokenEmbedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class TokenEmbedding(nn.Embedding):\n",
    "    def __init__(self, vocab_size, embed_size=512):\n",
    "        super().__init__(vocab_size, embed_size, padding_idx=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PositionalEmbedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class PositionalEmbedding(nn.Module):\n",
    "    def __init__(self, max_len, d_model):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.pe = nn.Embedding(max_len+1, d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        pose = (x > 0) * (x > 0).sum(dim=-1).unsqueeze(1).repeat(1, x.size(-1))\n",
    "        pose += torch.arange(start=-(x.size(1)-1), end=1, step=1, device=x.device)\n",
    "        pose = pose * (x > 0)\n",
    "\n",
    "        return self.pe(pose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class GELU(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return 0.5 * x * (1 + torch.tanh(math.sqrt(2 / math.pi) * (x + 0.044715 * torch.pow(x, 3))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class PositionwiseFeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_ff):\n",
    "        super().__init__()\n",
    "        self.w_1 = nn.Linear(d_model, d_ff)\n",
    "        self.w_2 = nn.Linear(d_ff, d_model)\n",
    "        self.activation = GELU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.w_2(self.activation(self.w_1(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, features, eps=1e-6):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.ones(features))\n",
    "        self.bias = nn.Parameter(torch.zeros(features))\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(-1, keepdim=True)\n",
    "        std = x.std(-1, keepdim=True)\n",
    "        return self.weight * (x - mean) / (std + self.eps) + self.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class SublayerConnection(nn.Module):\n",
    "    \"\"\"layer norm and dropout (dropout and then layer norm)\n",
    "    \"\"\"\n",
    "    def __init__(self, size, dropout):\n",
    "        super().__init__()\n",
    "        self.layer_norm = LayerNorm(size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, sublayer):\n",
    "        # return x + self.dropout(sublayer(self.norm(x)))  # original implementation\n",
    "        return self.layer_norm(x + self.dropout(sublayer(x)))  # BERT4Rec implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class Attention(nn.Module):\n",
    "    def forward(self, query, key, value, mask=None, dropout=None, sas=False):\n",
    "        scores = torch.matmul(query, key.transpose(-2, -1)) \\\n",
    "            / math.sqrt(query.size(-1))\n",
    "\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, -1e9)\n",
    "\n",
    "        if sas:\n",
    "            direction_mask = torch.ones_like(scores)\n",
    "            direction_mask = torch.tril(direction_mask)\n",
    "            scores = scores.masked_fill(direction_mask == 0, -1e9)\n",
    "\n",
    "        p_attn = F.softmax(scores, dim=-1)\n",
    "\n",
    "        if dropout is not None:\n",
    "            p_attn = dropout(p_attn)\n",
    "\n",
    "        return torch.matmul(p_attn, value), p_attn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ScaledDotProductAttention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class ScaledDotProductAttention(nn.Module):\n",
    "    \"\"\" Scaled Dot-Product Attention \n",
    "    Ref: https://zhuanlan.zhihu.com/p/47812375\n",
    "    \"\"\"\n",
    "    def __init__(self, dropout_rate=0.):\n",
    "        super(ScaledDotProductAttention, self).__init__()\n",
    "        self.dropout = None\n",
    "        if dropout_rate > 0:\n",
    "            self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.softmax = nn.Softmax(dim=2)\n",
    "\n",
    "    def forward(self, W_q, W_k, W_v, scale=None, mask=None):\n",
    "        attention = torch.bmm(W_q, W_k.transpose(1, 2))\n",
    "        if scale:\n",
    "            attention = attention / scale\n",
    "        if mask:\n",
    "            attention = attention.masked_fill_(mask, -np.inf)\n",
    "        attention = self.softmax(attention)\n",
    "        if self.dropout is not None:\n",
    "            attention = self.dropout(attention)\n",
    "        output = torch.bmm(attention, W_v)\n",
    "        return output, attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MultiHeadAttention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\" Multi-head attention module \"\"\"\n",
    "\n",
    "    def __init__(self, input_dim, attention_dim=None, num_heads=1, dropout_rate=0., \n",
    "                 use_residual=True, use_scale=False, layer_norm=False, align_to=\"input\"):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        if attention_dim is None:\n",
    "            attention_dim = input_dim // num_heads\n",
    "        self.attention_dim = attention_dim\n",
    "        self.output_dim = num_heads * attention_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.use_residual = use_residual\n",
    "        self.align_to = align_to\n",
    "        self.scale = attention_dim ** 0.5 if use_scale else None\n",
    "        self.W_q = nn.Linear(input_dim, self.output_dim, bias=False)\n",
    "        self.W_k = nn.Linear(input_dim, self.output_dim, bias=False)\n",
    "        self.W_v = nn.Linear(input_dim, self.output_dim, bias=False)\n",
    "        if input_dim != self.output_dim:\n",
    "            if align_to == \"output\":\n",
    "                self.W_res = nn.Linear(input_dim, self.output_dim, bias=False)\n",
    "            elif align_to == \"input\":\n",
    "                self.W_res = nn.Linear(self.output_dim, input_dim, bias=False)\n",
    "        else:\n",
    "            self.W_res = None\n",
    "        self.dot_product_attention = ScaledDotProductAttention(dropout_rate)\n",
    "        self.layer_norm = nn.LayerNorm(self.output_dim) if layer_norm else None\n",
    "        self.dropout = nn.Dropout(dropout_rate) if dropout_rate > 0 else None\n",
    "\n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        residual = query\n",
    "        \n",
    "        # linear projection\n",
    "        query = self.W_q(query)\n",
    "        key = self.W_k(key)\n",
    "        value = self.W_v(value)\n",
    "        \n",
    "        # split by heads\n",
    "        batch_size = query.size(0)\n",
    "        query = query.view(batch_size * self.num_heads, -1, self.attention_dim)\n",
    "        key = key.view(batch_size * self.num_heads, -1, self.attention_dim)\n",
    "        value = value.view(batch_size * self.num_heads, -1, self.attention_dim)\n",
    "        if mask:\n",
    "            mask = mask.repeat(self.num_heads, 1, 1)\n",
    "        # scaled dot product attention\n",
    "        output, attention = self.dot_product_attention(query, key, value, self.scale, mask)\n",
    "        # concat heads\n",
    "        output = output.view(batch_size, -1, self.output_dim)\n",
    "        # final linear projection\n",
    "        if self.W_res is not None:\n",
    "            if self.align_to == \"output\": # AutoInt style\n",
    "                residual = self.W_res(residual)\n",
    "            elif self.align_to == \"input\": # Transformer stype\n",
    "                output = self.W_res(output)\n",
    "        if self.dropout is not None:\n",
    "            output = self.dropout(output)\n",
    "        if self.use_residual:\n",
    "            output = output + residual\n",
    "        if self.layer_norm is not None:\n",
    "            output = self.layer_norm(output)\n",
    "        output = output.relu()\n",
    "        return output, attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MultiHeadedAttention_v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class MultiHeadedAttention_v2(nn.Module):\n",
    "    def __init__(self, h, d_model, head_size=None, dropout=0.1):\n",
    "        super().__init__()\n",
    "        assert d_model % h == 0\n",
    "\n",
    "        self.h = h\n",
    "        self.d_k = d_model // h\n",
    "        if head_size is not None:\n",
    "            self.head_size = head_size\n",
    "        else:\n",
    "            self.head_size = d_model // h\n",
    "\n",
    "        self.linear_layers = nn.ModuleList(\n",
    "            [nn.Linear(d_model, self.h * self.head_size) for _ in range(3)])\n",
    "        self.attention = Attention()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        self.output_linear = nn.Linear(self.h * self.head_size, d_model)\n",
    "\n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        batch_size = query.size(0)\n",
    "\n",
    "        # 1) do all the linear projections in batch from d_model => h x d_k\n",
    "        query, key, value = [l(x).view(batch_size, -1, self.h, self.head_size).transpose(1, 2)\n",
    "                             for l, x in zip(self.linear_layers, (query, key, value))]\n",
    "        \n",
    "        # 2) apply attention on all the projected vectors in batch.\n",
    "        x, attn = self.attention(\n",
    "            query, key, value, mask=mask, dropout=self.dropout)\n",
    "\n",
    "        # 3) \"concat\" using a view and apply a final linear.\n",
    "        x = x.transpose(1, 2).contiguous().view(\n",
    "            batch_size, -1, self.h * self.head_size)\n",
    "        return self.output_linear(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MultiHeadSelfAttention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class MultiHeadSelfAttention(MultiHeadAttention):\n",
    "    def forward(self, X):\n",
    "        output, attention = super(MultiHeadSelfAttention, self).forward(X, X, X)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TransformerBlock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, hidden, attn_heads, head_size, feed_forward_hidden, dropout, attn_dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.attention = MultiHeadedAttention(\n",
    "            h=attn_heads, d_model=hidden, head_size=head_size, dropout=attn_dropout)\n",
    "        self.feed_forward = PositionwiseFeedForward(\n",
    "            d_model=hidden, d_ff=feed_forward_hidden)\n",
    "        self.input_sublayer = SublayerConnection(size=hidden, dropout=dropout)\n",
    "        self.output_sublayer = SublayerConnection(size=hidden, dropout=dropout)\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        x = self.input_sublayer(\n",
    "            x, lambda _x: self.attention.forward(_x, _x, _x, mask=mask))\n",
    "        x = self.output_sublayer(x, self.feed_forward)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SqueezeExcitationLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class SqueezeExcitationLayer(nn.Module):\n",
    "    def __init__(self, num_fields, reduction_ratio=3):\n",
    "        super(SqueezeExcitationLayer, self).__init__()\n",
    "        reduced_size = max(1, int(num_fields / reduction_ratio))\n",
    "        self.excitation = nn.Sequential(nn.Linear(num_fields, reduced_size, bias=False),\n",
    "                                        nn.ReLU(),\n",
    "                                        nn.Linear(reduced_size, num_fields, bias=False),\n",
    "                                        nn.ReLU())\n",
    "\n",
    "    def forward(self, feature_emb):\n",
    "        Z = torch.mean(feature_emb, dim=-1, out=None)\n",
    "        A = self.excitation(Z)\n",
    "        V = feature_emb * A.unsqueeze(-1)\n",
    "        return V"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SASMultiHeadedAttention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class SASMultiHeadedAttention(nn.Module):\n",
    "    def __init__(self, h, d_model, head_size=None, dropout=0.1):\n",
    "        super().__init__()\n",
    "        assert d_model % h == 0\n",
    "\n",
    "        self.h = h\n",
    "        self.d_k = d_model // h\n",
    "        if head_size is not None:\n",
    "            self.head_size = head_size\n",
    "        else:\n",
    "            self.head_size = d_model // h\n",
    "\n",
    "        self.linear_layers = nn.ModuleList(\n",
    "            [nn.Linear(d_model, self.h * self.head_size) for _ in range(3)])\n",
    "        self.attention = Attention()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        self.layer_norm = LayerNorm(d_model)\n",
    "\n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        batch_size = query.size(0)\n",
    "\n",
    "        # 1) do all the linear projections in batch from d_model => h x d_k\n",
    "        query_, key_, value_ = [l(x).view(batch_size, -1, self.h, self.head_size).transpose(1, 2)\n",
    "                             for l, x in zip(self.linear_layers, (query, key, value))]\n",
    "        \n",
    "        # 2) apply attention on all the projected vectors in batch.\n",
    "        x, attn = self.attention(\n",
    "            query_, key_, value_, mask=mask, dropout=self.dropout, sas=True)\n",
    "\n",
    "        # 3) \"concat\" using a view and apply a final linear.\n",
    "        x = x.transpose(1, 2).contiguous().view(\n",
    "            batch_size, -1, self.h * self.head_size)\n",
    "        \n",
    "        return self.layer_norm(x + query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class SASPositionwiseFeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv1d(in_channels=d_model, out_channels=d_ff, kernel_size=1)\n",
    "        self.activation = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.conv2 = nn.Conv1d(in_channels=d_ff, out_channels=d_model, kernel_size=1)\n",
    "        self.layer_norm = LayerNorm(d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_ = self.dropout(self.activation(self.conv1(x.permute(0, 2, 1))))\n",
    "        return self.layer_norm(self.dropout(self.conv2(x_)).permute(0, 2, 1) + x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SASTransformerBlock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class SASTransformerBlock(nn.Module):\n",
    "    def __init__(self, hidden, attn_heads, head_size, feed_forward_hidden, dropout, attn_dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.layer_norm = LayerNorm(hidden)\n",
    "        self.attention = SASMultiHeadedAttention(\n",
    "            h=attn_heads, d_model=hidden, head_size=head_size, dropout=attn_dropout)\n",
    "        self.feed_forward = SASPositionwiseFeedForward(\n",
    "            d_model=hidden, d_ff=feed_forward_hidden, dropout=dropout)\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        x = self.attention(self.layer_norm(x), x, x, mask)\n",
    "        x = self.feed_forward(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SelfAttention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class SelfAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    References:\n",
    "        1. https://github.com/RecoHut-Stanzas/STOSA/blob/ee14e2eabcc60922eb52cc7d3231df4954d9ff16/modules.py#L127\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 hidden_size,\n",
    "                 num_attention_heads,\n",
    "                 attention_probs_dropout_prob,\n",
    "                 hidden_dropout_prob):\n",
    "        super().__init__()\n",
    "        if hidden_size % num_attention_heads != 0:\n",
    "            raise ValueError(\n",
    "                \"The hidden size (%d) is not a multiple of the number of attention \"\n",
    "                \"heads (%d)\" % (hidden_size, num_attention_heads))\n",
    "        self.num_attention_heads = num_attention_heads\n",
    "        self.attention_head_size = int(hidden_size / num_attention_heads)\n",
    "        self.all_head_size = self.num_attention_heads * self.attention_head_size\n",
    "\n",
    "        self.query = nn.Linear(hidden_size, self.all_head_size)\n",
    "        self.key = nn.Linear(hidden_size, self.all_head_size)\n",
    "        self.value = nn.Linear(hidden_size, self.all_head_size)\n",
    "\n",
    "        self.attn_dropout = nn.Dropout(attention_probs_dropout_prob)\n",
    "\n",
    "        self.dense = nn.Linear(hidden_size, hidden_size)\n",
    "        self.layernorm = nn.LayerNorm(hidden_size, eps=1e-12)\n",
    "        self.out_dropout = nn.Dropout(hidden_dropout_prob)\n",
    "\n",
    "    def transpose_for_scores(self, x):\n",
    "        new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)\n",
    "        x = x.view(*new_x_shape)\n",
    "        return x.permute(0, 2, 1, 3)\n",
    "\n",
    "    def forward(self, input_tensor, attention_mask):\n",
    "        mixed_query_layer = self.query(input_tensor)\n",
    "        mixed_key_layer = self.key(input_tensor)\n",
    "        mixed_value_layer = self.value(input_tensor)\n",
    "\n",
    "        query_layer = self.transpose_for_scores(mixed_query_layer)\n",
    "        key_layer = self.transpose_for_scores(mixed_key_layer)\n",
    "        value_layer = self.transpose_for_scores(mixed_value_layer)\n",
    "\n",
    "        # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n",
    "        attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n",
    "\n",
    "        attention_scores = attention_scores / math.sqrt(self.attention_head_size)\n",
    "        attention_scores = attention_scores + attention_mask\n",
    "\n",
    "        # Normalize the attention scores to probabilities.\n",
    "        attention_probs = nn.Softmax(dim=-1)(attention_scores)\n",
    "        # This is actually dropping out entire tokens to attend to, which might\n",
    "        # seem a bit unusual, but is taken from the original Transformer paper.\n",
    "        attention_probs = self.attn_dropout(attention_probs)\n",
    "        context_layer = torch.matmul(attention_probs, value_layer)\n",
    "        context_layer = context_layer.permute(0, 2, 1, 3).contiguous()\n",
    "        new_context_layer_shape = context_layer.size()[:-2] + (self.all_head_size,)\n",
    "        context_layer = context_layer.view(*new_context_layer_shape)\n",
    "        hidden_states = self.dense(context_layer)\n",
    "        hidden_states = self.out_dropout(hidden_states)\n",
    "        hidden_states = self.layernorm(hidden_states + input_tensor)\n",
    "\n",
    "        return hidden_states, attention_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = 4\n",
    "num_attention_heads = 2\n",
    "hidden_dropout_prob = 0.2\n",
    "attention_probs_dropout_prob = 0.2\n",
    "\n",
    "layer = SelfAttention(hidden_size, num_attention_heads, hidden_dropout_prob,\n",
    "                      attention_probs_dropout_prob)\n",
    "\n",
    "input_tensor = torch.rand((2,4,4))\n",
    "attention_mask = torch.rand((4,4))\n",
    "\n",
    "hidden_states = torch.round(layer.forward(input_tensor, attention_mask)[0].detach()*1e4)/1e4\n",
    "\n",
    "test_eq(hidden_states.shape.numel(), 32)\n",
    "test_eq(list(hidden_states.shape), [2, 4, 4])\n",
    "\n",
    "attention_probs = torch.round(layer.forward(input_tensor, attention_mask)[1].detach()*1e4)/1e4\n",
    "\n",
    "test_eq(attention_probs.shape.numel(), 64)\n",
    "test_eq(list(attention_probs.shape), [2, 2, 4, 4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DistSelfAttention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class DistSelfAttention(nn.Module):\n",
    "    def __init__(self,\n",
    "                 hidden_size,\n",
    "                 num_attention_heads,\n",
    "                 hidden_dropout_prob,\n",
    "                 attention_probs_dropout_prob,\n",
    "                 distance_metric = 'wasserstein'):\n",
    "        super().__init__()\n",
    "        if hidden_size % num_attention_heads != 0:\n",
    "            raise ValueError(\n",
    "                \"The hidden size (%d) is not a multiple of the number of attention \"\n",
    "                \"heads (%d)\" % (hidden_size, num_attention_heads))\n",
    "        self.num_attention_heads = num_attention_heads\n",
    "        self.attention_head_size = int(hidden_size / num_attention_heads)\n",
    "        self.all_head_size = self.num_attention_heads * self.attention_head_size\n",
    "\n",
    "        self.mean_query = nn.Linear(hidden_size, self.all_head_size)\n",
    "        self.cov_query = nn.Linear(hidden_size, self.all_head_size)\n",
    "        self.mean_key = nn.Linear(hidden_size, self.all_head_size)\n",
    "        self.cov_key = nn.Linear(hidden_size, self.all_head_size)\n",
    "        self.mean_value = nn.Linear(hidden_size, self.all_head_size)\n",
    "        self.cov_value = nn.Linear(hidden_size, self.all_head_size)\n",
    "\n",
    "        self.activation = nn.ELU()\n",
    "\n",
    "        self.attn_dropout = nn.Dropout(attention_probs_dropout_prob)\n",
    "        self.mean_dense = nn.Linear(hidden_size, hidden_size)\n",
    "        self.cov_dense = nn.Linear(hidden_size, hidden_size)\n",
    "        self.out_dropout = nn.Dropout(hidden_dropout_prob)\n",
    "\n",
    "        self.distance_metric = distance_metric\n",
    "        self.layernorm = nn.LayerNorm(hidden_size, eps=1e-12)\n",
    "        \n",
    "    def transpose_for_scores(self, x):\n",
    "        new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)\n",
    "        x = x.view(*new_x_shape)\n",
    "        return x.permute(0, 2, 1, 3)\n",
    "\n",
    "    def forward(self, input_mean_tensor, input_cov_tensor, attention_mask):\n",
    "        mixed_mean_query_layer = self.mean_query(input_mean_tensor)\n",
    "        mixed_mean_key_layer = self.mean_key(input_mean_tensor)\n",
    "        mixed_mean_value_layer = self.mean_value(input_mean_tensor)\n",
    "\n",
    "        mean_query_layer = self.transpose_for_scores(mixed_mean_query_layer)\n",
    "        mean_key_layer = self.transpose_for_scores(mixed_mean_key_layer)\n",
    "        mean_value_layer = self.transpose_for_scores(mixed_mean_value_layer)\n",
    "\n",
    "        mixed_cov_query_layer = self.activation(self.cov_query(input_cov_tensor)) + 1\n",
    "        mixed_cov_key_layer = self.activation(self.cov_key(input_cov_tensor)) + 1\n",
    "        mixed_cov_value_layer = self.activation(self.cov_value(input_cov_tensor)) + 1\n",
    "\n",
    "        cov_query_layer = self.transpose_for_scores(mixed_cov_query_layer)\n",
    "        cov_key_layer = self.transpose_for_scores(mixed_cov_key_layer)\n",
    "        cov_value_layer = self.transpose_for_scores(mixed_cov_value_layer)\n",
    "\n",
    "        if self.distance_metric == 'wasserstein':\n",
    "            attention_scores = -wasserstein_distance_matmul(mean_query_layer, cov_query_layer, mean_key_layer, cov_key_layer)\n",
    "        else:\n",
    "            attention_scores = -kl_distance_matmul(mean_query_layer, cov_query_layer, mean_key_layer, cov_key_layer)\n",
    "\n",
    "        attention_scores = attention_scores / math.sqrt(self.attention_head_size)\n",
    "        attention_scores = attention_scores + attention_mask\n",
    "        attention_probs = nn.Softmax(dim=-1)(attention_scores)\n",
    "\n",
    "        attention_probs = self.attn_dropout(attention_probs)\n",
    "        mean_context_layer = torch.matmul(attention_probs, mean_value_layer)\n",
    "        cov_context_layer = torch.matmul(attention_probs ** 2, cov_value_layer)\n",
    "        mean_context_layer = mean_context_layer.permute(0, 2, 1, 3).contiguous()\n",
    "        cov_context_layer = cov_context_layer.permute(0, 2, 1, 3).contiguous()\n",
    "        new_context_layer_shape = mean_context_layer.size()[:-2] + (self.all_head_size,)\n",
    "\n",
    "        mean_context_layer = mean_context_layer.view(*new_context_layer_shape)\n",
    "        cov_context_layer = cov_context_layer.view(*new_context_layer_shape)\n",
    "\n",
    "        mean_hidden_states = self.mean_dense(mean_context_layer)\n",
    "        mean_hidden_states = self.out_dropout(mean_hidden_states)\n",
    "        mean_hidden_states = self.layernorm(mean_hidden_states + input_mean_tensor)\n",
    "\n",
    "        cov_hidden_states = self.cov_dense(cov_context_layer)\n",
    "        cov_hidden_states = self.out_dropout(cov_hidden_states)\n",
    "        cov_hidden_states = self.layernorm(cov_hidden_states + input_cov_tensor)\n",
    "\n",
    "        return mean_hidden_states, cov_hidden_states, attention_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = 4\n",
    "num_attention_heads = 2\n",
    "hidden_dropout_prob = 0.2\n",
    "attention_probs_dropout_prob = 0.2\n",
    "distance = 'wasserstein'\n",
    "\n",
    "layer = DistSelfAttention(hidden_size, num_attention_heads, hidden_dropout_prob,\n",
    "                          attention_probs_dropout_prob, distance)\n",
    "\n",
    "input_tensor = torch.rand((2,4,4))\n",
    "attention_mask = torch.rand((4,4))\n",
    "\n",
    "output = layer.forward(input_tensor, input_tensor, attention_mask)\n",
    "\n",
    "mean_hidden_states = torch.round(output[0].detach()*1e4)/1e4\n",
    "\n",
    "test_eq(mean_hidden_states.shape.numel(), 32)\n",
    "test_eq(list(mean_hidden_states.shape), [2, 4, 4])\n",
    "\n",
    "cov_hidden_states = torch.round(output[1].detach()*1e4)/1e4\n",
    "\n",
    "test_eq(cov_hidden_states.shape.numel(), 32)\n",
    "test_eq(list(cov_hidden_states.shape), [2, 4, 4])\n",
    "\n",
    "attention_probs = torch.round(output[2].detach()*1e4)/1e4\n",
    "\n",
    "test_eq(attention_probs.shape.numel(), 64)\n",
    "test_eq(list(attention_probs.shape), [2, 2, 4, 4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DistMeanSelfAttention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class DistMeanSelfAttention(nn.Module):\n",
    "    def __init__(self,\n",
    "                 hidden_size,\n",
    "                 num_attention_heads,\n",
    "                 attention_probs_dropout_prob,\n",
    "                 hidden_dropout_prob):\n",
    "        super().__init__()\n",
    "        if hidden_size % num_attention_heads != 0:\n",
    "            raise ValueError(\n",
    "                \"The hidden size (%d) is not a multiple of the number of attention \"\n",
    "                \"heads (%d)\" % (hidden_size, num_attention_heads))\n",
    "        self.num_attention_heads = num_attention_heads\n",
    "        self.attention_head_size = int(hidden_size / num_attention_heads)\n",
    "        self.all_head_size = self.num_attention_heads * self.attention_head_size\n",
    "\n",
    "        self.mean_query = nn.Linear(hidden_size, self.all_head_size)\n",
    "        self.mean_key = nn.Linear(hidden_size, self.all_head_size)\n",
    "        self.mean_value = nn.Linear(hidden_size, self.all_head_size)\n",
    "        self.cov_key = nn.Linear(hidden_size, self.all_head_size)\n",
    "        self.cov_query = nn.Linear(hidden_size, self.all_head_size)\n",
    "        self.cov_value = nn.Linear(hidden_size, self.all_head_size)\n",
    "\n",
    "        self.activation = nn.ELU()\n",
    "\n",
    "        self.attn_dropout = nn.Dropout(attention_probs_dropout_prob)\n",
    "        self.mean_dense = nn.Linear(hidden_size, hidden_size)\n",
    "        self.cov_dense = nn.Linear(hidden_size, hidden_size)\n",
    "        self.out_dropout = nn.Dropout(hidden_dropout_prob)\n",
    "\n",
    "        self.layernorm = nn.LayerNorm(hidden_size, eps=1e-12)\n",
    "\n",
    "    def transpose_for_scores(self, x):\n",
    "        new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)\n",
    "        x = x.view(*new_x_shape)\n",
    "        return x.permute(0, 2, 1, 3)\n",
    "\n",
    "    def forward(self, input_mean_tensor, input_cov_tensor, attention_mask):\n",
    "        mixed_mean_query_layer = self.mean_query(input_mean_tensor)\n",
    "        mixed_mean_key_layer = self.mean_key(input_mean_tensor)\n",
    "        mixed_mean_value_layer = self.mean_value(input_mean_tensor)\n",
    "\n",
    "        mean_query_layer = self.transpose_for_scores(mixed_mean_query_layer)\n",
    "        mean_key_layer = self.transpose_for_scores(mixed_mean_key_layer)\n",
    "        mean_value_layer = self.transpose_for_scores(mixed_mean_value_layer)\n",
    "\n",
    "        mixed_cov_query_layer = self.activation(self.cov_query(input_cov_tensor)) + 1\n",
    "        mixed_cov_key_layer = self.activation(self.cov_key(input_cov_tensor)) + 1\n",
    "        mixed_cov_value_layer = self.activation(self.cov_value(input_cov_tensor)) + 1\n",
    "\n",
    "        cov_query_layer = self.transpose_for_scores(mixed_cov_query_layer)\n",
    "        cov_key_layer = self.transpose_for_scores(mixed_cov_key_layer)\n",
    "        cov_value_layer = self.transpose_for_scores(mixed_cov_value_layer)\n",
    "\n",
    "        mean_attention_scores = torch.matmul(mean_query_layer, mean_key_layer.transpose(-1, -2))\n",
    "        cov_attention_scores = torch.matmul(cov_query_layer, cov_key_layer.transpose(-1, -2))\n",
    "\n",
    "        mean_attention_scores = mean_attention_scores / math.sqrt(self.attention_head_size)\n",
    "        mean_attention_scores = mean_attention_scores + attention_mask\n",
    "        mean_attention_probs = nn.Softmax(dim=-1)(mean_attention_scores)\n",
    "\n",
    "        cov_attention_scores = cov_attention_scores / math.sqrt(self.attention_head_size)\n",
    "        cov_attention_scores = cov_attention_scores + attention_mask\n",
    "        cov_attention_probs = nn.Softmax(dim=-1)(cov_attention_scores)\n",
    "\n",
    "        mean_attention_probs = self.attn_dropout(mean_attention_probs)\n",
    "        cov_attention_probs = self.attn_dropout(cov_attention_probs)\n",
    "        mean_context_layer = torch.matmul(mean_attention_probs, mean_value_layer)\n",
    "        cov_context_layer = torch.matmul(cov_attention_probs, cov_value_layer)\n",
    "        mean_context_layer = mean_context_layer.permute(0, 2, 1, 3).contiguous()\n",
    "        cov_context_layer = cov_context_layer.permute(0, 2, 1, 3).contiguous()\n",
    "        new_context_layer_shape = mean_context_layer.size()[:-2] + (self.all_head_size,)\n",
    "\n",
    "        mean_context_layer = mean_context_layer.view(*new_context_layer_shape)\n",
    "        cov_context_layer = cov_context_layer.view(*new_context_layer_shape)\n",
    "\n",
    "        mean_hidden_states = self.mean_dense(mean_context_layer)\n",
    "        mean_hidden_states = self.out_dropout(mean_hidden_states)\n",
    "        mean_hidden_states = self.layernorm(mean_hidden_states + input_mean_tensor)\n",
    "\n",
    "        cov_hidden_states = self.cov_dense(cov_context_layer)\n",
    "        cov_hidden_states = self.out_dropout(cov_hidden_states)\n",
    "        cov_hidden_states = self.layernorm(cov_hidden_states + input_cov_tensor)\n",
    "\n",
    "        return mean_hidden_states, cov_hidden_states, mean_attention_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = 4\n",
    "num_attention_heads = 2\n",
    "hidden_dropout_prob = 0.2\n",
    "attention_probs_dropout_prob = 0.2\n",
    "\n",
    "layer = DistMeanSelfAttention(hidden_size, num_attention_heads, hidden_dropout_prob,\n",
    "                          attention_probs_dropout_prob)\n",
    "\n",
    "input_tensor = torch.rand((2,4,4))\n",
    "attention_mask = torch.rand((4,4))\n",
    "\n",
    "output = layer.forward(input_tensor, input_tensor, attention_mask)\n",
    "\n",
    "mean_hidden_states = torch.round(output[0].detach()*1e4)/1e4\n",
    "\n",
    "test_eq(mean_hidden_states.shape.numel(), 32)\n",
    "test_eq(list(mean_hidden_states.shape), [2, 4, 4])\n",
    "\n",
    "cov_hidden_states = torch.round(output[1].detach()*1e4)/1e4\n",
    "\n",
    "test_eq(cov_hidden_states.shape.numel(), 32)\n",
    "test_eq(list(cov_hidden_states.shape), [2, 4, 4])\n",
    "\n",
    "attention_probs = torch.round(output[2].detach()*1e4)/1e4\n",
    "\n",
    "test_eq(attention_probs.shape.numel(), 64)\n",
    "test_eq(list(attention_probs.shape), [2, 2, 4, 4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> References\n",
    "1. https://github.com/sparsh-ai/stanza/blob/S714864/model/attention.py\n",
    "2. https://github.com/xue-pai/FuxiCTR/blob/main/fuxictr/pytorch/layers/attention.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Author: Sparsh A.\n",
      "\n",
      "Last updated: 2022-01-22 15:19:18\n",
      "\n",
      "recohut: 0.0.11\n",
      "\n",
      "Compiler    : GCC 7.5.0\n",
      "OS          : Linux\n",
      "Release     : 5.4.144+\n",
      "Machine     : x86_64\n",
      "Processor   : x86_64\n",
      "CPU cores   : 2\n",
      "Architecture: 64bit\n",
      "\n",
      "IPython: 5.5.0\n",
      "torch  : 1.10.0+cu111\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "%reload_ext watermark\n",
    "%watermark -a \"Sparsh A.\" -m -iv -u -t -d -p recohut"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
