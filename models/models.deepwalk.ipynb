{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp models.deepwalk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DeepWalk\n",
    "> Implementation of Deepwalk graph embedding-based recommender model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from gensim.models import Word2Vec\n",
    "import networkx as nx\n",
    "import random\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rating_df = pd.read_csv('https://raw.githubusercontent.com/sparsh-ai/rec-data-public/master/ml-other/ml100k_ratings.csv', sep=',', header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class DeepWalk:\n",
    "    def __init__(self, is_directed=False, p=1, q=1, num_walks=10, walk_length=80, n_factors=50):\n",
    "        # p=q=1 for DeeWalk as the random walks are completely unbiased.\n",
    "        self.is_directed = is_directed\n",
    "        self.p = p\n",
    "        self.q = q\n",
    "        self.num_walks = num_walks\n",
    "        self.walk_length = walk_length\n",
    "        self.n_factors = n_factors\n",
    "                \n",
    "    def fit(self, df, save_path=None, user_col='user_id', item_col='item_id', rating_col='rating'):\n",
    "        try:\n",
    "            dataset = pickle.load(open(save_path, 'rb'))\n",
    "            self.node_vecs = dataset['node_vecs']\n",
    "            self.user2dict = dataset['user2dict']\n",
    "            self.item2dict = dataset['item2dict']\n",
    "            self.reverse_user2dict = dataset['reverse_user2dict']\n",
    "            self.reverse_item2dict = dataset['reverse_item2dict']\n",
    "        except:\n",
    "            user_item_graph = nx.Graph()\n",
    "\n",
    "            user2dict = dict()\n",
    "            item2dict = dict()\n",
    "            cnt = 0\n",
    "\n",
    "            df = df[[user_col, item_col, rating_col]].copy()\n",
    "\n",
    "            for x in df.values:\n",
    "                usr = (x[0], 'user')\n",
    "                item = (x[1], 'item')\n",
    "                if usr in user2dict:\n",
    "                    pass\n",
    "                else:\n",
    "                    user2dict[usr] = cnt\n",
    "                    cnt += 1\n",
    "                if item in item2dict:\n",
    "                    pass\n",
    "                else:\n",
    "                    item2dict[item] = cnt\n",
    "                    cnt += 1\n",
    "\n",
    "            # create a user-movie weighted graph using python library networkx\n",
    "            for x in df.values:\n",
    "                usr = (x[0], 'user')\n",
    "                item = (x[1], 'item')\n",
    "                user_item_graph.add_node(user2dict[usr])\n",
    "                user_item_graph.add_node(item2dict[item])\n",
    "                user_item_graph.add_edge(user2dict[usr], item2dict[item], weight=float(x[2]))\n",
    "            self.user_item_graph = user_item_graph\n",
    "\n",
    "            # Compute the transition probabilities based on the edge weights. \n",
    "            self.preprocess_transition_probs()\n",
    "            walks = self.simulate_walks()\n",
    "            node_embeddings = self.learn_embeddings(walks)\n",
    "\n",
    "            self.item2dict = item2dict\n",
    "            self.user2dict = user2dict\n",
    "            self.reverse_item2dict = {k:v for v,k in item2dict.items()}\n",
    "            self.reverse_user2dict = {k:v for v,k in user2dict.items()}\n",
    "\n",
    "            node_vecs = [node_embeddings[str(i)] for i in range(cnt)]\n",
    "            self.node_vecs = np.array(node_vecs)\n",
    "\n",
    "            with open(save_path, 'wb') as f:\n",
    "                dataset = {\n",
    "                    'node_vecs': self.node_vecs,\n",
    "                    'user2dict': self.user2dict,\n",
    "                    'item2dict': self.item2dict,\n",
    "                    'reverse_user2dict': self.reverse_user2dict,\n",
    "                    'reverse_item2dict': self.reverse_item2dict\n",
    "                }\n",
    "                pickle.dump(dataset, f)\n",
    "\n",
    "    def recommend(self, user_id=None, item_id=None, top_k=5):\n",
    "\n",
    "        if item_id is not None:\n",
    "            item_idx = self.item2dict[item_id]\n",
    "            query = self.node_vecs[item_idx].reshape(1,-1)\n",
    "        elif user_id is not None:\n",
    "            \"\"\"\n",
    "            items are ranked for a given user in terms of the cosine similarities \n",
    "            of their corresponding embeddings with the embedding of the user.\n",
    "            \"\"\"\n",
    "            user_idx = self.user2dict[user_id]\n",
    "            query = self.node_vecs[user_idx].reshape(1,-1)\n",
    "\n",
    "        ranking = cosine_similarity(query, self.node_vecs)\n",
    "        top_ids = np.argsort(-ranking)[0]\n",
    "        top_item_ids = [self.reverse_item2dict[j] for j in top_ids if j in self.reverse_item2dict][:top_k]\n",
    "        top_item_ids = [int(x[0]) for x in top_item_ids]\n",
    "        return top_item_ids\n",
    "\n",
    "    def node2vec_walk(self, start_node):\n",
    "        '''\n",
    "        Simulate a random walk starting from start node.\n",
    "        '''\n",
    "        G = self.user_item_graph\n",
    "        alias_nodes = self.alias_nodes\n",
    "        alias_edges = self.alias_edges\n",
    "\n",
    "        walk = [start_node]\n",
    "\n",
    "        while len(walk) < self.walk_length:\n",
    "            cur = walk[-1]\n",
    "            cur_nbrs = sorted(G.neighbors(cur))\n",
    "            if len(cur_nbrs) > 0:\n",
    "                if len(walk) == 1:\n",
    "                    walk.append(cur_nbrs[self.alias_draw(alias_nodes[cur][0], alias_nodes[cur][1])])\n",
    "                else:\n",
    "                    prev = walk[-2]\n",
    "                    next = cur_nbrs[self.alias_draw(alias_edges[(prev, cur)][0], \n",
    "                        alias_edges[(prev, cur)][1])]\n",
    "                    walk.append(next)\n",
    "            else:\n",
    "                break\n",
    "\n",
    "        return walk\n",
    "\n",
    "    def simulate_walks(self):\n",
    "        '''\n",
    "        Repeatedly simulate random walks from each node.\n",
    "        '''\n",
    "        G = self.user_item_graph\n",
    "        walks = []\n",
    "        nodes = list(G.nodes())\n",
    "        print('Walk iteration:')\n",
    "        for walk_iter in range(self.num_walks):\n",
    "            random.shuffle(nodes)\n",
    "            for node in nodes:\n",
    "                walks.append(self.node2vec_walk(start_node=node))\n",
    "\n",
    "        return walks\n",
    "\n",
    "    def learn_embeddings(self, walks):\n",
    "        '''\n",
    "        Learn embeddings by optimizing the Skipgram objective using SGD.\n",
    "        Uses Gensim Word2Vec.\n",
    "        '''\n",
    "        walks = [list(map(str, walk)) for walk in walks]\n",
    "        model = Word2Vec(walks, size=50, window=10, min_count=0, sg=1, workers=8, iter=1)\n",
    "        return model.wv\n",
    "\n",
    "    def get_alias_edge(self, src, dst):\n",
    "        '''\n",
    "        Get the alias edge setup lists for a given edge.\n",
    "        '''\n",
    "        G = self.user_item_graph\n",
    "        p = self.p\n",
    "        q = self.q\n",
    "\n",
    "        unnormalized_probs = []\n",
    "        for dst_nbr in sorted(G.neighbors(dst)):\n",
    "            if dst_nbr == src:\n",
    "                unnormalized_probs.append(G[dst][dst_nbr]['weight']/p)\n",
    "            elif G.has_edge(dst_nbr, src):\n",
    "                unnormalized_probs.append(G[dst][dst_nbr]['weight'])\n",
    "            else:\n",
    "                unnormalized_probs.append(G[dst][dst_nbr]['weight']/q)\n",
    "        norm_const = sum(unnormalized_probs)\n",
    "        try:\n",
    "            normalized_probs =  [float(u_prob)/norm_const for u_prob in unnormalized_probs]\n",
    "        except:\n",
    "            normalized_probs =  [0.0 for u_prob in unnormalized_probs]\n",
    "\n",
    "        return self.alias_setup(normalized_probs)\n",
    "\n",
    "    def preprocess_transition_probs(self):\n",
    "        '''\n",
    "        Preprocessing of transition probabilities for guiding the random walks.\n",
    "        '''\n",
    "        G = self.user_item_graph\n",
    "        is_directed = self.is_directed\n",
    "\n",
    "        alias_nodes = {}\n",
    "        for node in G.nodes():\n",
    "            unnormalized_probs = [G[node][nbr]['weight'] for nbr in sorted(G.neighbors(node))]\n",
    "            norm_const = sum(unnormalized_probs)\n",
    "            try:\n",
    "                normalized_probs =  [float(u_prob)/norm_const for u_prob in unnormalized_probs]\n",
    "            except:\n",
    "                print(node)\n",
    "                normalized_probs =  [0.0 for u_prob in unnormalized_probs]\n",
    "            alias_nodes[node] = self.alias_setup(normalized_probs)\n",
    "\n",
    "        alias_edges = {}\n",
    "        triads = {}\n",
    "\n",
    "        if is_directed:\n",
    "            for edge in G.edges():\n",
    "                alias_edges[edge] = self.get_alias_edge(edge[0], edge[1])\n",
    "        else:\n",
    "            for edge in G.edges():\n",
    "                alias_edges[edge] = self.get_alias_edge(edge[0], edge[1])\n",
    "                alias_edges[(edge[1], edge[0])] = self.get_alias_edge(edge[1], edge[0])\n",
    "\n",
    "        self.alias_nodes = alias_nodes\n",
    "        self.alias_edges = alias_edges\n",
    "\n",
    "        return\n",
    "\n",
    "    def alias_setup(self, probs):\n",
    "        '''\n",
    "        Compute utility lists for non-uniform sampling from discrete distributions.\n",
    "        Refer to https://hips.seas.harvard.edu/blog/2013/03/03/the-alias-method-efficient-sampling-with-many-discrete-outcomes/\n",
    "        for details\n",
    "        '''\n",
    "        K = len(probs)\n",
    "        q = np.zeros(K)\n",
    "        J = np.zeros(K, dtype=np.int)\n",
    "\n",
    "        smaller = []\n",
    "        larger = []\n",
    "        for kk, prob in enumerate(probs):\n",
    "            q[kk] = K*prob\n",
    "            if q[kk] < 1.0:\n",
    "                smaller.append(kk)\n",
    "            else:\n",
    "                larger.append(kk)\n",
    "\n",
    "        while len(smaller) > 0 and len(larger) > 0:\n",
    "            small = smaller.pop()\n",
    "            large = larger.pop()\n",
    "\n",
    "            J[small] = large\n",
    "            q[large] = q[large] + q[small] - 1.0\n",
    "            if q[large] < 1.0:\n",
    "                smaller.append(large)\n",
    "            else:\n",
    "                larger.append(large)\n",
    "\n",
    "        return J, q\n",
    "\n",
    "    def alias_draw(self, J, q):\n",
    "        '''\n",
    "        Draw sample from a non-uniform discrete distribution using alias sampling.\n",
    "        '''\n",
    "        K = len(J)\n",
    "\n",
    "        kk = int(np.floor(np.random.rand()*K))\n",
    "        if np.random.rand() < q[kk]:\n",
    "            return kk\n",
    "        else:\n",
    "            return J[kk]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DeepWalk()\n",
    "\n",
    "model.fit(save_path='./data.pkl', df=rating_df, user_col='userId', item_col='movieId', rating_col='rating')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[260, 1196, 1210, 1198, 1291]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2 = DeepWalk()\n",
    "\n",
    "model2.fit(df=rating_df, save_path='./data.pkl', user_col='userId', item_col='movieId', rating_col='rating')\n",
    "model2.recommend(item_id=(260, 'item'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Author: Sparsh A.\n",
      "\n",
      "Last updated: 2022-01-28 11:22:48\n",
      "\n",
      "gensim: 3.6.0\n",
      "\n",
      "Compiler    : GCC 7.5.0\n",
      "OS          : Linux\n",
      "Release     : 5.4.144+\n",
      "Machine     : x86_64\n",
      "Processor   : x86_64\n",
      "CPU cores   : 2\n",
      "Architecture: 64bit\n",
      "\n",
      "networkx: 2.6.3\n",
      "IPython : 5.5.0\n",
      "pandas  : 1.1.5\n",
      "numpy   : 1.19.5\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%reload_ext watermark\n",
    "%watermark -a \"Sparsh A.\" -m -iv -u -t -d -p gensim"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
