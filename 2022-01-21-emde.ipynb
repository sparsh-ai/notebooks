{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"2022-01-21-emde.ipynb","provenance":[{"file_id":"https://github.com/recohut/nbs/blob/main/raw/T139728%20%7C%20Modeling%20Multi-Destination%20Trips%20with%20EMDE%20Model.ipynb","timestamp":1644658801150}],"collapsed_sections":[],"mount_file_id":"1xdIMpx-9hSIpHijwpLN7Cq47XHVyaBox","authorship_tag":"ABX9TyMqs4p7tc10xLaFUf6As76n"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Modeling Multi-Destination Trips with EMDE Model"],"metadata":{"id":"xf10Ini7TZxK"}},{"cell_type":"code","metadata":{"id":"qTL31U4-q5uG"},"source":["!wget https://github.com/Synerise/cleora/releases/download/v1.1.0/cleora-v1.1.0-x86_64-unknown-linux-gnu\n","!chmod +x cleora-v1.1.0-x86_64-unknown-linux-gnu"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7Y7oA76TriEK"},"source":["!pip install torch==1.7.1\n","!pip install pytorch_lightning==1.1.0\n","!pip install tqdm==4.50.2\n","!pip install pandas==1.1.5\n","!pip install numpy==1.19.1\n","!pip install scikit_learn==0.24.1"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"OQHRRhmpwfvj"},"source":["import os\n","project_name = \"booking-trip-recommender\"; branch = \"main\"; account = \"sparsh-ai\"\n","project_path = os.path.join('/content', branch)\n","\n","if not os.path.exists(project_path):\n","    !cp -r /content/drive/MyDrive/git_credentials/. ~\n","    !mkdir \"{project_path}\"\n","    %cd \"{project_path}\"\n","    !git init\n","    !git remote add origin https://github.com/\"{account}\"/\"{project_name}\".git\n","    !git pull origin \"{branch}\"\n","    !git checkout -b \"{branch}\"\n","    %reload_ext autoreload\n","    %autoreload 2\n","else:\n","    %cd \"{project_path}\""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YrjmlGO9wfvk"},"source":["!git add .\n","!git commit -m ''\n","!git push origin \"{branch}\""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"JZx1WUTBsO2o"},"source":["!pip install -U -q dvc dvc[gdrive]\n","!dvc get https://github.com/sparsh-ai/reco-data booking/v1/train.parquet.snappy\n","!dvc get https://github.com/sparsh-ai/reco-data booking/v1/test.parquet.snappy\n","!dvc get https://github.com/sparsh-ai/reco-data booking/v1/ground_truth.parquet.snappy\n","!dvc get https://github.com/sparsh-ai/reco-data booking/v1/submission.parquet.snappy"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ybaBeLpawyhj"},"source":["!mkdir -p data/bronze data/silver\n","!mv *.snappy data/bronze"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"FLD2ETZVsTII"},"source":["## Data split\n","\n","> Split dataset into training and validation set that imitates the hidden test set"]},{"cell_type":"code","metadata":{"id":"nQVIICEzsxgg"},"source":["import argparse\n","import logging\n","import os\n","import random\n","import pandas as pd\n","import numpy as np\n","from collections import Counter"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"w_5kHOArsvr_"},"source":["log = logging.getLogger(__name__)\n","\n","\n","class Args:\n","    train = 'data/bronze/train.parquet.snappy' # Filename of training dataset provided by challenge organizers\n","    validation_size = 70662 # Number of trips in validation dataset\n","    train_output_filename = 'train.csv' # Filename of output training dataset\n","    valid_output_filename = 'valid.csv' # Filename of output validation dataset\n","    ground_truth_filename = 'ground_truth.csv' # Filename of ground truth for validation dataset\n","    working_dir = 'data/silver' # Directory where files will be saved\n","\n","args = Args()\n","\n","\n","def preprocess_data(data, utrips_counter):\n","    \"\"\"\n","    Released test set contains additional columns: `row_num`, `total_rows`.\n","    Those columns are added here.\n","    \"\"\"\n","    data['total_rows'] = data.apply(lambda row: utrips_counter[row['utrip_id']], axis = 1)\n","    row_num = []\n","    counter = 1\n","    for row in data.itertuples():\n","        row_num.append(counter)\n","        counter += 1\n","        if counter > row.total_rows:\n","            counter = 1\n","    data['row_num'] = row_num\n","\n","\n","def get_validation_utrips(data, utrips_less_than_4, validation_size):\n","    val_utrips = list()\n","    train_cities_so_far = set()\n","    utrip_cities = []\n","\n","    for row in data.itertuples():\n","        utrip_id = row.utrip_id\n","        if utrip_id in utrips_less_than_4:\n","            # test set has at least 4 cities in a trip\n","            continue\n","\n","        utrip_cities.append(row.city_id)\n","        if row.total_rows == row.row_num:\n","            if all(elem in train_cities_so_far for elem in utrip_cities) and random.random() < 0.5:\n","                val_utrips.append(row.utrip_id)\n","            else:\n","                train_cities_so_far.update(set(utrip_cities))\n","            utrip_cities = []\n","\n","        if len(val_utrips) == validation_size:\n","            break\n","\n","    log.info(f\"Number of validation trips: {len(val_utrips)}\")\n","    return val_utrips\n","\n","\n","def get_ground_truth(test):\n","    ground_truth = []\n","    for i, row in test.iterrows():\n","        if row['row_num'] == row['total_rows']:\n","            # this city should be predicted\n","            ground_truth.append({'utrip_id': row['utrip_id'],\n","                                'city_id': row['city_id'],\n","                                'hotel_country': row['hotel_country']})\n","            test.at[i, 'city_id'] = np.int64(0)\n","            test.at[i, 'hotel_country'] = ''\n","    return ground_truth\n","\n","\n","def main(params):\n","    os.makedirs(params.working_dir, exist_ok=True)\n","    data = pd.read_parquet(params.train)\n","    data['checkin'] = pd.to_datetime(data['checkin'])\n","    data['checkout'] = pd.to_datetime(data['checkout'])\n","    utrips_counter = Counter(data['utrip_id'])\n","    utrips_single = []\n","    utrips_less_than_4 = []\n","    for k, v in utrips_counter.items():\n","        if v == 1:\n","            utrips_single.append(k)\n","        if v < 4:\n","            utrips_less_than_4.append(k)\n","\n","    log.info(f\"Remove {len(utrips_single)} trips with single row\")\n","    data = data.loc[~data['utrip_id'].isin(utrips_single)]\n","    data = data.sort_values(['utrip_id', 'checkin'])\n","    data.reset_index(inplace=True, drop=True)\n","    preprocess_data(data, utrips_counter)\n","    val_utrips = get_validation_utrips(data, utrips_less_than_4, params.validation_size)\n","\n","    train = data.loc[~data['utrip_id'].isin(val_utrips)]\n","    train.reset_index(inplace=True, drop=True)\n","\n","    test = data.loc[data['utrip_id'].isin(val_utrips)]\n","    test.reset_index(inplace=True, drop=True)\n","    log.info(f\"Length of train set: {len(train)}\")\n","    log.info(f\"Length of test set: {len(test)}\")\n","\n","    ground_truth = get_ground_truth(test)\n","    pd.DataFrame(ground_truth).to_csv(os.path.join(params.working_dir, params.ground_truth_filename), index=False, sep='\\t')\n","    train.to_csv(os.path.join(params.working_dir, params.train_output_filename), index=False, sep='\\t')\n","    test.to_csv(os.path.join(params.working_dir, params.valid_output_filename), index=False, sep='\\t')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Lbjia868svpx","executionInfo":{"status":"ok","timestamp":1634566364877,"user_tz":-330,"elapsed":120326,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}},"outputId":"44a95441-666f-4cf1-a5d2-8b5c75c04a29"},"source":["if __name__ == \"__main__\":\n","    logging.basicConfig(level=logging.INFO)\n","    main(args)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["INFO:__main__:Remove 113 trips with single row\n","INFO:__main__:Number of validation trips: 70662\n","INFO:__main__:Length of train set: 792502\n","INFO:__main__:Length of test set: 374220\n"]}]},{"cell_type":"markdown","metadata":{"id":"T8qUoTpqsvnl"},"source":["This will create three files: `data/valid.csv`, `data/train.csv` and `data/ground_truth.csv`."]},{"cell_type":"markdown","metadata":{"id":"ijjN4-JuxS2c"},"source":["## City embedding\n","\n","> Compute city sketches using Cleora. This will create LSH codes for each city and save it into data/codes."]},{"cell_type":"code","metadata":{"id":"TM3Mhv5SyFLe"},"source":["import logging\n","import os\n","import subprocess\n","import numpy as np\n","import pandas as pd\n","from typing import List"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xMxZ8H9XxttA"},"source":["### Cleora"]},{"cell_type":"code","metadata":{"id":"h16-lwjHyHN1"},"source":["log = logging.getLogger(__name__)\n","\n","\n","def prepare_cleora_directed_input(filename: str, data: pd.DataFrame):\n","    \"\"\"\n","    Prepare file such as, for trip X->H->Z->X:\n","    START X_B\n","    X_A H_B\n","    H_A Z_B\n","    Z_A X_B\n","    X_A END\n","    \"\"\"\n","    log.info(\"Preparing input file to Cleora\")\n","    data_grouped_utrip = data.groupby('utrip_id')\n","    with open(filename, 'w') as f:\n","        for utrip_id, rows in data_grouped_utrip:\n","            for i in range(0, len(rows)+1):\n","                if i == 0:\n","                    f.write(f\"START\\t{rows['city_id'].tolist()[0]}_B\\n\")\n","                elif i == len(rows):\n","                    f.write(f\"{rows['city_id'].tolist()[-1]}_A\\tEND\\n\")\n","                else:\n","                    f.write(f\"{rows['city_id'].tolist()[i-1]}_A\\t{rows['city_id'].tolist()[i]}_B\\n\")\n","\n","\n","def get_cleora_output_directed(filename: str, all_cities: List[str]):\n","    \"\"\"\n","    Read embeddings from file generated by cleora.\n","    \"\"\"\n","    id2embedding = {}\n","    with open(filename, 'r') as f:\n","        next(f) # skip cleora header\n","        for index, line in enumerate(f):\n","            line_splitted = line.split(sep=' ')\n","            id = str(line_splitted[0])\n","            embedding = np.array([float(i) for i in line_splitted[2:]])\n","            id2embedding[id] = embedding\n","\n","    ids = []\n","    embeddings = []\n","    for city in all_cities:\n","        ids.append(city)\n","        embeddings.append(np.concatenate((id2embedding[f'{city}_A'], id2embedding[f'{city}_B'])))\n","\n","    return ids, np.stack(embeddings)\n","\n","\n","def train_cleora(dim: int, iter_: int, columns: str, input_filename: str, working_dir: str):\n","    \"\"\"\n","    Training Cleora. See more details: https://github.com/Synerise/cleora/\n","    \"\"\"\n","    command = ['/content/cleora-v1.1.0-x86_64-unknown-linux-gnu',\n","               '--columns', columns,\n","               '--dimension', str(dim),\n","               '-n', str(iter_),\n","               '--input', input_filename,\n","               '--output-dir', working_dir]\n","    subprocess.run(command, check=True)\n","\n","\n","def run_cleora_directed(working_dir: str, input_file: str, dim: int, iter_: int ,all_cities: List[str]):\n","    train_cleora(dim, iter_, 'nodeStart nodeEnd', input_file, working_dir)\n","    return get_cleora_output_directed(os.path.join(working_dir, 'emb__nodeStart__nodeEnd.out'), all_cities)"],"execution_count":null,"outputs":[]}]}