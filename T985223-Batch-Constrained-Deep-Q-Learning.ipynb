{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"T985223 | Batch-Constrained Deep Q-Learning","provenance":[],"collapsed_sections":[],"toc_visible":true,"authorship_tag":"ABX9TyMcJYOIvRy9GoZ1KEG5I59R"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"Q47M1niZgJON"},"source":["# Batch-Constrained Deep Q-Learning"]},{"cell_type":"markdown","metadata":{"id":"YT1iD-K6gXqq"},"source":["Current off-policy deep reinforcement learning algorithms fail to address extrapolation error by selecting actions with respect to a learned value estimate, without consideration of the accuracy of the estimate. As a result, certain outof-distribution actions can be erroneously extrapolated to higher values. However, the value of an off-policy agent can be accurately evaluated in regions where data is available. \n","\n","Batch-Constrained deep Q-learning (BCQ), uses a state-conditioned generative model to produce only previously seen actions. This generative model is combined with a Q-network, to select the highest valued action which is similar to the data in the batch. Unlike any previous continuous control deep reinforcement learning algorithms, BCQ is able to learn successfully without interacting with the environment by considering extrapolation error.\n","\n","BCQ is based on a simple idea: to avoid extrapolation error a policy should induce a similar state-action visitation to the batch. We denote policies which satisfy this notion as batch-constrained. To optimize off-policy learning for a given batch, batch-constrained policies are trained to select actions with respect to three objectives:\n","\n","1. Minimize the distance of selected actions to the data in the batch.\n","2. Lead to states where familiar data can be observed.\n","3. Maximize the value function."]},{"cell_type":"markdown","metadata":{"id":"TWKqz5cofdJg"},"source":["## Setup"]},{"cell_type":"code","metadata":{"id":"VpFbnCh_uOd5"},"source":["!pip install gym pyvirtualdisplay > /dev/null 2>&1\n","!apt-get install -y xvfb python-opengl ffmpeg > /dev/null 2>&1\n","\n","!apt-get update > /dev/null 2>&1\n","!apt-get install cmake > /dev/null 2>&1\n","!pip install --upgrade setuptools 2>&1\n","!pip install ez_setup > /dev/null 2>&1\n","!pip install gym[atari] > /dev/null 2>&1\n","\n","!wget http://www.atarimania.com/roms/Roms.rar\n","!mkdir /content/ROM/\n","!unrar e /content/Roms.rar /content/ROM/\n","!python -m atari_py.import_roms /content/ROM/"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"UJa3NdKyvorg"},"source":["Restart the runtime. Required."]},{"cell_type":"code","metadata":{"id":"QvAdeGPduQ0a"},"source":["import gym\n","from gym.wrappers import Monitor\n","import glob\n","import io\n","import base64\n","from IPython.display import HTML\n","from pyvirtualdisplay import Display\n","from IPython import display as ipythondisplay\n","\n","display = Display(visible=0, size=(1400, 900))\n","display.start()\n","\n","\"\"\"\n","Utility functions to enable video recording of gym environment \n","and displaying it.\n","To enable video, just do \"env = wrap_env(env)\"\"\n","\"\"\"\n","\n","def show_video():\n","  mp4list = glob.glob('video/*.mp4')\n","  if len(mp4list) > 0:\n","    mp4 = mp4list[0]\n","    video = io.open(mp4, 'r+b').read()\n","    encoded = base64.b64encode(video)\n","    ipythondisplay.display(HTML(data='''<video alt=\"test\" autoplay \n","                loop controls style=\"height: 400px;\">\n","                <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n","             </video>'''.format(encoded.decode('ascii'))))\n","  else: \n","    print(\"Could not find video\")\n","    \n","\n","def wrap_env(env):\n","  env = Monitor(env, './video', force=True)\n","  return env"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Zk4AGo6tp70a"},"source":["import cv2\n","import gym\n","import numpy as np\n","import torch\n","import importlib\n","import json\n","import os\n","\n","import copy\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ePikuiYJq8tq"},"source":["## Params"]},{"cell_type":"code","metadata":{"id":"mHf8KoElq97f"},"source":["class Args:\n","\n","    # env = \"PongNoFrameskip-v0\" # OpenAI gym environment name\n","    env = \"CartPole-v0\" # OpenAI gym environment name\n","    seed = 0 # Sets Gym, PyTorch and Numpy seeds\n","    buffer_name = \"Default\" # Prepends name to filename\n","    max_timesteps = 1e4 # Max time steps to run environment or train for\n","    BCQ_threshold = 0.3 # Threshold hyper-parameter for BCQ\n","    low_noise_p = 0.2 # Probability of a low noise episode when generating buffer\n","    rand_action_p = 0.2 # Probability of taking a random action when generating buffer, during non-low noise episode\n","\n","    # Atari Specific\n","    atari_preprocessing = {\n","        \"frame_skip\": 4,\n","        \"frame_size\": 84,\n","        \"state_history\": 4,\n","        \"done_on_life_loss\": False,\n","        \"reward_clipping\": True,\n","        \"max_episode_timesteps\": 27e3\n","    }\n","    \n","    atari_parameters = {\n","\t\t# Exploration\n","\t\t\"start_timesteps\": 2e4,\n","\t\t\"initial_eps\": 1,\n","\t\t\"end_eps\": 1e-2,\n","\t\t\"eps_decay_period\": 25e4,\n","\t\t# Evaluation\n","\t\t\"eval_freq\": 5e4,\n","\t\t\"eval_eps\": 1e-3,\n","\t\t# Learning\n","\t\t\"discount\": 0.99,\n","\t\t\"buffer_size\": 1e6,\n","\t\t\"batch_size\": 32,\n","\t\t\"optimizer\": \"Adam\",\n","\t\t\"optimizer_parameters\": {\n","\t\t\t\"lr\": 0.0000625,\n","\t\t\t\"eps\": 0.00015\n","\t\t},\n","\t\t\"train_freq\": 4,\n","\t\t\"polyak_target_update\": False,\n","\t\t\"target_update_freq\": 8e3,\n","\t\t\"tau\": 1\n","\t}\n","    \n","    regular_parameters = {\n","\t\t# Exploration\n","\t\t\"start_timesteps\": 1e3,\n","\t\t\"initial_eps\": 0.1,\n","\t\t\"end_eps\": 0.1,\n","\t\t\"eps_decay_period\": 1,\n","\t\t# Evaluation\n","\t\t\"eval_freq\": 5e3,\n","\t\t\"eval_eps\": 0,\n","\t\t# Learning\n","\t\t\"discount\": 0.99,\n","\t\t\"buffer_size\": 1e6,\n","\t\t\"batch_size\": 64,\n","\t\t\"optimizer\": \"Adam\",\n","\t\t\"optimizer_parameters\": {\n","\t\t\t\"lr\": 3e-4\n","\t\t},\n","\t\t\"train_freq\": 1,\n","\t\t\"polyak_target_update\": True,\n","\t\t\"target_update_freq\": 1,\n","\t\t\"tau\": 0.005\n","\t}\n","\n","\n","args = Args()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qiYgVCp7rPGb"},"source":["if not os.path.exists(\"./results\"):\n","    os.makedirs(\"./results\")\n","\n","if not os.path.exists(\"./models\"):\n","    os.makedirs(\"./models\")\n","\n","if not os.path.exists(\"./buffers\"):\n","    os.makedirs(\"./buffers\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6_UXNzIlt4Ca"},"source":["# Set seeds\n","torch.manual_seed(args.seed)\n","np.random.seed(args.seed)\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gEnEkznUp_1c"},"source":["## Replay buffer"]},{"cell_type":"code","metadata":{"id":"WiuL9NeSqBWt"},"source":["def ReplayBuffer(state_dim, is_atari, atari_preprocessing, batch_size, buffer_size, device):\n","\tif is_atari: \n","\t\treturn AtariBuffer(state_dim, atari_preprocessing, batch_size, buffer_size, device)\n","\telse: \n","\t\treturn StandardBuffer(state_dim, batch_size, buffer_size, device)\n","  \n","\n","class AtariBuffer(object):\n","\tdef __init__(self, state_dim, atari_preprocessing, batch_size, buffer_size, device):\n","\t\tself.batch_size = batch_size\n","\t\tself.max_size = int(buffer_size)\n","\t\tself.device = device\n","\n","\t\tself.state_history = atari_preprocessing[\"state_history\"]\n","\n","\t\tself.ptr = 0\n","\t\tself.crt_size = 0\n","\n","\t\tself.state = np.zeros((\n","\t\t\tself.max_size + 1,\n","\t\t\tatari_preprocessing[\"frame_size\"],\n","\t\t\tatari_preprocessing[\"frame_size\"]\n","\t\t), dtype=np.uint8)\n","\n","\t\tself.action = np.zeros((self.max_size, 1), dtype=np.int64)\n","\t\tself.reward = np.zeros((self.max_size, 1))\n","\t\t\n","\t\t# not_done only consider \"done\" if episode terminates due to failure condition\n","\t\t# if episode terminates due to timelimit, the transition is not added to the buffer\n","\t\tself.not_done = np.zeros((self.max_size, 1))\n","\t\tself.first_timestep = np.zeros(self.max_size, dtype=np.uint8)\n","\n","\n","\tdef add(self, state, action, next_state, reward, done, env_done, first_timestep):\n","\t\t# If dones don't match, env has reset due to timelimit\n","\t\t# and we don't add the transition to the buffer\n","\t\tif done != env_done:\n","\t\t\treturn\n","\n","\t\tself.state[self.ptr] = state[0]\n","\t\tself.action[self.ptr] = action\n","\t\tself.reward[self.ptr] = reward\n","\t\tself.not_done[self.ptr] = 1. - done\n","\t\tself.first_timestep[self.ptr] = first_timestep\n","\n","\t\tself.ptr = (self.ptr + 1) % self.max_size\n","\t\tself.crt_size = min(self.crt_size + 1, self.max_size)\n","\n","\n","\tdef sample(self):\n","\t\tind = np.random.randint(0, self.crt_size, size=self.batch_size)\n","\n","\t\t# Note + is concatenate here\n","\t\tstate = np.zeros(((self.batch_size, self.state_history) + self.state.shape[1:]), dtype=np.uint8)\n","\t\tnext_state = np.array(state)\n","\n","\t\tstate_not_done = 1.\n","\t\tnext_not_done = 1.\n","\t\tfor i in range(self.state_history):\n","\n","\t\t\t# Wrap around if the buffer is filled\n","\t\t\tif self.crt_size == self.max_size:\n","\t\t\t\tj = (ind - i) % self.max_size\n","\t\t\t\tk = (ind - i + 1) % self.max_size\n","\t\t\telse:\n","\t\t\t\tj = ind - i\n","\t\t\t\tk = (ind - i + 1).clip(min=0)\n","\t\t\t\t# If j == -1, then we set state_not_done to 0.\n","\t\t\t\tstate_not_done *= (j + 1).clip(min=0, max=1).reshape(-1, 1, 1) #np.where(j < 0, state_not_done * 0, state_not_done)\n","\t\t\t\tj = j.clip(min=0)\n","\n","\t\t\t# State should be all 0s if the episode terminated previously\n","\t\t\tstate[:, i] = self.state[j] * state_not_done\n","\t\t\tnext_state[:, i] = self.state[k] * next_not_done\n","\n","\t\t\t# If this was the first timestep, make everything previous = 0\n","\t\t\tnext_not_done *= state_not_done\n","\t\t\tstate_not_done *= (1. - self.first_timestep[j]).reshape(-1, 1, 1)\n","\n","\t\treturn (\n","\t\t\ttorch.ByteTensor(state).to(self.device).float(),\n","\t\t\ttorch.LongTensor(self.action[ind]).to(self.device),\n","\t\t\ttorch.ByteTensor(next_state).to(self.device).float(),\n","\t\t\ttorch.FloatTensor(self.reward[ind]).to(self.device),\n","\t\t\ttorch.FloatTensor(self.not_done[ind]).to(self.device)\n","\t\t)\n","\n","\n","\tdef save(self, save_folder, chunk=int(1e5)):\n","\t\tnp.save(f\"{save_folder}_action.npy\", self.action[:self.crt_size])\n","\t\tnp.save(f\"{save_folder}_reward.npy\", self.reward[:self.crt_size])\n","\t\tnp.save(f\"{save_folder}_not_done.npy\", self.not_done[:self.crt_size])\n","\t\tnp.save(f\"{save_folder}_first_timestep.npy\", self.first_timestep[:self.crt_size])\n","\t\tnp.save(f\"{save_folder}_replay_info.npy\", [self.ptr, chunk])\n","\n","\t\tcrt = 0\n","\t\tend = min(chunk, self.crt_size + 1)\n","\t\twhile crt < self.crt_size + 1:\n","\t\t\tnp.save(f\"{save_folder}_state_{end}.npy\", self.state[crt:end])\n","\t\t\tcrt = end\n","\t\t\tend = min(end + chunk, self.crt_size + 1)\n","\n","\n","\tdef load(self, save_folder, size=-1):\n","\t\treward_buffer = np.load(f\"{save_folder}_reward.npy\")\n","\t\tsize = min(int(size), self.max_size) if size > 0 else self.max_size\n","\t\tself.crt_size = min(reward_buffer.shape[0], size)\n","\t\t\n","\t\t# Adjust crt_size if we're using a custom size\n","\t\tsize = min(int(size), self.max_size) if size > 0 else self.max_size\n","\t\tself.crt_size = min(reward_buffer.shape[0], size)\n","\n","\t\tself.action[:self.crt_size] = np.load(f\"{save_folder}_action.npy\")[:self.crt_size]\n","\t\tself.reward[:self.crt_size] = reward_buffer[:self.crt_size]\n","\t\tself.not_done[:self.crt_size] = np.load(f\"{save_folder}_not_done.npy\")[:self.crt_size]\n","\t\tself.first_timestep[:self.crt_size] = np.load(f\"{save_folder}_first_timestep.npy\")[:self.crt_size]\n","\n","\t\tself.ptr, chunk = np.load(f\"{save_folder}_replay_info.npy\")\n","\n","\t\tcrt = 0\n","\t\tend = min(chunk, self.crt_size + 1)\n","\t\twhile crt < self.crt_size + 1:\n","\t\t\tself.state[crt:end] = np.load(f\"{save_folder}_state_{end}.npy\")\n","\t\t\tcrt = end\n","\t\t\tend = min(end + chunk, self.crt_size + 1)\n","\n","\n","# Generic replay buffer for standard gym tasks\n","class StandardBuffer(object):\n","\tdef __init__(self, state_dim, batch_size, buffer_size, device):\n","\t\tself.batch_size = batch_size\n","\t\tself.max_size = int(buffer_size)\n","\t\tself.device = device\n","\n","\t\tself.ptr = 0\n","\t\tself.crt_size = 0\n","\n","\t\tself.state = np.zeros((self.max_size, state_dim))\n","\t\tself.action = np.zeros((self.max_size, 1))\n","\t\tself.next_state = np.array(self.state)\n","\t\tself.reward = np.zeros((self.max_size, 1))\n","\t\tself.not_done = np.zeros((self.max_size, 1))\n","\n","\n","\tdef add(self, state, action, next_state, reward, done, episode_done, episode_start):\n","\t\tself.state[self.ptr] = state\n","\t\tself.action[self.ptr] = action\n","\t\tself.next_state[self.ptr] = next_state\n","\t\tself.reward[self.ptr] = reward\n","\t\tself.not_done[self.ptr] = 1. - done\n","\n","\t\tself.ptr = (self.ptr + 1) % self.max_size\n","\t\tself.crt_size = min(self.crt_size + 1, self.max_size)\n","\n","\n","\tdef sample(self):\n","\t\tind = np.random.randint(0, self.crt_size, size=self.batch_size)\n","\t\treturn (\n","\t\t\ttorch.FloatTensor(self.state[ind]).to(self.device),\n","\t\t\ttorch.LongTensor(self.action[ind]).to(self.device),\n","\t\t\ttorch.FloatTensor(self.next_state[ind]).to(self.device),\n","\t\t\ttorch.FloatTensor(self.reward[ind]).to(self.device),\n","\t\t\ttorch.FloatTensor(self.not_done[ind]).to(self.device)\n","\t\t)\n","\n","\n","\tdef save(self, save_folder):\n","\t\tnp.save(f\"{save_folder}_state.npy\", self.state[:self.crt_size])\n","\t\tnp.save(f\"{save_folder}_action.npy\", self.action[:self.crt_size])\n","\t\tnp.save(f\"{save_folder}_next_state.npy\", self.next_state[:self.crt_size])\n","\t\tnp.save(f\"{save_folder}_reward.npy\", self.reward[:self.crt_size])\n","\t\tnp.save(f\"{save_folder}_not_done.npy\", self.not_done[:self.crt_size])\n","\t\tnp.save(f\"{save_folder}_ptr.npy\", self.ptr)\n","\n","\n","\tdef load(self, save_folder, size=-1):\n","\t\treward_buffer = np.load(f\"{save_folder}_reward.npy\")\n","\t\t\n","\t\t# Adjust crt_size if we're using a custom size\n","\t\tsize = min(int(size), self.max_size) if size > 0 else self.max_size\n","\t\tself.crt_size = min(reward_buffer.shape[0], size)\n","\n","\t\tself.state[:self.crt_size] = np.load(f\"{save_folder}_state.npy\")[:self.crt_size]\n","\t\tself.action[:self.crt_size] = np.load(f\"{save_folder}_action.npy\")[:self.crt_size]\n","\t\tself.next_state[:self.crt_size] = np.load(f\"{save_folder}_next_state.npy\")[:self.crt_size]\n","\t\tself.reward[:self.crt_size] = reward_buffer[:self.crt_size]\n","\t\tself.not_done[:self.crt_size] = np.load(f\"{save_folder}_not_done.npy\")[:self.crt_size]\n","\n","\t\tprint(f\"Replay Buffer loaded with {self.crt_size} elements.\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"QMaKbJpuqPMO"},"source":["## Atari preprocessing"]},{"cell_type":"code","metadata":{"id":"nJybxr6AqPJr"},"source":["# Atari Preprocessing\n","# Code is based on https://github.com/openai/gym/blob/master/gym/wrappers/atari_preprocessing.py\n","class AtariPreprocessing(object):\n","\tdef __init__(\n","\t\tself,\n","\t\tenv,\n","\t\tframe_skip=4,\n","\t\tframe_size=84,\n","\t\tstate_history=4,\n","\t\tdone_on_life_loss=False,\n","\t\treward_clipping=True, # Clips to a range of -1,1\n","\t\tmax_episode_timesteps=27000\n","\t):\n","\t\tself.env = env.env\n","\t\tself.done_on_life_loss = done_on_life_loss\n","\t\tself.frame_skip = frame_skip\n","\t\tself.frame_size = frame_size\n","\t\tself.reward_clipping = reward_clipping\n","\t\tself._max_episode_steps = max_episode_timesteps\n","\t\tself.observation_space = np.zeros((frame_size, frame_size))\n","\t\tself.action_space = self.env.action_space\n","\n","\t\tself.lives = 0\n","\t\tself.episode_length = 0\n","\n","\t\t# Tracks previous 2 frames\n","\t\tself.frame_buffer = np.zeros(\n","\t\t\t(2,\n","\t\t\tself.env.observation_space.shape[0],\n","\t\t\tself.env.observation_space.shape[1]),\n","\t\t\tdtype=np.uint8\n","\t\t)\n","\t\t# Tracks previous 4 states\n","\t\tself.state_buffer = np.zeros((state_history, frame_size, frame_size), dtype=np.uint8)\n","\n","\n","\tdef reset(self):\n","\t\tself.env.reset()\n","\t\tself.lives = self.env.ale.lives()\n","\t\tself.episode_length = 0\n","\t\tself.env.ale.getScreenGrayscale(self.frame_buffer[0])\n","\t\tself.frame_buffer[1] = 0\n","\n","\t\tself.state_buffer[0] = self.adjust_frame()\n","\t\tself.state_buffer[1:] = 0\n","\t\treturn self.state_buffer\n","\n","\n","\t# Takes single action is repeated for frame_skip frames (usually 4)\n","\t# Reward is accumulated over those frames\n","\tdef step(self, action):\n","\t\ttotal_reward = 0.\n","\t\tself.episode_length += 1\n","\n","\t\tfor frame in range(self.frame_skip):\n","\t\t\t_, reward, done, _ = self.env.step(action)\n","\t\t\ttotal_reward += reward\n","\n","\t\t\tif self.done_on_life_loss:\n","\t\t\t\tcrt_lives = self.env.ale.lives()\n","\t\t\t\tdone = True if crt_lives < self.lives else done\n","\t\t\t\tself.lives = crt_lives\n","\n","\t\t\tif done: \n","\t\t\t\tbreak\n","\n","\t\t\t# Second last and last frame\n","\t\t\tf = frame + 2 - self.frame_skip \n","\t\t\tif f >= 0:\n","\t\t\t\tself.env.ale.getScreenGrayscale(self.frame_buffer[f])\n","\n","\t\tself.state_buffer[1:] = self.state_buffer[:-1]\n","\t\tself.state_buffer[0] = self.adjust_frame()\n","\n","\t\tdone_float = float(done)\n","\t\tif self.episode_length >= self._max_episode_steps:\n","\t\t\tdone = True\n","\n","\t\treturn self.state_buffer, total_reward, done, [np.clip(total_reward, -1, 1), done_float]\n","\n","\n","\tdef adjust_frame(self):\n","\t\t# Take maximum over last two frames\n","\t\tnp.maximum(\n","\t\t\tself.frame_buffer[0],\n","\t\t\tself.frame_buffer[1],\n","\t\t\tout=self.frame_buffer[0]\n","\t\t)\n","\n","\t\t# Resize\n","\t\timage = cv2.resize(\n","\t\t\tself.frame_buffer[0],\n","\t\t\t(self.frame_size, self.frame_size),\n","\t\t\tinterpolation=cv2.INTER_AREA\n","\t\t)\n","\t\treturn np.array(image, dtype=np.uint8)\n","\n","\n","\tdef seed(self, seed):\n","\t\tself.env.seed(seed)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"G9e6SK5TqMVH"},"source":["## Create Environment"]},{"cell_type":"code","metadata":{"id":"vGg1Z3pSqMRT"},"source":["# Create environment, add wrapper if necessary and create env_properties\n","def make_env(env_name, atari_preprocessing):\n","\tenv = wrap_env(gym.make(env_name))\n","\t\n","\tis_atari = gym.envs.registry.spec(env_name).entry_point == 'gym.envs.atari:AtariEnv'\n","\tenv = AtariPreprocessing(env, **atari_preprocessing) if is_atari else env\n","\n","\tstate_dim = (\n","\t\tatari_preprocessing[\"state_history\"], \n","\t\tatari_preprocessing[\"frame_size\"], \n","\t\tatari_preprocessing[\"frame_size\"]\n","\t) if is_atari else env.observation_space.shape[0]\n","\n","\treturn (\n","\t\tenv,\n","\t\tis_atari,\n","\t\tstate_dim,\n","\t\tenv.action_space.n\n","\t)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ueZKn76hqoBB"},"source":["## DQN"]},{"cell_type":"code","metadata":{"id":"3ewLgMzlteCx"},"source":["# Make env and determine properties\n","env, is_atari, state_dim, num_actions = make_env(args.env, args.atari_preprocessing)\n","parameters = args.atari_parameters if is_atari else args.regular_parameters\n","\n","\n","# Set seeds\n","env.seed(args.seed)\n","env.action_space.seed(args.seed)\n","\n","\n","# Initialize buffer\n","replay_buffer = ReplayBuffer(state_dim, is_atari, args.atari_preprocessing, parameters[\"batch_size\"], parameters[\"buffer_size\"], device)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"K4KSK5hoqn-j"},"source":["# Used for Atari\n","class Conv_Q(nn.Module):\n","\tdef __init__(self, frames, num_actions):\n","\t\tsuper(Conv_Q, self).__init__()\n","\t\tself.c1 = nn.Conv2d(frames, 32, kernel_size=8, stride=4)\n","\t\tself.c2 = nn.Conv2d(32, 64, kernel_size=4, stride=2)\n","\t\tself.c3 = nn.Conv2d(64, 64, kernel_size=3, stride=1)\n","\t\tself.l1 = nn.Linear(3136, 512)\n","\t\tself.l2 = nn.Linear(512, num_actions)\n","\n","\n","\tdef forward(self, state):\n","\t\tq = F.relu(self.c1(state))\n","\t\tq = F.relu(self.c2(q))\n","\t\tq = F.relu(self.c3(q))\n","\t\tq = F.relu(self.l1(q.reshape(-1, 3136)))\n","\t\treturn self.l2(q)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wX6vCgwNqoqr"},"source":["# Used for Box2D / Toy problems\n","class FC_Q(nn.Module):\n","\tdef __init__(self, state_dim, num_actions):\n","\t\tsuper(FC_Q, self).__init__()\n","\t\tself.l1 = nn.Linear(state_dim, 256)\n","\t\tself.l2 = nn.Linear(256, 256)\n","\t\tself.l3 = nn.Linear(256, num_actions)\n","\n","\n","\tdef forward(self, state):\n","\t\tq = F.relu(self.l1(state))\n","\t\tq = F.relu(self.l2(q))\n","\t\treturn self.l3(q)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"e5p4OXgiq5VA"},"source":["class DQN(object):\n","\tdef __init__(\n","\t\tself, \n","\t\tis_atari,\n","\t\tnum_actions,\n","\t\tstate_dim,\n","\t\tdevice,\n","\t\tdiscount=0.99,\n","\t\toptimizer=\"Adam\",\n","\t\toptimizer_parameters={},\n","\t\tpolyak_target_update=False,\n","\t\ttarget_update_frequency=8e3,\n","\t\ttau=0.005,\n","\t\tinitial_eps = 1,\n","\t\tend_eps = 0.001,\n","\t\teps_decay_period = 25e4,\n","\t\teval_eps=0.001,\n","\t):\n","\t\n","\t\tself.device = device\n","\n","\t\t# Determine network type\n","\t\tself.Q = Conv_Q(state_dim[0], num_actions).to(self.device) if is_atari else FC_Q(state_dim, num_actions).to(self.device)\n","\t\tself.Q_target = copy.deepcopy(self.Q)\n","\t\tself.Q_optimizer = getattr(torch.optim, optimizer)(self.Q.parameters(), **optimizer_parameters)\n","\n","\t\tself.discount = discount\n","\n","\t\t# Target update rule\n","\t\tself.maybe_update_target = self.polyak_target_update if polyak_target_update else self.copy_target_update\n","\t\tself.target_update_frequency = target_update_frequency\n","\t\tself.tau = tau\n","\n","\t\t# Decay for eps\n","\t\tself.initial_eps = initial_eps\n","\t\tself.end_eps = end_eps\n","\t\tself.slope = (self.end_eps - self.initial_eps) / eps_decay_period\n","\n","\t\t# Evaluation hyper-parameters\n","\t\tself.state_shape = (-1,) + state_dim if is_atari else (-1, state_dim)\n","\t\tself.eval_eps = eval_eps\n","\t\tself.num_actions = num_actions\n","\n","\t\t# Number of training iterations\n","\t\tself.iterations = 0\n","\n","\n","\tdef select_action(self, state, eval=False):\n","\t\teps = self.eval_eps if eval \\\n","\t\t\telse max(self.slope * self.iterations + self.initial_eps, self.end_eps)\n","\n","\t\t# Select action according to policy with probability (1-eps)\n","\t\t# otherwise, select random action\n","\t\tif np.random.uniform(0,1) > eps:\n","\t\t\twith torch.no_grad():\n","\t\t\t\tstate = torch.FloatTensor(state).reshape(self.state_shape).to(self.device)\n","\t\t\t\treturn int(self.Q(state).argmax(1))\n","\t\telse:\n","\t\t\treturn np.random.randint(self.num_actions)\n","\n","\n","\tdef train(self, replay_buffer):\n","\t\t# Sample replay buffer\n","\t\tstate, action, next_state, reward, done = replay_buffer.sample()\n","\n","\t\t# Compute the target Q value\n","\t\twith torch.no_grad():\n","\t\t\ttarget_Q = reward + done * self.discount * self.Q_target(next_state).max(1, keepdim=True)[0]\n","\n","\t\t# Get current Q estimate\n","\t\tcurrent_Q = self.Q(state).gather(1, action)\n","\n","\t\t# Compute Q loss\n","\t\tQ_loss = F.smooth_l1_loss(current_Q, target_Q)\n","\n","\t\t# Optimize the Q\n","\t\tself.Q_optimizer.zero_grad()\n","\t\tQ_loss.backward()\n","\t\tself.Q_optimizer.step()\n","\n","\t\t# Update target network by polyak or full copy every X iterations.\n","\t\tself.iterations += 1\n","\t\tself.maybe_update_target()\n","\n","\n","\tdef polyak_target_update(self):\n","\t\tfor param, target_param in zip(self.Q.parameters(), self.Q_target.parameters()):\n","\t\t   target_param.data.copy_(self.tau * param.data + (1 - self.tau) * target_param.data)\n","\n","\n","\tdef copy_target_update(self):\n","\t\tif self.iterations % self.target_update_frequency == 0:\n","\t\t\t self.Q_target.load_state_dict(self.Q.state_dict())\n","\n","\n","\tdef save(self, filename):\n","\t\ttorch.save(self.Q.state_dict(), filename + \"_Q\")\n","\t\ttorch.save(self.Q_optimizer.state_dict(), filename + \"_optimizer\")\n","\n","\n","\tdef load(self, filename):\n","\t\tself.Q.load_state_dict(torch.load(filename + \"_Q\"))\n","\t\tself.Q_target = copy.deepcopy(self.Q)\n","\t\tself.Q_optimizer.load_state_dict(torch.load(filename + \"_optimizer\"))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"yKxl3XyN0JJ6"},"source":["# Runs policy for X episodes and returns average reward\n","# A fixed seed is used for the eval environment\n","def eval_policy(policy, env_name, seed, eval_episodes=10):\n","\teval_env, _, _, _ = make_env(env_name, args.atari_preprocessing)\n","\teval_env.seed(seed + 100)\n","\n","\tavg_reward = 0.\n","\tfor _ in range(eval_episodes):\n","\t\tstate, done = eval_env.reset(), False\n","\t\twhile not done:\n","\t\t\taction = policy.select_action(np.array(state), eval=True)\n","\t\t\tstate, reward, done, _ = eval_env.step(action)\n","\t\t\tavg_reward += reward\n","\n","\tavg_reward /= eval_episodes\n","\n","\tprint(\"---------------------------------------\")\n","\tprint(f\"Evaluation over {eval_episodes} episodes: {avg_reward:.3f}\")\n","\tprint(\"---------------------------------------\")\n","\treturn avg_reward"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Q4jTDjzCwOFP","executionInfo":{"status":"ok","timestamp":1634987412783,"user_tz":-330,"elapsed":49215,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}},"outputId":"406cd817-4185-4535-acc6-19577a5a941d"},"source":["# For saving files\n","setting = f\"{args.env}_{args.seed}\"\n","buffer_name = f\"{args.buffer_name}_{setting}\"\n","\n","# Initialize and load policy\n","policy = DQN(\n","    is_atari,\n","    num_actions,\n","    state_dim,\n","    device,\n","    parameters[\"discount\"],\n","    parameters[\"optimizer\"],\n","    parameters[\"optimizer_parameters\"],\n","    parameters[\"polyak_target_update\"],\n","    parameters[\"target_update_freq\"],\n","    parameters[\"tau\"],\n","    parameters[\"initial_eps\"],\n","    parameters[\"end_eps\"],\n","    parameters[\"eps_decay_period\"],\n","    parameters[\"eval_eps\"],\n",")\n","\n","evaluations = []\n","\n","state, done = env.reset(), False\n","episode_start = True\n","episode_reward = 0\n","episode_timesteps = 0\n","episode_num = 0\n","low_noise_ep = np.random.uniform(0,1) < args.low_noise_p\n","max_episode_steps = gym.make(args.env)._max_episode_steps\n","\n","# Interact with the environment for max_timesteps\n","for t in range(int(args.max_timesteps)):\n","\n","    episode_timesteps += 1\n","\n","    if t < parameters[\"start_timesteps\"]:\n","        action = env.action_space.sample()\n","    else:\n","        action = policy.select_action(np.array(state))\n","\n","    # Perform action and log results\n","    next_state, reward, done, info = env.step(action)\n","    episode_reward += reward\n","\n","    # Only consider \"done\" if episode terminates due to failure condition\n","    done_float = float(done) if episode_timesteps < max_episode_steps else 0\n","\n","    # For atari, info[0] = clipped reward, info[1] = done_float\n","    if is_atari:\n","        reward = info[0]\n","        done_float = info[1]\n","        \n","    # Store data in replay buffer\n","    replay_buffer.add(state, action, next_state, reward, done_float, done, episode_start)\n","    state = copy.copy(next_state)\n","    episode_start = False\n","\n","    # Train agent after collecting sufficient data\n","    if t >= parameters[\"start_timesteps\"] and (t+1) % parameters[\"train_freq\"] == 0:\n","        policy.train(replay_buffer)\n","\n","    if done:\n","        # +1 to account for 0 indexing. +0 on ep_timesteps since it will increment +1 even if done=True\n","        print(f\"Total T: {t+1} Episode Num: {episode_num+1} Episode T: {episode_timesteps} Reward: {episode_reward:.3f}\")\n","        # Reset environment\n","        state, done = env.reset(), False\n","        episode_start = True\n","        episode_reward = 0\n","        episode_timesteps = 0\n","        episode_num += 1\n","        low_noise_ep = np.random.uniform(0,1) < args.low_noise_p\n","\n","    # Evaluate episode\n","    if (t + 1) % parameters[\"eval_freq\"] == 0:\n","        evaluations.append(eval_policy(policy, args.env, args.seed))\n","        np.save(f\"./results/behavioral_{setting}\", evaluations)\n","        policy.save(f\"./models/behavioral_{setting}\")\n","\n","# Save final policy\n","policy.save(f\"./models/behavioral_{setting}\")"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Total T: 52 Episode Num: 1 Episode T: 52 Reward: 52.000\n","Total T: 107 Episode Num: 2 Episode T: 55 Reward: 55.000\n","Total T: 136 Episode Num: 3 Episode T: 29 Reward: 29.000\n","Total T: 156 Episode Num: 4 Episode T: 20 Reward: 20.000\n","Total T: 168 Episode Num: 5 Episode T: 12 Reward: 12.000\n","Total T: 187 Episode Num: 6 Episode T: 19 Reward: 19.000\n","Total T: 207 Episode Num: 7 Episode T: 20 Reward: 20.000\n","Total T: 220 Episode Num: 8 Episode T: 13 Reward: 13.000\n","Total T: 249 Episode Num: 9 Episode T: 29 Reward: 29.000\n","Total T: 268 Episode Num: 10 Episode T: 19 Reward: 19.000\n","Total T: 286 Episode Num: 11 Episode T: 18 Reward: 18.000\n","Total T: 306 Episode Num: 12 Episode T: 20 Reward: 20.000\n","Total T: 321 Episode Num: 13 Episode T: 15 Reward: 15.000\n","Total T: 368 Episode Num: 14 Episode T: 47 Reward: 47.000\n","Total T: 405 Episode Num: 15 Episode T: 37 Reward: 37.000\n","Total T: 429 Episode Num: 16 Episode T: 24 Reward: 24.000\n","Total T: 461 Episode Num: 17 Episode T: 32 Reward: 32.000\n","Total T: 475 Episode Num: 18 Episode T: 14 Reward: 14.000\n","Total T: 500 Episode Num: 19 Episode T: 25 Reward: 25.000\n","Total T: 519 Episode Num: 20 Episode T: 19 Reward: 19.000\n","Total T: 528 Episode Num: 21 Episode T: 9 Reward: 9.000\n","Total T: 546 Episode Num: 22 Episode T: 18 Reward: 18.000\n","Total T: 566 Episode Num: 23 Episode T: 20 Reward: 20.000\n","Total T: 596 Episode Num: 24 Episode T: 30 Reward: 30.000\n","Total T: 613 Episode Num: 25 Episode T: 17 Reward: 17.000\n","Total T: 627 Episode Num: 26 Episode T: 14 Reward: 14.000\n","Total T: 642 Episode Num: 27 Episode T: 15 Reward: 15.000\n","Total T: 670 Episode Num: 28 Episode T: 28 Reward: 28.000\n","Total T: 682 Episode Num: 29 Episode T: 12 Reward: 12.000\n","Total T: 693 Episode Num: 30 Episode T: 11 Reward: 11.000\n","Total T: 707 Episode Num: 31 Episode T: 14 Reward: 14.000\n","Total T: 727 Episode Num: 32 Episode T: 20 Reward: 20.000\n","Total T: 754 Episode Num: 33 Episode T: 27 Reward: 27.000\n","Total T: 784 Episode Num: 34 Episode T: 30 Reward: 30.000\n","Total T: 819 Episode Num: 35 Episode T: 35 Reward: 35.000\n","Total T: 849 Episode Num: 36 Episode T: 30 Reward: 30.000\n","Total T: 870 Episode Num: 37 Episode T: 21 Reward: 21.000\n","Total T: 885 Episode Num: 38 Episode T: 15 Reward: 15.000\n","Total T: 910 Episode Num: 39 Episode T: 25 Reward: 25.000\n","Total T: 925 Episode Num: 40 Episode T: 15 Reward: 15.000\n","Total T: 936 Episode Num: 41 Episode T: 11 Reward: 11.000\n","Total T: 949 Episode Num: 42 Episode T: 13 Reward: 13.000\n","Total T: 966 Episode Num: 43 Episode T: 17 Reward: 17.000\n","Total T: 987 Episode Num: 44 Episode T: 21 Reward: 21.000\n","Total T: 1001 Episode Num: 45 Episode T: 14 Reward: 14.000\n","Total T: 1011 Episode Num: 46 Episode T: 10 Reward: 10.000\n","Total T: 1021 Episode Num: 47 Episode T: 10 Reward: 10.000\n","Total T: 1029 Episode Num: 48 Episode T: 8 Reward: 8.000\n","Total T: 1056 Episode Num: 49 Episode T: 27 Reward: 27.000\n","Total T: 1065 Episode Num: 50 Episode T: 9 Reward: 9.000\n","Total T: 1075 Episode Num: 51 Episode T: 10 Reward: 10.000\n","Total T: 1085 Episode Num: 52 Episode T: 10 Reward: 10.000\n","Total T: 1096 Episode Num: 53 Episode T: 11 Reward: 11.000\n","Total T: 1104 Episode Num: 54 Episode T: 8 Reward: 8.000\n","Total T: 1112 Episode Num: 55 Episode T: 8 Reward: 8.000\n","Total T: 1121 Episode Num: 56 Episode T: 9 Reward: 9.000\n","Total T: 1131 Episode Num: 57 Episode T: 10 Reward: 10.000\n","Total T: 1142 Episode Num: 58 Episode T: 11 Reward: 11.000\n","Total T: 1154 Episode Num: 59 Episode T: 12 Reward: 12.000\n","Total T: 1166 Episode Num: 60 Episode T: 12 Reward: 12.000\n","Total T: 1176 Episode Num: 61 Episode T: 10 Reward: 10.000\n","Total T: 1185 Episode Num: 62 Episode T: 9 Reward: 9.000\n","Total T: 1195 Episode Num: 63 Episode T: 10 Reward: 10.000\n","Total T: 1205 Episode Num: 64 Episode T: 10 Reward: 10.000\n","Total T: 1216 Episode Num: 65 Episode T: 11 Reward: 11.000\n","Total T: 1225 Episode Num: 66 Episode T: 9 Reward: 9.000\n","Total T: 1234 Episode Num: 67 Episode T: 9 Reward: 9.000\n","Total T: 1244 Episode Num: 68 Episode T: 10 Reward: 10.000\n","Total T: 1252 Episode Num: 69 Episode T: 8 Reward: 8.000\n","Total T: 1262 Episode Num: 70 Episode T: 10 Reward: 10.000\n","Total T: 1272 Episode Num: 71 Episode T: 10 Reward: 10.000\n","Total T: 1282 Episode Num: 72 Episode T: 10 Reward: 10.000\n","Total T: 1291 Episode Num: 73 Episode T: 9 Reward: 9.000\n","Total T: 1300 Episode Num: 74 Episode T: 9 Reward: 9.000\n","Total T: 1310 Episode Num: 75 Episode T: 10 Reward: 10.000\n","Total T: 1320 Episode Num: 76 Episode T: 10 Reward: 10.000\n","Total T: 1330 Episode Num: 77 Episode T: 10 Reward: 10.000\n","Total T: 1343 Episode Num: 78 Episode T: 13 Reward: 13.000\n","Total T: 1352 Episode Num: 79 Episode T: 9 Reward: 9.000\n","Total T: 1361 Episode Num: 80 Episode T: 9 Reward: 9.000\n","Total T: 1371 Episode Num: 81 Episode T: 10 Reward: 10.000\n","Total T: 1380 Episode Num: 82 Episode T: 9 Reward: 9.000\n","Total T: 1390 Episode Num: 83 Episode T: 10 Reward: 10.000\n","Total T: 1403 Episode Num: 84 Episode T: 13 Reward: 13.000\n","Total T: 1416 Episode Num: 85 Episode T: 13 Reward: 13.000\n","Total T: 1426 Episode Num: 86 Episode T: 10 Reward: 10.000\n","Total T: 1438 Episode Num: 87 Episode T: 12 Reward: 12.000\n","Total T: 1449 Episode Num: 88 Episode T: 11 Reward: 11.000\n","Total T: 1457 Episode Num: 89 Episode T: 8 Reward: 8.000\n","Total T: 1468 Episode Num: 90 Episode T: 11 Reward: 11.000\n","Total T: 1478 Episode Num: 91 Episode T: 10 Reward: 10.000\n","Total T: 1498 Episode Num: 92 Episode T: 20 Reward: 20.000\n","Total T: 1508 Episode Num: 93 Episode T: 10 Reward: 10.000\n","Total T: 1519 Episode Num: 94 Episode T: 11 Reward: 11.000\n","Total T: 1528 Episode Num: 95 Episode T: 9 Reward: 9.000\n","Total T: 1549 Episode Num: 96 Episode T: 21 Reward: 21.000\n","Total T: 1560 Episode Num: 97 Episode T: 11 Reward: 11.000\n","Total T: 1569 Episode Num: 98 Episode T: 9 Reward: 9.000\n","Total T: 1590 Episode Num: 99 Episode T: 21 Reward: 21.000\n","Total T: 1642 Episode Num: 100 Episode T: 52 Reward: 52.000\n","Total T: 1692 Episode Num: 101 Episode T: 50 Reward: 50.000\n","Total T: 1828 Episode Num: 102 Episode T: 136 Reward: 136.000\n","Total T: 1977 Episode Num: 103 Episode T: 149 Reward: 149.000\n","Total T: 2112 Episode Num: 104 Episode T: 135 Reward: 135.000\n","Total T: 2259 Episode Num: 105 Episode T: 147 Reward: 147.000\n","Total T: 2400 Episode Num: 106 Episode T: 141 Reward: 141.000\n","Total T: 2588 Episode Num: 107 Episode T: 188 Reward: 188.000\n","Total T: 2761 Episode Num: 108 Episode T: 173 Reward: 173.000\n","Total T: 2961 Episode Num: 109 Episode T: 200 Reward: 200.000\n","Total T: 3112 Episode Num: 110 Episode T: 151 Reward: 151.000\n","Total T: 3305 Episode Num: 111 Episode T: 193 Reward: 193.000\n","Total T: 3505 Episode Num: 112 Episode T: 200 Reward: 200.000\n","Total T: 3674 Episode Num: 113 Episode T: 169 Reward: 169.000\n","Total T: 3837 Episode Num: 114 Episode T: 163 Reward: 163.000\n","Total T: 4005 Episode Num: 115 Episode T: 168 Reward: 168.000\n","Total T: 4161 Episode Num: 116 Episode T: 156 Reward: 156.000\n","Total T: 4361 Episode Num: 117 Episode T: 200 Reward: 200.000\n","Total T: 4561 Episode Num: 118 Episode T: 200 Reward: 200.000\n","Total T: 4735 Episode Num: 119 Episode T: 174 Reward: 174.000\n","Total T: 4919 Episode Num: 120 Episode T: 184 Reward: 184.000\n","---------------------------------------\n","Evaluation over 10 episodes: 195.400\n","---------------------------------------\n","Total T: 5119 Episode Num: 121 Episode T: 200 Reward: 200.000\n","Total T: 5292 Episode Num: 122 Episode T: 173 Reward: 173.000\n","Total T: 5454 Episode Num: 123 Episode T: 162 Reward: 162.000\n","Total T: 5606 Episode Num: 124 Episode T: 152 Reward: 152.000\n","Total T: 5806 Episode Num: 125 Episode T: 200 Reward: 200.000\n","Total T: 5980 Episode Num: 126 Episode T: 174 Reward: 174.000\n","Total T: 6155 Episode Num: 127 Episode T: 175 Reward: 175.000\n","Total T: 6351 Episode Num: 128 Episode T: 196 Reward: 196.000\n","Total T: 6537 Episode Num: 129 Episode T: 186 Reward: 186.000\n","Total T: 6707 Episode Num: 130 Episode T: 170 Reward: 170.000\n","Total T: 6849 Episode Num: 131 Episode T: 142 Reward: 142.000\n","Total T: 7014 Episode Num: 132 Episode T: 165 Reward: 165.000\n","Total T: 7189 Episode Num: 133 Episode T: 175 Reward: 175.000\n","Total T: 7382 Episode Num: 134 Episode T: 193 Reward: 193.000\n","Total T: 7554 Episode Num: 135 Episode T: 172 Reward: 172.000\n","Total T: 7754 Episode Num: 136 Episode T: 200 Reward: 200.000\n","Total T: 7922 Episode Num: 137 Episode T: 168 Reward: 168.000\n","Total T: 8092 Episode Num: 138 Episode T: 170 Reward: 170.000\n","Total T: 8292 Episode Num: 139 Episode T: 200 Reward: 200.000\n","Total T: 8459 Episode Num: 140 Episode T: 167 Reward: 167.000\n","Total T: 8650 Episode Num: 141 Episode T: 191 Reward: 191.000\n","Total T: 8724 Episode Num: 142 Episode T: 74 Reward: 74.000\n","Total T: 8924 Episode Num: 143 Episode T: 200 Reward: 200.000\n","Total T: 9074 Episode Num: 144 Episode T: 150 Reward: 150.000\n","Total T: 9264 Episode Num: 145 Episode T: 190 Reward: 190.000\n","Total T: 9430 Episode Num: 146 Episode T: 166 Reward: 166.000\n","Total T: 9630 Episode Num: 147 Episode T: 200 Reward: 200.000\n","Total T: 9820 Episode Num: 148 Episode T: 190 Reward: 190.000\n","---------------------------------------\n","Evaluation over 10 episodes: 196.700\n","---------------------------------------\n"]}]},{"cell_type":"markdown","metadata":{"id":"46sQwBLx6m1M"},"source":["## Generate Buffer"]},{"cell_type":"code","metadata":{"id":"5pnM3-jF7sa9"},"source":["# Make env and determine properties\n","env, is_atari, state_dim, num_actions = make_env(args.env, args.atari_preprocessing)\n","parameters = args.atari_parameters if is_atari else args.regular_parameters\n","\n","\n","# Set seeds\n","env.seed(args.seed)\n","env.action_space.seed(args.seed)\n","\n","\n","# Initialize buffer\n","replay_buffer = ReplayBuffer(state_dim, is_atari, args.atari_preprocessing, parameters[\"batch_size\"], parameters[\"buffer_size\"], device)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nTtLegdr6zyd","executionInfo":{"status":"ok","timestamp":1634988961512,"user_tz":-330,"elapsed":15980,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}},"outputId":"991a5e62-6c8f-4243-9442-9b43d024171b"},"source":["setting = f\"{args.env}_{args.seed}\"\n","buffer_name = f\"{args.buffer_name}_{setting}\"\n","\n","# Initialize and load policy\n","policy = DQN(\n","    is_atari,\n","    num_actions,\n","    state_dim,\n","    device,\n","    parameters[\"discount\"],\n","    parameters[\"optimizer\"],\n","    parameters[\"optimizer_parameters\"],\n","    parameters[\"polyak_target_update\"],\n","    parameters[\"target_update_freq\"],\n","    parameters[\"tau\"],\n","    parameters[\"initial_eps\"],\n","    parameters[\"end_eps\"],\n","    parameters[\"eps_decay_period\"],\n","    parameters[\"eval_eps\"],\n",")\n","\n","policy.load(f\"./models/behavioral_{setting}\")\n","\n","evaluations = []\n","\n","state, done = env.reset(), False\n","episode_start = True\n","episode_reward = 0\n","episode_timesteps = 0\n","episode_num = 0\n","low_noise_ep = np.random.uniform(0,1) < args.low_noise_p\n","max_episode_steps = gym.make(args.env)._max_episode_steps\n","\n","# Interact with the environment for max_timesteps\n","for t in range(int(args.max_timesteps)):\n","\n","    episode_timesteps += 1\n","\n","    # If generating the buffer, episode is low noise with p=low_noise_p.\n","    # If policy is low noise, we take random actions with p=eval_eps.\n","    # If the policy is high noise, we take random actions with p=rand_action_p.\n","    if not low_noise_ep and np.random.uniform(0,1) < args.rand_action_p - parameters[\"eval_eps\"]:\n","        action = env.action_space.sample()\n","    else:\n","        action = policy.select_action(np.array(state), eval=True)\n","\n","    # Perform action and log results\n","    next_state, reward, done, info = env.step(action)\n","    episode_reward += reward\n","\n","    # Only consider \"done\" if episode terminates due to failure condition\n","    done_float = float(done) if episode_timesteps < max_episode_steps else 0\n","\n","    # For atari, info[0] = clipped reward, info[1] = done_float\n","    if is_atari:\n","        reward = info[0]\n","        done_float = info[1]\n","        \n","    # Store data in replay buffer\n","    replay_buffer.add(state, action, next_state, reward, done_float, done, episode_start)\n","    state = copy.copy(next_state)\n","    episode_start = False\n","\n","    if done:\n","        # +1 to account for 0 indexing. +0 on ep_timesteps since it will increment +1 even if done=True\n","        print(f\"Total T: {t+1} Episode Num: {episode_num+1} Episode T: {episode_timesteps} Reward: {episode_reward:.3f}\")\n","        # Reset environment\n","        state, done = env.reset(), False\n","        episode_start = True\n","        episode_reward = 0\n","        episode_timesteps = 0\n","        episode_num += 1\n","        low_noise_ep = np.random.uniform(0,1) < args.low_noise_p\n","\n","# Save final buffer and performance\n","evaluations.append(eval_policy(policy, args.env, args.seed))\n","np.save(f\"./results/buffer_performance_{setting}\", evaluations)\n","replay_buffer.save(f\"./buffers/{buffer_name}\")"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Total T: 200 Episode Num: 1 Episode T: 200 Reward: 200.000\n","Total T: 400 Episode Num: 2 Episode T: 200 Reward: 200.000\n","Total T: 600 Episode Num: 3 Episode T: 200 Reward: 200.000\n","Total T: 721 Episode Num: 4 Episode T: 121 Reward: 121.000\n","Total T: 878 Episode Num: 5 Episode T: 157 Reward: 157.000\n","Total T: 1078 Episode Num: 6 Episode T: 200 Reward: 200.000\n","Total T: 1254 Episode Num: 7 Episode T: 176 Reward: 176.000\n","Total T: 1423 Episode Num: 8 Episode T: 169 Reward: 169.000\n","Total T: 1623 Episode Num: 9 Episode T: 200 Reward: 200.000\n","Total T: 1817 Episode Num: 10 Episode T: 194 Reward: 194.000\n","Total T: 2017 Episode Num: 11 Episode T: 200 Reward: 200.000\n","Total T: 2212 Episode Num: 12 Episode T: 195 Reward: 195.000\n","Total T: 2412 Episode Num: 13 Episode T: 200 Reward: 200.000\n","Total T: 2445 Episode Num: 14 Episode T: 33 Reward: 33.000\n","Total T: 2616 Episode Num: 15 Episode T: 171 Reward: 171.000\n","Total T: 2655 Episode Num: 16 Episode T: 39 Reward: 39.000\n","Total T: 2855 Episode Num: 17 Episode T: 200 Reward: 200.000\n","Total T: 2934 Episode Num: 18 Episode T: 79 Reward: 79.000\n","Total T: 3030 Episode Num: 19 Episode T: 96 Reward: 96.000\n","Total T: 3228 Episode Num: 20 Episode T: 198 Reward: 198.000\n","Total T: 3428 Episode Num: 21 Episode T: 200 Reward: 200.000\n","Total T: 3450 Episode Num: 22 Episode T: 22 Reward: 22.000\n","Total T: 3642 Episode Num: 23 Episode T: 192 Reward: 192.000\n","Total T: 3842 Episode Num: 24 Episode T: 200 Reward: 200.000\n","Total T: 3855 Episode Num: 25 Episode T: 13 Reward: 13.000\n","Total T: 4055 Episode Num: 26 Episode T: 200 Reward: 200.000\n","Total T: 4251 Episode Num: 27 Episode T: 196 Reward: 196.000\n","Total T: 4420 Episode Num: 28 Episode T: 169 Reward: 169.000\n","Total T: 4584 Episode Num: 29 Episode T: 164 Reward: 164.000\n","Total T: 4784 Episode Num: 30 Episode T: 200 Reward: 200.000\n","Total T: 4975 Episode Num: 31 Episode T: 191 Reward: 191.000\n","Total T: 5136 Episode Num: 32 Episode T: 161 Reward: 161.000\n","Total T: 5147 Episode Num: 33 Episode T: 11 Reward: 11.000\n","Total T: 5347 Episode Num: 34 Episode T: 200 Reward: 200.000\n","Total T: 5541 Episode Num: 35 Episode T: 194 Reward: 194.000\n","Total T: 5741 Episode Num: 36 Episode T: 200 Reward: 200.000\n","Total T: 5926 Episode Num: 37 Episode T: 185 Reward: 185.000\n","Total T: 6113 Episode Num: 38 Episode T: 187 Reward: 187.000\n","Total T: 6311 Episode Num: 39 Episode T: 198 Reward: 198.000\n","Total T: 6504 Episode Num: 40 Episode T: 193 Reward: 193.000\n","Total T: 6704 Episode Num: 41 Episode T: 200 Reward: 200.000\n","Total T: 6904 Episode Num: 42 Episode T: 200 Reward: 200.000\n","Total T: 7104 Episode Num: 43 Episode T: 200 Reward: 200.000\n","Total T: 7304 Episode Num: 44 Episode T: 200 Reward: 200.000\n","Total T: 7504 Episode Num: 45 Episode T: 200 Reward: 200.000\n","Total T: 7703 Episode Num: 46 Episode T: 199 Reward: 199.000\n","Total T: 7903 Episode Num: 47 Episode T: 200 Reward: 200.000\n","Total T: 8090 Episode Num: 48 Episode T: 187 Reward: 187.000\n","Total T: 8290 Episode Num: 49 Episode T: 200 Reward: 200.000\n","Total T: 8461 Episode Num: 50 Episode T: 171 Reward: 171.000\n","Total T: 8661 Episode Num: 51 Episode T: 200 Reward: 200.000\n","Total T: 8861 Episode Num: 52 Episode T: 200 Reward: 200.000\n","Total T: 9061 Episode Num: 53 Episode T: 200 Reward: 200.000\n","Total T: 9261 Episode Num: 54 Episode T: 200 Reward: 200.000\n","Total T: 9410 Episode Num: 55 Episode T: 149 Reward: 149.000\n","Total T: 9420 Episode Num: 56 Episode T: 10 Reward: 10.000\n","Total T: 9613 Episode Num: 57 Episode T: 193 Reward: 193.000\n","Total T: 9813 Episode Num: 58 Episode T: 200 Reward: 200.000\n","Total T: 9981 Episode Num: 59 Episode T: 168 Reward: 168.000\n","---------------------------------------\n","Evaluation over 10 episodes: 196.700\n","---------------------------------------\n"]}]},{"cell_type":"markdown","metadata":{"id":"BrK18C73qMOU"},"source":["## Discrete BCQ"]},{"cell_type":"code","metadata":{"id":"dBq599JWqciE"},"source":["# Used for Atari\n","class Conv_Q(nn.Module):\n","\tdef __init__(self, frames, num_actions):\n","\t\tsuper(Conv_Q, self).__init__()\n","\t\tself.c1 = nn.Conv2d(frames, 32, kernel_size=8, stride=4)\n","\t\tself.c2 = nn.Conv2d(32, 64, kernel_size=4, stride=2)\n","\t\tself.c3 = nn.Conv2d(64, 64, kernel_size=3, stride=1)\n","\n","\t\tself.q1 = nn.Linear(3136, 512)\n","\t\tself.q2 = nn.Linear(512, num_actions)\n","\n","\t\tself.i1 = nn.Linear(3136, 512)\n","\t\tself.i2 = nn.Linear(512, num_actions)\n","\n","\n","\tdef forward(self, state):\n","\t\tc = F.relu(self.c1(state))\n","\t\tc = F.relu(self.c2(c))\n","\t\tc = F.relu(self.c3(c))\n","\n","\t\tq = F.relu(self.q1(c.reshape(-1, 3136)))\n","\t\ti = F.relu(self.i1(c.reshape(-1, 3136)))\n","\t\ti = self.i2(i)\n","\t\treturn self.q2(q), F.log_softmax(i, dim=1), i"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8Rb4Ok01qgvh"},"source":["# Used for Box2D / Toy problems\n","class FC_Q(nn.Module):\n","\tdef __init__(self, state_dim, num_actions):\n","\t\tsuper(FC_Q, self).__init__()\n","\t\tself.q1 = nn.Linear(state_dim, 256)\n","\t\tself.q2 = nn.Linear(256, 256)\n","\t\tself.q3 = nn.Linear(256, num_actions)\n","\n","\t\tself.i1 = nn.Linear(state_dim, 256)\n","\t\tself.i2 = nn.Linear(256, 256)\n","\t\tself.i3 = nn.Linear(256, num_actions)\t\t\n","\n","\n","\tdef forward(self, state):\n","\t\tq = F.relu(self.q1(state))\n","\t\tq = F.relu(self.q2(q))\n","\n","\t\ti = F.relu(self.i1(state))\n","\t\ti = F.relu(self.i2(i))\n","\t\ti = self.i3(i)\n","\t\treturn self.q3(q), F.log_softmax(i, dim=1), i"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"UHEdtWB0qi21"},"source":["class discrete_BCQ(object):\n","\tdef __init__(\n","\t\tself, \n","\t\tis_atari,\n","\t\tnum_actions,\n","\t\tstate_dim,\n","\t\tdevice,\n","\t\tBCQ_threshold=0.3,\n","\t\tdiscount=0.99,\n","\t\toptimizer=\"Adam\",\n","\t\toptimizer_parameters={},\n","\t\tpolyak_target_update=False,\n","\t\ttarget_update_frequency=8e3,\n","\t\ttau=0.005,\n","\t\tinitial_eps = 1,\n","\t\tend_eps = 0.001,\n","\t\teps_decay_period = 25e4,\n","\t\teval_eps=0.001,\n","\t):\n","\t\n","\t\tself.device = device\n","\n","\t\t# Determine network type\n","\t\tself.Q = Conv_Q(state_dim[0], num_actions).to(self.device) if is_atari else FC_Q(state_dim, num_actions).to(self.device)\n","\t\tself.Q_target = copy.deepcopy(self.Q)\n","\t\tself.Q_optimizer = getattr(torch.optim, optimizer)(self.Q.parameters(), **optimizer_parameters)\n","\n","\t\tself.discount = discount\n","\n","\t\t# Target update rule\n","\t\tself.maybe_update_target = self.polyak_target_update if polyak_target_update else self.copy_target_update\n","\t\tself.target_update_frequency = target_update_frequency\n","\t\tself.tau = tau\n","\n","\t\t# Decay for eps\n","\t\tself.initial_eps = initial_eps\n","\t\tself.end_eps = end_eps\n","\t\tself.slope = (self.end_eps - self.initial_eps) / eps_decay_period\n","\n","\t\t# Evaluation hyper-parameters\n","\t\tself.state_shape = (-1,) + state_dim if is_atari else (-1, state_dim)\n","\t\tself.eval_eps = eval_eps\n","\t\tself.num_actions = num_actions\n","\n","\t\t# Threshold for \"unlikely\" actions\n","\t\tself.threshold = BCQ_threshold\n","\n","\t\t# Number of training iterations\n","\t\tself.iterations = 0\n","\n","\n","\tdef select_action(self, state, eval=False):\n","\t\t# Select action according to policy with probability (1-eps)\n","\t\t# otherwise, select random action\n","\t\tif np.random.uniform(0,1) > self.eval_eps:\n","\t\t\twith torch.no_grad():\n","\t\t\t\tstate = torch.FloatTensor(state).reshape(self.state_shape).to(self.device)\n","\t\t\t\tq, imt, i = self.Q(state)\n","\t\t\t\timt = imt.exp()\n","\t\t\t\timt = (imt/imt.max(1, keepdim=True)[0] > self.threshold).float()\n","\t\t\t\t# Use large negative number to mask actions from argmax\n","\t\t\t\treturn int((imt * q + (1. - imt) * -1e8).argmax(1))\n","\t\telse:\n","\t\t\treturn np.random.randint(self.num_actions)\n","\n","\n","\tdef train(self, replay_buffer):\n","\t\t# Sample replay buffer\n","\t\tstate, action, next_state, reward, done = replay_buffer.sample()\n","\n","\t\t# Compute the target Q value\n","\t\twith torch.no_grad():\n","\t\t\tq, imt, i = self.Q(next_state)\n","\t\t\timt = imt.exp()\n","\t\t\timt = (imt/imt.max(1, keepdim=True)[0] > self.threshold).float()\n","\n","\t\t\t# Use large negative number to mask actions from argmax\n","\t\t\tnext_action = (imt * q + (1 - imt) * -1e8).argmax(1, keepdim=True)\n","\n","\t\t\tq, imt, i = self.Q_target(next_state)\n","\t\t\ttarget_Q = reward + done * self.discount * q.gather(1, next_action).reshape(-1, 1)\n","\n","\t\t# Get current Q estimate\n","\t\tcurrent_Q, imt, i = self.Q(state)\n","\t\tcurrent_Q = current_Q.gather(1, action)\n","\n","\t\t# Compute Q loss\n","\t\tq_loss = F.smooth_l1_loss(current_Q, target_Q)\n","\t\ti_loss = F.nll_loss(imt, action.reshape(-1))\n","\n","\t\tQ_loss = q_loss + i_loss + 1e-2 * i.pow(2).mean()\n","\n","\t\t# Optimize the Q\n","\t\tself.Q_optimizer.zero_grad()\n","\t\tQ_loss.backward()\n","\t\tself.Q_optimizer.step()\n","\n","\t\t# Update target network by polyak or full copy every X iterations.\n","\t\tself.iterations += 1\n","\t\tself.maybe_update_target()\n","\n","\n","\tdef polyak_target_update(self):\n","\t\tfor param, target_param in zip(self.Q.parameters(), self.Q_target.parameters()):\n","\t\t   target_param.data.copy_(self.tau * param.data + (1 - self.tau) * target_param.data)\n","\n","\n","\tdef copy_target_update(self):\n","\t\tif self.iterations % self.target_update_frequency == 0:\n","\t\t\t self.Q_target.load_state_dict(self.Q.state_dict())"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"pvX6nG1h8FaT"},"source":["# Make env and determine properties\n","env, is_atari, state_dim, num_actions = make_env(args.env, args.atari_preprocessing)\n","parameters = args.atari_parameters if is_atari else args.regular_parameters\n","\n","\n","# Set seeds\n","env.seed(args.seed)\n","env.action_space.seed(args.seed)\n","\n","\n","# Initialize buffer\n","replay_buffer = ReplayBuffer(state_dim, is_atari, args.atari_preprocessing, parameters[\"batch_size\"], parameters[\"buffer_size\"], device)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6Gm1z1JW8Bxk","executionInfo":{"status":"ok","timestamp":1634989116620,"user_tz":-330,"elapsed":66537,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}},"outputId":"090c84ac-a153-4680-a87e-bfe9fed9f24e"},"source":["# For saving files\n","setting = f\"{args.env}_{args.seed}\"\n","buffer_name = f\"{args.buffer_name}_{setting}\"\n","\n","# Initialize and load policy\n","policy = discrete_BCQ(\n","    is_atari,\n","    num_actions,\n","    state_dim,\n","    device,\n","    args.BCQ_threshold,\n","    parameters[\"discount\"],\n","    parameters[\"optimizer\"],\n","    parameters[\"optimizer_parameters\"],\n","    parameters[\"polyak_target_update\"],\n","    parameters[\"target_update_freq\"],\n","    parameters[\"tau\"],\n","    parameters[\"initial_eps\"],\n","    parameters[\"end_eps\"],\n","    parameters[\"eps_decay_period\"],\n","    parameters[\"eval_eps\"]\n",")\n","\n","# Load replay buffer\t\n","replay_buffer.load(f\"./buffers/{buffer_name}\")\n","\n","evaluations = []\n","episode_num = 0\n","done = True \n","training_iters = 0\n","\n","while training_iters < args.max_timesteps: \n","    \n","    for _ in range(int(parameters[\"eval_freq\"])):\n","        policy.train(replay_buffer)\n","\n","    evaluations.append(eval_policy(policy, args.env, args.seed))\n","    np.save(f\"./results/BCQ_{setting}\", evaluations)\n","\n","    training_iters += int(parameters[\"eval_freq\"])\n","    print(f\"Training iterations: {training_iters}\")"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Replay Buffer loaded with 10000 elements.\n","---------------------------------------\n","Evaluation over 10 episodes: 175.300\n","---------------------------------------\n","Training iterations: 5000\n","---------------------------------------\n","Evaluation over 10 episodes: 198.500\n","---------------------------------------\n","Training iterations: 10000\n"]}]},{"cell_type":"code","metadata":{"id":"X6UL_5H08QVC"},"source":["!apt-get -qq install tree"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bLap25tR8jon","executionInfo":{"status":"ok","timestamp":1634989150242,"user_tz":-330,"elapsed":461,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}},"outputId":"9a8eff7b-b1dd-46c4-9069-e020307f3716"},"source":["!tree --du -h -C ."],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[01;34m.\u001b[00m\n","├── [864K]  \u001b[01;34mbuffers\u001b[00m\n","│   ├── [ 78K]  Default_CartPole-v0_0_action.npy\n","│   ├── [313K]  Default_CartPole-v0_0_next_state.npy\n","│   ├── [ 78K]  Default_CartPole-v0_0_not_done.npy\n","│   ├── [ 136]  Default_CartPole-v0_0_ptr.npy\n","│   ├── [ 78K]  Default_CartPole-v0_0_reward.npy\n","│   └── [313K]  Default_CartPole-v0_0_state.npy\n","├── [801K]  \u001b[01;34mmodels\u001b[00m\n","│   ├── [531K]  behavioral_CartPole-v0_0_optimizer\n","│   └── [266K]  behavioral_CartPole-v0_0_Q\n","├── [4.4K]  \u001b[01;34mresults\u001b[00m\n","│   ├── [ 144]  BCQ_CartPole-v0_0.npy\n","│   ├── [ 144]  behavioral_CartPole-v0_0.npy\n","│   └── [ 136]  buffer_performance_CartPole-v0_0.npy\n","├── [ 19M]  \u001b[01;34mROM\u001b[00m\n","│   ├── [ 11M]  \u001b[01;31mHC ROMS.zip\u001b[00m\n","│   └── [7.8M]  \u001b[01;31mROMS.zip\u001b[00m\n","├── [ 11M]  Roms.rar\n","├── [ 54M]  \u001b[01;34msample_data\u001b[00m\n","│   ├── [1.7K]  \u001b[01;32manscombe.json\u001b[00m\n","│   ├── [294K]  california_housing_test.csv\n","│   ├── [1.6M]  california_housing_train.csv\n","│   ├── [ 17M]  mnist_test.csv\n","│   ├── [ 35M]  mnist_train_small.csv\n","│   └── [ 930]  \u001b[01;32mREADME.md\u001b[00m\n","└── [ 65K]  \u001b[01;34mvideo\u001b[00m\n","    ├── [ 491]  openaigym.episode_batch.8.2302.stats.json\n","    ├── [ 406]  openaigym.manifest.8.2302.manifest.json\n","    ├── [2.0K]  openaigym.video.8.2302.video000000.meta.json\n","    ├── [ 18K]  openaigym.video.8.2302.video000000.mp4\n","    ├── [2.0K]  openaigym.video.8.2302.video000001.meta.json\n","    ├── [ 18K]  openaigym.video.8.2302.video000001.mp4\n","    ├── [2.0K]  openaigym.video.8.2302.video000008.meta.json\n","    └── [ 19K]  openaigym.video.8.2302.video000008.mp4\n","\n","  86M used in 6 directories, 28 files\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":421},"id":"l4Cx77m-8mJE","executionInfo":{"status":"ok","timestamp":1634989208793,"user_tz":-330,"elapsed":661,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}},"outputId":"ae4aedc8-82cd-48a8-83ac-61ff2820e765"},"source":["show_video()"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/html":["<video alt=\"test\" autoplay \n","                loop controls style=\"height: 400px;\">\n","                <source src=\"data:video/mp4;base64,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\" type=\"video/mp4\" />\n","             </video>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}}]}]}