{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"2022-01-26-tagnn.ipynb","provenance":[{"file_id":"https://github.com/recohut/nbs/blob/main/raw/T355636%20%7C%20TAGNN%20Session-based%20Recommendation%20on%20Sample%20dataset.ipynb","timestamp":1644673795612},{"file_id":"1buphX5uNmwXsebanVqUfOxSP1MkUBXsx","timestamp":1637912183864}],"collapsed_sections":[],"toc_visible":true,"authorship_tag":"ABX9TyMKbpuWoIRPobesV2fnQrtu"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"c7xKnhAxAf_6"},"source":["# TAGNN Session-based Recommendation on Sample dataset"]},{"cell_type":"markdown","metadata":{"id":"mf9MP8VFvJZo"},"source":["## Setup"]},{"cell_type":"code","metadata":{"id":"oTkBob7DTs2L"},"source":["import torch\n","from torch import nn\n","from torch.nn import Module, Parameter\n","import torch.nn.functional as F\n","\n","import pickle\n","import time\n","import networkx as nx\n","import numpy as np\n","import datetime\n","import math\n","import csv\n","import operator\n","import os"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Uk_7AdvECZxr"},"source":["import warnings\n","warnings.filterwarnings('ignore')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"yuc8-bSu0VqP"},"source":["## Dataset"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Ri6_IVOZOZdd","executionInfo":{"status":"ok","timestamp":1638342519154,"user_tz":-330,"elapsed":1049,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}},"outputId":"613c925f-478e-4d7f-f8bf-fd1f45ca47c1"},"source":["!wget -q --show-progress https://github.com/RecoHut-Datasets/sample_session/raw/v1/sample_train-item-views.csv"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\r          sample_tr   0%[                    ]       0  --.-KB/s               \rsample_train-item-v 100%[===================>] 386.09K  --.-KB/s    in 0.03s   \n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rlCqxESlSOLe","executionInfo":{"status":"ok","timestamp":1638342519156,"user_tz":-330,"elapsed":28,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}},"outputId":"7d051580-52fc-4480-b3f9-e5f475fcbb26"},"source":["!head sample_train-item-views.csv"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["session_id;user_id;item_id;timeframe;eventdate\n","1;NA;81766;526309;2016-05-09\n","1;NA;31331;1031018;2016-05-09\n","1;NA;32118;243569;2016-05-09\n","1;NA;9654;75848;2016-05-09\n","1;NA;32627;1112408;2016-05-09\n","1;NA;33043;173912;2016-05-09\n","1;NA;12352;329870;2016-05-09\n","1;NA;35077;390072;2016-05-09\n","1;NA;36118;487369;2016-05-09\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WkbOArguST-H","executionInfo":{"status":"ok","timestamp":1638342519723,"user_tz":-330,"elapsed":582,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}},"outputId":"45df9c5a-7af5-49cb-9190-cbe0bca13527"},"source":["print(\"-- Starting @ %ss\" % datetime.datetime.now())\n","\n","dataset = 'sample_train-item-views.csv'\n","\n","with open(dataset, \"r\") as f:\n","    reader = csv.DictReader(f, delimiter=';')\n","    sess_clicks = {}\n","    sess_date = {}\n","    ctr = 0\n","    curid = -1\n","    curdate = None\n","    for data in reader:\n","        sessid = data['session_id']\n","        if curdate and not curid == sessid:\n","            date = ''\n","            date = time.mktime(time.strptime(curdate, '%Y-%m-%d'))\n","            sess_date[curid] = date\n","        curid = sessid\n","        item = data['item_id'], int(data['timeframe'])\n","        curdate = ''\n","        curdate = data['eventdate']\n","\n","        if sessid in sess_clicks:\n","            sess_clicks[sessid] += [item]\n","        else:\n","            sess_clicks[sessid] = [item]\n","        ctr += 1\n","    date = ''\n","    date = time.mktime(time.strptime(curdate, '%Y-%m-%d'))\n","    for i in list(sess_clicks):\n","        sorted_clicks = sorted(sess_clicks[i], key=operator.itemgetter(1))\n","        sess_clicks[i] = [c[0] for c in sorted_clicks]\n","    sess_date[curid] = date\n","\n","print(\"-- Reading data @ %ss\" % datetime.datetime.now())\n","\n","# Filter out length 1 sessions\n","for s in list(sess_clicks):\n","    if len(sess_clicks[s]) == 1:\n","        del sess_clicks[s]\n","        del sess_date[s]\n","\n","# Count number of times each item appears\n","iid_counts = {}\n","for s in sess_clicks:\n","    seq = sess_clicks[s]\n","    for iid in seq:\n","        if iid in iid_counts:\n","            iid_counts[iid] += 1\n","        else:\n","            iid_counts[iid] = 1\n","\n","sorted_counts = sorted(iid_counts.items(), key=operator.itemgetter(1))\n","\n","length = len(sess_clicks)\n","for s in list(sess_clicks):\n","    curseq = sess_clicks[s]\n","    filseq = list(filter(lambda i: iid_counts[i] >= 5, curseq))\n","    if len(filseq) < 2:\n","        del sess_clicks[s]\n","        del sess_date[s]\n","    else:\n","        sess_clicks[s] = filseq\n","\n","# Split out test set based on dates\n","dates = list(sess_date.items())\n","maxdate = dates[0][1]\n","\n","for _, date in dates:\n","    if maxdate < date:\n","        maxdate = date\n","\n","# 7 days for test\n","splitdate = 0\n","splitdate = maxdate - 86400 * 7\n","\n","print('Splitting date', splitdate)      # Yoochoose: ('Split date', 1411930799.0)\n","tra_sess = filter(lambda x: x[1] < splitdate, dates)\n","tes_sess = filter(lambda x: x[1] > splitdate, dates)\n","\n","# Sort sessions by date\n","tra_sess = sorted(tra_sess, key=operator.itemgetter(1))     # [(session_id, timestamp), (), ]\n","tes_sess = sorted(tes_sess, key=operator.itemgetter(1))     # [(session_id, timestamp), (), ]\n","\n","print(len(tra_sess))    # 186670    # 7966257\n","print(len(tes_sess))    # 15979     # 15324\n","print(tra_sess[:3])\n","print(tes_sess[:3])\n","\n","print(\"-- Splitting train set and test set @ %ss\" % datetime.datetime.now())\n","\n","\n","# Choosing item count >=5 gives approximately the same number of items as reported in paper\n","item_dict = {}\n","# Convert training sessions to sequences and renumber items to start from 1\n","def obtian_tra():\n","    train_ids = []\n","    train_seqs = []\n","    train_dates = []\n","    item_ctr = 1\n","    for s, date in tra_sess:\n","        seq = sess_clicks[s]\n","        outseq = []\n","        for i in seq:\n","            if i in item_dict:\n","                outseq += [item_dict[i]]\n","            else:\n","                outseq += [item_ctr]\n","                item_dict[i] = item_ctr\n","                item_ctr += 1\n","        if len(outseq) < 2:  # Doesn't occur\n","            continue\n","        train_ids += [s]\n","        train_dates += [date]\n","        train_seqs += [outseq]\n","    print(item_ctr)     # 43098, 37484\n","    return train_ids, train_dates, train_seqs\n","\n","\n","# Convert test sessions to sequences, ignoring items that do not appear in training set\n","def obtian_tes():\n","    test_ids = []\n","    test_seqs = []\n","    test_dates = []\n","    for s, date in tes_sess:\n","        seq = sess_clicks[s]\n","        outseq = []\n","        for i in seq:\n","            if i in item_dict:\n","                outseq += [item_dict[i]]\n","        if len(outseq) < 2:\n","            continue\n","        test_ids += [s]\n","        test_dates += [date]\n","        test_seqs += [outseq]\n","    return test_ids, test_dates, test_seqs\n","\n","\n","tra_ids, tra_dates, tra_seqs = obtian_tra()\n","tes_ids, tes_dates, tes_seqs = obtian_tes()\n","\n","\n","def process_seqs(iseqs, idates):\n","    out_seqs = []\n","    out_dates = []\n","    labs = []\n","    ids = []\n","    for id, seq, date in zip(range(len(iseqs)), iseqs, idates):\n","        for i in range(1, len(seq)):\n","            tar = seq[-i]\n","            labs += [tar]\n","            out_seqs += [seq[:-i]]\n","            out_dates += [date]\n","            ids += [id]\n","    return out_seqs, out_dates, labs, ids\n","\n","\n","tr_seqs, tr_dates, tr_labs, tr_ids = process_seqs(tra_seqs, tra_dates)\n","te_seqs, te_dates, te_labs, te_ids = process_seqs(tes_seqs, tes_dates)\n","tra = (tr_seqs, tr_labs)\n","tes = (te_seqs, te_labs)\n","\n","print(len(tr_seqs))\n","print(len(te_seqs))\n","print(tr_seqs[:3], tr_dates[:3], tr_labs[:3])\n","print(te_seqs[:3], te_dates[:3], te_labs[:3])\n","\n","all = 0\n","\n","for seq in tra_seqs:\n","    all += len(seq)\n","for seq in tes_seqs:\n","    all += len(seq)\n","print('avg length: ', all/(len(tra_seqs) + len(tes_seqs) * 1.0))\n","\n","pickle.dump(tra, open('train.txt', 'wb'))\n","pickle.dump(tes, open('test.txt', 'wb'))\n","pickle.dump(tra_seqs, open('all_train_seq.txt', 'wb'))\n","\n","print('Done.')"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["-- Starting @ 2021-12-01 07:08:41.814918s\n","-- Reading data @ 2021-12-01 07:08:41.972016s\n","Splitting date 1464134400.0\n","469\n","47\n","[('2671', 1451952000.0), ('1211', 1452384000.0), ('3780', 1452384000.0)]\n","[('1864', 1464220800.0), ('1867', 1464220800.0), ('1868', 1464220800.0)]\n","-- Splitting train set and test set @ 2021-12-01 07:08:41.985112s\n","310\n","1205\n","99\n","[[1, 2], [1], [4]] [1451952000.0, 1451952000.0, 1452384000.0] [3, 2, 5]\n","[[282], [281, 308], [281]] [1464220800.0, 1464220800.0, 1464220800.0] [282, 281, 308]\n","avg length:  3.5669291338582676\n","Done.\n"]}]},{"cell_type":"code","metadata":{"id":"BH4N35xr90o9"},"source":["def data_masks(all_usr_pois, item_tail):\n","    us_lens = [len(upois) for upois in all_usr_pois]\n","    len_max = max(us_lens)\n","    us_pois = [upois + item_tail * (len_max - le) for upois, le in zip(all_usr_pois, us_lens)]\n","    us_msks = [[1] * le + [0] * (len_max - le) for le in us_lens]\n","    return us_pois, us_msks, len_max"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qZNFztyz0YRU"},"source":["def split_validation(train_set, valid_portion):\n","    train_set_x, train_set_y = train_set\n","    n_samples = len(train_set_x)\n","    sidx = np.arange(n_samples, dtype='int32')\n","    np.random.shuffle(sidx)\n","    n_train = int(np.round(n_samples * (1. - valid_portion)))\n","    valid_set_x = [train_set_x[s] for s in sidx[n_train:]]\n","    valid_set_y = [train_set_y[s] for s in sidx[n_train:]]\n","    train_set_x = [train_set_x[s] for s in sidx[:n_train]]\n","    train_set_y = [train_set_y[s] for s in sidx[:n_train]]\n","\n","    return (train_set_x, train_set_y), (valid_set_x, valid_set_y)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"R8teMyqv0ZwU"},"source":["class Data():\n","    def __init__(self, data, shuffle=False, graph=None):\n","        inputs = data[0]\n","        inputs, mask, len_max = data_masks(inputs, [0])\n","        self.inputs = np.asarray(inputs)\n","        self.mask = np.asarray(mask)\n","        self.len_max = len_max\n","        self.targets = np.asarray(data[1])\n","        self.length = len(inputs)\n","        self.shuffle = shuffle\n","        self.graph = graph\n","\n","    def generate_batch(self, batch_size):\n","        if self.shuffle:\n","            shuffled_arg = np.arange(self.length)\n","            np.random.shuffle(shuffled_arg)\n","            self.inputs = self.inputs[shuffled_arg]\n","            self.mask = self.mask[shuffled_arg]\n","            self.targets = self.targets[shuffled_arg]\n","        n_batch = int(self.length / batch_size)\n","        if self.length % batch_size != 0:\n","            n_batch += 1\n","        slices = np.split(np.arange(n_batch * batch_size), n_batch)\n","        slices[-1] = slices[-1][:(self.length - batch_size * (n_batch - 1))]\n","        return slices\n","\n","    def get_slice(self, i):\n","        inputs, mask, targets = self.inputs[i], self.mask[i], self.targets[i]\n","        items, n_node, A, alias_inputs = [], [], [], []\n","        for u_input in inputs:\n","            n_node.append(len(np.unique(u_input)))\n","        max_n_node = np.max(n_node)\n","        for u_input in inputs:\n","            node = np.unique(u_input)\n","            items.append(node.tolist() + (max_n_node - len(node)) * [0])\n","            u_A = np.zeros((max_n_node, max_n_node))\n","            for i in np.arange(len(u_input) - 1):\n","                if u_input[i + 1] == 0:\n","                    break\n","                u = np.where(node == u_input[i])[0][0]\n","                v = np.where(node == u_input[i + 1])[0][0]\n","                u_A[u][v] = 1\n","            u_sum_in = np.sum(u_A, 0)\n","            u_sum_in[np.where(u_sum_in == 0)] = 1\n","            u_A_in = np.divide(u_A, u_sum_in)\n","            u_sum_out = np.sum(u_A, 1)\n","            u_sum_out[np.where(u_sum_out == 0)] = 1\n","            u_A_out = np.divide(u_A.transpose(), u_sum_out)\n","            u_A = np.concatenate([u_A_in, u_A_out]).transpose()\n","            A.append(u_A)\n","            alias_inputs.append([np.where(node == i)[0][0] for i in u_input])\n","        return alias_inputs, A, items, mask, targets"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"bh9kL1iM0r3g"},"source":["## Model"]},{"cell_type":"code","metadata":{"id":"bQaHam5ALl_E"},"source":["class GNN(Module):\n","    def __init__(self, hidden_size, step=1):\n","        super(GNN, self).__init__()\n","        self.step = step\n","        self.hidden_size = hidden_size\n","        self.input_size = hidden_size * 2\n","        self.gate_size = 3 * hidden_size\n","        self.w_ih = Parameter(torch.Tensor(self.gate_size, self.input_size))\n","        self.w_hh = Parameter(torch.Tensor(self.gate_size, self.hidden_size))\n","        self.b_ih = Parameter(torch.Tensor(self.gate_size))\n","        self.b_hh = Parameter(torch.Tensor(self.gate_size))\n","        self.b_iah = Parameter(torch.Tensor(self.hidden_size))\n","        self.b_oah = Parameter(torch.Tensor(self.hidden_size))\n","\n","        self.linear_edge_in = nn.Linear(self.hidden_size, self.hidden_size, bias=True)\n","        self.linear_edge_out = nn.Linear(self.hidden_size, self.hidden_size, bias=True)\n","        self.linear_edge_f = nn.Linear(self.hidden_size, self.hidden_size, bias=True)\n","\n","    def GNNCell(self, A, hidden):\n","        input_in = torch.matmul(A[:, :, :A.shape[1]], self.linear_edge_in(hidden)) + self.b_iah\n","        input_out = torch.matmul(A[:, :, A.shape[1]: 2 * A.shape[1]], self.linear_edge_out(hidden)) + self.b_oah\n","        inputs = torch.cat([input_in, input_out], 2)\n","        gi = F.linear(inputs, self.w_ih, self.b_ih)\n","        gh = F.linear(hidden, self.w_hh, self.b_hh)\n","        i_r, i_i, i_n = gi.chunk(3, 2)\n","        h_r, h_i, h_n = gh.chunk(3, 2)\n","        resetgate = torch.sigmoid(i_r + h_r)\n","        inputgate = torch.sigmoid(i_i + h_i)\n","        newgate = torch.tanh(i_n + resetgate * h_n)\n","        hy = newgate + inputgate * (hidden - newgate)\n","        return hy\n","\n","    def forward(self, A, hidden):\n","        for i in range(self.step):\n","            hidden = self.GNNCell(A, hidden)\n","        return hidden"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1UBHFlJW0c1P"},"source":["class SessionGraph(Module):\n","    def __init__(self, opt, n_node):\n","        super(SessionGraph, self).__init__()\n","        self.hidden_size = opt.hiddenSize\n","        self.n_node = n_node\n","        self.batch_size = opt.batchSize\n","        self.nonhybrid = opt.nonhybrid\n","        self.embedding = nn.Embedding(self.n_node, self.hidden_size)\n","        self.gnn = GNN(self.hidden_size, step=opt.step)\n","        self.linear_one = nn.Linear(self.hidden_size, self.hidden_size, bias=True)\n","        self.linear_two = nn.Linear(self.hidden_size, self.hidden_size, bias=True)\n","        self.linear_three = nn.Linear(self.hidden_size, 1, bias=False)\n","        self.linear_transform = nn.Linear(self.hidden_size * 2, self.hidden_size, bias=True)\n","        self.linear_t = nn.Linear(self.hidden_size, self.hidden_size, bias=False)  #target attention\n","        self.loss_function = nn.CrossEntropyLoss()\n","        self.optimizer = torch.optim.Adam(self.parameters(), lr=opt.lr, weight_decay=opt.l2)\n","        self.scheduler = torch.optim.lr_scheduler.StepLR(self.optimizer, step_size=opt.lr_dc_step, gamma=opt.lr_dc)\n","        self.reset_parameters()\n","\n","    def reset_parameters(self):\n","        stdv = 1.0 / math.sqrt(self.hidden_size)\n","        for weight in self.parameters():\n","            weight.data.uniform_(-stdv, stdv)\n","\n","    def compute_scores(self, hidden, mask):\n","        ht = hidden[torch.arange(mask.shape[0]).long(), torch.sum(mask, 1) - 1]  # batch_size x latent_size\n","        q1 = self.linear_one(ht).view(ht.shape[0], 1, ht.shape[1])  # batch_size x 1 x latent_size\n","        q2 = self.linear_two(hidden)  # batch_size x seq_length x latent_size\n","        alpha = self.linear_three(torch.sigmoid(q1 + q2))  # (b,s,1)\n","        # alpha = torch.sigmoid(alpha) # B,S,1\n","        alpha = F.softmax(alpha, 1) # B,S,1\n","        a = torch.sum(alpha * hidden * mask.view(mask.shape[0], -1, 1).float(), 1)  # (b,d)\n","        if not self.nonhybrid:\n","            a = self.linear_transform(torch.cat([a, ht], 1))\n","        b = self.embedding.weight[1:]  # n_nodes x latent_size\n","        # target attention: sigmoid(hidden M b)\n","        # mask  # batch_size x seq_length\n","        hidden = hidden * mask.view(mask.shape[0], -1, 1).float()  # batch_size x seq_length x latent_size\n","        qt = self.linear_t(hidden)  # batch_size x seq_length x latent_size\n","        # beta = torch.sigmoid(b @ qt.transpose(1,2))  # batch_size x n_nodes x seq_length\n","        beta = F.softmax(b @ qt.transpose(1,2), -1)  # batch_size x n_nodes x seq_length\n","        target = beta @ hidden  # batch_size x n_nodes x latent_size\n","        a = a.view(ht.shape[0], 1, ht.shape[1])  # b,1,d\n","        a = a + target  # b,n,d\n","        scores = torch.sum(a * b, -1)  # b,n\n","        # scores = torch.matmul(a, b.transpose(1, 0))\n","        return scores\n","\n","    def forward(self, inputs, A):\n","        hidden = self.embedding(inputs)\n","        hidden = self.gnn(A, hidden)\n","        return hidden"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"IIzp8Hxm0hoH"},"source":["def trans_to_cuda(variable):\n","    if torch.cuda.is_available():\n","        return variable.cuda()\n","    else:\n","        return variable\n","\n","\n","def trans_to_cpu(variable):\n","    if torch.cuda.is_available():\n","        return variable.cpu()\n","    else:\n","        return variable\n","\n","\n","def forward(model, i, data):\n","    alias_inputs, A, items, mask, targets = data.get_slice(i)\n","    alias_inputs = trans_to_cuda(torch.Tensor(alias_inputs).long())\n","    items = trans_to_cuda(torch.Tensor(items).long())\n","    A = trans_to_cuda(torch.Tensor(A).float())\n","    mask = trans_to_cuda(torch.Tensor(mask).long())\n","    hidden = model(items, A)\n","    get = lambda i: hidden[i][alias_inputs[i]]\n","    seq_hidden = torch.stack([get(i) for i in torch.arange(len(alias_inputs)).long()])\n","    return targets, model.compute_scores(seq_hidden, mask)\n","\n","\n","def train_test(model, train_data, test_data):\n","    model.scheduler.step()\n","    print('start training: ', datetime.datetime.now())\n","    model.train()\n","    total_loss = 0.0\n","    slices = train_data.generate_batch(model.batch_size)\n","    for i, j in zip(slices, np.arange(len(slices))):\n","        model.optimizer.zero_grad()\n","        targets, scores = forward(model, i, train_data)\n","        targets = trans_to_cuda(torch.Tensor(targets).long())\n","        loss = model.loss_function(scores, targets - 1)\n","        loss.backward()\n","        model.optimizer.step()\n","        total_loss += loss.item()\n","        if j % int(len(slices) / 5 + 1) == 0:\n","            print('[%d/%d] Loss: %.4f' % (j, len(slices), loss.item()))\n","    print('\\tLoss:\\t%.3f' % total_loss)\n","\n","    print('start predicting: ', datetime.datetime.now())\n","    model.eval()\n","    hit, mrr = [], []\n","    slices = test_data.generate_batch(model.batch_size)\n","    for i in slices:\n","        targets, scores = forward(model, i, test_data)\n","        sub_scores = scores.topk(20)[1]\n","        sub_scores = trans_to_cpu(sub_scores).detach().numpy()\n","        for score, target, mask in zip(sub_scores, targets, test_data.mask):\n","            hit.append(np.isin(target - 1, score))\n","            if len(np.where(score == target - 1)[0]) == 0:\n","                mrr.append(0)\n","            else:\n","                mrr.append(1 / (np.where(score == target - 1)[0][0] + 1))\n","    hit = np.mean(hit) * 100\n","    mrr = np.mean(mrr) * 100\n","    return hit, mrr"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MO5yEePq9ZtY"},"source":["class Args():\n","    dataset = 'sample'\n","    batchSize = 100 # input batch size\n","    hiddenSize = 100 # hidden state size\n","    epoch = 30 # the number of epochs to train for\n","    lr = 0.001 # learning rate')  # [0.001, 0.0005, 0.000\n","    lr_dc = 0.1 # learning rate decay rate\n","    lr_dc_step = 3 # the number of steps after which the learning rate decay\n","    l2 = 1e-5 # l2 penalty')  # [0.001, 0.0005, 0.0001, 0.00005, 0.0000\n","    step = 1 # gnn propogation steps\n","    patience = 10 # the number of epoch to wait before early stop \n","    nonhybrid = True # only use the global preference to predict\n","    validation = True # validation\n","    valid_portion = 0.1 # split the portion of training set as validation set\n","\n","args = Args()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"uCfErpBd9ZrY","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1638342827307,"user_tz":-330,"elapsed":17589,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}},"outputId":"ec3e0753-6ab8-4390-dc43-12936a1349ca"},"source":["train_data = pickle.load(open('train.txt', 'rb'))\n","\n","if args.validation:\n","    train_data, valid_data = split_validation(train_data, args.valid_portion)\n","    test_data = valid_data\n","else:\n","    test_data = pickle.load(open('test.txt', 'rb'))\n","\n","train_data = Data(train_data, shuffle=True)\n","test_data = Data(test_data, shuffle=False)\n","\n","n_node = 310\n","\n","model = trans_to_cuda(SessionGraph(args, n_node))\n","\n","start = time.time()\n","best_result = [0, 0]\n","best_epoch = [0, 0]\n","bad_counter = 0\n","\n","for epoch in range(args.epoch):\n","    print('-------------------------------------------------------')\n","    print('epoch: ', epoch)\n","    hit, mrr = train_test(model, train_data, test_data)\n","    flag = 0\n","    if hit >= best_result[0]:\n","        best_result[0] = hit\n","        best_epoch[0] = epoch\n","        flag = 1\n","    if mrr >= best_result[1]:\n","        best_result[1] = mrr\n","        best_epoch[1] = epoch\n","        flag = 1\n","    print('Best Result:')\n","    print('\\tRecall@20:\\t%.4f\\tMMR@20:\\t%.4f\\tEpoch:\\t%d,\\t%d'% (best_result[0], best_result[1], best_epoch[0], best_epoch[1]))\n","    bad_counter += 1 - flag\n","    if bad_counter >= args.patience:\n","        break\n","print('-------------------------------------------------------')\n","end = time.time()\n","print(\"Run time: %f s\" % (end - start))"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["-------------------------------------------------------\n","epoch:  0\n","start training:  2021-12-01 07:13:41.607073\n","[0/11] Loss: 5.7073\n","[3/11] Loss: 5.7068\n","[6/11] Loss: 5.7014\n","[9/11] Loss: 5.6999\n","\tLoss:\t62.727\n","start predicting:  2021-12-01 07:13:42.496269\n","Best Result:\n","\tRecall@20:\t59.5041\tMMR@20:\t41.1035\tEpoch:\t0,\t0\n","-------------------------------------------------------\n","epoch:  1\n","start training:  2021-12-01 07:13:42.549178\n","[0/11] Loss: 5.6888\n","[3/11] Loss: 5.6861\n","[6/11] Loss: 5.6607\n","[9/11] Loss: 5.6585\n","\tLoss:\t62.418\n","start predicting:  2021-12-01 07:13:43.197435\n","Best Result:\n","\tRecall@20:\t59.5041\tMMR@20:\t41.1035\tEpoch:\t0,\t0\n","-------------------------------------------------------\n","epoch:  2\n","start training:  2021-12-01 07:13:43.244367\n","[0/11] Loss: 5.6426\n","[3/11] Loss: 5.6428\n","[6/11] Loss: 5.6577\n","[9/11] Loss: 5.6456\n","\tLoss:\t62.139\n","start predicting:  2021-12-01 07:13:43.899614\n","Best Result:\n","\tRecall@20:\t59.5041\tMMR@20:\t41.1035\tEpoch:\t0,\t0\n","-------------------------------------------------------\n","epoch:  3\n","start training:  2021-12-01 07:13:43.943481\n","[0/11] Loss: 5.6509\n","[3/11] Loss: 5.6631\n","[6/11] Loss: 5.6420\n","[9/11] Loss: 5.6374\n","\tLoss:\t62.064\n","start predicting:  2021-12-01 07:13:44.593958\n","Best Result:\n","\tRecall@20:\t59.5041\tMMR@20:\t41.1035\tEpoch:\t0,\t0\n","-------------------------------------------------------\n","epoch:  4\n","start training:  2021-12-01 07:13:44.637298\n","[0/11] Loss: 5.6482\n","[3/11] Loss: 5.6230\n","[6/11] Loss: 5.6163\n","[9/11] Loss: 5.6295\n","\tLoss:\t61.993\n","start predicting:  2021-12-01 07:13:45.294765\n","Best Result:\n","\tRecall@20:\t59.5041\tMMR@20:\t41.1035\tEpoch:\t0,\t0\n","-------------------------------------------------------\n","epoch:  5\n","start training:  2021-12-01 07:13:45.350388\n","[0/11] Loss: 5.6179\n","[3/11] Loss: 5.6674\n","[6/11] Loss: 5.6390\n","[9/11] Loss: 5.6204\n","\tLoss:\t61.942\n","start predicting:  2021-12-01 07:13:45.992867\n","Best Result:\n","\tRecall@20:\t59.5041\tMMR@20:\t41.1035\tEpoch:\t0,\t0\n","-------------------------------------------------------\n","epoch:  6\n","start training:  2021-12-01 07:13:46.035217\n","[0/11] Loss: 5.6026\n","[3/11] Loss: 5.6169\n","[6/11] Loss: 5.6576\n","[9/11] Loss: 5.6292\n","\tLoss:\t61.929\n","start predicting:  2021-12-01 07:13:46.718760\n","Best Result:\n","\tRecall@20:\t59.5041\tMMR@20:\t41.1035\tEpoch:\t0,\t0\n","-------------------------------------------------------\n","epoch:  7\n","start training:  2021-12-01 07:13:46.761229\n","[0/11] Loss: 5.6529\n","[3/11] Loss: 5.6556\n","[6/11] Loss: 5.6333\n","[9/11] Loss: 5.6431\n","\tLoss:\t61.924\n","start predicting:  2021-12-01 07:13:47.411988\n","Best Result:\n","\tRecall@20:\t59.5041\tMMR@20:\t41.1035\tEpoch:\t0,\t0\n","-------------------------------------------------------\n","epoch:  8\n","start training:  2021-12-01 07:13:47.454573\n","[0/11] Loss: 5.6367\n","[3/11] Loss: 5.6163\n","[6/11] Loss: 5.6375\n","[9/11] Loss: 5.6401\n","\tLoss:\t61.915\n","start predicting:  2021-12-01 07:13:48.119518\n","Best Result:\n","\tRecall@20:\t59.5041\tMMR@20:\t41.1035\tEpoch:\t0,\t0\n","-------------------------------------------------------\n","epoch:  9\n","start training:  2021-12-01 07:13:48.167828\n","[0/11] Loss: 5.6224\n","[3/11] Loss: 5.6403\n","[6/11] Loss: 5.6184\n","[9/11] Loss: 5.6328\n","\tLoss:\t61.917\n","start predicting:  2021-12-01 07:13:48.833919\n","Best Result:\n","\tRecall@20:\t59.5041\tMMR@20:\t41.1035\tEpoch:\t0,\t0\n","-------------------------------------------------------\n","epoch:  10\n","start training:  2021-12-01 07:13:48.880490\n","[0/11] Loss: 5.6165\n","[3/11] Loss: 5.6363\n","[6/11] Loss: 5.6206\n","[9/11] Loss: 5.6014\n","\tLoss:\t61.920\n","start predicting:  2021-12-01 07:13:49.538163\n","Best Result:\n","\tRecall@20:\t59.5041\tMMR@20:\t41.1035\tEpoch:\t0,\t0\n","-------------------------------------------------------\n","Run time: 7.979653 s\n"]}]},{"cell_type":"markdown","metadata":{"id":"DuDoPVTyU6Ku"},"source":["---"]},{"cell_type":"code","metadata":{"id":"Z3MdwS_RU6Kv","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1638342844510,"user_tz":-330,"elapsed":6113,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}},"outputId":"0896c375-f014-4031-b011-0ca767764408"},"source":["!apt-get -qq install tree\n","!rm -r sample_data"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Selecting previously unselected package tree.\n","(Reading database ... 155222 files and directories currently installed.)\n","Preparing to unpack .../tree_1.7.0-5_amd64.deb ...\n","Unpacking tree (1.7.0-5) ...\n","Setting up tree (1.7.0-5) ...\n","Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n"]}]},{"cell_type":"code","metadata":{"id":"JlRTFfEXU6Kw","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1638342845230,"user_tz":-330,"elapsed":728,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}},"outputId":"d7f6c4f7-ad00-4dc5-973e-dc405c895de5"},"source":["!tree -h --du ."],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":[".\n","├── [6.4K]  all_train_seq.txt\n","├── [386K]  sample_train-item-views.csv\n","├── [1.2K]  test.txt\n","└── [ 18K]  train.txt\n","\n"," 416K used in 0 directories, 4 files\n"]}]},{"cell_type":"code","metadata":{"id":"lR0es6P7U6Kw","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1638342877862,"user_tz":-330,"elapsed":3569,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}},"outputId":"5e483e5d-e445-4198-bfbf-6d79bb807b6f"},"source":["!pip install -q watermark\n","%reload_ext watermark\n","%watermark -a \"Sparsh A.\" -m -iv -u -t -d"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Author: Sparsh A.\n","\n","Last updated: 2021-12-01 07:14:40\n","\n","Compiler    : GCC 7.5.0\n","OS          : Linux\n","Release     : 5.4.104+\n","Machine     : x86_64\n","Processor   : x86_64\n","CPU cores   : 2\n","Architecture: 64bit\n","\n","networkx: 2.6.3\n","torch   : 1.10.0+cu111\n","IPython : 5.5.0\n","numpy   : 1.19.5\n","csv     : 1.0\n","\n"]}]},{"cell_type":"markdown","metadata":{"id":"VpW4NFdUU6Kw"},"source":["---"]},{"cell_type":"markdown","metadata":{"id":"izsndoKwU6Kx"},"source":["**END**"]}]}