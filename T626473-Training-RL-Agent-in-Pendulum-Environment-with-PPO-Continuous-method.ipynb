{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"T626473 | Training RL Agent in Pendulum Environment with PPO Continuous method","provenance":[{"file_id":"1Uy9HVE3IdljpGYBZLjSeWHi6ePRnB1Ys","timestamp":1638449179447},{"file_id":"1yNQHhhvOVs_VM48DN7r6K5em_RaSLllG","timestamp":1638448522366},{"file_id":"1PG1q81afxiJCXPZbQKhgfgSBbDx9HITY","timestamp":1638447916295},{"file_id":"11_5lxJoi19PUPuTzS5RBlSitYErAUY3h","timestamp":1638447606203}],"collapsed_sections":[],"authorship_tag":"ABX9TyPczYCS3+ML/milEHH8w2BS"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"tyMytwWZFUhp"},"source":["# Training RL Agent in Pendulum Environment with PPO Continuous method"]},{"cell_type":"code","metadata":{"id":"ysNR-nNZEHYs","executionInfo":{"status":"ok","timestamp":1638449458410,"user_tz":-330,"elapsed":3287,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}}},"source":["import argparse\n","import os\n","from datetime import datetime\n","import gym\n","import numpy as np\n","import tensorflow as tf\n","from tensorflow.keras.layers import Dense, Input, Lambda"],"execution_count":1,"outputs":[]},{"cell_type":"code","metadata":{"id":"doMRsX9OEImC","executionInfo":{"status":"ok","timestamp":1638449463142,"user_tz":-330,"elapsed":576,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}}},"source":["tf.keras.backend.set_floatx(\"float64\")"],"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dCLXisl6ELQ0","executionInfo":{"status":"ok","timestamp":1638449476727,"user_tz":-330,"elapsed":674,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}},"outputId":"3689a83a-527e-4150-f65a-55ca61946e06"},"source":["parser = argparse.ArgumentParser()\n","parser.add_argument(\"--env\", default=\"Pendulum-v0\")\n","parser.add_argument(\"--update-freq\", type=int, default=5)\n","parser.add_argument(\"--epochs\", type=int, default=3)\n","parser.add_argument(\"--actor-lr\", type=float, default=0.0005)\n","parser.add_argument(\"--critic-lr\", type=float, default=0.001)\n","parser.add_argument(\"--clip-ratio\", type=float, default=0.1)\n","parser.add_argument(\"--gae-lambda\", type=float, default=0.95)\n","parser.add_argument(\"--gamma\", type=float, default=0.99)\n","parser.add_argument(\"--logdir\", default=\"logs\")\n","\n","args = parser.parse_args([])\n","logdir = os.path.join(\n","    args.logdir, parser.prog, args.env, datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",")\n","print(f\"Saving training logs to:{logdir}\")\n","writer = tf.summary.create_file_writer(logdir)"],"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Saving training logs to:logs/ipykernel_launcher.py/Pendulum-v0/20211202-125118\n"]}]},{"cell_type":"code","metadata":{"id":"Y56QUuw5K2EH","executionInfo":{"status":"ok","timestamp":1638449498337,"user_tz":-330,"elapsed":781,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}}},"source":["class Actor:\n","    def __init__(self, state_dim, action_dim, action_bound, std_bound):\n","        self.state_dim = state_dim\n","        self.action_dim = action_dim\n","        self.action_bound = action_bound\n","        self.std_bound = std_bound\n","        self.model = self.nn_model()\n","        self.opt = tf.keras.optimizers.Adam(args.actor_lr)\n","\n","    def nn_model(self):\n","        state_input = Input((self.state_dim,))\n","        dense_1 = Dense(32, activation=\"relu\")(state_input)\n","        dense_2 = Dense(32, activation=\"relu\")(dense_1)\n","        out_mu = Dense(self.action_dim, activation=\"tanh\")(dense_2)\n","        mu_output = Lambda(lambda x: x * self.action_bound)(out_mu)\n","        std_output = Dense(self.action_dim, activation=\"softplus\")(dense_2)\n","        return tf.keras.models.Model(state_input, [mu_output, std_output])\n","\n","    def get_action(self, state):\n","        state = np.reshape(state, [1, self.state_dim])\n","        mu, std = self.model.predict(state)\n","        action = np.random.normal(mu[0], std[0], size=self.action_dim)\n","        action = np.clip(action, -self.action_bound, self.action_bound)\n","        log_policy = self.log_pdf(mu, std, action)\n","\n","        return log_policy, action\n","\n","    def log_pdf(self, mu, std, action):\n","        std = tf.clip_by_value(std, self.std_bound[0], self.std_bound[1])\n","        var = std ** 2\n","        log_policy_pdf = -0.5 * (action - mu) ** 2 / var - 0.5 * tf.math.log(\n","            var * 2 * np.pi\n","        )\n","        return tf.reduce_sum(log_policy_pdf, 1, keepdims=True)\n","\n","    def compute_loss(self, log_old_policy, log_new_policy, actions, gaes):\n","        ratio = tf.exp(log_new_policy - tf.stop_gradient(log_old_policy))\n","        gaes = tf.stop_gradient(gaes)\n","        clipped_ratio = tf.clip_by_value(\n","            ratio, 1.0 - args.clip_ratio, 1.0 + args.clip_ratio\n","        )\n","        surrogate = -tf.minimum(ratio * gaes, clipped_ratio * gaes)\n","        return tf.reduce_mean(surrogate)\n","\n","    def train(self, log_old_policy, states, actions, gaes):\n","        with tf.GradientTape() as tape:\n","            mu, std = self.model(states, training=True)\n","            log_new_policy = self.log_pdf(mu, std, actions)\n","            loss = self.compute_loss(log_old_policy, log_new_policy, actions, gaes)\n","        grads = tape.gradient(loss, self.model.trainable_variables)\n","        self.opt.apply_gradients(zip(grads, self.model.trainable_variables))\n","        return loss"],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"id":"p9IrVspCMzrU","executionInfo":{"status":"ok","timestamp":1638449501817,"user_tz":-330,"elapsed":4,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}}},"source":["class Critic:\n","    def __init__(self, state_dim):\n","        self.state_dim = state_dim\n","        self.model = self.nn_model()\n","        self.opt = tf.keras.optimizers.Adam(args.critic_lr)\n","\n","    def nn_model(self):\n","        return tf.keras.Sequential(\n","            [\n","                Input((self.state_dim,)),\n","                Dense(32, activation=\"relu\"),\n","                Dense(32, activation=\"relu\"),\n","                Dense(16, activation=\"relu\"),\n","                Dense(1, activation=\"linear\"),\n","            ]\n","        )\n","\n","    def compute_loss(self, v_pred, td_targets):\n","        mse = tf.keras.losses.MeanSquaredError()\n","        return mse(td_targets, v_pred)\n","\n","    def train(self, states, td_targets):\n","        with tf.GradientTape() as tape:\n","            v_pred = self.model(states, training=True)\n","            # assert v_pred.shape == td_targets.shape\n","            loss = self.compute_loss(v_pred, tf.stop_gradient(td_targets))\n","        grads = tape.gradient(loss, self.model.trainable_variables)\n","        self.opt.apply_gradients(zip(grads, self.model.trainable_variables))\n","        return loss"],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"id":"uSFhh4DuM0v4","executionInfo":{"status":"ok","timestamp":1638449509532,"user_tz":-330,"elapsed":565,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}}},"source":["class Agent:\n","    def __init__(self, env):\n","        self.env = env\n","        self.state_dim = self.env.observation_space.shape[0]\n","        self.action_dim = self.env.action_space.shape[0]\n","        self.action_bound = self.env.action_space.high[0]\n","        self.std_bound = [1e-2, 1.0]\n","\n","        self.actor_opt = tf.keras.optimizers.Adam(args.actor_lr)\n","        self.critic_opt = tf.keras.optimizers.Adam(args.critic_lr)\n","        self.actor = Actor(\n","            self.state_dim, self.action_dim, self.action_bound, self.std_bound\n","        )\n","        self.critic = Critic(self.state_dim)\n","\n","    def gae_target(self, rewards, v_values, next_v_value, done):\n","        n_step_targets = np.zeros_like(rewards)\n","        gae = np.zeros_like(rewards)\n","        gae_cumulative = 0\n","        forward_val = 0\n","\n","        if not done:\n","            forward_val = next_v_value\n","\n","        for k in reversed(range(0, len(rewards))):\n","            delta = rewards[k] + args.gamma * forward_val - v_values[k]\n","            gae_cumulative = args.gamma * args.gae_lambda * gae_cumulative + delta\n","            gae[k] = gae_cumulative\n","            forward_val = v_values[k]\n","            n_step_targets[k] = gae[k] + v_values[k]\n","        return gae, n_step_targets\n","\n","    def train(self, max_episodes=1000):\n","        with writer.as_default():\n","            for ep in range(max_episodes):\n","                state_batch = []\n","                action_batch = []\n","                reward_batch = []\n","                old_policy_batch = []\n","\n","                episode_reward, done = 0, False\n","\n","                state = self.env.reset()\n","\n","                while not done:\n","                    # self.env.render()\n","                    log_old_policy, action = self.actor.get_action(state)\n","\n","                    next_state, reward, done, _ = self.env.step(action)\n","\n","                    state = np.reshape(state, [1, self.state_dim])\n","                    action = np.reshape(action, [1, 1])\n","                    next_state = np.reshape(next_state, [1, self.state_dim])\n","                    reward = np.reshape(reward, [1, 1])\n","                    log_old_policy = np.reshape(log_old_policy, [1, 1])\n","\n","                    state_batch.append(state)\n","                    action_batch.append(action)\n","                    reward_batch.append((reward + 8) / 8)\n","                    old_policy_batch.append(log_old_policy)\n","\n","                    if len(state_batch) >= args.update_freq or done:\n","                        states = np.array([state.squeeze() for state in state_batch])\n","                        actions = np.array(\n","                            [action.squeeze() for action in action_batch]\n","                        )\n","                        rewards = np.array(\n","                            [reward.squeeze() for reward in reward_batch]\n","                        )\n","                        old_policies = np.array(\n","                            [old_pi.squeeze() for old_pi in old_policy_batch]\n","                        )\n","\n","                        v_values = self.critic.model.predict(states)\n","                        next_v_value = self.critic.model.predict(next_state)\n","\n","                        gaes, td_targets = self.gae_target(\n","                            rewards, v_values, next_v_value, done\n","                        )\n","                        actor_losses, critic_losses = [], []\n","                        for epoch in range(args.epochs):\n","                            actor_loss = self.actor.train(\n","                                old_policies, states, actions, gaes\n","                            )\n","                            actor_losses.append(actor_loss)\n","                            critic_loss = self.critic.train(states, td_targets)\n","                            critic_losses.append(critic_loss)\n","                        # Plot mean actor & critic losses on every update\n","                        tf.summary.scalar(\"actor_loss\", np.mean(actor_losses), step=ep)\n","                        tf.summary.scalar(\n","                            \"critic_loss\", np.mean(critic_losses), step=ep\n","                        )\n","\n","                        state_batch = []\n","                        action_batch = []\n","                        reward_batch = []\n","                        old_policy_batch = []\n","\n","                    episode_reward += reward[0][0]\n","                    state = next_state[0]\n","\n","                print(f\"Episode#{ep} Reward:{episode_reward}\")\n","                tf.summary.scalar(\"episode_reward\", episode_reward, step=ep)"],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1JQRAq0yM2gq","executionInfo":{"status":"ok","timestamp":1638449757022,"user_tz":-330,"elapsed":153376,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}},"outputId":"6bc3e775-a8a6-41c9-b212-f710ed851b12"},"source":["if __name__ == \"__main__\":\n","    env_name = \"Pendulum-v0\"\n","    env = gym.make(env_name)\n","    agent = Agent(env)\n","    agent.train(max_episodes=10)  # Increase max_episodes value"],"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["Episode#0 Reward:-1179.001295232197\n","Episode#1 Reward:-1308.678861771321\n","Episode#2 Reward:-1169.1961267895783\n","Episode#3 Reward:-1303.452114463507\n","Episode#4 Reward:-1211.6568689167416\n","Episode#5 Reward:-1296.2827962059878\n","Episode#6 Reward:-1441.2591604653464\n","Episode#7 Reward:-1513.486998623439\n","Episode#8 Reward:-1626.9159148821086\n","Episode#9 Reward:-1543.8298267551827\n"]}]},{"cell_type":"markdown","metadata":{"id":"ixk34zYIEgB_"},"source":["---"]},{"cell_type":"code","metadata":{"id":"VDwjGZWIE5il","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1638449776240,"user_tz":-330,"elapsed":3392,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}},"outputId":"f54c665e-8d2b-476d-fbae-5761e30af909"},"source":["!pip install -q watermark\n","%reload_ext watermark\n","%watermark -a \"Sparsh A.\" -m -iv -u -t -d"],"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["Author: Sparsh A.\n","\n","Last updated: 2021-12-02 12:56:18\n","\n","Compiler    : GCC 7.5.0\n","OS          : Linux\n","Release     : 5.4.104+\n","Machine     : x86_64\n","Processor   : x86_64\n","CPU cores   : 2\n","Architecture: 64bit\n","\n","IPython   : 5.5.0\n","numpy     : 1.19.5\n","argparse  : 1.1\n","gym       : 0.17.3\n","tensorflow: 2.7.0\n","\n"]}]},{"cell_type":"markdown","metadata":{"id":"haKsOAX2E1XI"},"source":["---"]},{"cell_type":"markdown","metadata":{"id":"4s5DJf8WE2as"},"source":["**END**"]}]}