{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"2022-01-18-fgnn.ipynb","provenance":[{"file_id":"https://github.com/recohut/nbs/blob/main/raw/T020674%20%7C%20FGNN%20for%20Session%20Recommendation%20on%20Sample%20Data.ipynb","timestamp":1644650514983}],"collapsed_sections":[],"authorship_tag":"ABX9TyP/1GERgMQrfZ428rf5zU+W"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"cRez1G9Reb0i"},"source":["# FGNN for Session Recommendation on Sample Data"]},{"cell_type":"markdown","metadata":{"id":"rKne4VhzjmJD"},"source":["## Setup"]},{"cell_type":"code","metadata":{"id":"scBUqujvfEPO"},"source":["!pip install torch-scatter -f https://data.pyg.org/whl/torch-1.10.0+cu111.html\n","!pip install torch-sparse -f https://data.pyg.org/whl/torch-1.10.0+cu111.html\n","!pip install torch-geometric"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ITI1SE8ye9Wh"},"source":["import pickle\n","import collections\n","import numpy as np\n","import logging\n","import math\n","import os\n","import argparse\n","import time\n","from tqdm import tqdm\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.nn import Parameter\n","from torch.utils.tensorboard import SummaryWriter\n","\n","from torch_geometric.data import DataLoader\n","from torch_geometric.nn import GCNConv, GATConv, GatedGraphConv, SAGEConv\n","from torch_geometric.data import InMemoryDataset, Data\n","from torch_scatter import scatter_add\n","from torch_geometric.nn.conv import MessagePassing\n","from torch_geometric.utils import remove_self_loops, add_self_loops, softmax\n","from torch_geometric.nn.inits import glorot, zeros"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ZIwOhkJ2jiNR"},"source":["## Dataset"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"IG0uROICgcV0","executionInfo":{"status":"ok","timestamp":1638288064931,"user_tz":-330,"elapsed":10708,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}},"outputId":"fb1341f5-6684-4a89-b301-5ad9e2fc3bbc"},"source":["!mkdir -p raw\n","%cd raw\n","!wget -q --show-progress -O sample_data_preprocessing.ipynb https://gist.githubusercontent.com/sparsh-ai/12d1f5ca07add606f27b0f841b550a82/raw/4e6521f0633e3bb20f601e04eb9fbc87390231b8/t182546-preprocessing-of-sample-data-for-session-based-recommendations.ipynb\n","%run sample_data_preprocessing.ipynb\n","%cd /content"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/raw\n","sample_data_preproc 100%[===================>]  14.71K  --.-KB/s    in 0s      \n","sample_train-item-v 100%[===================>] 386.09K  --.-KB/s    in 0.01s   \n","session_id;user_id;item_id;timeframe;eventdate\n","1;NA;81766;526309;2016-05-09\n","1;NA;31331;1031018;2016-05-09\n","1;NA;32118;243569;2016-05-09\n","1;NA;9654;75848;2016-05-09\n","1;NA;32627;1112408;2016-05-09\n","1;NA;33043;173912;2016-05-09\n","1;NA;12352;329870;2016-05-09\n","1;NA;35077;390072;2016-05-09\n","1;NA;36118;487369;2016-05-09\n","-- Starting @ 2021-11-30 16:00:57.053410s\n","-- Reading data @ 2021-11-30 16:00:57.236532s\n","Splitting date 1464134400.0\n","469\n","47\n","[('2671', 1451952000.0), ('1211', 1452384000.0), ('3780', 1452384000.0)]\n","[('1864', 1464220800.0), ('1867', 1464220800.0), ('1868', 1464220800.0)]\n","-- Splitting train set and test set @ 2021-11-30 16:00:57.250412s\n","310\n","1205\n","99\n","[[1, 2], [1], [4]] [1451952000.0, 1451952000.0, 1452384000.0] [3, 2, 5]\n","[[282], [281, 308], [281]] [1464220800.0, 1464220800.0, 1464220800.0] [282, 281, 308]\n","avg length:  3.5669291338582676\n","Done.\n","Selecting previously unselected package tree.\n","(Reading database ... 155222 files and directories currently installed.)\n","Preparing to unpack .../tree_1.7.0-5_amd64.deb ...\n","Unpacking tree (1.7.0-5) ...\n","Setting up tree (1.7.0-5) ...\n","Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",".\n","├── [6.4K]  all_train_seq.txt\n","├── [ 15K]  sample_data_preprocessing.ipynb\n","├── [386K]  sample_train-item-views.csv\n","├── [1.2K]  test.txt\n","└── [ 18K]  train.txt\n","\n"," 430K used in 0 directories, 5 files\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","markdown 3.3.6 requires importlib-metadata>=4.4; python_version < \"3.10\", but you have importlib-metadata 2.1.2 which is incompatible.\u001b[0m\n","Author: Sparsh A.\n","\n","Last updated: 2021-11-30 16:01:05\n","\n","Compiler    : GCC 7.5.0\n","OS          : Linux\n","Release     : 5.4.104+\n","Machine     : x86_64\n","Processor   : x86_64\n","CPU cores   : 2\n","Architecture: 64bit\n","\n","numpy     : 1.19.5\n","torch     : 1.10.0+cu111\n","logging   : 0.5.1.2\n","ipywidgets: 7.6.5\n","csv       : 1.0\n","IPython   : 5.5.0\n","argparse  : 1.1\n","\n","/content\n"]}]},{"cell_type":"code","metadata":{"id":"hlX_QPG9fSRa"},"source":["class MultiSessionsGraph(InMemoryDataset):\n","    \"\"\"Every session is a graph.\"\"\"\n","    def __init__(self, root, phrase, transform=None, pre_transform=None):\n","        assert phrase in ['train', 'test']\n","        self.phrase = phrase\n","        super(MultiSessionsGraph, self).__init__(root, transform, pre_transform)\n","        self.data, self.slices = torch.load(self.processed_paths[0])\n","     \n","    @property\n","    def raw_file_names(self):\n","        return [self.phrase + '.txt']\n","    \n","    @property\n","    def processed_file_names(self):\n","        return [self.phrase + '.pt']\n","    \n","    def download(self):\n","        pass\n","    \n","    def process(self):\n","        data = pickle.load(open(self.raw_dir + '/' + self.raw_file_names[0], 'rb'))\n","        data_list = []\n","        \n","        for sequence, y in zip(data[0], data[1]):\n","            # sequence = [1, 3, 2, 2, 1, 3, 4]\n","            i = 0\n","            nodes = {}    # dict{15: 0, 16: 1, 18: 2, ...}\n","            senders = []\n","            x = []\n","            for node in sequence:\n","                if node not in nodes:\n","                    nodes[node] = i\n","                    x.append([node])\n","                    i += 1\n","                senders.append(nodes[node])\n","            receivers = senders[:]\n","            \n","            if len(senders) != 1:\n","                del senders[-1]    # the last item is a receiver\n","                del receivers[0]    # the first item is a sender\n","\n","            # undirected\n","            # senders, receivers = senders + receivers, receivers + senders\n","\n","            pair = {}\n","            sur_senders = senders[:]\n","            sur_receivers = receivers[:]\n","            i = 0\n","            for sender, receiver in zip(sur_senders, sur_receivers):\n","                if str(sender)+'-'+str(receiver) in pair:\n","                    pair[str(sender)+'-'+str(receiver)] += 1\n","                    del senders[i]\n","                    del receivers[i]\n","                else:\n","                    pair[str(sender)+'-'+str(receiver)] = 1\n","                    i += 1\n","\n","            count = collections.Counter(senders)\n","            out_degree_inv = torch.tensor([1/count[i] for i in senders], dtype=torch.float)\n","\n","            count = collections.Counter(receivers)\n","            in_degree_inv = torch.tensor([1 / count[i] for i in receivers], dtype=torch.float)\n","            \n","            edge_attr = torch.tensor([pair[str(senders[i])+'-'+str(receivers[i])] for i in range(len(senders))],\n","                                     dtype=torch.float)\n","\n","            edge_index = torch.tensor([senders, receivers], dtype=torch.long)\n","            x = torch.tensor(x, dtype=torch.long)\n","            y = torch.tensor([y], dtype=torch.long)\n","            sequence = torch.tensor(sequence, dtype=torch.long)\n","            sequence_len = torch.tensor([len(sequence)], dtype=torch.long)\n","            session_graph = Data(x=x, y=y,\n","                                 edge_index=edge_index, edge_attr=edge_attr,\n","                                 sequence=sequence, sequence_len=sequence_len,\n","                                 out_degree_inv=out_degree_inv, in_degree_inv=in_degree_inv)\n","            data_list.append(session_graph)\n","            \n","        data, slices = self.collate(data_list)\n","        torch.save((data, slices), self.processed_paths[0])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"YwzLkwhKj0ZQ"},"source":["## Modules"]},{"cell_type":"code","metadata":{"id":"cFwq1_GdfcIk"},"source":["class GRUSet2Set(torch.nn.Module):\n","    r\"\"\"The global pooling operator based on iterative content-based attention\n","    from the `\"Order Matters: Sequence to sequence for sets\"\n","    <https://arxiv.org/abs/1511.06391>`_ paper\n","    .. math::\n","        \\mathbf{q}_t &= \\mathrm{LSTM}(\\mathbf{q}^{*}_{t-1})\n","        \\alpha_{i,t} &= \\mathrm{softmax}(\\mathbf{x}_i \\cdot \\mathbf{q}_t)\n","        \\mathbf{r}_t &= \\sum_{i=1}^N \\alpha_{i,t} \\mathbf{x}_i\n","        \\mathbf{q}^{*}_t &= \\mathbf{q}_t \\, \\Vert \\, \\mathbf{r}_t,\n","    where :math:`\\mathbf{q}^{*}_T` defines the output of the layer with twice\n","    the dimensionality as the input.\n","    Args:\n","        in_channels (int): Size of each input sample.\n","        processing_steps (int): Number of iterations :math:`T`.\n","        num_layers (int, optional): Number of recurrent layers, *.e.g*, setting\n","            :obj:`num_layers=2` would mean stacking two LSTMs together to form\n","            a stacked LSTM, with the second LSTM taking in outputs of the first\n","            LSTM and computing the final results. (default: :obj:`1`)\n","    \"\"\"\n","\n","    def __init__(self, in_channels, processing_steps, num_layers=1):\n","        super(GRUSet2Set, self).__init__()\n","\n","        self.in_channels = in_channels\n","        self.out_channels = 2 * in_channels\n","        self.processing_steps = processing_steps\n","        self.num_layers = num_layers\n","\n","        self.rnn = nn.GRU(self.out_channels, self.in_channels,\n","                          num_layers)\n","        self.linear = nn.Linear(in_channels * 3, in_channels)\n","\n","        self.reset_parameters()\n","\n","    def reset_parameters(self):\n","        for weight in self.rnn.parameters():\n","            if len(weight.size()) > 1:\n","                torch.nn.init.orthogonal_(weight.data)\n","\n","    def forward(self, x, batch):\n","        \"\"\"\"\"\"\n","        batch_size = batch.max().item() + 1\n","\n","        h = (x.new_zeros((self.num_layers, batch_size, self.in_channels)),\n","             x.new_zeros((self.num_layers, batch_size, self.in_channels)))\n","        q_star = x.new_zeros(batch_size, self.out_channels)\n","\n","        sections = torch.bincount(batch)\n","        v_i = torch.split(x, tuple(sections.cpu().numpy()))  # split whole x back into graphs G_i\n","        v_n_repeat = tuple(nodes[-1].view(1, -1).repeat(nodes.shape[0], 1) for nodes in\n","                           v_i)  # repeat |V|_i times for the last node embedding\n","        \n","        # x = x * v_n_repeat\n","        \n","        for i in range(self.processing_steps):\n","            if i == 0:\n","                q, h = self.rnn(q_star.unsqueeze(0))\n","            else:\n","                q, h = self.rnn(q_star.unsqueeze(0), h)\n","            q = q.view(batch_size, self.in_channels)\n","\n","            # e = self.linear(torch.cat((x, q[batch], torch.cat(v_n_repeat, dim=0)), dim=-1)).sum(dim=-1, keepdim=True)\n","            e = (x * q[batch]).sum(dim=-1, keepdim=True)\n","            a = softmax(e, batch, num_nodes=batch_size)\n","            r = scatter_add(a * x, batch, dim=0, dim_size=batch_size)\n","            q_star = torch.cat([q, r], dim=-1)\n","\n","        return q_star\n","\n","    def __repr__(self):\n","        return '{}({}, {})'.format(self.__class__.__name__, self.in_channels,\n","                                   self.out_channels)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6AhbGQJJfKz5"},"source":["class WeightedGATConv(MessagePassing):\n","    r\"\"\"The graph attentional operator from the `\"Graph Attention Networks\"\n","    <https://arxiv.org/abs/1710.10903>`_ paper\n","    .. math::\n","        \\mathbf{x}^{\\prime}_i = \\alpha_{i,i}\\mathbf{\\Theta}\\mathbf{x}_{i} +\n","        \\sum_{j \\in \\mathcal{N}(i)} \\alpha_{i,j}\\mathbf{\\Theta}\\mathbf{x}_{j},\n","    where the attention coefficients :math:`\\alpha_{i,j}` are computed as\n","    .. math::\n","        \\alpha_{i,j} =\n","        \\frac{\n","        \\exp\\left(\\mathrm{LeakyReLU}\\left(\\mathbf{a}^{\\top}\n","        [\\mathbf{\\Theta}\\mathbf{x}_i \\, \\Vert \\, \\mathbf{\\Theta}\\mathbf{x}_j]\n","        \\right)\\right)}\n","        {\\sum_{k \\in \\mathcal{N}(i) \\cup \\{ i \\}}\n","        \\exp\\left(\\mathrm{LeakyReLU}\\left(\\mathbf{a}^{\\top}\n","        [\\mathbf{\\Theta}\\mathbf{x}_i \\, \\Vert \\, \\mathbf{\\Theta}\\mathbf{x}_k]\n","        \\right)\\right)}.\n","    Args:\n","        in_channels (int): Size of each input sample.\n","        out_channels (int): Size of each output sample.\n","        heads (int, optional): Number of multi-head-attentions.\n","            (default: :obj:`1`)\n","        concat (bool, optional): If set to :obj:`False`, the multi-head\n","        attentions are averaged instead of concatenated. (default: :obj:`True`)\n","        negative_slope (float, optional): LeakyReLU angle of the negative\n","            slope. (default: :obj:`0.2`)\n","        dropout (float, optional): Dropout probability of the normalized\n","            attention coefficients which exposes each node to a stochastically\n","            sampled neighborhood during training. (default: :obj:`0`)\n","        bias (bool, optional): If set to :obj:`False`, the layer will not learn\n","            an additive bias. (default: :obj:`True`)\n","    \"\"\"\n","\n","    def __init__(self,\n","                 in_channels,\n","                 out_channels,\n","                 heads=1,\n","                 concat=True,\n","                 negative_slope=0.2,\n","                 dropout=0,\n","                 bias=True,\n","                 weighted=True):\n","        super(WeightedGATConv, self).__init__('add')\n","\n","        self.weighted = weighted\n","        self.in_channels = in_channels\n","        self.out_channels = out_channels\n","        self.heads = heads\n","        self.concat = concat\n","        self.negative_slope = negative_slope\n","        self.dropout = dropout\n","\n","        self.weight = Parameter(\n","            torch.Tensor(in_channels, heads * out_channels))\n","        self.att = Parameter(torch.Tensor(1, heads, 2 * out_channels + 1))\n","\n","        if bias and concat:\n","            self.bias = Parameter(torch.Tensor(heads * out_channels))\n","        elif bias and not concat:\n","            self.bias = Parameter(torch.Tensor(out_channels))\n","        else:\n","            self.register_parameter('bias', None)\n","\n","        self.reset_parameters()\n","\n","    def reset_parameters(self):\n","        glorot(self.weight)\n","        glorot(self.att)\n","        zeros(self.bias)\n","\n","    def forward(self, x, edge_index, edge_attr=None):\n","        \"\"\"\"\"\"\n","        edge_index, edge_attr = add_self_loops(edge_index, edge_attr)\n","\n","        x = torch.mm(x, self.weight).view(-1, self.heads, self.out_channels)\n","        return self.propagate(edge_index, x=x, num_nodes=edge_index.max() + 1, edge_attr=edge_attr)\n","\n","    def message(self, x_i, x_j, edge_index, num_nodes, edge_attr):\n","        # Compute attention coefficients.\n","        if edge_attr is not None:\n","            # alpha = ((torch.cat([x_i, x_j], dim=-1) * self.att) * edge_attr.view(-1, 1, 1)).sum(dim=-1)\n","            alpha = (torch.cat([x_i, x_j, edge_attr.view(-1, 1).repeat(1, x_i.shape[1]).view(-1, x_i.shape[1], 1)],\n","                               dim=-1) * self.att).sum(dim=-1)\n","        else:\n","            alpha = (torch.cat([x_i, x_j], dim=-1) * self.att).sum(dim=-1)\n","        alpha = F.leaky_relu(alpha, self.negative_slope)\n","        alpha = softmax(alpha, edge_index[0], num_nodes)\n","\n","        # Sample attention coefficients stochastically.\n","        if self.training and self.dropout > 0:\n","            alpha = F.dropout(alpha, p=self.dropout, training=True)\n","\n","        return x_j * alpha.view(-1, self.heads, 1)\n","\n","    def update(self, aggr_out):\n","        if self.concat is True:\n","            aggr_out = aggr_out.view(-1, self.heads * self.out_channels)\n","        else:\n","            aggr_out = aggr_out.mean(dim=1)\n","\n","        if self.bias is not None:\n","            aggr_out = aggr_out + self.bias\n","        return aggr_out\n","\n","    def __repr__(self):\n","        return '{}({}, {}, heads={})'.format(self.__class__.__name__,\n","                                             self.in_channels,\n","                                             self.out_channels, self.heads)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xTHP0D0dfsWe"},"source":["class Embedding2Score(nn.Module):\n","    def __init__(self, hidden_size):\n","        super(Embedding2Score, self).__init__()\n","        self.hidden_size = hidden_size\n","        self.W_1 = nn.Linear(self.hidden_size, self.hidden_size)\n","        self.W_2 = nn.Linear(self.hidden_size, self.hidden_size)\n","        self.q = nn.Linear(self.hidden_size, 1)\n","        self.W_3 = nn.Linear(2 * self.hidden_size, self.hidden_size)\n","\n","    def forward(self, session_embedding, all_item_embedding, batch):\n","        sections = torch.bincount(batch)\n","        v_i = torch.split(session_embedding, tuple(sections.cpu().numpy()))    # split whole x back into graphs G_i\n","        v_n_repeat = tuple(nodes[-1].view(1, -1).repeat(nodes.shape[0], 1) for nodes in v_i)    # repeat |V|_i times for the last node embedding\n","\n","        # Eq(6)\n","        alpha = self.q(torch.sigmoid(self.W_1(torch.cat(v_n_repeat, dim=0)) + self.W_2(session_embedding)))    # |V|_i * 1\n","        s_g_whole = alpha * session_embedding    # |V|_i * hidden_size\n","        s_g_split = torch.split(s_g_whole, tuple(sections.cpu().numpy()))    # split whole s_g into graphs G_i\n","        s_g = tuple(torch.sum(embeddings, dim=0).view(1, -1) for embeddings in s_g_split)\n","        \n","        # Eq(7)\n","        v_n = tuple(nodes[-1].view(1, -1) for nodes in v_i)\n","        s_h = self.W_3(torch.cat((torch.cat(v_n, dim=0), torch.cat(s_g, dim=0)), dim=1))\n","        \n","        # Eq(8)\n","        z_i_hat = torch.mm(s_h, all_item_embedding.weight.transpose(1, 0))\n","        \n","        return z_i_hat\n","\n","\n","class GNNModel(nn.Module):\n","    \"\"\"\n","    Args:\n","        hidden_size: the number of units in a hidden layer.\n","        n_node: the number of items in the whole item set for embedding layer.\n","    \"\"\"\n","    def __init__(self, hidden_size, n_node):\n","        super(GNNModel, self).__init__()\n","        self.hidden_size, self.n_node = hidden_size, n_node\n","        self.embedding = nn.Embedding(self.n_node, self.hidden_size)\n","        self.gat1 = GATConv(self.hidden_size, self.hidden_size, heads=8, negative_slope=0.2)\n","        self.gat2 = GATConv(8 * self.hidden_size, self.hidden_size, heads=1, negative_slope=0.2)\n","        self.sage1 = SAGEConv(self.hidden_size, self.hidden_size)\n","        self.sage2 = SAGEConv(self.hidden_size, self.hidden_size)\n","        self.gated = GatedGraphConv(self.hidden_size, num_layers=2)\n","        self.e2s = Embedding2Score(self.hidden_size)\n","        self.loss_function = nn.CrossEntropyLoss()\n","        self.reset_parameters()\n","        \n","    def reset_parameters(self):\n","        stdv = 1.0 / math.sqrt(self.hidden_size)\n","        for weight in self.parameters():\n","            weight.data.uniform_(-stdv, stdv)\n","\n","    def forward(self, data):\n","        x, edge_index, batch, edge_attr = data.x - 1, data.edge_index, data.batch, data.edge_attr\n","\n","        embedding = self.embedding(x).squeeze()\n","        hidden = self.gated(embedding, edge_index)#, edge_attr.float())\n","        #\n","        # hidden = F.relu(self.gat1(embedding, edge_index))\n","        # hidden = self.gat2(hidden, edge_index)\n","        \n","        # hidden1 = F.relu(self.sage1(embedding, edge_index))\n","        # hidden2 = F.relu(self.sage2(hidden1, edge_index))\n","        return self.e2s(hidden, self.embedding, batch)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"AkknSFG8fhf8"},"source":["def forward(model, loader, device, writer, epoch, top_k=20, optimizer=None, train_flag=True):\n","    if train_flag:\n","        model.train()\n","    else:\n","        model.eval()\n","        hit, mrr = [], []\n","\n","    mean_loss = 0.0\n","    updates_per_epoch = len(loader)\n","\n","    for i, batch in enumerate(loader):\n","        if train_flag:\n","            optimizer.zero_grad()\n","        scores = model(batch.to(device))\n","        targets = batch.y - 1\n","        loss = model.loss_function(scores, targets)\n","\n","        if train_flag:\n","            loss.backward()\n","            # nn.utils.clip_grad_norm(model.parameters(), 0.5)\n","            optimizer.step()\n","            writer.add_scalar('loss/train_batch_loss', loss.item(), epoch * updates_per_epoch + i)\n","        else:\n","            sub_scores = scores.topk(top_k)[1]    # batch * top_k\n","            for score, target in zip(sub_scores.detach().cpu().numpy(), targets.detach().cpu().numpy()):\n","                hit.append(np.isin(target, score))\n","                if len(np.where(score == target)[0]) == 0:\n","                    mrr.append(0)\n","                else:\n","                    mrr.append(1 / (np.where(score == target)[0][0] + 1))\n","\n","        mean_loss += loss / batch.num_graphs\n","\n","    if train_flag:\n","        writer.add_scalar('loss/train_loss', mean_loss.item(), epoch)\n","    else:\n","        writer.add_scalar('loss/test_loss', mean_loss.item(), epoch)\n","        hit = np.mean(hit) * 100\n","        mrr = np.mean(mrr) * 100\n","        writer.add_scalar('index/hit', hit, epoch)\n","        writer.add_scalar('index/mrr', mrr, epoch)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"JpiwWBZXjgX4"},"source":["## Main"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":34},"id":"iVIoR-5Jf1zX","executionInfo":{"status":"ok","timestamp":1638288075473,"user_tz":-330,"elapsed":1296,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}},"outputId":"812a24e9-25d3-4be9-f579-7e54086a246f"},"source":["parser = argparse.ArgumentParser()\n","parser.add_argument('--dataset', default='sample', help='dataset name')\n","parser.add_argument('--batch_size', type=int, default=100, help='input batch size')\n","parser.add_argument('--hidden_size', type=int, default=100, help='hidden state size')\n","parser.add_argument('--epoch', type=int, default=10, help='the number of epochs to train for')\n","parser.add_argument('--lr', type=float, default=0.001, help='learning rate')  # [0.001, 0.0005, 0.0001]\n","parser.add_argument('--lr_dc', type=float, default=0.1, help='learning rate decay rate')\n","parser.add_argument('--lr_dc_step', type=int, default=3, help='the number of steps after which the learning rate decay')\n","parser.add_argument('--l2', type=float, default=1e-5, help='l2 penalty')  # [0.001, 0.0005, 0.0001, 0.00005, 0.00001]\n","parser.add_argument('--top_k', type=int, default=20, help='top K indicator for evaluation')\n","parser.add_argument('--momentum', type=float, default=0.9, help='momentum for SGD')\n","opt = parser.parse_args(args={})\n","logging.warning(opt)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["WARNING:root:Namespace(batch_size=100, dataset='sample', epoch=10, hidden_size=100, l2=1e-05, lr=0.001, lr_dc=0.1, lr_dc_step=3, momentum=0.9, top_k=20)\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":471},"id":"rkgJxnqif695","executionInfo":{"status":"ok","timestamp":1638288102032,"user_tz":-330,"elapsed":7364,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}},"outputId":"1156f2d3-8cb2-489c-8dc6-5394b28598d1"},"source":["def main():\n","    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","    cur_dir = os.getcwd()\n","    train_dataset = MultiSessionsGraph('.', phrase='train')\n","    train_loader = DataLoader(train_dataset, batch_size=opt.batch_size, shuffle=True)\n","    test_dataset = MultiSessionsGraph('.', phrase='test')\n","    test_loader = DataLoader(test_dataset, batch_size=opt.batch_size, shuffle=False)\n","\n","    log_dir = './logs'\n","    if not os.path.exists(log_dir):\n","        os.makedirs(log_dir)\n","    logging.warning('logging to {}'.format(log_dir))\n","    writer = SummaryWriter(log_dir)\n","\n","    n_node = 309\n","\n","    model = GNNModel(hidden_size=opt.hidden_size, n_node=n_node).to(device)\n","    optimizer = torch.optim.Adam(model.parameters(), lr=opt.lr, weight_decay=opt.l2)\n","    # optimizer = torch.optim.SGD(model.parameters(), lr=opt.lr, weight_decay=opt.l2, momentum=opt.momentum)\n","    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=opt.lr_dc_step, gamma=opt.lr_dc)\n","\n","    logging.warning(model)\n","    \n","    for epoch in tqdm(range(opt.epoch)):\n","        scheduler.step()\n","        forward(model, train_loader, device, writer, epoch, top_k=opt.top_k, optimizer=optimizer, train_flag=True)\n","        with torch.no_grad():\n","            forward(model, test_loader, device, writer, epoch, top_k=opt.top_k, train_flag=False)\n","\n","\n","if __name__ == '__main__':\n","    main()"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Processing...\n","Done!\n","/usr/local/lib/python3.7/dist-packages/torch_geometric/deprecation.py:13: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n","  warnings.warn(out)\n","Processing...\n","Done!\n","WARNING:root:logging to ./logs\n","WARNING:root:GNNModel(\n","  (embedding): Embedding(309, 100)\n","  (gat1): GATConv(100, 100, heads=8)\n","  (gat2): GATConv(800, 100, heads=1)\n","  (sage1): SAGEConv(100, 100)\n","  (sage2): SAGEConv(100, 100)\n","  (gated): GatedGraphConv(100, num_layers=2)\n","  (e2s): Embedding2Score(\n","    (W_1): Linear(in_features=100, out_features=100, bias=True)\n","    (W_2): Linear(in_features=100, out_features=100, bias=True)\n","    (q): Linear(in_features=100, out_features=1, bias=True)\n","    (W_3): Linear(in_features=200, out_features=100, bias=True)\n","  )\n","  (loss_function): CrossEntropyLoss()\n",")\n","  0%|          | 0/10 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:134: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n","  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n","100%|██████████| 10/10 [00:04<00:00,  2.48it/s]\n"]}]},{"cell_type":"markdown","metadata":{"id":"ow2QwwhjlP-8"},"source":["---"]},{"cell_type":"code","metadata":{"id":"cgQpjxIrlP-8"},"source":["!apt-get -qq install tree\n","!rm -r sample_data"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jXMEsvGFlP-9","executionInfo":{"status":"ok","timestamp":1638288415068,"user_tz":-330,"elapsed":14,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}},"outputId":"b4b66849-c7d6-4fca-c17f-99ae4de69abf"},"source":["!tree -h --du ."],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":[".\n","├── [ 13K]  logs\n","│   └── [9.5K]  events.out.tfevents.1638288098.78729cf41c84.145.0\n","├── [224K]  processed\n","│   ├── [ 431]  pre_filter.pt\n","│   ├── [ 431]  pre_transform.pt\n","│   ├── [ 19K]  test.pt\n","│   └── [200K]  train.pt\n","├── [430K]  raw\n","│   ├── [6.4K]  all_train_seq.txt\n","│   ├── [ 15K]  sample_data_preprocessing.ipynb\n","│   ├── [386K]  sample_train-item-views.csv\n","│   ├── [1.2K]  test.txt\n","│   └── [ 18K]  train.txt\n","└── [4.0K]  var_inspect.ipynb\n","\n"," 676K used in 3 directories, 11 files\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YefCxtvJlP-9","executionInfo":{"status":"ok","timestamp":1638288437035,"user_tz":-330,"elapsed":3530,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}},"outputId":"8cc2d129-9cab-49b7-b955-6601c0376c05"},"source":["!pip install -q watermark\n","%reload_ext watermark\n","%watermark -a \"Sparsh A.\" -m -iv -u -t -d"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Author: Sparsh A.\n","\n","Last updated: 2021-11-30 16:07:17\n","\n","Compiler    : GCC 7.5.0\n","OS          : Linux\n","Release     : 5.4.104+\n","Machine     : x86_64\n","Processor   : x86_64\n","CPU cores   : 2\n","Architecture: 64bit\n","\n","numpy      : 1.19.5\n","torch      : 1.10.0+cu111\n","logging    : 0.5.1.2\n","ipywidgets : 7.6.5\n","csv        : 1.0\n","IPython    : 5.5.0\n","argparse   : 1.1\n","tensorboard: 2.7.0\n","\n"]}]},{"cell_type":"markdown","metadata":{"id":"qER4Raj6lP--"},"source":["---"]},{"cell_type":"markdown","metadata":{"id":"Nolfb57llP_A"},"source":["**END**"]}]}