{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.13"},"colab":{"name":"T506315 | PyTorch Profiler","provenance":[{"file_id":"https://github.com/pytorch/tutorials/blob/gh-pages/_downloads/3c3c70158f5825ae0c92271bc6270099/profiler_recipe.ipynb","timestamp":1631261844169}],"collapsed_sections":[]}},"cells":[{"cell_type":"code","metadata":{"id":"i6Z7DqdwyFU5"},"source":["%matplotlib inline"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xUe3daPgyFVC"},"source":["\n","PyTorch Profiler\n","====================================\n","This recipe explains how to use PyTorch profiler and measure the time and\n","memory consumption of the model's operators.\n","\n","Introduction\n","------------\n","PyTorch includes a simple profiler API that is useful when user needs\n","to determine the most expensive operators in the model.\n","\n","In this recipe, we will use a simple Resnet model to demonstrate how to\n","use profiler to analyze model performance.\n","\n","Setup\n","-----\n","To install ``torch`` and ``torchvision`` use the following command:\n","\n","::\n","\n","   pip install torch torchvision\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"ARHIoMe0yFVG"},"source":["Steps\n","-----\n","\n","1. Import all necessary libraries\n","2. Instantiate a simple Resnet model\n","3. Using profiler to analyze execution time\n","4. Using profiler to analyze memory consumption\n","5. Using tracing functionality\n","6. Examining stack traces\n","7. Visualizing data as a flamegraph\n","8. Using profiler to analyze long-running jobs\n","\n","1. Import all necessary libraries\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","\n","In this recipe we will use ``torch``, ``torchvision.models``\n","and ``profiler`` modules:\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"Jn2f7hStyFVH"},"source":["import torch\n","import torchvision.models as models\n","from torch.profiler import profile, record_function, ProfilerActivity"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zXYxDCvdyFVJ"},"source":["2. Instantiate a simple Resnet model\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","\n","Let's create an instance of a Resnet model and prepare an input\n","for it:\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"QPJ7xSIXyFVK"},"source":["model = models.resnet18()\n","inputs = torch.randn(5, 3, 224, 224)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VYRb9s9tyFVL"},"source":["3. Using profiler to analyze execution time\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","\n","PyTorch profiler is enabled through the context manager and accepts\n","a number of parameters, some of the most useful are:\n","\n","- ``activities`` - a list of activities to profile:\n","   - ``ProfilerActivity.CPU`` - PyTorch operators, TorchScript functions and\n","     user-defined code labels (see ``record_function`` below);\n","   - ``ProfilerActivity.CUDA`` - on-device CUDA kernels;\n","- ``record_shapes`` - whether to record shapes of the operator inputs;\n","- ``profile_memory`` - whether to report amount of memory consumed by\n","  model's Tensors;\n","- ``use_cuda`` - whether to measure execution time of CUDA kernels.\n","\n","Note: when using CUDA, profiler also shows the runtime CUDA events\n","occuring on the host.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"xwtbVJ6yyFVN"},"source":["Let's see how we can use profiler to analyze the execution time:\n","\n"]},{"cell_type":"code","metadata":{"id":"i410LYVUyFVO","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1631261886958,"user_tz":-330,"elapsed":1993,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}},"outputId":"6ba1ce5f-1c71-4be5-c2c3-f81d404481fd"},"source":["with profile(activities=[ProfilerActivity.CPU], record_shapes=True) as prof:\n","    with record_function(\"model_inference\"):\n","        model(inputs)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)\n","  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n"]}]},{"cell_type":"markdown","metadata":{"id":"e7lmmOemyFVP"},"source":["Note that we can use ``record_function`` context manager to label\n","arbitrary code ranges with user provided names\n","(``model_inference`` is used as a label in the example above).\n","\n","Profiler allows one to check which operators were called during the\n","execution of a code range wrapped with a profiler context manager.\n","If multiple profiler ranges are active at the same time (e.g. in\n","parallel PyTorch threads), each profiling context manager tracks only\n","the operators of its corresponding range.\n","Profiler also automatically profiles the async tasks launched\n","with ``torch.jit._fork`` and (in case of a backward pass)\n","the backward pass operators launched with ``backward()`` call.\n","\n","Let's print out the stats for the execution above:\n","\n"]},{"cell_type":"code","metadata":{"id":"yhiIzBcUyFVQ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1631261889877,"user_tz":-330,"elapsed":581,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}},"outputId":"b5a74a8b-bbd9-4c24-9c6c-0cbcef47522f"},"source":["print(prof.key_averages().table(sort_by=\"cpu_time_total\", row_limit=10))"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["---------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n","                             Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg    # of Calls  \n","---------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n","                  model_inference         3.90%      29.904ms        99.71%     764.855ms     764.855ms             1  \n","                     aten::conv2d         0.03%     259.000us        71.82%     550.883ms      27.544ms            20  \n","                aten::convolution         0.04%     311.000us        71.78%     550.624ms      27.531ms            20  \n","               aten::_convolution         0.06%     439.000us        71.74%     550.313ms      27.516ms            20  \n","         aten::mkldnn_convolution        71.59%     549.142ms        71.69%     549.874ms      27.494ms            20  \n","                 aten::batch_norm         0.41%       3.134ms        12.91%      99.012ms       4.951ms            20  \n","     aten::_batch_norm_impl_index         0.06%     483.000us        12.50%      95.878ms       4.794ms            20  \n","          aten::native_batch_norm         7.49%      57.440ms        12.43%      95.336ms       4.767ms            20  \n","                 aten::max_pool2d         0.00%      30.000us         7.19%      55.123ms      55.123ms             1  \n","    aten::max_pool2d_with_indices         7.18%      55.093ms         7.18%      55.093ms      55.093ms             1  \n","---------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n","Self CPU time total: 767.061ms\n","\n"]}]},{"cell_type":"markdown","metadata":{"id":"HBST4pgXyFVR"},"source":["The output will look like (omitting some columns):\n","\n"]},{"cell_type":"code","metadata":{"id":"95hkLGsVyFVR"},"source":["# ---------------------------------  ------------  ------------  ------------  ------------\n","#                              Name      Self CPU     CPU total  CPU time avg    # of Calls\n","# ---------------------------------  ------------  ------------  ------------  ------------\n","#                   model_inference       5.509ms      57.503ms      57.503ms             1\n","#                      aten::conv2d     231.000us      31.931ms       1.597ms            20\n","#                 aten::convolution     250.000us      31.700ms       1.585ms            20\n","#                aten::_convolution     336.000us      31.450ms       1.573ms            20\n","#          aten::mkldnn_convolution      30.838ms      31.114ms       1.556ms            20\n","#                  aten::batch_norm     211.000us      14.693ms     734.650us            20\n","#      aten::_batch_norm_impl_index     319.000us      14.482ms     724.100us            20\n","#           aten::native_batch_norm       9.229ms      14.109ms     705.450us            20\n","#                        aten::mean     332.000us       2.631ms     125.286us            21\n","#                      aten::select       1.668ms       2.292ms       8.988us           255\n","# ---------------------------------  ------------  ------------  ------------  ------------\n","# Self CPU time total: 57.549ms"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"sqWfyF9CyFVT"},"source":["Here we see that, as expected, most of the time is spent in convolution (and specifically in ``mkldnn_convolution``\n","for PyTorch compiled with MKL-DNN support).\n","Note the difference between self cpu time and cpu time - operators can call other operators, self cpu time exludes time\n","spent in children operator calls, while total cpu time includes it. You can choose to sort by the self cpu time by passing\n","``sort_by=\"self_cpu_time_total\"`` into the ``table`` call.\n","\n","To get a finer granularity of results and include operator input shapes, pass ``group_by_input_shape=True``\n","(note: this requires running the profiler with ``record_shapes=True``):\n","\n"]},{"cell_type":"code","metadata":{"id":"rulyRHKtyFVU","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1631261899878,"user_tz":-330,"elapsed":680,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}},"outputId":"95c92b37-155d-42aa-9a75-c3c9320d444d"},"source":["print(prof.key_averages(group_by_input_shape=True).table(sort_by=\"cpu_time_total\", row_limit=10))\n","\n","# (omitting some columns)\n","# ---------------------------------  ------------  -------------------------------------------\n","#                              Name     CPU total                                 Input Shapes\n","# ---------------------------------  ------------  -------------------------------------------\n","#                   model_inference      57.503ms                                           []\n","#                      aten::conv2d       8.008ms      [5,64,56,56], [64,64,3,3], [], ..., []]\n","#                 aten::convolution       7.956ms     [[5,64,56,56], [64,64,3,3], [], ..., []]\n","#                aten::_convolution       7.909ms     [[5,64,56,56], [64,64,3,3], [], ..., []]\n","#          aten::mkldnn_convolution       7.834ms     [[5,64,56,56], [64,64,3,3], [], ..., []]\n","#                      aten::conv2d       6.332ms    [[5,512,7,7], [512,512,3,3], [], ..., []]\n","#                 aten::convolution       6.303ms    [[5,512,7,7], [512,512,3,3], [], ..., []]\n","#                aten::_convolution       6.273ms    [[5,512,7,7], [512,512,3,3], [], ..., []]\n","#          aten::mkldnn_convolution       6.233ms    [[5,512,7,7], [512,512,3,3], [], ..., []]\n","#                      aten::conv2d       4.751ms  [[5,256,14,14], [256,256,3,3], [], ..., []]\n","# ---------------------------------  ------------  -------------------------------------------\n","# Self CPU time total: 57.549ms"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["---------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  --------------------------------------------------------------------------------  \n","                             Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg    # of Calls                                                                      Input Shapes  \n","---------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  --------------------------------------------------------------------------------  \n","                  model_inference         3.90%      29.904ms        99.71%     764.855ms     764.855ms             1                                                                                []  \n","                     aten::conv2d         0.01%      57.000us        21.51%     165.014ms     165.014ms             1                             [[5, 3, 224, 224], [64, 3, 7, 7], [], [], [], [], []]  \n","                aten::convolution         0.00%      10.000us        21.51%     164.957ms     164.957ms             1                     [[5, 3, 224, 224], [64, 3, 7, 7], [], [], [], [], [], [], []]  \n","               aten::_convolution         0.00%      28.000us        21.50%     164.947ms     164.947ms             1     [[5, 3, 224, 224], [64, 3, 7, 7], [], [], [], [], [], [], [], [], [], [], []]  \n","         aten::mkldnn_convolution        21.47%     164.679ms        21.50%     164.919ms     164.919ms             1                             [[5, 3, 224, 224], [64, 3, 7, 7], [], [], [], [], []]  \n","                     aten::conv2d         0.01%      53.000us        13.87%     106.390ms      26.598ms             4                             [[5, 64, 56, 56], [64, 64, 3, 3], [], [], [], [], []]  \n","                aten::convolution         0.01%      89.000us        13.86%     106.337ms      26.584ms             4                     [[5, 64, 56, 56], [64, 64, 3, 3], [], [], [], [], [], [], []]  \n","               aten::_convolution         0.01%     108.000us        13.85%     106.248ms      26.562ms             4     [[5, 64, 56, 56], [64, 64, 3, 3], [], [], [], [], [], [], [], [], [], [], []]  \n","         aten::mkldnn_convolution        13.82%     106.017ms        13.84%     106.140ms      26.535ms             4                             [[5, 64, 56, 56], [64, 64, 3, 3], [], [], [], [], []]  \n","                     aten::conv2d         0.00%      25.000us        10.13%      77.700ms      25.900ms             3                            [[5, 512, 7, 7], [512, 512, 3, 3], [], [], [], [], []]  \n","---------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  --------------------------------------------------------------------------------  \n","Self CPU time total: 767.061ms\n","\n"]}]},{"cell_type":"markdown","metadata":{"id":"skZq1ut0yFVV"},"source":["Note the occurence of ``aten::convolution`` twice with different input shapes.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"4y-duwY1yFVW"},"source":["Profiler can also be used to analyze performance of models executed on GPUs:\n","\n"]},{"cell_type":"code","metadata":{"id":"BctNs37FyFVW","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1631261930531,"user_tz":-330,"elapsed":2048,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}},"outputId":"8357ac37-78dd-46b1-bb57-e9b0e11b6b7f"},"source":["# model = models.resnet18().cuda()\n","# inputs = torch.randn(5, 3, 224, 224).cuda()\n","\n","model = models.resnet18()\n","inputs = torch.randn(5, 3, 224, 224)\n","\n","with profile(activities=[\n","        ProfilerActivity.CPU, ProfilerActivity.CUDA], record_shapes=True) as prof:\n","    with record_function(\"model_inference\"):\n","        model(inputs)\n","\n","print(prof.key_averages().table(sort_by=\"cuda_time_total\", row_limit=10))"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/autograd/profiler.py:427: UserWarning: CUDA is not available, disabling CUDA profiling\n","  warn(\"CUDA is not available, disabling CUDA profiling\")\n"]},{"output_type":"stream","name":"stdout","text":["---------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n","                             Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg    # of Calls  \n","---------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n","                      aten::zeros         0.01%      37.000us         0.01%      57.000us      57.000us             1  \n","                      aten::empty         0.15%     798.000us         0.15%     798.000us       7.748us           103  \n","                      aten::zero_         0.00%       4.000us         0.00%       4.000us       4.000us             1  \n","                  model_inference         1.77%       9.098ms        99.99%     514.967ms     514.967ms             1  \n","                     aten::conv2d         0.04%     209.000us        72.92%     375.573ms      18.779ms            20  \n","                aten::convolution         0.05%     239.000us        72.88%     375.364ms      18.768ms            20  \n","               aten::_convolution         0.09%     451.000us        72.84%     375.125ms      18.756ms            20  \n","         aten::mkldnn_convolution        72.61%     373.977ms        72.75%     374.674ms      18.734ms            20  \n","                aten::as_strided_         0.04%     198.000us         0.04%     198.000us       9.900us            20  \n","                        aten::add         0.10%     501.000us         0.10%     501.000us      25.050us            20  \n","---------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n","Self CPU time total: 515.024ms\n","\n"]}]},{"cell_type":"markdown","metadata":{"id":"w7gOwk6qyFVX"},"source":["(Note: the first use of CUDA profiling may bring an extra overhead.)\n","\n"]},{"cell_type":"markdown","metadata":{"id":"aovprwr5yFVX"},"source":["The resulting table output:\n","\n"]},{"cell_type":"code","metadata":{"id":"am0XeO4fyFVY"},"source":["# (omitting some columns)\n","# -------------------------------------------------------  ------------  ------------\n","#                                                    Name     Self CUDA    CUDA total\n","# -------------------------------------------------------  ------------  ------------\n","#                                         model_inference       0.000us      11.666ms\n","#                                            aten::conv2d       0.000us      10.484ms\n","#                                       aten::convolution       0.000us      10.484ms\n","#                                      aten::_convolution       0.000us      10.484ms\n","#                              aten::_convolution_nogroup       0.000us      10.484ms\n","#                                       aten::thnn_conv2d       0.000us      10.484ms\n","#                               aten::thnn_conv2d_forward      10.484ms      10.484ms\n","# void at::native::im2col_kernel<float>(long, float co...       3.844ms       3.844ms\n","#                                       sgemm_32x32x32_NN       3.206ms       3.206ms\n","#                                   sgemm_32x32x32_NN_vec       3.093ms       3.093ms\n","# -------------------------------------------------------  ------------  ------------\n","# Self CPU time total: 23.015ms\n","# Self CUDA time total: 11.666ms"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"A2x3BXNXyFVY"},"source":["Note the occurence of on-device kernels in the output (e.g. ``sgemm_32x32x32_NN``).\n","\n"]},{"cell_type":"markdown","metadata":{"id":"UzsTxgbLyFVZ"},"source":["4. Using profiler to analyze memory consumption\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","\n","PyTorch profiler can also show the amount of memory (used by the model's tensors)\n","that was allocated (or released) during the execution of the model's operators.\n","In the output below, 'self' memory corresponds to the memory allocated (released)\n","by the operator, excluding the children calls to the other operators.\n","To enable memory profiling functionality pass ``profile_memory=True``.\n","\n"]},{"cell_type":"code","metadata":{"id":"DnTUeCStyFVZ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1631261947540,"user_tz":-330,"elapsed":7241,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}},"outputId":"5af44f07-4ba8-49e3-9eec-c2c3596f31ec"},"source":["model = models.resnet18()\n","inputs = torch.randn(5, 3, 224, 224)\n","\n","with profile(activities=[ProfilerActivity.CPU],\n","        profile_memory=True, record_shapes=True) as prof:\n","    model(inputs)\n","\n","print(prof.key_averages().table(sort_by=\"self_cpu_memory_usage\", row_limit=10))\n","\n","# (omitting some columns)\n","# ---------------------------------  ------------  ------------  ------------\n","#                              Name       CPU Mem  Self CPU Mem    # of Calls\n","# ---------------------------------  ------------  ------------  ------------\n","#                       aten::empty      94.79 Mb      94.79 Mb           121\n","#     aten::max_pool2d_with_indices      11.48 Mb      11.48 Mb             1\n","#                       aten::addmm      19.53 Kb      19.53 Kb             1\n","#               aten::empty_strided         572 b         572 b            25\n","#                     aten::resize_         240 b         240 b             6\n","#                         aten::abs         480 b         240 b             4\n","#                         aten::add         160 b         160 b            20\n","#               aten::masked_select         120 b         112 b             1\n","#                          aten::ne         122 b          53 b             6\n","#                          aten::eq          60 b          30 b             2\n","# ---------------------------------  ------------  ------------  ------------\n","# Self CPU time total: 53.064ms\n","\n","print(prof.key_averages().table(sort_by=\"cpu_memory_usage\", row_limit=10))\n","\n","# (omitting some columns)\n","# ---------------------------------  ------------  ------------  ------------\n","#                              Name       CPU Mem  Self CPU Mem    # of Calls\n","# ---------------------------------  ------------  ------------  ------------\n","#                       aten::empty      94.79 Mb      94.79 Mb           121\n","#                  aten::batch_norm      47.41 Mb           0 b            20\n","#      aten::_batch_norm_impl_index      47.41 Mb           0 b            20\n","#           aten::native_batch_norm      47.41 Mb           0 b            20\n","#                      aten::conv2d      47.37 Mb           0 b            20\n","#                 aten::convolution      47.37 Mb           0 b            20\n","#                aten::_convolution      47.37 Mb           0 b            20\n","#          aten::mkldnn_convolution      47.37 Mb           0 b            20\n","#                  aten::max_pool2d      11.48 Mb           0 b             1\n","#     aten::max_pool2d_with_indices      11.48 Mb      11.48 Mb             1\n","# ---------------------------------  ------------  ------------  ------------\n","# Self CPU time total: 53.064ms"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["---------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n","                             Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg       CPU Mem  Self CPU Mem    # of Calls  \n","---------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n","                      aten::empty         0.29%       1.540ms         0.29%       1.540ms      15.248us      94.79 Mb      94.79 Mb           101  \n","    aten::max_pool2d_with_indices         8.99%      47.763ms         8.99%      47.763ms      47.763ms      11.48 Mb      11.48 Mb             1  \n","                      aten::addmm         0.09%     487.000us         0.10%     505.000us     505.000us      19.53 Kb      19.53 Kb             1  \n","                        aten::add         0.14%     743.000us         0.14%     743.000us      37.150us         160 b         160 b            20  \n","              aten::empty_strided         0.05%     270.000us         0.05%     270.000us      12.857us          84 b          84 b            21  \n","                     aten::conv2d         0.06%     324.000us        71.89%     381.950ms      19.098ms      47.37 Mb           0 b            20  \n","                aten::convolution         0.07%     362.000us        71.82%     381.626ms      19.081ms      47.37 Mb           0 b            20  \n","               aten::_convolution         0.12%     654.000us        71.76%     381.264ms      19.063ms      47.37 Mb           0 b            20  \n","         aten::mkldnn_convolution        71.47%     379.759ms        71.63%     380.610ms      19.030ms      47.37 Mb           0 b            20  \n","                aten::as_strided_         0.05%     243.000us         0.05%     243.000us      12.150us           0 b           0 b            20  \n","---------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n","Self CPU time total: 531.330ms\n","\n","---------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n","                             Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg       CPU Mem  Self CPU Mem    # of Calls  \n","---------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n","                      aten::empty         0.29%       1.540ms         0.29%       1.540ms      15.248us      94.79 Mb      94.79 Mb           101  \n","                 aten::batch_norm         0.06%     306.000us        16.71%      88.774ms       4.439ms      47.41 Mb           0 b            20  \n","     aten::_batch_norm_impl_index         0.11%     573.000us        16.65%      88.468ms       4.423ms      47.41 Mb           0 b            20  \n","          aten::native_batch_norm        11.24%      59.726ms        16.53%      87.819ms       4.391ms      47.41 Mb           0 b            20  \n","                     aten::conv2d         0.06%     324.000us        71.89%     381.950ms      19.098ms      47.37 Mb           0 b            20  \n","                aten::convolution         0.07%     362.000us        71.82%     381.626ms      19.081ms      47.37 Mb           0 b            20  \n","               aten::_convolution         0.12%     654.000us        71.76%     381.264ms      19.063ms      47.37 Mb           0 b            20  \n","         aten::mkldnn_convolution        71.47%     379.759ms        71.63%     380.610ms      19.030ms      47.37 Mb           0 b            20  \n","                 aten::max_pool2d         0.01%      36.000us         9.00%      47.799ms      47.799ms      11.48 Mb           0 b             1  \n","    aten::max_pool2d_with_indices         8.99%      47.763ms         8.99%      47.763ms      47.763ms      11.48 Mb      11.48 Mb             1  \n","---------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n","Self CPU time total: 531.330ms\n","\n"]}]},{"cell_type":"markdown","metadata":{"id":"H71iJ52GyFVb"},"source":["5. Using tracing functionality\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","\n","Profiling results can be outputted as a .json trace file:\n","\n"]},{"cell_type":"code","metadata":{"id":"vstzhaPKyFVb","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1631262071898,"user_tz":-330,"elapsed":1905,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}},"outputId":"26888ae1-7cee-4883-f8eb-618e938332da"},"source":["# model = models.resnet18().cuda()\n","# inputs = torch.randn(5, 3, 224, 224).cuda()\n","\n","model = models.resnet18()\n","inputs = torch.randn(5, 3, 224, 224)\n","\n","with profile(activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA]) as prof:\n","    model(inputs)\n","\n","prof.export_chrome_trace(\"trace.json\")"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/autograd/profiler.py:427: UserWarning: CUDA is not available, disabling CUDA profiling\n","  warn(\"CUDA is not available, disabling CUDA profiling\")\n","/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)\n","  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"unJBEdtfy_u7","executionInfo":{"status":"ok","timestamp":1631262089508,"user_tz":-330,"elapsed":11,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}},"outputId":"4931198f-bb68-4da1-8a40-c60700f5a89e"},"source":["!head trace.json"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","{\n","  \"schemaVersion\": 1,\n","  \"deviceProperties\": [\n","  ],\n","  \"traceEvents\": [\n","  {\n","    \"ph\": \"X\", \"cat\": \"Operator\", \n","    \"name\": \"aten::empty\", \"pid\": 129, \"tid\": \"129\",\n","    \"ts\": 1631262068462487, \"dur\": 311,\n"]}]},{"cell_type":"markdown","metadata":{"id":"-KT288cvyFVc"},"source":["You can examine the sequence of profiled operators and CUDA kernels\n","in Chrome trace viewer (``chrome://tracing``):\n","\n","![](https://github.com/pytorch/tutorials/blob/gh-pages/_static/img/trace_img.png?raw=1)\n","\n","   :scale: 25 %\n","\n"]},{"cell_type":"markdown","metadata":{"id":"BC1a70eNyFVc"},"source":["6. Examining stack traces\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","\n","Profiler can be used to analyze Python and TorchScript stack traces:\n","\n"]},{"cell_type":"code","metadata":{"id":"Ga4M5riGyFVd"},"source":["with profile(\n","    activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA],\n","    with_stack=True,\n",") as prof:\n","    model(inputs)\n","\n","# Print aggregated stats\n","print(prof.key_averages(group_by_stack_n=5).table(sort_by=\"self_cuda_time_total\", row_limit=2))\n","\n","# (omitting some columns)\n","# -------------------------  -----------------------------------------------------------\n","#                      Name  Source Location\n","# -------------------------  -----------------------------------------------------------\n","# aten::thnn_conv2d_forward  .../torch/nn/modules/conv.py(439): _conv_forward\n","#                            .../torch/nn/modules/conv.py(443): forward\n","#                            .../torch/nn/modules/module.py(1051): _call_impl\n","#                            .../site-packages/torchvision/models/resnet.py(63): forward\n","#                            .../torch/nn/modules/module.py(1051): _call_impl\n","#\n","# aten::thnn_conv2d_forward  .../torch/nn/modules/conv.py(439): _conv_forward\n","#                            .../torch/nn/modules/conv.py(443): forward\n","#                            .../torch/nn/modules/module.py(1051): _call_impl\n","#                            .../site-packages/torchvision/models/resnet.py(59): forward\n","#                            .../torch/nn/modules/module.py(1051): _call_impl\n","#\n","# -------------------------  -----------------------------------------------------------\n","# Self CPU time total: 34.016ms\n","# Self CUDA time total: 11.659ms"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"AUsAwwvzyFVe"},"source":["Note the two convolutions and the two callsites in ``torchvision/models/resnet.py`` script.\n","\n","(Warning: stack tracing adds an extra profiling overhead.)\n","\n"]},{"cell_type":"markdown","metadata":{"id":"Caw7ZhaLyFVe"},"source":["7. Visualizing data as a flamegraph\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","\n","Execution time (``self_cpu_time_total`` and ``self_cuda_time_total`` metrics) and stack traces\n","can also be visualized as a flame graph. To do this, first export the raw data using ``export_stacks`` (requires ``with_stack=True``):\n","\n"]},{"cell_type":"code","metadata":{"id":"lndU5ibwyFVf"},"source":["prof.export_stacks(\"/tmp/profiler_stacks.txt\", \"self_cuda_time_total\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"RECwQeTzyFVf"},"source":["We recommend using e.g. `Flamegraph tool <https://github.com/brendangregg/FlameGraph>`_ to generate an\n","interactive SVG:\n","\n"]},{"cell_type":"code","metadata":{"id":"It_Kql4TyFVf"},"source":["# git clone https://github.com/brendangregg/FlameGraph\n","# cd FlameGraph\n","# ./flamegraph.pl --title \"CUDA time\" --countname \"us.\" /tmp/profiler_stacks.txt > perf_viz.svg"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lCKzq_sKyFVg"},"source":["![](https://github.com/pytorch/tutorials/blob/gh-pages/_static/img/perf_viz.png?raw=1)\n","\n","   :scale: 25 %\n","\n"]},{"cell_type":"markdown","metadata":{"id":"RkovcmiGyFVg"},"source":["8. Using profiler to analyze long-running jobs\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","\n","PyTorch profiler offers an additional API to handle long-running jobs\n","(such as training loops). Tracing all of the execution can be\n","slow and result in very large trace files. To avoid this, use optional\n","arguments:\n","\n","- ``schedule`` - specifies a function that takes an integer argument (step number)\n","  as an input and returns an action for the profiler, the best way to use this parameter\n","  is to use ``torch.profiler.schedule`` helper function that can generate a schedule for you;\n","- ``on_trace_ready`` - specifies a function that takes a reference to the profiler as\n","  an input and is called by the profiler each time the new trace is ready.\n","\n","To illustrate how the API works, let's first consider the following example with\n","``torch.profiler.schedule`` helper function:\n","\n"]},{"cell_type":"code","metadata":{"id":"Ca7ZnMonyFVh"},"source":["from torch.profiler import schedule\n","\n","my_schedule = schedule(\n","    skip_first=10,\n","    wait=5,\n","    warmup=1,\n","    active=3,\n","    repeat=2)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2MFPEgUYyFVh"},"source":["Profiler assumes that the long-running job is composed of steps, numbered\n","starting from zero. The example above defines the following sequence of actions\n","for the profiler:\n","\n","1. Parameter ``skip_first`` tells profiler that it should ignore the first 10 steps\n","   (default value of ``skip_first`` is zero);\n","2. After the first ``skip_first`` steps, profiler starts executing profiler cycles;\n","3. Each cycle consists of three phases:\n","\n","   - idling (``wait=5`` steps), during this phase profiler is not active;\n","   - warming up (``warmup=1`` steps), during this phase profiler starts tracing, but\n","     the results are discarded; this phase is used to discard the samples obtained by\n","     the profiler at the beginning of the trace since they are usually skewed by an extra\n","     overhead;\n","   - active tracing (``active=3`` steps), during this phase profiler traces and records data;\n","4. An optional ``repeat`` parameter specifies an upper bound on the number of cycles.\n","   By default (zero value), profiler will execute cycles as long as the job runs.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"dX9JDpNPyFVh"},"source":["Thus, in the example above, profiler will skip the first 15 steps, spend the next step on the warm up,\n","actively record the next 3 steps, skip another 5 steps, spend the next step on the warm up, actively\n","record another 3 steps. Since the ``repeat=2`` parameter value is specified, the profiler will stop\n","the recording after the first two cycles.\n","\n","At the end of each cycle profiler calls the specified ``on_trace_ready`` function and passes itself as\n","an argument. This function is used to process the new trace - either by obtaining the table output or\n","by saving the output on disk as a trace file.\n","\n","To send the signal to the profiler that the next step has started, call ``prof.step()`` function.\n","The current profiler step is stored in ``prof.step_num``.\n","\n","The following example shows how to use all of the concepts above:\n","\n"]},{"cell_type":"code","metadata":{"id":"Tx3-H2u8yFVi"},"source":["def trace_handler(p):\n","    output = p.key_averages().table(sort_by=\"self_cuda_time_total\", row_limit=10)\n","    print(output)\n","    p.export_chrome_trace(\"/tmp/trace_\" + str(p.step_num) + \".json\")\n","\n","with profile(\n","    activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA],\n","    schedule=torch.profiler.schedule(\n","        wait=1,\n","        warmup=1,\n","        active=2),\n","    on_trace_ready=trace_handler\n",") as p:\n","    for idx in range(8):\n","        model(inputs)\n","        p.step()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WZ4mS0oVyFVi"},"source":["Learn More\n","----------\n","\n","Take a look at the following recipes/tutorials to continue your learning:\n","\n","-  `PyTorch Benchmark <https://pytorch.org/tutorials/recipes/recipes/benchmark.html>`_\n","-  `PyTorch Profiler with TensorBoard <https://pytorch.org/tutorials/intermediate/tensorboard_profiler_tutorial.html>`_ tutorial\n","-  `Visualizing models, data, and training with TensorBoard <https://pytorch.org/tutorials/intermediate/tensorboard_tutorial.html>`_ tutorial\n","\n","\n"]}]}