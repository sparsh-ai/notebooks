{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"2022-01-25-cmf.ipynb","provenance":[{"file_id":"https://github.com/recohut/nbs/blob/main/raw/T316836%20%7C%20Collective%20Matrix%20Factorization%20on%20ML-1m.ipynb","timestamp":1644670730069}],"collapsed_sections":[],"toc_visible":true,"authorship_tag":"ABX9TyPyu2rAIt4+kg4VLdErcd6x"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"Np5vPhJjEpNK"},"source":["# Collective Matrix Factorization on ML-1m"]},{"cell_type":"markdown","metadata":{"id":"_Klvaj0QEtkf"},"source":["## CMF on dummy dataset using PyCMF library"]},{"cell_type":"code","metadata":{"id":"k6CKGfEu_E_U"},"source":["!pip install git+https://github.com/smn-ailab/PyCMF"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9y-m5IzJ_IK4"},"source":["import numpy as np                                                                                          \n","import pycmf\n","\n","X = np.abs(np.random.randn(5, 4)); Y = np.abs(np.random.randn(4, 1))\n","model = pycmf.CMF(n_components=4)\n","U, V, Z = model.fit_transform(X, Y)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qqEXG-3i_Mbc","executionInfo":{"status":"ok","timestamp":1635744822247,"user_tz":-330,"elapsed":14,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}},"outputId":"3a4cd6d3-2b3e-4531-cb30-e418ab00a84c"},"source":["np.linalg.norm(X - U @ V.T) / np.linalg.norm(X)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.0015218365051785086"]},"metadata":{},"execution_count":7}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"q8a_IZUQ_Kr8","executionInfo":{"status":"ok","timestamp":1635744822249,"user_tz":-330,"elapsed":13,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}},"outputId":"1a62427d-37cc-435d-f266-7ee6ad9637ed"},"source":["np.linalg.norm(Y - V @ Z.T) / np.linalg.norm(Y)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.0007014936205661572"]},"metadata":{},"execution_count":8}]},{"cell_type":"markdown","metadata":{"id":"TpShyMNvExcz"},"source":["## CMF on ML-1m"]},{"cell_type":"code","metadata":{"id":"aYmPfufUAvtm"},"source":["!git clone https://github.com/VincentLiu3/CMF.git\n","%cd CMF"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"XDXyS0lV_k1s"},"source":["import numpy\n","from functools import reduce\n","\n","def logistic(vec):\n","\tout_vec = 1.0 / (1.0 + numpy.exp(-1 * vec))\n","\treturn out_vec\n","\n","def d_logistic(vec):\n","\tlog_vec = logistic(vec)\n","\tout_vec = numpy.multiply(log_vec, 1-log_vec)\n","\treturn out_vec\n","\n","'''\n","def loss_for_one_row(Xi, U, V, reg):\n","\tYi = numpy.dot(U, V.T)\n","\tloss = sum( pow( Xi-Yi, 2) ) + reg * numpy.linalg.norm(U) / 2\n","\treturn loss\n","def Armijo_line_search(U, one_step, Xi, V, reg):\n","\tprev_loss = \n","\twhile True:\n","\t\tU -= one_step\n","\t\tloss = loss_for_one_row(Xi, U, V, reg)\n","\t\tif prev_loss\n","'''\n","\n","def newton_update(Us, Xs, Xts, rc_schema, alphas, modes, K, reg, learn_rate, Ns, t):\n","\tnsize = Ns[t] # size for entity t t, e.g. number of user for type 0 (user) \n","\tU_t = Us[t] # random factor matrix for entity t\n","\n","\tA = numpy.zeros((K, K)) # place holders for hessian: q'(Ui)\n","\tb = numpy.zeros(K) # place holders for gradient: q(Ui)\n","\n","\tfor i in range(nsize): # randomly pick one instance in X\n","\t\tA[:] = 0\n","\t\tb[:] = 0\n","\t\tfor j in range(len(Xs)):  # for j = 1~number of relations\n","\n","\t\t\tif alphas[j] == 0:\n","\t\t\t\tcontinue\n","\t\t\t\n","\t\t\tif rc_schema[j, 0] == t or rc_schema[j, 1] == t:\n","\t\t\t\t# only need to update if type t is in relation j\n","\t\t\t\tif rc_schema[j, 0] == t:\n","\t\t\t\t\t# if type t = x-axis of relation j \n","\t\t\t\t\tX = Xts[j] # transpose X (n2, n1)\n","\t\t\t\t\tU = U_t[i, :] # (1 * k)\n","\t\t\t\t\tV = Us[rc_schema[j, 1]]  # (n2 * k)\n","\t\t\t\t\n","\t\t\t\t\tdata = X.data # content of the matrix\n","\t\t\t\t\tindptr = X.indptr \n","\t\t\t\t\tindices = X.indices \n","\n","\t\t\t\t\tind_i0, ind_i1 = (indptr[i], indptr[i+1]) \n","\t\t\t\t\t# Step 1: XiV\n","\t\t\t\t\tif ind_i0 == ind_i1:\n","\t\t\t\t\t\tif modes[j] == \"sparse\": # sparse -> no data on the i-th row of X -> no need to update\n","\t\t\t\t\t\t\tcontinue\n","\t\t\t\t\t\telse: # dense/log_dense -> 0 vector\n","\t\t\t\t\t\t \tXiV = numpy.zeros(K) # (1 * k)\n","\t\t\t\t\telse:\n","\t\t\t\t\t\tinds_i = indices[ind_i0:ind_i1] # index to the non-zero elements on the i-th row of X\n","\t\t\t\t\t\tdata_i = data[ind_i0:ind_i1] # non-zero element on the i-th row of X (1 * x)\n","\t\t\t\t\t\tXiV = numpy.dot(data_i, V[inds_i, :]) # (1 * x) (x * k) -> (1 * k)\n","\t\n","\t\t\t\t\tif modes[j] == \"sparse\":\n","\t\t\t\t\t\tV = V[inds_i, :] # only need those column factors for non-zero element in the i-th row\n","\t\t\t\t\t\t# Step 2: UVt\n","\t\t\t\t\t\tUiVt = numpy.dot(U, V.T) # (1*k) (k*n2) -> (1 * x)\n","\t\t\t\t\t\t# Step 3: UVtV\n","\t\t\t\t\t\tUiVtV = numpy.dot(UiVt, V)  # (1 * k)\n","\t\t\t\t\t\t# Step 4: VtDiV\n","\t\t\t\t\t\tHes = numpy.dot(numpy.multiply(V.T, UiVt), V)\n","\n","\t\t\t\t\telif modes[j] == 'log_dense':\n","\t\t\t\t\t\tUiVt = numpy.dot(U, V.T) # (1 * n2)\n","\t\t\t\t\t\tUiVtV = numpy.dot(logistic(UiVt), V) # (1 * k)\n","\t\t\t\t\t\tHes = numpy.dot(numpy.multiply(V.T, d_logistic(UiVt)), V)\n","\t\t\t\t\t\n","\t\t\t\t\telif modes[j] == 'dense':\n","\t\t\t\t\t\tUiVt = numpy.dot(U, V.T) # (1 * n2)\n","\t\t\t\t\t\tUiVtV = numpy.dot(UiVt, V)  # (1 * k)\n","\t\t\t\t\t\tHes = numpy.dot(numpy.multiply(V.T, UiVt), V)\n","\n","\t\t\t\t\tA += alphas[j] * Hes\n","\t\t\t\t\tb += alphas[j] * (UiVtV - XiV)\n","\n","\t\t\t\telif rc_schema[j, 1] == t:\n","\t\t\t\t\t# if type t = x-axis of relation j \n","\t\t\t\t\tX = Xs[j] # (n1 * n2)\n","\t\t\t\t\tU = Us[rc_schema[j, 0]] # (n1 * k)\n","\t\t\t\t\tV = U_t[i, :] # (1 * k)\n","\n","\t\t\t\t\tdata = X.data # content of the matrix\n","\t\t\t\t\tindptr = X.indptr\n","\t\t\t\t\tindices = X.indices\n","\n","\t\t\t\t\tind_i0, ind_i1 = (indptr[i], indptr[i+1])\n","\t\t\t\t\tif ind_i0 == ind_i1: \n","\t\t\t\t\t\tif modes[j] == \"sparse\": # no data on the i-th column of X -> no need to update\n","\t\t\t\t\t\t\tcontinue\n","\t\t\t\t\t\telse:\n","\t\t\t\t\t\t\tXiU = numpy.zeros(K) # (1 * k)\n","\t\t\t\t\telse:\n","\t\t\t\t\t\tinds_i = indices[ind_i0:ind_i1] \n","\t\t\t\t\t\tdata_i = data[ind_i0:ind_i1] # non-zero elements on the j-th column of X (1 * x)\n","\t\t\t\t\t\tXiU = numpy.dot(data_i, U[inds_i, :]) # (1 * k)\n","\n","\t\t\t\t\tif modes[j] == \"sparse\":\n","\t\t\t\t\t\tU = U[inds_i, :] # (x * k)\n","\t\t\t\t\t\tUVt = numpy.dot(U, V.T) # (x * k) (k * 1) -> (x * 1)\n","\t\t\t\t\t\tUVtU = numpy.dot(UVt.T, U) # (1 * k)\n","\t\t\t\t\t\tHes = numpy.dot(numpy.multiply(U.T, UVt), U)\n","\n","\t\t\t\t\telif modes[j] == 'log_dense':\n","\t\t\t\t\t\tUVt = numpy.dot(U, V.T) # (x * k) (k * 1) -> (x * 1)\n","\t\t\t\t\t\tUVtU = numpy.dot(logistic(UVt).T, U) # (1 * k)\n","\t\t\t\t\t\tHes = numpy.dot(numpy.multiply(U.T, d_logistic(UVt)), U)\n","\n","\t\t\t\t\telif modes[j] == 'dense':\n","\t\t\t\t\t\tUVt = numpy.dot(U, V.T) # (x * k) (k * 1) -> (x * 1)\n","\t\t\t\t\t\tUVtU = numpy.dot(UVt.T, U) # (1 * k)\n","\t\t\t\t\t\tHes = numpy.dot(numpy.multiply(U.T, UVt), U)\n","\n","\t\t\t\t\tA += alphas[j] * Hes\n","\t\t\t\t\tb += alphas[j] * (UVtU - XiU)\n","\t\t\t\n","\t\tif numpy.all(b == 0):\n","\t\t\tcontinue\n","\t\t\t\n","\t\t# regularizer\n","\t\tA += reg * numpy.eye(K, K)\n","\t\tb += reg * U_t[i, :].copy() # the previous factor for i-th data\n","\n","\t\td = numpy.dot(b, numpy.linalg.inv(A))\n","\t\tUs[t][i, :] -= learn_rate * d  \n","\n","\t# return change\n","\n","def old_newton_update(Us, Xs, Xts, rc_schema, alphas, modes, K, reg, learn_rate, Ns, t):\n","\t'''\n","\tcode from http://ihome.ust.hk/~zluab/code/\n","\t'''\n","\tassert(t <= len(Ns) and t >= 0)\n","\teyeK = reg * numpy.eye(K, K)\n","\tN = Ns[t] # number of instances for type t\n","\tV = Us[t] # U\n","\tA = numpy.zeros((K, K)) # place holders for hessian\n","\tb = numpy.zeros(K) # place holders for gradient\n","\tUtUs = numpy.empty(len(Xs),object)\n","\t# change = 0\n","\tfor j in range(len(Xs)):\n","\t\tif modes[j] == 'dense':\n","\t\t\tif rc_schema[j, 0] == t:\t\t\n","\t\t\t\tU = Us[rc_schema[j, 1]]\n","\t\t\telse:\n","\t\t\t\tU = Us[rc_schema[j, 0]] \n","\t\t\tUtUs[j] = numpy.dot(U.T,U) # UtUs = VtV\n","\tfor i in range(N):\n","\t\tA[:] = 0\n","\t\tb[:] = 0\n","\t\tfor j in range(len(Xs)):\n","\t\t\tif alphas[j] == 0:\n","\t\t\t\tcontinue\n","\t\t\tif rc_schema[j, 0] == t or rc_schema[j, 1] == t:\n","\t\t\t\tif rc_schema[j, 0] == t:\n","\t\t\t\t\tX = Xts[j]\n","\t\t\t\t\tU = Us[rc_schema[j, 1]] # V\n","\t\t\t\telse:\n","\t\t\t\t\tX = Xs[j]\n","\t\t\t\t\tU = Us[rc_schema[j, 0]]\n","\t\t\t\tdata = X.data\n","\t\t\t\tindptr = X.indptr\n","\t\t\t\tindices = X.indices\n","\t\t\t\t\n","\t\t\t\tind_i0, ind_i1 = (indptr[i], indptr[i+1])\n","\t\t\t\tif ind_i0 == ind_i1:\n","\t\t\t\t\tcontinue\n","\t\t\t\t\n","\t\t\t\tinds_i = indices[ind_i0:ind_i1] \n","\t\t\t\tdata_i = data[ind_i0:ind_i1]\n","\t\t\t\t\n","\t\t\t\tif modes[j] == \"dense\": # square loss, dense binary representation\n","\t\t\t\t\tUtU = UtUs[j]\n","\t\t\t\t\tUtemp = U[inds_i, :]\n","\t\t\t\t\tA += alphas[j] * UtU\n","\t\t\t\t\tb += alphas[j] * (numpy.dot(UtU,V[i,:])-numpy.dot(data_i, Utemp))\n","\t\t\t\telif modes[j] == \"log_dense\": # logistic loss\n","\t\t\t\t\tXi = numpy.dot(U, V[i, :])\n","\t\t\t\t\tYi = - 1 * numpy.ones(U.shape[0])\n","\t\t\t\t\tYi[inds_i] = 1\n","\t\t\t\t\t# (sigma(yx)-1)\n","\t\t\t\t\tWi = 1.0 / (1 + numpy.exp(-1 * numpy.multiply(Yi, Xi))) - 1 \n","\t\t\t\t\tWi = numpy.multiply(Wi, Yi)\n","\t\t\t\t\tgv = numpy.dot(Wi, U)\n","\t\t\t\t\t# compute sigmoid(x)\n","\t\t\t\t\tAi = 1 / (1 + numpy.exp(-Xi))\n","\t\t\t\t\tAi = numpy.multiply(Ai, 1 - Ai)\n","\t\t\t\t\tAi = Ai.reshape(Ai.size, 1)\n","\t\t\t\t\tAiU = numpy.multiply(Ai, U)\n","\t\t\t\t\tHv = numpy.dot(AiU.T, U)\n","\t\t\t\t\tA += alphas[j] * Hv\n","\t\t\t\t\tb += alphas[j] * gv\n","\t\t\t\t\t\n","\t\t\t\telif modes[j] == \"sparse\": # square loss\n","\t\t\t\t\tUtemp = U[inds_i, :]\n","\t\t\t\t\tUtU = numpy.dot(Utemp.T, Utemp)\n","\t\t\t\t\tA += alphas[j] * UtU\n","\t\t\t\t\tb += alphas[j] * (numpy.dot(UtU, V[i,:])-numpy.dot(data_i, Utemp))\n","\t\t\t\t\t\n","\t\tA += eyeK\n","\t\tb += reg*V[i, :]\n","\t\td = numpy.dot(numpy.linalg.inv(A), b)\n","\t\tvi = V[i,:].copy()\n","\t\tV[i, :] -= learn_rate*d\n","\t# return change\n","\n","# http://sebastianruder.com/optimizing-gradient-descent/"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"rmtAVlsW_oNU"},"source":["import numpy\n","import scipy.sparse\n","import os.path\n","\n","def read_dense_data(train_file, test_file, user_file, item_file, feature_mat_type):\n","    return \n","\n","def loadTripleData(filename):\n","    '''\n","    laod triple data (row, column, value) to csc_matrix format\n","    '''\n","    fData = numpy.loadtxt(filename, delimiter=',').T\n","    fData = fData.astype(int)\n","    fData = scipy.sparse.coo_matrix((fData[2],(fData[0],fData[1]))).tocsc()\n","    return(fData)\n","\n","def read_triple_data(train, test, user, item, feature_mat_type):\n","    '''\n","    read data from three column format (row, column, value)\n","    '''\n","    assert( feature_mat_type in ['sparse', 'dense', 'log_dense'] ), 'Unrecognized link function'\n","\n","    # need to make sure training & testing data with the same shapes as user and item features\n","    num_user = num_item = 0\t\n","    if user != '':\n","        X_userFeat = loadTripleData(user)\n","        num_user = X_userFeat.shape[0]\n","    if item != '':\n","        X_itemFeat = loadTripleData(item)\n","        num_item = X_itemFeat.shape[0]\n","\n","    Dtrain = numpy.loadtxt(train, delimiter = ',').T\n","    Dtest = numpy.loadtxt(test, delimiter = ',').T\n","    num_user = int( max(Dtrain[0].max(), Dtest[0].max(), num_user-1) ) + 1\n","    num_item = int( max(Dtrain[1].max(), Dtest[1].max(), num_item-1) ) + 1\n","    X_train = scipy.sparse.coo_matrix((Dtrain[2],(Dtrain[0],Dtrain[1])), shape=(num_user, num_item)).tocsc()\n","    X_test = scipy.sparse.coo_matrix((Dtest[2],(Dtest[0],Dtest[1])), shape=(num_user, num_item)).tocsc()\n","    # transform to csc format\n","    # X_train = scipy.sparse.csc_matrix(X_train)\n","    # X_test = scipy.sparse.csc_matrix(X_test)\n","\n","    # user or item features\n","    if user != '' and item != '':\n","        Xs_trn = [X_train, X_userFeat, X_itemFeat]\n","        Xs_tst = [X_test, None, None]\n","        \n","        rc_schema = numpy.array([[0, 1], [0, 2], [1, 3]])\n","        # [row entity number, column entity number]\n","        # 0=user, 1=item, 2=userFeat, 3=itemFeat\n","\n","        modes = ['sparse', feature_mat_type, feature_mat_type]\n","        # modes of each relation: sparse, dense or log_dense\n","        # dense if Wij = 1 for all ij \n","        # sparse if Wij = 1 if Xij>0\n","        # log if link function = logistic\n","\n","    elif user == '' and item != '':\n","        Xs_trn = [X_train, X_itemFeat]\n","        Xs_tst = [X_test, None]\n","\n","        rc_schema = numpy.array([[0, 1], [1, 2]]) # 0=user, 1=item, 2=itemFeat\n","        modes = ['sparse', feature_mat_type]\n","\n","    elif user != '' and item == '':\n","        Xs_trn = [X_train, X_userFeat]\n","        Xs_tst = [X_test, None]\n","\n","        rc_schema = numpy.array([[0, 1], [0, 2]]) # 0=user, 1=item, 2=userFeat\n","        modes = ['sparse', feature_mat_type]\n","\n","    elif user == '' and item == '':\n","        assert False, 'No user and item features.'\n","        Xs_trn = [X_train]\n","        Xs_tst = [X_test]\n","\n","        rc_schema = numpy.array([[0, 1]])\n","        modes = ['sparse']\n","\n","    return [Xs_trn, Xs_tst, rc_schema, modes] \n","\n","def get_config(Xs, rc_schema):\n","    '''\n","    get neccessary configurations of the given relation\n","    ---------------------\n","    S = number of entity\n","    Ns = number of instances for each entity\n","    '''\n","    assert(len(Xs)==len(rc_schema)), \"rc_schema lenth must be the same as input data.\"\n","\n","    S = rc_schema.max() + 1\n","    Ns = -1 * numpy.ones(S, int)\n","    for i in range(len(Xs)):\n","        ri = rc_schema[i, 0]\n","        ci = rc_schema[i, 1]\n","        \n","        [m, n] = Xs[i].shape\n","        \n","        if Ns[ri] < 0:\n","            Ns[ri] = m\n","        else:\n","            assert(Ns[ri] == m), \"rc_schema does not match data.\"\n","                            \n","        if Ns[ci] < 0:\n","            Ns[ci] = n\n","        else:\n","            assert(Ns[ci] == n), \"rc_schema does not match data.\"\n","    return [S, Ns]\n","\n","def RMSE(X, Y):\n","    '''\n","    X is prediction, Y is ground truth\n","    Both X and Y should be scipy.sparse.csc_matrix\n","    '''\n","    assert(X.size == Y.size and all(X.indices == Y.indices) and all(X.indptr == Y.indptr) and X.size > 0)\n","    return numpy.sqrt(sum(pow(X.data - Y.data, 2)) / X.size)\n","\n","def MAE(X, Y):\n","    assert(X.size == Y.size and all(X.indices == Y.indices) and all(X.indptr == Y.indptr) and X.size > 0)\n","    return sum(abs(X.data - Y.data)) / X.size\n","\n","def check_modes(modes):\n","    for mode in modes:\n","        if mode != 'sparse' and mode != 'dense' and mode != 'log_dense':\n","            assert False, 'Unrecognized mode: {}'.format(mode)\n","\n","def string2list(input_string, num, sep='-'):\n","    string_list = input_string.split(sep)\n","    assert( len(string_list) == num ), 'argument alphas must be the same length as numbers of relations.'\n","    return [float(x) for x in string_list]\n","\n","def save_result(args, rmse):\n","    if args.user != '' and args.item != '':\n","        cmf_type = 'useritem'\n","    elif args.user == '' and args.item != '':\n","        cmf_type = 'item'\n","    elif args.user != '' and args.item == '':\n","        cmf_type = 'user'\n","    elif args.user == '' and args.item == '':\n","        cmf_type = 'none'\n","\n","    if args.out != '':\n","        if os.path.exists(args.out) is False:\n","            with open(args.out, 'w') as fp:\n","                fp.write('type,k,reg,lr,tol,alphas,RMSE\\n')\n","        with open(args.out, 'a') as fp:\n","            fp.write('{},{},{},{},{},{},{:.4f}\\n'.format(cmf_type, args.k, args.reg, args.lr, args.tol, args.alphas, rmse))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ETbsC3iu_aa5","executionInfo":{"status":"ok","timestamp":1635745607975,"user_tz":-330,"elapsed":171727,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}},"outputId":"f36c4fd4-48dc-42ef-f7d7-e095b8784051"},"source":["import numpy\n","import time\n","import logging\n","import scipy.sparse\n","import argparse\n","\n","class Args:\n","    train = 'data/ml-1m/train.txt' # Training file\n","    test = 'data/ml-1m/test.txt' # Testing file\n","    user = 'data/ml-1m/user.txt' # User features file\n","    item = 'data/ml-1m/item.txt' # Item features file\n","    out = 'ml-1m.txt' # File where fianl result will be saved\n","    alphas = '0.5-0.5-0.5' # Alpha in [0, 1] weights the relative importance of relations\n","    link = 'log_dense' # link function for feature relations (dense or log_dense)\n","    k = 8 # Dimension of latent fectors\n","    reg = 0.1 # Regularization for latent facotrs\n","    lr = 0.1 # Initial learning rate for training\n","    iter = 50 # Max training iteration\n","    tol = 0 # Tolerant for change in training loss\n","    verbose = 1 # Verbose or not (1 for INFO, 0 for WARNING)\n","\n","args = Args()\n","\n","\n","def learn(Xs, Xstst, rc_schema, modes, alphas, K, reg, learn_rate, max_iter, tol):\n","    assert(rc_schema.shape[0] == len(Xs) and rc_schema.shape[1] == 2) # schema match data\n","    assert(numpy.all(rc_schema[:, 0] != rc_schema[:, 1])) # should not have symmetric relations\n","    assert(rc_schema.shape[0] == len(alphas))\n","    assert(rc_schema.shape[0] == len(modes))\n","    check_modes(modes) \n","\n","    Xts = [None] * len(Xs)\n","    for i in range(len(Xs)):\n","        if Xs[i] is not None:\n","            Xts[i] = scipy.sparse.csc_matrix(Xs[i].T) # Transpose\n","            Xs[i] = scipy.sparse.csc_matrix(Xs[i]) # no Transpose\n","        if Xstst[i] is not None:\n","            Xstst[i] = scipy.sparse.csc_matrix(Xstst[i])\n","\n","    [S, Ns] = get_config(Xs, rc_schema)\n","\n","    # randomly initialize factor matrices with small values\n","    Us = [None] * S\n","    for i in range(S):\n","        Us[i] = numpy.random.rand(Ns[i], K) * numpy.sqrt(1/K)  # so initial prediction will be in [0, 5]\n","\n","    Ys = predict(Us, Xs, rc_schema, modes)\n","    prev_loss = loss(Us, Xs, Ys, rc_schema, modes, alphas, reg, S)\n","    i = 0\n","    while i < max_iter:\n","        i += 1\n","        tic = time.time()\n","\n","        # training        \n","        for t in range(S): # update factors for entity t\n","            newton_update(Us, Xs, Xts, rc_schema, alphas, modes, K, reg, learn_rate, Ns, t)\n","        \n","        # evaluation\n","        Ys = predict(Us, Xs, rc_schema, modes)\n","        training_loss = loss(Us, Xs, Ys, rc_schema, modes, alphas, reg, S)\n","        train_rmse = RMSE(Xs[0], Ys[0])\n","        change_rate = (training_loss-prev_loss)/prev_loss * 100\n","        prev_loss = training_loss\n","        \n","        Ystst = predict(Us, Xstst, rc_schema, modes)\n","        test_rmse = RMSE(Xstst[0], Ystst[0])\n","\n","        toc = time.time()\n","        logger.info('Iter {}/{}. Time: {:.1f}'.format(i, max_iter, toc - tic))\n","        logger.info('Training Loss: {:.1f} (change {:.2f}%). Training RMSE: {:.2f}. Testing RMSE: {:.2f}'.format(training_loss, change_rate, train_rmse, test_rmse))\n","    \n","        # early stop\n","        if tol!=0 and i!=1 and change_rate > -tol :\n","            break\n","\n","    return Us\n","\n","def loss(Us, Xs, Ys, rc_schema, modes, alphas, reg, num_entities):\n","\t'''\n","\tCalculate objective loss\n","\tSee page 4: Generalizing to Arbitrary Schemas\n","\t'''\n","\tassert(rc_schema.shape[0] == len(Xs) and rc_schema.shape[1] == 2)\n","\n","\tres = 0\n","\tnum_relation = len(Xs)\n","\t# computing regularization for each latent factor\n","\tfor i in range(num_entities):\n","\t\tfor j in range(num_relation):\n","\t\t\tif rc_schema[j, 0]==i or rc_schema[j, 1]==i:\n","\t\t\t\tres += alphas[j] * reg * numpy.linalg.norm(Us[i].flat) / 2 # l2 norm\n","\n","\t# computing loss for each relation\n","\tfor j in range(num_relation):     \n","\t\talpha_j = alphas[j]\n","\t\tif Xs[j] is None or Ys[j] is None or alpha_j == 0:\n","\t\t\tcontinue\n","\n","\t\t# X = scipy.sparse.csc_matrix(Xs[j])\n","\t\t# Y = scipy.sparse.csc_matrix(Ys[j])\n","\t\tX = Xs[j]\n","\t\tY = Ys[j]\n","\t\t\n","\t\tif modes[j] == 'sparse':\n","\t\t\tassert( X.size == Y.size )\n","\t\t\tres += alpha_j * numpy.sum(pow(X.data - Y.data, 2))\n","\n","\t\telif modes[j] == 'dense' or modes[j] == 'log_dense':\n","\t\t\tassert( numpy.all(Y.shape == X.shape) )\n","\t\t\tres += alpha_j * numpy.sum(pow(X.toarray() - Y.toarray(), 2))   \n","\n","\treturn res\n","\n","def predict(Us, Xs, rc_schema, modes):\n","    '''\n","    see page 3: RELATIONAL SCHEMAS\n","    return a list of csc_matrix\n","    '''\n","    Ys = []\n","    for i in range(len(Xs)): # i = 1\n","        if Xs[i] is None:\n","        \t# no need to predict Y\n","            Ys.append(None) \n","            continue\n","        \n","        X = Xs[i]\n","        U = Us[rc_schema[i, 0]]\n","        V = Us[rc_schema[i, 1]]\n","\n","        if modes[i] == 'sparse':\n","            # predict only for non-zero elements in X\n","            # X = scipy.sparse.csc_matrix(X)\n","            data = X.data.copy()\n","            indices = X.indices.copy()\n","            indptr = X.indptr.copy()\n","           \n","            for j in range(X.shape[1]): # for each column in X\n","                inds_j = indices[indptr[j]:indptr[j+1]]\n","                # indptr[j]:indptr[j+1] points to the data on j-th column of X\n","                if inds_j.size == 0:\n","                    continue\n","                data[indptr[j]:indptr[j+1]] = numpy.dot(U[inds_j, :], V[j, :])\n","\n","            Y = scipy.sparse.csc_matrix((data, indices, indptr), X.shape)\n","            Ys.append(Y)\n","\n","        elif modes[i] == 'dense':\n","            # predict for all elements in X\n","            Y = numpy.dot(U, V.T)\n","            Y = scipy.sparse.csc_matrix(Y)\n","            Ys.append(Y)\n","\n","        elif modes[i] == 'log_dense':\n","            # predict for all elements in X\n","            Y = numpy.dot(U, V.T)\n","            Y = logistic(Y)\n","            Y = scipy.sparse.csc_matrix(Y)\n","            Ys.append(Y)\n","\n","    return Ys\n","\n","def run_cmf(Xs_trn, Xs_tst, rc_schema, modes, alphas, args):\n","    '''\n","    run cmf\n","    '''\n","    start_time = time.time()\n","\n","    Us = learn(Xs_trn, Xs_tst, rc_schema, modes, alphas, args.k, args.reg, args.lr, args.iter, args.tol)\n","    Ys_tst = predict(Us, Xs_tst, rc_schema, modes)\n","    rmse = RMSE(Xs_tst[0], Ys_tst[0])\n","\n","    end_time = time.time()\n","    logger.info('RMSE: {:.4f}'.format(rmse))\n","    logger.info('Total Time: {:.0f} s'.format(end_time - start_time) )\n","    \n","    save_result(args, rmse)\n","\n","    return \n","\n","\n","if __name__ == \"__main__\":\n","    \n","\t[Xs_trn, Xs_tst, rc_schema, modes] = read_triple_data(args.train, args.test, args.user, args.item, args.link)\n","\n","\tif(args.verbose == 1):\n","\t\tlogging.basicConfig(level=logging.INFO, format='[%(levelname)s] %(message)s')\n","\telse:\n","\t\tlogging.basicConfig(level=logging.WARNING, format='[%(levelname)s] %(message)s')\n","\t\n","\tlogger = logging.getLogger()\n","\t[S, Ns] = get_config(Xs_trn, rc_schema)\n","\talphas = string2list(args.alphas, len(modes))\n","\n","\tlogger.info('------------------- CMF -------------------')\n","\tlogger.info('Data: Number of instnace for each entity = {}'.format(list(Ns)))\n","\tlogger.info('Data: Training size = {}. Testing size = {}'.format(Xs_trn[0].size, Xs_tst[0].size))\n","\tlogger.info('Settings: k = {}. reg = {}. lr = {}. alpha = {}. modes = {}.'.format(args.k, args.reg, args.lr, alphas, modes))\n","\n","\trun_cmf(Xs_trn, Xs_tst, rc_schema, modes, alphas, args)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["[INFO] ------------------- CMF -------------------\n","[INFO] Data: Number of instnace for each entity = [6040, 3883, 29, 99]\n","[INFO] Data: Training size = 900188. Testing size = 100021\n","[INFO] Settings: k = 8. reg = 0.1. lr = 0.1. alpha = [0.5, 0.5, 0.5]. modes = ['sparse', 'log_dense', 'log_dense'].\n","[INFO] Iter 1/50. Time: 3.2\n","[INFO] Training Loss: 2715840.2 (change -52.18%). Training RMSE: 2.42. Testing RMSE: 2.54\n","[INFO] Iter 2/50. Time: 3.2\n","[INFO] Training Loss: 2208268.6 (change -18.69%). Training RMSE: 2.18. Testing RMSE: 2.34\n","[INFO] Iter 3/50. Time: 3.3\n","[INFO] Training Loss: 1852405.4 (change -16.12%). Training RMSE: 1.99. Testing RMSE: 2.18\n","[INFO] Iter 4/50. Time: 3.2\n","[INFO] Training Loss: 1587352.4 (change -14.31%). Training RMSE: 1.85. Testing RMSE: 2.06\n","[INFO] Iter 5/50. Time: 3.2\n","[INFO] Training Loss: 1382534.2 (change -12.90%). Training RMSE: 1.73. Testing RMSE: 1.95\n","[INFO] Iter 6/50. Time: 3.2\n","[INFO] Training Loss: 1220199.3 (change -11.74%). Training RMSE: 1.62. Testing RMSE: 1.85\n","[INFO] Iter 7/50. Time: 3.2\n","[INFO] Training Loss: 1089097.6 (change -10.74%). Training RMSE: 1.53. Testing RMSE: 1.74\n","[INFO] Iter 8/50. Time: 3.3\n","[INFO] Training Loss: 981660.8 (change -9.86%). Training RMSE: 1.46. Testing RMSE: 1.65\n","[INFO] Iter 9/50. Time: 3.3\n","[INFO] Training Loss: 892572.5 (change -9.08%). Training RMSE: 1.39. Testing RMSE: 1.56\n","[INFO] Iter 10/50. Time: 3.2\n","[INFO] Training Loss: 817972.0 (change -8.36%). Training RMSE: 1.33. Testing RMSE: 1.49\n","[INFO] Iter 11/50. Time: 3.2\n","[INFO] Training Loss: 754982.0 (change -7.70%). Training RMSE: 1.28. Testing RMSE: 1.42\n","[INFO] Iter 12/50. Time: 3.2\n","[INFO] Training Loss: 701412.5 (change -7.10%). Training RMSE: 1.23. Testing RMSE: 1.37\n","[INFO] Iter 13/50. Time: 3.2\n","[INFO] Training Loss: 655567.3 (change -6.54%). Training RMSE: 1.19. Testing RMSE: 1.32\n","[INFO] Iter 14/50. Time: 3.3\n","[INFO] Training Loss: 616113.4 (change -6.02%). Training RMSE: 1.16. Testing RMSE: 1.28\n","[INFO] Iter 15/50. Time: 3.2\n","[INFO] Training Loss: 581989.5 (change -5.54%). Training RMSE: 1.12. Testing RMSE: 1.24\n","[INFO] Iter 16/50. Time: 3.3\n","[INFO] Training Loss: 552341.8 (change -5.09%). Training RMSE: 1.10. Testing RMSE: 1.21\n","[INFO] Iter 17/50. Time: 3.2\n","[INFO] Training Loss: 526477.4 (change -4.68%). Training RMSE: 1.07. Testing RMSE: 1.18\n","[INFO] Iter 18/50. Time: 3.3\n","[INFO] Training Loss: 503827.1 (change -4.30%). Training RMSE: 1.05. Testing RMSE: 1.15\n","[INFO] Iter 19/50. Time: 3.2\n","[INFO] Training Loss: 483922.0 (change -3.95%). Training RMSE: 1.03. Testing RMSE: 1.13\n","[INFO] Iter 20/50. Time: 3.2\n","[INFO] Training Loss: 466372.4 (change -3.63%). Training RMSE: 1.01. Testing RMSE: 1.11\n","[INFO] Iter 21/50. Time: 3.3\n","[INFO] Training Loss: 450852.3 (change -3.33%). Training RMSE: 0.99. Testing RMSE: 1.10\n","[INFO] Iter 22/50. Time: 3.2\n","[INFO] Training Loss: 437087.5 (change -3.05%). Training RMSE: 0.97. Testing RMSE: 1.08\n","[INFO] Iter 23/50. Time: 3.2\n","[INFO] Training Loss: 424846.6 (change -2.80%). Training RMSE: 0.96. Testing RMSE: 1.07\n","[INFO] Iter 24/50. Time: 3.3\n","[INFO] Training Loss: 413932.7 (change -2.57%). Training RMSE: 0.95. Testing RMSE: 1.06\n","[INFO] Iter 25/50. Time: 3.2\n","[INFO] Training Loss: 404178.2 (change -2.36%). Training RMSE: 0.94. Testing RMSE: 1.05\n","[INFO] Iter 26/50. Time: 3.3\n","[INFO] Training Loss: 395439.5 (change -2.16%). Training RMSE: 0.93. Testing RMSE: 1.04\n","[INFO] Iter 27/50. Time: 3.2\n","[INFO] Training Loss: 387593.1 (change -1.98%). Training RMSE: 0.92. Testing RMSE: 1.03\n","[INFO] Iter 28/50. Time: 3.3\n","[INFO] Training Loss: 380532.8 (change -1.82%). Training RMSE: 0.91. Testing RMSE: 1.03\n","[INFO] Iter 29/50. Time: 3.2\n","[INFO] Training Loss: 374166.4 (change -1.67%). Training RMSE: 0.90. Testing RMSE: 1.02\n","[INFO] Iter 30/50. Time: 3.2\n","[INFO] Training Loss: 368414.1 (change -1.54%). Training RMSE: 0.89. Testing RMSE: 1.02\n","[INFO] Iter 31/50. Time: 3.3\n","[INFO] Training Loss: 363206.5 (change -1.41%). Training RMSE: 0.89. Testing RMSE: 1.01\n","[INFO] Iter 32/50. Time: 3.3\n","[INFO] Training Loss: 358482.9 (change -1.30%). Training RMSE: 0.88. Testing RMSE: 1.01\n","[INFO] Iter 33/50. Time: 3.3\n","[INFO] Training Loss: 354190.2 (change -1.20%). Training RMSE: 0.88. Testing RMSE: 1.01\n","[INFO] Iter 34/50. Time: 3.3\n","[INFO] Training Loss: 350282.1 (change -1.10%). Training RMSE: 0.87. Testing RMSE: 1.00\n","[INFO] Iter 35/50. Time: 3.3\n","[INFO] Training Loss: 346717.7 (change -1.02%). Training RMSE: 0.87. Testing RMSE: 1.00\n","[INFO] Iter 36/50. Time: 3.3\n","[INFO] Training Loss: 343460.9 (change -0.94%). Training RMSE: 0.86. Testing RMSE: 1.00\n","[INFO] Iter 37/50. Time: 3.2\n","[INFO] Training Loss: 340480.1 (change -0.87%). Training RMSE: 0.86. Testing RMSE: 1.00\n","[INFO] Iter 38/50. Time: 3.2\n","[INFO] Training Loss: 337747.3 (change -0.80%). Training RMSE: 0.85. Testing RMSE: 0.99\n","[INFO] Iter 39/50. Time: 3.2\n","[INFO] Training Loss: 335237.5 (change -0.74%). Training RMSE: 0.85. Testing RMSE: 0.99\n","[INFO] Iter 40/50. Time: 3.3\n","[INFO] Training Loss: 332928.7 (change -0.69%). Training RMSE: 0.85. Testing RMSE: 0.99\n","[INFO] Iter 41/50. Time: 3.3\n","[INFO] Training Loss: 330801.4 (change -0.64%). Training RMSE: 0.85. Testing RMSE: 0.99\n","[INFO] Iter 42/50. Time: 3.3\n","[INFO] Training Loss: 328838.0 (change -0.59%). Training RMSE: 0.84. Testing RMSE: 0.99\n","[INFO] Iter 43/50. Time: 3.3\n","[INFO] Training Loss: 327023.1 (change -0.55%). Training RMSE: 0.84. Testing RMSE: 0.99\n","[INFO] Iter 44/50. Time: 3.2\n","[INFO] Training Loss: 325342.7 (change -0.51%). Training RMSE: 0.84. Testing RMSE: 0.99\n","[INFO] Iter 45/50. Time: 3.3\n","[INFO] Training Loss: 323784.5 (change -0.48%). Training RMSE: 0.84. Testing RMSE: 0.99\n","[INFO] Iter 46/50. Time: 3.3\n","[INFO] Training Loss: 322337.2 (change -0.45%). Training RMSE: 0.83. Testing RMSE: 0.98\n","[INFO] Iter 47/50. Time: 3.2\n","[INFO] Training Loss: 320991.0 (change -0.42%). Training RMSE: 0.83. Testing RMSE: 0.98\n","[INFO] Iter 48/50. Time: 3.3\n","[INFO] Training Loss: 319736.9 (change -0.39%). Training RMSE: 0.83. Testing RMSE: 0.98\n","[INFO] Iter 49/50. Time: 3.2\n","[INFO] Training Loss: 318566.7 (change -0.37%). Training RMSE: 0.83. Testing RMSE: 0.98\n","[INFO] Iter 50/50. Time: 3.3\n","[INFO] Training Loss: 317473.4 (change -0.34%). Training RMSE: 0.83. Testing RMSE: 0.98\n","[INFO] RMSE: 0.9826\n","[INFO] Total Time: 163 s\n"]}]},{"cell_type":"markdown","metadata":{"id":"_U2FYVR0Ar6e"},"source":["## Citations\n","\n","Relational Learning via Collective Matrix Factorization. Singh et. al.. 2008. KDD. [http://www.cs.cmu.edu/~ggordon/singh-gordon-kdd-factorization.pdf](http://www.cs.cmu.edu/~ggordon/singh-gordon-kdd-factorization.pdf)"]}]}