{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp models.afn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AFN\n",
    "> A pytorch implementation of AFN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev.showdoc import *\n",
    "from fastcore.nb_imports import *\n",
    "from fastcore.test import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "from recohut.models.layers.embedding import EmbeddingLayer\n",
    "from recohut.models.layers.common import MLP_Layer, LR_Layer\n",
    "\n",
    "from recohut.models.bases.ctr import CTRModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class AFN(CTRModel):\n",
    "    def __init__(self, \n",
    "                 feature_map, \n",
    "                 model_id=\"AFN\",\n",
    "                 task=\"binary_classification\",\n",
    "                 learning_rate=1e-3, \n",
    "                 embedding_initializer=\"torch.nn.init.normal_(std=1e-4)\",\n",
    "                 embedding_dim=10, \n",
    "                 ensemble_dnn=True,\n",
    "                 dnn_hidden_units=[64, 64, 64], \n",
    "                 dnn_activations=\"ReLU\",\n",
    "                 dnn_dropout=0,\n",
    "                 afn_hidden_units=[64, 64, 64], \n",
    "                 afn_activations=\"ReLU\",\n",
    "                 afn_dropout=0,\n",
    "                 logarithmic_neurons=5,\n",
    "                 batch_norm=True,\n",
    "                 **kwargs):\n",
    "        super(AFN, self).__init__(feature_map, \n",
    "                                           model_id=model_id,\n",
    "                                           **kwargs)\n",
    "        self.num_fields = feature_map.num_fields\n",
    "        self.embedding_layer = EmbeddingLayer(feature_map, embedding_dim)\n",
    "        self.coefficient_W = nn.Linear(self.num_fields, logarithmic_neurons, bias=False)\n",
    "        self.dense_layer = MLP_Layer(input_dim=embedding_dim * logarithmic_neurons,\n",
    "                                     output_dim=1, \n",
    "                                     hidden_units=afn_hidden_units,\n",
    "                                     hidden_activations=afn_activations,\n",
    "                                     output_activation=None, \n",
    "                                     dropout_rates=afn_dropout, \n",
    "                                     batch_norm=batch_norm, \n",
    "                                     use_bias=True)\n",
    "        self.log_batch_norm = nn.BatchNorm1d(self.num_fields)\n",
    "        self.exp_batch_norm = nn.BatchNorm1d(logarithmic_neurons)\n",
    "        self.ensemble_dnn = ensemble_dnn\n",
    "        if ensemble_dnn:\n",
    "            self.embedding_layer2 = EmbeddingLayer(feature_map, \n",
    "                                                   embedding_dim, \n",
    "                                                   embedding_dropout)\n",
    "            self.dnn = MLP_Layer(input_dim=embedding_dim * self.num_fields,\n",
    "                                 output_dim=1, \n",
    "                                 hidden_units=dnn_hidden_units,\n",
    "                                 hidden_activations=dnn_activations,\n",
    "                                 output_activation=None, \n",
    "                                 dropout_rates=dnn_dropout, \n",
    "                                 batch_norm=batch_norm, \n",
    "                                 use_bias=True)\n",
    "            self.fc = nn.Linear(2, 1)\n",
    "        self.output_activation = self.get_final_activation(task)\n",
    "        self.init_weights(embedding_initializer=embedding_initializer)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        feature_emb = self.embedding_layer(inputs)\n",
    "        dnn_input = self.logarithmic_net(feature_emb)\n",
    "        afn_out = self.dense_layer(dnn_input)\n",
    "        if self.ensemble_dnn:\n",
    "            feature_emb_list2 = self.embedding_layer2(X)\n",
    "            concate_feature_emb = torch.cat(feature_emb_list2, dim=1)\n",
    "            dnn_out = self.dnn(concate_feature_emb)\n",
    "            y_pred = self.fc(torch.cat([afn_out, dnn_out], dim=-1))\n",
    "        else:\n",
    "            y_pred = afn_out\n",
    "\n",
    "        if self.output_activation is not None:\n",
    "            y_pred = self.output_activation(y_pred)\n",
    "        return y_pred\n",
    "\n",
    "    def logarithmic_net(self, feature_emb):\n",
    "        feature_emb = torch.abs(feature_emb)\n",
    "        feature_emb = torch.clamp(feature_emb, min=1e-5) # ReLU with min 1e-5 (better than 1e-7 suggested in paper)\n",
    "        log_feature_emb = torch.log(feature_emb) # element-wise log \n",
    "        log_feature_emb = self.log_batch_norm(log_feature_emb) # batch_size * num_fields * embedding_dim \n",
    "        logarithmic_out = self.coefficient_W(log_feature_emb.transpose(2, 1)).transpose(1, 2)\n",
    "        cross_out = torch.exp(logarithmic_out) # element-wise exp\n",
    "        cross_out = self.exp_batch_norm(cross_out)  # batch_size * logarithmic_neurons * embedding_dim\n",
    "        concat_out = torch.flatten(cross_out, start_dim=1)\n",
    "        return concat_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'model_id': 'AFN',\n",
    "              'data_dir': '/content/data',\n",
    "              'model_root': './checkpoints/',\n",
    "              'learning_rate': 1e-3,\n",
    "              'batch_norm': False,\n",
    "              'optimizer': 'adamw',\n",
    "              'task': 'binary_classification',\n",
    "              'loss': 'binary_crossentropy',\n",
    "              'metrics': ['logloss', 'AUC'],\n",
    "              'embedding_dim': 10,\n",
    "              'logarithmic_neurons': 1200,\n",
    "              'afn_hidden_units': [400, 400, 400],\n",
    "              'afn_activations': 'relu',\n",
    "              'afn_dropout': 0,\n",
    "              'ensemble_dnn': False,\n",
    "              'dnn_hidden_units': [400, 400, 400],\n",
    "              'dnn_activations': 'relu',\n",
    "              'dnn_dropout': 0,\n",
    "              'batch_size': 64,\n",
    "              'epochs': 3,\n",
    "              'shuffle': True,\n",
    "              'seed': 2019,\n",
    "              'use_hdf5': True,\n",
    "              'workers': 1,\n",
    "              'verbose': 0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AFN(ds.dataset.feature_map, **params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "\n",
      "  | Name              | Type           | Params\n",
      "-----------------------------------------------------\n",
      "0 | embedding_layer   | EmbeddingLayer | 4.8 K \n",
      "1 | coefficient_W     | Linear         | 16.8 K\n",
      "2 | dense_layer       | MLP_Layer      | 5.1 M \n",
      "3 | log_batch_norm    | BatchNorm1d    | 28    \n",
      "4 | exp_batch_norm    | BatchNorm1d    | 2.4 K \n",
      "5 | output_activation | Sigmoid        | 0     \n",
      "-----------------------------------------------------\n",
      "5.1 M     Trainable params\n",
      "0         Non-trainable params\n",
      "5.1 M     Total params\n",
      "20.582    Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4373d98cf97a4ae799ef2e9df8e77dc1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3fda041e4f4d4aacbf78af2ce4fbf3b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93d8a6e7b7184430b95e3556b1cde6e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{'Test Metrics': {'AUC': tensor(0.7091), 'logloss': tensor(0.3672)}}\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'Test Metrics': {'AUC': tensor(0.7091), 'logloss': tensor(0.3672)}}]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pl_trainer(model, ds, max_epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import math\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from recohut.models.layers.common import FeaturesEmbedding, FeaturesLinear, MultiLayerPerceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#exporti\n",
    "class LNN(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    A pytorch implementation of LNN layer\n",
    "    Input shape\n",
    "        - A 3D tensor with shape: ``(batch_size,field_size,embedding_size)``.\n",
    "    Output shape\n",
    "        - 2D tensor with shape:``(batch_size,LNN_dim*embedding_size)``.\n",
    "    Arguments\n",
    "        - **in_features** : Embedding of feature.\n",
    "        - **num_fields**: int.The field size of feature.\n",
    "        - **LNN_dim**: int.The number of Logarithmic neuron.\n",
    "        - **bias**: bool.Whether or not use bias in LNN.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_fields, embed_dim, LNN_dim, bias=False):\n",
    "        super(LNN, self).__init__()\n",
    "        self.num_fields = num_fields\n",
    "        self.embed_dim = embed_dim\n",
    "        self.LNN_dim = LNN_dim\n",
    "        self.lnn_output_dim = LNN_dim * embed_dim\n",
    "        self.weight = torch.nn.Parameter(torch.Tensor(LNN_dim, num_fields))\n",
    "        if bias:\n",
    "            self.bias = torch.nn.Parameter(torch.Tensor(LNN_dim, embed_dim))\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "        self.reset_parameters()\n",
    "    \n",
    "    def reset_parameters(self):\n",
    "        stdv = 1. / math.sqrt(self.weight.size(1))\n",
    "        self.weight.data.uniform_(-stdv, stdv)\n",
    "        if self.bias is not None:\n",
    "            self.bias.data.uniform_(-stdv, stdv)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        :param x: Long tensor of size ``(batch_size, num_fields, embedding_size)``\n",
    "        \"\"\"\n",
    "        embed_x_abs = torch.abs(x) # Computes the element-wise absolute value of the given input tensor.\n",
    "        embed_x_afn = torch.add(embed_x_abs, 1e-7)\n",
    "        # Logarithmic Transformation\n",
    "        embed_x_log = torch.log1p(embed_x_afn) # torch.log1p and torch.expm1\n",
    "        lnn_out = torch.matmul(self.weight, embed_x_log)\n",
    "        if self.bias is not None:\n",
    "            lnn_out += self.bias\n",
    "        lnn_exp = torch.expm1(lnn_out)\n",
    "        output = F.relu(lnn_exp).contiguous().view(-1, self.lnn_output_dim)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class AFN_v2(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    A pytorch implementation of AFN.\n",
    "    Reference:\n",
    "        Cheng W, et al. Adaptive Factorization Network: Learning Adaptive-Order Feature Interactions, 2019.\n",
    "    \"\"\"\n",
    "    def __init__(self, field_dims, embed_dim, LNN_dim, mlp_dims, dropouts):\n",
    "        super().__init__()\n",
    "        self.num_fields = len(field_dims)\n",
    "        self.linear = FeaturesLinear(field_dims)    # Linear\n",
    "        self.embedding = FeaturesEmbedding(field_dims, embed_dim)   # Embedding\n",
    "        self.LNN_dim = LNN_dim\n",
    "        self.LNN_output_dim = self.LNN_dim * embed_dim\n",
    "        self.LNN = LNN(self.num_fields, embed_dim, LNN_dim)\n",
    "        self.mlp = MultiLayerPerceptron(self.LNN_output_dim, mlp_dims, dropouts[0])\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        :param x: Long tensor of size ``(batch_size, num_fields)``\n",
    "        \"\"\"\n",
    "        embed_x = self.embedding(x)\n",
    "        lnn_out = self.LNN(embed_x)\n",
    "        x = self.linear(x) + self.mlp(lnn_out)\n",
    "        return torch.sigmoid(x.squeeze(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **References**\n",
    "> - Cheng W, et al. Adaptive Factorization Network: Learning Adaptive-Order Feature Interactions, 2019.\n",
    "> - https://github.com/rixwew/pytorch-fm/blob/master/torchfm/model/afn.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "%reload_ext watermark\n",
    "%watermark -a \"Sparsh A.\" -m -iv -u -t -d -p recohut"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
