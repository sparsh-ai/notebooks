{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"T473399 | Training RL Agent in CartPole Environment with DQN method","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyPOZ1gg7ohYhR8gsHFRpskb"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"tyMytwWZFUhp"},"source":["# Training RL Agent in CartPole Environment with DQN method"]},{"cell_type":"code","metadata":{"id":"ysNR-nNZEHYs","executionInfo":{"status":"ok","timestamp":1638447252506,"user_tz":-330,"elapsed":3832,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}}},"source":["import argparse\n","from datetime import datetime\n","import os\n","import random\n","from collections import deque\n","\n","import gym\n","import numpy as np\n","import tensorflow as tf\n","from tensorflow.keras.layers import Dense, Input\n","from tensorflow.keras.optimizers import Adam"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"id":"doMRsX9OEImC","executionInfo":{"status":"ok","timestamp":1638447252507,"user_tz":-330,"elapsed":4,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}}},"source":["tf.keras.backend.set_floatx(\"float64\")"],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dCLXisl6ELQ0","executionInfo":{"status":"ok","timestamp":1638447281630,"user_tz":-330,"elapsed":438,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}},"outputId":"3a8c0d7e-b074-4241-9d48-ff6480fe7d11"},"source":["parser = argparse.ArgumentParser()\n","parser.add_argument(\"--env\", default=\"CartPole-v0\")\n","parser.add_argument(\"--lr\", type=float, default=0.005)\n","parser.add_argument(\"--batch_size\", type=int, default=256)\n","parser.add_argument(\"--gamma\", type=float, default=0.95)\n","parser.add_argument(\"--eps\", type=float, default=1.0)\n","parser.add_argument(\"--eps_decay\", type=float, default=0.995)\n","parser.add_argument(\"--eps_min\", type=float, default=0.01)\n","parser.add_argument(\"--logdir\", default=\"logs\")\n","\n","args = parser.parse_args([])\n","logdir = os.path.join(\n","    args.logdir, parser.prog, args.env, datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",")\n","print(f\"Saving training logs to:{logdir}\")\n","writer = tf.summary.create_file_writer(logdir)"],"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Saving training logs to:logs/ipykernel_launcher.py/CartPole-v0/20211202-121443\n"]}]},{"cell_type":"code","metadata":{"id":"wl9MSHtJEXFT","executionInfo":{"status":"ok","timestamp":1638447289912,"user_tz":-330,"elapsed":424,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}}},"source":["class ReplayBuffer:\n","    def __init__(self, capacity=10000):\n","        self.buffer = deque(maxlen=capacity)\n","\n","    def store(self, state, action, reward, next_state, done):\n","        self.buffer.append([state, action, reward, next_state, done])\n","\n","    def sample(self):\n","        sample = random.sample(self.buffer, args.batch_size)\n","        states, actions, rewards, next_states, done = map(np.asarray, zip(*sample))\n","        states = np.array(states).reshape(args.batch_size, -1)\n","        next_states = np.array(next_states).reshape(args.batch_size, -1)\n","        return states, actions, rewards, next_states, done\n","\n","    def size(self):\n","        return len(self.buffer)"],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"id":"xFoB1mz4EZHC","executionInfo":{"status":"ok","timestamp":1638447297678,"user_tz":-330,"elapsed":425,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}}},"source":["class DQN:\n","    def __init__(self, state_dim, aciton_dim):\n","        self.state_dim = state_dim\n","        self.action_dim = aciton_dim\n","        self.epsilon = args.eps\n","\n","        self.model = self.nn_model()\n","\n","    def nn_model(self):\n","        model = tf.keras.Sequential(\n","            [\n","                Input((self.state_dim,)),\n","                Dense(32, activation=\"relu\"),\n","                Dense(16, activation=\"relu\"),\n","                Dense(self.action_dim),\n","            ]\n","        )\n","        model.compile(loss=\"mse\", optimizer=Adam(args.lr))\n","        return model\n","\n","    def predict(self, state):\n","        return self.model.predict(state)\n","\n","    def get_action(self, state):\n","        state = np.reshape(state, [1, self.state_dim])\n","        self.epsilon *= args.eps_decay\n","        self.epsilon = max(self.epsilon, args.eps_min)\n","        q_value = self.predict(state)[0]\n","        if np.random.random() < self.epsilon:\n","            return random.randint(0, self.action_dim - 1)\n","        return np.argmax(q_value)\n","\n","    def train(self, states, targets):\n","        self.model.fit(states, targets, epochs=1)"],"execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"id":"c3BmqKjDEatV","executionInfo":{"status":"ok","timestamp":1638447305244,"user_tz":-330,"elapsed":698,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}}},"source":["class Agent:\n","    def __init__(self, env):\n","        self.env = env\n","        self.state_dim = self.env.observation_space.shape[0]\n","        self.action_dim = self.env.action_space.n\n","\n","        self.model = DQN(self.state_dim, self.action_dim)\n","        self.target_model = DQN(self.state_dim, self.action_dim)\n","        self.update_target()\n","\n","        self.buffer = ReplayBuffer()\n","\n","    def update_target(self):\n","        weights = self.model.model.get_weights()\n","        self.target_model.model.set_weights(weights)\n","\n","    def replay_experience(self):\n","        for _ in range(10):\n","            states, actions, rewards, next_states, done = self.buffer.sample()\n","            targets = self.model.predict(states)\n","            next_q_values = self.target_model.predict(next_states)[\n","                range(args.batch_size),\n","                np.argmax(self.model.predict(next_states), axis=1),\n","            ]\n","            targets[range(args.batch_size), actions] = (\n","                rewards + (1 - done) * next_q_values * args.gamma\n","            )\n","            self.model.train(states, targets)\n","\n","    def train(self, max_episodes=1000):\n","        with writer.as_default():\n","            for ep in range(max_episodes):\n","                done, episode_reward = False, 0\n","                observation = self.env.reset()\n","                while not done:\n","                    action = self.model.get_action(observation)\n","                    next_observation, reward, done, _ = self.env.step(action)\n","                    self.buffer.store(\n","                        observation, action, reward, next_observation, done\n","                    )\n","                    episode_reward += reward\n","                    observation = next_observation\n","\n","                if self.buffer.size() >= args.batch_size:\n","                    self.replay_experience()\n","                self.update_target()\n","                print(f\"Episode#{ep} Reward:{episode_reward}\")\n","                tf.summary.scalar(\"episode_reward\", episode_reward, step=ep)"],"execution_count":8,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sw6rePMLEcxu","executionInfo":{"status":"ok","timestamp":1638447363342,"user_tz":-330,"elapsed":30987,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}},"outputId":"1ca7d8b7-83c3-4f3d-c981-143fb94ab5cd"},"source":["if __name__ == \"__main__\":\n","    env = gym.make(\"CartPole-v0\")\n","    agent = Agent(env)\n","    agent.train(max_episodes=10)  # Increase max_episodes value"],"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["Episode#0 Reward:12.0\n","Episode#1 Reward:56.0\n","Episode#2 Reward:31.0\n","Episode#3 Reward:63.0\n","Episode#4 Reward:34.0\n","8/8 [==============================] - 1s 3ms/step - loss: 0.4167\n","8/8 [==============================] - 0s 2ms/step - loss: 0.1804\n","8/8 [==============================] - 0s 2ms/step - loss: 0.0776\n","8/8 [==============================] - 0s 2ms/step - loss: 0.0478\n","8/8 [==============================] - 0s 4ms/step - loss: 0.0224\n","8/8 [==============================] - 0s 2ms/step - loss: 0.0153\n","8/8 [==============================] - 0s 3ms/step - loss: 0.0165\n","8/8 [==============================] - 0s 3ms/step - loss: 0.0113\n","8/8 [==============================] - 0s 3ms/step - loss: 0.0087\n","8/8 [==============================] - 0s 2ms/step - loss: 0.0074\n","Episode#5 Reward:96.0\n","8/8 [==============================] - 0s 2ms/step - loss: 0.3258\n","8/8 [==============================] - 0s 3ms/step - loss: 0.0377\n","8/8 [==============================] - 0s 2ms/step - loss: 0.0232\n","8/8 [==============================] - 0s 2ms/step - loss: 0.0184\n","8/8 [==============================] - 0s 3ms/step - loss: 0.0148\n","8/8 [==============================] - 0s 3ms/step - loss: 0.0097\n","8/8 [==============================] - 0s 3ms/step - loss: 0.0115\n","8/8 [==============================] - 0s 3ms/step - loss: 0.0099\n","8/8 [==============================] - 0s 2ms/step - loss: 0.0073\n","8/8 [==============================] - 0s 3ms/step - loss: 0.0113\n","Episode#6 Reward:10.0\n","8/8 [==============================] - 0s 2ms/step - loss: 0.3208\n","8/8 [==============================] - 0s 2ms/step - loss: 0.1120\n","8/8 [==============================] - 0s 2ms/step - loss: 0.0258\n","8/8 [==============================] - 0s 2ms/step - loss: 0.0363\n","8/8 [==============================] - 0s 3ms/step - loss: 0.0396\n","8/8 [==============================] - 0s 3ms/step - loss: 0.0371\n","8/8 [==============================] - 0s 3ms/step - loss: 0.0308\n","8/8 [==============================] - 0s 2ms/step - loss: 0.0297\n","8/8 [==============================] - 0s 2ms/step - loss: 0.0355\n","8/8 [==============================] - 0s 2ms/step - loss: 0.0357\n","Episode#7 Reward:9.0\n","8/8 [==============================] - 0s 3ms/step - loss: 0.3133\n","8/8 [==============================] - 0s 3ms/step - loss: 0.1493\n","8/8 [==============================] - 0s 2ms/step - loss: 0.0659\n","8/8 [==============================] - 0s 3ms/step - loss: 0.0858\n","8/8 [==============================] - 0s 3ms/step - loss: 0.0631\n","8/8 [==============================] - 0s 3ms/step - loss: 0.0803\n","8/8 [==============================] - 0s 2ms/step - loss: 0.0643\n","8/8 [==============================] - 0s 3ms/step - loss: 0.0686\n","8/8 [==============================] - 0s 3ms/step - loss: 0.0646\n","8/8 [==============================] - 0s 3ms/step - loss: 0.0752\n","Episode#8 Reward:26.0\n","8/8 [==============================] - 0s 3ms/step - loss: 0.3061\n","8/8 [==============================] - 0s 3ms/step - loss: 0.1950\n","8/8 [==============================] - 0s 3ms/step - loss: 0.1106\n","8/8 [==============================] - 0s 3ms/step - loss: 0.1045\n","8/8 [==============================] - 0s 3ms/step - loss: 0.0837\n","8/8 [==============================] - 0s 2ms/step - loss: 0.0809\n","8/8 [==============================] - 0s 3ms/step - loss: 0.0753\n","8/8 [==============================] - 0s 2ms/step - loss: 0.0678\n","8/8 [==============================] - 0s 2ms/step - loss: 0.0956\n","8/8 [==============================] - 0s 2ms/step - loss: 0.0841\n","Episode#9 Reward:10.0\n"]}]},{"cell_type":"markdown","metadata":{"id":"ixk34zYIEgB_"},"source":["---"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VDwjGZWIE5il","executionInfo":{"status":"ok","timestamp":1638447434341,"user_tz":-330,"elapsed":3370,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}},"outputId":"99504006-22e9-447b-aab4-aa6d3423da9c"},"source":["!pip install -q watermark\n","%reload_ext watermark\n","%watermark -a \"Sparsh A.\" -m -iv -u -t -d"],"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["Author: Sparsh A.\n","\n","Last updated: 2021-12-02 12:17:16\n","\n","Compiler    : GCC 7.5.0\n","OS          : Linux\n","Release     : 5.4.104+\n","Machine     : x86_64\n","Processor   : x86_64\n","CPU cores   : 2\n","Architecture: 64bit\n","\n","tensorflow: 2.7.0\n","gym       : 0.17.3\n","numpy     : 1.19.5\n","IPython   : 5.5.0\n","argparse  : 1.1\n","\n"]}]},{"cell_type":"markdown","metadata":{"id":"haKsOAX2E1XI"},"source":["---"]},{"cell_type":"markdown","metadata":{"id":"4s5DJf8WE2as"},"source":["**END**"]}]}