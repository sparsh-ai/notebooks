{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp models.layers.interaction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interaction Layers\n",
    "> Implementation of NN interaction layers in Pytorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev.showdoc import *\n",
    "from fastcore.nb_imports import *\n",
    "from fastcore.test import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import torch\n",
    "from torch import nn\n",
    "from itertools import combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#exporti\n",
    "def get_activation(activation):\n",
    "    if isinstance(activation, str):\n",
    "        if activation.lower() == \"relu\":\n",
    "            return nn.ReLU()\n",
    "        elif activation.lower() == \"sigmoid\":\n",
    "            return nn.Sigmoid()\n",
    "        elif activation.lower() == \"tanh\":\n",
    "            return nn.Tanh()\n",
    "        else:\n",
    "            return getattr(nn, activation)()\n",
    "    else:\n",
    "        return activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class InnerProductLayer(nn.Module):\n",
    "    \"\"\" output: product_sum_pooling (bs x 1), \n",
    "                Bi_interaction_pooling (bs * dim), \n",
    "                inner_product (bs x f2/2), \n",
    "                elementwise_product (bs x f2/2 x emb_dim)\n",
    "    \"\"\"\n",
    "    def __init__(self, num_fields=None, output=\"product_sum_pooling\"):\n",
    "        super(InnerProductLayer, self).__init__()\n",
    "        self._output_type = output\n",
    "        if output not in [\"product_sum_pooling\", \"Bi_interaction_pooling\", \"inner_product\", \"elementwise_product\"]:\n",
    "            raise ValueError(\"InnerProductLayer output={} is not supported.\".format(output))\n",
    "        if num_fields is None:\n",
    "            if output in [\"inner_product\", \"elementwise_product\"]:\n",
    "                raise ValueError(\"num_fields is required when InnerProductLayer output={}.\".format(output))\n",
    "        else:\n",
    "            p, q = zip(*list(combinations(range(num_fields), 2)))\n",
    "            self.field_p = nn.Parameter(torch.LongTensor(p), requires_grad=False)\n",
    "            self.field_q = nn.Parameter(torch.LongTensor(q), requires_grad=False)\n",
    "            self.interaction_units = int(num_fields * (num_fields - 1) / 2)\n",
    "            self.upper_triange_mask = nn.Parameter(torch.triu(torch.ones(num_fields, num_fields), 1).type(torch.ByteTensor),\n",
    "                                                   requires_grad=False)\n",
    "\n",
    "    def forward(self, feature_emb):\n",
    "        if self._output_type in [\"product_sum_pooling\", \"Bi_interaction_pooling\"]: \n",
    "            sum_of_square = torch.sum(feature_emb, dim=1) ** 2  # sum then square\n",
    "            square_of_sum = torch.sum(feature_emb ** 2, dim=1) # square then sum\n",
    "            bi_interaction = (sum_of_square - square_of_sum) * 0.5\n",
    "            if self._output_type == \"Bi_interaction_pooling\":\n",
    "                return bi_interaction\n",
    "            else:\n",
    "                return bi_interaction.sum(dim=-1, keepdim=True)\n",
    "        elif self._output_type == \"elementwise_product\":\n",
    "            emb1 =  torch.index_select(feature_emb, 1, self.field_p)\n",
    "            emb2 = torch.index_select(feature_emb, 1, self.field_q)\n",
    "            return emb1 * emb2\n",
    "        elif self._output_type == \"inner_product\":\n",
    "            inner_product_matrix = torch.bmm(feature_emb, feature_emb.transpose(1, 2))\n",
    "            flat_upper_triange = torch.masked_select(inner_product_matrix, self.upper_triange_mask)\n",
    "            return flat_upper_triange.view(-1, self.interaction_units)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class BilinearInteractionLayer(nn.Module):\n",
    "    def __init__(self, num_fields, embedding_dim, bilinear_type=\"field_interaction\"):\n",
    "        super(BilinearInteractionLayer, self).__init__()\n",
    "        self.bilinear_type = bilinear_type\n",
    "        if self.bilinear_type == \"field_all\":\n",
    "            self.bilinear_layer = nn.Linear(embedding_dim, embedding_dim, bias=False)\n",
    "        elif self.bilinear_type == \"field_each\":\n",
    "            self.bilinear_layer = nn.ModuleList([nn.Linear(embedding_dim, embedding_dim, bias=False)\n",
    "                                                 for i in range(num_fields)])\n",
    "        elif self.bilinear_type == \"field_interaction\":\n",
    "            self.bilinear_layer = nn.ModuleList([nn.Linear(embedding_dim, embedding_dim, bias=False)\n",
    "                                                 for i, j in combinations(range(num_fields), 2)])\n",
    "        else:\n",
    "            raise NotImplementedError()\n",
    "\n",
    "    def forward(self, feature_emb):\n",
    "        feature_emb_list = torch.split(feature_emb, 1, dim=1)\n",
    "        if self.bilinear_type == \"field_all\":\n",
    "            bilinear_list = [self.bilinear_layer(v_i) * v_j\n",
    "                             for v_i, v_j in combinations(feature_emb_list, 2)]\n",
    "        elif self.bilinear_type == \"field_each\":\n",
    "            bilinear_list = [self.bilinear_layer[i](feature_emb_list[i]) * feature_emb_list[j]\n",
    "                             for i, j in combinations(range(len(feature_emb_list)), 2)]\n",
    "        elif self.bilinear_type == \"field_interaction\":\n",
    "            bilinear_list = [self.bilinear_layer[i](v[0]) * v[1]\n",
    "                             for i, v in enumerate(combinations(feature_emb_list, 2))]\n",
    "        return torch.cat(bilinear_list, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class HolographicInteractionLayer(nn.Module):\n",
    "    def __init__(self, num_fields, interaction_type=\"circular_convolution\"):\n",
    "        super(HolographicInteractionLayer, self).__init__()\n",
    "        self.interaction_type = interaction_type\n",
    "        if self.interaction_type == \"circular_correlation\":\n",
    "            self.conj_sign =  nn.Parameter(torch.tensor([1., -1.]), requires_grad=False)\n",
    "        p, q = zip(*list(combinations(range(num_fields), 2)))\n",
    "        self.field_p = nn.Parameter(torch.LongTensor(p), requires_grad=False)\n",
    "        self.field_q = nn.Parameter(torch.LongTensor(q), requires_grad=False)\n",
    "\n",
    "    def forward(self, feature_emb):\n",
    "        emb1 =  torch.index_select(feature_emb, 1, self.field_p)\n",
    "        emb2 = torch.index_select(feature_emb, 1, self.field_q)\n",
    "        if self.interaction_type == \"hadamard_product\":\n",
    "            interact_tensor = emb1 * emb2\n",
    "        elif self.interaction_type == \"circular_convolution\":\n",
    "            fft1 = torch.rfft(emb1, 1, onesided=False)\n",
    "            fft2 = torch.rfft(emb2, 1, onesided=False)\n",
    "            fft_product = torch.stack([fft1[..., 0] * fft2[..., 0] - fft1[..., 1] * fft2[..., 1], \n",
    "                                       fft1[..., 0] * fft2[..., 1] + fft1[..., 1] * fft2[..., 0]], \n",
    "                                       dim=-1)\n",
    "            interact_tensor = torch.irfft(fft_product, 1, onesided=False)\n",
    "        elif self.interaction_type == \"circular_correlation\":\n",
    "            fft1_emb = torch.rfft(emb1, 1, onesided=False) \n",
    "            fft1 = fft1_emb * self.conj_sign.expand_as(fft1_emb)\n",
    "            fft2 = torch.rfft(emb2, 1, onesided=False)\n",
    "            fft_product = torch.stack([fft1[..., 0] * fft2[..., 0] - fft1[..., 1] * fft2[..., 1], \n",
    "                                       fft1[..., 0] * fft2[..., 1] + fft1[..., 1] * fft2[..., 0]], \n",
    "                                       dim=-1)\n",
    "            interact_tensor = torch.irfft(fft_product, 1, onesided=False)\n",
    "        else:\n",
    "            raise ValueError(\"interaction_type={} not supported.\".format(self.interaction_type))\n",
    "        return interact_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class CrossInteractionLayer(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(CrossInteractionLayer, self).__init__()\n",
    "        self.weight = nn.Linear(input_dim, 1, bias=False)\n",
    "        self.bias = nn.Parameter(torch.zeros(input_dim))\n",
    "\n",
    "    def forward(self, X_0, X_i):\n",
    "        interaction_out = self.weight(X_i) * X_0 + self.bias\n",
    "        return interaction_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class CrossNet(nn.Module):\n",
    "    def __init__(self, input_dim, num_layers):\n",
    "        super(CrossNet, self).__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.cross_net = nn.ModuleList(CrossInteractionLayer(input_dim)\n",
    "                                       for _ in range(self.num_layers))\n",
    "\n",
    "    def forward(self, X_0):\n",
    "        X_i = X_0 # b x dim\n",
    "        for i in range(self.num_layers):\n",
    "            X_i = X_i + self.cross_net[i](X_0, X_i)\n",
    "        return X_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class CompressedInteractionNet(nn.Module):\n",
    "    def __init__(self, num_fields, cin_layer_units, output_dim=1):\n",
    "        super(CompressedInteractionNet, self).__init__()\n",
    "        self.cin_layer_units = cin_layer_units\n",
    "        self.fc = nn.Linear(sum(cin_layer_units), output_dim)\n",
    "        self.cin_layer = nn.ModuleDict()\n",
    "        for i, unit in enumerate(self.cin_layer_units):\n",
    "            in_channels = num_fields * self.cin_layer_units[i - 1] if i > 0 else num_fields ** 2\n",
    "            out_channels = unit\n",
    "            self.cin_layer[\"layer_\" + str(i + 1)] = nn.Conv1d(in_channels,\n",
    "                                                              out_channels,  # how many filters\n",
    "                                                              kernel_size=1) # kernel output shape\n",
    "\n",
    "    def forward(self, feature_emb):\n",
    "        pooling_outputs = []\n",
    "        X_0 = feature_emb\n",
    "        batch_size = X_0.shape[0]\n",
    "        embedding_dim = X_0.shape[-1]\n",
    "        X_i = X_0\n",
    "        for i in range(len(self.cin_layer_units)):\n",
    "            hadamard_tensor = torch.einsum(\"bhd,bmd->bhmd\", X_0, X_i)\n",
    "            hadamard_tensor = hadamard_tensor.view(batch_size, -1, embedding_dim)\n",
    "            X_i = self.cin_layer[\"layer_\" + str(i + 1)](hadamard_tensor) \\\n",
    "                      .view(batch_size, -1, embedding_dim)\n",
    "            pooling_outputs.append(X_i.sum(dim=-1))\n",
    "        concate_vec = torch.cat(pooling_outputs, dim=-1)\n",
    "        output = self.fc(concate_vec)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class InteractionMachine(nn.Module):\n",
    "    def __init__(self, embedding_dim, order=2, batch_norm=False):\n",
    "        super(InteractionMachine, self).__init__()\n",
    "        assert order < 6, \"order={} is not supported.\".format(order)\n",
    "        self.order = order\n",
    "        self.bn = nn.BatchNorm1d(embedding_dim * order) if batch_norm else None\n",
    "        self.fc = nn.Linear(order * embedding_dim, 1)\n",
    "        \n",
    "    def second_order(self, p1, p2):\n",
    "        return (p1.pow(2) - p2) / 2\n",
    "\n",
    "    def third_order(self, p1, p2, p3):\n",
    "        return (p1.pow(3) - 3 * p1 * p2 + 2 * p3) / 6\n",
    "\n",
    "    def fourth_order(self, p1, p2, p3, p4):\n",
    "        return (p1.pow(4) - 6 * p1.pow(2) * p2 + 3 * p2.pow(2)\n",
    "                + 8 * p1 * p3 - 6 * p4) / 24\n",
    "\n",
    "    def fifth_order(self, p1, p2, p3, p4, p5):\n",
    "        return (p1.pow(5) - 10 * p1.pow(3) * p2 + 20 * p1.pow(2) * p3 - 30 * p1 * p4\n",
    "                - 20 * p2 * p3 + 15 * p1 * p2.pow(2) + 24 * p5) / 120\n",
    "\n",
    "    def forward(self, X):\n",
    "        out = []\n",
    "        Q = X\n",
    "        if self.order >= 1:\n",
    "            p1 = Q.sum(dim=1)\n",
    "            out.append(p1)\n",
    "            if self.order >= 2:\n",
    "                Q = Q * X\n",
    "                p2 = Q.sum(dim=1)\n",
    "                out.append(self.second_order(p1, p2))\n",
    "                if self.order >= 3:\n",
    "                    Q = Q * X\n",
    "                    p3 = Q.sum(dim=1)\n",
    "                    out.append(self.third_order(p1, p2, p3))\n",
    "                    if self.order >= 4:\n",
    "                        Q = Q * X\n",
    "                        p4 = Q.sum(dim=1)\n",
    "                        out.append(self.fourth_order(p1, p2, p3, p4))\n",
    "                        if self.order == 5:\n",
    "                            Q = Q * X\n",
    "                            p5 = Q.sum(dim=1)\n",
    "                            out.append(self.fifth_order(p1, p2, p3, p4, p5))\n",
    "        out = torch.cat(out, dim=-1)\n",
    "        if self.bn is not None:\n",
    "            out = self.bn(out)\n",
    "        y = self.fc(out)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **References**\n",
    "> - https://github.com/xue-pai/FuxiCTR/blob/main/fuxictr/pytorch/layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Author: Sparsh A.\n",
      "\n",
      "Last updated: 2022-01-11 12:07:54\n",
      "\n",
      "Compiler    : GCC 7.5.0\n",
      "OS          : Linux\n",
      "Release     : 5.4.144+\n",
      "Machine     : x86_64\n",
      "Processor   : x86_64\n",
      "CPU cores   : 2\n",
      "Architecture: 64bit\n",
      "\n",
      "torch     : 1.10.0+cu111\n",
      "numpy     : 1.19.5\n",
      "PIL       : 7.1.2\n",
      "IPython   : 5.5.0\n",
      "matplotlib: 3.2.2\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "%reload_ext watermark\n",
    "%watermark -a \"Sparsh A.\" -m -iv -u -t -d"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
