{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"T719060 | Discrete BCQ Model on IEEE 2021 RecSys dataset","provenance":[],"collapsed_sections":[],"mount_file_id":"1Owr37Bzv24jGzcbqVh7VbnGJtSni9-0C","authorship_tag":"ABX9TyMmcPaQ2PkdxC1Dep+8H1Ro"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"4YLrsBbObFta"},"source":["# Discrete BCQ Model on IEEE 2021 RecSys dataset"]},{"cell_type":"markdown","metadata":{"id":"3GofkbEjwQCI"},"source":["## Setup"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BaXpEKLdXNPa","executionInfo":{"status":"ok","timestamp":1636123055589,"user_tz":-330,"elapsed":1012,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}},"outputId":"e1ece07d-14e7-4895-a3d4-83a3eebe5f66"},"source":["import os\n","project_name = \"ieee21cup-recsys\"; branch = \"main\"; account = \"sparsh-ai\"\n","project_path = os.path.join('/content', branch)\n","\n","if not os.path.exists(project_path):\n","    !cp -r /content/drive/MyDrive/git_credentials/. ~\n","    !mkdir \"{project_path}\"\n","    %cd \"{project_path}\"\n","    !git init\n","    !git remote add origin https://github.com/\"{account}\"/\"{project_name}\".git\n","    !git pull origin \"{branch}\"\n","    !git checkout -b \"{branch}\"\n","else:\n","    %cd \"{project_path}\""],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/main\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MZvPHRyMXdlS","executionInfo":{"status":"ok","timestamp":1636123057582,"user_tz":-330,"elapsed":6,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}},"outputId":"b97f25bc-5ac9-44fd-fd4d-dc413b071b54"},"source":["%cd /content"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/content\n"]}]},{"cell_type":"code","metadata":{"id":"2eRcpGL6XfDs"},"source":["!cd /content/main && git add . && git commit -m 'commit' && git push origin main"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DctyNOSdx-7h","executionInfo":{"status":"ok","timestamp":1636120595541,"user_tz":-330,"elapsed":5202,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}},"outputId":"48a53075-14c3-444c-b70e-5d0b9ce8d245"},"source":["!pip install -q wget"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["  Building wheel for wget (setup.py) ... \u001b[?25l\u001b[?25hdone\n"]}]},{"cell_type":"code","metadata":{"id":"vrEmNkAAsQlM"},"source":["import io\n","from sklearn.preprocessing import StandardScaler\n","import numpy as np\n","from tqdm.notebook import tqdm\n","import copy\n","import sys\n","import wget\n","import os\n","import logging\n","import pandas as pd\n","from os import path as osp\n","from pathlib import Path\n","\n","import bz2\n","import pickle\n","import _pickle as cPickle\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"M4swQxyAsQnj"},"source":["class Args:\n","\n","    # Paths\n","    datapath_bronze = '/content/main/data/bronze'\n","    datapath_silver = '/content/main/data/silver/T719060'\n","\n","    filename_trainset = 'train.csv'\n","    filename_iteminfo = 'item_info.csv'\n","    filename_track2_testset = 'track2_testset.csv'\n","\n","    filename_trainset_processed = 'processed_trainset_data'\n","    filename_track2_testset_processed = 'processed_track2_testset_data'\n","\n","    # Exploration\n","    start_timesteps = 1e3\n","    initial_eps = 0.1\n","    end_eps = 0.1\n","    eps_decay_period = 1\n","\n","    # Evaluation\n","    eval_freq = 1000\n","    eval_eps = 0\n","\n","    # Learning\n","    discount = 0.99\n","    epoch_num = 2\n","    batch_size = 512\n","    optimizer = 'Adam'\n","    optimizer_parameters = {'lr':3e-4}\n","    train_freq = 1\n","    polyak_target_update = True\n","    target_update_freq = 1\n","    tau = 0.005\n","\n","    # Other\n","    data_sep = ' '\n","    state_dim = 273  \n","    num_actions = 381\n","\n","\n","args = Args()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YUnDjKgnKiIZ"},"source":["torch.manual_seed(2021)\n","np.random.seed(2021)\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wIDRSKqOtEdb"},"source":["logging.basicConfig(stream=sys.stdout,\n","                    level = logging.INFO,\n","                    format='%(asctime)s [%(levelname)s] : %(message)s',\n","                    datefmt='%d-%b-%y %H:%M:%S')\n","\n","logger = logging.getLogger('IEEE21 Logger')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"N1bmqnvQv27E"},"source":["## Utilities"]},{"cell_type":"code","metadata":{"id":"tH7lmOJbAOIf"},"source":["def save_pickle(data, title):\n"," with bz2.BZ2File(title + '.pbz2', 'w') as f: \n","    cPickle.dump(data, f)\n","\n","def load_pickle(path):\n","    data = bz2.BZ2File(path+'.pbz2', 'rb')\n","    data = cPickle.load(data)\n","    return data"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"AGTVUdmtwWgZ"},"source":["def download_dataset():\n","    # create bronze folder if not exist\n","    Path(args.datapath_bronze).mkdir(parents=True, exist_ok=True)\n","    # also creating silver folder for later use\n","    Path(args.datapath_silver).mkdir(parents=True, exist_ok=True)\n","    # for each of the file, download if not exist\n","    datasets = ['train.parquet.snappy', 'item_info.parquet.snappy',\n","                'track1_testset.parquet.snappy', 'track2_testset.parquet.snappy']\n","    for filename in datasets:\n","        file_savepath = osp.join(args.datapath_bronze,filename)\n","        if not osp.exists(file_savepath):\n","            logger.info('Downloading {}'.format(filename))\n","            wget.download(url='https://github.com/sparsh-ai/ieee21cup-recsys/raw/main/data/bronze/{}'.format(filename),\n","                          out=file_savepath)\n","        else:\n","            logger.info('{} file already exists, skipping!'.format(filename))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"vk93jRMwtEWP"},"source":["def parquet_to_csv(path):\n","    savepath = osp.join(str(Path(path).parent),str(Path(path).name).split('.')[0]+'.csv')\n","    pd.read_parquet(path).to_csv(savepath, index=False, sep=args.data_sep)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_F4vRpFCzYsf"},"source":["def convert_dataset():\n","    # for each of the file, convert into csv, if csv not exist\n","    datasets = ['train.parquet.snappy', 'item_info.parquet.snappy',\n","                'track1_testset.parquet.snappy', 'track2_testset.parquet.snappy']\n","    datasets = {x:str(Path(x).name).split('.')[0]+'.csv' for x in datasets}\n","    for sfilename, tfilename in datasets.items():\n","        file_loadpath = osp.join(args.datapath_bronze,sfilename)\n","        file_savepath = osp.join(args.datapath_bronze,tfilename)\n","        if not osp.exists(file_savepath):\n","            logger.info('Converting {} to {}'.format(sfilename, tfilename))\n","            parquet_to_csv(file_loadpath)\n","        else:\n","            logger.info('{} file already exists, skipping!'.format(tfilename))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"igLLZV6gGu-v"},"source":["---"]},{"cell_type":"code","metadata":{"id":"f1Mi6dcZsQjW"},"source":["def load_dataset(path):\n","    i = 0\n","    user_id, user_click_history, user_protrait, exposed_items, labels, time = [], [], [], [], [], []\n","    with io.open(path,'r') as file:\n","        for line in file:\n","            if i > 0:\n","                user_id_1, user_click_history_1, user_protrait_1, exposed_items_1, labels_1, time_1 = line.split(' ')\n","                user_id.append(user_id_1)\n","                user_click_history.append(user_click_history_1)\n","                user_protrait.append(user_protrait_1)\n","                exposed_items.append(exposed_items_1)\n","                labels.append(labels_1)\n","                time.append(time_1)\n","            i = i + 1\n","    return user_id, user_click_history, user_protrait, exposed_items, labels, time"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"g7hdThsIsQg3"},"source":["def data_processing(user_click_history, user_protrait, exposed_items, labels, item_info_list):\n","    user_click_history_processed = []\n","    for item in user_click_history:\n","        user_click_history_row = []\n","        item_split_list = item.split(',')\n","        for item_2 in item_split_list:\n","            click_history = float(item_2.split(':')[0])\n","            user_click_history_row.append(click_history)\n","\n","        if len(user_click_history_row) < 249:\n","            for i in range(249-len(user_click_history_row)):\n","                user_click_history_row.append(0.0)\n","        \n","        if len(user_click_history_row) > 249:\n","            print(\"len(user_click_history_row): \", len(user_click_history_row))\n","            user_click_history_row = user_click_history_row[:249]\n","\n","        user_click_history_processed.append(user_click_history_row)\n","\n","    user_click_history_avg_processed = []\n","    for item in user_click_history:\n","        user_click_history_row_avg = np.array([0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0])\n","        item_split_list = item.split(',')\n","        for item_2 in item_split_list:\n","            click_history_item_id = float(item_2.split(':')[0])\n","            \n","            if click_history_item_id == 0.0:\n","                continue\n","\n","            item_info_dic = item_info_list[int(click_history_item_id)-1]\n","            item_info = item_info_dic[float(click_history_item_id)]\n","            user_click_history_row_avg = user_click_history_row_avg + np.array(item_info)\n","            \n","        user_click_history_row_avg = user_click_history_row_avg / len(item_split_list)\n","        user_click_history_row_avg = user_click_history_row_avg.tolist()\n","\n","        user_click_history_avg_processed.append(user_click_history_row_avg)\n","\n","    user_protrait_processed = []\n","    for item in user_protrait:\n","        user_protrait_row = []\n","        item_split_list = item.split(',')\n","        for item_2 in item_split_list:\n","            user_protrait_row.append(float(item_2))\n","        user_protrait_processed.append(user_protrait_row)\n","\n","    exposed_items_id = []\n","    for item in exposed_items:\n","        exposed_items_id_row = []\n","        item_split_list = item.split(',')\n","        for item_id in item_split_list:\n","            exposed_items_id_row.append(float(item_id))\n","        exposed_items_id.append(exposed_items_id_row)\n","\n","    exposed_items_processed = []\n","    for item in exposed_items:\n","        exposed_items_row = []\n","        item_split_list = item.split(',')\n","        for item_id in item_split_list:\n","            item_info_dic = item_info_list[int(item_id)-1]\n","            item_info = item_info_dic[float(item_id)]\n","\n","            exposed_items_row.append(item_info)\n","        exposed_items_processed.append(exposed_items_row)\n","    \n","    labels_processed = []\n","    for item in labels:\n","        labels_row = []\n","        item_split_list = item.split(',')\n","        for item_2 in item_split_list:\n","            labels_row.append(float(item_2))\n","        labels_processed.append(labels_row)\n","        \n","    return user_click_history_processed, user_click_history_avg_processed, user_protrait_processed, exposed_items_processed, labels_processed, exposed_items_id"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"dQuZy-BOsvAu"},"source":["def load_item_info(path):\n","    price_max = 150.0\n","    price_min = 16621.0\n","    item_info_list = []\n","    item_id, item_vec, price, location = [], [], [], []\n","    i = 0\n","    with io.open(path,'r') as file:\n","        for line in file:\n","            if i > 0:\n","                item_info = {}\n","                item_vec_row = []\n","\n","                item_id_1, item_vec_1, price_1, location_1 = line.split(' ')\n","                item_id_1 = float(item_id_1)\n","                price_1 = float(price_1)\n","                price_1 = (price_1 - price_min) / (price_max - price_min)\n","                location_1 = float(location_1)\n","\n","                item_vec_list = item_vec_1.split(',')\n","                for item_2 in item_vec_list:\n","                    item_vec_row.append(float(item_2))\n","\n","                item_id.append(item_id_1)\n","                item_vec.append(item_vec_row)\n","                price.append(price_1)\n","                location.append(location_1)\n","\n","                item = []\n","                for j in range(len(item_vec_row)):\n","                    item.append(item_vec_row[j])\n","                item.append(price_1)\n","                item.append(location_1)\n","                item_info[item_id_1] = item\n","                \n","                item_info_list.insert(int(item_id_1)-1, item_info)\n","\n","            i = i + 1\n","    return item_info_list"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"uqfsrDgcszTk"},"source":["def preprocess_trainset_data():\n","\n","    trainset_path = osp.join(args.datapath_bronze, args.filename_trainset)\n","    iteminfo_path = osp.join(args.datapath_bronze, args.filename_iteminfo)\n","    savepath = osp.join(args.datapath_silver,args.filename_trainset_processed)\n","\n","    if not osp.exists(savepath):\n","        logger.info('Loading Items Info')\n","        item_info_list = load_item_info(iteminfo_path)\n","\n","        logger.info('Loading Trainset')\n","        user_id, user_click_history, user_protrait, exposed_items, labels, time = load_dataset(trainset_path)\n","\n","        logger.info('Processing Trainset')\n","        user_click_history_processed, user_click_history_avg_processed, user_protrait_processed, exposed_items_processed, labels_processed, exposed_items_id = data_processing(user_click_history, user_protrait, exposed_items, labels, item_info_list)\n","\n","        logger.info('Scaling Features')\n","        scaler = StandardScaler()\n","        user_click_history_processed = scaler.fit_transform(user_click_history_processed).tolist()\n","        user_protrait_processed = scaler.fit_transform(user_protrait_processed).tolist()\n","\n","        processed_trainset_data = {\n","            'user_click_history_processed':user_click_history_processed,\n","            'user_click_history_avg_processed':user_click_history_avg_processed,\n","            'user_protrait_processed':user_protrait_processed,\n","            'exposed_items_processed':exposed_items_processed,\n","            'labels_processed':labels_processed,\n","            'exposed_items_id':exposed_items_id,\n","        }\n","\n","        save_pickle(processed_trainset_data, savepath)\n","        logger.info('Processed data saved at {}'.format(savepath))\n","    else:\n","        logger.info('{} Processed data already exists, skipping!'.format(savepath))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3uQR8SuF2Szh"},"source":["---"]},{"cell_type":"code","metadata":{"id":"CG-uDDlHszRf"},"source":["def load_track2_test_dataset(path):\n","    i = 0\n","    user_id, user_click_history, user_protrait = [], [], []\n","    with io.open(path,'r') as file:\n","        for line in file:\n","            if i > 0:\n","                user_id_1, user_click_history_1, user_protrait_1 = line.split(' ')\n","                user_id.append(user_id_1)\n","                user_click_history.append(user_click_history_1)\n","                user_protrait.append(user_protrait_1)\n","            i = i + 1\n","    return user_id, user_click_history, user_protrait"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"DYSTaN_HszPe"},"source":["def data_track2_test_processing(user_click_history, user_protrait, item_info_list):\n","    user_click_history_processed = []\n","    for item in user_click_history:\n","        user_click_history_row = []\n","        item_split_list = item.split(',')\n","        for item_2 in item_split_list:\n","            click_history = float(item_2.split(':')[0])\n","            user_click_history_row.append(click_history)\n","\n","        if len(user_click_history_row) < 249:\n","            for i in range(249-len(user_click_history_row)):\n","                user_click_history_row.append(0.0)\n","        \n","        if len(user_click_history_row) > 249:\n","            user_click_history_row = user_click_history_row[:249]\n","\n","        user_click_history_processed.append(user_click_history_row)\n","\n","    user_click_history_avg_processed = []\n","    for item in user_click_history:\n","        user_click_history_row_avg = np.array([0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0])\n","        item_split_list = item.split(',')\n","        for item_2 in item_split_list:\n","            click_history_item_id = float(item_2.split(':')[0])\n","            \n","            if click_history_item_id == 0.0:\n","                continue\n","\n","            item_info_dic = item_info_list[int(click_history_item_id)-1]\n","            item_info = item_info_dic[float(click_history_item_id)]\n","            user_click_history_row_avg = user_click_history_row_avg + np.array(item_info)\n","            \n","        user_click_history_row_avg = user_click_history_row_avg / len(item_split_list)\n","        user_click_history_row_avg = user_click_history_row_avg.tolist()\n","\n","        user_click_history_avg_processed.append(user_click_history_row_avg)\n","\n","    user_protrait_processed = []\n","    for item in user_protrait:\n","        user_protrait_row = []\n","        item_split_list = item.split(',')\n","        for item_2 in item_split_list:\n","            user_protrait_row.append(float(item_2))\n","        user_protrait_processed.append(user_protrait_row)\n","\n","    return user_click_history_processed, user_click_history_avg_processed, user_protrait_processed"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"XDIfH7NWG8bE"},"source":["def preprocess_track2_testset_data():\n","\n","    track2_testset_path = osp.join(args.datapath_bronze, args.filename_track2_testset)\n","    iteminfo_path = osp.join(args.datapath_bronze, args.filename_iteminfo)\n","    savepath = osp.join(args.datapath_silver,args.filename_track2_testset_processed)\n","\n","    if not osp.exists(savepath):\n","        logger.info('Loading Items Info')\n","        item_info_list = load_item_info(iteminfo_path)\n","\n","        logger.info('Loading Track2 Testset')\n","        user_id, user_click_history, user_protrait = load_track2_test_dataset(track2_testset_path)\n","\n","        logger.info('Processing Track 2 Testset')\n","        user_click_history_processed, user_click_history_avg_processed, user_protrait_processed = data_track2_test_processing(user_click_history, user_protrait, item_info_list)\n","\n","        logger.info('Scaling Features')\n","        scaler = StandardScaler()\n","        user_click_history_processed = scaler.fit_transform(user_click_history_processed).tolist()\n","        user_protrait_processed = scaler.fit_transform(user_protrait_processed).tolist()\n","\n","        processed_track2_testset_data = {\n","            'user_click_history_processed':user_click_history_processed,\n","            'user_click_history_avg_processed':user_click_history_avg_processed,\n","            'user_protrait_processed':user_protrait_processed,\n","            'item_info_list':item_info_list,\n","        }\n","\n","\n","        save_pickle(processed_track2_testset_data, savepath)\n","        logger.info('Processed data saved at {}'.format(savepath))\n","    else:\n","        logger.info('{} Processed data already exists, skipping!'.format(savepath))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Xaa771VJIqUF"},"source":["---"]},{"cell_type":"code","metadata":{"id":"5oyQnqTRsw9p"},"source":["def concat_feature_batch(user_click_history_processed_batch, user_click_history_avg_processed_batch, user_protrait_processed_batch, exposed_item_feature_processed_batch):\n","    feature_batch = []\n","    for i in range(len(user_click_history_processed_batch)):\n","        feature_row = user_click_history_processed_batch[i] + user_click_history_avg_processed_batch[i] + user_protrait_processed_batch[i] + exposed_item_feature_processed_batch[i]\n","        feature_batch.append(feature_row)\n","    return feature_batch"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"--iq5N9Ts6iU"},"source":["def get_action_info(action, item_info_list):\n","    item_info_dic = item_info_list[int(action)-1]\n","    item_info = item_info_dic[float(action)]\n","    return item_info"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"KHbgoGpKNYOG"},"source":["def write_csv(action_result_list):\n","    import pandas as pd\n","    import csv\n","\n","    test2_set_path = osp.join(args.datapath_bronze, args.filename_track2_testset)\n","    test2_set = pd.read_csv(test2_set_path)\n","    item_id_list = test2_set['user_id'].tolist()\n","    res_list = []\n","\n","    for row_list in action_result_list:\n","        row_list = list(map(str, row_list))\n","        row_str = ' '.join(row_list)\n","        res_list.append(row_str)\n","\n","    path = osp.join(args.datapath_silver,'submissions.csv')\n","    with open(path, 'w', newline='', encoding='utf8') as f:\n","        csv_write = csv.writer(f)\n","        id = item_id_list\n","        id = [ str(i) for i in id]\n","\n","        pred = res_list\n","\n","        head = ('id', 'category')\n","        csv_write.writerow(head)\n","        for pair in zip(id, pred):\n","            csv_write.writerow(pair)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"AmnI-JVAJ-h5"},"source":["---"]},{"cell_type":"code","metadata":{"id":"vC3toVkNKEln"},"source":["class FC_Q(nn.Module):\n","\tdef __init__(self, state_dim, num_actions):\n","\t\tsuper(FC_Q, self).__init__()\n","\t\tself.q1 = nn.Linear(state_dim, 512)\n","\t\tself.q2 = nn.Linear(512, 256)\n","\t\tself.q3 = nn.Linear(256, num_actions)\n","\n","\t\tself.i1 = nn.Linear(state_dim, 512)\n","\t\tself.i2 = nn.Linear(512, 256)\n","\t\tself.i3 = nn.Linear(256, num_actions)\n","\n","\tdef forward(self, state):\n","\t\tq = F.relu(self.q1(state))\n","\t\tq = F.relu(self.q2(q))\n","\n","\t\ti = F.relu(self.i1(state))\n","\t\ti = F.relu(self.i2(i))\n","\t\ti = F.relu(self.i3(i))\n","\t\treturn self.q3(q), F.log_softmax(i, dim=1), i"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_Ym3ws6KKGlU"},"source":["class discrete_BCQ(object):\n","\tdef __init__(\n","\t\tself, \n","\t\tnum_actions,\n","\t\tstate_dim,\n","\t\tdevice,\n","\t\tBCQ_threshold=0.3,\n","\t\tdiscount=0.99,\n","\t\toptimizer=\"Adam\",\n","\t\toptimizer_parameters={},\n","\t\tpolyak_target_update=False,\n","\t\ttarget_update_frequency = 1000,\n","\t\ttau=0.005,\n","\t\tinitial_eps = 1,\n","\t\tend_eps = 0.001,\n","\t\teps_decay_period = 25e4,\n","\t\teval_eps=0.001,\n","\t):\n","\t\n","\t\tself.device = device\n","\n","\t\t# Determine network type\n","\t\tself.Q = FC_Q(state_dim, num_actions).to(self.device)\n","\t\tself.Q_target = copy.deepcopy(self.Q)\n","\t\tself.Q_optimizer = getattr(torch.optim, optimizer)(self.Q.parameters(), **optimizer_parameters)\n","\n","\t\tself.discount = discount\n","\n","\t\t# Target update rule\n","\t\tself.maybe_update_target = self.polyak_target_update if polyak_target_update else self.copy_target_update\n","\t\tself.target_update_frequency = target_update_frequency\n","\t\tself.tau = tau\n","\n","\t\t# Decay for eps\n","\t\tself.initial_eps = initial_eps\n","\t\tself.end_eps = end_eps\n","\t\tself.slope = (self.end_eps - self.initial_eps) / eps_decay_period\n","\n","\t\t# Evaluation hyper-parameters\n","\t\tself.state_shape =  (-1, state_dim)\n","\t\tself.eval_eps = eval_eps\n","\t\tself.num_actions = num_actions\n","\n","\t\t# Threshold for \"unlikely\" actions\n","\t\tself.threshold = BCQ_threshold\n","\n","\t\t# Number of training iterations\n","\t\tself.iterations = 0\n","\n","\n","\tdef select_action(self, state, step, action_list, eval=False):\n","\t\tmask_list = []\n","\t\tif 0<=step and step<=2:\n","\t\t\tmask_list += [a for a in range(39, 381)]\n","\t\telif 3<=step and step<=5:\n","\t\t\tmask_list += [a for a in range(0, 39)] + [b for b in range(147, 381)]\n","\t\telif 6<=step and step<=8:\n","\t\t\tmask_list += [a for a in range(0, 147)]\n","\t\taction_list = (np.array(action_list) - 1).tolist()\n","\t\tmask_list += action_list\n","\t\tmask_list = list(set(mask_list))\n","\n","\t\t# Select action according to policy with probability (1-eps) otherwise, select random action\n","\t\tif np.random.uniform(0, 1) > self.eval_eps:\n","\t\t\twith torch.no_grad():\n","\t\t\t\tstate = torch.FloatTensor(state).reshape(self.state_shape).to(self.device)\n","\t\t\t\tq, imt, i = self.Q(state)\n","\t\t\t\t# action mask\n","\t\t\t\tfor idx in range(q.shape[0]):\n","\t\t\t\t\timt[idx][mask_list] += -1e10\n","\t\t\t\timt = imt.exp()\n","\t\t\t\timt = (imt/imt.max(1, keepdim=True)[0] > self.threshold).float()\n","\t\t\t\treturn int((imt * q + (1. - imt) * -1e8).argmax(1))\n","\t\telse:\n","\t\t\treturn np.random.randint(self.num_actions)\n","\n","\n","\tdef train_batch(self, state, action, next_state, reward, done):\n","\t\tfor bth_idx in range(len(action)):\n","\t\t\taction[bth_idx] = action[bth_idx] - 1.0\n","\t\n","\t\tstate = torch.FloatTensor(state).to(self.device)\n","\t\tnext_state = torch.FloatTensor(next_state).to(self.device)\n","\t\taction = torch.LongTensor(action).unsqueeze(1).to(self.device)\n","\t\treward = torch.FloatTensor(reward).unsqueeze(1).to(self.device)\n","\t\tdone = torch.FloatTensor(done).unsqueeze(1).to(self.device)\n","\n","\t\t# Compute the target Q value\n","\t\twith torch.no_grad():\n","\t\t\tq, imt, i = self.Q(next_state)\n","\t\t\timt = imt.exp()\n","\t\t\timt = (imt/imt.max(1, keepdim=True)[0] > self.threshold).float()\n","\n","\t\t\t# Use large negative number to mask actions from argmax\n","\t\t\tnext_action = (imt * q + (1 - imt) * -1e8).argmax(1, keepdim=True)\n","\n","\t\t\tq, imt, i = self.Q_target(next_state)\n","\t\t\ttarget_Q = reward + (1.0 - done) * self.discount * q.gather(1, next_action).reshape(-1, 1)\n","\n","\t\t# Get current Q estimate\n","\t\tcurrent_Q, imt, i = self.Q(state)\n","\t\tcurrent_Q = current_Q.gather(1, action)\n","\n","\t\t# Compute Q loss\n","\t\tq_loss = F.smooth_l1_loss(current_Q, target_Q)\n","\t\ti_loss = F.nll_loss(imt, action.reshape(-1))\n","\n","\t\tQ_loss = q_loss + i_loss + 1e-2 * i.pow(2).mean()\n","\n","\t\t# Optimize the Q\n","\t\tself.Q_optimizer.zero_grad()\n","\t\tQ_loss.backward()\n","\t\tself.Q_optimizer.step()\n","\n","\t\t# Update target network by polyak or full copy every X iterations.\n","\t\tself.iterations += 1\n","\t\tself.maybe_update_target()\n","\n","\t\treturn Q_loss\n","\n","\t# hard update: theta' = theta\n","\t# soft update(or Polyak update): theta' = tau*theta + (1-tau)*theta', tau is a little value, such as 0.001\n","\t# θ_target = τ*θ_local + (1 - τ)*θ_target\n","\tdef polyak_target_update(self):\n","\t\tfor param, target_param in zip(self.Q.parameters(), self.Q_target.parameters()):\n","\t\t   target_param.data.copy_(self.tau * param.data + (1 - self.tau) * target_param.data)\n","\n","\t# hard update: theta' = theta\n","\tdef copy_target_update(self):\n","\t\tif self.iterations % self.target_update_frequency == 0:\n","\t\t\t self.Q_target.load_state_dict(self.Q.state_dict())\n","\t\n","\tdef train(self, state, action, next_state, reward, done):\n","\t\taction = action - 1.0\n","\n","\t\tstate = torch.FloatTensor(state).unsqueeze(0).to(self.device)\n","\t\tnext_state = torch.FloatTensor(next_state).unsqueeze(0).to(self.device)\n","\t\taction = torch.LongTensor(np.array(action)).unsqueeze(0).unsqueeze(0).to(self.device)\n","\t\treward = torch.FloatTensor(np.array(reward)).unsqueeze(0).unsqueeze(0).to(self.device)\n","\t\tdone = torch.FloatTensor(np.array(done)).unsqueeze(0).unsqueeze(0).to(self.device)\n","\n","\t\t# Compute the target Q value\n","\t\t# r_t + Q'(s_{t+1}, argmax_a Q(s_{t+1}, a))\n","\t\twith torch.no_grad():\n","\t\t\tq, imt, i = self.Q(next_state)#q:torch.Size([1, 381])\n","\t\t\timt = imt.exp()\n","\t\t\timt = (imt/imt.max(1, keepdim=True)[0] > self.threshold).float()\n","\n","\t\t\t# Use large negative number to mask actions from argmax\n","\t\t\tnext_action = (imt * q + (1 - imt) * -1e8).argmax(1, keepdim=True) #torch.Size([1, 1])\n","\n","\t\t\tq, imt, i = self.Q_target(next_state) #torch.Size([1, 381])\n","\t\t\t# target network\n","\t\t\ttarget_Q = reward + (1.0 - done) * self.discount * q.gather(1, next_action).reshape(-1, 1)\n","\n","\t\t# Get current Q estimate\n","\t\t# Q(s_t, a_t)\n","\t\tcurrent_Q, imt, i = self.Q(state)\n","\t\tcurrent_Q = current_Q.gather(1, action)\n","\n","\t\t# Compute Q loss\n","\t\tq_loss = F.smooth_l1_loss(current_Q, target_Q)\n","\t\ti_loss = F.nll_loss(imt, action.reshape(-1))\n","\n","\t\tQ_loss = q_loss + i_loss + 1e-2 * i.pow(2).mean()\n","\n","\t\t# Optimize the Q\n","\t\tself.Q_optimizer.zero_grad()\n","\t\tQ_loss.backward()\n","\t\tself.Q_optimizer.step()\n","\n","\t\t# Update target network by polyak or full copy every X iterations.\n","\t\tself.iterations += 1\n","\t\tself.maybe_update_target()\n","\n","\t\t# 返回Q_loss以供打印\n","\t\treturn Q_loss"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Zmq6G_83KEQD"},"source":["---"]},{"cell_type":"code","metadata":{"id":"C10TeZxeLVQN"},"source":["def eval_policy(policy):\n","    logger.info(\"Evaluating policy   ...\")\n","\n","    testset_path = osp.join(args.datapath_silver, args.filename_track2_testset_processed)\n","    track2_testset_processed = load_pickle(testset_path)\n","    test_user_click_history_processed = track2_testset_processed['user_click_history_processed']\n","    test_user_click_history_avg_processed = track2_testset_processed['user_click_history_avg_processed']\n","    test_user_protrait_processed = track2_testset_processed['user_protrait_processed']\n","    item_info_list = track2_testset_processed['item_info_list']\n","\n","    num_track2_test_set = len(test_user_click_history_processed)\n","\n","    action_result_list = []\n","    for test_iters in tqdm(range(num_track2_test_set)):\n","        test_user_click_history_processed_row = test_user_click_history_processed[test_iters]\n","        test_user_click_history_avg_processed_row = test_user_click_history_avg_processed[test_iters]\n","        test_user_protrait_processed_row = test_user_protrait_processed[test_iters]\n","\n","        num_row = 9\n","        action_list = []\n","        reward_row = 0.0\n","        \n","        for i in range(num_row):\n","            if i == 0:\n","                action_item_feature = [0, 0, 0, 0, 0, 0, 0]\n","            else:\n","                action_item_feature = get_action_info(action_list[i-1], item_info_list)\n","            \n","            state = test_user_click_history_processed_row + test_user_click_history_avg_processed_row + test_user_protrait_processed_row + action_item_feature\n","\n","            action = 1 + policy.select_action(state, i, action_list)\n","            \n","            action_list.append(action)\n","\n","            reward = action_item_feature[-2]\n","            reward_row += reward\n","        \n","        action_result_list.append(action_list)\n","\n","    return action_result_list"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"yHrGOevyMaDV"},"source":["def train_BCQ_batch():\n","    \"\"\"Trains BCQ offline Batch\"\"\"\n","\n","    logger.info('epoch_num: {}'.format(args.epoch_num))\n","    logger.info('batch_size: {}'.format(args.batch_size))\n","\n","    logger.info('Initialize and load policy')\n","    policy = discrete_BCQ(\n","        args.num_actions,\n","        args.state_dim,\n","        device,\n","        0.3,\n","        args.discount,\n","        args.optimizer,\n","        args.optimizer_parameters,\n","        args.polyak_target_update,\n","        args.target_update_freq,\n","        args.tau,\n","        args.initial_eps,\n","        args.end_eps,\n","        args.eps_decay_period,\n","        args.eval_eps\n","\t)\n"," \n","    logger.info('Load training dataset')\n","    trainset_path = osp.join(args.datapath_silver, args.filename_trainset_processed)\n","    trainset_processed = load_pickle(trainset_path)\n","    user_click_history_processed = trainset_processed['user_click_history_processed']\n","    user_click_history_avg_processed = trainset_processed['user_click_history_avg_processed']\n","    user_protrait_processed = trainset_processed['user_protrait_processed']\n","    exposed_items_processed = trainset_processed['exposed_items_processed']\n","    labels_processed = trainset_processed['labels_processed']\n","    exposed_items_id = trainset_processed['exposed_items_id']\n","\n","    num_train_set = len(user_click_history_processed)\n","    \n","    logger.info(\"Training  ...\")\n","    batch_num = num_train_set // args.batch_size\n","    q_loss_list = []\n","    reward_list = []\n","    for epoch in range(args.epoch_num):\n","        idx = np.random.permutation(num_train_set)\n","        q_loss_total = 0.0\n","        reward_total = 0.0\n","        for i in range(batch_num):\n","            if i%10==0:\n","                logger.info('epoch={},batch={}/{}'.format(epoch,i,batch_num))\n","            batch_idx = idx[i*args.batch_size:(i+1)*args.batch_size].tolist()\n","            user_click_history_processed_batch = []\n","            user_click_history_avg_processed_batch = []\n","            user_protrait_processed_batch = []\n","            exposed_item_batch = []\n","            labels_batch = []\n","            exposed_items_id_batch = []\n","            for i_idx in batch_idx:\n","                user_click_history_processed_batch.append(user_click_history_processed[i_idx])\n","                user_click_history_avg_processed_batch.append(user_click_history_avg_processed[i_idx])\n","                user_protrait_processed_batch.append(user_protrait_processed[i_idx])\n","                exposed_item_batch.append(exposed_items_processed[i_idx])\n","                labels_batch.append(labels_processed[i_idx])\n","                exposed_items_id_batch.append(exposed_items_id[i_idx])\n","\n","            num_step = 9\n","            for step in range(num_step):                \n","                if step == 0:\n","                    exposed_item_feature_last_batch = []\n","                    for bth in range(args.batch_size):\n","                        exposed_item_feature_last_batch.append([0, 0, 0, 0, 0, 0, 0])\n","                else:\n","                    exposed_item_feature_last_batch = []\n","                    for bth in range(args.batch_size):\n","                        exposed_item_feature_last_batch.append(exposed_item_batch[bth][step - 1])\n","                exposed_item_feature_cur_batch = []\n","                for bth in range(args.batch_size):\n","                    exposed_item_feature_cur_batch.append(exposed_item_batch[bth][step])\n","\n","                # state : user_click_history + user_protrait + last product features\n","                state = concat_feature_batch(user_click_history_processed_batch, user_click_history_avg_processed_batch, user_protrait_processed_batch, exposed_item_feature_last_batch) #list 266\n","\n","                # next_state : user_click_history + user_protrait + current product features\n","                next_state = concat_feature_batch(user_click_history_processed_batch, user_click_history_avg_processed_batch, user_protrait_processed_batch, exposed_item_feature_cur_batch)#list 266\n","\n","                # action\n","                reward = []\n","                action = []\n","                for bth in range(args.batch_size):\n","                    if labels_batch[bth][step] == 1.0:\n","                        reward.append(labels_batch[bth][step] * exposed_item_feature_cur_batch[bth][-2])\n","                    else:\n","                        reward.append((-0.25) * exposed_item_feature_cur_batch[bth][-2])\n","                    action.append(exposed_items_id_batch[bth][step])\n","\n","                done = args.batch_size*[1.0] if (step == num_step-1) else args.batch_size*[0.0]\n","                q_loss = policy.train_batch(state, action, next_state, reward, done)\n","                q_loss_total += q_loss.item()\n","                reward_total += sum(reward)\n","        \n","        q_loss_list.append(q_loss_total)\n","        reward_list.append(reward_total)\n","\n","        # evaluations\n","        if epoch > 0 and epoch % 30 == 0:\n","            _ = eval_policy(policy)\n","\n","    logger.info(\"Predicting & writing csv file  ...\")\n","    action_result_list = eval_policy(policy)\n","    write_csv(action_result_list)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"koFQxtgos6gE"},"source":["## Jobs"]},{"cell_type":"code","metadata":{"id":"2y8mdDjds6dr"},"source":["logger.info('JOB START: DOWNLOAD_RAW_DATASET')\n","download_dataset()\n","logger.info('JOB END: DOWNLOAD_RAW_DATASET')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3ig3tPpB2Fx-"},"source":["logger.info('JOB START: DATASET_CONVERSION_PARQUET_TO_CSV')\n","convert_dataset()\n","logger.info('JOB END: DATASET_CONVERSION_PARQUET_TO_CSV')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"pbQUhQUDGHlz"},"source":["logger.info('JOB START: TRAINSET_DATA_PREPROCESSING')\n","preprocess_trainset_data()\n","logger.info('JOB END: TRAINSET_DATA_PREPROCESSING')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hh3rb38eJU7b"},"source":["logger.info('JOB START: TRACK2_TESTSET_DATA_PREPROCESSING')\n","preprocess_track2_testset_data()\n","logger.info('JOB END: TRACK2_TESTSET_DATA_PREPROCESSING')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"eHZDmoEUahTW"},"source":["logger.info('JOB START: BCQ_MODEL_TRAINING_AND_EVALUATION')\n","train_BCQ_batch()\n","logger.info('JOB END: BCQ_MODEL_TRAINING_AND_EVALUATION')"],"execution_count":null,"outputs":[]}]}