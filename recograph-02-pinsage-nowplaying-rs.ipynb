{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"recograph-02-pinsage-nowplaying-rs.ipynb","provenance":[{"file_id":"1ZUkH1-pOvwaAA3srO3XNdFSmhNz9flTA","timestamp":1621238595044}],"collapsed_sections":[],"authorship_tag":"ABX9TyMYIZv4QIVZ7Xbv4Rk6vAWL"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"id":"3cJyL6GKJbfy"},"source":["%reload_ext google.colab.data_table"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"D-GeM90JLOTw"},"source":["import warnings\n","warnings.filterwarnings('ignore')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_JkgOWMtcajy","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1621242857176,"user_tz":-330,"elapsed":6732,"user":{"displayName":"Sparsh Agarwal","photoUrl":"","userId":"13037694610922482904"}},"outputId":"784d8161-05a9-4ef1-bae7-e5a675069bb7"},"source":["!pip install dgl"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Collecting dgl\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/71/c4/ce24841375cf4393787dbf9a645e271c19a03d2d9a0e5770b08ba76bcfde/dgl-0.6.1-cp37-cp37m-manylinux1_x86_64.whl (4.4MB)\n","\u001b[K     |████████████████████████████████| 4.4MB 3.9MB/s \n","\u001b[?25hRequirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from dgl) (2.23.0)\n","Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.7/dist-packages (from dgl) (1.19.5)\n","Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from dgl) (1.4.1)\n","Requirement already satisfied: networkx>=2.1 in /usr/local/lib/python3.7/dist-packages (from dgl) (2.5.1)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->dgl) (1.24.3)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->dgl) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->dgl) (2020.12.5)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->dgl) (3.0.4)\n","Requirement already satisfied: decorator<5,>=4.3 in /usr/local/lib/python3.7/dist-packages (from networkx>=2.1->dgl) (4.4.2)\n","Installing collected packages: dgl\n","Successfully installed dgl-0.6.1\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"RLjy0-wdCB4Y","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1621243017301,"user_tz":-330,"elapsed":154108,"user":{"displayName":"Sparsh Agarwal","photoUrl":"","userId":"13037694610922482904"}},"outputId":"68e0bb9c-4235-4c8e-961e-4406a47a99fc"},"source":["!wget https://zenodo.org/record/3248543/files/nowplayingrs.zip?download=1\n","!unzip /content/nowplayingrs.zip?download=1 -d /content"],"execution_count":null,"outputs":[{"output_type":"stream","text":["--2021-05-17 09:14:28--  https://zenodo.org/record/3248543/files/nowplayingrs.zip?download=1\n","Resolving zenodo.org (zenodo.org)... 137.138.76.77\n","Connecting to zenodo.org (zenodo.org)|137.138.76.77|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 1329064666 (1.2G) [application/octet-stream]\n","Saving to: ‘nowplayingrs.zip?download=1’\n","\n","nowplayingrs.zip?do 100%[===================>]   1.24G  6.72MB/s    in 1m 51s  \n","\n","2021-05-17 09:16:20 (11.5 MB/s) - ‘nowplayingrs.zip?download=1’ saved [1329064666/1329064666]\n","\n","Archive:  /content/nowplayingrs.zip?download=1\n","   creating: /content/nowplaying_rs_dataset/\n","  inflating: /content/nowplaying_rs_dataset/sentiment_values.csv  \n","  inflating: /content/nowplaying_rs_dataset/context_content_features.csv  \n","  inflating: /content/nowplaying_rs_dataset/README.txt  \n","  inflating: /content/nowplaying_rs_dataset/user_track_hashtag_timestamp.csv  \n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"m88Ip1w9H7Qn","executionInfo":{"status":"ok","timestamp":1621243017304,"user_tz":-330,"elapsed":153111,"user":{"displayName":"Sparsh Agarwal","photoUrl":"","userId":"13037694610922482904"}},"outputId":"58af2a4b-09d9-434d-ebdd-70a759e52972"},"source":["%%writefile builder.py\n","\n","\"\"\"Graph builder from pandas dataframes\"\"\"\n","from collections import namedtuple\n","from pandas.api.types import is_numeric_dtype, is_categorical_dtype, is_categorical\n","import dgl\n","\n","__all__ = ['PandasGraphBuilder']\n","\n","def _series_to_tensor(series):\n","    if is_categorical(series):\n","        return torch.LongTensor(series.cat.codes.values.astype('int64'))\n","    else:       # numeric\n","        return torch.FloatTensor(series.values)\n","\n","class PandasGraphBuilder(object):\n","    \"\"\"Creates a heterogeneous graph from multiple pandas dataframes.\n","\n","    Examples\n","    --------\n","    Let's say we have the following three pandas dataframes:\n","\n","    User table ``users``:\n","\n","    ===========  ===========  =======\n","    ``user_id``  ``country``  ``age``\n","    ===========  ===========  =======\n","    XYZZY        U.S.         25\n","    FOO          China        24\n","    BAR          China        23\n","    ===========  ===========  =======\n","\n","    Game table ``games``:\n","\n","    ===========  =========  ==============  ==================\n","    ``game_id``  ``title``  ``is_sandbox``  ``is_multiplayer``\n","    ===========  =========  ==============  ==================\n","    1            Minecraft  True            True\n","    2            Tetris 99  False           True\n","    ===========  =========  ==============  ==================\n","\n","    Play relationship table ``plays``:\n","\n","    ===========  ===========  =========\n","    ``user_id``  ``game_id``  ``hours``\n","    ===========  ===========  =========\n","    XYZZY        1            24\n","    FOO          1            20\n","    FOO          2            16\n","    BAR          2            28\n","    ===========  ===========  =========\n","\n","    One could then create a bidirectional bipartite graph as follows:\n","    >>> builder = PandasGraphBuilder()\n","    >>> builder.add_entities(users, 'user_id', 'user')\n","    >>> builder.add_entities(games, 'game_id', 'game')\n","    >>> builder.add_binary_relations(plays, 'user_id', 'game_id', 'plays')\n","    >>> builder.add_binary_relations(plays, 'game_id', 'user_id', 'played-by')\n","    >>> g = builder.build()\n","    >>> g.number_of_nodes('user')\n","    3\n","    >>> g.number_of_edges('plays')\n","    4\n","    \"\"\"\n","    def __init__(self):\n","        self.entity_tables = {}\n","        self.relation_tables = {}\n","\n","        self.entity_pk_to_name = {}     # mapping from primary key name to entity name\n","        self.entity_pk = {}             # mapping from entity name to primary key\n","        self.entity_key_map = {}        # mapping from entity names to primary key values\n","        self.num_nodes_per_type = {}\n","        self.edges_per_relation = {}\n","        self.relation_name_to_etype = {}\n","        self.relation_src_key = {}      # mapping from relation name to source key\n","        self.relation_dst_key = {}      # mapping from relation name to destination key\n","\n","    def add_entities(self, entity_table, primary_key, name):\n","        entities = entity_table[primary_key].astype('category')\n","        if not (entities.value_counts() == 1).all():\n","            raise ValueError('Different entity with the same primary key detected.')\n","        # preserve the category order in the original entity table\n","        entities = entities.cat.reorder_categories(entity_table[primary_key].values)\n","\n","        self.entity_pk_to_name[primary_key] = name\n","        self.entity_pk[name] = primary_key\n","        self.num_nodes_per_type[name] = entity_table.shape[0]\n","        self.entity_key_map[name] = entities\n","        self.entity_tables[name] = entity_table\n","\n","    def add_binary_relations(self, relation_table, source_key, destination_key, name):\n","        src = relation_table[source_key].astype('category')\n","        src = src.cat.set_categories(\n","            self.entity_key_map[self.entity_pk_to_name[source_key]].cat.categories)\n","        dst = relation_table[destination_key].astype('category')\n","        dst = dst.cat.set_categories(\n","            self.entity_key_map[self.entity_pk_to_name[destination_key]].cat.categories)\n","        if src.isnull().any():\n","            raise ValueError(\n","                'Some source entities in relation %s do not exist in entity %s.' %\n","                (name, source_key))\n","        if dst.isnull().any():\n","            raise ValueError(\n","                'Some destination entities in relation %s do not exist in entity %s.' %\n","                (name, destination_key))\n","\n","        srctype = self.entity_pk_to_name[source_key]\n","        dsttype = self.entity_pk_to_name[destination_key]\n","        etype = (srctype, name, dsttype)\n","        self.relation_name_to_etype[name] = etype\n","        self.edges_per_relation[etype] = (src.cat.codes.values.astype('int64'), dst.cat.codes.values.astype('int64'))\n","        self.relation_tables[name] = relation_table\n","        self.relation_src_key[name] = source_key\n","        self.relation_dst_key[name] = destination_key\n","\n","    def build(self):\n","        # Create heterograph\n","        graph = dgl.heterograph(self.edges_per_relation, self.num_nodes_per_type)\n","        return graph"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Writing builder.py\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rL9meK9zIF6x","executionInfo":{"status":"ok","timestamp":1621243017306,"user_tz":-330,"elapsed":152786,"user":{"displayName":"Sparsh Agarwal","photoUrl":"","userId":"13037694610922482904"}},"outputId":"6f6b070f-242f-49b5-bcce-d48e25be961a"},"source":["%%writefile data_utils.py\n","\n","import torch\n","import dgl\n","import numpy as np\n","import scipy.sparse as ssp\n","import tqdm\n","import dask.dataframe as dd\n","\n","# This is the train-test split method most of the recommender system papers running on MovieLens\n","# takes.  It essentially follows the intuition of \"training on the past and predict the future\".\n","# One can also change the threshold to make validation and test set take larger proportions.\n","def train_test_split_by_time(df, timestamp, user):\n","    df['train_mask'] = np.ones((len(df),), dtype=np.bool)\n","    df['val_mask'] = np.zeros((len(df),), dtype=np.bool)\n","    df['test_mask'] = np.zeros((len(df),), dtype=np.bool)\n","    df = dd.from_pandas(df, npartitions=10)\n","    def train_test_split(df):\n","        df = df.sort_values([timestamp])\n","        if df.shape[0] > 1:\n","            df.iloc[-1, -3] = False\n","            df.iloc[-1, -1] = True\n","        if df.shape[0] > 2:\n","            df.iloc[-2, -3] = False\n","            df.iloc[-2, -2] = True\n","        return df\n","    df = df.groupby(user, group_keys=False).apply(train_test_split).compute(scheduler='processes').sort_index()\n","    print(df[df[user] == df[user].unique()[0]].sort_values(timestamp))\n","    return df['train_mask'].to_numpy().nonzero()[0], \\\n","           df['val_mask'].to_numpy().nonzero()[0], \\\n","           df['test_mask'].to_numpy().nonzero()[0]\n","\n","def build_train_graph(g, train_indices, utype, itype, etype, etype_rev):\n","    train_g = g.edge_subgraph(\n","        {etype: train_indices, etype_rev: train_indices},\n","        preserve_nodes=True)\n","    # remove the induced node IDs - should be assigned by model instead\n","    del train_g.nodes[utype].data[dgl.NID]\n","    del train_g.nodes[itype].data[dgl.NID]\n","\n","    # copy features\n","    for ntype in g.ntypes:\n","        for col, data in g.nodes[ntype].data.items():\n","            train_g.nodes[ntype].data[col] = data\n","    for etype in g.etypes:\n","        for col, data in g.edges[etype].data.items():\n","            train_g.edges[etype].data[col] = data[train_g.edges[etype].data[dgl.EID]]\n","\n","    return train_g\n","\n","def build_val_test_matrix(g, val_indices, test_indices, utype, itype, etype):\n","    n_users = g.number_of_nodes(utype)\n","    n_items = g.number_of_nodes(itype)\n","    val_src, val_dst = g.find_edges(val_indices, etype=etype)\n","    test_src, test_dst = g.find_edges(test_indices, etype=etype)\n","    val_src = val_src.numpy()\n","    val_dst = val_dst.numpy()\n","    test_src = test_src.numpy()\n","    test_dst = test_dst.numpy()\n","    val_matrix = ssp.coo_matrix((np.ones_like(val_src), (val_src, val_dst)), (n_users, n_items))\n","    test_matrix = ssp.coo_matrix((np.ones_like(test_src), (test_src, test_dst)), (n_users, n_items))\n","\n","    return val_matrix, test_matrix\n","\n","def linear_normalize(values):\n","    return (values - values.min(0, keepdims=True)) / \\\n","        (values.max(0, keepdims=True) - values.min(0, keepdims=True))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Writing data_utils.py\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"r9YUPFDbIWjM","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1621243023050,"user_tz":-330,"elapsed":157861,"user":{"displayName":"Sparsh Agarwal","photoUrl":"","userId":"13037694610922482904"}},"outputId":"9438e18b-f222-4000-94b1-d7c687a5820f"},"source":["!pip install dask[dataframe]"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: dask[dataframe] in /usr/local/lib/python3.7/dist-packages (2.12.0)\n","Requirement already satisfied: numpy>=1.13.0; extra == \"dataframe\" in /usr/local/lib/python3.7/dist-packages (from dask[dataframe]) (1.19.5)\n","Requirement already satisfied: pandas>=0.23.0; extra == \"dataframe\" in /usr/local/lib/python3.7/dist-packages (from dask[dataframe]) (1.1.5)\n","Collecting fsspec>=0.6.0; extra == \"dataframe\"\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/bc/52/816d1a3a599176057bf29dfacb1f8fadb61d35fbd96cb1bab4aaa7df83c0/fsspec-2021.5.0-py3-none-any.whl (111kB)\n","\u001b[K     |████████████████████████████████| 112kB 3.9MB/s \n","\u001b[?25hRequirement already satisfied: toolz>=0.7.3; extra == \"dataframe\" in /usr/local/lib/python3.7/dist-packages (from dask[dataframe]) (0.11.1)\n","Collecting partd>=0.3.10; extra == \"dataframe\"\n","  Downloading https://files.pythonhosted.org/packages/41/94/360258a68b55f47859d72b2d0b2b3cfe0ca4fbbcb81b78812bd00ae86b7c/partd-1.2.0-py3-none-any.whl\n","Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.23.0; extra == \"dataframe\"->dask[dataframe]) (2018.9)\n","Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.23.0; extra == \"dataframe\"->dask[dataframe]) (2.8.1)\n","Collecting locket\n","  Downloading https://files.pythonhosted.org/packages/50/b8/e789e45b9b9c2db75e9d9e6ceb022c8d1d7e49b2c085ce8c05600f90a96b/locket-0.2.1-py2.py3-none-any.whl\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas>=0.23.0; extra == \"dataframe\"->dask[dataframe]) (1.15.0)\n","Installing collected packages: fsspec, locket, partd\n","Successfully installed fsspec-2021.5.0 locket-0.2.1 partd-1.2.0\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"nK2226CYHmCC","colab":{"base_uri":"https://localhost:8080/","height":103},"executionInfo":{"status":"ok","timestamp":1621243023053,"user_tz":-330,"elapsed":157381,"user":{"displayName":"Sparsh Agarwal","photoUrl":"","userId":"13037694610922482904"}},"outputId":"424153d5-6e82-4e00-d887-f3649ec7bbb0"},"source":["\"\"\"\n","Script that reads from raw MovieLens-1M data and dumps into a pickle\n","file the following:\n","* A heterogeneous graph with categorical features.\n","* A list with all the movie titles.  The movie titles correspond to\n","  the movie nodes in the heterogeneous graph.\n","This script exemplifies how to prepare tabular data with textual\n","features.  Since DGL graphs do not store variable-length features, we\n","instead put variable-length features into a more suitable container\n","(e.g. torchtext to handle list of texts)\n","\"\"\""],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'\\nScript that reads from raw MovieLens-1M data and dumps into a pickle\\nfile the following:\\n* A heterogeneous graph with categorical features.\\n* A list with all the movie titles.  The movie titles correspond to\\n  the movie nodes in the heterogeneous graph.\\nThis script exemplifies how to prepare tabular data with textual\\nfeatures.  Since DGL graphs do not store variable-length features, we\\ninstead put variable-length features into a more suitable container\\n(e.g. torchtext to handle list of texts)\\n'"]},"metadata":{"tags":[]},"execution_count":9}]},{"cell_type":"code","metadata":{"id":"zKlLCpOlCanU","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1621243027159,"user_tz":-330,"elapsed":161284,"user":{"displayName":"Sparsh Agarwal","photoUrl":"","userId":"13037694610922482904"}},"outputId":"bacb223f-ccb8-49c7-a46a-e71c9cafe37f"},"source":["import os\n","import argparse\n","import pandas as pd\n","import scipy.sparse as ssp\n","import pickle\n","from builder import PandasGraphBuilder\n","from data_utils import *"],"execution_count":null,"outputs":[{"output_type":"stream","text":["DGL backend not selected or invalid.  Assuming PyTorch for now.\n"],"name":"stderr"},{"output_type":"stream","text":["Setting the default backend to \"pytorch\". You can change it in the ~/.dgl/config.json file or export the DGLBACKEND environment variable.  Valid options are: pytorch, mxnet, tensorflow (all lowercase)\n"],"name":"stdout"},{"output_type":"stream","text":["Using backend: pytorch\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"8ZUqdOZtI6o2"},"source":["# parser = argparse.ArgumentParser()\n","# parser.add_argument('directory', type=str)\n","# parser.add_argument('output_path', type=str)\n","# args = parser.parse_args()\n","directory = './nowplaying_rs_dataset'\n","output_path = './nowplaying-graph-data.pkl'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"vJSbnupmI6lw"},"source":["## Build heterogeneous graph"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"JRf3Cn7KI6i5"},"source":["data = pd.read_csv(os.path.join(directory, 'context_content_features.csv'))\n","track_feature_cols = list(data.columns[1:13])\n","data = data[['user_id', 'track_id', 'created_at'] + track_feature_cols].dropna()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":371},"id":"TDptSUWlaWVz","executionInfo":{"status":"ok","timestamp":1621243062918,"user_tz":-330,"elapsed":195848,"user":{"displayName":"Sparsh Agarwal","photoUrl":"","userId":"13037694610922482904"}},"outputId":"70488742-fca9-4c29-ca71-4c72e0b624e2"},"source":["data.head()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.module+javascript":"\n      import \"https://ssl.gstatic.com/colaboratory/data_table/a6224c040fa35dcf/data_table.js\";\n\n      window.createDataTable({\n        data: [[{\n            'v': 0,\n            'f': \"0\",\n        },\n{\n            'v': 81496937,\n            'f': \"81496937\",\n        },\n\"cd52b3e5b51da29e5893dba82a418a4b\",\n\"2014-01-01 05:54:21\",\n{\n            'v': 0.00479,\n            'f': \"0.00479\",\n        },\n{\n            'v': 0.18,\n            'f': \"0.18\",\n        },\n{\n            'v': 0.0294,\n            'f': \"0.0294\",\n        },\n{\n            'v': 0.634,\n            'f': \"0.634\",\n        },\n{\n            'v': 0.342,\n            'f': \"0.342\",\n        },\n{\n            'v': -8.345,\n            'f': \"-8.345\",\n        },\n{\n            'v': 125.044,\n            'f': \"125.044\",\n        },\n{\n            'v': 0.00035,\n            'f': \"0.00035\",\n        },\n{\n            'v': 0.6970000000000001,\n            'f': \"0.6970000000000001\",\n        },\n{\n            'v': 1.0,\n            'f': \"1.0\",\n        },\n{\n            'v': 6.0,\n            'f': \"6.0\",\n        },\n\"b2980c722a1ace7a30303718ce5491d8\"],\n [{\n            'v': 1,\n            'f': \"1\",\n        },\n{\n            'v': 2205686924,\n            'f': \"2205686924\",\n        },\n\"da3110a77b724072b08f231c9d6f7534\",\n\"2014-01-01 05:54:22\",\n{\n            'v': 0.0177,\n            'f': \"0.0177\",\n        },\n{\n            'v': 0.0638,\n            'f': \"0.0638\",\n        },\n{\n            'v': 0.0624,\n            'f': \"0.0624\",\n        },\n{\n            'v': 0.769,\n            'f': \"0.769\",\n        },\n{\n            'v': 0.752,\n            'f': \"0.752\",\n        },\n{\n            'v': -8.252,\n            'f': \"-8.252\",\n        },\n{\n            'v': 95.86200000000001,\n            'f': \"95.86200000000001\",\n        },\n{\n            'v': 0.267,\n            'f': \"0.267\",\n        },\n{\n            'v': 0.826,\n            'f': \"0.826\",\n        },\n{\n            'v': 1.0,\n            'f': \"1.0\",\n        },\n{\n            'v': 7.0,\n            'f': \"7.0\",\n        },\n\"5cddcd0e314e2f2223ab21937d2c8778\"],\n [{\n            'v': 2,\n            'f': \"2\",\n        },\n{\n            'v': 132588395,\n            'f': \"132588395\",\n        },\n\"ba84d88c10fb0e42d4754a27ead10546\",\n\"2014-01-01 05:54:22\",\n{\n            'v': 0.0,\n            'f': \"0.0\",\n        },\n{\n            'v': 0.086,\n            'f': \"0.086\",\n        },\n{\n            'v': 0.0436,\n            'f': \"0.0436\",\n        },\n{\n            'v': 0.675,\n            'f': \"0.675\",\n        },\n{\n            'v': 0.775,\n            'f': \"0.775\",\n        },\n{\n            'v': -4.4319999999999995,\n            'f': \"-4.4319999999999995\",\n        },\n{\n            'v': 97.03,\n            'f': \"97.03\",\n        },\n{\n            'v': 0.217,\n            'f': \"0.217\",\n        },\n{\n            'v': 0.885,\n            'f': \"0.885\",\n        },\n{\n            'v': 0.0,\n            'f': \"0.0\",\n        },\n{\n            'v': 1.0,\n            'f': \"1.0\",\n        },\n\"e41273f43af504714d85465294f1f369\"],\n [{\n            'v': 3,\n            'f': \"3\",\n        },\n{\n            'v': 97675221,\n            'f': \"97675221\",\n        },\n\"33f95122281f76e7134f9cbea3be980f\",\n\"2014-01-01 05:54:24\",\n{\n            'v': 0.0,\n            'f': \"0.0\",\n        },\n{\n            'v': 0.14300000000000002,\n            'f': \"0.14300000000000002\",\n        },\n{\n            'v': 0.0292,\n            'f': \"0.0292\",\n        },\n{\n            'v': 0.324,\n            'f': \"0.324\",\n        },\n{\n            'v': 0.33299999999999996,\n            'f': \"0.33299999999999996\",\n        },\n{\n            'v': -5.647,\n            'f': \"-5.647\",\n        },\n{\n            'v': 74.101,\n            'f': \"74.101\",\n        },\n{\n            'v': 0.239,\n            'f': \"0.239\",\n        },\n{\n            'v': 0.574,\n            'f': \"0.574\",\n        },\n{\n            'v': 1.0,\n            'f': \"1.0\",\n        },\n{\n            'v': 7.0,\n            'f': \"7.0\",\n        },\n\"557ce373bd29743eb00a3723ab19ebe8\"],\n [{\n            'v': 4,\n            'f': \"4\",\n        },\n{\n            'v': 17945688,\n            'f': \"17945688\",\n        },\n\"b5c42e81e15cd54b9b0ee34711dedf05\",\n\"2014-01-01 05:54:24\",\n{\n            'v': 0.000183,\n            'f': \"0.000183\",\n        },\n{\n            'v': 0.36200000000000004,\n            'f': \"0.36200000000000004\",\n        },\n{\n            'v': 0.0524,\n            'f': \"0.0524\",\n        },\n{\n            'v': 0.767,\n            'f': \"0.767\",\n        },\n{\n            'v': 0.8079999999999999,\n            'f': \"0.8079999999999999\",\n        },\n{\n            'v': -5.011,\n            'f': \"-5.011\",\n        },\n{\n            'v': 114.23700000000001,\n            'f': \"114.23700000000001\",\n        },\n{\n            'v': 0.0364,\n            'f': \"0.0364\",\n        },\n{\n            'v': 0.7390000000000001,\n            'f': \"0.7390000000000001\",\n        },\n{\n            'v': 0.0,\n            'f': \"0.0\",\n        },\n{\n            'v': 10.0,\n            'f': \"10.0\",\n        },\n\"77bd64b4bf77e10001fd02964985ae0f\"]],\n        columns: [[\"number\", \"index\"], [\"number\", \"user_id\"], [\"string\", \"track_id\"], [\"string\", \"created_at\"], [\"number\", \"instrumentalness\"], [\"number\", \"liveness\"], [\"number\", \"speechiness\"], [\"number\", \"danceability\"], [\"number\", \"valence\"], [\"number\", \"loudness\"], [\"number\", \"tempo\"], [\"number\", \"acousticness\"], [\"number\", \"energy\"], [\"number\", \"mode\"], [\"number\", \"key\"], [\"string\", \"artist_id\"]],\n        columnOptions: [{\"width\": \"1px\", \"className\": \"index_column\"}],\n        rowsPerPage: 25,\n        helpUrl: \"https://colab.research.google.com/notebooks/data_table.ipynb\",\n        suppressOutputScrolling: true,\n        minimumWidth: undefined,\n      });\n    ","text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>user_id</th>\n","      <th>track_id</th>\n","      <th>created_at</th>\n","      <th>instrumentalness</th>\n","      <th>liveness</th>\n","      <th>speechiness</th>\n","      <th>danceability</th>\n","      <th>valence</th>\n","      <th>loudness</th>\n","      <th>tempo</th>\n","      <th>acousticness</th>\n","      <th>energy</th>\n","      <th>mode</th>\n","      <th>key</th>\n","      <th>artist_id</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>81496937</td>\n","      <td>cd52b3e5b51da29e5893dba82a418a4b</td>\n","      <td>2014-01-01 05:54:21</td>\n","      <td>0.004790</td>\n","      <td>0.1800</td>\n","      <td>0.0294</td>\n","      <td>0.634</td>\n","      <td>0.342</td>\n","      <td>-8.345</td>\n","      <td>125.044</td>\n","      <td>0.00035</td>\n","      <td>0.697</td>\n","      <td>1.0</td>\n","      <td>6.0</td>\n","      <td>b2980c722a1ace7a30303718ce5491d8</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>2205686924</td>\n","      <td>da3110a77b724072b08f231c9d6f7534</td>\n","      <td>2014-01-01 05:54:22</td>\n","      <td>0.017700</td>\n","      <td>0.0638</td>\n","      <td>0.0624</td>\n","      <td>0.769</td>\n","      <td>0.752</td>\n","      <td>-8.252</td>\n","      <td>95.862</td>\n","      <td>0.26700</td>\n","      <td>0.826</td>\n","      <td>1.0</td>\n","      <td>7.0</td>\n","      <td>5cddcd0e314e2f2223ab21937d2c8778</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>132588395</td>\n","      <td>ba84d88c10fb0e42d4754a27ead10546</td>\n","      <td>2014-01-01 05:54:22</td>\n","      <td>0.000000</td>\n","      <td>0.0860</td>\n","      <td>0.0436</td>\n","      <td>0.675</td>\n","      <td>0.775</td>\n","      <td>-4.432</td>\n","      <td>97.030</td>\n","      <td>0.21700</td>\n","      <td>0.885</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>e41273f43af504714d85465294f1f369</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>97675221</td>\n","      <td>33f95122281f76e7134f9cbea3be980f</td>\n","      <td>2014-01-01 05:54:24</td>\n","      <td>0.000000</td>\n","      <td>0.1430</td>\n","      <td>0.0292</td>\n","      <td>0.324</td>\n","      <td>0.333</td>\n","      <td>-5.647</td>\n","      <td>74.101</td>\n","      <td>0.23900</td>\n","      <td>0.574</td>\n","      <td>1.0</td>\n","      <td>7.0</td>\n","      <td>557ce373bd29743eb00a3723ab19ebe8</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>17945688</td>\n","      <td>b5c42e81e15cd54b9b0ee34711dedf05</td>\n","      <td>2014-01-01 05:54:24</td>\n","      <td>0.000183</td>\n","      <td>0.3620</td>\n","      <td>0.0524</td>\n","      <td>0.767</td>\n","      <td>0.808</td>\n","      <td>-5.011</td>\n","      <td>114.237</td>\n","      <td>0.03640</td>\n","      <td>0.739</td>\n","      <td>0.0</td>\n","      <td>10.0</td>\n","      <td>77bd64b4bf77e10001fd02964985ae0f</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["      user_id  ...                         artist_id\n","0    81496937  ...  b2980c722a1ace7a30303718ce5491d8\n","1  2205686924  ...  5cddcd0e314e2f2223ab21937d2c8778\n","2   132588395  ...  e41273f43af504714d85465294f1f369\n","3    97675221  ...  557ce373bd29743eb00a3723ab19ebe8\n","4    17945688  ...  77bd64b4bf77e10001fd02964985ae0f\n","\n","[5 rows x 15 columns]"]},"metadata":{"tags":[]},"execution_count":14}]},{"cell_type":"code","metadata":{"id":"VHJuPW2_achi"},"source":["users = data[['user_id']].drop_duplicates()\n","tracks = data[['track_id'] + track_feature_cols].drop_duplicates()\n","assert tracks['track_id'].value_counts().max() == 1\n","tracks = tracks.astype({'mode': 'int64', 'key': 'int64', 'artist_id': 'category'})\n","events = data[['user_id', 'track_id', 'created_at']]\n","events['created_at'] = events['created_at'].values.astype('datetime64[s]').astype('int64')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":194},"id":"0_2dzxwMadpu","executionInfo":{"status":"ok","timestamp":1621243072439,"user_tz":-330,"elapsed":203425,"user":{"displayName":"Sparsh Agarwal","photoUrl":"","userId":"13037694610922482904"}},"outputId":"7b2a17ba-a04e-40aa-a037-9ae4036d0b2b"},"source":["events.head()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.module+javascript":"\n      import \"https://ssl.gstatic.com/colaboratory/data_table/a6224c040fa35dcf/data_table.js\";\n\n      window.createDataTable({\n        data: [[{\n            'v': 0,\n            'f': \"0\",\n        },\n{\n            'v': 81496937,\n            'f': \"81496937\",\n        },\n\"cd52b3e5b51da29e5893dba82a418a4b\",\n{\n            'v': 1388555661,\n            'f': \"1388555661\",\n        }],\n [{\n            'v': 1,\n            'f': \"1\",\n        },\n{\n            'v': 2205686924,\n            'f': \"2205686924\",\n        },\n\"da3110a77b724072b08f231c9d6f7534\",\n{\n            'v': 1388555662,\n            'f': \"1388555662\",\n        }],\n [{\n            'v': 2,\n            'f': \"2\",\n        },\n{\n            'v': 132588395,\n            'f': \"132588395\",\n        },\n\"ba84d88c10fb0e42d4754a27ead10546\",\n{\n            'v': 1388555662,\n            'f': \"1388555662\",\n        }],\n [{\n            'v': 3,\n            'f': \"3\",\n        },\n{\n            'v': 97675221,\n            'f': \"97675221\",\n        },\n\"33f95122281f76e7134f9cbea3be980f\",\n{\n            'v': 1388555664,\n            'f': \"1388555664\",\n        }],\n [{\n            'v': 4,\n            'f': \"4\",\n        },\n{\n            'v': 17945688,\n            'f': \"17945688\",\n        },\n\"b5c42e81e15cd54b9b0ee34711dedf05\",\n{\n            'v': 1388555664,\n            'f': \"1388555664\",\n        }]],\n        columns: [[\"number\", \"index\"], [\"number\", \"user_id\"], [\"string\", \"track_id\"], [\"number\", \"created_at\"]],\n        columnOptions: [{\"width\": \"1px\", \"className\": \"index_column\"}],\n        rowsPerPage: 25,\n        helpUrl: \"https://colab.research.google.com/notebooks/data_table.ipynb\",\n        suppressOutputScrolling: true,\n        minimumWidth: undefined,\n      });\n    ","text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>user_id</th>\n","      <th>track_id</th>\n","      <th>created_at</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>81496937</td>\n","      <td>cd52b3e5b51da29e5893dba82a418a4b</td>\n","      <td>1388555661</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>2205686924</td>\n","      <td>da3110a77b724072b08f231c9d6f7534</td>\n","      <td>1388555662</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>132588395</td>\n","      <td>ba84d88c10fb0e42d4754a27ead10546</td>\n","      <td>1388555662</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>97675221</td>\n","      <td>33f95122281f76e7134f9cbea3be980f</td>\n","      <td>1388555664</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>17945688</td>\n","      <td>b5c42e81e15cd54b9b0ee34711dedf05</td>\n","      <td>1388555664</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["      user_id                          track_id  created_at\n","0    81496937  cd52b3e5b51da29e5893dba82a418a4b  1388555661\n","1  2205686924  da3110a77b724072b08f231c9d6f7534  1388555662\n","2   132588395  ba84d88c10fb0e42d4754a27ead10546  1388555662\n","3    97675221  33f95122281f76e7134f9cbea3be980f  1388555664\n","4    17945688  b5c42e81e15cd54b9b0ee34711dedf05  1388555664"]},"metadata":{"tags":[]},"execution_count":16}]},{"cell_type":"code","metadata":{"id":"p6cjVytial-L"},"source":["graph_builder = PandasGraphBuilder()\n","graph_builder.add_entities(users, 'user_id', 'user')\n","graph_builder.add_entities(tracks, 'track_id', 'track')\n","graph_builder.add_binary_relations(events, 'user_id', 'track_id', 'listened')\n","graph_builder.add_binary_relations(events, 'track_id', 'user_id', 'listened-by')\n","\n","g = graph_builder.build()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"q5GiN3Cwaoc_"},"source":["float_cols = []\n","for col in tracks.columns:\n","    if col == 'track_id':\n","        continue\n","    elif col == 'artist_id':\n","        g.nodes['track'].data[col] = torch.LongTensor(tracks[col].cat.codes.values)\n","    elif tracks.dtypes[col] == 'float64':\n","        float_cols.append(col)\n","    else:\n","        g.nodes['track'].data[col] = torch.LongTensor(tracks[col].values)\n","g.nodes['track'].data['song_features'] = torch.FloatTensor(linear_normalize(tracks[float_cols].values))\n","g.edges['listened'].data['created_at'] = torch.LongTensor(events['created_at'].values)\n","g.edges['listened-by'].data['created_at'] = torch.LongTensor(events['created_at'].values)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"One7WKGNaqON","executionInfo":{"status":"ok","timestamp":1621243457360,"user_tz":-330,"elapsed":587617,"user":{"displayName":"Sparsh Agarwal","photoUrl":"","userId":"13037694610922482904"}},"outputId":"7263f632-025e-4856-dae2-300f56c50041"},"source":["n_edges = g.number_of_edges('listened')\n","train_indices, val_indices, test_indices = train_test_split_by_time(events, 'created_at', 'user_id')\n","train_g = build_train_graph(g, train_indices, 'user', 'track', 'listened', 'listened-by')\n","assert train_g.out_degrees(etype='listened').min() > 0\n","val_matrix, test_matrix = build_val_test_matrix(\n","    g, val_indices, test_indices, 'user', 'track', 'listened')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["           user_id                          track_id  ...  val_mask  test_mask\n","0         81496937  cd52b3e5b51da29e5893dba82a418a4b  ...     False      False\n","919       81496937  29cb3f8f366888158226c810b3fee372  ...     False      False\n","1079      81496937  f62b0e51fc59cca8af70942e12554765  ...     False      False\n","1543      81496937  d99853238d61833e3158a0fe76425ca2  ...     False      False\n","1661      81496937  977837bd00f5374b5d0586ba2538523c  ...     False      False\n","...            ...                               ...  ...       ...        ...\n","11613299  81496937  f15ae084e89942084df0f4f989247de9  ...     False      False\n","11613401  81496937  a5ad13a10a9a6ad7ba3784d185de4fec  ...     False      False\n","11613508  81496937  9b664ec984823d11e70287a5201ede35  ...     False      False\n","11613984  81496937  744c145d74cf4f65162a749bebda0db7  ...      True      False\n","11614273  81496937  c4eb7de403a7e55b968defca65398f78  ...     False       True\n","\n","[23737 rows x 6 columns]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"n8UYEBV6aFh7"},"source":["dataset = {\n","    'train-graph': train_g,\n","    'val-matrix': val_matrix,\n","    'test-matrix': test_matrix,\n","    'item-texts': {},\n","    'item-images': None,\n","    'user-type': 'user',\n","    'item-type': 'track',\n","    'user-to-item-type': 'listened',\n","    'item-to-user-type': 'listened-by',\n","    'timestamp-edge-column': 'created_at'}\n","\n","with open(output_path, 'wb') as f:\n","    pickle.dump(dataset, f)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AmkRQU-rIQZ6","executionInfo":{"status":"ok","timestamp":1621243460434,"user_tz":-330,"elapsed":590372,"user":{"displayName":"Sparsh Agarwal","photoUrl":"","userId":"13037694610922482904"}},"outputId":"7e7c7f51-3627-4c5b-c69e-17f88d281f47"},"source":["%%writefile evaluation.py\n","\n","import numpy as np\n","import torch\n","import pickle\n","import dgl\n","import argparse\n","\n","def prec(recommendations, ground_truth):\n","    n_users, n_items = ground_truth.shape\n","    K = recommendations.shape[1]\n","    user_idx = np.repeat(np.arange(n_users), K)\n","    item_idx = recommendations.flatten()\n","    relevance = ground_truth[user_idx, item_idx].reshape((n_users, K))\n","    hit = relevance.any(axis=1).mean()\n","    return hit\n","\n","class LatestNNRecommender(object):\n","    def __init__(self, user_ntype, item_ntype, user_to_item_etype, timestamp, batch_size):\n","        self.user_ntype = user_ntype\n","        self.item_ntype = item_ntype\n","        self.user_to_item_etype = user_to_item_etype\n","        self.batch_size = batch_size\n","        self.timestamp = timestamp\n","\n","    def recommend(self, full_graph, K, h_user, h_item):\n","        \"\"\"\n","        Return a (n_user, K) matrix of recommended items for each user\n","        \"\"\"\n","        graph_slice = full_graph.edge_type_subgraph([self.user_to_item_etype])\n","        n_users = full_graph.number_of_nodes(self.user_ntype)\n","        latest_interactions = dgl.sampling.select_topk(graph_slice, 1, self.timestamp, edge_dir='out')\n","        user, latest_items = latest_interactions.all_edges(form='uv', order='srcdst')\n","        # each user should have at least one \"latest\" interaction\n","        assert torch.equal(user, torch.arange(n_users))\n","\n","        recommended_batches = []\n","        user_batches = torch.arange(n_users).split(self.batch_size)\n","        for user_batch in user_batches:\n","            latest_item_batch = latest_items[user_batch].to(device=h_item.device)\n","            dist = h_item[latest_item_batch] @ h_item.t()\n","            # exclude items that are already interacted\n","            for i, u in enumerate(user_batch.tolist()):\n","                interacted_items = full_graph.successors(u, etype=self.user_to_item_etype)\n","                dist[i, interacted_items] = -np.inf\n","            recommended_batches.append(dist.topk(K, 1)[1])\n","\n","        recommendations = torch.cat(recommended_batches, 0)\n","        return recommendations\n","\n","\n","def evaluate_nn(dataset, h_item, k, batch_size):\n","    g = dataset['train-graph']\n","    val_matrix = dataset['val-matrix'].tocsr()\n","    test_matrix = dataset['test-matrix'].tocsr()\n","    item_texts = dataset['item-texts']\n","    user_ntype = dataset['user-type']\n","    item_ntype = dataset['item-type']\n","    user_to_item_etype = dataset['user-to-item-type']\n","    timestamp = dataset['timestamp-edge-column']\n","\n","    rec_engine = LatestNNRecommender(\n","        user_ntype, item_ntype, user_to_item_etype, timestamp, batch_size)\n","\n","    recommendations = rec_engine.recommend(g, k, None, h_item).cpu().numpy()\n","    return prec(recommendations, val_matrix)\n","\n","if __name__ == '__main__':\n","    parser = argparse.ArgumentParser()\n","    parser.add_argument('dataset_path', type=str)\n","    parser.add_argument('item_embedding_path', type=str)\n","    parser.add_argument('-k', type=int, default=10)\n","    parser.add_argument('--batch-size', type=int, default=32)\n","    args = parser.parse_args()\n","\n","    with open(args.dataset_path, 'rb') as f:\n","        dataset = pickle.load(f)\n","    with open(args.item_embedding_path, 'rb') as f:\n","        emb = torch.FloatTensor(pickle.load(f))\n","    print(evaluate_nn(dataset, emb, args.k, args.batch_size))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Writing evaluation.py\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3xmC2B58IQWf","executionInfo":{"status":"ok","timestamp":1621243460436,"user_tz":-330,"elapsed":590254,"user":{"displayName":"Sparsh Agarwal","photoUrl":"","userId":"13037694610922482904"}},"outputId":"d574d01f-e5c2-43a6-b209-9707fc602f21"},"source":["%%writefile layers.py\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import dgl\n","import dgl.nn.pytorch as dglnn\n","import dgl.function as fn\n","\n","def disable_grad(module):\n","    for param in module.parameters():\n","        param.requires_grad = False\n","\n","def _init_input_modules(g, ntype, textset, hidden_dims):\n","    # We initialize the linear projections of each input feature ``x`` as\n","    # follows:\n","    # * If ``x`` is a scalar integral feature, we assume that ``x`` is a categorical\n","    #   feature, and assume the range of ``x`` is 0..max(x).\n","    # * If ``x`` is a float one-dimensional feature, we assume that ``x`` is a\n","    #   numeric vector.\n","    # * If ``x`` is a field of a textset, we process it as bag of words.\n","    module_dict = nn.ModuleDict()\n","\n","    for column, data in g.nodes[ntype].data.items():\n","        if column == dgl.NID:\n","            continue\n","        if data.dtype == torch.float32:\n","            assert data.ndim == 2\n","            m = nn.Linear(data.shape[1], hidden_dims)\n","            nn.init.xavier_uniform_(m.weight)\n","            nn.init.constant_(m.bias, 0)\n","            module_dict[column] = m\n","        elif data.dtype == torch.int64:\n","            assert data.ndim == 1\n","            m = nn.Embedding(\n","                data.max() + 2, hidden_dims, padding_idx=-1)\n","            nn.init.xavier_uniform_(m.weight)\n","            module_dict[column] = m\n","\n","    if textset is not None:\n","        for column, field in textset.fields.items():\n","            if field.vocab.vectors:\n","                module_dict[column] = BagOfWordsPretrained(field, hidden_dims)\n","            else:\n","                module_dict[column] = BagOfWords(field, hidden_dims)\n","\n","    return module_dict\n","\n","class BagOfWordsPretrained(nn.Module):\n","    def __init__(self, field, hidden_dims):\n","        super().__init__()\n","\n","        input_dims = field.vocab.vectors.shape[1]\n","        self.emb = nn.Embedding(\n","            len(field.vocab.itos), input_dims,\n","            padding_idx=field.vocab.stoi[field.pad_token])\n","        self.emb.weight[:] = field.vocab.vectors\n","        self.proj = nn.Linear(input_dims, hidden_dims)\n","        nn.init.xavier_uniform_(self.proj.weight)\n","        nn.init.constant_(self.proj.bias, 0)\n","\n","        disable_grad(self.emb)\n","\n","    def forward(self, x, length):\n","        \"\"\"\n","        x: (batch_size, max_length) LongTensor\n","        length: (batch_size,) LongTensor\n","        \"\"\"\n","        x = self.emb(x).sum(1) / length.unsqueeze(1).float()\n","        return self.proj(x)\n","\n","class BagOfWords(nn.Module):\n","    def __init__(self, field, hidden_dims):\n","        super().__init__()\n","\n","        self.emb = nn.Embedding(\n","            len(field.vocab.itos), hidden_dims,\n","            padding_idx=field.vocab.stoi[field.pad_token])\n","        nn.init.xavier_uniform_(self.emb.weight)\n","\n","    def forward(self, x, length):\n","        return self.emb(x).sum(1) / length.unsqueeze(1).float()\n","\n","class LinearProjector(nn.Module):\n","    \"\"\"\n","    Projects each input feature of the graph linearly and sums them up\n","    \"\"\"\n","    def __init__(self, full_graph, ntype, textset, hidden_dims):\n","        super().__init__()\n","\n","        self.ntype = ntype\n","        self.inputs = _init_input_modules(full_graph, ntype, textset, hidden_dims)\n","\n","    def forward(self, ndata):\n","        projections = []\n","        for feature, data in ndata.items():\n","            if feature == dgl.NID or feature.endswith('__len'):\n","                # This is an additional feature indicating the length of the ``feature``\n","                # column; we shouldn't process this.\n","                continue\n","\n","            module = self.inputs[feature]\n","            if isinstance(module, (BagOfWords, BagOfWordsPretrained)):\n","                # Textual feature; find the length and pass it to the textual module.\n","                length = ndata[feature + '__len']\n","                result = module(data, length)\n","            else:\n","                result = module(data)\n","            projections.append(result)\n","\n","        return torch.stack(projections, 1).sum(1)\n","\n","class WeightedSAGEConv(nn.Module):\n","    def __init__(self, input_dims, hidden_dims, output_dims, act=F.relu):\n","        super().__init__()\n","\n","        self.act = act\n","        self.Q = nn.Linear(input_dims, hidden_dims)\n","        self.W = nn.Linear(input_dims + hidden_dims, output_dims)\n","        self.reset_parameters()\n","        self.dropout = nn.Dropout(0.5)\n","\n","    def reset_parameters(self):\n","        gain = nn.init.calculate_gain('relu')\n","        nn.init.xavier_uniform_(self.Q.weight, gain=gain)\n","        nn.init.xavier_uniform_(self.W.weight, gain=gain)\n","        nn.init.constant_(self.Q.bias, 0)\n","        nn.init.constant_(self.W.bias, 0)\n","\n","    def forward(self, g, h, weights):\n","        \"\"\"\n","        g : graph\n","        h : node features\n","        weights : scalar edge weights\n","        \"\"\"\n","        h_src, h_dst = h\n","        with g.local_scope():\n","            g.srcdata['n'] = self.act(self.Q(self.dropout(h_src)))\n","            g.edata['w'] = weights.float()\n","            g.update_all(fn.u_mul_e('n', 'w', 'm'), fn.sum('m', 'n'))\n","            g.update_all(fn.copy_e('w', 'm'), fn.sum('m', 'ws'))\n","            n = g.dstdata['n']\n","            ws = g.dstdata['ws'].unsqueeze(1).clamp(min=1)\n","            z = self.act(self.W(self.dropout(torch.cat([n / ws, h_dst], 1))))\n","            z_norm = z.norm(2, 1, keepdim=True)\n","            z_norm = torch.where(z_norm == 0, torch.tensor(1.).to(z_norm), z_norm)\n","            z = z / z_norm\n","            return z\n","\n","class SAGENet(nn.Module):\n","    def __init__(self, hidden_dims, n_layers):\n","        \"\"\"\n","        g : DGLHeteroGraph\n","            The user-item interaction graph.\n","            This is only for finding the range of categorical variables.\n","        item_textsets : torchtext.data.Dataset\n","            The textual features of each item node.\n","        \"\"\"\n","        super().__init__()\n","\n","        self.convs = nn.ModuleList()\n","        for _ in range(n_layers):\n","            self.convs.append(WeightedSAGEConv(hidden_dims, hidden_dims, hidden_dims))\n","\n","    def forward(self, blocks, h):\n","        for layer, block in zip(self.convs, blocks):\n","            h_dst = h[:block.number_of_nodes('DST/' + block.ntypes[0])]\n","            h = layer(block, (h, h_dst), block.edata['weights'])\n","        return h\n","\n","class ItemToItemScorer(nn.Module):\n","    def __init__(self, full_graph, ntype):\n","        super().__init__()\n","\n","        n_nodes = full_graph.number_of_nodes(ntype)\n","        self.bias = nn.Parameter(torch.zeros(n_nodes))\n","\n","    def _add_bias(self, edges):\n","        bias_src = self.bias[edges.src[dgl.NID]]\n","        bias_dst = self.bias[edges.dst[dgl.NID]]\n","        return {'s': edges.data['s'] + bias_src + bias_dst}\n","\n","    def forward(self, item_item_graph, h):\n","        \"\"\"\n","        item_item_graph : graph consists of edges connecting the pairs\n","        h : hidden state of every node\n","        \"\"\"\n","        with item_item_graph.local_scope():\n","            item_item_graph.ndata['h'] = h\n","            item_item_graph.apply_edges(fn.u_dot_v('h', 'h', 's'))\n","            item_item_graph.apply_edges(self._add_bias)\n","            pair_score = item_item_graph.edata['s']\n","        return pair_score"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Writing layers.py\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zkv9CEXiNMSR","executionInfo":{"status":"ok","timestamp":1621243460439,"user_tz":-330,"elapsed":589782,"user":{"displayName":"Sparsh Agarwal","photoUrl":"","userId":"13037694610922482904"}},"outputId":"2250b478-f498-43b4-95c8-9f678bd7dfd9"},"source":["%%writefile sampler.py\n","\n","import numpy as np\n","import dgl\n","import torch\n","from torch.utils.data import IterableDataset, DataLoader\n","\n","def compact_and_copy(frontier, seeds):\n","    block = dgl.to_block(frontier, seeds)\n","    for col, data in frontier.edata.items():\n","        if col == dgl.EID:\n","            continue\n","        block.edata[col] = data[block.edata[dgl.EID]]\n","    return block\n","\n","class ItemToItemBatchSampler(IterableDataset):\n","    def __init__(self, g, user_type, item_type, batch_size):\n","        self.g = g\n","        self.user_type = user_type\n","        self.item_type = item_type\n","        self.user_to_item_etype = list(g.metagraph()[user_type][item_type])[0]\n","        self.item_to_user_etype = list(g.metagraph()[item_type][user_type])[0]\n","        self.batch_size = batch_size\n","\n","    def __iter__(self):\n","        while True:\n","            heads = torch.randint(0, self.g.number_of_nodes(self.item_type), (self.batch_size,))\n","            tails = dgl.sampling.random_walk(\n","                self.g,\n","                heads,\n","                metapath=[self.item_to_user_etype, self.user_to_item_etype])[0][:, 2]\n","            neg_tails = torch.randint(0, self.g.number_of_nodes(self.item_type), (self.batch_size,))\n","\n","            mask = (tails != -1)\n","            yield heads[mask], tails[mask], neg_tails[mask]\n","\n","class NeighborSampler(object):\n","    def __init__(self, g, user_type, item_type, random_walk_length, random_walk_restart_prob,\n","                 num_random_walks, num_neighbors, num_layers):\n","        self.g = g\n","        self.user_type = user_type\n","        self.item_type = item_type\n","        self.user_to_item_etype = list(g.metagraph()[user_type][item_type])[0]\n","        self.item_to_user_etype = list(g.metagraph()[item_type][user_type])[0]\n","        self.samplers = [\n","            dgl.sampling.PinSAGESampler(g, item_type, user_type, random_walk_length,\n","                random_walk_restart_prob, num_random_walks, num_neighbors)\n","            for _ in range(num_layers)]\n","\n","    def sample_blocks(self, seeds, heads=None, tails=None, neg_tails=None):\n","        blocks = []\n","        for sampler in self.samplers:\n","            frontier = sampler(seeds)\n","            if heads is not None:\n","                eids = frontier.edge_ids(torch.cat([heads, heads]), torch.cat([tails, neg_tails]), return_uv=True)[2]\n","                if len(eids) > 0:\n","                    old_frontier = frontier\n","                    frontier = dgl.remove_edges(old_frontier, eids)\n","                    #print(old_frontier)\n","                    #print(frontier)\n","                    #print(frontier.edata['weights'])\n","                    #frontier.edata['weights'] = old_frontier.edata['weights'][frontier.edata[dgl.EID]]\n","            block = compact_and_copy(frontier, seeds)\n","            seeds = block.srcdata[dgl.NID]\n","            blocks.insert(0, block)\n","        return blocks\n","\n","    def sample_from_item_pairs(self, heads, tails, neg_tails):\n","        # Create a graph with positive connections only and another graph with negative\n","        # connections only.\n","        pos_graph = dgl.graph(\n","            (heads, tails),\n","            num_nodes=self.g.number_of_nodes(self.item_type))\n","        neg_graph = dgl.graph(\n","            (heads, neg_tails),\n","            num_nodes=self.g.number_of_nodes(self.item_type))\n","        pos_graph, neg_graph = dgl.compact_graphs([pos_graph, neg_graph])\n","        seeds = pos_graph.ndata[dgl.NID]\n","\n","        blocks = self.sample_blocks(seeds, heads, tails, neg_tails)\n","        return pos_graph, neg_graph, blocks\n","\n","def assign_simple_node_features(ndata, g, ntype, assign_id=False):\n","    \"\"\"\n","    Copies data to the given block from the corresponding nodes in the original graph.\n","    \"\"\"\n","    for col in g.nodes[ntype].data.keys():\n","        if not assign_id and col == dgl.NID:\n","            continue\n","        induced_nodes = ndata[dgl.NID]\n","        ndata[col] = g.nodes[ntype].data[col][induced_nodes]\n","\n","def assign_textual_node_features(ndata, textset, ntype):\n","    \"\"\"\n","    Assigns numericalized tokens from a torchtext dataset to given block.\n","\n","    The numericalized tokens would be stored in the block as node features\n","    with the same name as ``field_name``.\n","\n","    The length would be stored as another node feature with name\n","    ``field_name + '__len'``.\n","\n","    block : DGLHeteroGraph\n","        First element of the compacted blocks, with \"dgl.NID\" as the\n","        corresponding node ID in the original graph, hence the index to the\n","        text dataset.\n","\n","        The numericalized tokens (and lengths if available) would be stored\n","        onto the blocks as new node features.\n","    textset : torchtext.data.Dataset\n","        A torchtext dataset whose number of examples is the same as that\n","        of nodes in the original graph.\n","    \"\"\"\n","    node_ids = ndata[dgl.NID].numpy()\n","\n","    for field_name, field in textset.fields.items():\n","        examples = [getattr(textset[i], field_name) for i in node_ids]\n","\n","        tokens, lengths = field.process(examples)\n","\n","        if not field.batch_first:\n","            tokens = tokens.t()\n","\n","        ndata[field_name] = tokens\n","        ndata[field_name + '__len'] = lengths\n","\n","def assign_features_to_blocks(blocks, g, textset, ntype):\n","    # For the first block (which is closest to the input), copy the features from\n","    # the original graph as well as the texts.\n","    assign_simple_node_features(blocks[0].srcdata, g, ntype)\n","    assign_textual_node_features(blocks[0].srcdata, textset, ntype)\n","    assign_simple_node_features(blocks[-1].dstdata, g, ntype)\n","    assign_textual_node_features(blocks[-1].dstdata, textset, ntype)\n","\n","class PinSAGECollator(object):\n","    def __init__(self, sampler, g, ntype, textset):\n","        self.sampler = sampler\n","        self.ntype = ntype\n","        self.g = g\n","        self.textset = textset\n","\n","    def collate_train(self, batches):\n","        heads, tails, neg_tails = batches[0]\n","        # Construct multilayer neighborhood via PinSAGE...\n","        pos_graph, neg_graph, blocks = self.sampler.sample_from_item_pairs(heads, tails, neg_tails)\n","        assign_features_to_blocks(blocks, self.g, self.textset, self.ntype)\n","\n","        return pos_graph, neg_graph, blocks\n","\n","    def collate_test(self, samples):\n","        batch = torch.LongTensor(samples)\n","        blocks = self.sampler.sample_blocks(batch)\n","        assign_features_to_blocks(blocks, self.g, self.textset, self.ntype)\n","        return blocks"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Writing sampler.py\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"3n-wpT1VNWam"},"source":["import pickle\n","import argparse\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","from torch.utils.data import DataLoader\n","import torchtext\n","import dgl\n","import tqdm\n","\n","import layers\n","import sampler as sampler_module\n","import evaluation"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"dBZNZjVzNYtH"},"source":["class PinSAGEModel(nn.Module):\n","    def __init__(self, full_graph, ntype, textsets, hidden_dims, n_layers):\n","        super().__init__()\n","\n","        self.proj = layers.LinearProjector(full_graph, ntype, textsets, hidden_dims)\n","        self.sage = layers.SAGENet(hidden_dims, n_layers)\n","        self.scorer = layers.ItemToItemScorer(full_graph, ntype)\n","\n","    def forward(self, pos_graph, neg_graph, blocks):\n","        h_item = self.get_repr(blocks)\n","        pos_score = self.scorer(pos_graph, h_item)\n","        neg_score = self.scorer(neg_graph, h_item)\n","        return (neg_score - pos_score + 1).clamp(min=0)\n","\n","    def get_repr(self, blocks):\n","        h_item = self.proj(blocks[0].srcdata)\n","        h_item_dst = self.proj(blocks[-1].dstdata)\n","        return h_item_dst + self.sage(blocks, h_item)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"IZFUHO4blk4C"},"source":["device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"GX-PImUXNuZZ"},"source":["parser = argparse.ArgumentParser()\n","parser.add_argument('--dataset_path', type=str, default=output_path)\n","parser.add_argument('--random-walk-length', type=int, default=2)\n","parser.add_argument('--random-walk-restart-prob', type=float, default=0.5)\n","parser.add_argument('--num-random-walks', type=int, default=10)\n","parser.add_argument('--num-neighbors', type=int, default=3)\n","parser.add_argument('--num-layers', type=int, default=2)\n","parser.add_argument('--hidden-dims', type=int, default=16)\n","parser.add_argument('--batch-size', type=int, default=32)\n","parser.add_argument('--device', type=str, default=device)        # can also be \"cuda:0\"\n","parser.add_argument('--num-epochs', type=int, default=1)\n","parser.add_argument('--batches-per-epoch', type=int, default=20000)\n","parser.add_argument('--num-workers', type=int, default=0)\n","parser.add_argument('--lr', type=float, default=3e-5)\n","parser.add_argument('-k', type=int, default=10)\n","args, unknown = parser.parse_known_args()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Pt3zREy8Qtcl"},"source":["# Load dataset\n","with open(args.dataset_path, 'rb') as f:\n","    dataset = pickle.load(f)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"RO7FR1jnQ2ok"},"source":["g = dataset['train-graph']\n","val_matrix = dataset['val-matrix'].tocsr()\n","test_matrix = dataset['test-matrix'].tocsr()\n","item_texts = dataset['item-texts']\n","user_ntype = dataset['user-type']\n","item_ntype = dataset['item-type']\n","user_to_item_etype = dataset['user-to-item-type']\n","timestamp = dataset['timestamp-edge-column']"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"uPUJQ48eQ4rf"},"source":["device = torch.device(args.device)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"OXlNFiRcQ8N2"},"source":["# Assign user and movie IDs and use them as features (to learn an individual trainable\n","# embedding for each entity)\n","g.nodes[user_ntype].data['id'] = torch.arange(g.number_of_nodes(user_ntype))\n","g.nodes[item_ntype].data['id'] = torch.arange(g.number_of_nodes(item_ntype))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Ps7F5tajQ8JN"},"source":["# Prepare torchtext dataset and vocabulary\n","fields = {}\n","examples = []\n","for key, texts in item_texts.items():\n","    fields[key] = torchtext.legacy.data.Field(include_lengths=True, lower=True, batch_first=True)\n","for i in range(g.number_of_nodes(item_ntype)):\n","    example = torchtext.legacy.data.Example.fromlist(\n","        [item_texts[key][i] for key in item_texts.keys()],\n","        [(key, fields[key]) for key in item_texts.keys()])\n","    examples.append(example)\n","textset = torchtext.legacy.data.Dataset(examples, fields)\n","for key, field in fields.items():\n","    field.build_vocab(getattr(textset, key))\n","    #field.build_vocab(getattr(textset, key), vectors='fasttext.simple.300d')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2hSz7EwMNYoc"},"source":["# Sampler\n","batch_sampler = sampler_module.ItemToItemBatchSampler(g, user_ntype, item_ntype, args.batch_size)\n","neighbor_sampler = sampler_module.NeighborSampler(g, user_ntype, item_ntype, args.random_walk_length,\n","                                                  args.random_walk_restart_prob, args.num_random_walks,\n","                                                  args.num_neighbors, args.num_layers)\n","collator = sampler_module.PinSAGECollator(neighbor_sampler, g, item_ntype, textset)\n","dataloader = DataLoader(batch_sampler, collate_fn=collator.collate_train, num_workers=args.num_workers)\n","dataloader_test = DataLoader(torch.arange(g.number_of_nodes(item_ntype)), batch_size=args.batch_size,\n","                             collate_fn=collator.collate_test, num_workers=args.num_workers)\n","dataloader_it = iter(dataloader)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"B4r309MtNYim"},"source":["# Model\n","model = PinSAGEModel(g, item_ntype, textset, args.hidden_dims, args.num_layers).to(device)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"R7WgJFFsSI-R"},"source":["# Optimizer\n","opt = torch.optim.Adam(model.parameters(), lr=args.lr)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_NsYPL1bSseZ","executionInfo":{"status":"ok","timestamp":1621247042077,"user_tz":-330,"elapsed":4156985,"user":{"displayName":"Sparsh Agarwal","photoUrl":"","userId":"13037694610922482904"}},"outputId":"77d12dc9-a3e7-44f0-9308-fdecb5b4e2d8"},"source":["# For each batch of head-tail-negative triplets...\n","for epoch_id in range(args.num_epochs):\n","    model.train()\n","    for batch_id in tqdm.trange(args.batches_per_epoch):\n","        pos_graph, neg_graph, blocks = next(dataloader_it)\n","        # Copy to GPU\n","        for i in range(len(blocks)):\n","            blocks[i] = blocks[i].to(device)\n","        pos_graph = pos_graph.to(device)\n","        neg_graph = neg_graph.to(device)\n","\n","        loss = model(pos_graph, neg_graph, blocks).mean()\n","        opt.zero_grad()\n","        loss.backward()\n","        opt.step()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["100%|██████████| 20000/20000 [59:40<00:00,  5.59it/s]\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KSe9dRhQS0t_","executionInfo":{"status":"ok","timestamp":1621247784222,"user_tz":-330,"elapsed":624762,"user":{"displayName":"Sparsh Agarwal","photoUrl":"","userId":"13037694610922482904"}},"outputId":"c3be1fcb-fdca-497d-8dd1-7301b0a81c12"},"source":["# Evaluate HIT@10\n","model.eval()\n","with torch.no_grad():\n","    item_batches = torch.arange(g.number_of_nodes(item_ntype)).split(args.batch_size)\n","    h_item_batches = []\n","    for blocks in dataloader_test:\n","        for i in range(len(blocks)):\n","            blocks[i] = blocks[i].to(device)\n","\n","        h_item_batches.append(model.get_repr(blocks))\n","    h_item = torch.cat(h_item_batches, 0)\n","\n","    print(evaluation.evaluate_nn(dataset, h_item, args.k, args.batch_size))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["0.000526236114214863\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"xE3cBBjIVev-"},"source":["https://github.com/dmlc/dgl/tree/master/examples/pytorch/pinsage"]}]}