{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp evaluation.metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metrics\n",
    "> Metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev.showdoc import *\n",
    "from fastcore.nb_imports import *\n",
    "from fastcore.test import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from typing import List, Tuple\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "\n",
    "from recohut.utils.common_utils import remove_duplicates, count_a_in_b_unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def NDCG(true, pred):\n",
    "    match = pred.eq(true).nonzero(as_tuple=True)[1]\n",
    "    ncdg = torch.log(torch.Tensor([2])).div(torch.log(match + 2))\n",
    "    ncdg = ncdg.sum().div(pred.shape[0]).item()\n",
    "    return ncdg\n",
    "\n",
    "\n",
    "def APAK(true, pred):\n",
    "    k = pred.shape[1]\n",
    "    apak = pred.eq(true).div(torch.arange(k) + 1)\n",
    "    apak = apak.sum().div(pred.shape[0]).item()\n",
    "    return apak\n",
    "\n",
    "\n",
    "def HR(true, pred):\n",
    "    hr = pred.eq(true).sum().div(pred.shape[0]).item()\n",
    "    return hr\n",
    "\n",
    "\n",
    "def get_eval_metrics(scores, true, k=10):\n",
    "    test_items = [torch.LongTensor(list(item_scores.keys())) for item_scores in scores]\n",
    "    test_scores = [torch.Tensor(list(item_scores.values())) for item_scores in scores]\n",
    "    topk_indices = [s.topk(k).indices for s in test_scores]\n",
    "    topk_items = [item[idx] for item, idx in zip(test_items, topk_indices)]\n",
    "    pred = torch.vstack(topk_items)\n",
    "    ncdg = NDCG(true, pred)\n",
    "    apak = APAK(true, pred)\n",
    "    hr = HR(true, pred)\n",
    "\n",
    "    return ncdg, apak, hr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.4261859357357025, 0.36666667461395264, 0.6000000238418579)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores = [{1: 0.2, 2: 0.3, 3: 0.4, 4: 0.5, 9: 0.1},\n",
    "          {1: 0.2, 2: 0.3, 3: 0.4, 4: 0.5, 9: 0.1},\n",
    "          {1: 0.2, 2: 0.3, 3: 0.4, 4: 0.5, 9: 0.1},\n",
    "          {1: 0.2, 2: 0.3, 3: 0.4, 4: 0.5, 9: 0.1},\n",
    "          {1: 0.2, 2: 0.3, 3: 0.4, 4: 0.5, 9: 0.1}]\n",
    "\n",
    "true = torch.tensor([[1],[1],[2],[3],[4]])\n",
    "metric = get_eval_metrics(scores, true, k=3)\n",
    "metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.0, 1.0, 1.0)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# it should all 1, because all relevant items are in range k=3\n",
    "true = torch.tensor([[4],[4],[4],[4],[4]])\n",
    "metric = get_eval_metrics(scores, true, k=3)\n",
    "metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.0, 0.0, 0.0)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# it should all 0, because no relevant item is in range k=3\n",
    "true = torch.tensor([[9],[1],[9],[1],[1]])\n",
    "metric = get_eval_metrics(scores, true, k=3)\n",
    "metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def get_eval_metrics_v2(pred_list, topk=10):\n",
    "    NDCG = 0.0\n",
    "    HIT = 0.0\n",
    "    MRR = 0.0\n",
    "    for rank in pred_list:\n",
    "        if rank < topk:\n",
    "            MRR += 1.0 / (rank + 1.0)\n",
    "            NDCG += 1.0 / np.log2(rank + 2.0)\n",
    "            HIT += 1.0\n",
    "    return HIT /len(pred_list), NDCG /len(pred_list), MRR /len(pred_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_eq(np.round(get_eval_metrics_v2(pred_list = [1,3,2], topk=3), 2),\n",
    "        np.array([0.67, 0.38, 0.28]))\n",
    "test_eq(np.round(get_eval_metrics_v2(pred_list = [1,3,2], topk=2), 2),\n",
    "        np.array([0.33, 0.21, 0.17]))\n",
    "test_eq(np.round(get_eval_metrics_v2(pred_list = [0,0,0], topk=2), 2),\n",
    "        np.array([1., 1., 1.]))\n",
    "test_eq(np.round(get_eval_metrics_v2(pred_list = [3,3,3], topk=2), 2),\n",
    "        np.array([0., 0., 0.]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def precision_at_k_per_sample(actual, predicted, topk):\n",
    "    num_hits = 0\n",
    "    for place in predicted:\n",
    "        if place in actual:\n",
    "            num_hits += 1\n",
    "    return num_hits / (topk + 0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = [0,1,4]\n",
    "actual = [0,1,2,3]\n",
    "test_eq(np.round(precision_at_k_per_sample(actual, predicted, topk=2), 2),\n",
    "        np.array([1.]))\n",
    "test_eq(np.round(precision_at_k_per_sample(actual, predicted, topk=3), 2),\n",
    "        np.array([0.67]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def precision_at_k(actual, predicted, topk):\n",
    "    sum_precision = 0.0\n",
    "    num_users = len(predicted)\n",
    "    for i in range(num_users):\n",
    "        act_set = set(actual[i])\n",
    "        pred_set = set(predicted[i][:topk])\n",
    "        sum_precision += len(act_set & pred_set) / float(topk)\n",
    "\n",
    "    return sum_precision / num_users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = [[0,1,4], [1,3]]\n",
    "actual = [[0,1,2,3], [0,1,2]]\n",
    "test_eq(np.round(precision_at_k(actual, predicted, topk=2), 2),\n",
    "        np.array([0.75]))\n",
    "test_eq(np.round(precision_at_k(actual, predicted, topk=3), 2),\n",
    "        np.array([0.5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def ap_at_k(actual, predicted, topk=10):\n",
    "    \"\"\"\n",
    "    Computes the average precision at topk.\n",
    "    This function computes the average precision at topk between two lists of\n",
    "    items.\n",
    "    Parameters\n",
    "    ----------\n",
    "    actual : list\n",
    "             A list of elements that are to be predicted (order doesn't matter)\n",
    "    predicted : list\n",
    "                A list of predicted elements (order does matter)\n",
    "    topk : int, optional\n",
    "        The maximum number of predicted elements\n",
    "    Returns\n",
    "    -------\n",
    "    score : double\n",
    "            The average precision at topk over the input lists\n",
    "    \"\"\"\n",
    "    if len(predicted)>topk:\n",
    "        predicted = predicted[:topk]\n",
    "\n",
    "    score = 0.0\n",
    "    num_hits = 0.0\n",
    "\n",
    "    for i,p in enumerate(predicted):\n",
    "        if p in actual and p not in predicted[:i]:\n",
    "            num_hits += 1.0\n",
    "            score += num_hits / (i+1.0)\n",
    "\n",
    "    if not actual:\n",
    "        return 0.0\n",
    "\n",
    "    return score / min(len(actual), topk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = [0,1,4]\n",
    "actual = [0,1,2,3]\n",
    "test_eq(np.round(ap_at_k(actual, predicted, topk=2), 2),\n",
    "        np.array([1.]))\n",
    "test_eq(np.round(ap_at_k(actual, predicted, topk=3), 2),\n",
    "        np.array([0.67]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def map_at_k(actual, predicted, topk=10):\n",
    "    \"\"\"\n",
    "    Computes the mean average precision at topk.\n",
    "    This function computes the mean average prescision at topk between two lists\n",
    "    of lists of items.\n",
    "    Parameters\n",
    "    ----------\n",
    "    actual : list\n",
    "             A list of lists of elements that are to be predicted\n",
    "             (order doesn't matter in the lists)\n",
    "    predicted : list\n",
    "                A list of lists of predicted elements\n",
    "                (order matters in the lists)\n",
    "    topk : int, optional\n",
    "        The maximum number of predicted elements\n",
    "    Returns\n",
    "    -------\n",
    "    score : double\n",
    "            The mean average precision at topk over the input lists\n",
    "    \"\"\"\n",
    "    return np.mean([ap_at_k(a, p, topk) for a, p in zip(actual, predicted)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = [[0,1,4], [1,3]]\n",
    "actual = [[0,1,2,3], [0,1,2]]\n",
    "test_eq(np.round(map_at_k(actual, predicted, topk=2), 2),\n",
    "        np.array([0.75]))\n",
    "test_eq(np.round(map_at_k(actual, predicted, topk=3), 2),\n",
    "        np.array([0.5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def recall_at_k(actual, predicted, topk):\n",
    "    sum_recall = 0.0\n",
    "    num_users = len(predicted)\n",
    "    true_users = 0\n",
    "    recall_dict = {}\n",
    "    for i in range(num_users):\n",
    "        act_set = set(actual[i])\n",
    "        pred_set = set(predicted[i][:topk])\n",
    "        if len(act_set) != 0:\n",
    "            #sum_recall += len(act_set & pred_set) / float(len(act_set))\n",
    "            one_user_recall = len(act_set & pred_set) / float(len(act_set))\n",
    "            recall_dict[i] = one_user_recall\n",
    "            sum_recall += one_user_recall\n",
    "            true_users += 1\n",
    "    return sum_recall / true_users, recall_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = [[0,1,4], [1,3]]\n",
    "actual = [[0,1,2,3], [0,1,2]]\n",
    "test_eq(np.round(recall_at_k(actual, predicted, topk=2)[0], 2),\n",
    "        np.array([0.42]))\n",
    "test_eq(np.round(recall_at_k(actual, predicted, topk=3)[0], 2),\n",
    "        np.array([0.42]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def cal_mrr(actual, predicted):\n",
    "    sum_mrr = 0.\n",
    "    true_users = 0\n",
    "    num_users = len(predicted)\n",
    "    mrr_dict = {}\n",
    "    for i in range(num_users):\n",
    "        r = []\n",
    "        act_set = set(actual[i])\n",
    "        pred_list = predicted[i]\n",
    "        for item in pred_list:\n",
    "            if item in act_set:\n",
    "                r.append(1)\n",
    "            else:\n",
    "                r.append(0)\n",
    "        r = np.array(r)\n",
    "        if np.sum(r) > 0:\n",
    "            #sum_mrr += np.reciprocal(np.where(r==1)[0]+1, dtype=np.float)[0]\n",
    "            one_user_mrr = np.reciprocal(np.where(r==1)[0]+1, dtype=np.float)[0]\n",
    "            sum_mrr += one_user_mrr\n",
    "            true_users += 1\n",
    "            mrr_dict[i] = one_user_mrr\n",
    "        else:\n",
    "            mrr_dict[i] = 0.\n",
    "    return sum_mrr / len(predicted), mrr_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = [[0,1,4], [1,3]]\n",
    "actual = [[0,1], [0,1]]\n",
    "test_eq(np.round(cal_mrr(actual, predicted)[0], 2),\n",
    "        np.array([1.]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def ndcg_at_k(actual, predicted, topk):\n",
    "    res = 0\n",
    "    ndcg_dict = {}\n",
    "    for user_id in range(len(actual)):\n",
    "        k = min(topk, len(actual[user_id]))\n",
    "        # idcg = idcg_at_k(k)\n",
    "        res = sum([1.0/math.log(i+2, 2) for i in range(k)])\n",
    "        idcg = res if res else 1.0\n",
    "        dcg_k = sum([int(predicted[user_id][j] in\n",
    "                         set(actual[user_id])) / math.log(j+2, 2) for j in range(topk)])\n",
    "        res += dcg_k / idcg\n",
    "        ndcg_dict[user_id] = dcg_k / idcg\n",
    "    return res / float(len(actual)), ndcg_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = [[0,1,4]]\n",
    "actual = [[0,1,2,3]]\n",
    "test_eq(np.round(ndcg_at_k(actual, predicted, topk=2)[0], 2),\n",
    "        np.array([2.63]))\n",
    "test_eq(np.round(ndcg_at_k(actual, predicted, topk=3)[0], 2),\n",
    "        np.array([2.9]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def precision(ground_truth, prediction):\n",
    "    \"\"\"\n",
    "    Compute Precision metric\n",
    "    :param ground_truth: the ground truth set or sequence\n",
    "    :param prediction: the predicted set or sequence\n",
    "    :return: the value of the metric\n",
    "    \"\"\"\n",
    "    ground_truth = remove_duplicates(ground_truth)\n",
    "    prediction = remove_duplicates(prediction)\n",
    "    precision_score = count_a_in_b_unique(prediction, ground_truth) / float(len(prediction))\n",
    "    assert 0 <= precision_score <= 1\n",
    "    return precision_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truth = [[1],[3],[4],[8],[9]]\n",
    "prediction = [[1],[4],[5],[9]]\n",
    "\n",
    "test_eq(precision(ground_truth, prediction), 0.75)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def recall(ground_truth, prediction):\n",
    "    \"\"\"\n",
    "    Compute Recall metric\n",
    "    :param ground_truth: the ground truth set or sequence\n",
    "    :param prediction: the predicted set or sequence\n",
    "    :return: the value of the metric\n",
    "    \"\"\"\n",
    "    ground_truth = remove_duplicates(ground_truth)\n",
    "    prediction = remove_duplicates(prediction)\n",
    "    recall_score = 0 if len(prediction) == 0 else count_a_in_b_unique(prediction, ground_truth) / float(\n",
    "        len(ground_truth))\n",
    "    assert 0 <= recall_score <= 1\n",
    "    return recall_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truth = [[1],[3],[4],[8],[9]]\n",
    "prediction = [[1],[4],[5],[9]]\n",
    "\n",
    "test_eq(recall(ground_truth, prediction), 0.6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## mrr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def mrr(ground_truth, prediction):\n",
    "    \"\"\"\n",
    "    Compute Mean Reciprocal Rank metric. Reciprocal Rank is set 0 if no predicted item is in contained the ground truth.\n",
    "    :param ground_truth: the ground truth set or sequence\n",
    "    :param prediction: the predicted set or sequence\n",
    "    :return: the value of the metric\n",
    "    \"\"\"\n",
    "    rr = 0.\n",
    "    for rank, p in enumerate(prediction):\n",
    "        if p in ground_truth:\n",
    "            rr = 1. / (rank + 1)\n",
    "            break\n",
    "    return rr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truth = [[1],[3],[4],[8],[9]]\n",
    "\n",
    "prediction = [[1],[4],[5],[9]]\n",
    "test_eq(mrr(ground_truth, prediction), 1.)\n",
    "\n",
    "prediction = [[5],[1],[4],[9]]\n",
    "test_eq(mrr(ground_truth, prediction), 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## novelty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def novelty(predictions: List[list], \n",
    "            train_df: pd.DataFrame, \n",
    "            user_col: str = 'user_id', \n",
    "            item_col: str = 'item_id') -> Tuple[float, List[Tuple[float, float]]]:\n",
    "    pop = train_df[item_col].value_counts().to_dict()\n",
    "    u = train_df[user_col].nunique() # number of users in the training data\n",
    "    n = max(map(len, predictions)) # length of recommended lists per user\n",
    "    mean_self_information = []\n",
    "    k = 0\n",
    "    for sublist in predictions:\n",
    "        self_information = 0\n",
    "        k += 1\n",
    "        for i in sublist:\n",
    "            self_information += np.sum(-np.log2(pop[i]/u))\n",
    "        mean_self_information.append(self_information/n)\n",
    "    novelty = sum(mean_self_information)/k\n",
    "    return novelty, mean_self_information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <div id=\"df-15cc19d1-dbf9-4779-98fe-79c5506c0e4f\">\n",
       "    <div class=\"colab-df-container\">\n",
       "      <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>song_id</th>\n",
       "      <th>user_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>16</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>17</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>18</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>60</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>61</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-15cc19d1-dbf9-4779-98fe-79c5506c0e4f')\"\n",
       "              title=\"Convert this dataframe to an interactive table.\"\n",
       "              style=\"display:none;\">\n",
       "        \n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "       width=\"24px\">\n",
       "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
       "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
       "  </svg>\n",
       "      </button>\n",
       "      \n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      flex-wrap:wrap;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "      <script>\n",
       "        const buttonEl =\n",
       "          document.querySelector('#df-15cc19d1-dbf9-4779-98fe-79c5506c0e4f button.colab-df-convert');\n",
       "        buttonEl.style.display =\n",
       "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "        async function convertToInteractive(key) {\n",
       "          const element = document.querySelector('#df-15cc19d1-dbf9-4779-98fe-79c5506c0e4f');\n",
       "          const dataTable =\n",
       "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                     [key], {});\n",
       "          if (!dataTable) return;\n",
       "\n",
       "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "            + ' to learn more about interactive tables.';\n",
       "          element.innerHTML = '';\n",
       "          dataTable['output_type'] = 'display_data';\n",
       "          await google.colab.output.renderOutput(dataTable, element);\n",
       "          const docLink = document.createElement('div');\n",
       "          docLink.innerHTML = docLinkHtml;\n",
       "          element.appendChild(docLink);\n",
       "        }\n",
       "      </script>\n",
       "    </div>\n",
       "  </div>\n",
       "  "
      ],
      "text/plain": [
       "  song_id user_id\n",
       "0      16       4\n",
       "1      17       4\n",
       "2      18       4\n",
       "3      60      10\n",
       "4      61      10"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_df = pd.DataFrame({\n",
    "    'song_id': {0: '16', 1: '17', 2: '18', 3: '60', 4: '61'},\n",
    "    'user_id': {0: '4', 1: '4', 2: '4', 3: '10', 4: '10'}\n",
    "    })\n",
    "_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.8333333333333333, [1.0, 0.6666666666666666])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = [['16','17','18'],['16','60']]\n",
    "print(novelty(predictions, _df, item_col='song_id'))\n",
    "test_eq(novelty(predictions, _df, item_col='song_id')[0].round(2), 0.83)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## coverage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def coverage(predictions: List[list], \n",
    "             train_df: pd.DataFrame,\n",
    "             item_col: str = 'item_id') -> float:\n",
    "    catalog = train_df[item_col].unique().tolist() # list of items in the training data\n",
    "    predictions_flattened = [p for sublist in predictions for p in sublist]\n",
    "    unique_predictions = len(set(predictions_flattened))\n",
    "    prediction_coverage = round(unique_predictions/(len(catalog)* 1.0)*100,2)\n",
    "    return prediction_coverage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "80.0"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = [['16','17','18'],['16','60']]\n",
    "test_eq(coverage(predictions, _df, item_col='song_id'), 80.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **References:-**\n",
    "- https://github.com/massquantity/DBRL/blob/master/dbrl/evaluate/metrics.py\n",
    "- [https://github.com/NVIDIA-Merlin/Transformers4Rec/blob/main/transformers4rec/torch/ranking_metric.py](https://github.com/NVIDIA-Merlin/Transformers4Rec/blob/main/transformers4rec/torch/ranking_metric.py)\n",
    "- [https://github.com/karlhigley/ranking-metrics-torch](https://github.com/karlhigley/ranking-metrics-torch)\n",
    "- [https://github.com/mquad/sars_tutorial/blob/master/util/metrics.py](https://github.com/mquad/sars_tutorial/blob/master/util/metrics.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Author: Sparsh A.\n",
      "\n",
      "Last updated: 2022-01-06 09:02:26\n",
      "\n",
      "recohut: 0.0.9\n",
      "\n",
      "Compiler    : GCC 7.5.0\n",
      "OS          : Linux\n",
      "Release     : 5.4.144+\n",
      "Machine     : x86_64\n",
      "Processor   : x86_64\n",
      "CPU cores   : 2\n",
      "Architecture: 64bit\n",
      "\n",
      "torchmetrics: 0.6.2\n",
      "numpy       : 1.19.5\n",
      "torch       : 1.10.0+cu111\n",
      "PIL         : 7.1.2\n",
      "matplotlib  : 3.2.2\n",
      "IPython     : 5.5.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "%reload_ext watermark\n",
    "%watermark -a \"Sparsh A.\" -m -iv -u -t -d -p recohut"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
