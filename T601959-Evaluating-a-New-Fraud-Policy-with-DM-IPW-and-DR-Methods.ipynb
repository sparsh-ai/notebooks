{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"T601959 | Evaluating a New Fraud Policy with DM, IPW, and DR Methods","provenance":[],"collapsed_sections":[],"toc_visible":true,"authorship_tag":"ABX9TyOzW2W3zUjW/cst/Y6RoYSh"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"PEXQVw-snLo0"},"source":["# Evaluating a New Fraud Policy with DM, IPW, and DR Methods"]},{"cell_type":"markdown","metadata":{"id":"V93SrzBK3Exx"},"source":["## IPS"]},{"cell_type":"code","metadata":{"id":"FMAnagid3SmI"},"source":["from typing import Callable, Dict, List\n","import pandas as pd\n","import statistics\n","\n","\n","P95_Z_SCORE = 1.96\n","\n","\n","def compute_list_stats(input: List):\n","    \"\"\"Compute mean and P95 CI of mean for a list of floats.\"\"\"\n","    mean = statistics.mean(input)\n","    std_dev = statistics.stdev(input) if len(input) > 1 else None\n","    ci_low = round(mean - P95_Z_SCORE * std_dev, 2) if std_dev else None\n","    ci_high = round(mean + P95_Z_SCORE * std_dev, 2) if std_dev else None\n","\n","    return {\"mean\": round(mean, 2), \"ci_low\": ci_low, \"ci_high\": ci_high}\n","\n","\n","def evaluate(\n","    df: pd.DataFrame, action_prob_function: Callable, num_bootstrap_samples: int = 0\n",") -> Dict[str, Dict[str, float]]:\n","\n","    results = [\n","        evaluate_raw(df, action_prob_function, sample=True)\n","        for _ in range(num_bootstrap_samples)\n","    ]\n","\n","    if not results:\n","        results = [evaluate_raw(df, action_prob_function, sample=False)]\n","\n","    logging_policy_rewards = [result[\"logging_policy\"] for result in results]\n","    new_policy_rewards = [result[\"new_policy\"] for result in results]\n","\n","    return {\n","        \"expected_reward_logging_policy\": compute_list_stats(logging_policy_rewards),\n","        \"expected_reward_new_policy\": compute_list_stats(new_policy_rewards),\n","    }\n","\n","\n","def evaluate_raw(\n","    df: pd.DataFrame, action_prob_function: Callable, sample: bool\n",") -> Dict[str, float]:\n","\n","    tmp_df = df.sample(df.shape[0], replace=True) if sample else df\n","\n","    cum_reward_new_policy = 0\n","    for _, row in tmp_df.iterrows():\n","        action_probabilities = action_prob_function(row[\"context\"])\n","        cum_reward_new_policy += (\n","            action_probabilities[row[\"action\"]] / row[\"action_prob\"]\n","        ) * row[\"reward\"]\n","\n","    return {\n","        \"logging_policy\": tmp_df.reward.sum() / len(tmp_df),\n","        \"new_policy\": cum_reward_new_policy / len(tmp_df),\n","    }"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VHRaYu-q3Ghn"},"source":["### Scenario"]},{"cell_type":"markdown","metadata":{"id":"bddlO29L0Cn9"},"source":["Assume we have a fraud model in production that blocks transactions if the P(fraud) > 0.05.\n","\n","Let's build some sample logs from that policy running in production. One thing to note, we need some basic exploration in the production logs (e.g. epsilon-greedy w/Îµ = 0.1). That is, 10% of the time we take a random action. Rewards represent revenue gained from allowing the transaction. A negative reward indicates the transaction was fraud and resulted in a chargeback."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":175},"id":"8X8VoM5g005u","executionInfo":{"status":"ok","timestamp":1632587985909,"user_tz":-330,"elapsed":615,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}},"outputId":"fd6607eb-c4c4-44cd-f26b-c7c364bac579"},"source":["import pandas as pd\n","\n","logs_df = pd.DataFrame([\n","    {\"context\": {\"p_fraud\": 0.08}, \"action\": \"blocked\", \"action_prob\": 0.90, \"reward\": 0},\n","    {\"context\": {\"p_fraud\": 0.03}, \"action\": \"allowed\", \"action_prob\": 0.90, \"reward\": 20},\n","    {\"context\": {\"p_fraud\": 0.01}, \"action\": \"allowed\", \"action_prob\": 0.90, \"reward\": 10},    \n","    {\"context\": {\"p_fraud\": 0.09}, \"action\": \"allowed\", \"action_prob\": 0.10, \"reward\": -20}, # only allowed due to exploration \n","])\n","\n","logs_df"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>context</th>\n","      <th>action</th>\n","      <th>action_prob</th>\n","      <th>reward</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>{'p_fraud': 0.08}</td>\n","      <td>blocked</td>\n","      <td>0.9</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>{'p_fraud': 0.03}</td>\n","      <td>allowed</td>\n","      <td>0.9</td>\n","      <td>20</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>{'p_fraud': 0.01}</td>\n","      <td>allowed</td>\n","      <td>0.9</td>\n","      <td>10</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>{'p_fraud': 0.09}</td>\n","      <td>allowed</td>\n","      <td>0.1</td>\n","      <td>-20</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["             context   action  action_prob  reward\n","0  {'p_fraud': 0.08}  blocked          0.9       0\n","1  {'p_fraud': 0.03}  allowed          0.9      20\n","2  {'p_fraud': 0.01}  allowed          0.9      10\n","3  {'p_fraud': 0.09}  allowed          0.1     -20"]},"metadata":{},"execution_count":2}]},{"cell_type":"markdown","metadata":{"id":"e3Ef6Seb033L"},"source":["Now let's use IPS to score a more lenient fraud model that blocks transactions only if the P(fraud) > 0.10.\n","\n","IPS requires that we know P(action | context) for the new policy. We can easily describe our new policy:"]},{"cell_type":"code","metadata":{"id":"IpDL7Rr51YXS"},"source":["def action_probabilities(context):\n","    epsilon = 0.10\n","    if context[\"p_fraud\"] > 0.10:\n","        return {\"allowed\": epsilon, \"blocked\": 1 - epsilon}    \n","    \n","    return {\"allowed\": 1 - epsilon, \"blocked\": epsilon}"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Xnnc9EmI1bfa"},"source":["We can now get the probability that the new policy takes the same action that was taken in the production logs above."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":175},"id":"h9eiFqfl2MJI","executionInfo":{"status":"ok","timestamp":1632588323194,"user_tz":-330,"elapsed":770,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}},"outputId":"4af191f3-5f2f-452b-8264-b02703dbbaf8"},"source":["logs_df[\"new_action_prob\"] = logs_df.apply(\n","    lambda row: action_probabilities(row[\"context\"])[row[\"action\"]],\n","    axis=1\n",")\n","logs_df"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>context</th>\n","      <th>action</th>\n","      <th>action_prob</th>\n","      <th>reward</th>\n","      <th>new_action_prob</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>{'p_fraud': 0.08}</td>\n","      <td>blocked</td>\n","      <td>0.9</td>\n","      <td>0</td>\n","      <td>0.1</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>{'p_fraud': 0.03}</td>\n","      <td>allowed</td>\n","      <td>0.9</td>\n","      <td>20</td>\n","      <td>0.9</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>{'p_fraud': 0.01}</td>\n","      <td>allowed</td>\n","      <td>0.9</td>\n","      <td>10</td>\n","      <td>0.9</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>{'p_fraud': 0.09}</td>\n","      <td>allowed</td>\n","      <td>0.1</td>\n","      <td>-20</td>\n","      <td>0.9</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["             context   action  action_prob  reward  new_action_prob\n","0  {'p_fraud': 0.08}  blocked          0.9       0              0.1\n","1  {'p_fraud': 0.03}  allowed          0.9      20              0.9\n","2  {'p_fraud': 0.01}  allowed          0.9      10              0.9\n","3  {'p_fraud': 0.09}  allowed          0.1     -20              0.9"]},"metadata":{},"execution_count":4}]},{"cell_type":"markdown","metadata":{"id":"QjIYi90P2NMh"},"source":["We see that the new policy lets through a fraud example (row: 3) at a much higher probability. This should make the new model get penalized in offline evaluation. We also see that for row: 0, the new model has a 90% chance of allowing the transaction, but we don't have the counterfactual knowledge of whether or not this would have been a non-fraud transaction since in production this transaction was blocked. This demonstrates one of the drawbacks of offline policy evaluation, but with more data we'd ideally see a different action taken in the same situation (due to exploration)."]},{"cell_type":"markdown","metadata":{"id":"CRssjM_a2ssR"},"source":["Now we will score the new model using IPS:"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aRjUmUWC2vRd","executionInfo":{"status":"ok","timestamp":1632588917379,"user_tz":-330,"elapsed":772,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}},"outputId":"22993b72-4df7-4b3a-b608-67194ea61044"},"source":["evaluate(logs_df, action_probabilities, num_bootstrap_samples=100)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'expected_reward_logging_policy': {'ci_high': 17.59,\n","  'ci_low': -11.19,\n","  'mean': 3.2},\n"," 'expected_reward_new_policy': {'ci_high': 46.68,\n","  'ci_low': -109.88,\n","  'mean': -31.6}}"]},"metadata":{},"execution_count":7}]},{"cell_type":"markdown","metadata":{"id":"5RSi1iVB4eNJ"},"source":["The expected reward per observation for the new policy is much worse than the logging policy (due to the observation that allowed fraud to go through (row: 3)) so we wouldn't roll out this new policy into an A/B test or production and instead should test some different policies offline.\n","\n","However, the confidence intervals around the expected rewards for our old and new policies overlap. If we want to be really certain, it's might be best to gather some more data to ensure the difference is signal and not noise. In this case, fortunately, we have strong reason to suspect the new policy is worse, but these confidence intervals can be important in cases where we have less prior certainty."]},{"cell_type":"markdown","metadata":{"id":"51nqgdt-4qLQ"},"source":["## DM"]},{"cell_type":"code","metadata":{"id":"KuBPMOyS4yTw"},"source":["from typing import NoReturn, Tuple, Dict, Callable\n","\n","import numpy as np\n","import pandas as pd\n","from sklearn.preprocessing import OneHotEncoder\n","from sklearn import ensemble\n","from sklearn.metrics import accuracy_score, mean_squared_error\n","from sklearn.linear_model import Ridge\n","\n","\n","def fit_gbdt_regression(\n","    X_train: np.ndarray,\n","    y_train: np.ndarray,\n","    X_test: np.ndarray = None,\n","    y_test: np.ndarray = None,\n",") -> Dict:\n","    \"\"\"Off the shelf sklearn GBDT regressor.\"\"\"\n","\n","    clf = ensemble.GradientBoostingRegressor()\n","    clf.fit(X_train, y_train)\n","\n","    mse_train = mean_squared_error(y_train, clf.predict(X_train))\n","\n","    mse_test = None\n","    if X_test and y_test:\n","        mse_test = mean_squared_error(y_test, clf.predict(X_test))\n","\n","    return {\"model\": clf, \"mse_train\": mse_train, \"mse_test\": mse_test}\n","\n","\n","def fit_gbdt_classification(\n","    X_train: np.ndarray,\n","    y_train: np.ndarray,\n","    X_test: np.ndarray = None,\n","    y_test: np.ndarray = None,\n",") -> Dict:\n","    \"\"\"Off the shelf sklearn GBDT classifier.\"\"\"\n","\n","    clf = ensemble.GradientBoostingClassifier()\n","    clf.fit(X_train, y_train)\n","\n","    acc_train = accuracy_score(y_train, clf.predict(X_train))\n","\n","    acc_test = None\n","    if X_test and y_test:\n","        acc_test = accuracy_score(y_test, clf.predict(X_test))\n","\n","    return {\"model\": clf, \"acc_train\": acc_train, \"acc_test\": acc_test}\n","\n","\n","def fit_ridge_regression(\n","    X_train: np.ndarray,\n","    y_train: np.ndarray,\n","    X_test: np.ndarray = None,\n","    y_test: np.ndarray = None,\n",") -> Dict:\n","    \"\"\"Off the shelf sklearn Ridge regression.\"\"\"\n","\n","    clf = Ridge()\n","    clf.fit(X_train, y_train)\n","\n","    mse_train = mean_squared_error(y_train, clf.predict(X_train))\n","\n","    mse_test = None\n","    if X_test and y_test:\n","        mse_test = mean_squared_error(y_test, clf.predict(X_test))\n","\n","    return {\"model\": clf, \"mse_train\": mse_train, \"mse_test\": mse_test}\n","\n","\n","def fit_ridge_classification(\n","    X_train: np.ndarray,\n","    y_train: np.ndarray,\n","    X_test: np.ndarray = None,\n","    y_test: np.ndarray = None,\n",") -> Dict:\n","    \"\"\"Off the shelf sklearn Ridge classifier.\"\"\"\n","\n","    clf = RidgeClassifier()\n","    clf.fit(X_train, y_train)\n","\n","    acc_train = accuracy_score(y_train, clf.predict(X_train))\n","\n","    acc_test = None\n","    if X_test and y_test:\n","        acc_test = accuracy_score(y_test, clf.predict(X_test))\n","\n","    return {\"model\": clf, \"acc_train\": acc_train, \"acc_test\": acc_test}\n","\n","\n","class Predictor:\n","    def _preprocess_data(self, df: pd.DataFrame) -> Tuple[np.ndarray, np.ndarray]:\n","        # preprocess context\n","        context_df = df.context.apply(pd.Series)\n","        self.context_column_order = list(context_df.columns)\n","\n","        # preprocess actions\n","        self.action_preprocessor = OneHotEncoder(sparse=False)\n","        action_values = df.action.values.reshape(-1, 1)\n","        self.possible_actions = set(action_values.squeeze().tolist())\n","        one_hot_action_values = self.action_preprocessor.fit_transform(action_values)\n","\n","        X_train = np.concatenate((context_df.values, one_hot_action_values), axis=1)\n","        y_train = df.reward.values\n","\n","        return X_train, y_train\n","\n","    def fit(self, df: pd.DataFrame) -> NoReturn:\n","        X_train, y_train = self._preprocess_data(df)\n","        results = fit_gbdt_regression(X_train, y_train)\n","        self.model = results.pop(\"model\")\n","        self.training_stats = results\n","\n","    def predict(self, input: np.ndarray) -> float:\n","        return self.model.predict(input)[0]\n","\n","\n","def evaluate(\n","    df: pd.DataFrame, action_prob_function: Callable, num_bootstrap_samples: int = 0\n",") -> Dict[str, Dict[str, float]]:\n","\n","    # train a model that predicts reward given (context, action)\n","    reward_model = Predictor()\n","    reward_model.fit(df)\n","\n","    results = [\n","        evaluate_raw(df, action_prob_function, sample=True, reward_model=reward_model)\n","        for _ in range(num_bootstrap_samples)\n","    ]\n","\n","    if not results:\n","        results = [\n","            evaluate_raw(\n","                df, action_prob_function, sample=False, reward_model=reward_model\n","            )\n","        ]\n","\n","    logging_policy_rewards = [result[\"logging_policy\"] for result in results]\n","    new_policy_rewards = [result[\"new_policy\"] for result in results]\n","\n","    return {\n","        \"expected_reward_logging_policy\": compute_list_stats(logging_policy_rewards),\n","        \"expected_reward_new_policy\": compute_list_stats(new_policy_rewards),\n","    }\n","\n","\n","def evaluate_raw(\n","    df: pd.DataFrame,\n","    action_prob_function: Callable,\n","    sample: bool,\n","    reward_model: Predictor,\n",") -> Dict[str, float]:\n","\n","    tmp_df = df.sample(df.shape[0], replace=True) if sample else df\n","\n","    context_df = tmp_df.context.apply(pd.Series)\n","    context_array = context_df[reward_model.context_column_order].values\n","    cum_reward_new_policy = 0\n","\n","    for idx, row in tmp_df.iterrows():\n","        observation_expected_reward = 0\n","        action_probabilities = action_prob_function(row[\"context\"])\n","        for action, action_probability in action_probabilities.items():\n","            one_hot_action = reward_model.action_preprocessor.transform(\n","                np.array(action).reshape(-1, 1)\n","            )\n","            observation = np.concatenate((context_array[idx], one_hot_action.squeeze()))\n","            predicted_reward = reward_model.predict(observation.reshape(1, -1))\n","            observation_expected_reward += action_probability * predicted_reward\n","        cum_reward_new_policy += observation_expected_reward\n","\n","    return {\n","        \"logging_policy\": tmp_df.reward.sum() / len(tmp_df),\n","        \"new_policy\": cum_reward_new_policy / len(tmp_df),\n","    }"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"PRqNkkRA4w2o"},"source":["### Scenario"]},{"cell_type":"markdown","metadata":{"id":"3j7dWVMK-pRV"},"source":["Assume we have a fraud model in production that blocks transactions if the P(fraud) > 0.05.\n","\n","Let's build some sample logs from that policy running in production. One thing to note, we need some basic exploration in the production logs (e.g. epsilon-greedy w/Îµ = 0.1). That is, 10% of the time we take a random action. Rewards represent revenue gained from allowing the transaction. A negative reward indicates the transaction was fraud and resulted in a chargeback."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":238},"id":"Vh4ybGYJ-aOQ","executionInfo":{"status":"ok","timestamp":1632590505641,"user_tz":-330,"elapsed":917,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}},"outputId":"17ee1273-e949-4a04-be3e-705da5c5ed40"},"source":["logs_df = pd.DataFrame([\n","    {\"context\": {\"p_fraud\": 0.08}, \"action\": \"blocked\", \"action_prob\": 0.90, \"reward\": 0},\n","    {\"context\": {\"p_fraud\": 0.03}, \"action\": \"allowed\", \"action_prob\": 0.90, \"reward\": 20},\n","    {\"context\": {\"p_fraud\": 0.02}, \"action\": \"allowed\", \"action_prob\": 0.90, \"reward\": 10}, \n","    {\"context\": {\"p_fraud\": 0.01}, \"action\": \"allowed\", \"action_prob\": 0.90, \"reward\": 20},     \n","    {\"context\": {\"p_fraud\": 0.09}, \"action\": \"allowed\", \"action_prob\": 0.10, \"reward\": -20}, # only allowed due to exploration \n","    {\"context\": {\"p_fraud\": 0.40}, \"action\": \"allowed\", \"action_prob\": 0.10, \"reward\": -10}, # only allowed due to exploration     \n","])\n","\n","logs_df"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>context</th>\n","      <th>action</th>\n","      <th>action_prob</th>\n","      <th>reward</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>{'p_fraud': 0.08}</td>\n","      <td>blocked</td>\n","      <td>0.9</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>{'p_fraud': 0.03}</td>\n","      <td>allowed</td>\n","      <td>0.9</td>\n","      <td>20</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>{'p_fraud': 0.02}</td>\n","      <td>allowed</td>\n","      <td>0.9</td>\n","      <td>10</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>{'p_fraud': 0.01}</td>\n","      <td>allowed</td>\n","      <td>0.9</td>\n","      <td>20</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>{'p_fraud': 0.09}</td>\n","      <td>allowed</td>\n","      <td>0.1</td>\n","      <td>-20</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>{'p_fraud': 0.4}</td>\n","      <td>allowed</td>\n","      <td>0.1</td>\n","      <td>-10</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["             context   action  action_prob  reward\n","0  {'p_fraud': 0.08}  blocked          0.9       0\n","1  {'p_fraud': 0.03}  allowed          0.9      20\n","2  {'p_fraud': 0.02}  allowed          0.9      10\n","3  {'p_fraud': 0.01}  allowed          0.9      20\n","4  {'p_fraud': 0.09}  allowed          0.1     -20\n","5   {'p_fraud': 0.4}  allowed          0.1     -10"]},"metadata":{},"execution_count":9}]},{"cell_type":"markdown","metadata":{"id":"ORLinrUq-h_B"},"source":["Now let's use the direct method to score a more lenient fraud model that blocks transactions only if the P(fraud) > 0.10.\n","\n","The direct method requires that we have a function that computes P(action | context)for all possible actions under our new policy. We can define that for our new policy easily here:"]},{"cell_type":"code","metadata":{"id":"oWecXmKE-xCu"},"source":["def action_probabilities(context):\n","    epsilon = 0.10\n","    if context[\"p_fraud\"] > 0.10:\n","        return {\"allowed\": epsilon, \"blocked\": 1 - epsilon}    \n","    \n","    return {\"allowed\": 1 - epsilon, \"blocked\": epsilon}"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SZRzqWLY-2zX"},"source":["We will use the same production logs above and run them through the new policy:"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XRzWhvd_-34u","executionInfo":{"status":"ok","timestamp":1632590710659,"user_tz":-330,"elapsed":1563,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}},"outputId":"fd5c2a95-a9bb-4799-b2be-873c5d79947f"},"source":["evaluate(logs_df, action_probabilities, num_bootstrap_samples=100)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'expected_reward_logging_policy': {'ci_high': 15.89,\n","  'ci_low': -7.89,\n","  'mean': 4.0},\n"," 'expected_reward_new_policy': {'ci_high': 19.34,\n","  'ci_low': -11.12,\n","  'mean': 4.11}}"]},"metadata":{},"execution_count":13}]},{"cell_type":"markdown","metadata":{"id":"99nE9Gyi-6HQ"},"source":["The direct method estimates that the expected reward per observation for the new policy is slightly better than the logging policy so we would think about rolling out this new policy into an A/B test or production.\n","\n","However, the confidence intervals around the expected rewards for our old and new policies overlap heavily. If we want to be really certain, it's might be best to gather some more data to ensure the difference is signal and not noise. In this case, fortunately, we have strong reason to suspect the new policy is worse, but these confidence intervals can be important in cases where we have less prior certainty."]},{"cell_type":"markdown","metadata":{"id":"3vep9LPS_5fa"},"source":["## DR"]},{"cell_type":"code","metadata":{"id":"SbZtj3rLAAG5"},"source":["from typing import NoReturn, Tuple, Dict, Callable\n","\n","import numpy as np\n","import pandas as pd\n","from sklearn.preprocessing import OneHotEncoder\n","from sklearn import ensemble\n","from sklearn.metrics import accuracy_score, mean_squared_error\n","from sklearn.linear_model import Ridge\n","\n","\n","def fit_gbdt_regression(\n","    X_train: np.ndarray,\n","    y_train: np.ndarray,\n","    X_test: np.ndarray = None,\n","    y_test: np.ndarray = None,\n",") -> Dict:\n","    \"\"\"Off the shelf sklearn GBDT regressor.\"\"\"\n","\n","    clf = ensemble.GradientBoostingRegressor()\n","    clf.fit(X_train, y_train)\n","\n","    mse_train = mean_squared_error(y_train, clf.predict(X_train))\n","\n","    mse_test = None\n","    if X_test and y_test:\n","        mse_test = mean_squared_error(y_test, clf.predict(X_test))\n","\n","    return {\"model\": clf, \"mse_train\": mse_train, \"mse_test\": mse_test}\n","\n","\n","def fit_gbdt_classification(\n","    X_train: np.ndarray,\n","    y_train: np.ndarray,\n","    X_test: np.ndarray = None,\n","    y_test: np.ndarray = None,\n",") -> Dict:\n","    \"\"\"Off the shelf sklearn GBDT classifier.\"\"\"\n","\n","    clf = ensemble.GradientBoostingClassifier()\n","    clf.fit(X_train, y_train)\n","\n","    acc_train = accuracy_score(y_train, clf.predict(X_train))\n","\n","    acc_test = None\n","    if X_test and y_test:\n","        acc_test = accuracy_score(y_test, clf.predict(X_test))\n","\n","    return {\"model\": clf, \"acc_train\": acc_train, \"acc_test\": acc_test}\n","\n","\n","def fit_ridge_regression(\n","    X_train: np.ndarray,\n","    y_train: np.ndarray,\n","    X_test: np.ndarray = None,\n","    y_test: np.ndarray = None,\n",") -> Dict:\n","    \"\"\"Off the shelf sklearn Ridge regression.\"\"\"\n","\n","    clf = Ridge()\n","    clf.fit(X_train, y_train)\n","\n","    mse_train = mean_squared_error(y_train, clf.predict(X_train))\n","\n","    mse_test = None\n","    if X_test and y_test:\n","        mse_test = mean_squared_error(y_test, clf.predict(X_test))\n","\n","    return {\"model\": clf, \"mse_train\": mse_train, \"mse_test\": mse_test}\n","\n","\n","def fit_ridge_classification(\n","    X_train: np.ndarray,\n","    y_train: np.ndarray,\n","    X_test: np.ndarray = None,\n","    y_test: np.ndarray = None,\n",") -> Dict:\n","    \"\"\"Off the shelf sklearn Ridge classifier.\"\"\"\n","\n","    clf = RidgeClassifier()\n","    clf.fit(X_train, y_train)\n","\n","    acc_train = accuracy_score(y_train, clf.predict(X_train))\n","\n","    acc_test = None\n","    if X_test and y_test:\n","        acc_test = accuracy_score(y_test, clf.predict(X_test))\n","\n","    return {\"model\": clf, \"acc_train\": acc_train, \"acc_test\": acc_test}\n","\n","\n","class Predictor:\n","    def _preprocess_data(self, df: pd.DataFrame) -> Tuple[np.ndarray, np.ndarray]:\n","        # preprocess context\n","        context_df = df.context.apply(pd.Series)\n","        self.context_column_order = list(context_df.columns)\n","\n","        # preprocess actions\n","        self.action_preprocessor = OneHotEncoder(sparse=False)\n","        action_values = df.action.values.reshape(-1, 1)\n","        self.possible_actions = set(action_values.squeeze().tolist())\n","        one_hot_action_values = self.action_preprocessor.fit_transform(action_values)\n","\n","        X_train = np.concatenate((context_df.values, one_hot_action_values), axis=1)\n","        y_train = df.reward.values\n","\n","        return X_train, y_train\n","\n","    def fit(self, df: pd.DataFrame) -> NoReturn:\n","        X_train, y_train = self._preprocess_data(df)\n","        results = fit_gbdt_regression(X_train, y_train)\n","        self.model = results.pop(\"model\")\n","        self.training_stats = results\n","\n","    def predict(self, input: np.ndarray) -> float:\n","        return self.model.predict(input)[0]\n","\n","\n","def evaluate(\n","    df: pd.DataFrame, action_prob_function: Callable, num_bootstrap_samples: int = 0\n",") -> Dict[str, Dict[str, float]]:\n","\n","    # train a model that predicts reward given (context, action)\n","    reward_model = Predictor()\n","    reward_model.fit(df)\n","\n","    results = [\n","        evaluate_raw(df, action_prob_function, sample=True, reward_model=reward_model)\n","        for _ in range(num_bootstrap_samples)\n","    ]\n","\n","    if not results:\n","        results = [\n","            evaluate_raw(\n","                df, action_prob_function, sample=False, reward_model=reward_model\n","            )\n","        ]\n","\n","    logging_policy_rewards = [result[\"logging_policy\"] for result in results]\n","    new_policy_rewards = [result[\"new_policy\"] for result in results]\n","\n","    return {\n","        \"expected_reward_logging_policy\": compute_list_stats(logging_policy_rewards),\n","        \"expected_reward_new_policy\": compute_list_stats(new_policy_rewards),\n","    }\n","\n","\n","def evaluate_raw(\n","    df: pd.DataFrame,\n","    action_prob_function: Callable,\n","    sample: bool,\n","    reward_model: Predictor,\n",") -> Dict[str, float]:\n","\n","    tmp_df = df.sample(df.shape[0], replace=True) if sample else df\n","\n","    context_df = tmp_df.context.apply(pd.Series)\n","    context_array = context_df[reward_model.context_column_order].values\n","    cum_reward_new_policy = 0\n","\n","    for idx, row in tmp_df.iterrows():\n","        observation_expected_reward = 0\n","        processed_context = context_array[idx]\n","\n","        # first compute the left hand term, which is the direct method\n","        action_probabilities = action_prob_function(row[\"context\"])\n","        for action, action_probability in action_probabilities.items():\n","            one_hot_action = reward_model.action_preprocessor.transform(\n","                np.array(action).reshape(-1, 1)\n","            )\n","            observation = np.concatenate((processed_context, one_hot_action.squeeze()))\n","            predicted_reward = reward_model.predict(observation.reshape(1, -1))\n","            observation_expected_reward += action_probability * predicted_reward\n","\n","        # then compute the right hand term, which is similar to IPS\n","        logged_action = row[\"action\"]\n","        new_action_probability = action_probabilities[logged_action]\n","        weight = new_action_probability / row[\"action_prob\"]\n","        one_hot_action = reward_model.action_preprocessor.transform(\n","            np.array(row[\"action\"]).reshape(-1, 1)\n","        )\n","        observation = np.concatenate((processed_context, one_hot_action.squeeze()))\n","        predicted_reward = reward_model.predict(observation.reshape(1, -1))\n","        observation_expected_reward += weight * (row[\"reward\"] - predicted_reward)\n","\n","        cum_reward_new_policy += observation_expected_reward\n","\n","    return {\n","        \"logging_policy\": tmp_df.reward.sum() / len(tmp_df),\n","        \"new_policy\": cum_reward_new_policy / len(tmp_df),\n","    }"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"cztyBE9Z_-ej"},"source":["### Scenario"]},{"cell_type":"markdown","metadata":{"id":"nE4_lmcBAY1r"},"source":["Assume we have a fraud model in production that blocks transactions if the P(fraud) > 0.05.\n","\n","Let's build some sample logs from that policy running in production. One thing to note, we need some basic exploration in the production logs (e.g. epsilon-greedy w/Îµ = 0.1). That is, 10% of the time we take a random action. Rewards represent revenue gained from allowing the transaction. A negative reward indicates the transaction was fraud and resulted in a chargeback."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":238},"id":"_MN5k5qJAY18","executionInfo":{"status":"ok","timestamp":1632590505641,"user_tz":-330,"elapsed":917,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}},"outputId":"17ee1273-e949-4a04-be3e-705da5c5ed40"},"source":["logs_df = pd.DataFrame([\n","    {\"context\": {\"p_fraud\": 0.08}, \"action\": \"blocked\", \"action_prob\": 0.90, \"reward\": 0},\n","    {\"context\": {\"p_fraud\": 0.03}, \"action\": \"allowed\", \"action_prob\": 0.90, \"reward\": 20},\n","    {\"context\": {\"p_fraud\": 0.02}, \"action\": \"allowed\", \"action_prob\": 0.90, \"reward\": 10}, \n","    {\"context\": {\"p_fraud\": 0.01}, \"action\": \"allowed\", \"action_prob\": 0.90, \"reward\": 20},     \n","    {\"context\": {\"p_fraud\": 0.09}, \"action\": \"allowed\", \"action_prob\": 0.10, \"reward\": -20}, # only allowed due to exploration \n","    {\"context\": {\"p_fraud\": 0.40}, \"action\": \"allowed\", \"action_prob\": 0.10, \"reward\": -10}, # only allowed due to exploration     \n","])\n","\n","logs_df"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>context</th>\n","      <th>action</th>\n","      <th>action_prob</th>\n","      <th>reward</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>{'p_fraud': 0.08}</td>\n","      <td>blocked</td>\n","      <td>0.9</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>{'p_fraud': 0.03}</td>\n","      <td>allowed</td>\n","      <td>0.9</td>\n","      <td>20</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>{'p_fraud': 0.02}</td>\n","      <td>allowed</td>\n","      <td>0.9</td>\n","      <td>10</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>{'p_fraud': 0.01}</td>\n","      <td>allowed</td>\n","      <td>0.9</td>\n","      <td>20</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>{'p_fraud': 0.09}</td>\n","      <td>allowed</td>\n","      <td>0.1</td>\n","      <td>-20</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>{'p_fraud': 0.4}</td>\n","      <td>allowed</td>\n","      <td>0.1</td>\n","      <td>-10</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["             context   action  action_prob  reward\n","0  {'p_fraud': 0.08}  blocked          0.9       0\n","1  {'p_fraud': 0.03}  allowed          0.9      20\n","2  {'p_fraud': 0.02}  allowed          0.9      10\n","3  {'p_fraud': 0.01}  allowed          0.9      20\n","4  {'p_fraud': 0.09}  allowed          0.1     -20\n","5   {'p_fraud': 0.4}  allowed          0.1     -10"]},"metadata":{},"execution_count":9}]},{"cell_type":"markdown","metadata":{"id":"HlboiuS9AZ9w"},"source":["Now let's use the doubly robust method to score a more lenient fraud model that blocks transactions only if the P(fraud) > 0.10.\n","\n","The doubly robust method requires that we have a function that computes P(action | context)for all possible actions under our new policy. We can define that for our new policy easily here:"]},{"cell_type":"code","metadata":{"id":"PQc2rRODAd3y"},"source":["def action_probabilities(context):\n","    epsilon = 0.10\n","    if context[\"p_fraud\"] > 0.10:\n","        return {\"allowed\": epsilon, \"blocked\": 1 - epsilon}    \n","    \n","    return {\"allowed\": 1 - epsilon, \"blocked\": epsilon}"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"173ugbysAfM7"},"source":["We will use the same production logs above and run them through the new policy.\n","\n"]},{"cell_type":"code","metadata":{"id":"byvcpV4KAhaD","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1632591034455,"user_tz":-330,"elapsed":1751,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}},"outputId":"d2d08b58-08f6-4ee8-d8aa-cc6c6053af31"},"source":["evaluate(logs_df, action_probabilities, num_bootstrap_samples=50)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'expected_reward_logging_policy': {'ci_high': 10.58,\n","  'ci_low': -8.24,\n","  'mean': 1.17},\n"," 'expected_reward_new_policy': {'ci_high': 52.14,\n","  'ci_low': -110.68,\n","  'mean': -29.27}}"]},"metadata":{},"execution_count":16}]},{"cell_type":"markdown","metadata":{"id":"35UdAVZVAi4c"},"source":["The doubly robust method estimates that the expected reward per observation for the new policy is much worse than the logging policy so we wouldn't roll out this new policy into an A/B test or production and instead should test some different policies offline.\n","\n","However, the confidence intervals around the expected rewards for our old and new policies overlap heavily. If we want to be really certain, it's might be best to gather some more data to ensure the difference is signal and not noise. In this case, fortunately, we have strong reason to suspect the new policy is worse, but these confidence intervals can be important in cases where we have less prior certainty."]}]}