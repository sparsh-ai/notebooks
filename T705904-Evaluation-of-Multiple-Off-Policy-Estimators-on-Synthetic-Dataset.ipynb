{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"T705904 | Evaluation of Multiple Off-Policy Estimators on Synthetic Dataset","provenance":[],"collapsed_sections":[],"toc_visible":true,"authorship_tag":"ABX9TyNaLt3Ql7zPSyNeSrasSstV"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"wL8SFvvcMI16"},"source":["# Evaluation of Multiple Off-Policy Estimators on Synthetic Dataset"]},{"cell_type":"markdown","metadata":{"id":"l6Z4nt5WmM__"},"source":["We evaluate the estimation performances of the following off-policy estimators using the ground-truth policy value of an evaluation policy calculable with synthetic data.\n","\n","- Direct Method (DM)\n","- Inverse Probability Weighting (IPW)\n","- Self-Normalized Inverse Probability Weighting (SNIPW)\n","- Doubly Robust (DR)\n","- Self-Normalized Doubly Robust (SNDR)\n","- Switch Doubly Robust (Switch-DR)\n","- Doubly Robust with Optimistic Shrinkage (DRos)"]},{"cell_type":"code","metadata":{"id":"JMfMNBMDoqB4"},"source":["# !git clone https://github.com/st-tech/zr-obp.git"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4bW6pkA9w0J3"},"source":["## Imports"]},{"cell_type":"code","metadata":{"id":"z5gPZJY4wZqx"},"source":["from abc import ABCMeta\n","from abc import abstractmethod\n","from dataclasses import dataclass\n","from typing import Dict\n","from typing import Optional\n","\n","import numpy as np\n","from sklearn.utils import check_scalar\n","\n","from typing import Dict\n","from typing import Optional\n","from typing import Union\n","\n","import numpy as np\n","import pandas as pd\n","from sklearn.utils import check_random_state\n","from sklearn.utils import check_scalar\n","import torch\n","\n","from typing import Optional\n","\n","from abc import ABCMeta\n","from abc import abstractmethod\n","\n","import numpy as np\n","from sklearn.utils import check_scalar\n","\n","import argparse\n","from pathlib import Path\n","\n","from joblib import delayed\n","from joblib import Parallel\n","import numpy as np\n","from pandas import DataFrame\n","from sklearn.ensemble import GradientBoostingClassifier\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.linear_model import LogisticRegression\n","import yaml\n","\n","from dataclasses import dataclass\n","from typing import Callable\n","from typing import Optional\n","\n","import enum\n","\n","import numpy as np\n","from scipy.stats import truncnorm\n","from sklearn.utils import check_random_state\n","from sklearn.utils import check_scalar\n","\n","from collections import OrderedDict\n","from dataclasses import dataclass\n","from typing import Dict\n","from typing import Optional\n","from typing import Tuple\n","from typing import Union\n","\n","import numpy as np\n","from scipy.special import softmax\n","from sklearn.base import ClassifierMixin\n","from sklearn.base import clone\n","from sklearn.base import is_classifier\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.utils import check_random_state\n","from sklearn.utils import check_scalar\n","import torch\n","import torch.nn as nn\n","from torch.nn.functional import mse_loss\n","import torch.optim as optim\n","from tqdm import tqdm\n","\n","from dataclasses import dataclass\n","from typing import Optional\n","\n","import numpy as np\n","from sklearn.base import BaseEstimator\n","from sklearn.base import clone\n","from sklearn.base import is_classifier\n","from sklearn.model_selection import KFold\n","from sklearn.utils import check_random_state\n","from sklearn.utils import check_scalar\n","\n","from dataclasses import dataclass\n","from logging import getLogger\n","from pathlib import Path\n","from typing import Dict\n","from typing import List\n","from typing import Optional\n","from typing import Tuple\n","from typing import Union\n","\n","import matplotlib.pyplot as plt\n","import numpy as np\n","from pandas import DataFrame\n","import seaborn as sns\n","from sklearn.utils import check_scalar"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8Ud34TJrwJ2Z"},"source":["## Utils"]},{"cell_type":"code","metadata":{"cellView":"form","id":"29trTk5Jwr36"},"source":["#@markdown main_utils\n","def check_confidence_interval_arguments(\n","    alpha: float = 0.05,\n","    n_bootstrap_samples: int = 10000,\n","    random_state: Optional[int] = None,\n",") -> Optional[ValueError]:\n","    \"\"\"Check confidence interval arguments.\n","    Parameters\n","    ----------\n","    alpha: float, default=0.05\n","        Significance level.\n","    n_bootstrap_samples: int, default=10000\n","        Number of resampling performed in the bootstrap procedure.\n","    random_state: int, default=None\n","        Controls the random seed in bootstrap sampling.\n","    Returns\n","    ----------\n","    estimated_confidence_interval: Dict[str, float]\n","        Dictionary storing the estimated mean and upper-lower confidence bounds.\n","    \"\"\"\n","    check_random_state(random_state)\n","    check_scalar(alpha, \"alpha\", float, min_val=0.0, max_val=1.0)\n","    check_scalar(n_bootstrap_samples, \"n_bootstrap_samples\", int, min_val=1)\n","\n","\n","def estimate_confidence_interval_by_bootstrap(\n","    samples: np.ndarray,\n","    alpha: float = 0.05,\n","    n_bootstrap_samples: int = 10000,\n","    random_state: Optional[int] = None,\n",") -> Dict[str, float]:\n","    \"\"\"Estimate confidence interval by nonparametric bootstrap-like procedure.\n","    Parameters\n","    ----------\n","    samples: array-like\n","        Empirical observed samples to be used to estimate cumulative distribution function.\n","    alpha: float, default=0.05\n","        Significance level.\n","    n_bootstrap_samples: int, default=10000\n","        Number of resampling performed in the bootstrap procedure.\n","    random_state: int, default=None\n","        Controls the random seed in bootstrap sampling.\n","    Returns\n","    ----------\n","    estimated_confidence_interval: Dict[str, float]\n","        Dictionary storing the estimated mean and upper-lower confidence bounds.\n","    \"\"\"\n","    check_confidence_interval_arguments(\n","        alpha=alpha, n_bootstrap_samples=n_bootstrap_samples, random_state=random_state\n","    )\n","\n","    boot_samples = list()\n","    random_ = check_random_state(random_state)\n","    for _ in np.arange(n_bootstrap_samples):\n","        boot_samples.append(np.mean(random_.choice(samples, size=samples.shape[0])))\n","    lower_bound = np.percentile(boot_samples, 100 * (alpha / 2))\n","    upper_bound = np.percentile(boot_samples, 100 * (1.0 - alpha / 2))\n","    return {\n","        \"mean\": np.mean(boot_samples),\n","        f\"{100 * (1. - alpha)}% CI (lower)\": lower_bound,\n","        f\"{100 * (1. - alpha)}% CI (upper)\": upper_bound,\n","    }\n","\n","\n","def sample_action_fast(\n","    action_dist: np.ndarray, random_state: Optional[int] = None\n",") -> np.ndarray:\n","    \"\"\"Sample actions faster based on a given action distribution.\n","    Parameters\n","    ----------\n","    action_dist: array-like, shape (n_rounds, n_actions)\n","        Distribution over actions.\n","    random_state: Optional[int], default=None\n","        Controls the random seed in sampling synthetic bandit dataset.\n","    Returns\n","    ---------\n","    sampled_action: array-like, shape (n_rounds,)\n","        Actions sampled based on `action_dist`.\n","    \"\"\"\n","    random_ = check_random_state(random_state)\n","    uniform_rvs = random_.uniform(size=action_dist.shape[0])[:, np.newaxis]\n","    cum_action_dist = action_dist.cumsum(axis=1)\n","    flg = cum_action_dist > uniform_rvs\n","    sampled_action = flg.argmax(axis=1)\n","    return sampled_action\n","\n","\n","def convert_to_action_dist(\n","    n_actions: int,\n","    selected_actions: np.ndarray,\n",") -> np.ndarray:\n","    \"\"\"Convert selected actions (output of `run_bandit_simulation`) to distribution over actions.\n","    Parameters\n","    ----------\n","    n_actions: int\n","        Number of actions.\n","    selected_actions: array-like, shape (n_rounds, len_list)\n","            Sequence of actions selected by evaluation policy\n","            at each round in offline bandit simulation.\n","    Returns\n","    ----------\n","    action_dist: array-like, shape (n_rounds, n_actions, len_list)\n","        Action choice probabilities (can be deterministic).\n","    \"\"\"\n","    n_rounds, len_list = selected_actions.shape\n","    action_dist = np.zeros((n_rounds, n_actions, len_list))\n","    for pos in np.arange(len_list):\n","        selected_actions_ = selected_actions[:, pos]\n","        action_dist[\n","            np.arange(n_rounds),\n","            selected_actions_,\n","            pos * np.ones(n_rounds, int),\n","        ] = 1\n","    return action_dist\n","\n","\n","def check_array(\n","    array: np.ndarray,\n","    name: str,\n","    expected_dim: int = 1,\n",") -> ValueError:\n","    \"\"\"Input validation on an array.\n","    Parameters\n","    -------------\n","    array: object\n","        Input object to check.\n","    name: str\n","        Name of the input array.\n","    expected_dim: int, default=1\n","        Expected dimension of the input array.\n","    \"\"\"\n","    if not isinstance(array, np.ndarray):\n","        raise ValueError(f\"{name} must be {expected_dim}D array, but got {type(array)}\")\n","    if array.ndim != expected_dim:\n","        raise ValueError(\n","            f\"{name} must be {expected_dim}D array, but got {array.ndim}D array\"\n","        )\n","\n","\n","def check_tensor(\n","    tensor: torch.tensor,\n","    name: str,\n","    expected_dim: int = 1,\n",") -> ValueError:\n","    \"\"\"Input validation on a tensor.\n","    Parameters\n","    -------------\n","    array: object\n","        Input object to check.\n","    name: str\n","        Name of the input array.\n","    expected_dim: int, default=1\n","        Expected dimension of the input array.\n","    \"\"\"\n","    if not isinstance(tensor, torch.Tensor):\n","        raise ValueError(\n","            f\"{name} must be {expected_dim}D tensor, but got {type(tensor)}\"\n","        )\n","    if tensor.ndim != expected_dim:\n","        raise ValueError(\n","            f\"{name} must be {expected_dim}D tensor, but got {tensor.ndim}D tensor\"\n","        )\n","\n","\n","def check_bandit_feedback_inputs(\n","    context: np.ndarray,\n","    action: np.ndarray,\n","    reward: np.ndarray,\n","    expected_reward: Optional[np.ndarray] = None,\n","    position: Optional[np.ndarray] = None,\n","    pscore: Optional[np.ndarray] = None,\n","    action_context: Optional[np.ndarray] = None,\n",") -> Optional[ValueError]:\n","    \"\"\"Check inputs for bandit learning or simulation.\n","    Parameters\n","    -----------\n","    context: array-like, shape (n_rounds, dim_context)\n","        Context vectors in each round, i.e., :math:`x_t`.\n","    action: array-like, shape (n_rounds,)\n","        Action sampled by behavior policy in each round of the logged bandit feedback, i.e., :math:`a_t`.\n","    reward: array-like, shape (n_rounds,)\n","        Observed rewards (or outcome) in each round, i.e., :math:`r_t`.\n","    expected_reward: array-like, shape (n_rounds, n_actions), default=None\n","        Expected rewards (or outcome) in each round, i.e., :math:`\\\\mathbb{E}[r_t]`.\n","    position: array-like, shape (n_rounds,), default=None\n","        Position of recommendation interface where action was presented in each round of the given logged bandit data.\n","    pscore: array-like, shape (n_rounds,), default=None\n","        Propensity scores, the probability of selecting each action by behavior policy,\n","        in the given logged bandit data.\n","    action_context: array-like, shape (n_actions, dim_action_context)\n","        Context vectors characterizing each action.\n","    \"\"\"\n","    check_array(array=context, name=\"context\", expected_dim=2)\n","    check_array(array=action, name=\"action\", expected_dim=1)\n","    check_array(array=reward, name=\"reward\", expected_dim=1)\n","    if not (np.issubdtype(action.dtype, np.integer) and action.min() >= 0):\n","        raise ValueError(\"action elements must be non-negative integers\")\n","\n","    if expected_reward is not None:\n","        check_array(array=expected_reward, name=\"expected_reward\", expected_dim=2)\n","        if not (\n","            context.shape[0]\n","            == action.shape[0]\n","            == reward.shape[0]\n","            == expected_reward.shape[0]\n","        ):\n","            raise ValueError(\n","                \"Expected `context.shape[0] == action.shape[0] == reward.shape[0] == expected_reward.shape[0]`\"\n","                \", but found it False\"\n","            )\n","        if action.max() >= expected_reward.shape[1]:\n","            raise ValueError(\n","                \"action elements must be smaller than `expected_reward.shape[1]`\"\n","            )\n","    if pscore is not None:\n","        check_array(array=pscore, name=\"pscore\", expected_dim=1)\n","        if not (\n","            context.shape[0] == action.shape[0] == reward.shape[0] == pscore.shape[0]\n","        ):\n","            raise ValueError(\n","                \"Expected `context.shape[0] == action.shape[0] == reward.shape[0] == pscore.shape[0]`\"\n","                \", but found it False\"\n","            )\n","        if np.any(pscore <= 0):\n","            raise ValueError(\"pscore must be positive\")\n","\n","    if position is not None:\n","        check_array(array=position, name=\"position\", expected_dim=1)\n","        if not (\n","            context.shape[0] == action.shape[0] == reward.shape[0] == position.shape[0]\n","        ):\n","            raise ValueError(\n","                \"Expected `context.shape[0] == action.shape[0] == reward.shape[0] == position.shape[0]`\"\n","                \", but found it False\"\n","            )\n","        if not (np.issubdtype(position.dtype, np.integer) and position.min() >= 0):\n","            raise ValueError(\"position elements must be non-negative integers\")\n","    else:\n","        if not (context.shape[0] == action.shape[0] == reward.shape[0]):\n","            raise ValueError(\n","                \"Expected `context.shape[0] == action.shape[0] == reward.shape[0]`\"\n","                \", but found it False\"\n","            )\n","    if action_context is not None:\n","        check_array(array=action_context, name=\"action_context\", expected_dim=2)\n","        if action.max() >= action_context.shape[0]:\n","            raise ValueError(\n","                \"action elements must be smaller than `action_context.shape[0]`\"\n","            )\n","\n","\n","def check_ope_inputs(\n","    action_dist: np.ndarray,\n","    position: Optional[np.ndarray] = None,\n","    action: Optional[np.ndarray] = None,\n","    reward: Optional[np.ndarray] = None,\n","    pscore: Optional[np.ndarray] = None,\n","    estimated_rewards_by_reg_model: Optional[np.ndarray] = None,\n",") -> Optional[ValueError]:\n","    \"\"\"Check inputs for ope.\n","    Parameters\n","    -----------\n","    action_dist: array-like, shape (n_rounds, n_actions, len_list)\n","        Action choice probabilities of evaluation policy (can be deterministic), i.e., :math:`\\\\pi_e(a_t|x_t)`.\n","    position: array-like, shape (n_rounds,), default=None\n","        Position of recommendation interface where action was presented in each round of the given logged bandit data.\n","    action: array-like, shape (n_rounds,), default=None\n","        Action sampled by behavior policy in each round of the logged bandit feedback, i.e., :math:`a_t`.\n","    reward: array-like, shape (n_rounds,), default=None\n","        Observed rewards (or outcome) in each round, i.e., :math:`r_t`.\n","    pscore: array-like, shape (n_rounds,), default=None\n","        Propensity scores, the probability of selecting each action by behavior policy,\n","        in the given logged bandit data.\n","    estimated_rewards_by_reg_model: array-like, shape (n_rounds, n_actions, len_list), default=None\n","        Expected rewards given context, action, and position estimated by regression model, i.e., :math:`\\\\hat{q}(x_t,a_t)`.\n","    \"\"\"\n","    # action_dist\n","    check_array(array=action_dist, name=\"action_dist\", expected_dim=3)\n","    if not np.allclose(action_dist.sum(axis=1), 1):\n","        raise ValueError(\"action_dist must be a probability distribution\")\n","\n","    # position\n","    if position is not None:\n","        check_array(array=position, name=\"position\", expected_dim=1)\n","        if not (position.shape[0] == action_dist.shape[0]):\n","            raise ValueError(\n","                \"Expected `position.shape[0] == action_dist.shape[0]`, but found it False\"\n","            )\n","        if not (np.issubdtype(position.dtype, np.integer) and position.min() >= 0):\n","            raise ValueError(\"position elements must be non-negative integers\")\n","        if position.max() >= action_dist.shape[2]:\n","            raise ValueError(\n","                \"position elements must be smaller than `action_dist.shape[2]`\"\n","            )\n","    elif action_dist.shape[2] > 1:\n","        raise ValueError(\n","            \"position elements must be given when `action_dist.shape[2] > 1`\"\n","        )\n","\n","    # estimated_rewards_by_reg_model\n","    if estimated_rewards_by_reg_model is not None:\n","        if estimated_rewards_by_reg_model.shape != action_dist.shape:\n","            raise ValueError(\n","                \"Expected `estimated_rewards_by_reg_model.shape == action_dist.shape`, but found it False\"\n","            )\n","\n","    # action, reward\n","    if action is not None or reward is not None:\n","        check_array(array=action, name=\"action\", expected_dim=1)\n","        check_array(array=reward, name=\"reward\", expected_dim=1)\n","        if not (action.shape[0] == reward.shape[0]):\n","            raise ValueError(\n","                \"Expected `action.shape[0] == reward.shape[0]`, but found it False\"\n","            )\n","        if not (np.issubdtype(action.dtype, np.integer) and action.min() >= 0):\n","            raise ValueError(\"action elements must be non-negative integers\")\n","        if action.max() >= action_dist.shape[1]:\n","            raise ValueError(\n","                \"action elements must be smaller than `action_dist.shape[1]`\"\n","            )\n","\n","    # pscore\n","    if pscore is not None:\n","        if pscore.ndim != 1:\n","            raise ValueError(\"pscore must be 1-dimensional\")\n","        if not (action.shape[0] == reward.shape[0] == pscore.shape[0]):\n","            raise ValueError(\n","                \"Expected `action.shape[0] == reward.shape[0] == pscore.shape[0]`, but found it False\"\n","            )\n","        if np.any(pscore <= 0):\n","            raise ValueError(\"pscore must be positive\")\n","\n","\n","def check_continuous_bandit_feedback_inputs(\n","    context: np.ndarray,\n","    action_by_behavior_policy: np.ndarray,\n","    reward: np.ndarray,\n","    expected_reward: Optional[np.ndarray] = None,\n","    pscore: Optional[np.ndarray] = None,\n",") -> Optional[ValueError]:\n","    \"\"\"Check inputs for bandit learning or simulation with continuous actions.\n","    Parameters\n","    -----------\n","    context: array-like, shape (n_rounds, dim_context)\n","        Context vectors in each round, i.e., :math:`x_t`.\n","    action_by_behavior_policy: array-like, shape (n_rounds,)\n","        Continuous action values sampled by a behavior policy in each round of the logged bandit feedback, i.e., :math:`a_t`.\n","    reward: array-like, shape (n_rounds,)\n","        Observed rewards (or outcome) in each round, i.e., :math:`r_t`.\n","    expected_reward: array-like, shape (n_rounds, n_actions), default=None\n","        Expected rewards (or outcome) in each round, i.e., :math:`\\\\mathbb{E}[r_t]`.\n","    pscore: array-like, shape (n_rounds,), default=None\n","        Probability densities of the continuous action values sampled by a behavior policy\n","        (generalized propensity scores), i.e., :math:`\\\\pi_b(a_t|x_t)`.\n","    \"\"\"\n","    check_array(array=context, name=\"context\", expected_dim=2)\n","    check_array(\n","        array=action_by_behavior_policy,\n","        name=\"action_by_behavior_policy\",\n","        expected_dim=1,\n","    )\n","    check_array(array=reward, name=\"reward\", expected_dim=1)\n","\n","    if expected_reward is not None:\n","        check_array(array=expected_reward, name=\"expected_reward\", expected_dim=1)\n","        if not (\n","            context.shape[0]\n","            == action_by_behavior_policy.shape[0]\n","            == reward.shape[0]\n","            == expected_reward.shape[0]\n","        ):\n","            raise ValueError(\n","                \"Expected `context.shape[0] == action_by_behavior_policy.shape[0]\"\n","                \"== reward.shape[0] == expected_reward.shape[0]`, but found it False\"\n","            )\n","    if pscore is not None:\n","        check_array(array=pscore, name=\"pscore\", expected_dim=1)\n","        if not (\n","            context.shape[0]\n","            == action_by_behavior_policy.shape[0]\n","            == reward.shape[0]\n","            == pscore.shape[0]\n","        ):\n","            raise ValueError(\n","                \"Expected `context.shape[0] == action_by_behavior_policy.shape[0]\"\n","                \"== reward.shape[0] == pscore.shape[0]`, but found it False\"\n","            )\n","        if np.any(pscore <= 0):\n","            raise ValueError(\"pscore must be positive\")\n","\n","\n","def check_continuous_ope_inputs(\n","    action_by_evaluation_policy: np.ndarray,\n","    action_by_behavior_policy: Optional[np.ndarray] = None,\n","    reward: Optional[np.ndarray] = None,\n","    pscore: Optional[np.ndarray] = None,\n","    estimated_rewards_by_reg_model: Optional[np.ndarray] = None,\n",") -> Optional[ValueError]:\n","    \"\"\"Check inputs for OPE with continuous actions.\n","    Parameters\n","    -----------\n","    action_by_evaluation_policy: array-like, shape (n_rounds,)\n","        Continuous action values given by the evaluation policy (can be deterministic), i.e., :math:`\\\\pi_e(x_t)`.\n","    action_by_behavior_policy: array-like, shape (n_rounds,), default=None\n","        Continuous action values sampled by a behavior policy in each round of the logged bandit feedback, i.e., :math:`a_t`.\n","    reward: array-like, shape (n_rounds,), default=None\n","        Observed rewards (or outcome) in each round, i.e., :math:`r_t`.\n","    pscore: array-like, shape (n_rounds,), default=None\n","        Probability densities of the continuous action values sampled by a behavior policy\n","        (generalized propensity scores), i.e., :math:`\\\\pi_b(a_t|x_t)`.\n","    estimated_rewards_by_reg_model: array-like, shape (n_rounds,), default=None\n","            Expected rewards given context and action estimated by a regression model, i.e., :math:`\\\\hat{q}(x_t,a_t)`.\n","    \"\"\"\n","    # action_by_evaluation_policy\n","    check_array(\n","        array=action_by_evaluation_policy,\n","        name=\"action_by_evaluation_policy\",\n","        expected_dim=1,\n","    )\n","\n","    # estimated_rewards_by_reg_model\n","    if estimated_rewards_by_reg_model is not None:\n","        check_array(\n","            array=estimated_rewards_by_reg_model,\n","            name=\"estimated_rewards_by_reg_model\",\n","            expected_dim=1,\n","        )\n","        if (\n","            estimated_rewards_by_reg_model.shape[0]\n","            != action_by_evaluation_policy.shape[0]\n","        ):\n","            raise ValueError(\n","                \"Expected `estimated_rewards_by_reg_model.shape[0] == action_by_evaluation_policy.shape[0]`\"\n","                \", but found if False\"\n","            )\n","\n","    # action, reward\n","    if action_by_behavior_policy is not None or reward is not None:\n","        check_array(\n","            array=action_by_behavior_policy,\n","            name=\"action_by_behavior_policy\",\n","            expected_dim=1,\n","        )\n","        check_array(array=reward, name=\"reward\", expected_dim=1)\n","        if not (action_by_behavior_policy.shape[0] == reward.shape[0]):\n","            raise ValueError(\n","                \"Expected `action_by_behavior_policy.shape[0] == reward.shape[0]`\"\n","                \", but found it False\"\n","            )\n","        if not (\n","            action_by_behavior_policy.shape[0] == action_by_evaluation_policy.shape[0]\n","        ):\n","            raise ValueError(\n","                \"Expected `action_by_behavior_policy.shape[0] == action_by_evaluation_policy.shape[0]`\"\n","                \", but found it False\"\n","            )\n","\n","    # pscore\n","    if pscore is not None:\n","        check_array(array=pscore, name=\"pscore\", expected_dim=1)\n","        if not (\n","            action_by_behavior_policy.shape[0] == reward.shape[0] == pscore.shape[0]\n","        ):\n","            raise ValueError(\n","                \"Expected `action_by_behavior_policy.shape[0] == reward.shape[0] == pscore.shape[0]`\"\n","                \", but found it False\"\n","            )\n","        if np.any(pscore <= 0):\n","            raise ValueError(\"pscore must be positive\")\n","\n","\n","def _check_slate_ope_inputs(\n","    slate_id: np.ndarray,\n","    reward: np.ndarray,\n","    position: np.ndarray,\n","    pscore: np.ndarray,\n","    evaluation_policy_pscore: np.ndarray,\n","    pscore_type: str,\n",") -> Optional[ValueError]:\n","    \"\"\"Check inputs of Slate OPE estimators.\n","    Parameters\n","    -----------\n","    slate_id: array-like, shape (<= n_rounds * len_list,)\n","        Slate id observed in each round of the logged bandit feedback.\n","    reward: array-like, shape (<= n_rounds * len_list,)\n","        Reward observed at each slot in each round of the logged bandit feedback, i.e., :math:`r_{t}(k)`.\n","    position: array-like, shape (<= n_rounds * len_list,)\n","        Positions of each round and slot in the given logged bandit data.\n","    pscore: array-like, shape (<= n_rounds * len_list,)\n","        Action choice probabilities of behavior policy (propensity scores).\n","    evaluation_policy_pscore: array-like, shape (<= n_rounds * len_list,)\n","        Action choice probabilities of evaluation policy.\n","    pscore_type: str\n","        Either \"pscore\", \"pscore_item_position\", or \"pscore_cascade\".\n","    \"\"\"\n","    # position\n","    check_array(array=position, name=\"position\", expected_dim=1)\n","    if not (position.dtype == int and position.min() >= 0):\n","        raise ValueError(\"position elements must be non-negative integers\")\n","\n","    # reward\n","    check_array(array=reward, name=\"reward\", expected_dim=1)\n","\n","    # pscore\n","    check_array(array=pscore, name=f\"{pscore_type}\", expected_dim=1)\n","    if np.any(pscore <= 0) or np.any(pscore > 1):\n","        raise ValueError(f\"{pscore_type} must be in the range of (0, 1]\")\n","\n","    # evaluation_policy_pscore\n","    check_array(\n","        array=evaluation_policy_pscore,\n","        name=f\"evaluation_policy_{pscore_type}\",\n","        expected_dim=1,\n","    )\n","    if np.any(evaluation_policy_pscore < 0) or np.any(evaluation_policy_pscore > 1):\n","        raise ValueError(\n","            f\"evaluation_policy_{pscore_type} must be in the range of [0, 1]\"\n","        )\n","\n","    # slate id\n","    check_array(array=slate_id, name=\"slate_id\", expected_dim=1)\n","    if not (slate_id.dtype == int and slate_id.min() >= 0):\n","        raise ValueError(\"slate_id elements must be non-negative integers\")\n","    if not (\n","        slate_id.shape[0]\n","        == position.shape[0]\n","        == reward.shape[0]\n","        == pscore.shape[0]\n","        == evaluation_policy_pscore.shape[0]\n","    ):\n","        raise ValueError(\n","            f\"slate_id, position, reward, {pscore_type}, and evaluation_policy_{pscore_type} \"\n","            \"must have the same number of samples.\"\n","        )\n","\n","\n","def check_sips_inputs(\n","    slate_id: np.ndarray,\n","    reward: np.ndarray,\n","    position: np.ndarray,\n","    pscore: np.ndarray,\n","    evaluation_policy_pscore: np.ndarray,\n",") -> Optional[ValueError]:\n","    \"\"\"Check inputs of SlateStandardIPS.\n","    Parameters\n","    -----------\n","    slate_id: array-like, shape (<= n_rounds * len_list,)\n","        Slate id observed in each round of the logged bandit feedback.\n","    reward: array-like, shape (<= n_rounds * len_list,)\n","        Reward observed at each slot in each round of the logged bandit feedback, i.e., :math:`r_{t}(k)`.\n","    position: array-like, shape (<= n_rounds * len_list,)\n","        Positions of each round and slot in the given logged bandit data.\n","    pscore: array-like, shape (<= n_rounds * len_list,)\n","        Action choice probabilities of behavior policy (propensity scores), i.e., :math:`\\\\pi_b(a_t|x_t)`.\n","    evaluation_policy_pscore: array-like, shape (<= n_rounds * len_list,)\n","        Action choice probabilities of evaluation policy, i.e., :math:`\\\\pi_e(a_t|x_t)`.\n","    \"\"\"\n","    _check_slate_ope_inputs(\n","        slate_id=slate_id,\n","        reward=reward,\n","        position=position,\n","        pscore=pscore,\n","        evaluation_policy_pscore=evaluation_policy_pscore,\n","        pscore_type=\"pscore\",\n","    )\n","\n","    bandit_feedback_df = pd.DataFrame()\n","    bandit_feedback_df[\"slate_id\"] = slate_id\n","    bandit_feedback_df[\"reward\"] = reward\n","    bandit_feedback_df[\"position\"] = position\n","    bandit_feedback_df[\"pscore\"] = pscore\n","    bandit_feedback_df[\"evaluation_policy_pscore\"] = evaluation_policy_pscore\n","    # check uniqueness\n","    if bandit_feedback_df.duplicated([\"slate_id\", \"position\"]).sum() > 0:\n","        raise ValueError(\"position must not be duplicated in each slate\")\n","    # check pscore uniqueness\n","    distinct_count_pscore_in_slate = bandit_feedback_df.groupby(\"slate_id\").apply(\n","        lambda x: x[\"pscore\"].unique().shape[0]\n","    )\n","    if (distinct_count_pscore_in_slate != 1).sum() > 0:\n","        raise ValueError(\"pscore must be unique in each slate\")\n","    # check pscore uniqueness of evaluation policy\n","    distinct_count_evaluation_policy_pscore_in_slate = bandit_feedback_df.groupby(\n","        \"slate_id\"\n","    ).apply(lambda x: x[\"evaluation_policy_pscore\"].unique().shape[0])\n","    if (distinct_count_evaluation_policy_pscore_in_slate != 1).sum() > 0:\n","        raise ValueError(\"evaluation_policy_pscore must be unique in each slate\")\n","\n","\n","def check_iips_inputs(\n","    slate_id: np.ndarray,\n","    reward: np.ndarray,\n","    position: np.ndarray,\n","    pscore_item_position: np.ndarray,\n","    evaluation_policy_pscore_item_position: np.ndarray,\n",") -> Optional[ValueError]:\n","    \"\"\"Check inputs of SlateIndependentIPS.\n","    Parameters\n","    -----------\n","    slate_id: array-like, shape (<= n_rounds * len_list,)\n","        Slate id observed in each round of the logged bandit feedback.\n","    reward: array-like, shape (<= n_rounds * len_list,)\n","        Reward observed at each slot in each round of the logged bandit feedback, i.e., :math:`r_{t}(k)`.\n","    position: array-like, shape (<= n_rounds * len_list,)\n","        Positions of each round and slot in the given logged bandit data.\n","    pscore_item_position: array-like, shape (<= n_rounds * len_list,)\n","        Marginal action choice probabilities of the slot (:math:`k`) by a behavior policy (propensity scores), i.e., :math:`\\\\pi_b(a_{t}(k) |x_t)`.\n","    evaluation_policy_pscore_item_position: array-like, shape (<= n_rounds * len_list,)\n","        Marginal action choice probabilities of the slot (:math:`k`) by the evaluation policy, i.e., :math:`\\\\pi_e(a_{t}(k) |x_t)`.\n","    \"\"\"\n","    _check_slate_ope_inputs(\n","        slate_id=slate_id,\n","        reward=reward,\n","        position=position,\n","        pscore=pscore_item_position,\n","        evaluation_policy_pscore=evaluation_policy_pscore_item_position,\n","        pscore_type=\"pscore_item_position\",\n","    )\n","\n","    bandit_feedback_df = pd.DataFrame()\n","    bandit_feedback_df[\"slate_id\"] = slate_id\n","    bandit_feedback_df[\"position\"] = position\n","    # check uniqueness\n","    if bandit_feedback_df.duplicated([\"slate_id\", \"position\"]).sum() > 0:\n","        raise ValueError(\"position must not be duplicated in each slate\")\n","\n","\n","def check_rips_inputs(\n","    slate_id: np.ndarray,\n","    reward: np.ndarray,\n","    position: np.ndarray,\n","    pscore_cascade: np.ndarray,\n","    evaluation_policy_pscore_cascade: np.ndarray,\n",") -> Optional[ValueError]:\n","    \"\"\"Check inputs of SlateRewardInteractionIPS.\n","    Parameters\n","    -----------\n","    slate_id: array-like, shape (<= n_rounds * len_list,)\n","        Slate id observed in each round of the logged bandit feedback.\n","    reward: array-like, shape (<= n_rounds * len_list,)\n","        Reward observed at each slot in each round of the logged bandit feedback, i.e., :math:`r_{t}(k)`.\n","    position: array-like, shape (<= n_rounds * len_list,)\n","        Positions of each round and slot in the given logged bandit data.\n","    pscore_cascade: array-like, shape (<= n_rounds * len_list,)\n","        Action choice probabilities of behavior policy (propensity scores), i.e., :math:`\\\\pi_b(a_t|x_t)`.\n","    evaluation_policy_pscore_cascade: array-like, shape (<= n_rounds * len_list,)\n","        Action choice probabilities above the slot (:math:`k`) by the evaluation policy, i.e., :math:`\\\\pi_e(\\\\{a_{t, j}\\\\}_{j \\\\le k}|x_t)`.\n","    \"\"\"\n","    _check_slate_ope_inputs(\n","        slate_id=slate_id,\n","        reward=reward,\n","        position=position,\n","        pscore=pscore_cascade,\n","        evaluation_policy_pscore=evaluation_policy_pscore_cascade,\n","        pscore_type=\"pscore_cascade\",\n","    )\n","\n","    bandit_feedback_df = pd.DataFrame()\n","    bandit_feedback_df[\"slate_id\"] = slate_id\n","    bandit_feedback_df[\"reward\"] = reward\n","    bandit_feedback_df[\"position\"] = position\n","    bandit_feedback_df[\"pscore_cascade\"] = pscore_cascade\n","    bandit_feedback_df[\n","        \"evaluation_policy_pscore_cascade\"\n","    ] = evaluation_policy_pscore_cascade\n","    # sort dataframe\n","    bandit_feedback_df = (\n","        bandit_feedback_df.sort_values([\"slate_id\", \"position\"])\n","        .reset_index(drop=True)\n","        .copy()\n","    )\n","    # check uniqueness\n","    if bandit_feedback_df.duplicated([\"slate_id\", \"position\"]).sum() > 0:\n","        raise ValueError(\"position must not be duplicated in each slate\")\n","    # check pscore_cascade structure\n","    previous_minimum_pscore_cascade = (\n","        bandit_feedback_df.groupby(\"slate_id\")[\"pscore_cascade\"]\n","        .expanding()\n","        .min()\n","        .values\n","    )\n","    if (\n","        previous_minimum_pscore_cascade < bandit_feedback_df[\"pscore_cascade\"]\n","    ).sum() > 0:\n","        raise ValueError(\"pscore_cascade must be non-increasing sequence in each slate\")\n","    # check pscore_cascade structure of evaluation policy\n","    previous_minimum_evaluation_policy_pscore_cascade = (\n","        bandit_feedback_df.groupby(\"slate_id\")[\"evaluation_policy_pscore_cascade\"]\n","        .expanding()\n","        .min()\n","        .values\n","    )\n","    if (\n","        previous_minimum_evaluation_policy_pscore_cascade\n","        < bandit_feedback_df[\"evaluation_policy_pscore_cascade\"]\n","    ).sum() > 0:\n","        raise ValueError(\n","            \"evaluation_policy_pscore_cascade must be non-increasing sequence in each slate\"\n","        )\n","\n","\n","def sigmoid(x: Union[float, np.ndarray]) -> Union[float, np.ndarray]:\n","    \"\"\"Calculate sigmoid function.\"\"\"\n","    return 1.0 / (1.0 + np.exp(-x))\n","\n","\n","def softmax(x: Union[float, np.ndarray]) -> Union[float, np.ndarray]:\n","    \"\"\"Calculate softmax function.\"\"\"\n","    b = np.max(x, axis=1)[:, np.newaxis]\n","    numerator = np.exp(x - b)\n","    denominator = np.sum(numerator, axis=1)[:, np.newaxis]\n","    return numerator / denominator"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"cellView":"form","id":"S-1fenfSxCLW"},"source":["#@markdown helpers\n","def estimate_bias_in_ope(\n","    reward: np.ndarray,\n","    iw: np.ndarray,\n","    iw_hat: np.ndarray,\n","    q_hat: Optional[np.ndarray] = None,\n",") -> float:\n","    \"\"\"Helper to estimate a bias in OPE.\n","    Parameters\n","    ----------\n","    reward: array-like, shape (n_rounds,)\n","        Reward observed in each round of the logged bandit feedback, i.e., :math:`r_t`.\n","    iw: array-like, shape (n_rounds,)\n","        Importance weight in each round of the logged bandit feedback, i.e., :math:`w(x,a)=\\\\pi_e(a|x)/ \\\\pi_b(a|x)`.\n","    iw_hat: array-like, shape (n_rounds,)\n","        Importance weight (IW) modified by a hyparpareter. How IW is modified depends on the estimator as follows.\n","            - clipping: :math:`\\\\hat{w}(x,a) := \\\\min \\\\{ \\\\lambda, w(x,a) \\\\}`\n","            - switching: :math:`\\\\hat{w}(x,a) := w(x,a) \\\\cdot \\\\mathbb{I} \\\\{ w(x,a) < \\\\lambda \\\\}`\n","            - shrinkage: :math:`\\\\hat{w}(x,a) := (\\\\lambda w(x,a)) / (\\\\lambda + w^2(x,a))`\n","        where :math:`\\\\lambda` is a hyperparameter value.\n","    q_hat: array-like, shape (n_rounds,), default=None\n","        Estimated expected reward given context :math:`x_t` and action :math:`a_t`.\n","    Returns\n","    ----------\n","    estimated_bias: float\n","        Estimated the bias in OPE.\n","        This is based on the direct bias estimation stated on page 17 of Su et al.(2020).\n","    References\n","    ----------\n","    Yi Su, Maria Dimakopoulou, Akshay Krishnamurthy, and Miroslav Dudik.\n","    \"Doubly Robust Off-Policy Evaluation with Shrinkage.\", 2020.\n","    \"\"\"\n","    n_rounds = reward.shape[0]\n","    if q_hat is None:\n","        q_hat = np.zeros(n_rounds)\n","    estimated_bias_arr = (iw - iw_hat) * (reward - q_hat)\n","    estimated_bias = np.abs(estimated_bias_arr.mean())\n","\n","    return estimated_bias\n","\n","\n","def estimate_high_probability_upper_bound_bias(\n","    reward: np.ndarray,\n","    iw: np.ndarray,\n","    iw_hat: np.ndarray,\n","    q_hat: Optional[np.ndarray] = None,\n","    delta: float = 0.05,\n",") -> float:\n","    \"\"\"Helper to estimate a high probability upper bound of bias in OPE.\n","    Parameters\n","    ----------\n","    reward: array-like, shape (n_rounds,)\n","        Reward observed in each round of the logged bandit feedback, i.e., :math:`r_t`.\n","    iw: array-like, shape (n_rounds,)\n","        Importance weight in each round of the logged bandit feedback, i.e., :math:`w(x,a)=\\\\pi_e(a|x)/ \\\\pi_b(a|x)`.\n","    iw_hat: array-like, shape (n_rounds,)\n","        Importance weight (IW) modified by a hyparpareter. How IW is modified depends on the estimator as follows.\n","            - clipping: :math:`\\\\hat{w}(x,a) := \\\\min \\\\{ \\\\lambda, w(x,a) \\\\}`\n","            - switching: :math:`\\\\hat{w}(x,a) := w(x,a) \\\\cdot \\\\mathbb{I} \\\\{ w(x,a) < \\\\lambda \\\\}`\n","            - shrinkage: :math:`\\\\hat{w}(x,a) := (\\\\lambda w(x,a)) / (\\\\lambda + w^2(x,a))`\n","        where :math:`\\\\lambda` and :math:`\\\\lambda` are hyperparameters.\n","    q_hat: array-like, shape (n_rounds,), default=None\n","        Estimated expected reward given context :math:`x_t` and action :math:`a_t`.\n","    delta: float, default=0.05\n","        A confidence delta to construct a high probability upper bound based on the Bernstein’s inequality.\n","    Returns\n","    ----------\n","    bias_upper_bound: float\n","        Estimated (high probability) upper bound of the bias.\n","        This upper bound is based on the direct bias estimation\n","        stated on page 17 of Su et al.(2020).\n","    References\n","    ----------\n","    Yi Su, Maria Dimakopoulou, Akshay Krishnamurthy, and Miroslav Dudik.\n","    \"Doubly Robust Off-Policy Evaluation with Shrinkage.\", 2020.\n","    \"\"\"\n","    check_scalar(delta, \"delta\", (int, float), min_val=0.0, max_val=1.0)\n","\n","    bias_upper_bound = estimate_bias_in_ope(\n","        reward=reward,\n","        iw=iw,\n","        iw_hat=iw_hat,\n","        q_hat=q_hat,\n","    )\n","    n_rounds = reward.shape[0]\n","    bias_upper_bound += np.sqrt((2 * (iw ** 2).mean() * np.log(2 / delta)) / n_rounds)\n","    bias_upper_bound += (2 * iw.max() * np.log(2 / delta)) / (3 * n_rounds)\n","\n","    return bias_upper_bound"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"cellView":"form","id":"1uKenT50yI9P"},"source":["#@markdown reward type\n","class RewardType(enum.Enum):\n","    \"\"\"Reward type.\n","    Attributes\n","    ----------\n","    BINARY:\n","        The reward type is binary.\n","    CONTINUOUS:\n","        The reward type is continuous.\n","    \"\"\"\n","\n","    BINARY = \"binary\"\n","    CONTINUOUS = \"continuous\"\n","\n","    def __repr__(self) -> str:\n","\n","        return str(self)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"cellView":"form","id":"aZBo5vEuzYBP"},"source":["#@markdown policy type\n","class PolicyType(enum.Enum):\n","    \"\"\"Policy type.\n","    Attributes\n","    ----------\n","    CONTEXT_FREE:\n","        The policy type is contextfree.\n","    CONTEXTUAL:\n","        The policy type is contextual.\n","    OFFLINE:\n","        The policy type is offline.\n","    \"\"\"\n","\n","    CONTEXT_FREE = enum.auto()\n","    CONTEXTUAL = enum.auto()\n","    OFFLINE = enum.auto()\n","\n","    def __repr__(self) -> str:\n","\n","        return str(self)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-Bd6ynsIxcCf"},"source":["## Dataset"]},{"cell_type":"code","metadata":{"id":"H45pS7eaxdF1"},"source":["# from ..types import BanditFeedback\n","# from ..utils import check_array\n","# from ..utils import sample_action_fast\n","# from ..utils import sigmoid\n","# from ..utils import softmax\n","# from .base import BaseBanditDataset\n","# from .reward_type import RewardType"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"owWf6vwYyCLg"},"source":["class BaseBanditDataset(metaclass=ABCMeta):\n","    \"\"\"Base Class for Synthetic Bandit Dataset.\"\"\"\n","\n","    @abstractmethod\n","    def obtain_batch_bandit_feedback(self) -> None:\n","        \"\"\"Obtain batch logged bandit feedback.\"\"\"\n","        raise NotImplementedError\n","\n","\n","class BaseRealBanditDataset(BaseBanditDataset):\n","    \"\"\"Base Class for Real-World Bandit Dataset.\"\"\"\n","\n","    @abstractmethod\n","    def load_raw_data(self) -> None:\n","        \"\"\"Load raw dataset.\"\"\"\n","        raise NotImplementedError\n","\n","    @abstractmethod\n","    def pre_process(self) -> None:\n","        \"\"\"Preprocess raw dataset.\"\"\"\n","        raise NotImplementedError"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"dLvROuE_x0iJ"},"source":["# dataset type\n","BanditFeedback = Dict[str, Union[int, np.ndarray]]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"cellView":"form","id":"hji2HuzuxkyI"},"source":["#@markdown synthetic dataset\n","@dataclass\n","class SyntheticBanditDataset(BaseBanditDataset):\n","    \"\"\"Class for generating synthetic bandit dataset.\n","    Note\n","    -----\n","    By calling the `obtain_batch_bandit_feedback` method several times,\n","    we have different bandit samples with the same setting.\n","    This can be used to estimate confidence intervals of the performances of OPE estimators.\n","    If None is set as `behavior_policy_function`, the synthetic data will be context-free bandit feedback.\n","    Parameters\n","    -----------\n","    n_actions: int\n","        Number of actions.\n","    dim_context: int, default=1\n","        Number of dimensions of context vectors.\n","    reward_type: str, default='binary'\n","        Type of reward variable, which must be either 'binary' or 'continuous'.\n","        When 'binary' is given, rewards are sampled from the Bernoulli distribution.\n","        When 'continuous' is given, rewards are sampled from the truncated Normal distribution with `scale=1`.\n","        The mean parameter of the reward distribution is determined by the `reward_function` specified by the next argument.\n","    reward_function: Callable[[np.ndarray, np.ndarray], np.ndarray]], default=None\n","        Function generating expected reward for each given action-context pair,\n","        i.e., :math:`\\\\mu: \\\\mathcal{X} \\\\times \\\\mathcal{A} \\\\rightarrow \\\\mathbb{R}`.\n","        If None is set, context **independent** expected reward for each action will be\n","        sampled from the uniform distribution automatically.\n","    reward_std: float, default=1.0\n","        Standard deviation of the reward distribution.\n","        A larger value leads to a noisy reward distribution.\n","        This argument is valid only when `reward_type=\"continuous\"`.\n","    behavior_policy_function: Callable[[np.ndarray, np.ndarray], np.ndarray], default=None\n","        Function generating probability distribution over action space,\n","        i.e., :math:`\\\\pi: \\\\mathcal{X} \\\\rightarrow \\\\Delta(\\\\mathcal{A})`.\n","        If None is set, context **independent** uniform distribution will be used (uniform random behavior policy).\n","    tau: float, default=1.0\n","        A temperature hyperparameer which controls the behavior policy.\n","        A large value leads to a near-uniform behavior policy,\n","        while a small value leads to a near-deterministic behavior policy.\n","    random_state: int, default=12345\n","        Controls the random seed in sampling synthetic bandit dataset.\n","    dataset_name: str, default='synthetic_bandit_dataset'\n","        Name of the dataset.\n","    Examples\n","    ----------\n","    .. code-block:: python\n","        >>> import numpy as np\n","        >>> from obp.dataset import (\n","            SyntheticBanditDataset,\n","            linear_reward_function,\n","            linear_behavior_policy\n","        )\n","        # generate synthetic contextual bandit feedback with 10 actions.\n","        >>> dataset = SyntheticBanditDataset(\n","                n_actions=10,\n","                dim_context=5,\n","                reward_function=logistic_reward_function,\n","                behavior_policy=linear_behavior_policy,\n","                random_state=12345\n","            )\n","        >>> bandit_feedback = dataset.obtain_batch_bandit_feedback(n_rounds=100000)\n","        >>> bandit_feedback\n","        {\n","            'n_rounds': 100000,\n","            'n_actions': 10,\n","            'context': array([[-0.20470766,  0.47894334, -0.51943872, -0.5557303 ,  1.96578057],\n","                    [ 1.39340583,  0.09290788,  0.28174615,  0.76902257,  1.24643474],\n","                    [ 1.00718936, -1.29622111,  0.27499163,  0.22891288,  1.35291684],\n","                    ...,\n","                    [ 1.36946256,  0.58727761, -0.69296769, -0.27519988, -2.10289159],\n","                    [-0.27428715,  0.52635353,  1.02572168, -0.18486381,  0.72464834],\n","                    [-1.25579833, -1.42455203, -0.26361242,  0.27928604,  1.21015571]]),\n","            'action_context': array([[1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n","                    [0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n","                    [0, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n","                    [0, 0, 0, 1, 0, 0, 0, 0, 0, 0],\n","                    [0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n","                    [0, 0, 0, 0, 0, 1, 0, 0, 0, 0],\n","                    [0, 0, 0, 0, 0, 0, 1, 0, 0, 0],\n","                    [0, 0, 0, 0, 0, 0, 0, 1, 0, 0],\n","                    [0, 0, 0, 0, 0, 0, 0, 0, 1, 0],\n","                    [0, 0, 0, 0, 0, 0, 0, 0, 0, 1]]),\n","            'action': array([7, 4, 0, ..., 7, 9, 6]),\n","            'position': None,\n","            'reward': array([0, 1, 1, ..., 0, 1, 0]),\n","            'expected_reward': array([[0.80210203, 0.73828559, 0.83199558, ..., 0.81190503, 0.70617705,\n","                    0.68985306],\n","                    [0.94119582, 0.93473317, 0.91345213, ..., 0.94140688, 0.93152449,\n","                    0.90132868],\n","                    [0.87248862, 0.67974991, 0.66965669, ..., 0.79229752, 0.82712978,\n","                    0.74923536],\n","                    ...,\n","                    [0.64856003, 0.38145901, 0.84476094, ..., 0.40962057, 0.77114661,\n","                    0.65752798],\n","                    [0.73208527, 0.82012699, 0.78161352, ..., 0.72361416, 0.8652249 ,\n","                    0.82571751],\n","                    [0.40348366, 0.24485417, 0.24037926, ..., 0.49613133, 0.30714854,\n","                    0.5527749 ]]),\n","            'pscore': array([0.05423855, 0.10339675, 0.09756788, ..., 0.05423855, 0.07250876,\n","                    0.14065505])\n","        }\n","    \"\"\"\n","\n","    n_actions: int\n","    dim_context: int = 1\n","    reward_type: str = RewardType.BINARY.value\n","    reward_function: Optional[Callable[[np.ndarray, np.ndarray], np.ndarray]] = None\n","    reward_std: float = 1.0\n","    behavior_policy_function: Optional[\n","        Callable[[np.ndarray, np.ndarray], np.ndarray]\n","    ] = None\n","    tau: float = 1.0\n","    random_state: int = 12345\n","    dataset_name: str = \"synthetic_bandit_dataset\"\n","\n","    def __post_init__(self) -> None:\n","        \"\"\"Initialize Class.\"\"\"\n","        check_scalar(self.n_actions, \"n_actions\", int, min_val=2)\n","        check_scalar(self.dim_context, \"dim_context\", int, min_val=1)\n","        if RewardType(self.reward_type) not in [\n","            RewardType.BINARY,\n","            RewardType.CONTINUOUS,\n","        ]:\n","            raise ValueError(\n","                f\"reward_type must be either '{RewardType.BINARY.value}' or '{RewardType.CONTINUOUS.value}', but {self.reward_type} is given.'\"\n","            )\n","        check_scalar(self.reward_std, \"reward_std\", (int, float), min_val=0)\n","        check_scalar(self.tau, \"tau\", (int, float), min_val=0)\n","        if self.random_state is None:\n","            raise ValueError(\"`random_state` must be given\")\n","        self.random_ = check_random_state(self.random_state)\n","        if self.reward_function is None:\n","            self.expected_reward = self.sample_contextfree_expected_reward()\n","        if self.behavior_policy_function is None:\n","            self.behavior_policy = np.ones(self.n_actions) / self.n_actions\n","        if RewardType(self.reward_type) == RewardType.CONTINUOUS:\n","            self.reward_min = 0\n","            self.reward_max = 1e10\n","        # one-hot encoding representations characterizing each action\n","        self.action_context = np.eye(self.n_actions, dtype=int)\n","\n","    @property\n","    def len_list(self) -> int:\n","        \"\"\"Length of recommendation lists.\"\"\"\n","        return 1\n","\n","    def sample_contextfree_expected_reward(self) -> np.ndarray:\n","        \"\"\"Sample expected reward for each action from the uniform distribution.\"\"\"\n","        return self.random_.uniform(size=self.n_actions)\n","\n","    def calc_expected_reward(self, context: np.ndarray) -> np.ndarray:\n","        \"\"\"Sample expected rewards given contexts\"\"\"\n","        # sample reward for each round based on the reward function\n","        if self.reward_function is None:\n","            expected_reward_ = np.tile(self.expected_reward, (context.shape[0], 1))\n","        else:\n","            expected_reward_ = self.reward_function(\n","                context=context,\n","                action_context=self.action_context,\n","                random_state=self.random_state,\n","            )\n","\n","        return expected_reward_\n","\n","    def sample_reward_given_expected_reward(\n","        self,\n","        expected_reward: np.ndarray,\n","        action: np.ndarray,\n","    ) -> np.ndarray:\n","        \"\"\"Sample reward given expected rewards\"\"\"\n","        expected_reward_factual = expected_reward[np.arange(action.shape[0]), action]\n","        if RewardType(self.reward_type) == RewardType.BINARY:\n","            reward = self.random_.binomial(n=1, p=expected_reward_factual)\n","        elif RewardType(self.reward_type) == RewardType.CONTINUOUS:\n","            mean = expected_reward_factual\n","            a = (self.reward_min - mean) / self.reward_std\n","            b = (self.reward_max - mean) / self.reward_std\n","            reward = truncnorm.rvs(\n","                a=a,\n","                b=b,\n","                loc=mean,\n","                scale=self.reward_std,\n","                random_state=self.random_state,\n","            )\n","        else:\n","            raise NotImplementedError\n","\n","        return reward\n","\n","    def sample_reward(self, context: np.ndarray, action: np.ndarray) -> np.ndarray:\n","        \"\"\"Sample rewards given contexts and actions, i.e., :math:`r \\\\sim p(r \\\\mid x, a)`.\n","        Parameters\n","        -----------\n","        context: array-like, shape (n_rounds, dim_context)\n","            Context vectors characterizing each round (such as user information).\n","        action: array-like, shape (n_rounds,)\n","            Selected actions to the contexts.\n","        Returns\n","        ---------\n","        reward: array-like, shape (n_rounds,)\n","            Sampled rewards given contexts and actions.\n","        \"\"\"\n","        check_array(array=context, name=\"context\", expected_dim=2)\n","        check_array(array=action, name=\"action\", expected_dim=1)\n","        if context.shape[0] != action.shape[0]:\n","            raise ValueError(\n","                \"Expected `context.shape[0] == action.shape[0]`, but found it False\"\n","            )\n","        if not np.issubdtype(action.dtype, np.integer):\n","            raise ValueError(\"the dtype of action must be a subdtype of int\")\n","\n","        expected_reward_ = self.calc_expected_reward(context)\n","\n","        return self.sample_reward_given_expected_reward(expected_reward_, action)\n","\n","    def obtain_batch_bandit_feedback(self, n_rounds: int) -> BanditFeedback:\n","        \"\"\"Obtain batch logged bandit feedback.\n","        Parameters\n","        ----------\n","        n_rounds: int\n","            Number of rounds for synthetic bandit feedback data.\n","        Returns\n","        ---------\n","        bandit_feedback: BanditFeedback\n","            Generated synthetic bandit feedback dataset.\n","        \"\"\"\n","        check_scalar(n_rounds, \"n_rounds\", int, min_val=1)\n","        context = self.random_.normal(size=(n_rounds, self.dim_context))\n","        # sample actions for each round based on the behavior policy\n","        if self.behavior_policy_function is None:\n","            behavior_policy_ = np.tile(self.behavior_policy, (n_rounds, 1))\n","            behavior_policy_ = softmax(behavior_policy_ / self.tau)\n","            action = self.random_.choice(\n","                np.arange(self.n_actions), p=self.behavior_policy, size=n_rounds\n","            )\n","        else:\n","            behavior_policy_ = self.behavior_policy_function(\n","                context=context,\n","                action_context=self.action_context,\n","                random_state=self.random_state,\n","            )\n","            behavior_policy_ = softmax(behavior_policy_ / self.tau)\n","            action = sample_action_fast(\n","                behavior_policy_, random_state=self.random_state\n","            )\n","        pscore = behavior_policy_[np.arange(n_rounds), action]\n","\n","        # sample reward based on the context and action\n","        expected_reward_ = self.calc_expected_reward(context)\n","        if RewardType(self.reward_type) == RewardType.CONTINUOUS:\n","            # correct expected_reward_, as we use truncated normal distribution here\n","            mean = expected_reward_\n","            a = (self.reward_min - mean) / self.reward_std\n","            b = (self.reward_max - mean) / self.reward_std\n","            expected_reward_ = truncnorm.stats(\n","                a=a, b=b, loc=mean, scale=self.reward_std, moments=\"m\"\n","            )\n","        reward = self.sample_reward_given_expected_reward(expected_reward_, action)\n","\n","        return dict(\n","            n_rounds=n_rounds,\n","            n_actions=self.n_actions,\n","            context=context,\n","            action_context=self.action_context,\n","            action=action,\n","            position=None,  # position effect is not considered in synthetic data\n","            reward=reward,\n","            expected_reward=expected_reward_,\n","            pscore=pscore,\n","        )\n","\n","    def calc_ground_truth_policy_value(\n","        self, expected_reward: np.ndarray, action_dist: np.ndarray\n","    ) -> float:\n","        \"\"\"Calculate the policy value of given action distribution on the given expected_reward.\n","        Parameters\n","        -----------\n","        expected_reward: array-like, shape (n_rounds, n_actions)\n","            Expected reward given context (:math:`x`) and action (:math:`a`), i.e., :math:`q(x,a):=\\\\mathbb{E}[r|x,a]`.\n","            This is often the expected_reward of the test set of logged bandit feedback data.\n","        action_dist: array-like, shape (n_rounds, n_actions, len_list)\n","            Action choice probabilities of evaluation policy (can be deterministic), i.e., :math:`\\\\pi_e(a_t|x_t)`.\n","        Returns\n","        ----------\n","        policy_value: float\n","            The policy value of the given action distribution on the given bandit feedback data.\n","        \"\"\"\n","        check_array(array=expected_reward, name=\"expected_reward\", expected_dim=2)\n","        check_array(array=action_dist, name=\"action_dist\", expected_dim=3)\n","        if expected_reward.shape[0] != action_dist.shape[0]:\n","            raise ValueError(\n","                \"Expected `expected_reward.shape[0] = action_dist.shape[0]`, but found it False\"\n","            )\n","        if expected_reward.shape[1] != action_dist.shape[1]:\n","            raise ValueError(\n","                \"Expected `expected_reward.shape[1] = action_dist.shape[1]`, but found it False\"\n","            )\n","\n","        return np.average(expected_reward, weights=action_dist[:, :, 0], axis=1).mean()\n","\n","\n","def logistic_reward_function(\n","    context: np.ndarray,\n","    action_context: np.ndarray,\n","    random_state: Optional[int] = None,\n",") -> np.ndarray:\n","    \"\"\"Logistic mean reward function for synthetic bandit datasets.\n","    Parameters\n","    -----------\n","    context: array-like, shape (n_rounds, dim_context)\n","        Context vectors characterizing each round (such as user information).\n","    action_context: array-like, shape (n_actions, dim_action_context)\n","        Vector representation for each action.\n","    random_state: int, default=None\n","        Controls the random seed in sampling dataset.\n","    Returns\n","    ---------\n","    expected_reward: array-like, shape (n_rounds, n_actions)\n","        Expected reward given context (:math:`x`) and action (:math:`a`), i.e., :math:`q(x,a):=\\\\mathbb{E}[r|x,a]`.\n","    \"\"\"\n","    check_array(array=context, name=\"context\", expected_dim=2)\n","    check_array(array=action_context, name=\"action_context\", expected_dim=2)\n","\n","    random_ = check_random_state(random_state)\n","    logits = np.zeros((context.shape[0], action_context.shape[0]))\n","    # each arm has different coefficient vectors\n","    coef_ = random_.uniform(size=(action_context.shape[0], context.shape[1]))\n","    action_coef_ = random_.uniform(size=action_context.shape[1])\n","    for d in np.arange(action_context.shape[0]):\n","        logits[:, d] = context @ coef_[d] + action_context[d] @ action_coef_\n","\n","    return sigmoid(logits)\n","\n","\n","def linear_reward_function(\n","    context: np.ndarray,\n","    action_context: np.ndarray,\n","    random_state: Optional[int] = None,\n",") -> np.ndarray:\n","    \"\"\"Linear mean reward function for synthetic bandit datasets.\n","    Parameters\n","    -----------\n","    context: array-like, shape (n_rounds, dim_context)\n","        Context vectors characterizing each round (such as user information).\n","    action_context: array-like, shape (n_actions, dim_action_context)\n","        Vector representation for each action.\n","    random_state: int, default=None\n","        Controls the random seed in sampling dataset.\n","    Returns\n","    ---------\n","    expected_reward: array-like, shape (n_rounds, n_actions)\n","        Expected reward given context (:math:`x`) and action (:math:`a`), i.e., :math:`q(x,a):=\\\\mathbb{E}[r|x,a]`.\n","    \"\"\"\n","    check_array(array=context, name=\"context\", expected_dim=2)\n","    check_array(array=action_context, name=\"action_context\", expected_dim=2)\n","\n","    random_ = check_random_state(random_state)\n","    expected_reward = np.zeros((context.shape[0], action_context.shape[0]))\n","    # each arm has different coefficient vectors\n","    coef_ = random_.uniform(size=(action_context.shape[0], context.shape[1]))\n","    action_coef_ = random_.uniform(size=action_context.shape[1])\n","    for d in np.arange(action_context.shape[0]):\n","        expected_reward[:, d] = context @ coef_[d] + action_context[d] @ action_coef_\n","\n","    return expected_reward\n","\n","\n","def linear_behavior_policy(\n","    context: np.ndarray,\n","    action_context: np.ndarray,\n","    random_state: Optional[int] = None,\n",") -> np.ndarray:\n","    \"\"\"Linear contextual behavior policy for synthetic bandit datasets.\n","    Parameters\n","    -----------\n","    context: array-like, shape (n_rounds, dim_context)\n","        Context vectors characterizing each round (such as user information).\n","    action_context: array-like, shape (n_actions, dim_action_context)\n","        Vector representation for each action.\n","    random_state: int, default=None\n","        Controls the random seed in sampling dataset.\n","    Returns\n","    ---------\n","    behavior_policy: array-like, shape (n_rounds, n_actions)\n","        Logit values given context (:math:`x`), i.e., :math:`\\\\pi: \\\\mathcal{X} \\\\rightarrow \\\\Delta(\\\\mathcal{A})`.\n","    \"\"\"\n","    check_array(array=context, name=\"context\", expected_dim=2)\n","    check_array(array=action_context, name=\"action_context\", expected_dim=2)\n","\n","    random_ = check_random_state(random_state)\n","    logits = np.zeros((context.shape[0], action_context.shape[0]))\n","    coef_ = random_.uniform(size=context.shape[1])\n","    action_coef_ = random_.uniform(size=action_context.shape[1])\n","    for d in np.arange(action_context.shape[0]):\n","        logits[:, d] = context @ coef_ + action_context[d] @ action_coef_\n","\n","    return logits"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jOiYdOljzxGO"},"source":["## Base Models"]},{"cell_type":"code","metadata":{"cellView":"form","id":"1lsiTpWxzyqf"},"source":["#@markdown Regression model\n","@dataclass\n","class RegressionModel(BaseEstimator):\n","    \"\"\"Machine learning model to estimate the mean reward function (:math:`q(x,a):= \\\\mathbb{E}[r|x,a]`).\n","    Note\n","    -------\n","    Reward (or outcome) :math:`r` must be either binary or continuous.\n","    Parameters\n","    ------------\n","    base_model: BaseEstimator\n","        A machine learning model used to estimate the mean reward function.\n","    n_actions: int\n","        Number of actions.\n","    len_list: int, default=1\n","        Length of a list of actions recommended in each impression.\n","        When Open Bandit Dataset is used, 3 should be set.\n","    action_context: array-like, shape (n_actions, dim_action_context), default=None\n","        Context vector characterizing action (i.e., vector representation of each action).\n","        If not given, one-hot encoding of the action variable is used as default.\n","    fitting_method: str, default='normal'\n","        Method to fit the regression model.\n","        Must be one of ['normal', 'iw', 'mrdr'] where 'iw' stands for importance weighting and\n","        'mrdr' stands for more robust doubly robust.\n","    References\n","    -----------\n","    Mehrdad Farajtabar, Yinlam Chow, and Mohammad Ghavamzadeh.\n","    \"More Robust Doubly Robust Off-policy Evaluation.\", 2018.\n","    Yi Su, Maria Dimakopoulou, Akshay Krishnamurthy, and Miroslav Dudik.\n","    \"Doubly Robust Off-Policy Evaluation with Shrinkage.\", 2020.\n","    Yusuke Narita, Shota Yasui, and Kohei Yata.\n","    \"Off-policy Bandit and Reinforcement Learning.\", 2020.\n","    \"\"\"\n","\n","    base_model: BaseEstimator\n","    n_actions: int\n","    len_list: int = 1\n","    action_context: Optional[np.ndarray] = None\n","    fitting_method: str = \"normal\"\n","\n","    def __post_init__(self) -> None:\n","        \"\"\"Initialize Class.\"\"\"\n","        check_scalar(self.n_actions, \"n_actions\", int, min_val=2)\n","        check_scalar(self.len_list, \"len_list\", int, min_val=1)\n","        if not (\n","            isinstance(self.fitting_method, str)\n","            and self.fitting_method in [\"normal\", \"iw\", \"mrdr\"]\n","        ):\n","            raise ValueError(\n","                f\"fitting_method must be one of 'normal', 'iw', or 'mrdr', but {self.fitting_method} is given\"\n","            )\n","        if not isinstance(self.base_model, BaseEstimator):\n","            raise ValueError(\n","                \"base_model must be BaseEstimator or a child class of BaseEstimator\"\n","            )\n","\n","        self.base_model_list = [\n","            clone(self.base_model) for _ in np.arange(self.len_list)\n","        ]\n","        if self.action_context is None:\n","            self.action_context = np.eye(self.n_actions, dtype=int)\n","\n","    def fit(\n","        self,\n","        context: np.ndarray,\n","        action: np.ndarray,\n","        reward: np.ndarray,\n","        pscore: Optional[np.ndarray] = None,\n","        position: Optional[np.ndarray] = None,\n","        action_dist: Optional[np.ndarray] = None,\n","    ) -> None:\n","        \"\"\"Fit the regression model on given logged bandit feedback data.\n","        Parameters\n","        ----------\n","        context: array-like, shape (n_rounds, dim_context)\n","            Context vectors observed in each round of the logged bandit feedback, i.e., :math:`x_t`.\n","        action: array-like, shape (n_rounds,)\n","            Action sampled by behavior policy in each round of the logged bandit feedback, i.e., :math:`a_t`.\n","        reward: array-like, shape (n_rounds,)\n","            Reward observed in each round of the logged bandit feedback, i.e., :math:`r_t`.\n","        pscore: array-like, shape (n_rounds,)\n","            Action choice probabilities of behavior policy (propensity scores), i.e., :math:`\\\\pi_b(a_t|x_t)`.\n","            When None is given, behavior policy is assumed to be uniform.\n","        position: array-like, shape (n_rounds,), default=None\n","            Position of recommendation interface where action was presented in each round of the given logged bandit data.\n","            If None is set, a regression model assumes that there is only one position.\n","            When `len_list` > 1, this position argument has to be set.\n","        action_dist: array-like, shape (n_rounds, n_actions, len_list), default=None\n","            Action choice probabilities of evaluation policy (can be deterministic), i.e., :math:`\\\\pi_e(a_t|x_t)`.\n","            When either of 'iw' or 'mrdr' is used as the 'fitting_method' argument, then `action_dist` must be given.\n","        \"\"\"\n","        check_bandit_feedback_inputs(\n","            context=context,\n","            action=action,\n","            reward=reward,\n","            pscore=pscore,\n","            position=position,\n","            action_context=self.action_context,\n","        )\n","        n_rounds = context.shape[0]\n","\n","        if position is None or self.len_list == 1:\n","            position = np.zeros_like(action)\n","        else:\n","            if position.max() >= self.len_list:\n","                raise ValueError(\n","                    f\"position elements must be smaller than len_list, but the maximum value is {position.max()} (>= {self.len_list})\"\n","                )\n","        if self.fitting_method in [\"iw\", \"mrdr\"]:\n","            if not (isinstance(action_dist, np.ndarray) and action_dist.ndim == 3):\n","                raise ValueError(\n","                    \"when fitting_method is either 'iw' or 'mrdr', action_dist (a 3-dimensional ndarray) must be given\"\n","                )\n","            if action_dist.shape != (n_rounds, self.n_actions, self.len_list):\n","                raise ValueError(\n","                    f\"shape of action_dist must be (n_rounds, n_actions, len_list)=({n_rounds, self.n_actions, self.len_list}), but is {action_dist.shape}\"\n","                )\n","            if not np.allclose(action_dist.sum(axis=1), 1):\n","                raise ValueError(\"action_dist must be a probability distribution\")\n","        if pscore is None:\n","            pscore = np.ones_like(action) / self.n_actions\n","\n","        for position_ in np.arange(self.len_list):\n","            idx = position == position_\n","            X = self._pre_process_for_reg_model(\n","                context=context[idx],\n","                action=action[idx],\n","                action_context=self.action_context,\n","            )\n","            if X.shape[0] == 0:\n","                raise ValueError(f\"No training data at position {position_}\")\n","            # train the base model according to the given `fitting method`\n","            if self.fitting_method == \"normal\":\n","                self.base_model_list[position_].fit(X, reward[idx])\n","            else:\n","                action_dist_at_position = action_dist[\n","                    np.arange(n_rounds),\n","                    action,\n","                    position_ * np.ones(n_rounds, dtype=int),\n","                ][idx]\n","                if self.fitting_method == \"iw\":\n","                    sample_weight = action_dist_at_position / pscore[idx]\n","                    self.base_model_list[position_].fit(\n","                        X, reward[idx], sample_weight=sample_weight\n","                    )\n","                elif self.fitting_method == \"mrdr\":\n","                    sample_weight = action_dist_at_position\n","                    sample_weight *= 1.0 - pscore[idx]\n","                    sample_weight /= pscore[idx] ** 2\n","                    self.base_model_list[position_].fit(\n","                        X, reward[idx], sample_weight=sample_weight\n","                    )\n","\n","    def predict(self, context: np.ndarray) -> np.ndarray:\n","        \"\"\"Predict the mean reward function.\n","        Parameters\n","        -----------\n","        context: array-like, shape (n_rounds_of_new_data, dim_context)\n","            Context vectors of new data.\n","        Returns\n","        -----------\n","        estimated_rewards_by_reg_model: array-like, shape (n_rounds_of_new_data, n_actions, len_list)\n","            Expected rewards of new data estimated by the regression model.\n","        \"\"\"\n","        n_rounds_of_new_data = context.shape[0]\n","        ones_n_rounds_arr = np.ones(n_rounds_of_new_data, int)\n","        estimated_rewards_by_reg_model = np.zeros(\n","            (n_rounds_of_new_data, self.n_actions, self.len_list)\n","        )\n","        for action_ in np.arange(self.n_actions):\n","            for position_ in np.arange(self.len_list):\n","                X = self._pre_process_for_reg_model(\n","                    context=context,\n","                    action=action_ * ones_n_rounds_arr,\n","                    action_context=self.action_context,\n","                )\n","                estimated_rewards_ = (\n","                    self.base_model_list[position_].predict_proba(X)[:, 1]\n","                    if is_classifier(self.base_model_list[position_])\n","                    else self.base_model_list[position_].predict(X)\n","                )\n","                estimated_rewards_by_reg_model[\n","                    np.arange(n_rounds_of_new_data),\n","                    action_ * ones_n_rounds_arr,\n","                    position_ * ones_n_rounds_arr,\n","                ] = estimated_rewards_\n","        return estimated_rewards_by_reg_model\n","\n","    def fit_predict(\n","        self,\n","        context: np.ndarray,\n","        action: np.ndarray,\n","        reward: np.ndarray,\n","        pscore: Optional[np.ndarray] = None,\n","        position: Optional[np.ndarray] = None,\n","        action_dist: Optional[np.ndarray] = None,\n","        n_folds: int = 1,\n","        random_state: Optional[int] = None,\n","    ) -> np.ndarray:\n","        \"\"\"Fit the regression model on given logged bandit feedback data and predict the reward function of the same data.\n","        Note\n","        ------\n","        When `n_folds` is larger than 1, then the cross-fitting procedure is applied.\n","        See the reference for the details about the cross-fitting technique.\n","        Parameters\n","        ----------\n","        context: array-like, shape (n_rounds, dim_context)\n","            Context vectors observed in each round of the logged bandit feedback, i.e., :math:`x_t`.\n","        action: array-like, shape (n_rounds,)\n","            Action sampled by behavior policy in each round of the logged bandit feedback, i.e., :math:`a_t`.\n","        reward: array-like, shape (n_rounds,)\n","            Observed rewards (or outcome) in each round, i.e., :math:`r_t`.\n","        pscore: array-like, shape (n_rounds,), default=None\n","            Action choice probabilities (propensity score) of a behavior policy\n","            in the training logged bandit feedback.\n","            When None is given, the the behavior policy is assumed to be a uniform one.\n","        position: array-like, shape (n_rounds,), default=None\n","            Position of recommendation interface where action was presented in each round of the given logged bandit data.\n","            If None is set, a regression model assumes that there is only one position.\n","            When `len_list` > 1, this position argument has to be set.\n","        action_dist: array-like, shape (n_rounds, n_actions, len_list), default=None\n","            Action choice probabilities of evaluation policy (can be deterministic), i.e., :math:`\\\\pi_e(a_t|x_t)`.\n","            When either of 'iw' or 'mrdr' is used as the 'fitting_method' argument, then `action_dist` must be given.\n","        n_folds: int, default=1\n","            Number of folds in the cross-fitting procedure.\n","            When 1 is given, the regression model is trained on the whole logged bandit feedback data.\n","            Please refer to https://arxiv.org/abs/2002.08536 about the details of the cross-fitting procedure.\n","        random_state: int, default=None\n","            `random_state` affects the ordering of the indices, which controls the randomness of each fold.\n","            See https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.KFold.html for the details.\n","        Returns\n","        -----------\n","        estimated_rewards_by_reg_model: array-like, shape (n_rounds, n_actions, len_list)\n","            Expected rewards of new data estimated by the regression model.\n","        \"\"\"\n","        check_bandit_feedback_inputs(\n","            context=context,\n","            action=action,\n","            reward=reward,\n","            pscore=pscore,\n","            position=position,\n","            action_context=self.action_context,\n","        )\n","        n_rounds = context.shape[0]\n","\n","        check_scalar(n_folds, \"n_folds\", int, min_val=1)\n","        check_random_state(random_state)\n","\n","        if position is None or self.len_list == 1:\n","            position = np.zeros_like(action)\n","        else:\n","            if position.max() >= self.len_list:\n","                raise ValueError(\n","                    f\"position elements must be smaller than len_list, but the maximum value is {position.max()} (>= {self.len_list})\"\n","                )\n","        if self.fitting_method in [\"iw\", \"mrdr\"]:\n","            if not (isinstance(action_dist, np.ndarray) and action_dist.ndim == 3):\n","                raise ValueError(\n","                    \"when fitting_method is either 'iw' or 'mrdr', action_dist (a 3-dimensional ndarray) must be given\"\n","                )\n","            if action_dist.shape != (n_rounds, self.n_actions, self.len_list):\n","                raise ValueError(\n","                    f\"shape of action_dist must be (n_rounds, n_actions, len_list)=({n_rounds, self.n_actions, self.len_list}), but is {action_dist.shape}\"\n","                )\n","        if pscore is None:\n","            pscore = np.ones_like(action) / self.n_actions\n","\n","        if n_folds == 1:\n","            self.fit(\n","                context=context,\n","                action=action,\n","                reward=reward,\n","                pscore=pscore,\n","                position=position,\n","                action_dist=action_dist,\n","            )\n","            return self.predict(context=context)\n","        else:\n","            estimated_rewards_by_reg_model = np.zeros(\n","                (n_rounds, self.n_actions, self.len_list)\n","            )\n","        kf = KFold(n_splits=n_folds, shuffle=True, random_state=random_state)\n","        kf.get_n_splits(context)\n","        for train_idx, test_idx in kf.split(context):\n","            action_dist_tr = (\n","                action_dist[train_idx] if action_dist is not None else action_dist\n","            )\n","            self.fit(\n","                context=context[train_idx],\n","                action=action[train_idx],\n","                reward=reward[train_idx],\n","                pscore=pscore[train_idx],\n","                position=position[train_idx],\n","                action_dist=action_dist_tr,\n","            )\n","            estimated_rewards_by_reg_model[test_idx, :, :] = self.predict(\n","                context=context[test_idx]\n","            )\n","        return estimated_rewards_by_reg_model\n","\n","    def _pre_process_for_reg_model(\n","        self,\n","        context: np.ndarray,\n","        action: np.ndarray,\n","        action_context: np.ndarray,\n","    ) -> np.ndarray:\n","        \"\"\"Preprocess feature vectors to train a regression model.\n","        Note\n","        -----\n","        Please override this method if you want to use another feature enginnering\n","        for training the regression model.\n","        Parameters\n","        -----------\n","        context: array-like, shape (n_rounds,)\n","            Context vectors observed in each round of the logged bandit feedback, i.e., :math:`x_t`.\n","        action: array-like, shape (n_rounds,)\n","            Action sampled by behavior policy in each round of the logged bandit feedback, i.e., :math:`a_t`.\n","        action_context: array-like, shape shape (n_actions, dim_action_context)\n","            Context vector characterizing action (i.e., vector representation of each action).\n","        \"\"\"\n","        return np.c_[context, action_context[action]]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"eJtM3cS7sNO0"},"source":["## Estimators"]},{"cell_type":"code","metadata":{"id":"mFQzKhqZwdqF"},"source":["# from ..utils import check_array\n","# from ..utils import check_ope_inputs\n","# from ..utils import estimate_confidence_interval_by_bootstrap\n","# from .helper import estimate_bias_in_ope\n","# from .helper import estimate_high_probability_upper_bound_bias"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"cellView":"form","id":"7DXsSwoewN3P"},"source":["#@markdown estimator classes\n","@dataclass\n","class BaseOffPolicyEstimator(metaclass=ABCMeta):\n","    \"\"\"Base class for OPE estimators.\"\"\"\n","\n","    @abstractmethod\n","    def _estimate_round_rewards(self) -> np.ndarray:\n","        \"\"\"Estimate round-wise (or sample-wise) rewards.\"\"\"\n","        raise NotImplementedError\n","\n","    @abstractmethod\n","    def estimate_policy_value(self) -> float:\n","        \"\"\"Estimate the policy value of evaluation policy.\"\"\"\n","        raise NotImplementedError\n","\n","    @abstractmethod\n","    def estimate_interval(self) -> Dict[str, float]:\n","        \"\"\"Estimate confidence interval of policy value by nonparametric bootstrap procedure.\"\"\"\n","        raise NotImplementedError\n","\n","\n","@dataclass\n","class ReplayMethod(BaseOffPolicyEstimator):\n","    \"\"\"Relpay Method (RM).\n","\n","    Note\n","    -------\n","    Replay Method (RM) estimates the policy value of evaluation policy :math:`\\\\pi_e` by\n","\n","    .. math::\n","\n","        \\\\hat{V}_{\\\\mathrm{RM}} (\\\\pi_e; \\\\mathcal{D}) :=\n","        \\\\frac{\\\\mathbb{E}_{\\\\mathcal{D}}[\\\\mathbb{I} \\\\{ \\\\pi_e (x_t) = a_t \\\\} r_t ]}{\\\\mathbb{E}_{\\\\mathcal{D}}[\\\\mathbb{I} \\\\{ \\\\pi_e (x_t) = a_t \\\\}]},\n","\n","    where :math:`\\\\mathcal{D}=\\\\{(x_t,a_t,r_t)\\\\}_{t=1}^{T}` is logged bandit feedback data with :math:`T` rounds collected by\n","    a behavior policy :math:`\\\\pi_b`. :math:`\\\\pi_e: \\\\mathcal{X} \\\\rightarrow \\\\mathcal{A}` is the function\n","    representing action choices by the evaluation policy realized during offline bandit simulation.\n","    :math:`\\\\mathbb{E}_{\\\\mathcal{D}}[\\\\cdot]` is the empirical average over :math:`T` observations in :math:`\\\\mathcal{D}`.\n","\n","    Parameters\n","    ----------\n","    estimator_name: str, default='rm'.\n","        Name of the estimator.\n","\n","    References\n","    ------------\n","    Lihong Li, Wei Chu, John Langford, and Xuanhui Wang.\n","    \"Unbiased Offline Evaluation of Contextual-bandit-based News Article Recommendation Algorithms.\", 2011.\n","\n","    \"\"\"\n","\n","    estimator_name: str = \"rm\"\n","\n","    def _estimate_round_rewards(\n","        self,\n","        reward: np.ndarray,\n","        action: np.ndarray,\n","        action_dist: np.ndarray,\n","        position: Optional[np.ndarray] = None,\n","        **kwargs,\n","    ) -> np.ndarray:\n","        \"\"\"Estimate round-wise (or sample-wise) rewards.\n","\n","        Parameters\n","        ------------\n","        reward: array-like, shape (n_rounds,)\n","            Reward observed in each round of the logged bandit feedback, i.e., :math:`r_t`.\n","\n","        action: array-like, shape (n_rounds,)\n","            Action sampled by behavior policy in each round of the logged bandit feedback, i.e., :math:`a_t`.\n","\n","        action_dist: array-like, shape (n_rounds, n_actions, len_list)\n","            Action choice probabilities of evaluation policy (must be deterministic), i.e., :math:`\\\\pi_e(a_t|x_t)`.\n","\n","        position: array-like, shape (n_rounds,), default=None\n","            Position of recommendation interface where action was presented in each round of the given logged bandit data.\n","            When None is given, the effect of position on the reward will be ignored.\n","            (If only one action is chosen and there is no posion, then you can just ignore this argument.)\n","\n","        Returns\n","        ----------\n","        estimated_rewards: array-like, shape (n_rounds,)\n","            Rewards of each round estimated by the Replay Method.\n","\n","        \"\"\"\n","        if position is None:\n","            position = np.zeros(action_dist.shape[0], dtype=int)\n","        action_match = np.array(\n","            action_dist[np.arange(action.shape[0]), action, position] == 1\n","        )\n","        estimated_rewards = np.zeros_like(action_match)\n","        if action_match.sum() > 0.0:\n","            estimated_rewards = action_match * reward / action_match.mean()\n","        return estimated_rewards\n","\n","    def estimate_policy_value(\n","        self,\n","        reward: np.ndarray,\n","        action: np.ndarray,\n","        action_dist: np.ndarray,\n","        position: Optional[np.ndarray] = None,\n","        **kwargs,\n","    ) -> float:\n","        \"\"\"Estimate the policy value of evaluation policy.\n","\n","        Parameters\n","        ------------\n","        reward: array-like, shape (n_rounds,)\n","            Reward observed in each round of the logged bandit feedback, i.e., :math:`r_t`.\n","\n","        action: array-like, shape (n_rounds,)\n","            Action sampled by behavior policy in each round of the logged bandit feedback, i.e., :math:`a_t`.\n","\n","        action_dist: array-like, shape (n_rounds, n_actions, len_list)\n","            Action choice probabilities of evaluation policy (must be deterministic), i.e., :math:`\\\\pi_e(a_t|x_t)`.\n","\n","        position: array-like, shape (n_rounds,), default=None\n","            Position of recommendation interface where action was presented in each round of the given logged bandit data.\n","            When None is given, the effect of position on the reward will be ignored.\n","            (If only one action is chosen and there is no posion, then you can just ignore this argument.)\n","\n","        Returns\n","        ----------\n","        V_hat: float\n","            Estimated policy value (performance) of a given evaluation policy.\n","\n","        \"\"\"\n","        check_array(array=reward, name=\"reward\", expected_dim=1)\n","        check_array(array=action, name=\"action\", expected_dim=1)\n","        check_ope_inputs(\n","            action_dist=action_dist, position=position, action=action, reward=reward\n","        )\n","        if position is None:\n","            position = np.zeros(action_dist.shape[0], dtype=int)\n","\n","        return self._estimate_round_rewards(\n","            reward=reward,\n","            action=action,\n","            position=position,\n","            action_dist=action_dist,\n","        ).mean()\n","\n","    def estimate_interval(\n","        self,\n","        reward: np.ndarray,\n","        action: np.ndarray,\n","        action_dist: np.ndarray,\n","        position: Optional[np.ndarray] = None,\n","        alpha: float = 0.05,\n","        n_bootstrap_samples: int = 100,\n","        random_state: Optional[int] = None,\n","        **kwargs,\n","    ) -> Dict[str, float]:\n","        \"\"\"Estimate confidence interval of policy value by nonparametric bootstrap procedure.\n","\n","        Parameters\n","        ----------\n","        reward: array-like, shape (n_rounds,)\n","            Reward observed in each round of the logged bandit feedback, i.e., :math:`r_t`.\n","\n","        action: array-like, shape (n_rounds,)\n","            Action sampled by behavior policy in each round of the logged bandit feedback, i.e., :math:`a_t`.\n","\n","        action_dist: array-like, shape (n_rounds, n_actions, len_list)\n","            Action choice probabilities of evaluation policy (must be deterministic), i.e., :math:`\\\\pi_e(a_t|x_t)`.\n","\n","        position: array-like, shape (n_rounds,), default=None\n","            Position of recommendation interface where action was presented in each round of the given logged bandit data.\n","            When None is given, the effect of position on the reward will be ignored.\n","            (If only one action is chosen and there is no posion, then you can just ignore this argument.)\n","\n","        alpha: float, default=0.05\n","            Significance level.\n","\n","        n_bootstrap_samples: int, default=10000\n","            Number of resampling performed in the bootstrap procedure.\n","\n","        random_state: int, default=None\n","            Controls the random seed in bootstrap sampling.\n","\n","        Returns\n","        ----------\n","        estimated_confidence_interval: Dict[str, float]\n","            Dictionary storing the estimated mean and upper-lower confidence bounds.\n","\n","        \"\"\"\n","        check_array(array=reward, name=\"reward\", expected_dim=1)\n","        check_array(array=action, name=\"action\", expected_dim=1)\n","        check_ope_inputs(\n","            action_dist=action_dist, position=position, action=action, reward=reward\n","        )\n","        if position is None:\n","            position = np.zeros(action_dist.shape[0], dtype=int)\n","\n","        estimated_round_rewards = self._estimate_round_rewards(\n","            reward=reward,\n","            action=action,\n","            position=position,\n","            action_dist=action_dist,\n","        )\n","        return estimate_confidence_interval_by_bootstrap(\n","            samples=estimated_round_rewards,\n","            alpha=alpha,\n","            n_bootstrap_samples=n_bootstrap_samples,\n","            random_state=random_state,\n","        )\n","\n","\n","@dataclass\n","class InverseProbabilityWeighting(BaseOffPolicyEstimator):\n","    \"\"\"Inverse Probability Weighting (IPW) Estimator.\n","\n","    Note\n","    -------\n","    Inverse Probability Weighting (IPW) estimates the policy value of evaluation policy :math:`\\\\pi_e` by\n","\n","    .. math::\n","\n","        \\\\hat{V}_{\\\\mathrm{IPW}} (\\\\pi_e; \\\\mathcal{D}) := \\\\mathbb{E}_{\\\\mathcal{D}} [ w(x_t,a_t) r_t],\n","\n","    where :math:`\\\\mathcal{D}=\\\\{(x_t,a_t,r_t)\\\\}_{t=1}^{T}` is logged bandit feedback data with :math:`T` rounds collected by\n","    a behavior policy :math:`\\\\pi_b`. :math:`w(x,a):=\\\\pi_e (a|x)/\\\\pi_b (a|x)` is the importance weight given :math:`x` and :math:`a`.\n","    :math:`\\\\mathbb{E}_{\\\\mathcal{D}}[\\\\cdot]` is the empirical average over :math:`T` observations in :math:`\\\\mathcal{D}`.\n","    When the weight-clipping is applied, a large importance weight is clipped as :math:`\\\\hat{w}(x,a) := \\\\min \\\\{ \\\\lambda, w(x,a) \\\\}`\n","    where :math:`\\\\lambda (>0)` is a hyperparameter that decides a maximum allowed importance weight.\n","\n","    IPW re-weights the rewards by the ratio of the evaluation policy and behavior policy (importance weight).\n","    When the behavior policy is known, IPW is unbiased and consistent for the true policy value.\n","    However, it can have a large variance, especially when the evaluation policy significantly deviates from the behavior policy.\n","\n","    Parameters\n","    ------------\n","    lambda_: float, default=np.inf\n","        A maximum possible value of the importance weight.\n","        When a positive finite value is given, importance weights larger than `lambda_` will be clipped.\n","\n","    estimator_name: str, default='ipw'.\n","        Name of the estimator.\n","\n","    References\n","    ------------\n","    Alex Strehl, John Langford, Lihong Li, and Sham M Kakade.\n","    \"Learning from Logged Implicit Exploration Data\"., 2010.\n","\n","    Miroslav Dudík, Dumitru Erhan, John Langford, and Lihong Li.\n","    \"Doubly Robust Policy Evaluation and Optimization.\", 2014.\n","\n","    Yi Su, Maria Dimakopoulou, Akshay Krishnamurthy, and Miroslav Dudik.\n","    \"Doubly Robust Off-Policy Evaluation with Shrinkage.\", 2020.\n","\n","    \"\"\"\n","\n","    lambda_: float = np.inf\n","    estimator_name: str = \"ipw\"\n","\n","    def __post_init__(self) -> None:\n","        \"\"\"Initialize Class.\"\"\"\n","        check_scalar(\n","            self.lambda_,\n","            name=\"lambda_\",\n","            target_type=(int, float),\n","            min_val=0.0,\n","        )\n","        if self.lambda_ != self.lambda_:\n","            raise ValueError(\"lambda_ must not be nan\")\n","\n","    def _estimate_round_rewards(\n","        self,\n","        reward: np.ndarray,\n","        action: np.ndarray,\n","        pscore: np.ndarray,\n","        action_dist: np.ndarray,\n","        position: Optional[np.ndarray] = None,\n","        **kwargs,\n","    ) -> np.ndarray:\n","        \"\"\"Estimate round-wise (or sample-wise) rewards.\n","\n","        Parameters\n","        ----------\n","        reward: array-like or Tensor, shape (n_rounds,)\n","            Reward observed in each round of the logged bandit feedback, i.e., :math:`r_t`.\n","\n","        action: array-like or Tensor, shape (n_rounds,)\n","            Action sampled by behavior policy in each round of the logged bandit feedback, i.e., :math:`a_t`.\n","\n","        pscore: array-like or Tensor, shape (n_rounds,)\n","            Action choice probabilities of behavior policy (propensity scores), i.e., :math:`\\\\pi_b(a_t|x_t)`.\n","\n","        action_dist: array-like or Tensor, shape (n_rounds, n_actions, len_list)\n","            Action choice probabilities of evaluation policy (can be deterministic), i.e., :math:`\\\\pi_e(a_t|x_t)`.\n","\n","        position: array-like or Tensor, shape (n_rounds,), default=None\n","            Position of recommendation interface where action was presented in each round of the given logged bandit data.\n","            When None is given, the effect of position on the reward will be ignored.\n","            (If only one action is chosen and there is no posion, then you can just ignore this argument.)\n","\n","        Returns\n","        ----------\n","        estimated_rewards: array-like or Tensor, shape (n_rounds,)\n","            Rewards of each round estimated by IPW.\n","\n","        \"\"\"\n","        if position is None:\n","            position = np.zeros(action_dist.shape[0], dtype=int)\n","        iw = action_dist[np.arange(action.shape[0]), action, position] / pscore\n","        # weight clipping\n","        if isinstance(iw, np.ndarray):\n","            iw = np.minimum(iw, self.lambda_)\n","        return reward * iw\n","\n","    def estimate_policy_value(\n","        self,\n","        reward: np.ndarray,\n","        action: np.ndarray,\n","        pscore: np.ndarray,\n","        action_dist: np.ndarray,\n","        position: Optional[np.ndarray] = None,\n","        **kwargs,\n","    ) -> np.ndarray:\n","        \"\"\"Estimate the policy value of evaluation policy.\n","\n","        Parameters\n","        ----------\n","        reward: array-like, shape (n_rounds,)\n","            Reward observed in each round of the logged bandit feedback, i.e., :math:`r_t`.\n","\n","        action: array-like, shape (n_rounds,)\n","            Action sampled by behavior policy in each round of the logged bandit feedback, i.e., :math:`a_t`.\n","\n","        pscore: array-like, shape (n_rounds,)\n","            Action choice probabilities of behavior policy (propensity scores), i.e., :math:`\\\\pi_b(a_t|x_t)`.\n","\n","        action_dist: array-like, shape (n_rounds, n_actions, len_list)\n","            Action choice probabilities of evaluation policy (can be deterministic), i.e., :math:`\\\\pi_e(a_t|x_t)`.\n","\n","        position: array-like, shape (n_rounds,), default=None\n","            Position of recommendation interface where action was presented in each round of the given logged bandit data.\n","            When None is given, the effect of position on the reward will be ignored.\n","            (If only one action is chosen and there is no posion, then you can just ignore this argument.)\n","\n","        Returns\n","        ----------\n","        V_hat: float\n","            Estimated policy value (performance) of a given evaluation policy.\n","\n","        \"\"\"\n","        check_array(array=reward, name=\"reward\", expected_dim=1)\n","        check_array(array=action, name=\"action\", expected_dim=1)\n","        check_array(array=pscore, name=\"pscore\", expected_dim=1)\n","        check_ope_inputs(\n","            action_dist=action_dist,\n","            position=position,\n","            action=action,\n","            reward=reward,\n","            pscore=pscore,\n","        )\n","        if position is None:\n","            position = np.zeros(action_dist.shape[0], dtype=int)\n","\n","        return self._estimate_round_rewards(\n","            reward=reward,\n","            action=action,\n","            position=position,\n","            pscore=pscore,\n","            action_dist=action_dist,\n","        ).mean()\n","\n","    def estimate_interval(\n","        self,\n","        reward: np.ndarray,\n","        action: np.ndarray,\n","        pscore: np.ndarray,\n","        action_dist: np.ndarray,\n","        position: Optional[np.ndarray] = None,\n","        alpha: float = 0.05,\n","        n_bootstrap_samples: int = 10000,\n","        random_state: Optional[int] = None,\n","        **kwargs,\n","    ) -> Dict[str, float]:\n","        \"\"\"Estimate confidence interval of policy value by nonparametric bootstrap procedure.\n","\n","        Parameters\n","        ----------\n","        reward: array-like, shape (n_rounds,)\n","            Reward observed in each round of the logged bandit feedback, i.e., :math:`r_t`.\n","\n","        action: array-like, shape (n_rounds,)\n","            Action sampled by behavior policy in each round of the logged bandit feedback, i.e., :math:`a_t`.\n","\n","        pscore: array-like, shape (n_rounds,)\n","            Action choice probabilities of behavior policy (propensity scores), i.e., :math:`\\\\pi_b(a_t|x_t)`.\n","\n","        action_dist: array-like, shape (n_rounds, n_actions, len_list)\n","            Action choice probabilities of evaluation policy (can be deterministic), i.e., :math:`\\\\pi_e(a_t|x_t)`.\n","\n","        position: array-like, shape (n_rounds,), default=None\n","            Position of recommendation interface where action was presented in each round of the given logged bandit data.\n","            When None is given, the effect of position on the reward will be ignored.\n","            (If only one action is chosen and there is no posion, then you can just ignore this argument.)\n","\n","        alpha: float, default=0.05\n","            Significance level.\n","\n","        n_bootstrap_samples: int, default=10000\n","            Number of resampling performed in the bootstrap procedure.\n","\n","        random_state: int, default=None\n","            Controls the random seed in bootstrap sampling.\n","\n","        Returns\n","        ----------\n","        estimated_confidence_interval: Dict[str, float]\n","            Dictionary storing the estimated mean and upper-lower confidence bounds.\n","\n","        \"\"\"\n","        check_array(array=reward, name=\"reward\", expected_dim=1)\n","        check_array(array=action, name=\"action\", expected_dim=1)\n","        check_array(array=pscore, name=\"pscore\", expected_dim=1)\n","        check_ope_inputs(\n","            action_dist=action_dist,\n","            position=position,\n","            action=action,\n","            reward=reward,\n","            pscore=pscore,\n","        )\n","        if position is None:\n","            position = np.zeros(action_dist.shape[0], dtype=int)\n","\n","        estimated_round_rewards = self._estimate_round_rewards(\n","            reward=reward,\n","            action=action,\n","            position=position,\n","            pscore=pscore,\n","            action_dist=action_dist,\n","        )\n","        return estimate_confidence_interval_by_bootstrap(\n","            samples=estimated_round_rewards,\n","            alpha=alpha,\n","            n_bootstrap_samples=n_bootstrap_samples,\n","            random_state=random_state,\n","        )\n","\n","    def _estimate_mse_score(\n","        self,\n","        reward: np.ndarray,\n","        action: np.ndarray,\n","        pscore: np.ndarray,\n","        action_dist: np.ndarray,\n","        position: Optional[np.ndarray] = None,\n","        use_bias_upper_bound: bool = True,\n","        delta: float = 0.05,\n","        **kwargs,\n","    ) -> float:\n","        \"\"\"Estimate the MSE score of a given clipping hyperparameter to conduct hyperparameter tuning.\n","\n","        Parameters\n","        ----------\n","        reward: array-like, shape (n_rounds,)\n","            Reward observed in each round of the logged bandit feedback, i.e., :math:`r_t`.\n","\n","        action: array-like, shape (n_rounds,)\n","            Action sampled by behavior policy in each round of the logged bandit feedback, i.e., :math:`a_t`.\n","\n","        pscore: array-like, shape (n_rounds,)\n","            Action choice probabilities of behavior policy (propensity scores), i.e., :math:`\\\\pi_b(a_t|x_t)`.\n","\n","        action_dist: array-like, shape (n_rounds, n_actions, len_list)\n","            Action choice probabilities of evaluation policy (can be deterministic), i.e., :math:`\\\\pi_e(a_t|x_t)`.\n","\n","        position: array-like, shape (n_rounds,), default=None\n","            Position of recommendation interface where action was presented in each round of the given logged bandit data.\n","\n","        use_bias_upper_bound: bool, default=True\n","            Whether to use bias upper bound in hyperparameter tuning.\n","            If False, direct bias estimator is used to estimate the MSE.\n","\n","        delta: float, default=0.05\n","            A confidence delta to construct a high probability upper bound based on the Bernstein’s inequality.\n","\n","        Returns\n","        ----------\n","        estimated_mse_score: float\n","            Estimated MSE score of a given clipping hyperparameter `lambda_`.\n","            MSE score is the sum of (high probability) upper bound of bias and the sample variance.\n","            This is estimated using the automatic hyperparameter tuning procedure\n","            based on Section 5 of Su et al.(2020).\n","\n","        \"\"\"\n","        n_rounds = reward.shape[0]\n","        # estimate the sample variance of IPW with clipping\n","        sample_variance = np.var(\n","            self._estimate_round_rewards(\n","                reward=reward,\n","                action=action,\n","                pscore=pscore,\n","                action_dist=action_dist,\n","                position=position,\n","            )\n","        )\n","        sample_variance /= n_rounds\n","\n","        # estimate the (high probability) upper bound of the bias of IPW with clipping\n","        iw = action_dist[np.arange(n_rounds), action, position] / pscore\n","        if use_bias_upper_bound:\n","            bias_term = estimate_high_probability_upper_bound_bias(\n","                reward=reward, iw=iw, iw_hat=np.minimum(iw, self.lambda_), delta=delta\n","            )\n","        else:\n","            bias_term = estimate_bias_in_ope(\n","                reward=reward,\n","                iw=iw,\n","                iw_hat=np.minimum(iw, self.lambda_),\n","            )\n","        estimated_mse_score = sample_variance + (bias_term ** 2)\n","\n","        return estimated_mse_score\n","\n","\n","@dataclass\n","class SelfNormalizedInverseProbabilityWeighting(InverseProbabilityWeighting):\n","    \"\"\"Self-Normalized Inverse Probability Weighting (SNIPW) Estimator.\n","\n","    Note\n","    -------\n","    Self-Normalized Inverse Probability Weighting (SNIPW) estimates the policy value of evaluation policy :math:`\\\\pi_e` by\n","\n","    .. math::\n","\n","        \\\\hat{V}_{\\\\mathrm{SNIPW}} (\\\\pi_e; \\\\mathcal{D}) :=\n","        \\\\frac{\\\\mathbb{E}_{\\\\mathcal{D}} [w(x_t,a_t) r_t]}{ \\\\mathbb{E}_{\\\\mathcal{D}} [w(x_t,a_t)]},\n","\n","    where :math:`\\\\mathcal{D}=\\\\{(x_t,a_t,r_t)\\\\}_{t=1}^{T}` is logged bandit feedback data with :math:`T` rounds collected by\n","    a behavior policy :math:`\\\\pi_b`. :math:`w(x,a):=\\\\pi_e (a|x)/\\\\pi_b (a|x)` is the importance weight given :math:`x` and :math:`a`.\n","    :math:`\\\\mathbb{E}_{\\\\mathcal{D}}[\\\\cdot]` is the empirical average over :math:`T` observations in :math:`\\\\mathcal{D}`.\n","\n","    SNIPW re-weights the observed rewards by the self-normalized importance weihgt.\n","    This estimator is not unbiased even when the behavior policy is known.\n","    However, it is still consistent for the true policy value and increases the stability in some senses.\n","    See the references for the detailed discussions.\n","\n","    Parameters\n","    ----------\n","    estimator_name: str, default='snipw'.\n","        Name of the estimator.\n","\n","    References\n","    ----------\n","    Adith Swaminathan and Thorsten Joachims.\n","    \"The Self-normalized Estimator for Counterfactual Learning.\", 2015.\n","\n","    Nathan Kallus and Masatoshi Uehara.\n","    \"Intrinsically Efficient, Stable, and Bounded Off-Policy Evaluation for Reinforcement Learning.\", 2019.\n","\n","    \"\"\"\n","\n","    estimator_name: str = \"snipw\"\n","\n","    def _estimate_round_rewards(\n","        self,\n","        reward: np.ndarray,\n","        action: np.ndarray,\n","        pscore: np.ndarray,\n","        action_dist: np.ndarray,\n","        position: Optional[np.ndarray] = None,\n","        **kwargs,\n","    ) -> np.ndarray:\n","        \"\"\"Estimate round-wise (or sample-wise) rewards.\n","\n","        Parameters\n","        ----------\n","        reward: array-like or Tensor, shape (n_rounds,)\n","            Reward observed in each round of the logged bandit feedback, i.e., :math:`r_t`.\n","\n","        action: array-like or Tensor, shape (n_rounds,)\n","            Action sampled by behavior policy in each round of the logged bandit feedback, i.e., :math:`a_t`.\n","\n","        pscore: array-like or Tensor, shape (n_rounds,)\n","            Action choice probabilities of behavior policy (propensity scores), i.e., :math:`\\\\pi_b(a_t|x_t)`.\n","\n","        action_dist: array-like or Tensor, shape (n_rounds, n_actions, len_list)\n","            Action choice probabilities of evaluation policy (can be deterministic), i.e., :math:`\\\\pi_e(a_t|x_t)`.\n","\n","        position: array-like or Tensor, shape (n_rounds,), default=None\n","            Position of recommendation interface where action was presented in each round of the given logged bandit data.\n","\n","        Returns\n","        ----------\n","        estimated_rewards: array-like or Tensor, shape (n_rounds,)\n","            Rewards of each round estimated by the SNIPW estimator.\n","\n","        \"\"\"\n","        if position is None:\n","            position = np.zeros(action_dist.shape[0], dtype=int)\n","        iw = action_dist[np.arange(action.shape[0]), action, position] / pscore\n","        return reward * iw / iw.mean()\n","\n","\n","@dataclass\n","class DirectMethod(BaseOffPolicyEstimator):\n","    \"\"\"Direct Method (DM).\n","\n","    Note\n","    -------\n","    DM first learns a supervised machine learning model, such as ridge regression and gradient boosting,\n","    to estimate the mean reward function (:math:`q(x,a) = \\\\mathbb{E}[r|x,a]`).\n","    It then uses it to estimate the policy value as follows.\n","\n","    .. math::\n","\n","        \\\\hat{V}_{\\\\mathrm{DM}} (\\\\pi_e; \\\\mathcal{D}, \\\\hat{q})\n","        &:= \\\\mathbb{E}_{\\\\mathcal{D}} \\\\left[ \\\\sum_{a \\\\in \\\\mathcal{A}} \\\\hat{q} (x_t,a) \\\\pi_e(a|x_t) \\\\right],    \\\\\\\\\n","        & =  \\\\mathbb{E}_{\\\\mathcal{D}}[\\\\hat{q} (x_t,\\\\pi_e)],\n","\n","    where :math:`\\\\mathcal{D}=\\\\{(x_t,a_t,r_t)\\\\}_{t=1}^{T}` is logged bandit feedback data with :math:`T` rounds collected by\n","    a behavior policy :math:`\\\\pi_b`. :math:`\\\\mathbb{E}_{\\\\mathcal{D}}[\\\\cdot]` is the empirical average over :math:`T` observations in :math:`\\\\mathcal{D}`.\n","    :math:`\\\\hat{q} (x,a)` is an estimated expected reward given :math:`x` and :math:`a`.\n","    :math:`\\\\hat{q} (x_t,\\\\pi):= \\\\mathbb{E}_{a \\\\sim \\\\pi(a|x)}[\\\\hat{q}(x,a)]` is the expectation of the estimated reward function over :math:`\\\\pi`.\n","    To estimate the mean reward function, please use `obp.ope.regression_model.RegressionModel`, which supports several fitting methods specific to OPE.\n","\n","    If the regression model (:math:`\\\\hat{q}`) is a good approximation to the true mean reward function,\n","    this estimator accurately estimates the policy value of the evaluation policy.\n","    If the regression function fails to approximate the mean reward function well,\n","    however, the final estimator is no longer consistent.\n","\n","    Parameters\n","    ----------\n","    estimator_name: str, default='dm'.\n","        Name of the estimator.\n","\n","    References\n","    ----------\n","    Alina Beygelzimer and John Langford.\n","    \"The offset tree for learning with partial labels.\", 2009.\n","\n","    Miroslav Dudík, Dumitru Erhan, John Langford, and Lihong Li.\n","    \"Doubly Robust Policy Evaluation and Optimization.\", 2014.\n","\n","    \"\"\"\n","\n","    estimator_name: str = \"dm\"\n","\n","    def _estimate_round_rewards(\n","        self,\n","        action_dist: np.ndarray,\n","        estimated_rewards_by_reg_model: np.ndarray,\n","        position: Optional[np.ndarray] = None,\n","        **kwargs,\n","    ) -> np.ndarray:\n","        \"\"\"Estimate the policy value of evaluation policy.\n","\n","        Parameters\n","        ----------\n","        action_dist: array-like or Tensor, shape (n_rounds, n_actions, len_list)\n","            Action choice probabilities of evaluation policy (can be deterministic), i.e., :math:`\\\\pi_e(a_t|x_t)`.\n","\n","        estimated_rewards_by_reg_model: array-like or Tensor, shape (n_rounds, n_actions, len_list)\n","            Expected rewards given context, action, and position estimated by regression model, i.e., :math:`\\\\hat{q}(x_t,a_t)`.\n","\n","        position: array-like or Tensor, shape (n_rounds,), default=None\n","            Position of recommendation interface where action was presented in each round of the given logged bandit data.\n","            When None is given, the effect of position on the reward will be ignored.\n","            (If only one action is chosen and there is no posion, then you can just ignore this argument.)\n","\n","        Returns\n","        ----------\n","        estimated_rewards: array-like or Tensor, shape (n_rounds,)\n","            Rewards of each round estimated by the DM estimator.\n","\n","        \"\"\"\n","        if position is None:\n","            position = np.zeros(action_dist.shape[0], dtype=int)\n","        n_rounds = position.shape[0]\n","        q_hat_at_position = estimated_rewards_by_reg_model[\n","            np.arange(n_rounds), :, position\n","        ]\n","        pi_e_at_position = action_dist[np.arange(n_rounds), :, position]\n","\n","        if isinstance(action_dist, np.ndarray):\n","            return np.average(\n","                q_hat_at_position,\n","                weights=pi_e_at_position,\n","                axis=1,\n","            )\n","        else:\n","            raise ValueError(\"action must be 1D array\")\n","\n","    def estimate_policy_value(\n","        self,\n","        action_dist: np.ndarray,\n","        estimated_rewards_by_reg_model: np.ndarray,\n","        position: Optional[np.ndarray] = None,\n","        **kwargs,\n","    ) -> float:\n","        \"\"\"Estimate the policy value of evaluation policy.\n","\n","        Parameters\n","        ----------\n","        action_dist: array-like, shape (n_rounds, n_actions, len_list)\n","            Action choice probabilities of evaluation policy (can be deterministic), i.e., :math:`\\\\pi_e(a_t|x_t)`.\n","\n","        estimated_rewards_by_reg_model: array-like, shape (n_rounds, n_actions, len_list)\n","            Expected rewards given context, action, and position estimated by regression model, i.e., :math:`\\\\hat{q}(x_t,a_t)`.\n","\n","        position: array-like, shape (n_rounds,), default=None\n","            Position of recommendation interface where action was presented in each round of the given logged bandit data.\n","            When None is given, the effect of position on the reward will be ignored.\n","            (If only one action is chosen and there is no posion, then you can just ignore this argument.)\n","\n","        Returns\n","        ----------\n","        V_hat: float\n","            Estimated policy value (performance) of a given evaluation policy.\n","\n","        \"\"\"\n","        check_array(\n","            array=estimated_rewards_by_reg_model,\n","            name=\"estimated_rewards_by_reg_model\",\n","            expected_dim=3,\n","        )\n","        check_ope_inputs(\n","            action_dist=action_dist,\n","            estimated_rewards_by_reg_model=estimated_rewards_by_reg_model,\n","            position=position,\n","        )\n","        if position is None:\n","            position = np.zeros(action_dist.shape[0], dtype=int)\n","\n","        return self._estimate_round_rewards(\n","            position=position,\n","            estimated_rewards_by_reg_model=estimated_rewards_by_reg_model,\n","            action_dist=action_dist,\n","        ).mean()\n","\n","    def estimate_interval(\n","        self,\n","        action_dist: np.ndarray,\n","        estimated_rewards_by_reg_model: np.ndarray,\n","        position: Optional[np.ndarray] = None,\n","        alpha: float = 0.05,\n","        n_bootstrap_samples: int = 10000,\n","        random_state: Optional[int] = None,\n","        **kwargs,\n","    ) -> Dict[str, float]:\n","        \"\"\"Estimate confidence interval of policy value by nonparametric bootstrap procedure.\n","\n","        Parameters\n","        ----------\n","        action_dist: array-like, shape (n_rounds, n_actions, len_list)\n","            Action choice probabilities of evaluation policy (can be deterministic), i.e., :math:`\\\\pi_e(a_t|x_t)`.\n","\n","        estimated_rewards_by_reg_model: array-like, shape (n_rounds, n_actions, len_list)\n","            Expected rewards given context, action, and position estimated by regression model, i.e., :math:`\\\\hat{q}(x_t,a_t)`.\n","\n","        position: array-like, shape (n_rounds,), default=None\n","            Position of recommendation interface where action was presented in each round of the given logged bandit data.\n","            When None is given, the effect of position on the reward will be ignored.\n","            (If only one action is chosen and there is no posion, then you can just ignore this argument.)\n","\n","        alpha: float, default=0.05\n","            Significance level.\n","\n","        n_bootstrap_samples: int, default=10000\n","            Number of resampling performed in the bootstrap procedure.\n","\n","        random_state: int, default=None\n","            Controls the random seed in bootstrap sampling.\n","\n","        Returns\n","        ----------\n","        estimated_confidence_interval: Dict[str, float]\n","            Dictionary storing the estimated mean and upper-lower confidence bounds.\n","\n","        \"\"\"\n","        check_array(\n","            array=estimated_rewards_by_reg_model,\n","            name=\"estimated_rewards_by_reg_model\",\n","            expected_dim=3,\n","        )\n","        check_ope_inputs(\n","            action_dist=action_dist,\n","            estimated_rewards_by_reg_model=estimated_rewards_by_reg_model,\n","            position=position,\n","        )\n","        if position is None:\n","            position = np.zeros(action_dist.shape[0], dtype=int)\n","\n","        estimated_round_rewards = self._estimate_round_rewards(\n","            position=position,\n","            estimated_rewards_by_reg_model=estimated_rewards_by_reg_model,\n","            action_dist=action_dist,\n","        )\n","        return estimate_confidence_interval_by_bootstrap(\n","            samples=estimated_round_rewards,\n","            alpha=alpha,\n","            n_bootstrap_samples=n_bootstrap_samples,\n","            random_state=random_state,\n","        )\n","\n","\n","@dataclass\n","class DoublyRobust(BaseOffPolicyEstimator):\n","    \"\"\"Doubly Robust (DR) Estimator.\n","\n","    Note\n","    -------\n","    Similar to DM, DR first learns a supervised machine learning model, such as ridge regression and gradient boosting,\n","    to estimate the mean reward function (:math:`q(x,a) = \\\\mathbb{E}[r|x,a]`).\n","    It then uses it to estimate the policy value as follows.\n","\n","    .. math::\n","\n","        \\\\hat{V}_{\\\\mathrm{DR}} (\\\\pi_e; \\\\mathcal{D}, \\\\hat{q})\n","        := \\\\mathbb{E}_{\\\\mathcal{D}}[\\\\hat{q}(x_t,\\\\pi_e) +  w(x_t,a_t) (r_t - \\\\hat{q}(x_t,a_t))],\n","\n","    where :math:`\\\\mathcal{D}=\\\\{(x_t,a_t,r_t)\\\\}_{t=1}^{T}` is logged bandit feedback data with :math:`T` rounds collected by\n","    a behavior policy :math:`\\\\pi_b`.\n","    :math:`w(x,a):=\\\\pi_e (a|x)/\\\\pi_b (a|x)` is the importance weight given :math:`x` and :math:`a`.\n","    :math:`\\\\mathbb{E}_{\\\\mathcal{D}}[\\\\cdot]` is the empirical average over :math:`T` observations in :math:`\\\\mathcal{D}`.\n","    :math:`\\\\hat{q} (x,a)` is an estimated expected reward given :math:`x` and :math:`a`.\n","    :math:`\\\\hat{q} (x_t,\\\\pi):= \\\\mathbb{E}_{a \\\\sim \\\\pi(a|x)}[\\\\hat{q}(x,a)]` is the expectation of the estimated reward function over :math:`\\\\pi`.\n","    When the weight-clipping is applied, a large importance weight is clipped as :math:`\\\\hat{w}(x,a) := \\\\min \\\\{ \\\\lambda, w(x,a) \\\\}`\n","    where :math:`\\\\lambda (>0)` is a hyperparameter that decides a maximum allowed importance weight.\n","\n","    To estimate the mean reward function, please use `obp.ope.regression_model.RegressionModel`,\n","    which supports several fitting methods specific to OPE such as *more robust doubly robust*.\n","\n","    DR mimics IPW to use a weighted version of rewards, but DR also uses the estimated mean reward\n","    function (the regression model) as a control variate to decrease the variance.\n","    It preserves the consistency of IPW if either the importance weight or\n","    the mean reward estimator is accurate (a property called double robustness).\n","    Moreover, DR is semiparametric efficient when the mean reward estimator is correctly specified.\n","\n","    Parameters\n","    ----------\n","    lambda_: float, default=np.inf\n","        A maximum possible value of the importance weight.\n","        When a positive finite value is given, importance weights larger than `lambda_` will be clipped.\n","        DoublyRobust with a finite positive `lambda_` corresponds to Doubly Robust with Pessimistic Shrinkage of Su et al.(2020) or CAB-DR of Su et al.(2019).\n","\n","    estimator_name: str, default='dr'.\n","        Name of the estimator.\n","\n","    References\n","    ----------\n","    Miroslav Dudík, Dumitru Erhan, John Langford, and Lihong Li.\n","    \"Doubly Robust Policy Evaluation and Optimization.\", 2014.\n","\n","    Mehrdad Farajtabar, Yinlam Chow, and Mohammad Ghavamzadeh.\n","    \"More Robust Doubly Robust Off-policy Evaluation.\", 2018.\n","\n","    Yi Su, Lequn Wang, Michele Santacatterina, and Thorsten Joachims.\n","    \"CAB: Continuous Adaptive Blending Estimator for Policy Evaluation and Learning\", 2019.\n","\n","    Yi Su, Maria Dimakopoulou, Akshay Krishnamurthy, and Miroslav Dudík.\n","    \"Doubly robust off-policy evaluation with shrinkage.\", 2020.\n","\n","    \"\"\"\n","\n","    lambda_: float = np.inf\n","    estimator_name: str = \"dr\"\n","\n","    def __post_init__(self) -> None:\n","        \"\"\"Initialize Class.\"\"\"\n","        check_scalar(\n","            self.lambda_,\n","            name=\"lambda_\",\n","            target_type=(int, float),\n","            min_val=0.0,\n","        )\n","        if self.lambda_ != self.lambda_:\n","            raise ValueError(\"lambda_ must not be nan\")\n","\n","    def _estimate_round_rewards(\n","        self,\n","        reward: np.ndarray,\n","        action: np.ndarray,\n","        pscore: np.ndarray,\n","        action_dist: np.ndarray,\n","        estimated_rewards_by_reg_model: np.ndarray,\n","        position: Optional[np.ndarray] = None,\n","        **kwargs,\n","    ) -> np.ndarray:\n","        \"\"\"Estimate round-wise (or sample-wise) rewards.\n","\n","        Parameters\n","        ----------\n","        reward: array-like or Tensor, shape (n_rounds,)\n","            Reward observed in each round of the logged bandit feedback, i.e., :math:`r_t`.\n","\n","        action: array-like or Tensor, shape (n_rounds,)\n","            Action sampled by behavior policy in each round of the logged bandit feedback, i.e., :math:`a_t`.\n","\n","        pscore: array-like or Tensor, shape (n_rounds,)\n","            Action choice probabilities of behavior policy (propensity scores), i.e., :math:`\\\\pi_b(a_t|x_t)`.\n","\n","        action_dist: array-like or Tensor, shape (n_rounds, n_actions, len_list)\n","            Action choice probabilities of evaluation policy (can be deterministic), i.e., :math:`\\\\pi_e(a_t|x_t)`.\n","\n","        estimated_rewards_by_reg_model or Tensor: array-like, shape (n_rounds, n_actions, len_list)\n","            Expected rewards given context, action, and position estimated by regression model, i.e., :math:`\\\\hat{q}(x_t,a_t)`.\n","\n","        position: array-like or Tensor, shape (n_rounds,), default=None\n","            Position of recommendation interface where action was presented in each round of the given logged bandit data.\n","            When None is given, the effect of position on the reward will be ignored.\n","            (If only one action is chosen and there is no posion, then you can just ignore this argument.)\n","\n","        Returns\n","        ----------\n","        estimated_rewards: array-like or Tensor, shape (n_rounds,)\n","            Rewards of each round estimated by the DR estimator.\n","\n","        \"\"\"\n","        if position is None:\n","            position = np.zeros(action_dist.shape[0], dtype=int)\n","        n_rounds = action.shape[0]\n","        iw = action_dist[np.arange(n_rounds), action, position] / pscore\n","        # weight clipping\n","        if isinstance(iw, np.ndarray):\n","            iw = np.minimum(iw, self.lambda_)\n","        q_hat_at_position = estimated_rewards_by_reg_model[\n","            np.arange(n_rounds), :, position\n","        ]\n","        q_hat_factual = estimated_rewards_by_reg_model[\n","            np.arange(n_rounds), action, position\n","        ]\n","        pi_e_at_position = action_dist[np.arange(n_rounds), :, position]\n","\n","        if isinstance(reward, np.ndarray):\n","            estimated_rewards = np.average(\n","                q_hat_at_position,\n","                weights=pi_e_at_position,\n","                axis=1,\n","            )\n","        else:\n","            raise ValueError(\"reward must be 1D array\")\n","\n","        estimated_rewards += iw * (reward - q_hat_factual)\n","        return estimated_rewards\n","\n","    def estimate_policy_value(\n","        self,\n","        reward: np.ndarray,\n","        action: np.ndarray,\n","        pscore: np.ndarray,\n","        action_dist: np.ndarray,\n","        estimated_rewards_by_reg_model: np.ndarray,\n","        position: Optional[np.ndarray] = None,\n","    ) -> float:\n","        \"\"\"Estimate the policy value of evaluation policy.\n","\n","        Parameters\n","        ----------\n","        reward: array-like, shape (n_rounds,)\n","            Reward observed in each round of the logged bandit feedback, i.e., :math:`r_t`.\n","\n","        action: array-like, shape (n_rounds,)\n","            Action sampled by behavior policy in each round of the logged bandit feedback, i.e., :math:`a_t`.\n","\n","        pscore: array-like, shape (n_rounds,)\n","            Action choice probabilities of behavior policy (propensity scores), i.e., :math:`\\\\pi_b(a_t|x_t)`.\n","\n","        action_dist: array-like, shape (n_rounds, n_actions, len_list)\n","            Action choice probabilities of evaluation policy (can be deterministic), i.e., :math:`\\\\pi_e(a_t|x_t)`.\n","\n","        estimated_rewards_by_reg_model: array-like, shape (n_rounds, n_actions, len_list)\n","            Expected rewards given context, action, and position estimated by regression model, i.e., :math:`\\\\hat{q}(x_t,a_t)`.\n","\n","        position: array-like, shape (n_rounds,), default=None\n","            Position of recommendation interface where action was presented in each round of the given logged bandit data.\n","            When None is given, the effect of position on the reward will be ignored.\n","            (If only one action is chosen and there is no posion, then you can just ignore this argument.)\n","\n","        Returns\n","        ----------\n","        V_hat: float\n","            Policy value estimated by the DR estimator.\n","\n","        \"\"\"\n","        check_array(\n","            array=estimated_rewards_by_reg_model,\n","            name=\"estimated_rewards_by_reg_model\",\n","            expected_dim=3,\n","        )\n","        check_array(array=reward, name=\"reward\", expected_dim=1)\n","        check_array(array=action, name=\"action\", expected_dim=1)\n","        check_array(array=pscore, name=\"pscore\", expected_dim=1)\n","        check_ope_inputs(\n","            action_dist=action_dist,\n","            position=position,\n","            action=action,\n","            reward=reward,\n","            pscore=pscore,\n","            estimated_rewards_by_reg_model=estimated_rewards_by_reg_model,\n","        )\n","        if position is None:\n","            position = np.zeros(action_dist.shape[0], dtype=int)\n","\n","        return self._estimate_round_rewards(\n","            reward=reward,\n","            action=action,\n","            position=position,\n","            pscore=pscore,\n","            action_dist=action_dist,\n","            estimated_rewards_by_reg_model=estimated_rewards_by_reg_model,\n","        ).mean()\n","\n","    def estimate_interval(\n","        self,\n","        reward: np.ndarray,\n","        action: np.ndarray,\n","        pscore: np.ndarray,\n","        action_dist: np.ndarray,\n","        estimated_rewards_by_reg_model: np.ndarray,\n","        position: Optional[np.ndarray] = None,\n","        alpha: float = 0.05,\n","        n_bootstrap_samples: int = 10000,\n","        random_state: Optional[int] = None,\n","        **kwargs,\n","    ) -> Dict[str, float]:\n","        \"\"\"Estimate confidence interval of policy value by nonparametric bootstrap procedure.\n","\n","        Parameters\n","        ----------\n","        reward: array-like, shape (n_rounds,)\n","            Reward observed in each round of the logged bandit feedback, i.e., :math:`r_t`.\n","\n","        action: array-like, shape (n_rounds,)\n","            Action sampled by behavior policy in each round of the logged bandit feedback, i.e., :math:`a_t`.\n","\n","        pscore: array-like, shape (n_rounds,)\n","            Action choice probabilities of behavior policy (propensity scores), i.e., :math:`\\\\pi_b(a_t|x_t)`.\n","\n","        action_dist: array-like, shape (n_rounds, n_actions, len_list)\n","            Action choice probabilities of evaluation policy (can be deterministic), i.e., :math:`\\\\pi_e(a_t|x_t)`.\n","\n","        estimated_rewards_by_reg_model: array-like, shape (n_rounds, n_actions, len_list)\n","            Expected rewards given context, action, and position estimated by regression model, i.e., :math:`\\\\hat{q}(x_t,a_t)`.\n","\n","        position: array-like, shape (n_rounds,), default=None\n","            Position of recommendation interface where action was presented in each round of the given logged bandit data.\n","            When None is given, the effect of position on the reward will be ignored.\n","            (If only one action is chosen and there is no posion, then you can just ignore this argument.)\n","\n","        alpha: float, default=0.05\n","            Significance level.\n","\n","        n_bootstrap_samples: int, default=10000\n","            Number of resampling performed in the bootstrap procedure.\n","\n","        random_state: int, default=None\n","            Controls the random seed in bootstrap sampling.\n","\n","        Returns\n","        ----------\n","        estimated_confidence_interval: Dict[str, float]\n","            Dictionary storing the estimated mean and upper-lower confidence bounds.\n","\n","        \"\"\"\n","        check_array(\n","            array=estimated_rewards_by_reg_model,\n","            name=\"estimated_rewards_by_reg_model\",\n","            expected_dim=3,\n","        )\n","        check_array(array=reward, name=\"reward\", expected_dim=1)\n","        check_array(array=action, name=\"action\", expected_dim=1)\n","        check_array(array=pscore, name=\"pscore\", expected_dim=1)\n","        check_ope_inputs(\n","            action_dist=action_dist,\n","            position=position,\n","            action=action,\n","            reward=reward,\n","            pscore=pscore,\n","            estimated_rewards_by_reg_model=estimated_rewards_by_reg_model,\n","        )\n","        if position is None:\n","            position = np.zeros(action_dist.shape[0], dtype=int)\n","\n","        estimated_round_rewards = self._estimate_round_rewards(\n","            reward=reward,\n","            action=action,\n","            position=position,\n","            pscore=pscore,\n","            action_dist=action_dist,\n","            estimated_rewards_by_reg_model=estimated_rewards_by_reg_model,\n","        )\n","        return estimate_confidence_interval_by_bootstrap(\n","            samples=estimated_round_rewards,\n","            alpha=alpha,\n","            n_bootstrap_samples=n_bootstrap_samples,\n","            random_state=random_state,\n","        )\n","\n","    def _estimate_mse_score(\n","        self,\n","        reward: np.ndarray,\n","        action: np.ndarray,\n","        pscore: np.ndarray,\n","        action_dist: np.ndarray,\n","        estimated_rewards_by_reg_model: np.ndarray,\n","        position: Optional[np.ndarray] = None,\n","        use_bias_upper_bound: bool = True,\n","        delta: float = 0.05,\n","    ) -> float:\n","        \"\"\"Estimate the MSE score of a given clipping hyperparameter to conduct hyperparameter tuning.\n","\n","        Parameters\n","        ----------\n","        reward: array-like, shape (n_rounds,)\n","            Reward observed in each round of the logged bandit feedback, i.e., :math:`r_t`.\n","\n","        action: array-like, shape (n_rounds,)\n","            Action sampled by behavior policy in each round of the logged bandit feedback, i.e., :math:`a_t`.\n","\n","        pscore: array-like, shape (n_rounds,)\n","            Action choice probabilities of behavior policy (propensity scores), i.e., :math:`\\\\pi_b(a_t|x_t)`.\n","\n","        action_dist: array-like, shape (n_rounds, n_actions, len_list)\n","            Action choice probabilities of evaluation policy (can be deterministic), i.e., :math:`\\\\pi_e(a_t|x_t)`.\n","\n","        position: array-like, shape (n_rounds,), default=None\n","            Position of recommendation interface where action was presented in each round of the given logged bandit data.\n","            When None is given, the effect of position on the reward will be ignored.\n","            (If only one action is chosen and there is no posion, then you can just ignore this argument.)\n","\n","        estimated_rewards_by_reg_model: array-like, shape (n_rounds, n_actions, len_list)\n","            Expected rewards given context, action, and position estimated by regression model, i.e., :math:`\\\\hat{q}(x_t,a_t)`.\n","\n","        use_bias_upper_bound: bool, default=True\n","            Whether to use bias upper bound in hyperparameter tuning.\n","            If False, direct bias estimator is used to estimate the MSE.\n","\n","        delta: float, default=0.05\n","            A confidence delta to construct a high probability upper bound based on the Bernstein’s inequality.\n","\n","        Returns\n","        ----------\n","        estimated_mse_score: float\n","            Estimated MSE score of a given clipping hyperparameter `lambda_`.\n","            MSE score is the sum of (high probability) upper bound of bias and the sample variance.\n","            This is estimated using the automatic hyperparameter tuning procedure\n","            based on Section 5 of Su et al.(2020).\n","\n","        \"\"\"\n","        n_rounds = reward.shape[0]\n","        # estimate the sample variance of DR with clipping\n","        sample_variance = np.var(\n","            self._estimate_round_rewards(\n","                reward=reward,\n","                action=action,\n","                pscore=pscore,\n","                action_dist=action_dist,\n","                estimated_rewards_by_reg_model=estimated_rewards_by_reg_model,\n","                position=position,\n","            )\n","        )\n","        sample_variance /= n_rounds\n","\n","        # estimate the (high probability) upper bound of the bias of DR with clipping\n","        iw = action_dist[np.arange(n_rounds), action, position] / pscore\n","        if use_bias_upper_bound:\n","            bias_term = estimate_high_probability_upper_bound_bias(\n","                reward=reward,\n","                iw=iw,\n","                iw_hat=np.minimum(iw, self.lambda_),\n","                q_hat=estimated_rewards_by_reg_model[\n","                    np.arange(n_rounds), action, position\n","                ],\n","                delta=delta,\n","            )\n","        else:\n","            bias_term = estimate_bias_in_ope(\n","                reward=reward,\n","                iw=iw,\n","                iw_hat=np.minimum(iw, self.lambda_),\n","                q_hat=estimated_rewards_by_reg_model[\n","                    np.arange(n_rounds), action, position\n","                ],\n","            )\n","        estimated_mse_score = sample_variance + (bias_term ** 2)\n","\n","        return estimated_mse_score\n","\n","\n","@dataclass\n","class SelfNormalizedDoublyRobust(DoublyRobust):\n","    \"\"\"Self-Normalized Doubly Robust (SNDR) Estimator.\n","\n","    Note\n","    -------\n","    Self-Normalized Doubly Robust estimates the policy value of evaluation policy :math:`\\\\pi_e` by\n","\n","    .. math::\n","\n","        \\\\hat{V}_{\\\\mathrm{SNDR}} (\\\\pi_e; \\\\mathcal{D}, \\\\hat{q}) :=\n","        \\\\mathbb{E}_{\\\\mathcal{D}} \\\\left[\\\\hat{q}(x_t,\\\\pi_e) +  \\\\frac{w(x_t,a_t) (r_t - \\\\hat{q}(x_t,a_t))}{\\\\mathbb{E}_{\\\\mathcal{D}}[ w(x_t,a_t) ]} \\\\right],\n","\n","    where :math:`\\\\mathcal{D}=\\\\{(x_t,a_t,r_t)\\\\}_{t=1}^{T}` is logged bandit feedback data with :math:`T` rounds collected by\n","    a behavior policy :math:`\\\\pi_b`. :math:`w(x,a):=\\\\pi_e (a|x)/\\\\pi_b (a|x)` is the importance weight given :math:`x` and :math:`a`.\n","    :math:`\\\\mathbb{E}_{\\\\mathcal{D}}[\\\\cdot]` is the empirical average over :math:`T` observations in :math:`\\\\mathcal{D}`.\n","    :math:`\\\\hat{q} (x,a)` is an estimated expected reward given :math:`x` and :math:`a`.\n","    :math:`\\\\hat{q} (x_t,\\\\pi):= \\\\mathbb{E}_{a \\\\sim \\\\pi(a|x)}[\\\\hat{q}(x,a)]` is the expectation of the estimated reward function over :math:`\\\\pi`.\n","    To estimate the mean reward function, please use `obp.ope.regression_model.RegressionModel`.\n","\n","    Similar to Self-Normalized Inverse Probability Weighting, SNDR estimator applies the self-normalized importance weighting technique to\n","    increase the stability of the original Doubly Robust estimator.\n","\n","    Parameters\n","    ----------\n","    estimator_name: str, default='sndr'.\n","        Name of the estimator.\n","\n","    References\n","    ----------\n","    Miroslav Dudík, Dumitru Erhan, John Langford, and Lihong Li.\n","    \"Doubly Robust Policy Evaluation and Optimization.\", 2014.\n","\n","    Nathan Kallus and Masatoshi Uehara.\n","    \"Intrinsically Efficient, Stable, and Bounded Off-Policy Evaluation for Reinforcement Learning.\", 2019.\n","\n","    \"\"\"\n","\n","    estimator_name: str = \"sndr\"\n","\n","    def _estimate_round_rewards(\n","        self,\n","        reward: np.ndarray,\n","        action: np.ndarray,\n","        pscore: np.ndarray,\n","        action_dist: np.ndarray,\n","        estimated_rewards_by_reg_model: np.ndarray,\n","        position: Optional[np.ndarray] = None,\n","        **kwargs,\n","    ) -> np.ndarray:\n","        \"\"\"Estimate round-wise (or sample-wise) rewards.\n","\n","        Parameters\n","        ----------\n","        reward: array-like or Tensor, shape (n_rounds,)\n","            Reward observed in each round of the logged bandit feedback, i.e., :math:`r_t`.\n","\n","        action: array-like or Tensor, shape (n_rounds,)\n","            Action sampled by behavior policy in each round of the logged bandit feedback, i.e., :math:`a_t`.\n","\n","        pscore: array-like or Tensor, shape (n_rounds,)\n","            Action choice probabilities of behavior policy (propensity scores), i.e., :math:`\\\\pi_b(a_t|x_t)`.\n","\n","        action_dist: array-like or Tensor, shape (n_rounds, n_actions, len_list)\n","            Action choice probabilities of evaluation policy (can be deterministic), i.e., :math:`\\\\pi_e(a_t|x_t)`.\n","\n","        estimated_rewards_by_reg_model: array-like or Tensor, shape (n_rounds, n_actions, len_list)\n","            Expected rewards given context, action, and position estimated by regression model, i.e., :math:`\\\\hat{q}(x_t,a_t)`.\n","\n","        position: array-like or Tensor, shape (n_rounds,), default=None\n","            Position of recommendation interface where action was presented in each round of the given logged bandit data.\n","            When None is given, the effect of position on the reward will be ignored.\n","            (If only one action is chosen and there is no posion, then you can just ignore this argument.)\n","\n","        Returns\n","        ----------\n","        estimated_rewards: array-like or Tensor, shape (n_rounds,)\n","            Rewards of each round estimated by the SNDR estimator.\n","\n","        \"\"\"\n","        n_rounds = action.shape[0]\n","        iw = action_dist[np.arange(n_rounds), action, position] / pscore\n","        q_hat_at_position = estimated_rewards_by_reg_model[\n","            np.arange(n_rounds), :, position\n","        ]\n","        pi_e_at_position = action_dist[np.arange(n_rounds), :, position]\n","\n","        if isinstance(reward, np.ndarray):\n","            estimated_rewards = np.average(\n","                q_hat_at_position,\n","                weights=pi_e_at_position,\n","                axis=1,\n","            )\n","        else:\n","            raise ValueError(\"reward must be 1D array\")\n","\n","        q_hat_factual = estimated_rewards_by_reg_model[\n","            np.arange(n_rounds), action, position\n","        ]\n","        estimated_rewards += iw * (reward - q_hat_factual) / iw.mean()\n","        return estimated_rewards\n","\n","\n","@dataclass\n","class SwitchDoublyRobust(DoublyRobust):\n","    \"\"\"Switch Doubly Robust (Switch-DR) Estimator.\n","\n","    Note\n","    -------\n","    Switch-DR aims to reduce the variance of the DR estimator by using direct method when the importance weight is large.\n","    This estimator estimates the policy value of evaluation policy :math:`\\\\pi_e` by\n","\n","    .. math::\n","\n","        \\\\hat{V}_{\\\\mathrm{SwitchDR}} (\\\\pi_e; \\\\mathcal{D}, \\\\hat{q}, \\\\lambda)\n","        := \\\\mathbb{E}_{\\\\mathcal{D}} [\\\\hat{q}(x_t,\\\\pi_e) +  w(x_t,a_t) (r_t - \\\\hat{q}(x_t,a_t)) \\\\mathbb{I} \\\\{ w(x_t,a_t) \\\\le \\\\lambda \\\\}],\n","\n","    where :math:`\\\\mathcal{D}=\\\\{(x_t,a_t,r_t)\\\\}_{t=1}^{T}` is logged bandit feedback data with :math:`T` rounds collected by\n","    a behavior policy :math:`\\\\pi_b`. :math:`w(x,a):=\\\\pi_e (a|x)/\\\\pi_b (a|x)` is the importance weight given :math:`x` and :math:`a`.\n","    :math:`\\\\mathbb{E}_{\\\\mathcal{D}}[\\\\cdot]` is the empirical average over :math:`T` observations in :math:`\\\\mathcal{D}`.\n","    :math:`\\\\lambda (\\\\ge 0)` is a switching hyperparameter, which decides the threshold for the importance weight.\n","    :math:`\\\\hat{q} (x,a)` is an estimated expected reward given :math:`x` and :math:`a`.\n","    :math:`\\\\hat{q} (x_t,\\\\pi):= \\\\mathbb{E}_{a \\\\sim \\\\pi(a|x)}[\\\\hat{q}(x,a)]` is the expectation of the estimated reward function over :math:`\\\\pi`.\n","    To estimate the mean reward function, please use `obp.ope.regression_model.RegressionModel`.\n","\n","    Parameters\n","    ----------\n","    lambda_: float, default=np.inf\n","        Switching hyperparameter. When importance weight is larger than this parameter, DM is applied, otherwise DR is used.\n","        This hyperparameter should be larger than or equal to 0., otherwise it is meaningless.\n","\n","    estimator_name: str, default='switch-dr'.\n","        Name of the estimator.\n","\n","    References\n","    ----------\n","    Miroslav Dudík, Dumitru Erhan, John Langford, and Lihong Li.\n","    \"Doubly Robust Policy Evaluation and Optimization.\", 2014.\n","\n","    Yu-Xiang Wang, Alekh Agarwal, and Miroslav Dudík.\n","    \"Optimal and Adaptive Off-policy Evaluation in Contextual Bandits\", 2016.\n","\n","    Yi Su, Maria Dimakopoulou, Akshay Krishnamurthy, and Miroslav Dudik.\n","    \"Doubly Robust Off-Policy Evaluation with Shrinkage.\", 2020.\n","\n","    \"\"\"\n","\n","    lambda_: float = np.inf\n","    estimator_name: str = \"switch-dr\"\n","\n","    def __post_init__(self) -> None:\n","        \"\"\"Initialize Class.\"\"\"\n","        check_scalar(\n","            self.lambda_,\n","            name=\"lambda_\",\n","            target_type=(int, float),\n","            min_val=0.0,\n","        )\n","        if self.lambda_ != self.lambda_:\n","            raise ValueError(\"lambda_ must not be nan\")\n","\n","    def _estimate_round_rewards(\n","        self,\n","        reward: np.ndarray,\n","        action: np.ndarray,\n","        pscore: np.ndarray,\n","        action_dist: np.ndarray,\n","        estimated_rewards_by_reg_model: np.ndarray,\n","        position: Optional[np.ndarray] = None,\n","        **kwargs,\n","    ) -> np.ndarray:\n","        \"\"\"Estimate round-wise (or sample-wise) rewards.\n","\n","        Parameters\n","        ----------\n","        reward: array-like, shape (n_rounds,)\n","            Reward observed in each round of the logged bandit feedback, i.e., :math:`r_t`.\n","\n","        action: array-like, shape (n_rounds,)\n","            Action sampled by behavior policy in each round of the logged bandit feedback, i.e., :math:`a_t`.\n","\n","        pscore: array-like, shape (n_rounds,)\n","            Action choice probabilities of behavior policy (propensity scores), i.e., :math:`\\\\pi_b(a_t|x_t)`.\n","\n","        action_dist: array-like, shape (n_rounds, n_actions, len_list)\n","            Action choice probabilities of evaluation policy (can be deterministic), i.e., :math:`\\\\pi_e(a_t|x_t)`.\n","\n","        estimated_rewards_by_reg_model: array-like, shape (n_rounds, n_actions, len_list)\n","            Expected rewards given context, action, and position estimated by regression model, i.e., :math:`\\\\hat{q}(x_t,a_t)`.\n","\n","        position: array-like, shape (n_rounds,), default=None\n","            Position of recommendation interface where action was presented in each round of the given logged bandit data.\n","            When None is given, the effect of position on the reward will be ignored.\n","            (If only one action is chosen and there is no posion, then you can just ignore this argument.)\n","\n","        Returns\n","        ----------\n","        estimated_rewards: array-like, shape (n_rounds,)\n","            Rewards of each round estimated by the Switch-DR estimator.\n","\n","        \"\"\"\n","        n_rounds = action.shape[0]\n","        iw = action_dist[np.arange(n_rounds), action, position] / pscore\n","        switch_indicator = np.array(iw <= self.lambda_, dtype=int)\n","        q_hat_at_position = estimated_rewards_by_reg_model[\n","            np.arange(n_rounds), :, position\n","        ]\n","        q_hat_factual = estimated_rewards_by_reg_model[\n","            np.arange(n_rounds), action, position\n","        ]\n","        pi_e_at_position = action_dist[np.arange(n_rounds), :, position]\n","        estimated_rewards = np.average(\n","            q_hat_at_position,\n","            weights=pi_e_at_position,\n","            axis=1,\n","        )\n","        estimated_rewards += switch_indicator * iw * (reward - q_hat_factual)\n","        return estimated_rewards\n","\n","    def _estimate_mse_score(\n","        self,\n","        reward: np.ndarray,\n","        action: np.ndarray,\n","        pscore: np.ndarray,\n","        action_dist: np.ndarray,\n","        estimated_rewards_by_reg_model: np.ndarray,\n","        position: Optional[np.ndarray] = None,\n","        use_bias_upper_bound: bool = False,\n","        delta: float = 0.05,\n","    ) -> float:\n","        \"\"\"Estimate the MSE score of a given switching hyperparameter to conduct hyperparameter tuning.\n","\n","        Parameters\n","        ----------\n","        reward: array-like, shape (n_rounds,)\n","            Reward observed in each round of the logged bandit feedback, i.e., :math:`r_t`.\n","\n","        action: array-like, shape (n_rounds,)\n","            Action sampled by behavior policy in each round of the logged bandit feedback, i.e., :math:`a_t`.\n","\n","        pscore: array-like, shape (n_rounds,)\n","            Action choice probabilities of behavior policy (propensity scores), i.e., :math:`\\\\pi_b(a_t|x_t)`.\n","\n","        action_dist: array-like, shape (n_rounds, n_actions, len_list)\n","            Action choice probabilities of evaluation policy (can be deterministic), i.e., :math:`\\\\pi_e(a_t|x_t)`.\n","\n","        estimated_rewards_by_reg_model: array-like, shape (n_rounds, n_actions, len_list)\n","            Expected rewards given context, action, and position estimated by regression model, i.e., :math:`\\\\hat{q}(x_t,a_t)`.\n","\n","        position: array-like, shape (n_rounds,), default=None\n","            Position of recommendation interface where action was presented in each round of the given logged bandit data.\n","            When None is given, the effect of position on the reward will be ignored.\n","            (If only one action is chosen and there is no posion, then you can just ignore this argument.)\n","\n","        use_bias_upper_bound: bool, default=True\n","            Whether to use bias upper bound in hyperparameter tuning.\n","            If False, direct bias estimator is used to estimate the MSE.\n","\n","        delta: float, default=0.05\n","            A confidence delta to construct a high probability upper bound based on the Bernstein’s inequality.\n","\n","        Returns\n","        ----------\n","        estimated_mse_score: float\n","            Estimated MSE score of a given switching hyperparameter `lambda_`.\n","            MSE score is the sum of (high probability) upper bound of bias and the sample variance.\n","            This is estimated using the automatic hyperparameter tuning procedure\n","            based on Section 5 of Su et al.(2020).\n","\n","        \"\"\"\n","        n_rounds = reward.shape[0]\n","        # estimate the sample variance of Switch-DR (Eq.(8) of Wang et al.(2017))\n","        sample_variance = np.var(\n","            self._estimate_round_rewards(\n","                reward=reward,\n","                action=action,\n","                pscore=pscore,\n","                action_dist=action_dist,\n","                estimated_rewards_by_reg_model=estimated_rewards_by_reg_model,\n","                position=position,\n","            )\n","        )\n","        sample_variance /= n_rounds\n","\n","        # estimate the (high probability) upper bound of the bias of Switch-DR\n","        iw = action_dist[np.arange(n_rounds), action, position] / pscore\n","        if use_bias_upper_bound:\n","            bias_term = estimate_high_probability_upper_bound_bias(\n","                reward=reward,\n","                iw=iw,\n","                iw_hat=iw * np.array(iw <= self.lambda_, dtype=int),\n","                q_hat=estimated_rewards_by_reg_model[\n","                    np.arange(n_rounds), action, position\n","                ],\n","                delta=delta,\n","            )\n","        else:\n","            bias_term = estimate_bias_in_ope(\n","                reward=reward,\n","                iw=iw,\n","                iw_hat=iw * np.array(iw <= self.lambda_, dtype=int),\n","                q_hat=estimated_rewards_by_reg_model[\n","                    np.arange(n_rounds), action, position\n","                ],\n","            )\n","        estimated_mse_score = sample_variance + (bias_term ** 2)\n","\n","        return estimated_mse_score\n","\n","\n","@dataclass\n","class DoublyRobustWithShrinkage(DoublyRobust):\n","    \"\"\"Doubly Robust with optimistic shrinkage (DRos) Estimator.\n","\n","    Note\n","    ------\n","    DR with (optimistic) shrinkage replaces the importance weight in the original DR estimator with a new weight mapping\n","    found by directly optimizing sharp bounds on the resulting MSE.\n","\n","    .. math::\n","\n","        \\\\hat{V}_{\\\\mathrm{DRos}} (\\\\pi_e; \\\\mathcal{D}, \\\\hat{q}, \\\\lambda)\n","        := \\\\mathbb{E}_{\\\\mathcal{D}} [\\\\hat{q}(x_t,\\\\pi_e) +  w_o(x_t,a_t;\\\\lambda) (r_t - \\\\hat{q}(x_t,a_t))],\n","\n","    where :math:`\\\\mathcal{D}=\\\\{(x_t,a_t,r_t)\\\\}_{t=1}^{T}` is logged bandit feedback data with :math:`T` rounds collected by\n","    a behavior policy :math:`\\\\pi_b`.\n","    :math:`w(x,a):=\\\\pi_e (a|x)/\\\\pi_b (a|x)` is the importance weight given :math:`x` and :math:`a`.\n","    :math:`\\\\hat{q} (x_t,\\\\pi):= \\\\mathbb{E}_{a \\\\sim \\\\pi(a|x)}[\\\\hat{q}(x,a)]` is the expectation of the estimated reward function over :math:`\\\\pi`.\n","    :math:`\\\\mathbb{E}_{\\\\mathcal{D}}[\\\\cdot]` is the empirical average over :math:`T` observations in :math:`\\\\mathcal{D}`.\n","    :math:`\\\\hat{q} (x,a)` is an estimated expected reward given :math:`x` and :math:`a`.\n","    To estimate the mean reward function, please use `obp.ope.regression_model.RegressionModel`.\n","\n","    :math:`w_{o} (x_t,a_t;\\\\lambda)` is a new weight by the shrinkage technique which is defined as\n","\n","    .. math::\n","\n","        w_{o} (x_t,a_t;\\\\lambda) := \\\\frac{\\\\lambda}{w^2(x_t,a_t) + \\\\lambda} w(x_t,a_t).\n","\n","    When :math:`\\\\lambda=0`, we have :math:`w_{o} (x,a;\\\\lambda)=0` corresponding to the DM estimator.\n","    In contrast, as :math:`\\\\lambda \\\\rightarrow \\\\infty`, :math:`w_{o} (x,a;\\\\lambda)` increases and in the limit becomes equal to the original importance weight, corresponding to the standard DR estimator.\n","\n","    Parameters\n","    ----------\n","    lambda_: float\n","        Shrinkage hyperparameter.\n","        This hyperparameter should be larger than or equal to 0., otherwise it is meaningless.\n","\n","    estimator_name: str, default='dr-os'.\n","        Name of the estimator.\n","\n","    References\n","    ----------\n","    Miroslav Dudík, Dumitru Erhan, John Langford, and Lihong Li.\n","    \"Doubly Robust Policy Evaluation and Optimization.\", 2014.\n","\n","    Yi Su, Maria Dimakopoulou, Akshay Krishnamurthy, and Miroslav Dudik.\n","    \"Doubly Robust Off-Policy Evaluation with Shrinkage.\", 2020.\n","\n","    \"\"\"\n","\n","    lambda_: float = 0.0\n","    estimator_name: str = \"dr-os\"\n","\n","    def __post_init__(self) -> None:\n","        \"\"\"Initialize Class.\"\"\"\n","        check_scalar(\n","            self.lambda_,\n","            name=\"lambda_\",\n","            target_type=(int, float),\n","            min_val=0.0,\n","        )\n","        if self.lambda_ != self.lambda_:\n","            raise ValueError(\"lambda_ must not be nan\")\n","\n","    def _estimate_round_rewards(\n","        self,\n","        reward: np.ndarray,\n","        action: np.ndarray,\n","        pscore: np.ndarray,\n","        action_dist: np.ndarray,\n","        estimated_rewards_by_reg_model: np.ndarray,\n","        position: Optional[np.ndarray] = None,\n","        **kwargs,\n","    ) -> np.ndarray:\n","        \"\"\"Estimate round-wise (or sample-wise) rewards.\n","\n","        Parameters\n","        ----------\n","        reward: array-like or Tensor, shape (n_rounds,)\n","            Reward observed in each round of the logged bandit feedback, i.e., :math:`r_t`.\n","\n","        action: array-like or Tensor, shape (n_rounds,)\n","            Action sampled by behavior policy in each round of the logged bandit feedback, i.e., :math:`a_t`.\n","\n","        pscore: array-like or Tensor, shape (n_rounds,)\n","            Action choice probabilities of behavior policy (propensity scores), i.e., :math:`\\\\pi_b(a_t|x_t)`.\n","\n","        action_dist: array-like or Tensor, shape (n_rounds, n_actions, len_list)\n","            Action choice probabilities of evaluation policy (can be deterministic), i.e., :math:`\\\\pi_e(a_t|x_t)`.\n","\n","        estimated_rewards_by_reg_model: array-like or Tensor, shape (n_rounds, n_actions, len_list)\n","            Expected rewards given context, action, and position estimated by regression model, i.e., :math:`\\\\hat{q}(x_t,a_t)`.\n","\n","        position: array-like or Tensor, shape (n_rounds,), default=None\n","            Position of recommendation interface where action was presented in each round of the given logged bandit data.\n","            When None is given, the effect of position on the reward will be ignored.\n","            (If only one action is chosen and there is no posion, then you can just ignore this argument.)\n","\n","        Returns\n","        ----------\n","        estimated_rewards: array-like or Tensor, shape (n_rounds,)\n","            Rewards of each round estimated by the DRos estimator.\n","\n","        \"\"\"\n","        n_rounds = action.shape[0]\n","        iw = action_dist[np.arange(n_rounds), action, position] / pscore\n","        if self.lambda_ < np.inf:\n","            iw_hat = (self.lambda_ * iw) / (iw ** 2 + self.lambda_)\n","        else:\n","            iw_hat = iw\n","        q_hat_at_position = estimated_rewards_by_reg_model[\n","            np.arange(n_rounds), :, position\n","        ]\n","        q_hat_factual = estimated_rewards_by_reg_model[\n","            np.arange(n_rounds), action, position\n","        ]\n","        pi_e_at_position = action_dist[np.arange(n_rounds), :, position]\n","\n","        if isinstance(reward, np.ndarray):\n","            estimated_rewards = np.average(\n","                q_hat_at_position,\n","                weights=pi_e_at_position,\n","                axis=1,\n","            )\n","        else:\n","            raise ValueError(\"reward must be 1D array\")\n","\n","        estimated_rewards += iw_hat * (reward - q_hat_factual)\n","        return estimated_rewards\n","\n","    def _estimate_mse_score(\n","        self,\n","        reward: np.ndarray,\n","        action: np.ndarray,\n","        pscore: np.ndarray,\n","        action_dist: np.ndarray,\n","        estimated_rewards_by_reg_model: np.ndarray,\n","        position: Optional[np.ndarray] = None,\n","        use_bias_upper_bound: bool = False,\n","        delta: float = 0.05,\n","    ) -> float:\n","        \"\"\"Estimate the MSE score of a given shrinkage hyperparameter to conduct hyperparameter tuning.\n","\n","        Parameters\n","        ----------\n","        reward: array-like, shape (n_rounds,)\n","            Reward observed in each round of the logged bandit feedback, i.e., :math:`r_t`.\n","\n","        action: array-like, shape (n_rounds,)\n","            Action sampled by behavior policy in each round of the logged bandit feedback, i.e., :math:`a_t`.\n","\n","        pscore: array-like, shape (n_rounds,)\n","            Action choice probabilities of behavior policy (propensity scores), i.e., :math:`\\\\pi_b(a_t|x_t)`.\n","\n","        action_dist: array-like, shape (n_rounds, n_actions, len_list)\n","            Action choice probabilities of evaluation policy (can be deterministic), i.e., :math:`\\\\pi_e(a_t|x_t)`.\n","\n","        estimated_rewards_by_reg_model: array-like, shape (n_rounds, n_actions, len_list)\n","            Expected rewards given context, action, and position estimated by regression model, i.e., :math:`\\\\hat{q}(x_t,a_t)`.\n","\n","        position: array-like, shape (n_rounds,), default=None\n","            Position of recommendation interface where action was presented in each round of the given logged bandit data.\n","\n","        use_bias_upper_bound: bool, default=True\n","            Whether to use bias upper bound in hyperparameter tuning.\n","            If False, direct bias estimator is used to estimate the MSE.\n","\n","        delta: float, default=0.05\n","            A confidence delta to construct a high probability upper bound based on the Bernstein’s inequality.\n","\n","        Returns\n","        ----------\n","        estimated_mse_score: float\n","            Estimated MSE score of a given shrinkage hyperparameter `lambda_`.\n","            MSE score is the sum of (high probability) upper bound of bias and the sample variance.\n","            This is estimated using the automatic hyperparameter tuning procedure\n","            based on Section 5 of Su et al.(2020).\n","\n","        \"\"\"\n","        n_rounds = reward.shape[0]\n","        # estimate the sample variance of DRos\n","        sample_variance = np.var(\n","            self._estimate_round_rewards(\n","                reward=reward,\n","                action=action,\n","                pscore=pscore,\n","                action_dist=action_dist,\n","                estimated_rewards_by_reg_model=estimated_rewards_by_reg_model,\n","                position=position,\n","            )\n","        )\n","        sample_variance /= n_rounds\n","\n","        # estimate the (high probability) upper bound of the bias of DRos\n","        iw = action_dist[np.arange(n_rounds), action, position] / pscore\n","        if self.lambda_ < np.inf:\n","            iw_hat = (self.lambda_ * iw) / (iw ** 2 + self.lambda_)\n","        else:\n","            iw_hat = iw\n","        if use_bias_upper_bound:\n","            bias_term = estimate_high_probability_upper_bound_bias(\n","                reward=reward,\n","                iw=iw,\n","                iw_hat=iw_hat,\n","                q_hat=estimated_rewards_by_reg_model[\n","                    np.arange(n_rounds), action, position\n","                ],\n","                delta=0.05,\n","            )\n","        else:\n","            bias_term = estimate_bias_in_ope(\n","                reward=reward,\n","                iw=iw,\n","                iw_hat=iw_hat,\n","                q_hat=estimated_rewards_by_reg_model[\n","                    np.arange(n_rounds), action, position\n","                ],\n","            )\n","        estimated_mse_score = sample_variance + (bias_term ** 2)\n","\n","        return estimated_mse_score\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"cellView":"form","id":"GTufzyQm0omB"},"source":["#@markdown off-policy estimator\n","@dataclass\n","class OffPolicyEvaluation:\n","    \"\"\"Class to conduct OPE by multiple estimators simultaneously.\n","    Parameters\n","    -----------\n","    bandit_feedback: BanditFeedback\n","        Logged bandit feedback data used to conduct OPE.\n","    ope_estimators: List[BaseOffPolicyEstimator]\n","        List of OPE estimators used to evaluate the policy value of evaluation policy.\n","        Estimators must follow the interface of `obp.ope.BaseOffPolicyEstimator`.\n","    Examples\n","    ----------\n","    .. code-block:: python\n","        # a case for implementing OPE of the BernoulliTS policy\n","        # using log data generated by the Random policy\n","        >>> from obp.dataset import OpenBanditDataset\n","        >>> from obp.policy import BernoulliTS\n","        >>> from obp.ope import OffPolicyEvaluation, InverseProbabilityWeighting as IPW\n","        # (1) Data loading and preprocessing\n","        >>> dataset = OpenBanditDataset(behavior_policy='random', campaign='all')\n","        >>> bandit_feedback = dataset.obtain_batch_bandit_feedback()\n","        >>> bandit_feedback.keys()\n","        dict_keys(['n_rounds', 'n_actions', 'action', 'position', 'reward', 'pscore', 'context', 'action_context'])\n","        # (2) Off-Policy Learning\n","        >>> evaluation_policy = BernoulliTS(\n","            n_actions=dataset.n_actions,\n","            len_list=dataset.len_list,\n","            is_zozotown_prior=True, # replicate the policy in the ZOZOTOWN production\n","            campaign=\"all\",\n","            random_state=12345\n","        )\n","        >>> action_dist = evaluation_policy.compute_batch_action_dist(\n","            n_sim=100000, n_rounds=bandit_feedback[\"n_rounds\"]\n","        )\n","        # (3) Off-Policy Evaluation\n","        >>> ope = OffPolicyEvaluation(bandit_feedback=bandit_feedback, ope_estimators=[IPW()])\n","        >>> estimated_policy_value = ope.estimate_policy_values(action_dist=action_dist)\n","        >>> estimated_policy_value\n","        {'ipw': 0.004553...}\n","        # policy value improvement of BernoulliTS over the Random policy estimated by IPW\n","        >>> estimated_policy_value_improvement = estimated_policy_value['ipw'] / bandit_feedback['reward'].mean()\n","        # our OPE procedure suggests that BernoulliTS improves Random by 19.81%\n","        >>> print(estimated_policy_value_improvement)\n","        1.198126...\n","    \"\"\"\n","\n","    bandit_feedback: BanditFeedback\n","    ope_estimators: List[BaseOffPolicyEstimator]\n","\n","    def __post_init__(self) -> None:\n","        \"\"\"Initialize class.\"\"\"\n","        for key_ in [\"action\", \"position\", \"reward\", \"pscore\"]:\n","            if key_ not in self.bandit_feedback:\n","                raise RuntimeError(f\"Missing key of {key_} in 'bandit_feedback'.\")\n","        self.ope_estimators_ = dict()\n","        self.is_model_dependent = False\n","        for estimator in self.ope_estimators:\n","            self.ope_estimators_[estimator.estimator_name] = estimator\n","            if isinstance(estimator, DirectMethod) or isinstance(estimator, DoublyRobust):\n","                self.is_model_dependent = True\n","\n","    def _create_estimator_inputs(\n","        self,\n","        action_dist: np.ndarray,\n","        estimated_rewards_by_reg_model: Optional[\n","            Union[np.ndarray, Dict[str, np.ndarray]]\n","        ] = None,\n","    ) -> Dict[str, Dict[str, np.ndarray]]:\n","        \"\"\"Create input dictionary to estimate policy value using subclasses of `BaseOffPolicyEstimator`\"\"\"\n","        check_array(array=action_dist, name=\"action_dist\", expected_dim=3)\n","        if estimated_rewards_by_reg_model is None:\n","            pass\n","        elif isinstance(estimated_rewards_by_reg_model, dict):\n","            for estimator_name, value in estimated_rewards_by_reg_model.items():\n","                check_array(\n","                    array=value,\n","                    name=f\"estimated_rewards_by_reg_model[{estimator_name}]\",\n","                    expected_dim=3,\n","                )\n","                if value.shape != action_dist.shape:\n","                    raise ValueError(\n","                        f\"Expected `estimated_rewards_by_reg_model[{estimator_name}].shape == action_dist.shape`, but found it False.\"\n","                    )\n","        elif estimated_rewards_by_reg_model.shape != action_dist.shape:\n","            raise ValueError(\n","                \"Expected `estimated_rewards_by_reg_model.shape == action_dist.shape`, but found it False\"\n","            )\n","        estimator_inputs = {\n","            estimator_name: {\n","                input_: self.bandit_feedback[input_]\n","                for input_ in [\"reward\", \"action\", \"position\", \"pscore\"]\n","            }\n","            for estimator_name in self.ope_estimators_\n","        }\n","\n","        for estimator_name in self.ope_estimators_:\n","            estimator_inputs[estimator_name][\"action_dist\"] = action_dist\n","            if isinstance(estimated_rewards_by_reg_model, dict):\n","                if estimator_name in estimated_rewards_by_reg_model:\n","                    estimator_inputs[estimator_name][\n","                        \"estimated_rewards_by_reg_model\"\n","                    ] = estimated_rewards_by_reg_model[estimator_name]\n","                else:\n","                    estimator_inputs[estimator_name][\n","                        \"estimated_rewards_by_reg_model\"\n","                    ] = None\n","            else:\n","                estimator_inputs[estimator_name][\n","                    \"estimated_rewards_by_reg_model\"\n","                ] = estimated_rewards_by_reg_model\n","\n","        return estimator_inputs\n","\n","    def estimate_policy_values(\n","        self,\n","        action_dist: np.ndarray,\n","        estimated_rewards_by_reg_model: Optional[\n","            Union[np.ndarray, Dict[str, np.ndarray]]\n","        ] = None,\n","    ) -> Dict[str, float]:\n","        \"\"\"Estimate the policy value of evaluation policy.\n","        Parameters\n","        ------------\n","        action_dist: array-like, shape (n_rounds, n_actions, len_list)\n","            Action choice probabilities of evaluation policy (can be deterministic), i.e., :math:`\\\\pi_e(a_t|x_t)`.\n","        estimated_rewards_by_reg_model: array-like, shape (n_rounds, n_actions, len_list) or Dict[str, array-like], default=None\n","            Expected rewards given each round, action, and position estimated by regression model, i.e., :math:`\\\\hat{q}(x_t,a_t)`.\n","            When an array-like is given, all OPE estimators use it.\n","            When a dict is given, if the dict has the name of a estimator as a key, the corresponding value is used.\n","            When it is not given, model-dependent estimators such as DM and DR cannot be used.\n","        Returns\n","        ----------\n","        policy_value_dict: Dict[str, float]\n","            Dictionary containing estimated policy values by OPE estimators.\n","        \"\"\"\n","        if self.is_model_dependent:\n","            if estimated_rewards_by_reg_model is None:\n","                raise ValueError(\n","                    \"When model dependent estimators such as DM or DR are used, `estimated_rewards_by_reg_model` must be given\"\n","                )\n","\n","        policy_value_dict = dict()\n","        estimator_inputs = self._create_estimator_inputs(\n","            action_dist=action_dist,\n","            estimated_rewards_by_reg_model=estimated_rewards_by_reg_model,\n","        )\n","        for estimator_name, estimator in self.ope_estimators_.items():\n","            policy_value_dict[estimator_name] = estimator.estimate_policy_value(\n","                **estimator_inputs[estimator_name]\n","            )\n","\n","        return policy_value_dict\n","\n","    def estimate_intervals(\n","        self,\n","        action_dist: np.ndarray,\n","        estimated_rewards_by_reg_model: Optional[\n","            Union[np.ndarray, Dict[str, np.ndarray]]\n","        ] = None,\n","        alpha: float = 0.05,\n","        n_bootstrap_samples: int = 100,\n","        random_state: Optional[int] = None,\n","    ) -> Dict[str, Dict[str, float]]:\n","        \"\"\"Estimate confidence intervals of policy values using nonparametric bootstrap procedure.\n","        Parameters\n","        ------------\n","        action_dist: array-like, shape (n_rounds, n_actions, len_list)\n","            Action choice probabilities of evaluation policy (can be deterministic), i.e., :math:`\\\\pi_e(a_t|x_t)`.\n","        estimated_rewards_by_reg_model: array-like, shape (n_rounds, n_actions, len_list) or Dict[str, array-like], default=None\n","            Expected rewards given context, action, and position estimated by regression model, i.e., :math:`\\\\hat{q}(x_t,a_t)`.\n","            When an array-like is given, all OPE estimators use it.\n","            When a dict is given, if the dict has the name of a estimator as a key, the corresponding value is used.\n","            When it is not given, model-dependent estimators such as DM and DR cannot be used.\n","        alpha: float, default=0.05\n","            Significance level.\n","        n_bootstrap_samples: int, default=100\n","            Number of resampling performed in the bootstrap procedure.\n","        random_state: int, default=None\n","            Controls the random seed in bootstrap sampling.\n","        Returns\n","        ----------\n","        policy_value_interval_dict: Dict[str, Dict[str, float]]\n","            Dictionary containing confidence intervals of estimated policy value estimated\n","            using nonparametric bootstrap procedure.\n","        \"\"\"\n","        if self.is_model_dependent:\n","            if estimated_rewards_by_reg_model is None:\n","                raise ValueError(\n","                    \"When model dependent estimators such as DM or DR are used, `estimated_rewards_by_reg_model` must be given\"\n","                )\n","\n","        check_confidence_interval_arguments(\n","            alpha=alpha,\n","            n_bootstrap_samples=n_bootstrap_samples,\n","            random_state=random_state,\n","        )\n","        policy_value_interval_dict = dict()\n","        estimator_inputs = self._create_estimator_inputs(\n","            action_dist=action_dist,\n","            estimated_rewards_by_reg_model=estimated_rewards_by_reg_model,\n","        )\n","        for estimator_name, estimator in self.ope_estimators_.items():\n","            policy_value_interval_dict[estimator_name] = estimator.estimate_interval(\n","                **estimator_inputs[estimator_name],\n","                alpha=alpha,\n","                n_bootstrap_samples=n_bootstrap_samples,\n","                random_state=random_state,\n","            )\n","\n","        return policy_value_interval_dict\n","\n","    def summarize_off_policy_estimates(\n","        self,\n","        action_dist: np.ndarray,\n","        estimated_rewards_by_reg_model: Optional[\n","            Union[np.ndarray, Dict[str, np.ndarray]]\n","        ] = None,\n","        alpha: float = 0.05,\n","        n_bootstrap_samples: int = 100,\n","        random_state: Optional[int] = None,\n","    ) -> Tuple[DataFrame, DataFrame]:\n","        \"\"\"Summarize policy values and their confidence intervals estimated by OPE estimators.\n","        Parameters\n","        ------------\n","        action_dist: array-like, shape (n_rounds, n_actions, len_list)\n","            Action choice probabilities of evaluation policy (can be deterministic), i.e., :math:`\\\\pi_e(a_t|x_t)`.\n","        estimated_rewards_by_reg_model: array-like, shape (n_rounds, n_actions, len_list) or Dict[str, array-like], default=None\n","            Expected rewards given each round, action, and position estimated by regression model, i.e., :math:`\\\\hat{q}(x_t,a_t)`.\n","            When an array-like is given, all OPE estimators use it.\n","            When a dict is given, if the dict has the name of a estimator as a key, the corresponding value is used.\n","            When it is not given, model-dependent estimators such as DM and DR cannot be used.\n","        alpha: float, default=0.05\n","            Significance level.\n","        n_bootstrap_samples: int, default=100\n","            Number of resampling performed in the bootstrap procedure.\n","        random_state: int, default=None\n","            Controls the random seed in bootstrap sampling.\n","        Returns\n","        ----------\n","        (policy_value_df, policy_value_interval_df): Tuple[DataFrame, DataFrame]\n","            Policy values and their confidence intervals Estimated by OPE estimators.\n","        \"\"\"\n","        policy_value_df = DataFrame(\n","            self.estimate_policy_values(\n","                action_dist=action_dist,\n","                estimated_rewards_by_reg_model=estimated_rewards_by_reg_model,\n","            ),\n","            index=[\"estimated_policy_value\"],\n","        )\n","        policy_value_interval_df = DataFrame(\n","            self.estimate_intervals(\n","                action_dist=action_dist,\n","                estimated_rewards_by_reg_model=estimated_rewards_by_reg_model,\n","                alpha=alpha,\n","                n_bootstrap_samples=n_bootstrap_samples,\n","                random_state=random_state,\n","            )\n","        )\n","        policy_value_of_behavior_policy = self.bandit_feedback[\"reward\"].mean()\n","        policy_value_df = policy_value_df.T\n","        if policy_value_of_behavior_policy <= 0:\n","            logger.warning(\n","                f\"Policy value of the behavior policy is {policy_value_of_behavior_policy} (<=0); relative estimated policy value is set to np.nan\"\n","            )\n","            policy_value_df[\"relative_estimated_policy_value\"] = np.nan\n","        else:\n","            policy_value_df[\"relative_estimated_policy_value\"] = (\n","                policy_value_df.estimated_policy_value / policy_value_of_behavior_policy\n","            )\n","        return policy_value_df, policy_value_interval_df.T\n","\n","    def visualize_off_policy_estimates(\n","        self,\n","        action_dist: np.ndarray,\n","        estimated_rewards_by_reg_model: Optional[\n","            Union[np.ndarray, Dict[str, np.ndarray]]\n","        ] = None,\n","        alpha: float = 0.05,\n","        is_relative: bool = False,\n","        n_bootstrap_samples: int = 100,\n","        random_state: Optional[int] = None,\n","        fig_dir: Optional[Path] = None,\n","        fig_name: str = \"estimated_policy_value.png\",\n","    ) -> None:\n","        \"\"\"Visualize policy values estimated by OPE estimators.\n","        Parameters\n","        ----------\n","        action_dist: array-like, shape (n_rounds, n_actions, len_list)\n","            Action choice probabilities of evaluation policy (can be deterministic), i.e., :math:`\\\\pi_e(a_t|x_t)`.\n","        estimated_rewards_by_reg_model: array-like, shape (n_rounds, n_actions, len_list) or Dict[str, array-like], default=None\n","            Expected rewards given context, action, and position estimated by regression model, i.e., :math:`\\\\hat{q}(x_t,a_t)`.\n","            When an array-like is given, all OPE estimators use it.\n","            When a dict is given, if the dict has the name of a estimator as a key, the corresponding value is used.\n","            When it is not given, model-dependent estimators such as DM and DR cannot be used.\n","        alpha: float, default=0.05\n","            Significance level.\n","        n_bootstrap_samples: int, default=100\n","            Number of resampling performed in the bootstrap procedure.\n","        random_state: int, default=None\n","            Controls the random seed in bootstrap sampling.\n","        is_relative: bool, default=False,\n","            If True, the method visualizes the estimated policy values of evaluation policy\n","            relative to the ground-truth policy value of behavior policy.\n","        fig_dir: Path, default=None\n","            Path to store the bar figure.\n","            If 'None' is given, the figure will not be saved.\n","        fig_name: str, default=\"estimated_policy_value.png\"\n","            Name of the bar figure.\n","        \"\"\"\n","        if fig_dir is not None:\n","            assert isinstance(fig_dir, Path), \"fig_dir must be a Path\"\n","        if fig_name is not None:\n","            assert isinstance(fig_name, str), \"fig_dir must be a string\"\n","\n","        estimated_round_rewards_dict = dict()\n","        estimator_inputs = self._create_estimator_inputs(\n","            action_dist=action_dist,\n","            estimated_rewards_by_reg_model=estimated_rewards_by_reg_model,\n","        )\n","        for estimator_name, estimator in self.ope_estimators_.items():\n","            estimated_round_rewards_dict[\n","                estimator_name\n","            ] = estimator._estimate_round_rewards(**estimator_inputs[estimator_name])\n","        estimated_round_rewards_df = DataFrame(estimated_round_rewards_dict)\n","        estimated_round_rewards_df.rename(\n","            columns={key: key.upper() for key in estimated_round_rewards_dict.keys()},\n","            inplace=True,\n","        )\n","        if is_relative:\n","            estimated_round_rewards_df /= self.bandit_feedback[\"reward\"].mean()\n","\n","        plt.style.use(\"ggplot\")\n","        fig, ax = plt.subplots(figsize=(8, 6))\n","        sns.barplot(\n","            data=estimated_round_rewards_df,\n","            ax=ax,\n","            ci=100 * (1 - alpha),\n","            n_boot=n_bootstrap_samples,\n","            seed=random_state,\n","        )\n","        plt.xlabel(\"OPE Estimators\", fontsize=25)\n","        plt.ylabel(\n","            f\"Estimated Policy Value (± {np.int(100*(1 - alpha))}% CI)\", fontsize=20\n","        )\n","        plt.yticks(fontsize=15)\n","        plt.xticks(fontsize=25 - 2 * len(self.ope_estimators))\n","\n","        if fig_dir:\n","            fig.savefig(str(fig_dir / fig_name))\n","\n","    def evaluate_performance_of_estimators(\n","        self,\n","        ground_truth_policy_value: float,\n","        action_dist: np.ndarray,\n","        estimated_rewards_by_reg_model: Optional[\n","            Union[np.ndarray, Dict[str, np.ndarray]]\n","        ] = None,\n","        metric: str = \"relative-ee\",\n","    ) -> Dict[str, float]:\n","        \"\"\"Evaluate estimation performance of OPE estimators.\n","        Note\n","        ------\n","        Evaluate the estimation performance of OPE estimators by relative estimation error (relative-EE) or squared error (SE):\n","        .. math ::\n","            \\\\text{Relative-EE} (\\\\hat{V}; \\\\mathcal{D}) = \\\\left|  \\\\frac{\\\\hat{V}(\\\\pi; \\\\mathcal{D}) - V(\\\\pi)}{V(\\\\pi)} \\\\right|,\n","        .. math ::\n","            \\\\text{SE} (\\\\hat{V}; \\\\mathcal{D}) = \\\\left(\\\\hat{V}(\\\\pi; \\\\mathcal{D}) - V(\\\\pi) \\\\right)^2,\n","        where :math:`V({\\\\pi})` is the ground-truth policy value of the evalation policy :math:`\\\\pi_e` (often estimated using on-policy estimation).\n","        :math:`\\\\hat{V}(\\\\pi; \\\\mathcal{D})` is an estimated policy value by an OPE estimator :math:`\\\\hat{V}` and logged bandit feedback :math:`\\\\mathcal{D}`.\n","        Parameters\n","        ----------\n","        ground_truth policy value: float\n","            Ground_truth policy value of evaluation policy, i.e., :math:`V(\\\\pi_e)`.\n","            With Open Bandit Dataset, we use an on-policy estimate of the policy value as its ground-truth.\n","        action_dist: array-like, shape (n_rounds, n_actions, len_list)\n","            Action choice probabilities of evaluation policy (can be deterministic), i.e., :math:`\\\\pi_e(a_t|x_t)`.\n","        estimated_rewards_by_reg_model: array-like, shape (n_rounds, n_actions, len_list) or Dict[str, array-like], default=None\n","            Expected rewards given context, action, and position estimated by regression model, i.e., :math:`\\\\hat{q}(x_t,a_t)`.\n","            When an array-like is given, all OPE estimators use it.\n","            When a dict is given, if the dict has the name of a estimator as a key, the corresponding value is used.\n","            When it is not given, model-dependent estimators such as DM and DR cannot be used.\n","        metric: str, default=\"relative-ee\"\n","            Evaluation metric used to evaluate and compare the estimation performance of OPE estimators.\n","            Must be \"relative-ee\" or \"se\".\n","        Returns\n","        ----------\n","        eval_metric_ope_dict: Dict[str, float]\n","            Dictionary containing evaluation metric for evaluating the estimation performance of OPE estimators.\n","        \"\"\"\n","        check_scalar(\n","            ground_truth_policy_value,\n","            \"ground_truth_policy_value\",\n","            float,\n","        )\n","        if metric not in [\"relative-ee\", \"se\"]:\n","            raise ValueError(\n","                f\"metric must be either 'relative-ee' or 'se', but {metric} is given\"\n","            )\n","        if metric == \"relative-ee\" and ground_truth_policy_value == 0.0:\n","            raise ValueError(\n","                \"ground_truth_policy_value must be non-zero when metric is relative-ee\"\n","            )\n","\n","        eval_metric_ope_dict = dict()\n","        estimator_inputs = self._create_estimator_inputs(\n","            action_dist=action_dist,\n","            estimated_rewards_by_reg_model=estimated_rewards_by_reg_model,\n","        )\n","        for estimator_name, estimator in self.ope_estimators_.items():\n","            estimated_policy_value = estimator.estimate_policy_value(\n","                **estimator_inputs[estimator_name]\n","            )\n","            if metric == \"relative-ee\":\n","                relative_ee_ = estimated_policy_value - ground_truth_policy_value\n","                relative_ee_ /= ground_truth_policy_value\n","                eval_metric_ope_dict[estimator_name] = np.abs(relative_ee_)\n","            elif metric == \"se\":\n","                se_ = (estimated_policy_value - ground_truth_policy_value) ** 2\n","                eval_metric_ope_dict[estimator_name] = se_\n","        return eval_metric_ope_dict\n","\n","    def summarize_estimators_comparison(\n","        self,\n","        ground_truth_policy_value: float,\n","        action_dist: np.ndarray,\n","        estimated_rewards_by_reg_model: Optional[\n","            Union[np.ndarray, Dict[str, np.ndarray]]\n","        ] = None,\n","        metric: str = \"relative-ee\",\n","    ) -> DataFrame:\n","        \"\"\"Summarize performance comparisons of OPE estimators.\n","        Parameters\n","        ----------\n","        ground_truth policy value: float\n","            Ground_truth policy value of evaluation policy, i.e., :math:`V(\\\\pi_e)`.\n","            With Open Bandit Dataset, we use an on-policy estimate of the policy value as ground-truth.\n","        action_dist: array-like, shape (n_rounds, n_actions, len_list)\n","            Action choice probabilities of evaluation policy (can be deterministic), i.e., :math:`\\\\pi_e(a_t|x_t)`.\n","        estimated_rewards_by_reg_model: array-like, shape (n_rounds, n_actions, len_list), default=None\n","            Expected rewards given context, action, and position estimated by regression model, i.e., :math:`\\\\hat{q}(x_t,a_t)`.\n","            When it is not given, model-dependent estimators such as DM and DR cannot be used.\n","        metric: str, default=\"relative-ee\"\n","            Evaluation metric used to evaluate and compare the estimation performance of OPE estimators.\n","            Must be either \"relative-ee\" or \"se\".\n","        Returns\n","        ----------\n","        eval_metric_ope_df: DataFrame\n","            Evaluation metric to evaluate and compare the estimation performance of OPE estimators.\n","        \"\"\"\n","        eval_metric_ope_df = DataFrame(\n","            self.evaluate_performance_of_estimators(\n","                ground_truth_policy_value=ground_truth_policy_value,\n","                action_dist=action_dist,\n","                estimated_rewards_by_reg_model=estimated_rewards_by_reg_model,\n","                metric=metric,\n","            ),\n","            index=[metric],\n","        )\n","        return eval_metric_ope_df.T\n","\n","    def visualize_off_policy_estimates_of_multiple_policies(\n","        self,\n","        policy_name_list: List[str],\n","        action_dist_list: List[np.ndarray],\n","        estimated_rewards_by_reg_model: Optional[\n","            Union[np.ndarray, Dict[str, np.ndarray]]\n","        ] = None,\n","        alpha: float = 0.05,\n","        is_relative: bool = False,\n","        n_bootstrap_samples: int = 100,\n","        random_state: Optional[int] = None,\n","        fig_dir: Optional[Path] = None,\n","        fig_name: str = \"estimated_policy_value.png\",\n","    ) -> None:\n","        \"\"\"Visualize policy values estimated by OPE estimators.\n","        Parameters\n","        ----------\n","        policy_name_list: List[str]\n","            List of the names of evaluation policies.\n","        action_dist_list: List[array-like, shape (n_rounds, n_actions, len_list)]\n","            List of action choice probabilities by the evaluation policies (can be deterministic), i.e., :math:`\\\\pi_e(a_t|x_t)`.\n","        estimated_rewards_by_reg_model: array-like, shape (n_rounds, n_actions, len_list) or Dict[str, array-like], default=None\n","            Expected rewards given context, action, and position estimated by regression model, i.e., :math:`\\\\hat{q}(x_t,a_t)`.\n","            When an array-like is given, all OPE estimators use it.\n","            When a dict is given, if the dict has the name of an estimator as a key, the corresponding value is used.\n","            When it is not given, model-dependent estimators such as DM and DR cannot be used.\n","        alpha: float, default=0.05\n","            Significance level.\n","        n_bootstrap_samples: int, default=100\n","            Number of resampling performed in the bootstrap procedure.\n","        random_state: int, default=None\n","            Controls the random seed in bootstrap sampling.\n","        is_relative: bool, default=False,\n","            If True, the method visualizes the estimated policy values of evaluation policy\n","            relative to the ground-truth policy value of behavior policy.\n","        fig_dir: Path, default=None\n","            Path to store the bar figure.\n","            If 'None' is given, the figure will not be saved.\n","        fig_name: str, default=\"estimated_policy_value.png\"\n","            Name of the bar figure.\n","        \"\"\"\n","        if len(policy_name_list) != len(action_dist_list):\n","            raise ValueError(\n","                \"the length of policy_name_list must be the same as action_dist_list\"\n","            )\n","        if fig_dir is not None:\n","            assert isinstance(fig_dir, Path), \"fig_dir must be a Path\"\n","        if fig_name is not None:\n","            assert isinstance(fig_name, str), \"fig_dir must be a string\"\n","\n","        estimated_round_rewards_dict = {\n","            estimator_name: {} for estimator_name in self.ope_estimators_\n","        }\n","\n","        for policy_name, action_dist in zip(policy_name_list, action_dist_list):\n","            estimator_inputs = self._create_estimator_inputs(\n","                action_dist=action_dist,\n","                estimated_rewards_by_reg_model=estimated_rewards_by_reg_model,\n","            )\n","            for estimator_name, estimator in self.ope_estimators_.items():\n","                estimated_round_rewards_dict[estimator_name][\n","                    policy_name\n","                ] = estimator._estimate_round_rewards(\n","                    **estimator_inputs[estimator_name]\n","                )\n","\n","        plt.style.use(\"ggplot\")\n","        fig = plt.figure(figsize=(8, 6.2 * len(self.ope_estimators_)))\n","\n","        for i, estimator_name in enumerate(self.ope_estimators_):\n","            estimated_round_rewards_df = DataFrame(\n","                estimated_round_rewards_dict[estimator_name]\n","            )\n","            if is_relative:\n","                estimated_round_rewards_df /= self.bandit_feedback[\"reward\"].mean()\n","\n","            ax = fig.add_subplot(len(action_dist_list), 1, i + 1)\n","            sns.barplot(\n","                data=estimated_round_rewards_df,\n","                ax=ax,\n","                ci=100 * (1 - alpha),\n","                n_boot=n_bootstrap_samples,\n","                seed=random_state,\n","            )\n","            ax.set_title(estimator_name.upper(), fontsize=20)\n","            ax.set_ylabel(\n","                f\"Estimated Policy Value (± {np.int(100*(1 - alpha))}% CI)\", fontsize=20\n","            )\n","            plt.yticks(fontsize=15)\n","            plt.xticks(fontsize=25 - 2 * len(policy_name_list))\n","\n","        if fig_dir:\n","            fig.savefig(str(fig_dir / fig_name))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zLDfki-FyOw2"},"source":["## Policy"]},{"cell_type":"code","metadata":{"id":"HjERr2N2ydAW"},"source":["@dataclass\n","class BaseOfflinePolicyLearner(metaclass=ABCMeta):\n","    \"\"\"Base class for off-policy learners.\n","    Parameters\n","    -----------\n","    n_actions: int\n","        Number of actions.\n","    len_list: int, default=1\n","        Length of a list of actions recommended in each impression.\n","        When Open Bandit Dataset is used, 3 should be set.\n","    \"\"\"\n","\n","    n_actions: int\n","    len_list: int = 1\n","\n","    def __post_init__(self) -> None:\n","        \"\"\"Initialize class.\"\"\"\n","        check_scalar(self.n_actions, \"n_actions\", int, min_val=2)\n","        check_scalar(self.len_list, \"len_list\", int, min_val=1, max_val=self.n_actions)\n","\n","    @property\n","    def policy_type(self) -> PolicyType:\n","        \"\"\"Type of the bandit policy.\"\"\"\n","        return PolicyType.OFFLINE\n","\n","    @abstractmethod\n","    def fit(\n","        self,\n","    ) -> None:\n","        \"\"\"Fits an offline bandit policy using the given logged bandit feedback data.\"\"\"\n","        raise NotImplementedError\n","\n","    @abstractmethod\n","    def predict(self, context: np.ndarray) -> np.ndarray:\n","        \"\"\"Predict best action for new data.\n","        Parameters\n","        -----------\n","        context: array-like, shape (n_rounds_of_new_data, dim_context)\n","            Context vectors for new data.\n","        Returns\n","        -----------\n","        action: array-like, shape (n_rounds_of_new_data, n_actions, len_list)\n","            Action choices by a policy trained by calling the `fit` method.\n","        \"\"\"\n","        raise NotImplementedError"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"cellView":"form","id":"wRWI4rL7yc8p"},"source":["#@markdown IPWLearner Policy\n","\n","@dataclass\n","class IPWLearner(BaseOfflinePolicyLearner):\n","    \"\"\"Off-policy learner with Inverse Probability Weighting.\n","    Parameters\n","    -----------\n","    n_actions: int\n","        Number of actions.\n","    len_list: int, default=1\n","        Length of a list of actions recommended in each impression.\n","        When Open Bandit Dataset is used, 3 should be set.\n","    base_classifier: ClassifierMixin\n","        Machine learning classifier used to train an offline decision making policy.\n","    References\n","    ------------\n","    Miroslav Dudík, Dumitru Erhan, John Langford, and Lihong Li.\n","    \"Doubly Robust Policy Evaluation and Optimization.\", 2014.\n","    Damien Lefortier, Adith Swaminathan, Xiaotao Gu, Thorsten Joachims, and Maarten de Rijke.\n","    \"Large-scale Validation of Counterfactual Learning Methods: A Test-Bed.\", 2016.\n","    \"\"\"\n","\n","    base_classifier: Optional[ClassifierMixin] = None\n","\n","    def __post_init__(self) -> None:\n","        \"\"\"Initialize class.\"\"\"\n","        super().__post_init__()\n","        if self.base_classifier is None:\n","            self.base_classifier = LogisticRegression(random_state=12345)\n","        else:\n","            if not is_classifier(self.base_classifier):\n","                raise ValueError(\"base_classifier must be a classifier\")\n","        self.base_classifier_list = [\n","            clone(self.base_classifier) for _ in np.arange(self.len_list)\n","        ]\n","\n","    @staticmethod\n","    def _create_train_data_for_opl(\n","        context: np.ndarray,\n","        action: np.ndarray,\n","        reward: np.ndarray,\n","        pscore: np.ndarray,\n","    ) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:\n","        \"\"\"Create training data for off-policy learning.\n","        Parameters\n","        -----------\n","        context: array-like, shape (n_rounds, dim_context)\n","            Context vectors in each round, i.e., :math:`x_t`.\n","        action: array-like, shape (n_rounds,)\n","            Action sampled by behavior policy in each round of the logged bandit feedback, i.e., :math:`a_t`.\n","        reward: array-like, shape (n_rounds,)\n","            Observed rewards (or outcome) in each round, i.e., :math:`r_t`.\n","        pscore: array-like, shape (n_rounds,), default=None\n","            Propensity scores, the probability of selecting each action by behavior policy,\n","            in the given logged bandit data.\n","        Returns\n","        --------\n","        (X, sample_weight, y): Tuple[np.ndarray, np.ndarray, np.ndarray]\n","            Feature vectors, sample weights, and outcome for training the base machine learning model.\n","        \"\"\"\n","        return context, (reward / pscore), action\n","\n","    def fit(\n","        self,\n","        context: np.ndarray,\n","        action: np.ndarray,\n","        reward: np.ndarray,\n","        pscore: Optional[np.ndarray] = None,\n","        position: Optional[np.ndarray] = None,\n","    ) -> None:\n","        \"\"\"Fits an offline bandit policy using the given logged bandit feedback data.\n","        Note\n","        --------\n","        This `fit` method trains a deterministic policy :math:`\\\\pi: \\\\mathcal{X} \\\\rightarrow \\\\mathcal{A}`\n","        via a cost-sensitive classification reduction as follows:\n","        .. math::\n","            \\\\hat{\\\\pi}\n","            & \\\\in \\\\arg \\\\max_{\\\\pi \\\\in \\\\Pi} \\\\hat{V}_{\\\\mathrm{IPW}} (\\\\pi ; \\\\mathcal{D}) \\\\\\\\\n","            & = \\\\arg \\\\max_{\\\\pi \\\\in \\\\Pi} \\\\mathbb{E}_{\\\\mathcal{D}} \\\\left[\\\\frac{\\\\mathbb{I} \\\\{\\\\pi (x_{i})=a_{i} \\\\}}{\\\\pi_{b}(a_{i} | x_{i})} r_{i} \\\\right] \\\\\\\\\n","            & = \\\\arg \\\\min_{\\\\pi \\\\in \\\\Pi} \\\\mathbb{E}_{\\\\mathcal{D}} \\\\left[\\\\frac{r_i}{\\\\pi_{b}(a_{i} | x_{i})} \\\\mathbb{I} \\\\{\\\\pi (x_{i}) \\\\neq a_{i} \\\\} \\\\right],\n","        where :math:`\\\\mathbb{E}_{\\\\mathcal{D}} [\\cdot]` is the empirical average over observations in :math:`\\\\mathcal{D}`.\n","        See the reference for the details.\n","        Parameters\n","        -----------\n","        context: array-like, shape (n_rounds, dim_context)\n","            Context vectors in each round, i.e., :math:`x_t`.\n","        action: array-like, shape (n_rounds,)\n","            Action sampled by behavior policy in each round of the logged bandit feedback, i.e., :math:`a_t`.\n","        reward: array-like, shape (n_rounds,)\n","            Observed rewards (or outcome) in each round, i.e., :math:`r_t`.\n","        pscore: array-like, shape (n_rounds,), default=None\n","            Action choice probabilities of behavior policy (propensity scores), i.e., :math:`\\\\pi_b(a_t|x_t)`.\n","        position: array-like, shape (n_rounds,), default=None\n","            Position of recommendation interface where action was presented in each round of the given logged bandit data.\n","            If None is given, a learner assumes that there is only one position.\n","            When `len_list` > 1, position has to be set.\n","        \"\"\"\n","        check_bandit_feedback_inputs(\n","            context=context,\n","            action=action,\n","            reward=reward,\n","            pscore=pscore,\n","            position=position,\n","        )\n","        if (reward < 0).any():\n","            raise ValueError(\n","                \"A negative value is found in `reward`.\"\n","                \"`obp.policy.IPWLearner` cannot handle negative rewards,\"\n","                \"and please use `obp.policy.NNPolicyLearner` instead.\"\n","            )\n","        if pscore is None:\n","            n_actions = np.int(action.max() + 1)\n","            pscore = np.ones_like(action) / n_actions\n","        if self.len_list == 1:\n","            position = np.zeros_like(action, dtype=int)\n","        else:\n","            if position is None:\n","                raise ValueError(\"When `self.len_list=1`, `position` must be given.\")\n","\n","        for position_ in np.arange(self.len_list):\n","            X, sample_weight, y = self._create_train_data_for_opl(\n","                context=context[position == position_],\n","                action=action[position == position_],\n","                reward=reward[position == position_],\n","                pscore=pscore[position == position_],\n","            )\n","            self.base_classifier_list[position_].fit(\n","                X=X, y=y, sample_weight=sample_weight\n","            )\n","\n","    def predict(self, context: np.ndarray) -> np.ndarray:\n","        \"\"\"Predict best actions for new data.\n","        Note\n","        --------\n","        Action set predicted by this `predict` method can contain duplicate items.\n","        If you want a non-repetitive action set, then please use the `sample_action` method.\n","        Parameters\n","        -----------\n","        context: array-like, shape (n_rounds_of_new_data, dim_context)\n","            Context vectors for new data.\n","        Returns\n","        -----------\n","        action_dist: array-like, shape (n_rounds_of_new_data, n_actions, len_list)\n","            Action choices by a classifier, which can contain duplicate items.\n","            If you want a non-repetitive action set, please use the `sample_action` method.\n","        \"\"\"\n","        check_array(array=context, name=\"context\", expected_dim=2)\n","\n","        n_rounds = context.shape[0]\n","        action_dist = np.zeros((n_rounds, self.n_actions, self.len_list))\n","        for position_ in np.arange(self.len_list):\n","            predicted_actions_at_position = self.base_classifier_list[\n","                position_\n","            ].predict(context)\n","            action_dist[\n","                np.arange(n_rounds),\n","                predicted_actions_at_position,\n","                np.ones(n_rounds, dtype=int) * position_,\n","            ] += 1\n","        return action_dist\n","\n","    def predict_score(self, context: np.ndarray) -> np.ndarray:\n","        \"\"\"Predict non-negative scores for all possible products of action and position.\n","        Parameters\n","        -----------\n","        context: array-like, shape (n_rounds_of_new_data, dim_context)\n","            Context vectors for new data.\n","        Returns\n","        -----------\n","        score_predicted: array-like, shape (n_rounds_of_new_data, n_actions, len_list)\n","            Scores for all possible pairs of action and position predicted by a classifier.\n","        \"\"\"\n","        check_array(array=context, name=\"context\", expected_dim=2)\n","\n","        n_rounds = context.shape[0]\n","        score_predicted = np.zeros((n_rounds, self.n_actions, self.len_list))\n","        for position_ in np.arange(self.len_list):\n","            score_predicteds_at_position = self.base_classifier_list[\n","                position_\n","            ].predict_proba(context)\n","            score_predicted[:, :, position_] = score_predicteds_at_position\n","        return score_predicted\n","\n","    def sample_action(\n","        self,\n","        context: np.ndarray,\n","        tau: Union[int, float] = 1.0,\n","        random_state: Optional[int] = None,\n","    ) -> np.ndarray:\n","        \"\"\"Sample (non-repetitive) actions based on scores predicted by a classifier.\n","        Note\n","        --------\n","        This `sample_action` method samples a **non-repetitive** set of actions for new data :math:`x \\\\in \\\\mathcal{X}`\n","        by first computing non-negative scores for all possible candidate products of action and position\n","        :math:`(a, k) \\\\in \\\\mathcal{A} \\\\times \\\\mathcal{K}` (where :math:`\\\\mathcal{A}` is an action set and\n","        :math:`\\\\mathcal{K}` is a position set), and using softmax function as follows:\n","        .. math::\n","            & P (A_1 = a_1 | x) = \\\\frac{\\\\mathrm{exp}(f(x,a_1,1) / \\\\tau)}{\\\\sum_{a^{\\\\prime} \\\\in \\\\mathcal{A}} \\\\mathrm{exp}( f(x,a^{\\\\prime},1) / \\\\tau)} , \\\\\\\\\n","            & P (A_2 = a_2 | A_1 = a_1, x) = \\\\frac{\\\\mathrm{exp}(f(x,a_2,2) / \\\\tau)}{\\\\sum_{a^{\\\\prime} \\\\in \\\\mathcal{A} \\\\backslash \\\\{a_1\\\\}} \\\\mathrm{exp}(f(x,a^{\\\\prime},2) / \\\\tau )} ,\n","            \\\\ldots\n","        where :math:`A_k` is a random variable representing an action at a position :math:`k`.\n","        :math:`\\\\tau` is a temperature hyperparameter.\n","        :math:`f: \\\\mathcal{X} \\\\times \\\\mathcal{A} \\\\times \\\\mathcal{K} \\\\rightarrow \\\\mathbb{R}_{+}`\n","        is a scoring function which is now implemented in the `predict_score` method.\n","        Parameters\n","        ----------------\n","        context: array-like, shape (n_rounds_of_new_data, dim_context)\n","            Context vectors for new data.\n","        tau: int or float, default=1.0\n","            A temperature parameter, controlling the randomness of the action choice.\n","            As :math:`\\\\tau \\\\rightarrow \\\\infty`, the algorithm will select arms uniformly at random.\n","        random_state: int, default=None\n","            Controls the random seed in sampling actions.\n","        Returns\n","        -----------\n","        action: array-like, shape (n_rounds_of_new_data, n_actions, len_list)\n","            Action sampled by a trained classifier.\n","        \"\"\"\n","        check_array(array=context, name=\"context\", expected_dim=2)\n","        check_scalar(tau, name=\"tau\", target_type=(int, float), min_val=0)\n","\n","        n_rounds = context.shape[0]\n","        random_ = check_random_state(random_state)\n","        action = np.zeros((n_rounds, self.n_actions, self.len_list))\n","        score_predicted = self.predict_score(context=context)\n","        for i in tqdm(np.arange(n_rounds), desc=\"[sample_action]\", total=n_rounds):\n","            action_set = np.arange(self.n_actions)\n","            for position_ in np.arange(self.len_list):\n","                score_ = softmax(score_predicted[i, action_set, position_] / tau)\n","                action_sampled = random_.choice(action_set, p=score_, replace=False)\n","                action[i, action_sampled, position_] = 1\n","                action_set = np.delete(action_set, action_set == action_sampled)\n","        return action\n","\n","    def predict_proba(\n","        self,\n","        context: np.ndarray,\n","        tau: Union[int, float] = 1.0,\n","    ) -> np.ndarray:\n","        \"\"\"Obtains action choice probabilities for new data based on scores predicted by a classifier.\n","        Note\n","        --------\n","        This `predict_proba` method obtains action choice probabilities for new data :math:`x \\\\in \\\\mathcal{X}`\n","        by first computing non-negative scores for all possible candidate actions\n","        :math:`a \\\\in \\\\mathcal{A}` (where :math:`\\\\mathcal{A}` is an action set),\n","        and using a Plackett-Luce ranking model as follows:\n","        .. math::\n","            P (A = a | x) = \\\\frac{\\\\mathrm{exp}(f(x,a) / \\\\tau)}{\\\\sum_{a^{\\\\prime} \\\\in \\\\mathcal{A}} \\\\mathrm{exp}(f(x,a^{\\\\prime}) / \\\\tau)},\n","        where :math:`A` is a random variable representing an action, and :math:`\\\\tau` is a temperature hyperparameter.\n","        :math:`f: \\\\mathcal{X} \\\\times \\\\mathcal{A} \\\\rightarrow \\\\mathbb{R}_{+}`\n","        is a scoring function which is now implemented in the `predict_score` method.\n","        **Note that this method can be used only when `len_list=1`, please use the `sample_action` method otherwise.**\n","        Parameters\n","        ----------------\n","        context: array-like, shape (n_rounds_of_new_data, dim_context)\n","            Context vectors for new data.\n","        tau: int or float, default=1.0\n","            A temperature parameter, controlling the randomness of the action choice.\n","            As :math:`\\\\tau \\\\rightarrow \\\\infty`, the algorithm will select arms uniformly at random.\n","        Returns\n","        -----------\n","        choice_prob: array-like, shape (n_rounds_of_new_data, n_actions, len_list)\n","            Action choice probabilities obtained by a trained classifier.\n","        \"\"\"\n","        assert (\n","            self.len_list == 1\n","        ), \"predict_proba method cannot be used when `len_list != 1`\"\n","        check_array(array=context, name=\"context\", expected_dim=2)\n","        check_scalar(tau, name=\"tau\", target_type=(int, float), min_val=0)\n","\n","        score_predicted = self.predict_score(context=context)\n","        choice_prob = softmax(score_predicted / tau, axis=1)\n","        return choice_prob"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NVGmw7LrnkjN"},"source":["## Main"]},{"cell_type":"code","metadata":{"id":"Llj-xWcAnqqz"},"source":["# from obp.dataset import linear_behavior_policy\n","# from obp.dataset import logistic_reward_function\n","# from obp.dataset import SyntheticBanditDataset\n","# from obp.ope import DirectMethod\n","# from obp.ope import DoublyRobust\n","# from obp.ope import DoublyRobustWithShrinkage\n","# from obp.ope import InverseProbabilityWeighting\n","# from obp.ope import OffPolicyEvaluation\n","# from obp.ope import RegressionModel\n","# from obp.ope import SelfNormalizedDoublyRobust\n","# from obp.ope import SelfNormalizedInverseProbabilityWeighting\n","# from obp.ope import SwitchDoublyRobust\n","# from obp.policy import IPWLearner"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vIFAFZLNn0bx"},"source":["### Hyperparams"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rtjAGcXTnNGl","executionInfo":{"status":"ok","timestamp":1632118263894,"user_tz":-330,"elapsed":75,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}},"outputId":"5ed32f66-b5b1-4980-b721-fa6a96fbcdde"},"source":["%%writefile hyperparams.yaml\n","lightgbm:\n","  n_estimators: 30\n","  learning_rate: 0.01\n","  max_depth: 5\n","  min_samples_leaf: 10\n","  random_state: 12345\n","logistic_regression:\n","  max_iter: 10000\n","  C: 100\n","  random_state: 12345\n","random_forest:\n","  n_estimators: 30\n","  max_depth: 5\n","  min_samples_leaf: 10\n","  random_state: 12345"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Overwriting hyperparams.yaml\n"]}]},{"cell_type":"code","metadata":{"id":"j83Xy-9bn2k6"},"source":["# hyperparameters of the regression model used in model dependent OPE estimators\n","with open(\"hyperparams.yaml\", \"rb\") as f:\n","    hyperparams = yaml.safe_load(f)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"yI9sfhIXoBfU"},"source":["### Base Models"]},{"cell_type":"code","metadata":{"id":"ZyCKvJT_oBDW"},"source":["base_model_dict = dict(\n","    logistic_regression=LogisticRegression,\n","    lightgbm=GradientBoostingClassifier,\n","    random_forest=RandomForestClassifier,\n",")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WTLlloiaoLyI"},"source":["### OPE Estimators"]},{"cell_type":"code","metadata":{"id":"BpcgNZv9oOp_"},"source":["# compared OPE estimators\n","ope_estimators = [\n","    DirectMethod(),\n","    InverseProbabilityWeighting(),\n","    SelfNormalizedInverseProbabilityWeighting(),\n","    DoublyRobust(),\n","    SelfNormalizedDoublyRobust(),\n","    SwitchDoublyRobust(lambda_=1.0, estimator_name=\"switch-dr (lambda=1)\"),\n","    SwitchDoublyRobust(lambda_=100.0, estimator_name=\"switch-dr (lambda=100)\"),\n","    DoublyRobustWithShrinkage(lambda_=1.0, estimator_name=\"dr-os (lambda=1)\"),\n","    DoublyRobustWithShrinkage(lambda_=100.0, estimator_name=\"dr-os (lambda=100)\"),\n","]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BAWQv_YtpOjm"},"source":["### Arg Parse"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yHozhJsHpR33","executionInfo":{"status":"ok","timestamp":1632118263901,"user_tz":-330,"elapsed":57,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}},"outputId":"77e15fbb-6776-4fb1-addc-66383840a652"},"source":["    parser = argparse.ArgumentParser(\n","        description=\"evaluate off-policy estimators with synthetic bandit data.\"\n","    )\n","    parser.add_argument(\n","        \"--n_runs\", type=int, default=1, help=\"number of simulations in the experiment.\"\n","    )\n","    parser.add_argument(\n","        \"--n_rounds\",\n","        type=int,\n","        default=10000,\n","        help=\"number of rounds for synthetic bandit feedback.\",\n","    )\n","    parser.add_argument(\n","        \"--n_actions\",\n","        type=int,\n","        default=10,\n","        help=\"number of actions for synthetic bandit feedback.\",\n","    )\n","    parser.add_argument(\n","        \"--dim_context\",\n","        type=int,\n","        default=5,\n","        help=\"dimensions of context vectors characterizing each round.\",\n","    )\n","    parser.add_argument(\n","        \"--base_model_for_evaluation_policy\",\n","        type=str,\n","        choices=[\"logistic_regression\", \"lightgbm\", \"random_forest\"],\n","        default='random_forest',\n","        help=\"base ML model for evaluation policy, logistic_regression, random_forest or lightgbm.\",\n","    )\n","    parser.add_argument(\n","        \"--base_model_for_reg_model\",\n","        type=str,\n","        choices=[\"logistic_regression\", \"lightgbm\", \"random_forest\"],\n","        default='logistic_regression',\n","        help=\"base ML model for regression model, logistic_regression, random_forest or lightgbm.\",\n","    )\n","    parser.add_argument(\n","        \"--n_jobs\",\n","        type=int,\n","        default=2,\n","        help=\"the maximum number of concurrently running jobs.\",\n","    )\n","    parser.add_argument(\"--random_state\", type=int, default=12345)\n","    args = parser.parse_args(args={})\n","    print(args)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Namespace(base_model_for_evaluation_policy='random_forest', base_model_for_reg_model='logistic_regression', dim_context=5, n_actions=10, n_jobs=2, n_rounds=10000, n_runs=1, random_state=12345)\n"]}]},{"cell_type":"code","metadata":{"id":"aEbvz_7VpuE9"},"source":["# configurations\n","n_runs = args.n_runs\n","n_rounds = args.n_rounds\n","n_actions = args.n_actions\n","dim_context = args.dim_context\n","base_model_for_evaluation_policy = args.base_model_for_evaluation_policy\n","base_model_for_reg_model = args.base_model_for_reg_model\n","n_jobs = args.n_jobs\n","random_state = args.random_state"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"s5VxXYFopxns"},"source":["### Process"]},{"cell_type":"code","metadata":{"id":"3BDjUIP8py9d"},"source":["def process(i: int):\n","    # synthetic data generator\n","    dataset = SyntheticBanditDataset(\n","        n_actions=n_actions,\n","        dim_context=dim_context,\n","        reward_function=logistic_reward_function,\n","        behavior_policy_function=linear_behavior_policy,\n","        random_state=i,\n","    )\n","    # define evaluation policy using IPWLearner\n","    evaluation_policy = IPWLearner(\n","        n_actions=dataset.n_actions,\n","        base_classifier=base_model_dict[base_model_for_evaluation_policy](\n","            **hyperparams[base_model_for_evaluation_policy]\n","        ),\n","    )\n","    # sample new training and test sets of synthetic logged bandit feedback\n","    bandit_feedback_train = dataset.obtain_batch_bandit_feedback(n_rounds=n_rounds)\n","    bandit_feedback_test = dataset.obtain_batch_bandit_feedback(n_rounds=n_rounds)\n","    # train the evaluation policy on the training set of the synthetic logged bandit feedback\n","    evaluation_policy.fit(\n","        context=bandit_feedback_train[\"context\"],\n","        action=bandit_feedback_train[\"action\"],\n","        reward=bandit_feedback_train[\"reward\"],\n","        pscore=bandit_feedback_train[\"pscore\"],\n","    )\n","    # predict the action decisions for the test set of the synthetic logged bandit feedback\n","    action_dist = evaluation_policy.predict(\n","        context=bandit_feedback_test[\"context\"],\n","    )\n","    # estimate the mean reward function of the test set of synthetic bandit feedback with ML model\n","    regression_model = RegressionModel(\n","        n_actions=dataset.n_actions,\n","        action_context=dataset.action_context,\n","        base_model=base_model_dict[base_model_for_reg_model](\n","            **hyperparams[base_model_for_reg_model]\n","        ),\n","    )\n","    estimated_rewards_by_reg_model = regression_model.fit_predict(\n","        context=bandit_feedback_test[\"context\"],\n","        action=bandit_feedback_test[\"action\"],\n","        reward=bandit_feedback_test[\"reward\"],\n","        n_folds=3,  # 3-fold cross-fitting\n","        random_state=random_state,\n","    )\n","    # evaluate estimators' performances using relative estimation error (relative-ee)\n","    ope = OffPolicyEvaluation(\n","        bandit_feedback=bandit_feedback_test,\n","        ope_estimators=ope_estimators,\n","    )\n","    relative_ee_i = ope.evaluate_performance_of_estimators(\n","        ground_truth_policy_value=dataset.calc_ground_truth_policy_value(\n","            expected_reward=bandit_feedback_test[\"expected_reward\"],\n","            action_dist=action_dist,\n","        ),\n","        action_dist=action_dist,\n","        estimated_rewards_by_reg_model=estimated_rewards_by_reg_model,\n","    )\n","\n","    return relative_ee_i"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"O5apkt21p5cW"},"source":["### Run"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7KYAR6Z_nanI","executionInfo":{"status":"ok","timestamp":1632118266121,"user_tz":-330,"elapsed":2260,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}},"outputId":"0ff50898-0f7d-4831-feea-34bb27a2a5ae"},"source":["processed = Parallel(\n","    n_jobs=n_jobs,\n","    verbose=50,\n",")([delayed(process)(i) for i in np.arange(n_runs)])\n","relative_ee_dict = {est.estimator_name: dict() for est in ope_estimators}\n","for i, relative_ee_i in enumerate(processed):\n","    for (\n","        estimator_name,\n","        relative_ee_,\n","    ) in relative_ee_i.items():\n","        relative_ee_dict[estimator_name][i] = relative_ee_\n","relative_ee_df = DataFrame(relative_ee_dict).describe().T.round(6)\n","\n","print(\"=\" * 45)\n","print(f\"random_state={random_state}\")\n","print(\"-\" * 45)\n","print(relative_ee_df[[\"mean\", \"std\"]])\n","print(\"=\" * 45)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[Parallel(n_jobs=2)]: Using backend LokyBackend with 2 concurrent workers.\n","[Parallel(n_jobs=2)]: Done   1 tasks      | elapsed:    2.3s\n","[Parallel(n_jobs=2)]: Done   1 out of   1 | elapsed:    2.3s finished\n","=============================================\n","random_state=12345\n","---------------------------------------------\n","                            mean  std\n","dm                      0.120247  NaN\n","ipw                     0.062459  NaN\n","snipw                   0.021442  NaN\n","dr                      0.014321  NaN\n","sndr                    0.009687  NaN\n","switch-dr (lambda=1)    0.120247  NaN\n","switch-dr (lambda=100)  0.014321  NaN\n","dr-os (lambda=1)        0.118951  NaN\n","dr-os (lambda=100)      0.064524  NaN\n","=============================================\n"]}]},{"cell_type":"code","metadata":{"id":"brtzYQVKp2oK"},"source":["# save results of the evaluation of off-policy estimators in './logs' directory.\n","log_path = Path(\"./logs\")\n","log_path.mkdir(exist_ok=True, parents=True)\n","relative_ee_df.to_csv(log_path / \"relative_ee_of_ope_estimators.csv\")"],"execution_count":null,"outputs":[]}]}