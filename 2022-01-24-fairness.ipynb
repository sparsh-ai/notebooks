{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"2022-01-24-fairness.ipynb","provenance":[{"file_id":"https://github.com/recohut/nbs/blob/main/raw/T297944%20%7C%20Personalized%20Counterfactual%20Fairness%20in%20Recommendation.ipynb","timestamp":1644665651266}],"collapsed_sections":[],"toc_visible":true,"authorship_tag":"ABX9TyMUJVfB13uWFCOC0jY8eshV"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"b5c1118b4bfb4f9fa0695e91163a1e47":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_47532d7cbe0a4c5fb1f1b3575d836ea9","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_32db06de410a4481a5ca0b75b42f7ec2","IPY_MODEL_f0c36db39c5746e79a5643c8dcb21abf","IPY_MODEL_2d8aeab54e2d4b0ab739c96445f91394"]}},"47532d7cbe0a4c5fb1f1b3575d836ea9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":"row wrap","width":"100px","min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":"inline-flex","left":null}},"32db06de410a4481a5ca0b75b42f7ec2":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_a39ae408211c46bd95e4a333f2eabbff","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"Prepare Batches:  61%","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_79f7f84ff4b640c7bc7c3cc40f161798"}},"f0c36db39c5746e79a5643c8dcb21abf":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_62be46a350e846a5995ccad4fc4a5954","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"","max":196,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":120,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_b720c3aaf1644da789fdad2d70251972"}},"2d8aeab54e2d4b0ab739c96445f91394":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_4a16f523a0ca4d83b0dd9ce12ff32e4e","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 120/196 [15:05&lt;11:21,  8.97s/it]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_e02966e60c78464496a57d61417d7aa1"}},"a39ae408211c46bd95e4a333f2eabbff":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"79f7f84ff4b640c7bc7c3cc40f161798":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"62be46a350e846a5995ccad4fc4a5954":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"b720c3aaf1644da789fdad2d70251972":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":"2","_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"4a16f523a0ca4d83b0dd9ce12ff32e4e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"e02966e60c78464496a57d61417d7aa1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}}},"cells":[{"cell_type":"markdown","metadata":{"id":"QAVqXpOE4QaM"},"source":["# Personalized Counterfactual Fairness in Recommendation"]},{"cell_type":"markdown","metadata":{"id":"iVJdn7A33_bk"},"source":["## Setup"]},{"cell_type":"markdown","metadata":{"id":"BXJY8c9d4Xi5"},"source":["### Installations"]},{"cell_type":"code","metadata":{"id":"TGJ5fBMK4Xgj"},"source":["!pip install -q wget"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GB_yDppW3_Yt"},"source":["### Imports"]},{"cell_type":"code","metadata":{"id":"yebvVSIT3_WD"},"source":["import sys\n","import os\n","import wget\n","import logging\n","import os.path as osp\n","from pathlib import Path\n","import zipfile\n","import numpy as np\n","import pandas as pd\n","from tqdm.notebook import tqdm\n","import pickle\n","from sklearn.metrics import *\n","from sklearn.preprocessing import LabelBinarizer\n","import itertools as it\n","from time import time\n","import gc\n","from collections import defaultdict, namedtuple, OrderedDict\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.utils.data import DataLoader"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NyxCtlrJ3_Ta"},"source":["### Params"]},{"cell_type":"code","metadata":{"id":"MXBwnUCD3_RD"},"source":["class Args:\n","    path = '/content'\n","    train_suffix = '.train.tsv'             # train file suffix\n","    validation_suffix = '.validation.tsv'   # validation file suffix\n","    test_suffix = '.test.tsv'               # test file suffix\n","    all_suffix = '.all.tsv'                 # all data file\n","    feature_suffix = '.features.txt'         # feature file\n","    test_pkl_suffix = '.test.pkl'         # prepared test data pickle file suffix\n","    valid_pkl_suffix = '.validation.pkl'  # prepared validation data pickle file suffix\n","    USER = 'uid'                   # user column name\n","    ITEM = 'iid'                   # item column name\n","    LABEL = 'label'                 # label column name\n","    RANK_FILE_NAME = 'rank.csv'     # Trained model generated ranking list\n","    SAMPLE_ID = 'sample_id'         # sample id for each record\n","    gpu ='0' # Set CUDA_VISIBLE_DEVICES\n","    verbose = logging.INFO # Logging Level, 0, 10, ..., 50\n","    log_file = 'log.txt' # Logging file path\n","    result_file = 'result.npy' # Result file path\n","    random_seed = 2020 # Random seed of numpy and tensorflow\n","    train = 1 # To train the model or not\n","    dataset = 'ml1M'\n","    sep = '\\t' # sep of csv file\n","    label = 'label' # name of dataset label column\n","    disc_batch_size = 7000 # discriminator train batch size\n","    train_num_neg = 1 # Negative sample num for each instance in train set\n","    vt_num_neg = -1 # Number of negative sample in validation/testing stage\n","    model_path ='model.pt' # Model save path\n","    u_vector_size = 64 # user vector size\n","    i_vector_size = 64 # item vector size\n","    filter_mode = 'combine' # combine for using one filter per sensitive feature, separate for using one filter per sensitive feature combination\n","    load = 0 # Whether load model and continue to train\n","    load_attack = False # Whether load attacker model and continue to train\n","    epoch = 100 # Number of epochs\n","    disc_epoch = 500 # Number of epochs for training extra discriminator\n","    check_epoch = 1 # Check every epochs\n","    early_stop = 1 # whether to early-stop\n","    lr = 0.001 # Learning rate\n","    lr_attack = 0.001 # attacker learning rate\n","    batch_size = 128 # Batch size during training\n","    vt_batch_size = 512 # Batch size during testing\n","    dropout = 0.2 # Dropout probability for each deep layer\n","    l2 = 1e-4 # Weight of l2_regularize in loss\n","    l2_attack = 1e-4 # Weight of attacker l2_regularize in loss\n","    no_filter = False # if or not use filters\n","    reg_weight = 1 # Trade off for adversarial penalty\n","    d_steps = 10 # the number of steps of updating discriminator\n","    optimizer = 'GD' # 'optimizer: GD, Adam, Adagrad\n","    metric = \"RMSE\" # metrics: RMSE, MAE, AUC, F1, Accuracy, Precision, Recall\n","    skip_eval = 0 # number of epochs without evaluation\n","    num_worker = 2 # number of processes for multi-processing data loading\n","    fix_one = False # fix one feature for evaluation\n","    eval_disc = False # train extra discriminator for evaluation\n","    data_reader = 'RecDataReader' # Choose data_reader\n","    data_processor = 'RecDataset' # Choose data_processor\n","    model_name = 'BiasedMF' # Choose model to run\n","    runner = 'RecRunner' # Choose runner\n","\n","args = Args()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"DSwbYFgC_6F0"},"source":["LOWER_METRIC_LIST = [\"rmse\", 'mae']"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Q40X4lHf4JHw"},"source":["### Logger"]},{"cell_type":"code","metadata":{"id":"cibwpV5L4JFb"},"source":["logging.basicConfig(stream=sys.stdout,\n","                    level = logging.DEBUG,\n","                    format='%(asctime)s [%(levelname)s] : %(message)s',\n","                    datefmt='%d-%b-%y %H:%M:%S')\n","\n","logger = logging.getLogger('T297944 Logger')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"yTA5LxDK4ES0"},"source":["## Utilities"]},{"cell_type":"markdown","metadata":{"id":"fznS-E31A53b"},"source":["### Dataset"]},{"cell_type":"code","metadata":{"id":"fLUcWP4p4EQd"},"source":["def download_movielens():\n","    download_link = 'https://github.com/sparsh-ai/fairness-recsys/raw/main/data/bronze/ml1m/ml1m_t297944.zip'\n","    save_path = osp.join(args.path,args.dataset+'.zip')\n","    save_path_extracted = osp.join(args.path,args.dataset)\n","    Path(save_path_extracted).mkdir(parents=True, exist_ok=True)\n","    if not os.listdir(save_path_extracted):\n","        wget.download(download_link, out=save_path)\n","        with zipfile.ZipFile(save_path, 'r') as zip_ref:\n","            zip_ref.extractall(save_path_extracted)\n","        logger.info('Files saved in {}'.format(save_path_extracted))\n","    else:\n","        logger.info('Files already exists in {}, skipping!'.format(save_path_extracted))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MxKtfPC1A7nD"},"source":["class DataReader:\n","    def __init__(self, path, dataset_name, sep='\\t', seq_sep=','):\n","        self.path = osp.join(path, dataset_name)\n","        self.dataset_name = dataset_name\n","        self.sep = sep\n","        self.seq_sep = seq_sep\n","        self.train_file = osp.join(self.path, dataset_name + args.train_suffix)\n","        self.validation_file = osp.join(self.path, dataset_name + args.validation_suffix)\n","        self.test_file = osp.join(self.path, dataset_name + args.test_suffix)\n","        self.all_file = osp.join(self.path, dataset_name + args.all_suffix)\n","        self.feature_file = osp.join(self.path, dataset_name + args.feature_suffix)\n","        self._load_data()\n","        self.features = self._load_feature() if osp.exists(self.feature_file) else None\n","\n","    def _load_data(self):\n","        if osp.exists(self.all_file):\n","            logger.info(\"load all csv...\")\n","            self.all_df = pd.read_csv(self.all_file, sep=self.sep)\n","        else:\n","            raise FileNotFoundError('all file is not found.')\n","        if osp.exists(self.train_file):\n","            logger.info(\"load train csv...\")\n","            self.train_df = pd.read_csv(self.train_file, sep=self.sep)\n","            logger.info(\"size of train: %d\" % len(self.train_df))\n","        else:\n","            raise FileNotFoundError('train file is not found.')\n","        if osp.exists(self.validation_file):\n","            logger.info(\"load validation csv...\")\n","            self.validation_df = pd.read_csv(self.validation_file, sep=self.sep)\n","            logger.info(\"size of validation: %d\" % len(self.validation_df))\n","        else:\n","            raise FileNotFoundError('validation file is not found.')\n","        if osp.exists(self.test_file):\n","            logger.info(\"load test csv...\")\n","            self.test_df = pd.read_csv(self.test_file, sep=self.sep)\n","            logger.info(\"size of test: %d\" % len(self.test_df))\n","        else:\n","            raise FileNotFoundError('test file is not found.')\n","\n","    def _load_feature(self):\n","        \"\"\"\n","        load pre-trained/feature embeddings. It is saved as a numpy text file.\n","        :return:\n","        \"\"\"\n","        return np.loadtxt(self.feature_file, dtype=np.float32)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hCqf2POUCoeQ"},"source":["class RecDataReader(DataReader):\n","    def __init__(self, path, dataset_name, sep='\\t', seq_sep=','):\n","        super().__init__(path, dataset_name, sep, seq_sep)\n","        self.user_ids_set = set(self.all_df[args.USER].tolist())\n","        self.item_ids_set = set(self.all_df[args.ITEM].tolist())\n","        self.num_nodes = len(self.user_ids_set) + len(self.item_ids_set)\n","        self.train_item2users_dict = self._prepare_item2users_dict(self.train_df)\n","\n","        self.all_user2items_dict = self._prepare_user2items_dict(self.all_df)\n","        self.train_user2items_dict = self._prepare_user2items_dict(self.train_df)\n","        self.valid_user2items_dict = self._prepare_user2items_dict(self.validation_df)\n","        self.test_user2items_dict = self._prepare_user2items_dict(self.test_df)\n","        # add feature info for discriminator and filters\n","        uid_iid_label = [args.USER, args.ITEM, args.LABEL]\n","        self.feature_columns = [name for name in self.train_df.columns.tolist() if name not in uid_iid_label]\n","        Feature = namedtuple('Feature', ['num_class', 'label_min', 'label_max', 'name'])\n","        self.feature_info = \\\n","            OrderedDict({idx + 1: Feature(self.all_df[col].nunique(), self.all_df[col].min(), self.all_df[col].max(),\n","                                          col) for idx, col in enumerate(self.feature_columns)})\n","        self.num_features = len(self.feature_columns)\n","\n","    @staticmethod\n","    def _prepare_user2items_dict(df):\n","        df_groups = df.groupby(args.USER)\n","        user_sample_dict = defaultdict(set)\n","        for uid, group in df_groups:\n","            user_sample_dict[uid] = set(group[args.ITEM].tolist())\n","        return user_sample_dict\n","\n","    @staticmethod\n","    def _prepare_item2users_dict(df):\n","        df_groups = df.groupby(args.ITEM)\n","        user_sample_dict = defaultdict(set)\n","        for uid, group in df_groups:\n","            user_sample_dict[uid] = set(group[args.USER].tolist())\n","        return user_sample_dict"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Q_wJAytIDRoB"},"source":["class DiscriminatorDataReader:\n","    def __init__(self, path, dataset_name, sep='\\t', seq_sep=',', test_ratio=0.1):\n","        self.path = osp.join(path, dataset_name)\n","        self.sep = sep\n","        self.seq_sep = seq_sep\n","        self.all_file = osp.join(self.path, dataset_name + args.all_suffix)\n","        self.train_attacker_file = osp.join(self.path, dataset_name + '.attacker' + args.train_suffix)\n","        self.test_attacker_file = osp.join(self.path, dataset_name + '.attacker' + args.test_suffix)\n","        self.all_df = pd.read_csv(self.all_file, sep='\\t')\n","\n","        # add feature info for discriminator and filters\n","        uid_iid_label = [args.USER, args.ITEM, args.LABEL]\n","        self.feature_columns = [name for name in self.all_df.columns.tolist() if name not in uid_iid_label]\n","\n","        Feature = namedtuple('Feature', ['num_class', 'label_min', 'label_max', 'name'])\n","        self.feature_info = \\\n","            OrderedDict({idx + 1: Feature(self.all_df[col].nunique(), self.all_df[col].min(), self.all_df[col].max(),\n","                                          col) for idx, col in enumerate(self.feature_columns)})\n","        self.f_name_2_idx = {f_name: idx + 1 for idx, f_name in enumerate(self.feature_columns)}\n","        self.num_features = len(self.feature_columns)\n","        if osp.exists(self.train_attacker_file) and osp.exists(self.test_attacker_file):\n","            self.train_df = pd.read_csv(self.train_attacker_file, sep='\\t')\n","            self.test_df = pd.read_csv(self.test_attacker_file, sep='\\t')\n","        else:\n","            self.train_df, self.test_df = self._init_feature_df(self.all_df, test_ratio)\n","\n","    def _init_feature_df(self, all_df, test_ratio):\n","        logger.info('Initializing attacker train/test file...')\n","        feature_df = pd.DataFrame()\n","        all_df = all_df.sort_values(by='uid')\n","        all_group = all_df.groupby('uid')\n","\n","        uid_list = []\n","        feature_list_dict = {key: [] for key in self.feature_columns}\n","        for uid, group in all_group:\n","            uid_list.append(uid)\n","            for key in feature_list_dict:\n","                feature_list_dict[key].append(group[key].tolist()[0])\n","        feature_df[args.USER] = uid_list\n","        for f in self.feature_columns:\n","            feature_df[f] = feature_list_dict[f]\n","\n","        test_size = int(len(feature_df) * test_ratio)\n","        sign = True\n","        counter = 0\n","        while sign:\n","            test_set = feature_df.sample(n=test_size).sort_index()\n","            for f in self.feature_columns:\n","                num_class = self.feature_info[self.f_name_2_idx[f]].num_class\n","                val_range = set([i for i in range(num_class)])\n","                test_range = set(test_set[f].tolist())\n","                if len(val_range) != len(test_range):\n","                    sign = True\n","                    break\n","                else:\n","                    sign = False\n","            print(counter)\n","            counter += 1\n","\n","        train_set = feature_df.drop(test_set.index)\n","        train_set.to_csv(self.train_attacker_file, sep='\\t', index=False)\n","        test_set.to_csv(self.test_attacker_file, sep='\\t', index=False)\n","        return train_set, test_set"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"XlYwe4eAD8yt"},"source":["class RecDataset:\n","    def __init__(self, data_reader, stage, batch_size=128, num_neg=1):\n","        self.data_reader = data_reader\n","        self.num_user = len(data_reader.user_ids_set)\n","        self.num_item = len(data_reader.item_ids_set)\n","        self.batch_size = batch_size\n","        self.stage = stage\n","        self.num_neg = num_neg\n","        # prepare test/validation dataset\n","        valid_pkl_path = osp.join(self.data_reader.path, self.data_reader.dataset_name + args.valid_pkl_suffix)\n","        test_pkl_path = osp.join(self.data_reader.path, self.data_reader.dataset_name + args.test_pkl_suffix)\n","        if self.stage == 'valid':\n","            if osp.exists(valid_pkl_path):\n","                with open(valid_pkl_path, 'rb') as file:\n","                    logger.info('Load validation data from pickle file.')\n","                    self.data = pickle.load(file)\n","            else:\n","                self.data = self._get_data()\n","                with open(valid_pkl_path, 'wb') as file:\n","                    pickle.dump(self.data, file)\n","        elif self.stage == 'test':\n","            if osp.exists(test_pkl_path):\n","                with open(test_pkl_path, 'rb') as file:\n","                    logger.info('Load test data from pickle file.')\n","                    self.data = pickle.load(file)\n","            else:\n","                self.data = self._get_data()\n","                with open(test_pkl_path, 'wb') as file:\n","                    pickle.dump(self.data, file)\n","        else:\n","            self.data = self._get_data()\n","\n","    def __len__(self):\n","        return len(self.data)\n","\n","    def __getitem__(self, idx):\n","        return self.data[idx]\n","\n","    def _get_data(self):\n","        if self.stage == 'train':\n","            return self._get_train_data()\n","        else:\n","            return self._get_vt_data()\n","\n","    def _get_train_data(self):\n","        df = self.data_reader.train_df\n","        df[args.SAMPLE_ID] = df.index\n","        columns_order = [args.USER, args.ITEM, args.SAMPLE_ID, args.LABEL] + [f_col for f_col in self.data_reader.feature_columns]\n","        data = df[columns_order].to_numpy()\n","        return data\n","\n","    def _get_vt_data(self):\n","        if self.stage == 'valid':\n","            df = self.data_reader.validation_df\n","            logger.info('Prepare validation data...')\n","        elif self.stage == 'test':\n","            df = self.data_reader.test_df\n","            logger.info('Prepare test data...')\n","        else:\n","            raise ValueError('Wrong stage in dataset.')\n","            \n","        df[args.SAMPLE_ID] = df.index\n","        columns_order = [args.USER, args.ITEM, args.SAMPLE_ID, args.LABEL] + [f_col for f_col in self.data_reader.feature_columns]\n","        data = df[columns_order].to_numpy()\n","\n","        total_batches = int((len(df) + self.batch_size - 1) / self.batch_size)\n","        batches = []\n","\n","        for n_batch in tqdm(range(total_batches), leave=False, ncols=100, mininterval=1, desc='Prepare Batches'):\n","            batch_start = n_batch * self.batch_size\n","            batch_end = min(len(df), batch_start + self.batch_size)\n","\n","            real_batch_size = batch_end - batch_start\n","\n","            batch = data[batch_start:batch_start + real_batch_size, :]\n","\n","            inputs = np.asarray(batch)[:, 0:3]\n","            labels = np.asarray(batch)[:, 3]\n","            features = np.asarray(batch)[:, 4:]\n","            inputs = np.concatenate((inputs, features), axis=1)\n","\n","            neg_samples = self._neg_samples_from_all(inputs, self.num_neg)\n","            neg_labels = np.asarray([0] * neg_samples.shape[0])\n","\n","            tmp_sample = np.concatenate((inputs, neg_samples), axis=0)\n","            samples = torch.from_numpy(tmp_sample[:, 0:3])\n","            labels = torch.from_numpy(np.concatenate((labels, neg_labels), axis=0))\n","            features = torch.from_numpy(tmp_sample[:, 3:])\n","\n","            feed_dict = {'X': samples, args.LABEL: labels, 'features': features}\n","            batches.append(feed_dict)\n","\n","            gc.collect()\n","\n","        return batches\n","\n","    def collate_fn(self, batch):\n","        if self.stage == 'train':\n","            feed_dict = self._collate_train(batch)\n","        else:\n","            feed_dict = self._collate_vt(batch)\n","        return feed_dict\n","\n","    def _collate_train(self, batch):\n","        inputs = np.asarray(batch)[:, 0:3]\n","        labels = np.asarray(batch)[:, 3]\n","        features = np.asarray(batch)[:, 4:]\n","        neg_samples = self._neg_sampler(inputs)\n","        neg_samples = np.insert(neg_samples, 0, inputs[:, 0], axis=1)\n","        neg_samples = np.insert(neg_samples, 2, inputs[:, 2], axis=1)\n","        neg_labels = np.asarray([0] * neg_samples.shape[0])\n","        neg_features = np.copy(features)\n","        assert len(inputs) == len(neg_samples)\n","        samples = torch.from_numpy(np.concatenate((inputs, neg_samples), axis=0))\n","        labels = torch.from_numpy(np.concatenate((labels, neg_labels), axis=0))\n","        features = torch.from_numpy((np.concatenate((features, neg_features), axis=0)))\n","        feed_dict = {'X': samples, args.LABEL: labels, 'features': features}\n","        return feed_dict\n","\n","    @staticmethod\n","    def _collate_vt(data):\n","        return data\n","\n","    def _neg_sampler(self, batch):\n","        neg_items = np.random.randint(1, self.num_item, size=(len(batch), self.num_neg))\n","        for i, (user, _, _) in enumerate(batch):\n","            user_clicked_set = self.data_reader.all_user2items_dict[user]\n","            for j in range(self.num_neg):\n","                while neg_items[i][j] in user_clicked_set:\n","                    neg_items[i][j] = np.random.randint(1, self.num_item)\n","        return neg_items\n","\n","    def _neg_samples_from_all(self, batch, num_neg=-1):\n","        neg_items = None\n","        for idx, data in enumerate(batch):\n","            user = data[0]\n","            sample_id = data[2]\n","            features = data[3:]\n","            neg_candidates = list(self.data_reader.item_ids_set - self.data_reader.all_user2items_dict[user])\n","            if num_neg != -1:\n","                if num_neg <= len(neg_candidates):\n","                    neg_candidates = np.random.choice(neg_candidates, num_neg, replace=False)\n","                else:\n","                    neg_candidates = np.random.choice(neg_candidates, len(neg_candidates), replace=False)\n","            user_arr = np.asarray([user] * len(neg_candidates))\n","            id_arr = np.asarray([sample_id] * len(neg_candidates))\n","            feature_arr = np.tile(features, (len(neg_candidates), 1))\n","            neg_candidates = np.expand_dims(np.asarray(neg_candidates), axis=1)\n","            neg_candidates = np.insert(neg_candidates, 0, user_arr, axis=1)\n","            neg_candidates = np.insert(neg_candidates, 2, id_arr, axis=1)\n","            neg_candidates = np.concatenate((neg_candidates, feature_arr), axis=1)\n","\n","            if neg_items is None:\n","                neg_items = neg_candidates\n","            else:\n","                neg_items = np.concatenate((neg_items, neg_candidates), axis=0)\n","\n","        return neg_items"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"73FySA2dEOgn"},"source":["class DiscriminatorDataset:\n","    def __init__(self, data_reader, stage, batch_size=1000):\n","        self.data_reader = data_reader\n","        self.stage = stage\n","        self.batch_size = batch_size\n","        self.data = self._get_data()\n","\n","    def __len__(self):\n","        return len(self.data)\n","\n","    def __getitem__(self, idx):\n","        return self.data[idx]\n","\n","    def _get_data(self):\n","        if self.stage == 'train':\n","            return self._get_train_data()\n","        else:\n","            return self._get_test_data()\n","\n","    def _get_train_data(self):\n","        data = self.data_reader.train_df.to_numpy()\n","        return data\n","\n","    def _get_test_data(self):\n","        data = self.data_reader.test_df.to_numpy()\n","        return data\n","\n","    @staticmethod\n","    def collate_fn(data):\n","        feed_dict = dict()\n","        feed_dict['X'] = torch.from_numpy(np.asarray(data)[:, 0])\n","        feed_dict['features'] = torch.from_numpy(np.asarray(data)[:, 1:])\n","        return feed_dict"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3xeYrB4zLsZA"},"source":["### Models"]},{"cell_type":"code","metadata":{"id":"EvGqNW4cL6Z7"},"source":["class BaseRecModel(nn.Module):\n","    @staticmethod\n","    def init_weights(m):\n","        \"\"\"\n","        initialize nn weights，called in main.py\n","        :param m: parameter or the nn\n","        :return:\n","        \"\"\"\n","        if type(m) == torch.nn.Linear:\n","            torch.nn.init.normal_(m.weight, mean=0.0, std=0.01)\n","            if m.bias is not None:\n","                torch.nn.init.normal_(m.bias, mean=0.0, std=0.01)\n","        elif type(m) == torch.nn.Embedding:\n","            torch.nn.init.normal_(m.weight, mean=0.0, std=0.01)\n","\n","    def __init__(self, data_processor_dict, user_num, item_num, u_vector_size, i_vector_size,\n","                 random_seed=2020, dropout=0.2, model_path='../model/Model/Model.pt', filter_mode='combine'):\n","        \"\"\"\n","        :param data_processor_dict:\n","        :param user_num:\n","        :param item_num:\n","        :param u_vector_size:\n","        :param i_vector_size:\n","        :param random_seed:\n","        :param dropout:\n","        :param model_path:\n","        :param filter_mode: 'combine'-> for each combination train one filter;\n","        'separate' -> one filter for one sensitive feature, do combination for complex case.\n","        \"\"\"\n","        super(BaseRecModel, self).__init__()\n","        self.data_processor_dict = data_processor_dict\n","        self.user_num = user_num\n","        self.item_num = item_num\n","        self.u_vector_size = u_vector_size\n","        self.i_vector_size = i_vector_size\n","        self.dropout = dropout\n","        self.random_seed = random_seed\n","        self.filter_mode = filter_mode\n","        torch.manual_seed(self.random_seed)\n","        torch.cuda.manual_seed(self.random_seed)\n","        self.model_path = model_path\n","\n","        self._init_nn()\n","        self._init_sensitive_filter()\n","        logger.debug(list(self.parameters()))\n","\n","        self.total_parameters = self.count_variables()\n","        logger.info('# of params: %d' % self.total_parameters)\n","\n","        # optimizer assigned by *_runner.py\n","        self.optimizer = None\n","\n","    def _init_nn(self):\n","        \"\"\"\n","        Initialize neural networks\n","        :return:\n","        \"\"\"\n","        raise NotImplementedError\n","\n","    def _init_sensitive_filter(self):\n","        def get_sensitive_filter(embed_dim):\n","            sequential = nn.Sequential(\n","                nn.Linear(embed_dim, embed_dim * 2),\n","                nn.LeakyReLU(),\n","                nn.Linear(embed_dim * 2, embed_dim),\n","                nn.LeakyReLU(),\n","                nn.BatchNorm1d(embed_dim)\n","            )\n","            return sequential\n","        num_features = len(self.data_processor_dict['train'].data_reader.feature_columns)\n","        self.filter_num = num_features if self.filter_mode == 'combine' else 2**num_features\n","        self.num_features = num_features\n","        self.filter_dict = nn.ModuleDict(\n","            {str(i + 1): get_sensitive_filter(self.u_vector_size) for i in range(self.filter_num)})\n","\n","    def apply_filter(self, vectors, filter_mask):\n","        if self.filter_mode == 'separate' and np.sum(filter_mask) != 0:\n","            filter_mask = np.asarray(filter_mask)\n","            idx = filter_mask.dot(2**np.arange(filter_mask.size))\n","            sens_filter = self.filter_dict[str(idx)]\n","            result = sens_filter(vectors)\n","        elif self.filter_mode == 'combine' and np.sum(filter_mask) != 0:\n","            result = None\n","            for idx, val in enumerate(filter_mask):\n","                if val != 0:\n","                    sens_filter = self.filter_dict[str(idx + 1)]\n","                    result = sens_filter(vectors) if result is None else result + sens_filter(vectors)\n","            result = result / np.sum(filter_mask)   # average the embedding\n","        else:\n","            result = vectors\n","        return result\n","\n","    def count_variables(self):\n","        \"\"\"\n","        Total number of parameters in the model\n","        :return:\n","        \"\"\"\n","        total_parameters = sum(p.numel() for p in self.parameters() if p.requires_grad)\n","        return total_parameters\n","\n","    def l2(self):\n","        \"\"\"\n","        calc the summation of l2 of all parameters\n","        :return:\n","        \"\"\"\n","        l2 = 0\n","        for p in self.parameters():\n","            l2 += (p ** 2).sum()\n","        return l2\n","\n","    def predict(self, feed_dict, filter_mask):\n","        \"\"\"\n","        prediction only without loss calculation\n","        :param feed_dict: input dictionary\n","        :param filter_mask: mask for filter selection\n","        :return: output dictionary，with keys (at least)\n","                \"prediction\": predicted values;\n","                \"check\": intermediate results to be checked and printed out\n","        \"\"\"\n","        check_list = []\n","        x = self.x_bn(feed_dict['X'].float())\n","        x = torch.nn.Dropout(p=feed_dict['dropout'])(x)\n","        prediction = F.relu(self.prediction(x)).view([-1])\n","        out_dict = {'prediction': prediction,\n","                    'check': check_list}\n","        return out_dict\n","\n","    def forward(self, feed_dict, filter_mask):\n","        out_dict = self.predict(feed_dict, filter_mask)\n","        batch_size = int(feed_dict[args.LABEL].shape[0] / 2)\n","        pos, neg = out_dict['prediction'][:batch_size], out_dict['prediction'][batch_size:]\n","        loss = -(pos - neg).sigmoid().log().sum()\n","        out_dict['loss'] = loss\n","        return out_dict\n","\n","    def save_model(self, model_path=None):\n","        if model_path is None:\n","            model_path = self.model_path\n","        dir_path = osp.dirname(model_path)\n","        if not osp.exists(dir_path):\n","            os.mkdir(dir_path)\n","        torch.save(self.state_dict(), model_path)\n","        logger.info('Save model to ' + model_path)\n","\n","    def load_model(self, model_path=None):\n","        if model_path is None:\n","            model_path = self.model_path\n","        self.load_state_dict(torch.load(model_path))\n","        self.eval()\n","        logger.info('Load model from ' + model_path)\n","\n","    def freeze_model(self):\n","        self.eval()\n","        for params in self.parameters():\n","            params.requires_grad = False"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"tryT1aMRM_bh"},"source":["class PMF(BaseRecModel):\n","    def _init_nn(self):\n","        self.uid_embeddings = torch.nn.Embedding(self.user_num, self.u_vector_size)\n","        self.iid_embeddings = torch.nn.Embedding(self.item_num, self.u_vector_size)\n","\n","    def predict(self, feed_dict, filter_mask):\n","        check_list = []\n","        u_ids = feed_dict['X'][:, 0] - 1\n","        i_ids = feed_dict['X'][:, 1] - 1\n","\n","        pmf_u_vectors = self.uid_embeddings(u_ids)\n","        pmf_i_vectors = self.iid_embeddings(i_ids)\n","\n","        pmf_u_vectors = self.apply_filter(pmf_u_vectors, filter_mask)\n","\n","        prediction = (pmf_u_vectors * pmf_i_vectors).sum(dim=1).view([-1])\n","\n","        out_dict = {'prediction': prediction,\n","                    'check': check_list,\n","                    'u_vectors': pmf_u_vectors}\n","        return out_dict"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"JfnXVZLONt6h"},"source":["class RecRunner:\n","    def __init__(self, optimizer='GD', learning_rate=0.01, epoch=100, batch_size=128, eval_batch_size=128 * 128,\n","                 dropout=0.2, l2=1e-5, metrics='RMSE', check_epoch=10, early_stop=1, num_worker=1, no_filter=False,\n","                 reg_weight=0.1, d_steps=100, disc_epoch=1000):\n","        \"\"\"\n","        初始化\n","        :param optimizer: optimizer name\n","        :param learning_rate: learning rate\n","        :param epoch: total training epochs\n","        :param batch_size: batch size for training\n","        :param eval_batch_size: batch size for evaluation\n","        :param dropout: dropout rate\n","        :param l2: l2 weight\n","        :param metrics: evaluation metrics list\n","        :param check_epoch: check intermediate results in every n epochs\n","        :param early_stop: 1 for early stop, 0 for not.\n","        :param no_filter: if or not use filters\n","        :param reg_weight: adversarial penalty weight\n","        :param d_steps: the number of steps to optimize discriminator\n","        :param disc_epoch: number of epoch for training extra discriminator\n","        \"\"\"\n","        self.optimizer_name = optimizer\n","        self.learning_rate = learning_rate\n","        self.epoch = epoch\n","        self.batch_size = batch_size\n","        self.eval_batch_size = eval_batch_size\n","        self.dropout = dropout\n","        self.no_dropout = 0.0\n","        self.l2_weight = l2\n","        self.reg_weight = reg_weight\n","        self.d_steps = d_steps\n","        self.no_filter = no_filter\n","        self.disc_epoch = disc_epoch\n","\n","        # convert metrics to list of str\n","        self.metrics = metrics.lower().split(',')\n","        self.check_epoch = check_epoch\n","        self.early_stop = early_stop\n","        self.time = None\n","\n","        # record train, validation, test results\n","        self.train_results, self.valid_results, self.test_results = [], [], []\n","        self.disc_results = []\n","        self.num_worker = num_worker\n","\n","    def _build_optimizer(self, model, lr=None, l2_weight=None):\n","        optimizer_name = self.optimizer_name.lower()\n","        if lr is None:\n","            lr = self.learning_rate\n","        if l2_weight is None:\n","            l2_weight = self.l2_weight\n","\n","        if optimizer_name == 'gd':\n","            logger.info(\"Optimizer: GD\")\n","            optimizer = torch.optim.SGD(model.parameters(), lr=lr, weight_decay=l2_weight)\n","        elif optimizer_name == 'adagrad':\n","            logger.info(\"Optimizer: Adagrad\")\n","            optimizer = torch.optim.Adagrad(model.parameters(), lr=lr, weight_decay=l2_weight)\n","        elif optimizer_name == 'adam':\n","            logger.info(\"Optimizer: Adam\")\n","            optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=l2_weight)\n","        else:\n","            logging.error(\"Unknown Optimizer: \" + self.optimizer_name)\n","            assert self.optimizer_name in ['GD', 'Adagrad', 'Adam']\n","            optimizer = torch.optim.SGD(model.parameters(), lr=lr, weight_decay=l2_weight)\n","        return optimizer\n","\n","    def _check_time(self, start=False):\n","        if self.time is None or start:\n","            self.time = [time()] * 2\n","            return self.time[0]\n","        tmp_time = self.time[1]\n","        self.time[1] = time()\n","        return self.time[1] - tmp_time\n","\n","    @staticmethod\n","    def get_filter_mask(filter_num):\n","        return np.random.choice([0, 1], size=(filter_num,))\n","\n","    @staticmethod\n","    def _get_masked_disc(disc_dict, labels, mask):\n","        if np.sum(mask) == 0:\n","            return []\n","        masked_disc_label = [(disc_dict[i + 1], labels[:, i]) for i, val in enumerate(mask) if val != 0]\n","        return masked_disc_label\n","\n","    def fit(self, model, batches, fair_disc_dict, epoch=-1):  # fit the results for an input set\n","        \"\"\"\n","        Train the model\n","        :param model: model instance\n","        :param batches: train data in batches\n","        :param fair_disc_dict: fairness discriminator dictionary\n","        :param epoch: epoch number\n","        :return: return the output of the last round\n","        \"\"\"\n","        gc.collect()\n","        torch.cuda.empty_cache()\n","\n","        if model.optimizer is None:\n","            model.optimizer = self._build_optimizer(model)\n","        model.train()\n","\n","        for idx in fair_disc_dict:\n","            discriminator = fair_disc_dict[idx]\n","            if discriminator.optimizer is None:\n","                discriminator.optimizer = self._build_optimizer(discriminator)\n","            discriminator.train()\n","\n","        loss_list = list()\n","        output_dict = dict()\n","        eval_dict = None\n","        for batch in tqdm(batches, leave=False, desc='Epoch %5d' % (epoch + 1),\n","                          ncols=100, mininterval=1):\n","            # step1: use filter mask select filters\n","            # step2: use selected filter filter out the embeddings\n","            # step3: use the filtered embeddings for recommendation task and get rec loss rec_loss\n","            # step4: apply the discriminator with the filtered embeddings and get discriminator loss d_loss\n","            #  (use filter_mask to decide use which discriminator)\n","            # step5: combine rec_loss and d_loss and do optimization (use filter and rec model optimizer)\n","            # step6: use discriminator optimizer to optimize discriminator K times\n","            if self.no_filter:\n","                mask = [0] * model.num_features\n","                mask = np.asarray(mask)\n","            else:\n","                mask = self.get_filter_mask(model.num_features)\n","\n","            batch = batch_to_gpu(batch)\n","            model.optimizer.zero_grad()\n","\n","            labels = batch['features'][:len(batch['features'])//2, :]\n","            if not self.no_filter:\n","                masked_disc_label = \\\n","                    self._get_masked_disc(fair_disc_dict, labels, mask)\n","            else:\n","                masked_disc_label = \\\n","                    self._get_masked_disc(fair_disc_dict, labels, mask + 1)\n","\n","            # calculate recommendation loss + fair discriminator penalty\n","            result_dict = model(batch, mask)\n","            rec_loss = result_dict['loss']\n","            vectors = result_dict['u_vectors']\n","            vectors = vectors[:len(vectors) // 2, :]\n","\n","            fair_d_penalty = 0\n","            if not self.no_filter:\n","                for fair_disc, label in masked_disc_label:\n","                    fair_d_penalty += fair_disc(vectors, label)\n","                fair_d_penalty *= -1\n","                loss = rec_loss + self.reg_weight * fair_d_penalty\n","            else:\n","                loss = rec_loss\n","            loss.backward()\n","            model.optimizer.step()\n","\n","            loss_list.append(result_dict['loss'].detach().cpu().data.numpy())\n","            output_dict['check'] = result_dict['check']\n","\n","            # update discriminator\n","            if not self.no_filter:\n","                if len(masked_disc_label) != 0:\n","                    for _ in range(self.d_steps):\n","                        for discriminator, label in masked_disc_label:\n","                            discriminator.optimizer.zero_grad()\n","                            disc_loss = discriminator(vectors.detach(), label)\n","                            disc_loss.backward(retain_graph=False)\n","                            discriminator.optimizer.step()\n","\n","            # collect discriminator evaluation results\n","            if eval_dict is None:\n","                eval_dict = self._eval_discriminator(model, labels, vectors.detach(), fair_disc_dict, len(mask))\n","            else:\n","                batch_eval_dict = self._eval_discriminator(model, labels, vectors.detach(), fair_disc_dict, len(mask))\n","                for f_name in eval_dict:\n","                    new_label = batch_eval_dict[f_name]['label']\n","                    current_label = eval_dict[f_name]['label']\n","                    eval_dict[f_name]['label'] = torch.cat((current_label, new_label), dim=0)\n","\n","                    new_prediction = batch_eval_dict[f_name]['prediction']\n","                    current_prediction = eval_dict[f_name]['prediction']\n","                    eval_dict[f_name]['prediction'] = torch.cat((current_prediction, new_prediction), dim=0)\n","\n","        # generate discriminator evaluation scores\n","        d_score_dict = {}\n","        if eval_dict is not None:\n","            for f_name in eval_dict:\n","                l = eval_dict[f_name]['label']\n","                pred = eval_dict[f_name]['prediction']\n","                n_class = eval_dict[f_name]['num_class']\n","                d_score_dict[f_name] = self._disc_eval_method(l, pred, n_class)\n","\n","        output_dict['d_score'] = d_score_dict\n","        output_dict['loss'] = np.mean(loss_list)\n","        return output_dict\n","\n","    def train(self, model, dp_dict, fair_disc_dict, skip_eval=0, fix_one=False):\n","        \"\"\"\n","        Train model\n","        :param model: model obj\n","        :param dp_dict: Data processors for train valid and test\n","        :param skip_eval: number of epochs to skip for evaluations\n","        :param fair_disc_dict: fairness discriminator dictionary\n","        :return:\n","        \"\"\"\n","        train_data = DataLoader(dp_dict['train'], batch_size=self.batch_size, num_workers=self.num_worker,\n","                                shuffle=True, collate_fn=dp_dict['train'].collate_fn)\n","        validation_data = DataLoader(dp_dict['valid'], batch_size=None, num_workers=self.num_worker,\n","                                     pin_memory=True, collate_fn=dp_dict['test'].collate_fn)\n","        test_data = DataLoader(dp_dict['test'], batch_size=None, num_workers=self.num_worker,\n","                               pin_memory=True, collate_fn=dp_dict['test'].collate_fn)\n","\n","        self._check_time(start=True)  # start time\n","        try:\n","            for epoch in range(self.epoch):\n","                self._check_time()\n","                output_dict = \\\n","                    self.fit(model, train_data, fair_disc_dict, epoch=epoch)\n","                if self.check_epoch > 0 and (epoch == 1 or epoch % self.check_epoch == 0):\n","                    self.check(model, output_dict)\n","                training_time = self._check_time()\n","\n","                if epoch >= skip_eval:\n","                    valid_result_dict, test_result_dict = None, None\n","                    if self.no_filter:\n","                        valid_result = self.evaluate(model, validation_data) if \\\n","                            validation_data is not None else [-1.0] * len(self.metrics)\n","                        test_result = self.evaluate(model, test_data) \\\n","                            if test_data is not None else [-1.0] * len(self.metrics)\n","                    else:\n","                        valid_result, valid_result_dict = \\\n","                            self.eval_multi_combination(model, validation_data, fix_one) \\\n","                            if validation_data is not None else [-1.0] * len(self.metrics)\n","                        test_result, test_result_dict = self.eval_multi_combination(model, test_data, fix_one) \\\n","                            if test_data is not None else [-1.0] * len(self.metrics)\n","\n","                    testing_time = self._check_time()\n","\n","                    # self.train_results.append(train_result)\n","                    self.valid_results.append(valid_result)\n","                    self.test_results.append(test_result)\n","                    self.disc_results.append(output_dict['d_score'])\n","\n","                    if self.no_filter:\n","                        logger.info(\"Epoch %5d [%.1f s]\\n validation= %s test= %s [%.1f s] \"\n","                                     % (epoch + 1, training_time,\n","                                        format_metric(valid_result), format_metric(test_result),\n","                                        testing_time) + ','.join(self.metrics))\n","                    else:\n","                        logger.info(\"Epoch %5d [%.1f s]\\t Average: validation= %s test= %s [%.1f s] \"\n","                                     % (epoch + 1, training_time,\n","                                        format_metric(valid_result), format_metric(test_result),\n","                                        testing_time) + ','.join(self.metrics))\n","                        for key in valid_result_dict:\n","                            logger.info(\"validation= %s test= %s \"\n","                                         % (format_metric(valid_result_dict[key]),\n","                                            format_metric(test_result_dict[key])) + ','.join(self.metrics) +\n","                                         ' (' + key + ') ')\n","\n","                    if best_result(self.metrics[0], self.valid_results) == self.valid_results[-1]:\n","                        model.save_model()\n","                        for idx in fair_disc_dict:\n","                            fair_disc_dict[idx].save_model()\n","\n","                    if self.eva_termination() and self.early_stop == 1:\n","                        logger.info(\"Early stop at %d based on validation result.\" % (epoch + 1))\n","                        break\n","                if epoch < skip_eval:\n","                    logger.info(\"Epoch %5d [%.1f s]\" % (epoch + 1, training_time))\n","        except KeyboardInterrupt:\n","            logger.info(\"Early stop manually\")\n","            save_here = input(\"Save here? (1/0) (default 0):\")\n","            if str(save_here).lower().startswith('1'):\n","                model.save_model()\n","                for idx in fair_disc_dict:\n","                    fair_disc_dict[idx].save_model()\n","\n","        # Find the best validation result across iterations\n","        best_valid_score = best_result(self.metrics[0], self.valid_results)\n","        best_epoch = self.valid_results.index(best_valid_score)\n","        # prepare disc result string\n","        disc_info = self.disc_results[best_epoch]\n","        disc_info_str = ['{}={:.4f}'.format(key, disc_info[key]) for key in disc_info]\n","        disc_info_str = ','.join(disc_info_str)\n","        logger.info(\"Best Iter(validation)= %5d\\t valid= %s test= %s [%.1f s] \"\n","                     % (best_epoch + 1,\n","                        format_metric(self.valid_results[best_epoch]),\n","                        format_metric(self.test_results[best_epoch]),\n","                        self.time[1] - self.time[0]) + ','.join(self.metrics) + ' ' + disc_info_str +\n","                     ' AUC')\n","        best_test_score = best_result(self.metrics[0], self.test_results)\n","        best_epoch = self.test_results.index(best_test_score)\n","        disc_info = self.disc_results[best_epoch]\n","        disc_info_str = ['{}={:.4f}'.format(key, disc_info[key]) for key in disc_info]\n","        disc_info_str = ','.join(disc_info_str)\n","        logger.info(\"Best Iter(test)= %5d\\t valid= %s test= %s [%.1f s] \"\n","                     % (best_epoch + 1,\n","                        format_metric(self.valid_results[best_epoch]),\n","                        format_metric(self.test_results[best_epoch]),\n","                        self.time[1] - self.time[0]) + ','.join(self.metrics) + ' ' + disc_info_str +\n","                     ' AUC')\n","        model.load_model()\n","        for idx in fair_disc_dict:\n","            fair_disc_dict[idx].load_model()\n","\n","    def eval_multi_combination(self, model, data, fix_one=False):\n","        \"\"\"\n","        Evaluate model on validation/test dataset under different filter combinations.\n","        The output is the averaged result over all the possible combinations.\n","        :param model: trained model\n","        :param data: validation or test data (not train data)\n","        :param fix_one: if true, only evaluate on one feature instead of all the combinations (save running time)\n","        :return: averaged evaluated result on given dataset\n","        \"\"\"\n","        n_features = model.num_features\n","        feature_info = model.data_processor_dict['train'].data_reader.feature_info\n","\n","        if not fix_one:\n","            mask_list = [list(i) for i in it.product([0, 1], repeat=n_features)]\n","            mask_list.pop(0)\n","            # mask_list = [[1, 0, 0], [0, 1, 0], [0, 0, 1]]\n","        else:\n","            feature_range = np.arange(n_features)\n","            shape = (feature_range.size, feature_range.max() + 1)\n","            one_hot = np.zeros(shape).astype(int)\n","            one_hot[feature_range, feature_range] = 1\n","            mask_list = one_hot.tolist()\n","            mask_list = [mask_list[1]]\n","        result_dict = {}\n","        acc_result = None\n","        for mask in mask_list:\n","            mask = np.asarray(mask)\n","            feature_idx = np.where(mask == 1)[0]\n","            f_name_list = [feature_info[i + 1].name for i in feature_idx]\n","            f_name = ' '.join(f_name_list)\n","\n","            cur_result = self.evaluate(model, data, mask) if data is not None else [-1.0] * len(self.metrics)\n","            acc_result = np.array(cur_result) if acc_result is None else acc_result + np.asarray(cur_result)\n","\n","            result_dict[f_name] = cur_result\n","\n","        if acc_result is not None:\n","            acc_result /= len(mask_list)\n","\n","        return list(acc_result), result_dict\n","\n","    @torch.no_grad()\n","    def evaluate(self, model, batches, mask=None, metrics=None):\n","        \"\"\"\n","        evaluate recommendation performance\n","        :param model:\n","        :param batches: data batches, each batch is a dict.\n","        :param mask: filter mask\n","        :param metrics: list of str\n","        :return: list of float number for each metric\n","        \"\"\"\n","        if metrics is None:\n","            metrics = self.metrics\n","        model.eval()\n","\n","        if mask is None:\n","            mask = [0] * model.filter_num\n","            mask = np.asarray(mask)\n","\n","        result_dict = defaultdict(list)\n","        for batch in tqdm(batches, leave=False, ncols=100, mininterval=1, desc='Predict'):\n","            batch = batch_to_gpu(batch)\n","            out_dict = model.predict(batch, mask)\n","            prediction = out_dict['prediction']\n","            labels = batch[args.LABEL].cpu()\n","            sample_ids = batch['X'][:, 2].cpu()\n","            assert len(labels) == len(prediction)\n","            assert len(sample_ids == len(prediction))\n","            prediction = prediction.cpu().numpy()\n","            data_dict = {args.LABEL: labels, args.SAMPLE_ID: sample_ids}\n","            results = self.evaluate_method(prediction, data_dict, metrics=metrics)\n","            for key in results:\n","                result_dict[key].extend(results[key])\n","\n","        evaluations = []\n","        for metric in metrics:\n","            evaluations.append(np.average(result_dict[metric]))\n","\n","        return evaluations\n","\n","    @staticmethod\n","    def evaluate_method(p, data, metrics):\n","        \"\"\"\n","        Evaluate model predictions.\n","        :param p: predicted values, np.array\n","        :param data: data dictionary which include ground truth labels\n","        :param metrics: metrics list\n","        :return: a list of results. The order is consistent to metric list.\n","        \"\"\"\n","        label = data[args.LABEL]\n","        evaluations = {}\n","        for metric in metrics:\n","            if metric == 'rmse':\n","                evaluations[metric] = [np.sqrt(mean_squared_error(label, p))]\n","            elif metric == 'mae':\n","                evaluations[metric] = [mean_absolute_error(label, p)]\n","            elif metric == 'auc':\n","                evaluations[metric] = [roc_auc_score(label, p)]\n","            else:\n","                k = int(metric.split('@')[-1])\n","                df = pd.DataFrame()\n","                df[args.SAMPLE_ID] = data[args.SAMPLE_ID]\n","                df['p'] = p\n","                df['l'] = label\n","                df = df.sort_values(by='p', ascending=False)\n","                df_group = df.groupby(args.SAMPLE_ID)\n","                if metric.startswith('ndcg@'):\n","                    ndcgs = []\n","                    for uid, group in df_group:\n","                        ndcgs.append(ndcg_at_k(group['l'].tolist()[:k], k=k, method=1))\n","                    evaluations[metric] = ndcgs\n","                elif metric.startswith('hit@'):\n","                    hits = []\n","                    for uid, group in df_group:\n","                        hits.append(int(np.sum(group['l'][:k]) > 0))\n","                    evaluations[metric] = hits\n","                elif metric.startswith('precision@'):\n","                    precisions = []\n","                    for uid, group in df_group:\n","                        precisions.append(precision_at_k(group['l'].tolist()[:k], k=k))\n","                    evaluations[metric] = precisions\n","                elif metric.startswith('recall@'):\n","                    recalls = []\n","                    for uid, group in df_group:\n","                        recalls.append(1.0 * np.sum(group['l'][:k]) / np.sum(group['l']))\n","                    evaluations[metric] = recalls\n","                elif metric.startswith('f1@'):\n","                    f1 = []\n","                    for uid, group in df_group:\n","                        num_overlap = 1.0 * np.sum(group['l'][:k])\n","                        f1.append(2 * num_overlap / (k + 1.0 * np.sum(group['l'])))\n","                    evaluations[metric] = f1\n","        return evaluations\n","\n","    def eva_termination(self):\n","        \"\"\"\n","        Early stopper\n","        :return:\n","        \"\"\"\n","        metric = self.metrics[0]\n","        valid = self.valid_results\n","        if len(valid) > 20 and metric in LOWER_METRIC_LIST and strictly_increasing(valid[-5:]):\n","            return True\n","        elif len(valid) > 20 and metric not in LOWER_METRIC_LIST and strictly_decreasing(valid[-5:]):\n","            return True\n","        elif len(valid) - valid.index(best_result(metric, valid)) > 20:\n","            return True\n","        return False\n","\n","    @torch.no_grad()\n","    def _eval_discriminator(self, model, labels, u_vectors, fair_disc_dict, num_disc):\n","        feature_info = model.data_processor_dict['train'].data_reader.feature_info\n","        feature_eval_dict = {}\n","        for i in range(num_disc):\n","            discriminator = fair_disc_dict[i + 1]\n","            label = labels[:, i]\n","            # metric = 'auc' if feature_info[i + 1].num_class == 2 else 'f1'\n","            feature_name = feature_info[i + 1].name\n","            discriminator.eval()\n","            if feature_info[i + 1].num_class == 2:\n","                prediction = discriminator.predict(u_vectors)['prediction'].squeeze()\n","            else:\n","                prediction = discriminator.predict(u_vectors)['output']\n","            feature_eval_dict[feature_name] = {'label': label.cpu(), 'prediction': prediction.detach().cpu(),\n","                                               'num_class': feature_info[i + 1].num_class}\n","            discriminator.train()\n","        return feature_eval_dict\n","\n","    @staticmethod\n","    def _disc_eval_method(label, prediction, num_class, metric='auc'):\n","        if metric == 'auc':\n","            if num_class == 2:\n","                score = roc_auc_score(label, prediction, average='micro')\n","                # score = roc_auc_score(label, prediction)\n","                score = max(score, 1 - score)\n","                return score\n","            else:\n","                lb = LabelBinarizer()\n","                classes = [i for i in range(num_class)]\n","                lb.fit(classes)\n","                label = lb.transform(label)\n","                # label = lb.fit_transform(label)\n","                score = roc_auc_score(label, prediction, multi_class='ovo', average='macro')\n","                score = max(score, 1 - score)\n","                return score\n","        else:\n","            raise ValueError('Unknown evaluation metric in _disc_eval_method().')\n","\n","    def check(self, model, out_dict):\n","        \"\"\"\n","        Check intermediate results\n","        :param model: model obj\n","        :param out_dict: output dictionary\n","        :return:\n","        \"\"\"\n","        check = out_dict\n","        logger.info(os.linesep)\n","        for i, t in enumerate(check['check']):\n","            d = np.array(t[1].detach().cpu())\n","            logger.info(os.linesep.join([t[0] + '\\t' + str(d.shape), np.array2string(d, threshold=20)]) + os.linesep)\n","\n","        loss, l2 = check['loss'], model.l2()\n","        l2 = l2 * self.l2_weight\n","        l2 = l2.detach()\n","        logger.info('loss = %.4f, l2 = %.4f' % (loss, l2))\n","        if not (np.absolute(loss) * 0.005 < l2 < np.absolute(loss) * 0.1):\n","            logging.warning('l2 inappropriate: loss = %.4f, l2 = %.4f' % (loss, l2))\n","\n","        # for discriminator\n","        disc_score_dict = out_dict['d_score']\n","        for feature in disc_score_dict:\n","            logger.info('{} AUC = {:.4f}'.format(feature, disc_score_dict[feature]))\n","\n","    def train_discriminator(self, model, dp_dict, fair_disc_dict, lr_attack=None, l2_attack=None):\n","        \"\"\"\n","        Train discriminator to evaluate the quality of learned embeddings\n","        :param model: trained model\n","        :param dp_dict: Data processors for train valid and test\n","        :param fair_disc_dict: fairness discriminator dictionary\n","        :return:\n","        \"\"\"\n","        train_data = DataLoader(dp_dict['train'], batch_size=dp_dict['train'].batch_size, num_workers=self.num_worker,\n","                                shuffle=True, collate_fn=dp_dict['train'].collate_fn)\n","        test_data = DataLoader(dp_dict['test'], batch_size=dp_dict['test'].batch_size, num_workers=self.num_worker,\n","                               pin_memory=True, collate_fn=dp_dict['test'].collate_fn)\n","        self._check_time(start=True)  # 记录初始时间s\n","\n","        feature_results = defaultdict(list)\n","        best_results = dict()\n","        try:\n","            for epoch in range(self.disc_epoch):\n","                self._check_time()\n","                output_dict = \\\n","                    self.fit_disc(model, train_data, fair_disc_dict, epoch=epoch,\n","                                  lr_attack=lr_attack, l2_attack=l2_attack)\n","\n","                if self.check_epoch > 0 and (epoch == 1 or epoch % (self.disc_epoch // 4) == 0):\n","                    self.check_disc(output_dict)\n","                training_time = self._check_time()\n","\n","                test_result_dict = \\\n","                    self.evaluation_disc(model, fair_disc_dict, test_data, dp_dict['train'])\n","                d_score_dict = test_result_dict['d_score']\n","                # testing_time = self._check_time()\n","                if epoch % (self.disc_epoch // 4) == 0:\n","                    logger.info(\"Epoch %5d [%.1f s]\" % (epoch + 1, training_time))\n","                for f_name in d_score_dict:\n","                    if epoch % (self.disc_epoch // 4) == 0:\n","                        logger.info(\"{} AUC= {:.4f}\".format(f_name, d_score_dict[f_name]))\n","                    feature_results[f_name].append(d_score_dict[f_name])\n","                    if d_score_dict[f_name] == max(feature_results[f_name]):\n","                        best_results[f_name] = d_score_dict[f_name]\n","                        idx = dp_dict['train'].data_reader.f_name_2_idx[f_name]\n","                        fair_disc_dict[idx].save_model()\n","\n","        except KeyboardInterrupt:\n","            logger.info(\"Early stop manually\")\n","            save_here = input(\"Save here? (1/0) (default 0):\")\n","            if str(save_here).lower().startswith('1'):\n","                for idx in fair_disc_dict:\n","                    fair_disc_dict[idx].save_model()\n","\n","        for f_name in best_results:\n","            logger.info(\"{} best AUC: {:.4f}\".format(f_name, best_results[f_name]))\n","\n","        for idx in fair_disc_dict:\n","            fair_disc_dict[idx].load_model()\n","\n","    def fit_disc(self, model, batches, fair_disc_dict, epoch=-1, lr_attack=None, l2_attack=None):\n","        \"\"\"\n","        Train the discriminator\n","        :param model: model instance\n","        :param batches: train data in batches\n","        :param fair_disc_dict: fairness discriminator dictionary\n","        :param epoch: epoch number\n","        :param lr_attack: attacker learning rate\n","        :param l2_attack: l2 regularization weight for attacker\n","        :return: return the output of the last round\n","        \"\"\"\n","        gc.collect()\n","        torch.cuda.empty_cache()\n","\n","        for idx in fair_disc_dict:\n","            discriminator = fair_disc_dict[idx]\n","            if discriminator.optimizer is None:\n","                discriminator.optimizer = self._build_optimizer(discriminator, lr=lr_attack, l2_weight=l2_attack)\n","            discriminator.train()\n","\n","        output_dict = dict()\n","        loss_acc = defaultdict(list)\n","\n","        eval_dict = None\n","        for batch in tqdm(batches, leave=False, desc='Epoch %5d' % (epoch + 1),\n","                          ncols=100, mininterval=1):\n","            if self.no_filter:\n","                mask = [0] * model.num_features\n","                mask = np.asarray(mask)\n","            else:\n","                mask = self.get_filter_mask(model.num_features)\n","\n","            batch = batch_to_gpu(batch)\n","\n","            labels = batch['features']\n","            if not self.no_filter:\n","                masked_disc_label = \\\n","                    self._get_masked_disc(fair_disc_dict, labels, mask)\n","            else:\n","                masked_disc_label = \\\n","                    self._get_masked_disc(fair_disc_dict, labels, mask + 1)\n","\n","            # calculate recommendation loss + fair discriminator penalty\n","            uids = batch['X'] - 1\n","            vectors = model.apply_filter(model.uid_embeddings(uids), mask)\n","            output_dict['check'] = []\n","\n","            # update discriminator\n","            if len(masked_disc_label) != 0:\n","                for idx, (discriminator, label) in enumerate(masked_disc_label):\n","                    discriminator.optimizer.zero_grad()\n","                    disc_loss = discriminator(vectors.detach(), label)\n","                    disc_loss.backward()\n","                    discriminator.optimizer.step()\n","                    loss_acc[discriminator.name].append(disc_loss.detach().cpu())\n","\n","        for key in loss_acc:\n","            loss_acc[key] = np.mean(loss_acc[key])\n","\n","        output_dict['loss'] = loss_acc\n","        return output_dict\n","\n","    @torch.no_grad()\n","    def evaluation_disc(self, model, fair_disc_dict, test_data, dp):\n","        num_features = dp.data_reader.num_features\n","\n","        def eval_disc(labels, u_vectors, fair_disc_dict, mask):\n","            feature_info = dp.data_reader.feature_info\n","            feature_eval_dict = {}\n","            for i, val in enumerate(mask):\n","                if val == 0:\n","                    continue\n","                discriminator = fair_disc_dict[i + 1]\n","                label = labels[:, i]\n","                # metric = 'auc' if feature_info[i + 1].num_class == 2 else 'f1'\n","                feature_name = feature_info[i + 1].name\n","                discriminator.eval()\n","                if feature_info[i + 1].num_class == 2:\n","                    prediction = discriminator.predict(u_vectors)['prediction'].squeeze()\n","                else:\n","                    prediction = discriminator.predict(u_vectors)['output']\n","                feature_eval_dict[feature_name] = {'label': label.cpu(), 'prediction': prediction.detach().cpu(),\n","                                                   'num_class': feature_info[i + 1].num_class}\n","                discriminator.train()\n","            return feature_eval_dict\n","\n","        eval_dict = {}\n","        for batch in test_data:\n","            # VERSION 1\n","            # if self.no_filter:\n","            #     # mask = [0] * model.num_features\n","            #     feature_range = np.arange(num_features)\n","            #     shape = (feature_range.size, feature_range.max() + 1)\n","            #     one_hot = np.zeros(shape).astype(int)\n","            #     one_hot[feature_range, feature_range] = 1\n","            #     mask_list = one_hot.tolist()\n","            #     # if num_features == 3:\n","            #     #     mask_list = [[1, 0, 0], [0, 1, 0], [0, 0, 1]]\n","            #     # elif num_features == 4:\n","            #     #     mask_list = [[1, 0, 0, 0], [0, 1, 0, 0], [0, 0, 1, 0], [0, 0, 0, 1]]\n","            # else:\n","            #     mask_list = [list(i) for i in it.product([0, 1], repeat=num_features)]\n","            #     mask_list.pop(0)\n","\n","            # VERSION 2\n","            mask_list = [list(i) for i in it.product([0, 1], repeat=num_features)]\n","            mask_list.pop(0)\n","\n","            batch = batch_to_gpu(batch)\n","\n","            labels = batch['features']\n","            uids = batch['X'] - 1\n","\n","            for mask in mask_list:\n","                if self.no_filter:\n","                    vectors = model.uid_embeddings(uids)\n","                else:\n","                    vectors = model.apply_filter(model.uid_embeddings(uids), mask)\n","                batch_eval_dict = eval_disc(labels, vectors.detach(), fair_disc_dict, mask)\n","\n","                for f_name in batch_eval_dict:\n","                    if f_name not in eval_dict:\n","                        eval_dict[f_name] = batch_eval_dict[f_name]\n","                    else:\n","                        new_label = batch_eval_dict[f_name]['label']\n","                        current_label = eval_dict[f_name]['label']\n","                        eval_dict[f_name]['label'] = torch.cat((current_label, new_label), dim=0)\n","\n","                        new_prediction = batch_eval_dict[f_name]['prediction']\n","                        current_prediction = eval_dict[f_name]['prediction']\n","                        eval_dict[f_name]['prediction'] = torch.cat((current_prediction, new_prediction), dim=0)\n","\n","        # generate discriminator evaluation scores\n","        d_score_dict = {}\n","        if eval_dict is not None:\n","            for f_name in eval_dict:\n","                l = eval_dict[f_name]['label']\n","                pred = eval_dict[f_name]['prediction']\n","                n_class = eval_dict[f_name]['num_class']\n","                d_score_dict[f_name] = self._disc_eval_method(l, pred, n_class)\n","\n","        output_dict = dict()\n","        output_dict['d_score'] = d_score_dict\n","        return output_dict\n","\n","    @staticmethod\n","    def check_disc(out_dict):\n","        check = out_dict\n","        logger.info(os.linesep)\n","        for i, t in enumerate(check['check']):\n","            d = np.array(t[1].detach().cpu())\n","            logger.info(os.linesep.join([t[0] + '\\t' + str(d.shape), np.array2string(d, threshold=20)]) + os.linesep)\n","\n","        loss_dict = check['loss']\n","        for disc_name, disc_loss in loss_dict.items():\n","            logger.info('%s loss = %.4f' % (disc_name, disc_loss))\n","\n","        # for discriminator\n","        if 'd_score' in out_dict:\n","            disc_score_dict = out_dict['d_score']\n","            for feature in disc_score_dict:\n","                logger.info('{} AUC = {:.4f}'.format(feature, disc_score_dict[feature]))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-mBpD_r2A37k"},"source":["### Utils"]},{"cell_type":"code","metadata":{"id":"c5BXgA7TAa8s"},"source":["def balance_data(data):\n","    pos_indexes = np.where(data['Y'] == 1)[0]\n","    copy_num = int((len(data['Y']) - len(pos_indexes)) / len(pos_indexes))\n","    if copy_num > 1:\n","        copy_indexes = np.tile(pos_indexes, copy_num)\n","        sample_index = np.concatenate([np.arange(0, len(data['Y'])), copy_indexes])\n","        for k in data:\n","            data[k] = data[k][sample_index]\n","    return data\n","\n","\n","def input_data_is_list(data):\n","    if type(data) is list or type(data) is tuple:\n","        print(\"input_data_is_list\")\n","        new_data = {}\n","        for k in data[0]:\n","            new_data[k] = np.concatenate([d[k] for d in data])\n","        return new_data\n","    return data\n","\n","\n","def format_metric(metric):\n","    # print(metric, type(metric))\n","    if type(metric) is not tuple and type(metric) is not list:\n","        metric = [metric]\n","    format_str = []\n","    if type(metric) is tuple or type(metric) is list:\n","        for m in metric:\n","            # print(type(m))\n","            if type(m) is float or type(m) is np.float or type(m) is np.float32 or type(m) is np.float64:\n","                format_str.append('%.4f' % m)\n","            elif type(m) is int or type(m) is np.int or type(m) is np.int32 or type(m) is np.int64:\n","                format_str.append('%d' % m)\n","    return ','.join(format_str)\n","\n","\n","def shuffle_in_unison_scary(data):\n","    \"\"\"\n","    shuffle entire dataset\n","    :param data:\n","    :return:\n","    \"\"\"\n","    rng_state = np.random.get_state()\n","    for d in data:\n","        np.random.set_state(rng_state)\n","        np.random.shuffle(data[d])\n","    return data\n","\n","\n","def best_result(metric, results_list):\n","    if type(metric) is list or type(metric) is tuple:\n","        metric = metric[0]\n","    if metric in LOWER_METRIC_LIST:\n","        return min(results_list)\n","    return max(results_list)\n","\n","\n","def strictly_increasing(l):\n","    return all(x < y for x, y in zip(l, l[1:]))\n","\n","\n","def strictly_decreasing(l):\n","    return all(x > y for x, y in zip(l, l[1:]))\n","\n","\n","def non_increasing(l):\n","    return all(x >= y for x, y in zip(l, l[1:]))\n","\n","\n","def non_decreasing(l):\n","    return all(x <= y for x, y in zip(l, l[1:]))\n","\n","\n","def monotonic(l):\n","    return non_increasing(l) or non_decreasing(l)\n","\n","\n","def numpy_to_torch(d):\n","    t = torch.from_numpy(d)\n","    if torch.cuda.device_count() > 0:\n","        t = t.cuda()\n","    return t\n","\n","\n","def str2bool(v):\n","    if isinstance(v, bool):\n","        return v\n","    if v.lower() in ('yes', 'true', 't', 'y', '1'):\n","        return True\n","    elif v.lower() in ('no', 'false', 'f', 'n', '0'):\n","        return False\n","    else:\n","        raise argparse.ArgumentTypeError('Boolean value expected.')\n","\n","\n","def batch_to_gpu(batch):\n","    if torch.cuda.device_count() > 0:\n","        for c in batch:\n","            if type(batch[c]) is torch.Tensor:\n","                batch[c] = batch[c].cuda()\n","    return batch"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jKJUMRk6Af0x"},"source":["### Metrics"]},{"cell_type":"code","metadata":{"id":"yiC5cZjOAhD3"},"source":["def mean_reciprocal_rank(rs):\n","    \"\"\"Score is reciprocal of the rank of the first relevant item\n","    First element is 'rank 1'.  Relevance is binary (nonzero is relevant).\n","    Example from http://en.wikipedia.org/wiki/Mean_reciprocal_rank\n","    >>> rs = [[0, 0, 1], [0, 1, 0], [1, 0, 0]]\n","    >>> mean_reciprocal_rank(rs)\n","    0.61111111111111105\n","    >>> rs = np.array([[0, 0, 0], [0, 1, 0], [1, 0, 0]])\n","    >>> mean_reciprocal_rank(rs)\n","    0.5\n","    >>> rs = [[0, 0, 0, 1], [1, 0, 0], [1, 0, 0]]\n","    >>> mean_reciprocal_rank(rs)\n","    0.75\n","    Args:\n","        rs: Iterator of relevance scores (list or numpy) in rank order\n","            (first element is the first item)\n","    Returns:\n","        Mean reciprocal rank\n","    \"\"\"\n","    rs = (np.asarray(r).nonzero()[0] for r in rs)\n","    return np.mean([1. / (r[0] + 1) if r.size else 0. for r in rs])\n","\n","\n","def reciprocal_rank(rs):\n","    \"\"\"Score is reciprocal of the rank of the first relevant item\n","    Args:\n","        rs: Iterator of relevance scores (list or numpy) in rank order\n","            (first element is the first item)\n","    Returns:\n","        reciprocal rank\n","    \"\"\"\n","    rs = np.asarray(rs).nonzero()[0]\n","    return 1. / (rs[0] + 1) if rs.size else 0.\n","\n","\n","def r_precision(r):\n","    \"\"\"Score is precision after all relevant documents have been retrieved\n","    Relevance is binary (nonzero is relevant).\n","    >>> r = [0, 0, 1]\n","    >>> r_precision(r)\n","    0.33333333333333331\n","    >>> r = [0, 1, 0]\n","    >>> r_precision(r)\n","    0.5\n","    >>> r = [1, 0, 0]\n","    >>> r_precision(r)\n","    1.0\n","    Args:\n","        r: Relevance scores (list or numpy) in rank order\n","            (first element is the first item)\n","    Returns:\n","        R Precision\n","    \"\"\"\n","    r = np.asarray(r) != 0\n","    z = r.nonzero()[0]\n","    if not z.size:\n","        return 0.\n","    return np.mean(r[:z[-1] + 1])\n","\n","\n","def precision_at_k(r, k):\n","    \"\"\"Score is precision @ k\n","    Relevance is binary (nonzero is relevant).\n","    >>> r = [0, 0, 1]\n","    >>> precision_at_k(r, 1)\n","    0.0\n","    >>> precision_at_k(r, 2)\n","    0.0\n","    >>> precision_at_k(r, 3)\n","    0.33333333333333331\n","    >>> precision_at_k(r, 4)\n","    Traceback (most recent call last):\n","        File \"<stdin>\", line 1, in ?\n","    ValueError: Relevance score length < k\n","    Args:\n","        r: Relevance scores (list or numpy) in rank order\n","            (first element is the first item)\n","    Returns:\n","        Precision @ k\n","    Raises:\n","        ValueError: len(r) must be >= k\n","    \"\"\"\n","    assert k >= 1\n","    r = np.asarray(r)[:k] != 0\n","    if r.size != k:\n","        raise ValueError('Relevance score length < k')\n","    return np.mean(r)\n","\n","\n","def average_precision(r):\n","    \"\"\"Score is average precision (area under PR curve)\n","    Relevance is binary (nonzero is relevant).\n","    >>> r = [1, 1, 0, 1, 0, 1, 0, 0, 0, 1]\n","    >>> delta_r = 1. / sum(r)\n","    >>> sum([sum(r[:x + 1]) / (x + 1.) * delta_r for x, y in enumerate(r) if y])\n","    0.7833333333333333\n","    >>> average_precision(r)\n","    0.78333333333333333\n","    Args:\n","        r: Relevance scores (list or numpy) in rank order\n","            (first element is the first item)\n","    Returns:\n","        Average precision\n","    \"\"\"\n","    r = np.asarray(r) != 0\n","    out = [precision_at_k(r, k + 1) for k in range(r.size) if r[k]]\n","    if not out:\n","        return 0.\n","    return np.mean(out)\n","\n","\n","def mean_average_precision(rs):\n","    \"\"\"Score is mean average precision\n","    Relevance is binary (nonzero is relevant).\n","    >>> rs = [[1, 1, 0, 1, 0, 1, 0, 0, 0, 1]]\n","    >>> mean_average_precision(rs)\n","    0.78333333333333333\n","    >>> rs = [[1, 1, 0, 1, 0, 1, 0, 0, 0, 1], [0]]\n","    >>> mean_average_precision(rs)\n","    0.39166666666666666\n","    Args:\n","        rs: Iterator of relevance scores (list or numpy) in rank order\n","            (first element is the first item)\n","    Returns:\n","        Mean average precision\n","    \"\"\"\n","    return np.mean([average_precision(r) for r in rs])\n","\n","\n","def dcg_at_k(r, k, method=0):\n","    \"\"\"Score is discounted cumulative gain (dcg)\n","    Relevance is positive real values.  Can use binary\n","    as the previous methods.\n","    Example from\n","    http://www.stanford.edu/class/cs276/handouts/EvaluationNew-handout-6-per.pdf\n","    >>> r = [3, 2, 3, 0, 0, 1, 2, 2, 3, 0]\n","    >>> dcg_at_k(r, 1)\n","    3.0\n","    >>> dcg_at_k(r, 1, method=1)\n","    3.0\n","    >>> dcg_at_k(r, 2)\n","    5.0\n","    >>> dcg_at_k(r, 2, method=1)\n","    4.2618595071429155\n","    >>> dcg_at_k(r, 10)\n","    9.6051177391888114\n","    >>> dcg_at_k(r, 11)\n","    9.6051177391888114\n","    Args:\n","        r: Relevance scores (list or numpy) in rank order\n","            (first element is the first item)\n","        k: Number of results to consider\n","        method: If 0 then weights are [1.0, 1.0, 0.6309, 0.5, 0.4307, ...]\n","                If 1 then weights are [1.0, 0.6309, 0.5, 0.4307, ...]\n","    Returns:\n","        Discounted cumulative gain\n","    \"\"\"\n","    r = np.asfarray(r)[:k]\n","    if r.size:\n","        if method == 0:\n","            return r[0] + np.sum(r[1:] / np.log2(np.arange(2, r.size + 1)))\n","        elif method == 1:\n","            return np.sum(r / np.log2(np.arange(2, r.size + 2)))\n","        else:\n","            raise ValueError('method must be 0 or 1.')\n","    return 0.\n","\n","\n","def ndcg_at_k(r, k, method=0):\n","    \"\"\"Score is normalized discounted cumulative gain (ndcg)\n","    Relevance is positive real values.  Can use binary\n","    as the previous methods.\n","    Example from\n","    http://www.stanford.edu/class/cs276/handouts/EvaluationNew-handout-6-per.pdf\n","    >>> r = [3, 2, 3, 0, 0, 1, 2, 2, 3, 0]\n","    >>> ndcg_at_k(r, 1)\n","    1.0\n","    >>> r = [2, 1, 2, 0]\n","    >>> ndcg_at_k(r, 4)\n","    0.9203032077642922\n","    >>> ndcg_at_k(r, 4, method=1)\n","    0.96519546960144276\n","    >>> ndcg_at_k([0], 1)\n","    0.0\n","    >>> ndcg_at_k([1], 2)\n","    1.0\n","    Args:\n","        r: Relevance scores (list or numpy) in rank order\n","            (first element is the first item)\n","        k: Number of results to consider\n","        method: If 0 then weights are [1.0, 1.0, 0.6309, 0.5, 0.4307, ...]\n","                If 1 then weights are [1.0, 0.6309, 0.5, 0.4307, ...]\n","    Returns:\n","        Normalized discounted cumulative gain\n","    \"\"\"\n","    dcg_max = dcg_at_k(sorted(r, reverse=True), k, method)\n","    if not dcg_max:\n","        return 0.\n","    return dcg_at_k(r, k, method) / dcg_max"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"YqvAt2nl4EOL"},"source":["## Jobs"]},{"cell_type":"code","metadata":{"id":"nb4As1933lK_"},"source":["# # No filters\n","# python ./main.py --model_name BiasedMF --optimizer Adam --dataset ml1M --data_processor RecDataset --metric ndcg@5,ndcg@10,hit@5,hit@10 --l2 1e-4 --batch_size 1024 --model_path \"../model/biasedMF_ml1m_no_filter_neg_sample=100/biasedMF_ml1m_l2=1e-4_dim=64_no_filter_neg_sample=100.pt\" --runner RecRunner --d_step 10 --vt_num_neg 100 --vt_batch_size 1024 --no_filter --eval_dict\n","# python ./main.py --model_name PMF --optimizer Adam --dataset ml1M --data_processor RecDataset --metric ndcg@5,ndcg@10,hit@5,hit@10 --l2 1e-4 --batch_size 1024 --model_path \"../model/PMF_ml1m_no_filter_neg_sample=100/PMF_ml1m_l2=1e-4_dim=64_no_filter_neg_sample=100.pt\" --runner RecRunner --d_step 10 --vt_num_neg 100 --vt_batch_size 1024 --no_filter --eval_disc\n","# python ./main.py --model_name DMF --optimizer Adam --dataset ml1M --data_processor RecDataset --metric ndcg@5,ndcg@10,hit@5,hit@10 --l2 1e-4 --batch_size 1024 --model_path \"../model/DMF_ml1m_no_filter_neg_sample=100/DMF_ml1m_l2=1e-4_dim=64_no_filter_neg_sample=100.pt\" --runner RecRunner --d_step 10 --vt_num_neg 100 --vt_batch_size 1024 --no_filter --eval_disc\n","# python ./main.py --model_name MLP --optimizer Adam --dataset ml1M --data_processor RecDataset --metric ndcg@5,ndcg@10,hit@5,hit@10 --l2 1e-4 --batch_size 1024 --model_path \"../model/MLP_ml1m_no_filter_neg_sample=100/MLP_ml1m_l2=1e-4_dim=64_no_filter_neg_sample=100.pt\" --runner RecRunner --d_step 10 --vt_num_neg 100 --vt_batch_size 1024 --no_filter --eval_disc\n","\n","# # Sample command for separate method\n","# python ./main.py --model_name BiasedMF --optimizer Adam --dataset ml1M --data_processor RecDataset --metric ndcg@5,ndcg@10,hit@5,hit@10 --l2 1e-4 --batch_size 1024 --model_path \"../model/biasedMF_ml1m_neg_sample=100_reg_weight=20_separate/biasedMF_ml1m_l2=1e-4_dim=64_reg_weight=20_neg_sample=100_separate.pt\" --runner RecRunner --d_step 10 --reg_weight 20 --epoch 200 --vt_num_neg 100 --vt_batch_size 1024 --filter_mode separate --fix_one --eval_disc\n","# python ./main.py --model_name PMF --optimizer Adam --dataset ml1M --data_processor RecDataset --metric ndcg@5,ndcg@10,hit@5,hit@10 --l2 1e-4 --batch_size 1024 --model_path \"../model/PMF_ml1m_neg_sample=100_reg_weight=20_separate/PMF_ml1m_l2=1e-4_dim=64_reg_weight=20_neg_sample=100_separate.pt\" --runner RecRunner --d_step 10 --reg_weight 20 --epoch 200 --vt_num_neg 100 --vt_batch_size 1024 --filter_mode separate --fix_one --eval_disc\n","# python ./main.py --model_name DMF --optimizer Adam --dataset ml1M --data_processor RecDataset --metric ndcg@5,ndcg@10,hit@5,hit@10 --l2 1e-4 --batch_size 1024 --model_path \"../model/DMF_ml1m_neg_sample=100_reg_weight=20_separate/DMF_ml1m_l2=1e-4_dim=64_reg_weight=20_neg_sample=100_separate.pt\" --runner RecRunner --d_step 10 --reg_weight 20 --epoch 200 --vt_num_neg 100 --vt_batch_size 1024 --filter_mode separate --fix_one --eval_disc\n","# python ./main.py --model_name MLP --optimizer Adam --dataset ml1M --data_processor RecDataset --metric ndcg@5,ndcg@10,hit@5,hit@10 --l2 1e-4 --u_vector_size 32 --batch_size 1024 --model_path \"../model/MLP_ml1m_neg_sample=100_reg_weight=20_separate/MLP_ml1m_l2=1e-4_dim=32_reg_weight=20_neg_sample=100_separate.pt\" --runner RecRunner --d_step 10 --reg_weight 20 --vt_num_neg 100 --vt_batch_size 1024 --fix_one --filter_mode separate --eval_disc\n","\n","# # Sample command for combination method\n","# python ./main.py --model_name BiasedMF --optimizer Adam --dataset ml1M --data_processor RecDataset --metric ndcg@5,ndcg@10,hit@5,hit@10 --l2 1e-4 --batch_size 1024 --model_path \"../model/biasedMF_ml1m_reg_weight=20_neg_sample=100_combine/biasedMF_ml1m_l2=1e-4_dim=64_reg_weight=20_neg_sample=100_combine.pt\" --runner RecRunner --d_step 10 --reg_weight 20 --vt_num_neg 100 --vt_batch_size 1024 --fix_one --eval_disc\n","# python ./main.py --model_name PMF --optimizer Adam --dataset ml1M --data_processor RecDataset --metric ndcg@5,ndcg@10,hit@5,hit@10 --l2 1e-4 --batch_size 1024 --model_path \"../model/PMF_ml1m_reg_weight=20_neg_sample=100_combine/PMF_ml1m_l2=1e-4_dim=64_reg_weight=20_neg_sample=100_combine.pt\" --runner RecRunner --d_step 10 --reg_weight 20 --vt_num_neg 100 --vt_batch_size 1024 --fix_one --eval_disc\n","# python ./main.py --model_name DMF --optimizer Adam --dataset ml1M --data_processor RecDataset --metric ndcg@5,ndcg@10,hit@5,hit@10 --l2 1e-4 --batch_size 1024 --model_path \"../model/DMF_ml1m_reg_weight=20_neg_sample=100_combine/DMF_ml1m_l2=1e-4_dim=64_reg_weight=20_neg_sample=100_combine.pt\" --runner RecRunner --d_step 10 --reg_weight 20 --vt_num_neg 100 --vt_batch_size 1024 --fix_one --eval_disc\n","# python ./main.py --model_name MLP --optimizer Adam --dataset ml1M --data_processor RecDataset --metric ndcg@5,ndcg@10,hit@5,hit@10 --l2 1e-4 --u_vector_size 32 --batch_size 1024 --model_path \"../model/MLP_ml1m_l2=1e-4_dim=32_reg_weight=20_neg_sample=100/MLP_ml1m_l2=1e-4_dim=32_reg_weight=20_neg_sample=100.pt\" --runner RecRunner --d_step 10 --vt_num_neg 100 --vt_batch_size 1024 --reg_weight 20 --fix_one --eval_disc"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MmQfVXVf4EME","executionInfo":{"status":"ok","timestamp":1636457007116,"user_tz":-330,"elapsed":33,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}},"outputId":"300d7bd7-e1e9-4975-94cc-fb9a3d3f4b32"},"source":["download_movielens()"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["09-Nov-21 11:23:26 [INFO] : Files already exists in /content/ml1M, skipping!\n"]}]},{"cell_type":"code","metadata":{"id":"d8Pks02yWPt8"},"source":["args.model_name = 'PMF'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aOPPaVnacCDa","executionInfo":{"status":"ok","timestamp":1636457009002,"user_tz":-330,"elapsed":9,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}},"outputId":"7576be4c-da64-477e-9f0f-37fa01e0d53f"},"source":["# choose data_reader\n","data_reader_name = eval(args.data_reader)\n","\n","# choose model\n","model_name = eval(args.model_name)\n","runner_name = eval(args.runner)\n","\n","# choose data_processor\n","data_processor_name = eval(args.data_processor)\n","\n","# logging\n","logger.info('DataReader: ' + args.data_reader)\n","logger.info('Model: ' + args.model_name)\n","logger.info('Runner: ' + args.runner)\n","logger.info('DataProcessor: ' + args.data_processor)\n","\n","# random seed\n","torch.manual_seed(args.random_seed)\n","torch.cuda.manual_seed_all(args.random_seed)\n","np.random.seed(args.random_seed)\n","\n","# cuda\n","os.environ[\"CUDA_VISIBLE_DEVICES\"] = args.gpu\n","logger.info(\"# cuda devices: %d\" % torch.cuda.device_count())"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["09-Nov-21 11:23:28 [INFO] : DataReader: RecDataReader\n","09-Nov-21 11:23:28 [INFO] : Model: PMF\n","09-Nov-21 11:23:28 [INFO] : Runner: RecRunner\n","09-Nov-21 11:23:28 [INFO] : DataProcessor: RecDataset\n","09-Nov-21 11:23:29 [INFO] : # cuda devices: 0\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0WsqdYjkcEQP","executionInfo":{"status":"ok","timestamp":1636457020721,"user_tz":-330,"elapsed":5949,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}},"outputId":"395e7487-a552-422a-cd83-624b80e450e2"},"source":["# create data_reader\n","data_reader = data_reader_name(path=args.path, dataset_name=args.dataset, sep=args.sep)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["09-Nov-21 11:23:34 [INFO] : load all csv...\n","09-Nov-21 11:23:35 [INFO] : load train csv...\n","09-Nov-21 11:23:35 [INFO] : size of train: 800169\n","09-Nov-21 11:23:35 [INFO] : load validation csv...\n","09-Nov-21 11:23:35 [INFO] : size of validation: 100020\n","09-Nov-21 11:23:35 [INFO] : load test csv...\n","09-Nov-21 11:23:36 [INFO] : size of test: 100020\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":214,"referenced_widgets":["b5c1118b4bfb4f9fa0695e91163a1e47","47532d7cbe0a4c5fb1f1b3575d836ea9","32db06de410a4481a5ca0b75b42f7ec2","f0c36db39c5746e79a5643c8dcb21abf","2d8aeab54e2d4b0ab739c96445f91394","a39ae408211c46bd95e4a333f2eabbff","79f7f84ff4b640c7bc7c3cc40f161798","62be46a350e846a5995ccad4fc4a5954","b720c3aaf1644da789fdad2d70251972","4a16f523a0ca4d83b0dd9ce12ff32e4e","e02966e60c78464496a57d61417d7aa1"]},"id":"KZWHNAQWcThk","outputId":"45d8f03e-f982-4cff-e242-bcb7ad499d21"},"source":["# create data processor\n","data_processor_dict = {}\n","for stage in ['train', 'valid', 'test']:\n","    if stage == 'train':\n","        if args.data_processor in ['RecDataset']:\n","            data_processor_dict[stage] = data_processor_name(\n","                data_reader, stage, batch_size=args.batch_size, num_neg=args.train_num_neg)\n","        else:\n","            raise ValueError('Unknown DataProcessor')\n","    else:\n","        if args.data_processor in ['RecDataset']:\n","            data_processor_dict[stage] = data_processor_name(\n","                data_reader, stage, batch_size=args.vt_batch_size, num_neg=args.vt_num_neg)\n","        else:\n","            raise ValueError('Unknown DataProcessor')\n","    gc.collect()"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["09-Nov-21 11:25:21 [INFO] : Prepare validation data...\n"]},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"b5c1118b4bfb4f9fa0695e91163a1e47","version_minor":0,"version_major":2},"text/plain":["Prepare Batches:   0%|                                                      | 0/196 [00:00<?, ?it/s]"]},"metadata":{}}]},{"cell_type":"code","metadata":{"id":"4DT-IGUEiEMB"},"source":["# create model\n","if args.model_name in ['BiasedMF', 'PMF']:\n","    model = model_name(data_processor_dict, user_num=len(data_reader.user_ids_set),\n","                        item_num=len(data_reader.item_ids_set), u_vector_size=args.u_vector_size,\n","                        i_vector_size=args.i_vector_size, random_seed=args.random_seed, dropout=args.dropout,\n","                        model_path=args.model_path, filter_mode=args.filter_mode)\n","elif args.model_name in ['DMF', 'MLP']:\n","    model = model_name(data_processor_dict, user_num=len(data_reader.user_ids_set),\n","                        item_num=len(data_reader.item_ids_set), u_vector_size=args.u_vector_size,\n","                        i_vector_size=args.i_vector_size, num_layers=args.num_layers,\n","                        random_seed=args.random_seed, dropout=args.dropout,\n","                        model_path=args.model_path, filter_mode=args.filter_mode)\n","else:\n","    logger.error('Unknown Model: ' + args.model_name)\n","\n","# init model params\n","model.apply(model.init_weights)\n","\n","# use gpu\n","if torch.cuda.device_count() > 0:\n","    model = model.cuda()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"GCxtyvnmibfE"},"source":["# create discriminators\n","fair_disc_dict = {}\n","for feat_idx in data_reader.feature_info:\n","    fair_disc_dict[feat_idx] = \\\n","        Discriminator(args.u_vector_size, data_reader.feature_info[feat_idx],\n","                        random_seed=args.random_seed, dropout=args.dropout, neg_slope=args.neg_slope,\n","                        model_dir_path=os.path.dirname(args.model_path))\n","    fair_disc_dict[feat_idx].apply(fair_disc_dict[feat_idx].init_weights)\n","    if torch.cuda.device_count() > 0:\n","        fair_disc_dict[feat_idx] = fair_disc_dict[feat_idx].cuda()\n","\n","if args.runner in ['BaseRunner']:\n","    runner = runner_name(\n","        optimizer=args.optimizer, learning_rate=args.lr,\n","        epoch=args.epoch, batch_size=args.batch_size, eval_batch_size=args.vt_batch_size,\n","        dropout=args.dropout, l2=args.l2,\n","        metrics=args.metric, check_epoch=args.check_epoch, early_stop=args.early_stop)\n","elif args.runner in ['RecRunner']:\n","    runner = runner_name(\n","        optimizer=args.optimizer, learning_rate=args.lr,\n","        epoch=args.epoch, batch_size=args.batch_size, eval_batch_size=args.vt_batch_size,\n","        dropout=args.dropout, l2=args.l2,\n","        metrics=args.metric, check_epoch=args.check_epoch, early_stop=args.early_stop, num_worker=args.num_worker,\n","        no_filter=args.no_filter, reg_weight=args.reg_weight, d_steps=args.d_steps, disc_epoch=args.disc_epoch)\n","else:\n","    logger.error('Unknown Runner: ' + args.runner)\n","\n","if args.load > 0:\n","    model.load_model()\n","    for idx in fair_disc_dict:\n","        fair_disc_dict[idx].load_model()\n","if args.train > 0:\n","    runner.train(model, data_processor_dict, fair_disc_dict, skip_eval=args.skip_eval, fix_one=args.fix_one)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ptjwSmJUih-h"},"source":["# reset seed\n","torch.manual_seed(args.random_seed)\n","torch.cuda.manual_seed_all(args.random_seed)\n","np.random.seed(args.random_seed)\n","\n","if args.eval_disc:\n","    # Train extra discriminator for evaluation\n","    # create data reader\n","    disc_data_reader = DiscriminatorDataReader(path=args.path, dataset_name=args.dataset, sep=args.sep)\n","\n","    # create data processor\n","    extra_data_processor_dict = {}\n","    for stage in ['train', 'test']:\n","        extra_data_processor_dict[stage] = DiscriminatorDataset(disc_data_reader, stage, args.disc_batch_size)\n","\n","    # create discriminators\n","    extra_fair_disc_dict = {}\n","    for feat_idx in disc_data_reader.feature_info:\n","        if disc_data_reader.feature_info[feat_idx].num_class == 2:\n","            extra_fair_disc_dict[feat_idx] = \\\n","                BinaryAttacker(args.u_vector_size, disc_data_reader.feature_info[feat_idx],\n","                                random_seed=args.random_seed, dropout=args.dropout,\n","                                neg_slope=args.neg_slope, model_dir_path=os.path.dirname(args.model_path),\n","                                model_name='eval')\n","        else:\n","            extra_fair_disc_dict[feat_idx] = \\\n","                MultiClassAttacker(args.u_vector_size, disc_data_reader.feature_info[feat_idx],\n","                                    random_seed=args.random_seed, dropout=args.dropout, neg_slope=args.neg_slope,\n","                                    model_dir_path=os.path.dirname(args.model_path), model_name='eval')\n","        extra_fair_disc_dict[feat_idx].apply(extra_fair_disc_dict[feat_idx].init_weights)\n","        if torch.cuda.device_count() > 0:\n","            extra_fair_disc_dict[feat_idx] = extra_fair_disc_dict[feat_idx].cuda()\n","\n","    if args.load_attack:\n","        for idx in extra_fair_disc_dict:\n","            logger.info('load attacker model...')\n","            extra_fair_disc_dict[idx].load_model()\n","    model.load_model()\n","    model.freeze_model()\n","    runner.train_discriminator(model, extra_data_processor_dict, extra_fair_disc_dict, args.lr_attack,\n","                                args.l2_attack)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"IjNJ3ZmqikOC"},"source":["test_data = DataLoader(data_processor_dict['test'], batch_size=None, num_workers=args.num_worker,\n","                        pin_memory=True, collate_fn=data_processor_dict['test'].collate_fn)\n","\n","test_result_dict = dict()\n","if args.no_filter:\n","    test_result = runner.evaluate(model, test_data)\n","else:\n","    test_result, test_result_dict = runner.eval_multi_combination(model, test_data, args.fix_one)\n","\n","if args.no_filter:\n","    logger.info(\"Test After Training = %s \"\n","                    % (format_metric(test_result)) + ','.join(runner.metrics))\n","else:\n","    logger.info(\"Test After Training:\\t Average: %s \"\n","                    % (format_metric(test_result)) + ','.join(runner.metrics))\n","    for key in test_result_dict:\n","        logger.info(\"test= %s \"\n","                        % (format_metric(test_result_dict[key])) + ','.join(runner.metrics) +\n","                        ' (' + key + ') ')"],"execution_count":null,"outputs":[]}]}