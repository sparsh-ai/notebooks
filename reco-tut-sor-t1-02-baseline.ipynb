{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "project_name = \"reco-tut-sor\"; branch = \"main\"; account = \"sparsh-ai\"\n",
    "project_path = os.path.join('/content', project_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(project_path):\n",
    "    !cp /content/drive/MyDrive/mykeys.py /content\n",
    "    import mykeys\n",
    "    !rm /content/mykeys.py\n",
    "    path = \"/content/\" + project_name; \n",
    "    !mkdir \"{path}\"\n",
    "    %cd \"{path}\"\n",
    "    import sys; sys.path.append(path)\n",
    "    !git config --global user.email \"recotut@recohut.com\"\n",
    "    !git config --global user.name  \"reco-tut\"\n",
    "    !git init\n",
    "    !git remote add origin https://\"{mykeys.git_token}\":x-oauth-basic@github.com/\"{account}\"/\"{project_name}\".git\n",
    "    !git pull origin \"{branch}\"\n",
    "    !git checkout main\n",
    "else:\n",
    "    %cd \"{project_path}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git add . && git commit -m 'commit' && git push origin \"{branch}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "from sklearn.metrics import confusion_matrix,  accuracy_score\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./data/silver/userdata.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe().round(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(18,12))\n",
    "df.hist(ax=ax)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## User Snapshot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.id == 'e12aeaf2d47d42479ea1c4ac3d8286c6']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "user completed an offer 0b1e... and viewed ae26.... Offer 2906.. had been ignored twice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_categorical(df, columns):\n",
    "    for col in columns:\n",
    "        df[col] = pd.Categorical(df[col])\n",
    "        df[col] = df[col].cat.codes\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set embedding sizes\n",
    "N = len(df['id'].unique())\n",
    "M = len(df['offer_id'].unique())\n",
    "\n",
    "# Set embedding dimension\n",
    "D = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a neural network\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, n_users, n_items, embed_dim, output_dim, layers=[1024], p=0.4):\n",
    "        super(Model, self).__init__()\n",
    "        self.N = n_users\n",
    "        self.M = n_items\n",
    "        self.D = embed_dim\n",
    "\n",
    "        self.u_emb = nn.Embedding(self.N, self.D)\n",
    "        self.m_emb = nn.Embedding(self.M, self.D)\n",
    "        \n",
    "        layerlist = []\n",
    "        n_in = 2 * self.D\n",
    "        for i in layers:\n",
    "            layerlist.append(nn.Linear(n_in,i)) \n",
    "            layerlist.append(nn.ReLU())\n",
    "            layerlist.append(nn.BatchNorm1d(i))\n",
    "            layerlist.append(nn.Dropout(p))\n",
    "            n_in = i\n",
    "        layerlist.append(nn.Linear(layers[-1],output_dim))\n",
    "        self.layers = nn.Sequential(*layerlist)\n",
    "        \n",
    "        nn.init.xavier_uniform_(self.layers[0].weight)\n",
    "        nn.init.zeros_(self.layers[0].bias)\n",
    "        nn.init.xavier_uniform_(self.layers[-1].weight)\n",
    "        nn.init.zeros_(self.layers[-1].bias)\n",
    "\n",
    "    def forward(self, u, m):\n",
    "        u = self.u_emb(u) # output is (num_samples, D)\n",
    "        m = self.m_emb(m) # output is (num_samples, D)\n",
    "\n",
    "        # merge\n",
    "        out = torch.cat((u, m), 1) # output is (num_samples, 2D)\n",
    "\n",
    "        x = self.layers(out)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "model = Model(N, M, D, output_dim=df['event'].nunique(), layers=[512, 256])\n",
    "model.to(device)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "# optimizer = torch.optim.SGD(model.parameters(), lr=0.08, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = to_categorical(df, ['id','offer_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to tensors\n",
    "user_ids_t = torch.from_numpy(df['id'].values).long()\n",
    "offer_ids_t = torch.from_numpy(df['offer_id'].values).long()\n",
    "ratings_t = torch.from_numpy(df['event'].values).long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make datasets\n",
    "N_train = int(0.8 * len(df['event'].values))\n",
    "N_test = 1000\n",
    "train_dataset = torch.utils.data.TensorDataset(\n",
    "    user_ids_t[:N_train],\n",
    "    offer_ids_t[:N_train],\n",
    "    ratings_t[:N_train],\n",
    ")\n",
    "\n",
    "val_dataset = torch.utils.data.TensorDataset(\n",
    "    user_ids_t[N_train:-N_test],\n",
    "    offer_ids_t[N_train:-N_test],\n",
    "    ratings_t[N_train:-N_test],\n",
    ")\n",
    "test_df = df[-N_test:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data loaders\n",
    "batch_size = 32\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
    "                                           batch_size=batch_size, \n",
    "                                           shuffle=True)\n",
    "\n",
    "validation_loader = torch.utils.data.DataLoader(dataset=val_dataset, \n",
    "                                          batch_size=batch_size, \n",
    "                                          shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A function to encapsulate the training loop\n",
    "def batch_gd(model, criterion, optimizer, train_iter, test_iter, epochs):\n",
    "    \n",
    "    train_losses = np.zeros(epochs)\n",
    "    val_losses = np.zeros(epochs)\n",
    "    acc_list = []\n",
    "    \n",
    "    for it in range(epochs):\n",
    "        t0 = datetime.now()\n",
    "        train_loss = []\n",
    "        for users, offer, targets in train_loader:\n",
    "            \n",
    "\n",
    "            # move data to GPU\n",
    "            users, offer, targets = users.to(device), offer.to(device), targets.to(device)\n",
    "            #targets = targets.view(-1, 1).long()\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(users, offer)\n",
    "            \n",
    "            loss = criterion(outputs, targets.squeeze())\n",
    "\n",
    "            # Backward and optimize\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss.append(loss.item())\n",
    "            \n",
    "            # Track the accuracy\n",
    "            total = targets.size(0)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            correct = (predicted == targets).sum().item()\n",
    "            acc = correct / total\n",
    "            acc_list.append(acc)\n",
    "\n",
    "        # Get train loss and test loss\n",
    "        train_loss = np.mean(train_loss) # a little misleading\n",
    "        \n",
    "        val_loss = []\n",
    "        \n",
    "        # validation\n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "            \n",
    "            for users, offer, targets in validation_loader:\n",
    "                users, offer, targets = users.to(device), offer.to(device), targets.to(device)\n",
    "                #targets = targets.view(-1, 1).long()\n",
    "                outputs = model(users, offer)\n",
    "                loss = criterion(outputs, targets.squeeze())\n",
    "                val_loss.append(loss.item())\n",
    "        \n",
    "        val_loss = np.mean(val_loss)\n",
    "        # Save losses\n",
    "        train_losses[it] = train_loss\n",
    "        val_losses[it] = val_loss\n",
    "\n",
    "        dt = datetime.now() - t0\n",
    "        print(f'Epoch {it+1}/{epochs}, Validation Loss: {train_loss:.4f}, '\n",
    "              f'Test Loss: {val_loss:.4f}, Accuracy: {acc}, Duration: {dt}')\n",
    "\n",
    "    return train_losses, val_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses, val_losses = batch_gd(model, criterion, optimizer, train_loader, validation_loader, 25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the train loss and test loss per iteration\n",
    "plt.plot(train_losses, label='train loss')\n",
    "plt.plot(val_losses, label='test loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_ix = 10\n",
    "end_ix = 20\n",
    "test_X =  torch.from_numpy(test_df.iloc[start_ix:end_ix]['id'].values).long()\n",
    "test_y = torch.from_numpy(test_df.iloc[start_ix:end_ix]['event'].values).long()\n",
    "\n",
    "with torch.no_grad():\n",
    "    model.to('cpu')\n",
    "    model.eval()\n",
    "    pred = model(test_X, test_y)\n",
    "    print(pred)\n",
    "\n",
    "_, predicted = torch.max(pred.data, 1)\n",
    "print(predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "               horizontalalignment=\"center\",\n",
    "               color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot confusion matrix and baseline accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(test_y, predicted)\n",
    "classes = [0,1,2]\n",
    "plot_confusion_matrix(cm, classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Accuracy so far: \" + str(100*accuracy_score(test_y, predicted))+ \"%\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results are decent so far and almost twice better than random quessing.\n",
    "\n",
    "#### Show some misclassified examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = test_df.iloc[start_ix:end_ix][['age', 'became_member_on', 'gender', 'id', 'income', 'memberdays', 'event']]#['offer_id'].values\n",
    "pred_values = pd.DataFrame(predicted, columns=['predicted'], index=data.index)\n",
    "pd.concat([data, pred_values], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's save the model for future reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model, model_name, model_info):\n",
    "    # Save the parameters used to construct the model\n",
    "    with open(model_name, 'wb') as f:\n",
    "        torch.save(model_info, f)\n",
    "\n",
    "    # Save the model parameters\n",
    "    \n",
    "    with open(model_name, 'wb') as f:\n",
    "        torch.save(model.cpu().state_dict(), f)\n",
    "\n",
    "model_info = {\n",
    "         'n_users': M, \n",
    "          'n_items': N, \n",
    "          'embed_dim': D, \n",
    "          'output_dim': df['event'].nunique(), \n",
    "          'layers': [512, 256], \n",
    "          'p': 0.4\n",
    "    }\n",
    "save_model(model, './artifacts/models/BaselineModel.pth', model_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During the next step we improve the model to take additional paramenters that describe each user and each offer, which should hopefully, give the model insigths on why a particular customer may like or not like given offer."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyNOHpruVWwXaYCqhdxX+ox+",
   "collapsed_sections": [],
   "mount_file_id": "1Wx4reEOMnbZfD3tI9LeoARfGqj3vdLJK",
   "name": "reco-tut-sor-t1-02-baseline.ipynb",
   "provenance": [
    {
     "file_id": "1UuUa25pOjIh93SeHhRVnsmZgeohAt_qD",
     "timestamp": 1628779111526
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
