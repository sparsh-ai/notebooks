{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"reco-session-yoochoose-00.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyNisZ5TpeniKofFW6uwC8h1"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"id":"P6EIvBGnS7PW"},"source":["# Managing the category type\n","# The categories can be S (for promotion), 0 (when unknown), \n","# a number between 1-12 when it came from a category on the page\n","# or a 8-10 digit number that represents a brand\n","\n","# def assign_cat(x):\n","#   if x == \"S\":\n","#       return \"PROMOTION\"\n","#   elif np.int(x) == 0:\n","#       return \"NONE\"\n","#   elif np.int(x) < 13:\n","#       return \"CATEGORY\"\n","#   else:\n","#       return \"BRAND\"\n","\n","# df_clicks[\"Item_Type\"] = df_clicks.iloc[:,3].map(assign_cat)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9gdI5SlLS9mZ"},"source":["# fraction = 64\n","\n","# PATH_TO_PROCESSED_DATA = '../../data/'\n","\n","# data = pd.read_csv(PATH_TO_PROCESSED_DATA + 'rsc15_train_tr.txt', sep='\\t', dtype={'ItemId':np.int64})\n","# train = data\n","# length = len(data['ItemId'])\n","\n","# print('Full Training Set:\\n\\tEvents: {}\\n\\tSessions: {}\\n\\tItems: {}'.format(len(data), data.SessionId.nunique(), data.ItemId.nunique()))\n","\n","# print(\"\\nGetting most recent 1/{} fraction of training test...\\n\".format(fraction))\n","# first_session = train.iloc[length-length//fraction].SessionId\n","# train = train.loc[train['SessionId'] >= first_session]\n","\n","# itemids = train['ItemId'].unique()\n","# n_items = len(itemids)\n","\n","# print('Fractioned train set\\n\\tEvents: {}\\n\\tSessions: {}\\n\\tItems: {}'.format(len(train), train.SessionId.nunique(), train.ItemId.nunique()))\n","# train.to_csv(PATH_TO_PROCESSED_DATA + 'rsc15_train_fraction_1_{}.txt'.format(fraction), sep='\\t', index=False)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6O2xnAE_S_4r"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"cellView":"form","id":"nECbnLVMpovT"},"source":["#@title extractDwellTime.py\n","\n","from matplotlib import pyplot as plt\n","import argparse\n","import numpy as np\n","import pandas as pd\n","\n","\n","def preprocess_df(df):    \n","    n_items = len(train_data['ItemId'].unique())\n","    aux = list(train_data['ItemId'].unique())\n","    itemids = np.array(aux)\n","    itemidmap = pd.Series(data=np.arange(n_items), index=itemids)  # (id_item => (0, n_items))\n","    \n","    item_key = 'ItemId'\n","    session_key = 'SessionId'\n","    time_key = 'Time'\n","    \n","    data = pd.merge(df, pd.DataFrame({item_key:itemids, 'ItemIdx':itemidmap[itemids].values}), on=item_key, how='inner')\n","    data.sort_values([session_key, time_key], inplace=True)\n","\n","    length = len(data['ItemId'])\n","        \n","    return data\n","\n","\n","def compute_dwell_time(df):\n","    times_t = np.roll(df['Time'], -1)  # Take time row\n","    times_dt  = df['Time']             # Copy, then displace by one\n","    \n","    diffs = np.subtract(times_t, times_dt)  # Take the pairwise difference\n","    \n","    length = len(df['ItemId'])\n","    \n","    # cummulative offset start for each session\n","    offset_sessions = np.zeros(df['SessionId'].nunique()+1, dtype=np.int32)\n","    offset_sessions[1:] = df.groupby('SessionId').size().cumsum() \n","    \n","    offset_sessions = offset_sessions - 1\n","    offset_sessions = np.roll(offset_sessions, -1)\n","    \n","    # session transition implies zero-dwell-time\n","    # note: paper statistics do not consider null entries, \n","    # though they are still checked when augmenting\n","    np.put(diffs.values, offset_sessions, np.zeros((offset_sessions.shape)), mode='raise')\n","    return diffs\n","\n","\n","def get_statistics(dts):\n","    filtered = np.array(list(filter(lambda x: int(x) != 0, dts)))\n","    pd_dts = pd.DataFrame(filtered)\n","    pd_dts.boxplot(vert=False, showfliers=False) # no outliers in boxplot\n","    plt.show()\n","    pd_dts.describe()\n","\n","\n","def join_dwell_reps(df, dt, threshold=2000):\n","    # Calculate d_ti/threshold + 1, add column to dataFrame\n","    dt //= threshold\n","    dt += 1   \n","    df['DwellReps'] = pd.Series(dt.astype(np.int64), index=dt.index)\n","\n","\n","def augment(df):    \n","    col_names = list(df.columns.values)[:3]\n","    print(col_names)\n","    augmented = np.repeat(df.values, df['DwellReps'], axis=0) \n","    print(augmented[0][:3])  \n","    augmented = pd.DataFrame(data=augmented[:,:3],\n","                             columns=col_names)\n","    dtype = {'SessionId': np.int64, \n","             'ItemId': np.int64, \n","             'Time': np.float32}\n","    \n","    for k, v in dtype.items():\n","        augmented[k] = augmented[k].astype(v)\n","\n","    return augmented\n","\n","\n","if __name__ == '__main__':\n","    parser = argparse.ArgumentParser(description='DwellTime extractor')\n","    parser.add_argument('--train-path', type=str, default='../processedData/rsc15_train_tr.txt')\n","    parser.add_argument('--output-path', type=str, default='../processedData/augmented_train.csv')\n","    args = parser.parse_args()\n","\n","    # load RSC15 preprocessed train dataframe\n","    train_data = pd.read_csv(args.train_path, sep='\\t', dtype={'ItemId':np.int64})\n","\n","    new_df = preprocess_df(train_data)\n","    dts = compute_dwell_time(new_df)\n","\n","    # get_statistics(dts)\n","\n","    join_dwell_reps(new_df, dts, threshold=200000)\n","\n","    # Now, we augment the sessions copying each entry an additional (dwellReps[i]-1) times\n","    df_aug = augment(new_df)\n","    df_aug.to_csv(args.output_path, index=False, sep='\\t')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"S2ElKay9pp6u"},"source":["## Feature Engineering"]},{"cell_type":"markdown","metadata":{"id":"L6EgBZy6psH3"},"source":["| Name                               | Type    |\n","| ---------------------------------- | ------- |\n","| No. of clicks                      | Session |\n","| No. of unique items                | Session |\n","| Avg. no. of clicks per unique item | Session |\n","| Session duration in seconds        | Session |\n","| Average time between two clicks    | Session |\n","| Maximal time between two clicks    | Session |\n","| Day of the week                    | Session |\n","| Month of the year                  | Session |\n","| Time during the day                | Session |\n","| Total clicks on the item           | Item    |\n","| Total buys on the item             | Item    |\n","| Max price of the item              | Item    |\n","| Min price of the item              | Item    |\n","| Item id                            | Item    |\n","| Category id                        | Item    |"]},{"cell_type":"code","metadata":{"id":"5vBjkxSdTcI7"},"source":["#@title GRU4Rec.py\n","import torch\n","from torch import nn\n","from torch.nn.init import xavier_uniform_, xavier_normal_\n","\n","from recbole.model.abstract_recommender import SequentialRecommender\n","from recbole.model.loss import BPRLoss\n","\n","\n","class GRU4Rec(SequentialRecommender):\n","    r\"\"\"GRU4Rec is a model that incorporate RNN for recommendation.\n","    Note:\n","        Regarding the innovation of this article,we can only achieve the data augmentation mentioned\n","        in the paper and directly output the embedding of the item,\n","        in order that the generation method we used is common to other sequential models.\n","    \"\"\"\n","\n","    def __init__(self, config, dataset):\n","        super(GRU4Rec, self).__init__(config, dataset)\n","\n","        # load parameters info\n","        self.embedding_size = config['embedding_size']\n","        self.hidden_size = config['hidden_size']\n","        self.loss_type = config['loss_type']\n","        self.num_layers = config['num_layers']\n","        self.dropout_prob = config['dropout_prob']\n","\n","        # define layers and loss\n","        self.item_embedding = nn.Embedding(self.n_items, self.embedding_size, padding_idx=0)\n","        self.emb_dropout = nn.Dropout(self.dropout_prob)\n","        self.gru_layers = nn.GRU(\n","            input_size=self.embedding_size,\n","            hidden_size=self.hidden_size,\n","            num_layers=self.num_layers,\n","            bias=False,\n","            batch_first=True,\n","        )\n","        self.dense = nn.Linear(self.hidden_size, self.embedding_size)\n","        if self.loss_type == 'BPR':\n","            self.loss_fct = BPRLoss()\n","        elif self.loss_type == 'CE':\n","            self.loss_fct = nn.CrossEntropyLoss()\n","        else:\n","            raise NotImplementedError(\"Make sure 'loss_type' in ['BPR', 'CE']!\")\n","\n","        # parameters initialization\n","        self.apply(self._init_weights)\n","\n","    def _init_weights(self, module):\n","        if isinstance(module, nn.Embedding):\n","            xavier_normal_(module.weight)\n","        elif isinstance(module, nn.GRU):\n","            xavier_uniform_(module.weight_hh_l0)\n","            xavier_uniform_(module.weight_ih_l0)\n","\n","    def forward(self, item_seq, item_seq_len):\n","        item_seq_emb = self.item_embedding(item_seq)\n","        item_seq_emb_dropout = self.emb_dropout(item_seq_emb)\n","        gru_output, _ = self.gru_layers(item_seq_emb_dropout)\n","        gru_output = self.dense(gru_output)\n","        # the embedding of the predicted item, shape of (batch_size, embedding_size)\n","        seq_output = self.gather_indexes(gru_output, item_seq_len - 1)\n","        return seq_output\n","\n","    def calculate_loss(self, interaction):\n","        item_seq = interaction[self.ITEM_SEQ]\n","        item_seq_len = interaction[self.ITEM_SEQ_LEN]\n","        seq_output = self.forward(item_seq, item_seq_len)\n","        pos_items = interaction[self.POS_ITEM_ID]\n","        if self.loss_type == 'BPR':\n","            neg_items = interaction[self.NEG_ITEM_ID]\n","            pos_items_emb = self.item_embedding(pos_items)\n","            neg_items_emb = self.item_embedding(neg_items)\n","            pos_score = torch.sum(seq_output * pos_items_emb, dim=-1)  # [B]\n","            neg_score = torch.sum(seq_output * neg_items_emb, dim=-1)  # [B]\n","            loss = self.loss_fct(pos_score, neg_score)\n","            return loss\n","        else:  # self.loss_type = 'CE'\n","            test_item_emb = self.item_embedding.weight\n","            logits = torch.matmul(seq_output, test_item_emb.transpose(0, 1))\n","            loss = self.loss_fct(logits, pos_items)\n","            return loss\n","\n","    def predict(self, interaction):\n","        item_seq = interaction[self.ITEM_SEQ]\n","        item_seq_len = interaction[self.ITEM_SEQ_LEN]\n","        test_item = interaction[self.ITEM_ID]\n","        seq_output = self.forward(item_seq, item_seq_len)\n","        test_item_emb = self.item_embedding(test_item)\n","        scores = torch.mul(seq_output, test_item_emb).sum(dim=1)  # [B]\n","        return scores\n","\n","    def full_sort_predict(self, interaction):\n","        item_seq = interaction[self.ITEM_SEQ]\n","        item_seq_len = interaction[self.ITEM_SEQ_LEN]\n","        seq_output = self.forward(item_seq, item_seq_len)\n","        test_items_emb = self.item_embedding.weight\n","        scores = torch.matmul(seq_output, test_items_emb.transpose(0, 1))  # [B, n_items]\n","        return scores"],"execution_count":null,"outputs":[]}]}