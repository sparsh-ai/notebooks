{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"WikidataRecs.ipynb","provenance":[],"authorship_tag":"ABX9TyNIG4h9z56f4fIJlPyoKTzM"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sP1UYiJA28vr","executionInfo":{"status":"ok","timestamp":1626515193914,"user_tz":-330,"elapsed":3329,"user":{"displayName":"Sparsh Agarwal","photoUrl":"","userId":"13037694610922482904"}},"outputId":"900376e0-207b-433e-cc91-ecda337b1ef9"},"source":["!wget https://github.com/WikidataRec-developer/Wikidata_Recommender/raw/main/Wikidata_Active_Editor.csv"],"execution_count":1,"outputs":[{"output_type":"stream","text":["--2021-07-17 09:46:54--  https://github.com/WikidataRec-developer/Wikidata_Recommender/raw/main/Wikidata_Active_Editor.csv\n","Resolving github.com (github.com)... 140.82.114.4\n","Connecting to github.com (github.com)|140.82.114.4|:443... connected.\n","HTTP request sent, awaiting response... 302 Found\n","Location: https://media.githubusercontent.com/media/WikidataRec-developer/Wikidata_Recommender/main/Wikidata_Active_Editor.csv [following]\n","--2021-07-17 09:46:55--  https://media.githubusercontent.com/media/WikidataRec-developer/Wikidata_Recommender/main/Wikidata_Active_Editor.csv\n","Resolving media.githubusercontent.com (media.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n","Connecting to media.githubusercontent.com (media.githubusercontent.com)|185.199.108.133|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 59165661 (56M) [text/plain]\n","Saving to: ‘Wikidata_Active_Editor.csv’\n","\n","Wikidata_Active_Edi 100%[===================>]  56.42M  89.9MB/s    in 0.6s    \n","\n","2021-07-17 09:46:56 (89.9 MB/s) - ‘Wikidata_Active_Editor.csv’ saved [59165661/59165661]\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4yreVynj2-hz","executionInfo":{"status":"ok","timestamp":1626515198784,"user_tz":-330,"elapsed":1382,"user":{"displayName":"Sparsh Agarwal","photoUrl":"","userId":"13037694610922482904"}},"outputId":"7547a920-399d-4e86-d1dc-afd0c6ea4bc2"},"source":["!wget https://github.com/WikidataRec-developer/Wikidata_Recommender/raw/main/Items_content_of_Wikidata_Active_Editor.csv"],"execution_count":2,"outputs":[{"output_type":"stream","text":["--2021-07-17 09:47:01--  https://github.com/WikidataRec-developer/Wikidata_Recommender/raw/main/Items_content_of_Wikidata_Active_Editor.csv\n","Resolving github.com (github.com)... 140.82.112.4\n","Connecting to github.com (github.com)|140.82.112.4|:443... connected.\n","HTTP request sent, awaiting response... 302 Found\n","Location: https://media.githubusercontent.com/media/WikidataRec-developer/Wikidata_Recommender/main/Items_content_of_Wikidata_Active_Editor.csv [following]\n","--2021-07-17 09:47:01--  https://media.githubusercontent.com/media/WikidataRec-developer/Wikidata_Recommender/main/Items_content_of_Wikidata_Active_Editor.csv\n","Resolving media.githubusercontent.com (media.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n","Connecting to media.githubusercontent.com (media.githubusercontent.com)|185.199.108.133|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 22564258 (22M) [text/plain]\n","Saving to: ‘Items_content_of_Wikidata_Active_Editor.csv’\n","\n","Items_content_of_Wi 100%[===================>]  21.52M  43.5MB/s    in 0.5s    \n","\n","2021-07-17 09:47:02 (43.5 MB/s) - ‘Items_content_of_Wikidata_Active_Editor.csv’ saved [22564258/22564258]\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"DmFzyKr83EOX","executionInfo":{"status":"ok","timestamp":1626515202113,"user_tz":-330,"elapsed":387,"user":{"displayName":"Sparsh Agarwal","photoUrl":"","userId":"13037694610922482904"}}},"source":["import pandas as pd"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":419},"id":"b2ubsqL_3FrI","executionInfo":{"status":"ok","timestamp":1626515222737,"user_tz":-330,"elapsed":1954,"user":{"displayName":"Sparsh Agarwal","photoUrl":"","userId":"13037694610922482904"}},"outputId":"85654416-eaaa-446f-b94c-4cbb401b311d"},"source":["df = pd.read_csv('Wikidata_Active_Editor.csv')\n","df"],"execution_count":4,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>user_id</th>\n","      <th>item_id</th>\n","      <th>frequence</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>142217</td>\n","      <td>Q10062</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>142217</td>\n","      <td>Q1019992</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>142217</td>\n","      <td>Q102282</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>142217</td>\n","      <td>Q10260337</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>142217</td>\n","      <td>Q10307549</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>3272081</th>\n","      <td>1421695</td>\n","      <td>Q926046</td>\n","      <td>3</td>\n","    </tr>\n","    <tr>\n","      <th>3272082</th>\n","      <td>1421695</td>\n","      <td>Q926331</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>3272083</th>\n","      <td>1421695</td>\n","      <td>Q949046</td>\n","      <td>8</td>\n","    </tr>\n","    <tr>\n","      <th>3272084</th>\n","      <td>1421695</td>\n","      <td>Q977871</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>3272085</th>\n","      <td>1421695</td>\n","      <td>Q985459</td>\n","      <td>1</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>3272086 rows × 3 columns</p>\n","</div>"],"text/plain":["         user_id    item_id  frequence\n","0         142217     Q10062          1\n","1         142217   Q1019992          1\n","2         142217    Q102282          1\n","3         142217  Q10260337          1\n","4         142217  Q10307549          1\n","...          ...        ...        ...\n","3272081  1421695    Q926046          3\n","3272082  1421695    Q926331          2\n","3272083  1421695    Q949046          8\n","3272084  1421695    Q977871          2\n","3272085  1421695    Q985459          1\n","\n","[3272086 rows x 3 columns]"]},"metadata":{"tags":[]},"execution_count":4}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":419},"id":"vZVZUc7u3KKg","executionInfo":{"status":"ok","timestamp":1626515251487,"user_tz":-330,"elapsed":998,"user":{"displayName":"Sparsh Agarwal","photoUrl":"","userId":"13037694610922482904"}},"outputId":"252e4e08-8ce9-4419-f433-dfe5b27b6707"},"source":["df2 = pd.read_csv('Items_content_of_Wikidata_Active_Editor.csv')\n","df2"],"execution_count":5,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>item_id</th>\n","      <th>item_label</th>\n","      <th>item_description</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>Q10062</td>\n","      <td>Laura Berg</td>\n","      <td>American softball player, born 1975</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>Q1019992</td>\n","      <td>Stockholm Stock Exchange</td>\n","      <td>Stock exchange located in Stockholm, Sweden</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>Q102282</td>\n","      <td>Titan</td>\n","      <td>Wikimedia disambiguation page</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>Q10260337</td>\n","      <td>2014 FIFA World Cup Group H</td>\n","      <td>soccer tournament group</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>Q10307549</td>\n","      <td>Template:Soccerbase</td>\n","      <td>Wikimedia template</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>381779</th>\n","      <td>Q383234</td>\n","      <td>San Carlos</td>\n","      <td>city in San Mateo County, California, USA</td>\n","    </tr>\n","    <tr>\n","      <th>381780</th>\n","      <td>Q1768753</td>\n","      <td>Monleón</td>\n","      <td>municipality of Spain</td>\n","    </tr>\n","    <tr>\n","      <th>381781</th>\n","      <td>Q8463289</td>\n","      <td>Category:Food preparation</td>\n","      <td>Wikimedia category</td>\n","    </tr>\n","    <tr>\n","      <th>381782</th>\n","      <td>Q198694</td>\n","      <td>Mohammad Yunus Khalis</td>\n","      <td>Mujahideen commander</td>\n","    </tr>\n","    <tr>\n","      <th>381783</th>\n","      <td>Q11017620</td>\n","      <td>Template:J. League Manager of the Year</td>\n","      <td>Wikimedia template</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>381784 rows × 3 columns</p>\n","</div>"],"text/plain":["          item_id  ...                             item_description\n","0          Q10062  ...          American softball player, born 1975\n","1        Q1019992  ...  Stock exchange located in Stockholm, Sweden\n","2         Q102282  ...                Wikimedia disambiguation page\n","3       Q10260337  ...                      soccer tournament group\n","4       Q10307549  ...                           Wikimedia template\n","...           ...  ...                                          ...\n","381779    Q383234  ...    city in San Mateo County, California, USA\n","381780   Q1768753  ...                        municipality of Spain\n","381781   Q8463289  ...                           Wikimedia category\n","381782    Q198694  ...                         Mujahideen commander\n","381783  Q11017620  ...                           Wikimedia template\n","\n","[381784 rows x 3 columns]"]},"metadata":{"tags":[]},"execution_count":5}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HOMiHxRR3Rq_","executionInfo":{"status":"ok","timestamp":1626515363936,"user_tz":-330,"elapsed":1044,"user":{"displayName":"Sparsh Agarwal","photoUrl":"","userId":"13037694610922482904"}},"outputId":"480f165c-d513-4ac5-c937-9d9fbe3153d7"},"source":["!wget https://github.com/WikidataRec-developer/Wikidata_Recommender/raw/cefd891b3f70f21a970681ee4b2e205f88864cd5/Scripts/NMoE.py"],"execution_count":8,"outputs":[{"output_type":"stream","text":["--2021-07-17 09:49:47--  https://github.com/WikidataRec-developer/Wikidata_Recommender/raw/cefd891b3f70f21a970681ee4b2e205f88864cd5/Scripts/NMoE.py\n","Resolving github.com (github.com)... 140.82.112.4\n","Connecting to github.com (github.com)|140.82.112.4|:443... connected.\n","HTTP request sent, awaiting response... 302 Found\n","Location: https://media.githubusercontent.com/media/WikidataRec-developer/Wikidata_Recommender/cefd891b3f70f21a970681ee4b2e205f88864cd5/Scripts/NMoE.py [following]\n","--2021-07-17 09:49:47--  https://media.githubusercontent.com/media/WikidataRec-developer/Wikidata_Recommender/cefd891b3f70f21a970681ee4b2e205f88864cd5/Scripts/NMoE.py\n","Resolving media.githubusercontent.com (media.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n","Connecting to media.githubusercontent.com (media.githubusercontent.com)|185.199.108.133|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 7266 (7.1K) [application/octet-stream]\n","Saving to: ‘NMoE.py’\n","\n","NMoE.py             100%[===================>]   7.10K  --.-KB/s    in 0s      \n","\n","2021-07-17 09:49:47 (41.6 MB/s) - ‘NMoE.py’ saved [7266/7266]\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"1ZPyVZwY4Smn","executionInfo":{"status":"ok","timestamp":1626515552891,"user_tz":-330,"elapsed":2887,"user":{"displayName":"Sparsh Agarwal","photoUrl":"","userId":"13037694610922482904"}}},"source":["import numpy as np\n","import pandas as pd\n","from sklearn.model_selection import train_test_split\n","import random\n","import tensorflow.keras\n","from tensorflow.keras import backend as K\n","import pandas as pd\n","import scipy.sparse as sparse\n","import numpy as np\n","\n","from sklearn.model_selection import train_test_split\n","\n","import ast\n","\n","import numpy as np\n","import pandas as pd\n","from sklearn.model_selection import train_test_split\n","import matplotlib.pyplot as plt\n","import scipy.sparse as sp\n","import random\n","import csv\n","from scipy import sparse\n","from collections import defaultdict\n","\n","import tensorflow.keras\n","from tensorflow.keras import backend as K"],"execution_count":11,"outputs":[]},{"cell_type":"code","metadata":{"id":"NHNKGSIg3jIk","executionInfo":{"status":"ok","timestamp":1626515557938,"user_tz":-330,"elapsed":963,"user":{"displayName":"Sparsh Agarwal","photoUrl":"","userId":"13037694610922482904"}}},"source":["class Dataset(object):\n","    \n","    def __init__(self, interactions, item_content, item_relations):\n","\n","        ''' Constructor'''\n","    \n","        ##Loading the interactions data    \n","        self.interactions_df = interactions\n","        \n","        ##Loading the metadata of items\n","        self.item_content = item_content\n","        self.item_relations = item_relations\n","        \n","        self.interactions_with_metadata=pd.merge(self.interactions_df, self.item_content, on='item_id')\n","        self.interactions_with_metadata=pd.merge(self.interactions_with_metadata, self.item_relations, on='item_id')\n","        \n","        self.interactions_with_metadata['content_features'] = self.interactions_with_metadata['content_features'].apply(lambda x: ast.literal_eval(x)) \n","        self.interactions_with_metadata['relations_features'] = self.interactions_with_metadata['relations_features'].apply(lambda x: ast.literal_eval(x)) \n","        \n","        ## Converting the numbers to categories to be used for creating the categorical codes to avoid using long hash keys \n","        self.interactions_with_metadata['user_id'] = self.interactions_with_metadata['user_id'].astype(\"category\")\n","        self.interactions_with_metadata['item_id'] = self.interactions_with_metadata['item_id'].astype(\"category\")\n","        \n","        ## cat.codes creates a categorical id for the users and items\n","        self.interactions_with_metadata['user_id'] = self.interactions_with_metadata['user_id'].cat.codes\n","        self.interactions_with_metadata['item_id'] = self.interactions_with_metadata['item_id'].cat.codes\n","        #print(interactions_with_metadata)\n","        \n","        self.interactions_df = self.interactions_with_metadata.drop(['content_features', 'relations_features'], axis=1)\n","        \n","        ##Preparing the train and test sets \n","        self.train, self.test = self.get_train_test_files(self.interactions_df)        \n","        self.train_matrix= self.get_data_matrix(self.train, self.interactions_df)\n","        self.test_matrix= self.get_data_matrix(self.test, self.interactions_df)\n","        \n","        self.train_with_metadata, self.test_with_metadata = self.get_train_test_files(self.interactions_with_metadata)\n","    \n","    \n","    \n","    def get_train_test_files(self, interactions):\n","        '''Split the interactions into train/test sets 80%/20% in random way'''\n","        train, test = train_test_split(interactions, test_size=0.20, random_state=42, shuffle=False)\n","        \n","        return train, test\n","    \n","        \n","    def get_data_matrix(self, data_set, full_data):\n","        '''Convert train dataframe into matrix'''\n","        mat = np.zeros((max(full_data.user_id)+1, max(full_data.item_id)+1))\n","        for index, line in data_set.iterrows():\n","            user, item, frequence = line['user_id'], line['item_id'], line['frequence']\n","            mat[user, item] = frequence\n","        return mat"],"execution_count":12,"outputs":[]},{"cell_type":"code","metadata":{"id":"_RL9p4yM3xRf","executionInfo":{"status":"ok","timestamp":1626515558567,"user_tz":-330,"elapsed":637,"user":{"displayName":"Sparsh Agarwal","photoUrl":"","userId":"13037694610922482904"}}},"source":["class Matrix_Factorization(object):\n","    ##Step1: Loading and preparing the data\n","    def __init__(self, interactions, data, data_matrix, process):\n","    \n","        print('The interaction data') \n","        \n","         \n","        self.interactions_df = interactions\n","        self.all_items = self.interactions_df['item_id'].tolist()\n","\n","        self.n_users = len(self.interactions_df.user_id.unique())\n","        self.n_items = len(self.interactions_df.item_id.unique())\n","        print('full n_users')\n","        print(self.n_users)\n","        print('full n_items')\n","        print(self.n_items)\n","\n","        \n","        self.data, self.data_matrix= data, data_matrix\n","\n","        self.data_n_users = len(self.data.user_id.unique())\n","        self.data_n_items = len(self.data.item_id.unique())\n","\n","        print('data_set')\n","        print(self.data.shape)\n","        print(self.data_n_users)\n","        print(self.data_n_items)\n","\n","        print('data_matrix')\n","        print(self.data_matrix.shape)\n","        print(self.data_matrix)   \n","\n","        self.user_interactions = K.constant(data_matrix)\n","        self.item_interactions = K.constant(data_matrix.T)\n","\n","        print('The constants')\n","        print(self.user_interactions.shape)\n","        print(self.item_interactions.shape)\n","        \n","        self.process_name = process\n","        \n","        self.MF_model = self.main(self.user_interactions, self.item_interactions, self.data, self.data_n_users, self.data_n_items, self.all_items, self.process_name)\n","\n","\n","    ##Step2: Build the model\n","    def identity_loss(y_true, y_pred):\n","        \"\"\"Ignore y_true and return the mean of y_pred.\n","        This is a hack to work-around the design of the Keras API that is\n","        not really suited to train networks with a triplet loss by default.\n","        \"\"\"\n","        return K.mean(y_pred - 0 * y_true)\n","\n","\n","    class BPR_triplet_loss(tensorflow.keras.layers.Layer):\n","        \"\"\"\n","            Layer object to minimise the triplet loss.\n","            We implement the Bayesian Personal Ranking triplet loss.\n","        \"\"\"\n","        \n","        def __init__(self, **kwargs):\n","            super().__init__(**kwargs)\n","         \n","        def call(self, inputs):\n","            user_latent, positive_item_latent, negative_item_latent = inputs\n","        \n","            # BPR loss\n","            pos_similarity = K.dot(user_latent,K.transpose(positive_item_latent))\n","            neg_similarity = K.dot(user_latent,K.transpose(negative_item_latent))\n","            \n","            loss = 1.0 - K.sigmoid(pos_similarity - neg_similarity)\n","\n","            return loss\n","\n","\n","    class ScoreLayer(tensorflow.keras.layers.Layer):\n","        \"\"\"\n","            Layer object to predict positive matches.\n","        \"\"\"\n","        def __init__(self, **kwargs):\n","            super().__init__(**kwargs)\n","        \n","        def rec_similarity(self, inputs):\n","            \"\"\"\n","                rec_similarity function\n","            \"\"\"\n","            user, item = inputs\n","            score = K.dot(user,K.transpose(item))\n","            return score\n","        \n","        def call(self, inputs):\n","            pred = self.rec_similarity(inputs)\n","            return pred\n","\n","\n","    def get_model(self, n_users, n_items, latent_dim, u_interactions, i_interactions):\n","        #Input variables\n","        user_input = tensorflow.keras.layers.Input(shape=(1,), dtype='int32', name = 'user_input')\n","        item_input_pos = tensorflow.keras.layers.Input(shape=(1,), dtype='int32', name = 'item_input_positive')\n","        item_input_neg = tensorflow.keras.layers.Input(shape=(1,), dtype='int32', name = 'item_input_negative')\n","        \n","        user_editing_input = tensorflow.keras.layers.Lambda(lambda x: K.gather(u_interactions, x))(user_input)\n","        user_editing_vector = tensorflow.keras.layers.Flatten()(user_editing_input)\n","        \n","        pos_item_editing_input = tensorflow.keras.layers.Lambda(lambda x: K.gather(i_interactions, x))(item_input_pos)\n","        pos_item_editing_vector = tensorflow.keras.layers.Flatten()(pos_item_editing_input)\n","        neg_item_editing_input = tensorflow.keras.layers.Lambda(lambda x: K.gather(i_interactions, x))(item_input_neg)\n","        neg_item_editing_vector = tensorflow.keras.layers.Flatten()(neg_item_editing_input)\n","\n","        user_layer = tensorflow.keras.layers.Dense(units=latent_dim, name='user_layer')\n","        \n","        #Shared layer for positive and negative items\n","        item_layer = tensorflow.keras.layers.Dense(units=latent_dim, name='item_layer')\n","            \n","        #User\n","        user_latent = user_layer(user_editing_vector)\n","        \n","        #Positive\n","        item_latent_pos = item_layer(pos_item_editing_vector)\n","        \n","        #Negative\n","        item_latent_neg = item_layer(neg_item_editing_vector)\n","        \n","        #TripletLoss Layer\n","        self.loss_layer = self.BPR_triplet_loss(name='triplet_loss_layer')([user_latent, item_latent_pos, item_latent_neg])\n","        \n","        ##Model\n","        network_train = tensorflow.keras.models.Model(inputs=[user_input, item_input_pos, item_input_neg], outputs=self.loss_layer, name = 'training_model')\n","\n","            \n","        return network_train\n","        \n","        \n","        \n","    def main(self, user_interactions, item_interactions, data, n_users, n_items, all_items, process_name):\n","\n","        latent_dim = 1024\n","\n","        MF_train = self.get_model(n_users, n_items, latent_dim, user_interactions, item_interactions)\n","\n","        ##config the model with losses and metrics\n","        MF_train.compile(loss=self.identity_loss, optimizer=tensorflow.keras.optimizers.Adam(lr=0.001))\n","        print(MF_train.summary())\n","        '''print(MF_train.layers)\n","        print(MF_predict.layers)'''\n","\n","\n","\n","        ##Step4: train the model and check the performance\n","        def get_triplets(data):\n","            user_input,item_pos,item_neg = [],[],[]\n","            for ind in data.index:\n","                # Positive instance\n","                user_input.append(data['user_id'][ind])\n","                ##Pick one of the positve ids\n","                item_pos.append(data['item_id'][ind])\n","                ##Pick one of the negative ids\n","                nni = random.choice(all_items)\n","                item_neg.append(nni)\n","                                   \n","            return user_input,item_pos,item_neg\n","\n","\n","\n","        ### Hyper parameters\n","        batch_size = 32\n","        epochs = 100\n","\n","        ### Training\n","        print(\"Fit model on training data\")\n","        user_input, item_input_pos, item_input_neg = get_triplets(data, all_items)\n","        loss = MF_train.fit([np.array(user_input), np.array(item_input_pos), np.array(item_input_neg)], #Triplet input\n","                                 np.ones(len(user_input)), #Labels\n","                                 batch_size=batch_size, verbose=1, shuffle=True, validation_split = 0.10, epochs=epochs)\n","\n","\n","        train_model_out_file = 'Pretrain_MF_Wikidata_14M_{}.tf'.format(process_name) ## The process is train or test \n","\n","        MF_train.save_weights(train_model_out_file, overwrite=True)\n","        print('The train and predict models have saved successfully')\n","        \n","        \n","        return MF_train"],"execution_count":13,"outputs":[]},{"cell_type":"code","metadata":{"id":"d53L3zmE30e6"},"source":["#Step1: Loading and reading the data \n","\n","##Loading the interactions data \n","print('Loading interaction data and metadata data')    \n","dataset_interactions_file = pd.read_csv('Wikidata-14M.csv', sep=',', encoding= 'utf-8', header=0)\n","dataset_item_content = pd.read_csv('Wikidata-14M_ELMo_embeddings.csv', sep=',', encoding= 'utf-8', header=0)\n","dataset_item_relations = pd.read_csv('Wikidata-14M_TransR_ent_embeddings.csv', sep=',', encoding= 'utf-8', header=0)\n","\n","dataset = Dataset(dataset_interactions_file, dataset_item_content, dataset_item_relations)\n","\n","print('The full interactions data')\n","interactions_with_metadata_df = dataset.interactions_with_metadata\n","print(interactions_with_metadata_df.head())\n","\n","interactions_df = dataset.interactions_df\n","\n","all_items = interactions_df['item_id'].tolist()\n","\n","n_users = len(interactions_df.user_id.unique())\n","n_items = len(interactions_df.item_id.unique())\n","print('n_users')\n","print(n_users)\n","print('n_items')\n","print(n_items)\n","\n","\n","##Loading the train data \n","train, train_matrix = dataset.train, dataset.train_matrix\n","train_with_metadata = dataset.train_with_metadata\n","\n","train_n_users = len(train.user_id.unique())\n","train_n_items = len(train.item_id.unique())\n","\n","print('train_set')\n","print(train.shape)\n","print(train_n_users)\n","print(train_n_items)\n","\n","print(train_matrix.shape)\n","#print(train_matrix)   \n","\n","user_interactions = K.constant(train_matrix)\n","item_interactions = K.constant(train_matrix.T)\n","\n","print('The constants')\n","print(user_interactions.shape)\n","print(item_interactions.shape)\n","\n","\n","\n","##Step2: Build the model\n","\n","def get_model(MF_model, latent_dim):\n","        \n","    #Input variables\n","    user_input = tensorflow.keras.layers.Input(shape=(1,), dtype='int32', name = 'user_input')\n","    item_input = tensorflow.keras.layers.Input(shape=(1,), dtype='int32', name = 'item_input')\n","    \n","    item_contents_input = tensorflow.keras.layers.Input(shape=(1024,), name = 'item_contents_input') \n","    item_relations_input = tensorflow.keras.layers.Input(shape=(1024,), name = 'item_relations_input') \n","\n","    user_editing_input = tensorflow.keras.layers.Lambda(lambda x: K.gather(user_interactions, x))(user_input)\n","    user_editing_vector = tensorflow.keras.layers.Flatten()(user_editing_input)\n","    \n","    item_editing_input = tensorflow.keras.layers.Lambda(lambda x: K.gather(item_interactions, x))(item_input)\n","    item_editing_vector = tensorflow.keras.layers.Flatten()(item_editing_input)\n","    \n","    #User\n","    user_latent = MF_model.get_layer('user_layer').get_weights()\n","    user_latent = tensorflow.keras.layers.Dense(units=latent_dim, name='user_layer', weights=user_latent, trainable=False)(user_editing_vector)\n","   \n","    #Item\n","    item_latent = MF_model.get_layer('item_layer').get_weights()\n","    item_latent = tensorflow.keras.layers.Dense(units=latent_dim, name='item_layer', weights=item_latent, trainable=False)(item_editing_vector)\n","    \n","    #Item metadata\n","    item_contents = tensorflow.keras.layers.Dense(units=latent_dim, name='item_contents_features')(item_contents_input) \n","    item_relations = tensorflow.keras.layers.Dense(units=latent_dim, name='item_relations_features')(item_relations_input) \n","        \n","    #Reshape\n","    item_latent = tensorflow.keras.layers.Reshape((1,1024))(item_latent)\n","    item_contents = tensorflow.keras.layers.Reshape((1,1024))(item_contents)\n","    item_relations = tensorflow.keras.layers.Reshape((1,1024))(item_relations)\n","    \n","    #Concatenate layer\n","    concat_items = tensorflow.keras.layers.Concatenate(axis=1)([item_latent, item_contents, item_relations])\n","    \n","    #MoE layer - as CNN layers\n","    x = tensorflow.keras.layers.Conv1D(1024, 1, activation='relu')(concat_items)\n","    x = tensorflow.keras.layers.Conv1D(1024, 1, activation='relu')(x)\n","    ##Soft Gating\n","    weights = tensorflow.keras.layers.Conv1D(1024, 1, activation='softmax')(x)\n","    \n","    ##Adding weights to the features \n","    weighted_items = tensorflow.keras.layers.multiply([weights, concat_items])\n","    \n","    #Element-wise product of user and item \n","    input_vector = tensorflow.keras.layers.multiply([user_latent, weighted_items])\n","    input_vector = tensorflow.keras.layers.Flatten()(input_vector)\n","    \n","    prediction = tensorflow.keras.layers.Dense(1, activation='sigmoid', kernel_initializer='lecun_uniform', name = 'prediction')(input_vector)\n","    \n","    model = tensorflow.keras.models.Model(inputs=[user_input, item_input, item_contents_input, item_relations_input], outputs=prediction)\n","    \n","    return model\n","\n","  \n","latent_dim = 1024   \n","\n","MF = Matrix_Factorization(interactions_df, train, train_matrix, 'train')\n","MF_model = MF.MF_model\n","MF_model.load_weights('Pretrain_MF_BPR_s8.tf')\n","\n","Hybrid_model = get_model(MF_model, latent_dim)\n","\n","\n","##config the model with losses and metrics\n","Hybrid_model.compile(optimizer=tensorflow.keras.optimizers.Adam(lr=0.001), loss='binary_crossentropy', metrics=['accuracy', 'AUC', 'Precision', 'Recall'])\n","print(Hybrid_model.summary())\n","\n","\n","##Step4: train the model and check the performance\n","def get_train_instances(data, num_negatives):\n","    user_input, item_input, item_content, item_relations, labels = [],[],[],[],[]\n","    count=0\n","    for ind in data.index:\n","        count= count + 1\n","        user_input.append(data['user_id'][ind])\n","        item_input.append(data['item_id'][ind])\n","        content = data.loc[data['item_id'] == data['item_id'][ind], 'content_features'].iloc[0]\n","        relations = data.loc[data['item_id'] == data['item_id'][ind], 'relations_features'].iloc[0]\n","        item_content.append(content)\n","        item_relations.append(relations)\n","        labels.append(1)\n","        # negative instances\n","        for t in range(num_negatives):\n","            #Randomly return one item\n","            ni = random.choice(all_items)\n","            user_input.append(data['user_id'][ind])\n","            item_input.append(ni)\n","            content = interactions_df.loc[interactions_df['item_id'] == ni, 'content_features'].iloc[0]\n","            relations = interactions_df.loc[interactions_df['item_id'] == ni, 'relations_features'].iloc[0]\n","            item_content.append(content)\n","            item_relations.append(relations)\n","            labels.append(0)\n","    return user_input, item_input, item_content, item_relations, labels\n","\n","\n","user_input, item_input, item_content, item_relations, labels = get_train_instances(train_with_metadata, 1) \n","user_input = np.array(user_input)\n","item_input = np.array(item_input)\n","item_content = np.array(item_content)\n","item_relations = np.array(item_relations)\n","labels = np.array(labels)\n","\n","print(\"Fit model on training data\")\n","hist = Hybrid_model.fit([user_input, item_input, item_content, item_relations], #input\n","                     labels, # labels \n","                     batch_size=32, epochs=100, verbose=1, validation_split=0.10, shuffle=True)\n","\n","\n","model_out_file = 'Pretrain_NMoE_Wikidata-14M.h5'\n","\n","Hybrid_model.save(model_out_file, overwrite=True)\n","print('The NMoE model has saved successfully')"],"execution_count":null,"outputs":[]}]}