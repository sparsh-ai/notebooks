{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"2022-01-24-opl.ipynb","provenance":[{"file_id":"https://github.com/recohut/nbs/blob/main/raw/T257798%20%7C%20Off-Policy%20Learning%20in%20Two-stage%20Recommender%20Systems.ipynb","timestamp":1644669405457}],"collapsed_sections":[],"toc_visible":true,"authorship_tag":"ABX9TyOrh5KCfclKzGLkvz3STD8Q"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"Hv-tVJksaXvl"},"source":["# Off-Policy Learning in Two-stage Recommender Systems"]},{"cell_type":"markdown","metadata":{"id":"HojVVZWkoN8I"},"source":["Many real-world recommender systems need to be highly scalable: matching millions of items with billions of users, with milliseconds latency. The scalability requirement has led to widely used two-stage recommender systems, consisting of efficient candidate generation model(s) in the first stage and a more powerful ranking model in the second stage.\n","\n","Logged user feedback, e.g., user clicks or dwell time, are often used to build both candidate generation and ranking models for recommender systems. While itâ€™s easy to collect large amount of such data, they are inherently biased because the feedback can only be observed on items recommended by the previous systems. Recently, off-policy correction on such biases have attracted increasing interest in the field of recommender system research. However, most existing work either assumed that the recommender system is a single-stage system or only studied how to apply off-policy correction to the candidate generation stage of the system without explicitly considering the interactions between the two stages.\n","\n","In this work, we propose a two-stage off-policy policy gradient method, and showcase that ignoring the interaction between the two stages leads to a sub-optimal policy in two-stage recommender systems. The proposed method explicitly takes into account the ranking model when training the candidate generation model, which helps improve the performance of the whole system. We conduct experiments on real-world datasets with large item space and demonstrate the effectiveness of our proposed method.\n","\n","## Pseudo code\n","\n","![](https://github.com/recohut/drl-recsys/raw/a8a38a68e3606557f3501c8adc85fc6aa5726bc1/docs/_images/T257798_1.png)\n","\n","## Model structure\n","\n","![](https://github.com/recohut/drl-recsys/raw/a8a38a68e3606557f3501c8adc85fc6aa5726bc1/docs/_images/T257798_2.png)\n","\n","![](https://github.com/recohut/drl-recsys/raw/a8a38a68e3606557f3501c8adc85fc6aa5726bc1/docs/_images/T257798_3.png)\n","\n","## Training model\n","\n","1. The simulation model - divides MovieLens-1M into training set, validation set and test set at 3:1:1.\n","2. Behavior strategy model and ranking model - Use 10,000 user-item pairs randomly generated by the simulation model to train the behavior strategy model, and then obtain a bandit data set by sampling the top-5 items of each user predicted by the behavior strategy model. Train the ranking model based on this data set, divide 2000 users as the verification set, and 4000 users as the test set.\n","3. Candidate generation model - Set the optimizer to AdaGrad, the initial learning rate is 0.05, and the weight limit parameters of 1-IPS and 2-IPS are set c1 = 10, c2 = 0.01 c_1 = 10, c_2=0.01.\n","For each training method, we trained 20 candidate generation models initialized with different random seeds. The early stopping method is applied in both one-stage evaluation and two-stage evaluation."]},{"cell_type":"markdown","metadata":{"id":"MicXUzrTYrJk"},"source":["## Imports"]},{"cell_type":"code","metadata":{"id":"LlZYJdfqS9ON"},"source":["import os\n","import numpy as np\n","import pandas as pd\n","import torch\n","from torch.utils.data import Dataset\n","from tqdm import tqdm\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from collections import OrderedDict"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"QVgH_-gKXPnn"},"source":["## Model"]},{"cell_type":"code","metadata":{"id":"lBqeMgi0XQ10"},"source":["NUM_ITEMS = 3883\n","NUM_YEARS = 81\n","NUM_GENRES = 18\n","NUM_USERS = 6040\n","NUM_OCCUPS = 21\n","NUM_AGES = 7\n","NUM_ZIPS = 3439"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"oWN7pQP0XXe_"},"source":["class ItemRep(nn.Module):\n","    \"\"\"Item representation layer.\"\"\"\n","\n","    def __init__(self, item_emb_size=10, year_emb_size=5, genre_hidden=5):\n","        super(ItemRep, self).__init__()\n","        self.item_embedding = nn.Embedding(\n","            NUM_ITEMS + 1, item_emb_size, padding_idx=0)\n","        self.year_embedding = nn.Embedding(NUM_YEARS, year_emb_size)\n","        self.genre_linear = nn.Linear(NUM_GENRES, genre_hidden)\n","        self.rep_dim = item_emb_size + year_emb_size + genre_hidden\n","\n","    def forward(self, categorical_feats, real_feats):\n","        out = torch.cat(\n","            [\n","                self.item_embedding(categorical_feats[:, 0]),\n","                self.year_embedding(categorical_feats[:, 1]),\n","                self.genre_linear(real_feats)\n","            ],\n","            dim=1)\n","        return out"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4vrFd9FwXWGP"},"source":["class UserRep(nn.Module):\n","    \"\"\"User representation layer.\"\"\"\n","\n","    def __init__(self, user_emb_size=10, feature_emb_size=5):\n","        super(UserRep, self).__init__()\n","        self.user_embedding = nn.Embedding(\n","            NUM_USERS + 1, user_emb_size, padding_idx=0)\n","        self.gender_embedding = nn.Embedding(2, feature_emb_size)\n","        self.age_embedding = nn.Embedding(NUM_AGES, feature_emb_size)\n","        self.occup_embedding = nn.Embedding(NUM_OCCUPS, feature_emb_size)\n","        self.zip_embedding = nn.Embedding(NUM_ZIPS, feature_emb_size)\n","        self.rep_dim = user_emb_size + feature_emb_size * 4\n","\n","    def forward(self, categorical_feats, real_feats=None):\n","        reps = [\n","            self.user_embedding(categorical_feats[:, 0]),\n","            self.gender_embedding(categorical_feats[:, 1]),\n","            self.age_embedding(categorical_feats[:, 2]),\n","            self.occup_embedding(categorical_feats[:, 3]),\n","            self.zip_embedding(categorical_feats[:, 4])\n","        ]\n","        out = torch.cat(reps, dim=1)\n","        return out"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ni5trB4aXUxc"},"source":["class ImpressionSimulator(nn.Module):\n","    \"\"\"Simulator model that predicts the outcome of impression.\"\"\"\n","\n","    def __init__(self, hidden=100, use_impression_feats=False):\n","        super(ImpressionSimulator, self).__init__()\n","        self.user_rep = UserRep()\n","        self.item_rep = ItemRep()\n","        self.use_impression_feats = use_impression_feats\n","        input_dim = self.user_rep.rep_dim + self.item_rep.rep_dim\n","        if use_impression_feats:\n","            input_dim += 1\n","        self.linear = nn.Sequential(\n","            nn.Linear(input_dim, hidden),\n","            nn.ReLU(), nn.Linear(hidden, 50), nn.ReLU(), nn.Linear(50, 1))\n","\n","    def forward(self, user_feats, item_feats, impression_feats=None):\n","        users = self.user_rep(**user_feats)\n","        items = self.item_rep(**item_feats)\n","        inputs = torch.cat([users, items], dim=1)\n","        if self.use_impression_feats:\n","            inputs = torch.cat([inputs, impression_feats[\"real_feats\"]], dim=1)\n","        return self.linear(inputs).squeeze()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"eA_V_mUfXTIS"},"source":["class Nominator(nn.Module):\n","    \"\"\"Two tower nominator model.\"\"\"\n","\n","    def __init__(self):\n","        super(Nominator, self).__init__()\n","        self.item_rep = ItemRep()\n","        self.user_rep = UserRep()\n","        self.linear = nn.Linear(self.user_rep.rep_dim, self.item_rep.rep_dim)\n","        self.binary = True\n","\n","    def forward(self, user_feats, item_feats):\n","        users = self.linear(F.relu(self.user_rep(**user_feats)))\n","        users = torch.unsqueeze(users, 2)  # (b, h) -> (b, h, 1)\n","        items = self.item_rep(**item_feats)\n","        if self.binary:\n","            items = torch.unsqueeze(items, 1)  # (b, h) -> (b, 1, h)\n","        else:\n","            items = torch.unsqueeze(items, 0).expand(users.size(0), -1,\n","                                                     -1)  # (c, h) -> (b, c, h)\n","        logits = torch.bmm(items, users).squeeze()\n","        return logits\n","\n","    def set_binary(self, binary=True):\n","        self.binary = binary"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"edS3BzfQXRzW"},"source":["class Ranker(nn.Module):\n","    \"\"\"Ranker model.\"\"\"\n","\n","    def __init__(self):\n","        super(Ranker, self).__init__()\n","        self.item_rep = ItemRep()\n","        self.user_rep = UserRep()\n","        self.linear = nn.Linear(self.user_rep.rep_dim + 1,\n","                                self.item_rep.rep_dim)\n","        self.binary = True\n","\n","    def forward(self, user_feats, item_feats, impression_feats):\n","        users = self.user_rep(**user_feats)\n","        context_users = torch.cat(\n","            [users, impression_feats[\"real_feats\"]], dim=1)\n","        context_users = self.linear(context_users)\n","        context_users = torch.unsqueeze(context_users,\n","                                        2)  # (b, h) -> (b, h, 1)\n","        items = self.item_rep(**item_feats)\n","        if self.binary:\n","            items = torch.unsqueeze(items, 1)  # (b, h) -> (b, 1, h)\n","        else:\n","            items = torch.unsqueeze(items, 0).expand(\n","                users.size(0), -1, -1)  # (c, h) -> (b, c, h), c=#items\n","        logits = torch.bmm(items, context_users).squeeze()\n","        return logits\n","\n","    def set_binary(self, binary=True):\n","        self.binary = binary"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_wp1K2tmZavh"},"source":["## Download data"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TG0jGrFFZcjS","executionInfo":{"status":"ok","timestamp":1633453276159,"user_tz":-330,"elapsed":789,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}},"outputId":"82905f21-cb45-4ee8-eff9-74b6fc1d8875"},"source":["!wget -q --show-progress https://files.grouplens.org/datasets/movielens/ml-1m.zip\n","!unzip ml-1m.zip"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["ml-1m.zip           100%[===================>]   5.64M  32.7MB/s    in 0.2s    \n","Archive:  ml-1m.zip\n","   creating: ml-1m/\n","  inflating: ml-1m/movies.dat        \n","  inflating: ml-1m/ratings.dat       \n","  inflating: ml-1m/README            \n","  inflating: ml-1m/users.dat         \n"]}]},{"cell_type":"markdown","metadata":{"id":"l0VwA1nHS-Jn"},"source":["## Dataset"]},{"cell_type":"markdown","metadata":{"id":"QQJ1yQJqS-GS"},"source":["Create a torch dataset class for ML-1M; convert the label into binary labels (positive if rating > 3)."]},{"cell_type":"code","metadata":{"id":"VbiK3zhUXiPZ"},"source":["class MovieLensDataset(Dataset):\n","    def __init__(self, filepath, device=\"cuda:0\"):\n","        self.device = device\n","        ratings, users, items = self.load_data(filepath)\n","\n","        self.user_feats = {}\n","        self.item_feats = {}\n","        self.impression_feats = {}\n","\n","        self.user_feats[\"categorical_feats\"] = torch.LongTensor(\n","            users.values).to(device)\n","        self.item_feats[\"categorical_feats\"] = torch.LongTensor(\n","            items.values[:, :2]).to(device)\n","        self.item_feats[\"real_feats\"] = torch.FloatTensor(\n","            items.values[:, 2:]).to(device)\n","        self.impression_feats[\"user_ids\"] = torch.LongTensor(\n","            ratings.values[:, 0]).to(device)\n","        self.impression_feats[\"item_ids\"] = torch.LongTensor(\n","            ratings.values[:, 1]).to(device)\n","        self.impression_feats[\"real_feats\"] = torch.FloatTensor(\n","            ratings.values[:, 3]).view(-1, 1).to(device)\n","        self.impression_feats[\"labels\"] = torch.FloatTensor(\n","            ratings.values[:, 2]).to(device)\n","\n","    def __len__(self):\n","        return len(self.impression_feats[\"user_ids\"])\n","\n","    def __getitem__(self, idx):\n","        labels = self.impression_feats[\"labels\"][idx]\n","        feats = {}\n","        feats[\"impression_feats\"] = {}\n","        feats[\"impression_feats\"][\"real_feats\"] = self.impression_feats[\n","            \"real_feats\"][idx]\n","        user_id = self.impression_feats[\"user_ids\"][idx]\n","        item_id = self.impression_feats[\"item_ids\"][idx]\n","        feats[\"user_feats\"] = {\n","            key: value[user_id - 1]\n","            for key, value in self.user_feats.items()\n","        }\n","        feats[\"item_feats\"] = {\n","            key: value[item_id - 1]\n","            for key, value in self.item_feats.items()\n","        }\n","        return feats, labels\n","\n","    def load_data(self, filepath):\n","        names = \"UserID::MovieID::Rating::Timestamp\".split(\"::\")\n","        ratings = pd.read_csv(\n","            os.path.join(filepath, \"ratings.dat\"),\n","            sep=\"::\",\n","            names=names,\n","            engine=\"python\")\n","        ratings[\"Rating\"] = (ratings[\"Rating\"] > 3).astype(int)\n","        ratings[\"Timestamp\"] = (\n","            ratings[\"Timestamp\"] - ratings[\"Timestamp\"].min()\n","        ) / float(ratings[\"Timestamp\"].max() - ratings[\"Timestamp\"].min())\n","\n","        names = \"UserID::Gender::Age::Occupation::Zip-code\".split(\"::\")\n","        users = pd.read_csv(\n","            os.path.join(filepath, \"users.dat\"),\n","            sep=\"::\",\n","            names=names,\n","            engine=\"python\")\n","        for i in range(1, users.shape[1]):\n","            users.iloc[:, i] = pd.factorize(users.iloc[:, i])[0]\n","\n","        names = \"MovieID::Title::Genres\".split(\"::\")\n","        Genres = [\n","            \"Action\", \"Adventure\", \"Animation\", \"Children's\", \"Comedy\",\n","            \"Crime\", \"Documentary\", \"Drama\", \"Fantasy\", \"Film-Noir\", \"Horror\",\n","            \"Musical\", \"Mystery\", \"Romance\", \"Sci-Fi\", \"Thriller\", \"War\",\n","            \"Western\"\n","        ]\n","        movies = pd.read_csv(\n","            os.path.join(filepath, \"movies.dat\"),\n","            sep=\"::\",\n","            names=names,\n","            engine=\"python\")\n","        movies[\"Year\"] = movies[\"Title\"].apply(lambda x: x[-5:-1])\n","        for genre in Genres:\n","            movies[genre] = movies[\"Genres\"].apply(lambda x: genre in x)\n","        movies.iloc[:, 3] = pd.factorize(movies.iloc[:, 3])[0]\n","        movies.iloc[:, 4:] = movies.iloc[:, 4:].astype(float)\n","        movies = movies.loc[:, [\"MovieID\", \"Year\"] + Genres]\n","        movies.iloc[:, 2:] = movies.iloc[:, 2:].div(\n","            movies.iloc[:, 2:].sum(axis=1), axis=0)\n","\n","        movie_id_map = {}\n","        for i in range(movies.shape[0]):\n","            movie_id_map[movies.loc[i, \"MovieID\"]] = i + 1\n","\n","        movies[\"MovieID\"] = movies[\"MovieID\"].apply(lambda x: movie_id_map[x])\n","        ratings[\"MovieID\"] = ratings[\"MovieID\"].apply(\n","            lambda x: movie_id_map[x])\n","\n","        self.NUM_ITEMS = len(movies.MovieID.unique())\n","        self.NUM_YEARS = len(movies.Year.unique())\n","        self.NUM_GENRES = movies.shape[1] - 2\n","\n","        self.NUM_USERS = len(users.UserID.unique())\n","        self.NUM_OCCUPS = len(users.Occupation.unique())\n","        self.NUM_AGES = len(users.Age.unique())\n","        self.NUM_ZIPS = len(users[\"Zip-code\"].unique())\n","\n","        return ratings, users, movies"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Oatjq-lbXgCw"},"source":["class SyntheticMovieLensDataset(Dataset):\n","    def __init__(self, filepath, simulator_path, synthetic_data_path, cut=0.764506,\n","                 device=\"cuda:0\"):\n","        self.device = device\n","        self.cut = cut\n","        self.simulator = None\n","\n","        ratings, users, items = self.load_data(filepath)\n","\n","        self.user_feats = {}\n","        self.item_feats = {}\n","        self.impression_feats = {}\n","\n","        self.user_feats[\"categorical_feats\"] = torch.LongTensor(users.values)\n","        self.item_feats[\"categorical_feats\"] = torch.LongTensor(\n","            items.values[:, :2])\n","        self.item_feats[\"real_feats\"] = torch.FloatTensor(items.values[:, 2:])\n","\n","        if os.path.exists(synthetic_data_path):\n","            self.impression_feats = torch.load(synthetic_data_path)\n","            self.impression_feats[\"labels\"] = (\n","                self.impression_feats[\"label_probs\"] >= cut).to(\n","                    dtype=torch.float32)\n","            print(\"loaded full_impression_feats.pt\")\n","        else:\n","            print(\"generating impression_feats\")\n","            self.simulator = ImpressionSimulator(use_impression_feats=True)\n","            self.simulator.load_state_dict(torch.load(simulator_path))\n","            self.simulator = self.simulator.to(device)\n","\n","            impressions = self.get_full_impressions(ratings)\n","\n","            self.impression_feats[\"user_ids\"] = torch.LongTensor(\n","                impressions[:, 0])\n","            self.impression_feats[\"item_ids\"] = torch.LongTensor(\n","                impressions[:, 1])\n","            self.impression_feats[\"real_feats\"] = torch.FloatTensor(\n","                impressions[:, 2]).view(-1, 1)\n","            self.impression_feats[\"labels\"] = torch.zeros_like(\n","                self.impression_feats[\"real_feats\"])\n","\n","            self.impression_feats[\"label_probs\"] = self.generate_labels()\n","            self.impression_feats[\"labels\"] = (\n","                self.impression_feats[\"label_probs\"] >= cut).to(\n","                    dtype=torch.float32)\n","\n","            torch.save(self.impression_feats, synthetic_data_path)\n","            print(\"saved impression_feats\")\n","\n","    def __len__(self):\n","        return len(self.impression_feats[\"user_ids\"])\n","\n","    def __getitem__(self, idx):\n","        labels = self.impression_feats[\"labels\"][idx]\n","        feats = {}\n","        feats[\"impression_feats\"] = {}\n","        feats[\"impression_feats\"][\"real_feats\"] = self.impression_feats[\n","            \"real_feats\"][idx]\n","        user_id = self.impression_feats[\"user_ids\"][idx]\n","        item_id = self.impression_feats[\"item_ids\"][idx]\n","        feats[\"user_feats\"] = {\n","            key: value[user_id - 1]\n","            for key, value in self.user_feats.items()\n","        }\n","        feats[\"item_feats\"] = {\n","            key: value[item_id - 1]\n","            for key, value in self.item_feats.items()\n","        }\n","        return feats, labels\n","\n","    def load_data(self, filepath):\n","        names = \"UserID::MovieID::Rating::Timestamp\".split(\"::\")\n","        ratings = pd.read_csv(\n","            os.path.join(filepath, \"ratings.dat\"),\n","            sep=\"::\",\n","            names=names,\n","            engine=\"python\")\n","        ratings[\"Rating\"] = (ratings[\"Rating\"] > 3).astype(int)\n","        ratings[\"Timestamp\"] = (\n","            ratings[\"Timestamp\"] - ratings[\"Timestamp\"].min()\n","        ) / float(ratings[\"Timestamp\"].max() - ratings[\"Timestamp\"].min())\n","\n","        names = \"UserID::Gender::Age::Occupation::Zip-code\".split(\"::\")\n","        users = pd.read_csv(\n","            os.path.join(filepath, \"users.dat\"),\n","            sep=\"::\",\n","            names=names,\n","            engine=\"python\")\n","        for i in range(1, users.shape[1]):\n","            users.iloc[:, i] = pd.factorize(users.iloc[:, i])[0]\n","\n","        names = \"MovieID::Title::Genres\".split(\"::\")\n","        Genres = [\n","            \"Action\", \"Adventure\", \"Animation\", \"Children's\", \"Comedy\",\n","            \"Crime\", \"Documentary\", \"Drama\", \"Fantasy\", \"Film-Noir\", \"Horror\",\n","            \"Musical\", \"Mystery\", \"Romance\", \"Sci-Fi\", \"Thriller\", \"War\",\n","            \"Western\"\n","        ]\n","        movies = pd.read_csv(\n","            os.path.join(filepath, \"movies.dat\"),\n","            sep=\"::\",\n","            names=names,\n","            engine=\"python\")\n","        movies[\"Year\"] = movies[\"Title\"].apply(lambda x: x[-5:-1])\n","        for genre in Genres:\n","            movies[genre] = movies[\"Genres\"].apply(lambda x: genre in x)\n","        movies.iloc[:, 3] = pd.factorize(movies.iloc[:, 3])[0]\n","        movies.iloc[:, 4:] = movies.iloc[:, 4:].astype(float)\n","        movies = movies.loc[:, [\"MovieID\", \"Year\"] + Genres]\n","        movies.iloc[:, 2:] = movies.iloc[:, 2:].div(\n","            movies.iloc[:, 2:].sum(axis=1), axis=0)\n","\n","        movie_id_map = {}\n","        for i in range(movies.shape[0]):\n","            movie_id_map[movies.loc[i, \"MovieID\"]] = i + 1\n","\n","        movies[\"MovieID\"] = movies[\"MovieID\"].apply(lambda x: movie_id_map[x])\n","        ratings[\"MovieID\"] = ratings[\"MovieID\"].apply(\n","            lambda x: movie_id_map[x])\n","\n","        self.NUM_ITEMS = len(movies.MovieID.unique())\n","        self.NUM_YEARS = len(movies.Year.unique())\n","        self.NUM_GENRES = movies.shape[1] - 2\n","\n","        self.NUM_USERS = len(users.UserID.unique())\n","        self.NUM_OCCUPS = len(users.Occupation.unique())\n","        self.NUM_AGES = len(users.Age.unique())\n","        self.NUM_ZIPS = len(users[\"Zip-code\"].unique())\n","\n","        return ratings, users, movies\n","\n","    def get_full_impressions(self, ratings):\n","        \"\"\"Gets NUM_USERS x NUM_ITEMS impression features by iterating the user and item ids.\n","        The impression-level feature, i.e. the timestamp, is sampled from a normal distribution with\n","        mean and std as the empirical mean and std of each user's recorded timestamps in the real data.\n","        \"\"\"\n","        timestamps = {}\n","        for i in range(len(ratings)):\n","            u_id = ratings.loc[i, \"UserID\"]\n","            timestamps[u_id] = timestamps.get(u_id, [])\n","            timestamps[u_id].append(ratings.loc[i, \"Timestamp\"])\n","        rs = np.random.RandomState(0)\n","        t_samples = []\n","        for i in range(self.NUM_USERS):\n","            u_id = i + 1\n","            t_samples.append(\n","                rs.normal(\n","                    loc=np.mean(timestamps[u_id]),\n","                    scale=np.std(timestamps[u_id]),\n","                    size=(self.NUM_ITEMS, )))\n","        t_samples = np.array(t_samples)\n","\n","        impressions = []\n","        for i in range(self.NUM_USERS):\n","            for j in range(self.NUM_ITEMS):\n","                impressions.append([i + 1, j + 1, t_samples[i, j]])\n","        impressions = np.array(impressions)\n","        return impressions\n","\n","    def to_device(self, data):\n","        if isinstance(data, torch.Tensor):\n","            return data.to(self.device)\n","        if isinstance(data, dict):\n","            transformed_data = {}\n","            for key in data:\n","                transformed_data[key] = self.to_device(data[key])\n","        elif type(data) == list:\n","            transformed_data = []\n","            for x in data:\n","                transformed_data.append(self.to_device(x))\n","        else:\n","            raise NotImplementedError(\n","                \"Type {} not supported.\".format(type(data)))\n","        return transformed_data\n","\n","    def generate_labels(self):\n","        \"\"\"Generates the binary labels using the simulator on every user-item pair.\"\"\"\n","        with torch.no_grad():\n","            self.simulator.eval()\n","            preds = []\n","            for i in tqdm(range(len(self.impression_feats[\"labels\"]) // 500)):\n","                feats, _ = self.__getitem__(\n","                    list(range(i * 500, (i + 1) * 500)))\n","                feats = self.to_device(feats)\n","                outputs = torch.sigmoid(self.simulator(**feats))\n","                preds += list(outputs.squeeze().cpu().numpy())\n","            if (i + 1) * 500 < len(self.impression_feats[\"labels\"]):\n","                feats, _ = self.__getitem__(\n","                    list(\n","                        range((i + 1) * 500,\n","                              len(self.impression_feats[\"labels\"]))))\n","                feats = self.to_device(feats)\n","                outputs = torch.sigmoid(self.simulator(**feats))\n","                preds += list(outputs.squeeze().cpu().numpy())\n","        return torch.FloatTensor(np.array(preds))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DHxVBSLOS-EH"},"source":["## Metrics"]},{"cell_type":"code","metadata":{"id":"mG2JSOybYwSb"},"source":["class BaseMetric(object):\n","    def __init__(self, rel_threshold, k):\n","        self.rel_threshold = rel_threshold\n","        if np.isscalar(k):\n","            k = np.array([k])\n","        self.k = k\n","\n","    def __len__(self):\n","        return len(self.k)\n","\n","    def __call__(self, *args, **kwargs):\n","        raise NotImplementedError\n","\n","    def _compute(self, *args, **kwargs):\n","        raise NotImplementedError"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ORCmDzDbYyve"},"source":["class PrecisionRecall(BaseMetric):\n","    def __init__(self, rel_threshold=0, k=10):\n","        super(PrecisionRecall, self).__init__(rel_threshold, k)\n","\n","    def __len__(self):\n","        return 2 * len(self.k)\n","\n","    def __str__(self):\n","        str_precision = [('Precision@%1.f' % x) for x in self.k]\n","        str_recall = [('Recall@%1.f' % x) for x in self.k]\n","        return (','.join(str_precision)) + ',' + (','.join(str_recall))\n","\n","    def __call__(self, targets, predictions):\n","        precision, recall = zip(\n","            *[self._compute(targets, predictions, x) for x in self.k])\n","        result = np.concatenate((precision, recall), axis=0)\n","        return result\n","\n","    def _compute(self, targets, predictions, k):\n","        predictions = predictions[:k]\n","        num_hit = len(set(predictions).intersection(set(targets)))\n","\n","        return float(num_hit) / len(predictions), float(num_hit) / len(targets)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"vb-CoYr4Y041"},"source":["class MeanAP(BaseMetric):\n","    def __init__(self, rel_threshold=0, k=np.inf):\n","        super(MeanAP, self).__init__(rel_threshold, k)\n","\n","    def __call__(self, targets, predictions):\n","        result = [self._compute(targets, predictions, x) for x in self.k]\n","        return np.array(result)\n","\n","    def __str__(self):\n","        return ','.join([('MeanAP@%1.f' % x) for x in self.k])\n","\n","    def _compute(self, targets, predictions, k):\n","        if len(predictions) > k:\n","            predictions = predictions[:k]\n","\n","        score = 0.0\n","        num_hits = 0.0\n","\n","        for i, p in enumerate(predictions):\n","            if p in targets and p not in predictions[:i]:\n","                num_hits += 1.0\n","                score += num_hits / (i + 1.0)\n","\n","        if not list(targets):\n","            return 0.0\n","\n","        return score / min(len(targets), k)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3ik4wXqKY2bD"},"source":["class NormalizedDCG(BaseMetric):\n","    def __init__(self, rel_threshold=0, k=10):\n","        super(NormalizedDCG, self).__init__(rel_threshold, k)\n","\n","    def __call__(self, targets, predictions):\n","        result = [self._compute(targets, predictions, x) for x in self.k]\n","        return np.array(result)\n","\n","    def __str__(self):\n","        return ','.join([('NDCG@%1.f' % x) for x in self.k])\n","\n","    def _compute(self, targets, predictions, k):\n","        k = min(len(targets), k)\n","\n","        if len(predictions) > k:\n","            predictions = predictions[:k]\n","\n","        # compute idcg\n","        idcg = np.sum(1 / np.log2(np.arange(2, k + 2)))\n","        dcg = 0.0\n","        for i, p in enumerate(predictions):\n","            if p in targets:\n","                dcg += 1 / np.log2(i + 2)\n","        ndcg = dcg / idcg\n","\n","        return ndcg\n","\n","\n","all_metrics = [PrecisionRecall(k=[1, 5, 10]), NormalizedDCG(k=[5, 10, 20])]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8X-dbHHdY4Ke"},"source":["class Evaluator(object):\n","    \"\"\"Evaluator for both one-stage and two-stage evaluations.\"\"\"\n","\n","    def __init__(self, u, a, simulator, syn):\n","        self.u = u\n","        self.a = a\n","        self.simulator = simulator\n","        self.syn = syn\n","\n","        self.target_rankings = self.get_target_rankings()\n","        self.metrics = all_metrics\n","\n","    def get_target_rankings(self):\n","        target_rankings = []\n","        with torch.no_grad():\n","            self.simulator.eval()\n","            for i in range(NUM_USERS):\n","                impression_ids = range(i * NUM_ITEMS, (i + 1) * NUM_ITEMS)\n","                feats, _ = self.syn[impression_ids]\n","                feats[\"impression_feats\"][\"real_feats\"] = torch.mean(\n","                    feats[\"impression_feats\"][\"real_feats\"],\n","                    dim=0,\n","                    keepdim=True).repeat([NUM_ITEMS, 1])\n","                feats = self.syn.to_device(feats)\n","                outputs = torch.sigmoid(self.simulator(**feats))\n","                user_target_ranking = (outputs >\n","                                       self.syn.cut).nonzero().view(-1)\n","                target_rankings.append(user_target_ranking.cpu().numpy())\n","        return target_rankings\n","\n","    def one_stage_ranking_eval(self, logits, user_list):\n","        for i, user in enumerate(user_list):\n","            user_rated_items = self.a[self.u == user]\n","            logits[i, user_rated_items] = -np.inf\n","        sort_idx = torch.argsort(logits, dim=1, descending=True).cpu().numpy()\n","        # Init evaluation results.\n","        total_metrics_len = 0\n","        for metric in self.metrics:\n","            total_metrics_len += len(metric)\n","\n","        total_val_metrics = np.zeros(\n","            [len(user_list), total_metrics_len], dtype=np.float32)\n","        valid_rows = []\n","        for i, user in enumerate(user_list):\n","            pred_ranking = sort_idx[i].tolist()\n","            target_ranking = self.target_rankings[user]\n","            if len(target_ranking) <= 0:\n","                continue\n","            metric_results = list()\n","            for j, metric in enumerate(self.metrics):\n","                result = metric(\n","                    targets=target_ranking, predictions=pred_ranking)\n","                metric_results.append(result)\n","            total_val_metrics[i, :] = np.concatenate(metric_results)\n","            valid_rows.append(i)\n","        # Average evaluation results by user.\n","        total_val_metrics = total_val_metrics[valid_rows]\n","        avg_val_metrics = (total_val_metrics.mean(axis=0)).tolist()\n","        # Summary evaluation results into a dict.\n","        ind, result = 0, OrderedDict()\n","        for metric in self.metrics:\n","            values = avg_val_metrics[ind:ind + len(metric)]\n","            if len(values) <= 1:\n","                result[str(metric)] = values\n","            else:\n","                for name, value in zip(str(metric).split(','), values):\n","                    result[name] = value\n","            ind += len(metric)\n","        return result\n","\n","    def two_stage_ranking_eval(self, logits, ranker, user_list, k=30):\n","        sort_idx = torch.argsort(logits, dim=1, descending=True).cpu().numpy()\n","        topk_item_ids = []\n","        for i, user in enumerate(user_list):\n","            topk_item_ids.append([])\n","            for j in sort_idx[i]:\n","                if j not in self.a[self.u == user]:\n","                    topk_item_ids[-1].append(j)\n","                if len(topk_item_ids[-1]) == k:\n","                    break\n","        time_feats = self.syn.to_device(\n","            torch.mean(\n","                self.syn.impression_feats[\"real_feats\"].view(\n","                    NUM_USERS, NUM_ITEMS),\n","                dim=1).view(-1, 1))\n","        # Init evaluation results.\n","        total_metrics_len = 0\n","        for metric in self.metrics:\n","            total_metrics_len += len(metric)\n","\n","        total_val_metrics = np.zeros(\n","            [len(user_list), total_metrics_len], dtype=np.float32)\n","        valid_rows = []\n","        for i, user in enumerate(user_list):\n","            user_feats = {\n","                key: value[user].view(1, -1)\n","                for key, value in self.syn.user_feats.items()\n","            }\n","            item_feats = {\n","                key: value[topk_item_ids[i]]\n","                for key, value in self.syn.item_feats.items()\n","            }\n","\n","            user_feats = self.syn.to_device(user_feats)\n","            item_feats = self.syn.to_device(item_feats)\n","            impression_feats = {\"real_feats\": time_feats[user].view(1, -1)}\n","            ranker_logits = ranker(user_feats, item_feats,\n","                                   impression_feats).view(1, -1)\n","            _, pred = ranker_logits.topk(k=k)\n","            pred = pred[0].cpu().numpy()\n","            pred_ranking = sort_idx[i][pred].tolist()\n","            target_ranking = self.target_rankings[user]\n","            if len(target_ranking) <= 0:\n","                continue\n","            metric_results = list()\n","            for j, metric in enumerate(self.metrics):\n","                result = metric(\n","                    targets=target_ranking, predictions=pred_ranking)\n","                metric_results.append(result)\n","            total_val_metrics[i, :] = np.concatenate(metric_results)\n","            valid_rows.append(i)\n","            # Average evaluation results by user.\n","        total_val_metrics = total_val_metrics[valid_rows]\n","        avg_val_metrics = (total_val_metrics.mean(axis=0)).tolist()\n","        # Summary evaluation results into a dict.\n","        ind, result = 0, OrderedDict()\n","        for metric in self.metrics:\n","            values = avg_val_metrics[ind:ind + len(metric)]\n","            if len(values) <= 1:\n","                result[str(metric)] = values\n","            else:\n","                for name, value in zip(str(metric).split(','), values):\n","                    result[name] = value\n","            ind += len(metric)\n","        return result\n","\n","    def one_stage_eval(self, logits):\n","        sort_idx = torch.argsort(logits, dim=1, descending=True).cpu().numpy()\n","        impression_ids = []\n","        for i in range(NUM_USERS):\n","            for j in sort_idx[i]:\n","                if j not in self.a[self.u == i]:\n","                    break\n","            impression_ids.append(i * NUM_ITEMS + j)\n","        feats, labels = self.syn[impression_ids]\n","        feats[\"impression_feats\"][\"real_feats\"] = torch.mean(\n","            self.syn.impression_feats[\"real_feats\"].view(NUM_USERS, NUM_ITEMS),\n","            dim=1).view(-1, 1)\n","        with torch.no_grad():\n","            self.simulator.eval()\n","            feats = self.syn.to_device(feats)\n","            outputs = torch.sigmoid(self.simulator(**feats))\n","            return torch.mean(\n","                (outputs > self.syn.cut).to(dtype=torch.float32)).item()\n","\n","    def two_stage_eval(self, logits, ranker, k=30):\n","        sort_idx = torch.argsort(logits, dim=1, descending=True).cpu().numpy()\n","        topk_item_ids = []\n","        for i in range(NUM_USERS):\n","            topk_item_ids.append([])\n","            for j in sort_idx[i]:\n","                if j not in self.a[self.u == i]:\n","                    topk_item_ids[-1].append(j)\n","                if len(topk_item_ids[-1]) == k:\n","                    break\n","        time_feats = self.syn.to_device(\n","            torch.mean(\n","                self.syn.impression_feats[\"real_feats\"].view(\n","                    NUM_USERS, NUM_ITEMS),\n","                dim=1).view(-1, 1))\n","        recommneded = []\n","        for i in range(NUM_USERS):\n","            user_feats = {\n","                key: value[i].view(1, -1)\n","                for key, value in self.syn.user_feats.items()\n","            }\n","            item_feats = {\n","                key: value[topk_item_ids[i]]\n","                for key, value in self.syn.item_feats.items()\n","            }\n","\n","            user_feats = self.syn.to_device(user_feats)\n","            item_feats = self.syn.to_device(item_feats)\n","            impression_feats = {\"real_feats\": time_feats[i].view(1, -1)}\n","            ranker_logits = ranker(user_feats, item_feats,\n","                                   impression_feats).view(1, -1)\n","            _, pred = torch.max(ranker_logits, 1)\n","            pred = pred.squeeze().item()\n","            recommneded.append(topk_item_ids[i][pred])\n","\n","        impression_ids = []\n","        for i in range(NUM_USERS):\n","            impression_ids.append(i * NUM_ITEMS + recommneded[i])\n","        feats, labels = self.syn[impression_ids]\n","        feats[\"impression_feats\"][\"real_feats\"] = torch.mean(\n","            self.syn.impression_feats[\"real_feats\"].view(NUM_USERS, NUM_ITEMS),\n","            dim=1).view(-1, 1)\n","        with torch.no_grad():\n","            self.simulator.eval()\n","            feats = self.syn.to_device(feats)\n","            outputs = torch.sigmoid(self.simulator(**feats))\n","            return torch.mean(\n","                (outputs > self.syn.cut).to(dtype=torch.float32)).item()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5eFQHVu4Yjl3"},"source":["## Losses"]},{"cell_type":"code","metadata":{"id":"jxr1uWzxZDxK"},"source":["def batch_select(mat, idx):\n","    mask = torch.arange(mat.size(1)).expand_as(mat).to(\n","        mat.device, dtype=torch.long)\n","    mask = (mask == idx.view(-1, 1))\n","    return torch.masked_select(mat, mask)\n","\n","\n","def unique_and_padding(mat, padding_idx, dim=-1):\n","    \"\"\"Conducts unique operation along dim and pads to the same length.\"\"\"\n","    samples, _ = torch.sort(mat, dim=dim)\n","    samples_roll = torch.roll(samples, -1, dims=dim)\n","    samples_diff = samples - samples_roll\n","    samples_diff[:,\n","                 -1] = 1  # deal with the edge case that there is only one unique sample in a row\n","    samples_mask = torch.bitwise_not(samples_diff == 0)  # unique mask\n","    samples *= samples_mask.to(dtype=samples.dtype)\n","    samples += (1 - samples_mask.to(dtype=samples.dtype)) * padding_idx\n","    samples, _ = torch.sort(samples, dim=dim)\n","    # shrink size to max unique length\n","    samples = torch.unique(samples, dim=dim)\n","    return samples"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"e_YSbiXxZIA3"},"source":["def loss_ce(logits, a, unused_p=None, unused_ranker_logits=None):\n","    \"\"\"Cross entropy.\"\"\"\n","    return -torch.mean(batch_select(F.log_softmax(logits, dim=1), a))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"smu-8Qr5ZGsJ"},"source":["def loss_ips(logits,\n","             a,\n","             p,\n","             unused_ranker_logits=None,\n","             upper_limit=100,\n","             lower_limit=0.01):\n","    \"\"\"IPS loss (one-stage).\"\"\"\n","    importance_weight = batch_select(F.softmax(logits.detach(), dim=1), a) / p\n","    importance_weight = torch.where(\n","        importance_weight > lower_limit, importance_weight,\n","        lower_limit * torch.ones_like(importance_weight))\n","    importance_weight = torch.where(\n","        importance_weight < upper_limit, importance_weight,\n","        upper_limit * torch.ones_like(importance_weight))\n","    importance_weight /= torch.mean(importance_weight)\n","    return -torch.mean(\n","        batch_select(F.log_softmax(logits, dim=1), a) * importance_weight)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"cabVpWwPZFJa"},"source":["def loss_2s(logits,\n","            a,\n","            p,\n","            ranker_logits,\n","            slate_sample_size=100,\n","            slate_size=30,\n","            temperature=np.e,\n","            alpha=1e-5,\n","            upper_limit=100,\n","            lower_limit=0.01):\n","    \"\"\"Two stage loss.\"\"\"\n","    num_logits = logits.size(1)\n","    rls = ranker_logits.detach()\n","    probs = F.softmax(logits.detach() / temperature, dim=1)\n","    log_probs = F.log_softmax(logits, dim=1)\n","\n","    rls = torch.cat(\n","        [\n","            rls,\n","            torch.Tensor([float(\"-inf\")]).to(rls.device).view(1, 1).expand(\n","                rls.size(0), 1)\n","        ],\n","        dim=1)\n","    log_probs = torch.cat(\n","        [log_probs,\n","         torch.zeros(log_probs.size(0), 1).to(log_probs.device)],\n","        dim=1)\n","\n","    importance_weight = batch_select(F.softmax(logits.detach(), dim=1), a) / p\n","    importance_weight = torch.where(\n","        importance_weight > lower_limit, importance_weight,\n","        lower_limit * torch.ones_like(importance_weight))\n","    importance_weight = torch.where(\n","        importance_weight < upper_limit, importance_weight,\n","        upper_limit * torch.ones_like(importance_weight))\n","    importance_weight /= torch.mean(importance_weight)\n","\n","    log_action_res = []\n","    sampled_slate_res = []\n","    for i in range(probs.size(0)):\n","        samples = torch.multinomial(\n","            probs[i], slate_sample_size * slate_size, replacement=True).view(\n","                slate_sample_size, slate_size)\n","        samples = torch.cat(\n","            [samples, a[i].view(1, 1).expand(samples.size(0), 1)], dim=1)\n","        samples = unique_and_padding(samples, num_logits)\n","        rp = F.softmax(\n","            F.embedding(samples, rls[i].view(-1, 1)).squeeze(-1), dim=1)\n","        lp = torch.sum(\n","            F.embedding(samples, log_probs[i].view(-1, 1)).squeeze(-1),\n","            dim=1) - log_probs[i, a[i]]\n","        sampled_slate_res.append(\n","            torch.mean(importance_weight[i] * rp[samples == a[i]] * lp))\n","        log_action_res.append(\n","            torch.mean(importance_weight[i] * rp[samples == a[i]]))\n","    loss = -torch.mean(\n","        torch.stack(log_action_res) * batch_select(log_probs, a))\n","    loss += -torch.mean(torch.stack(sampled_slate_res)) * alpha\n","    return loss"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"oXehpz2RS9_m"},"source":["## Main"]},{"cell_type":"code","metadata":{"id":"zMBV6BttW-fg"},"source":["!pip install -q torchnet"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QLae9ZGJS99b"},"source":["import argparse\n","import os\n","\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from six.moves import cPickle as pickle\n","from torch.utils.data import DataLoader, Subset\n","from torchnet.meter import AUCMeter"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ALobTMq4T11c"},"source":["parser = argparse.ArgumentParser()\n","parser.add_argument(\"--verbose\", type=int, default=1, help=\"Verbose.\")\n","parser.add_argument(\"--seed\", type=int, default=0, help=\"Random seed.\")\n","parser.add_argument(\"--loss_type\", default=\"loss_ce\")\n","parser.add_argument(\"--device\", default=\"cuda:0\")\n","parser.add_argument(\"--alpha\", type=float, default=1e-3, help=\"Loss ratio.\")\n","parser.add_argument(\"--lr\", type=float, default=0.05, help=\"Learning rate.\")\n","args = parser.parse_args(args={})\n","\n","torch.manual_seed(0)\n","torch.cuda.manual_seed(0)\n","\n","filepath = \"./ml-1m\"\n","device = args.device\n","\n","dataset = MovieLensDataset(filepath, device=device)\n","\n","NUM_ITEMS = dataset.NUM_ITEMS\n","NUM_YEARS = dataset.NUM_YEARS\n","NUM_GENRES = dataset.NUM_GENRES\n","\n","NUM_USERS = dataset.NUM_USERS\n","NUM_OCCUPS = dataset.NUM_OCCUPS\n","NUM_AGES = dataset.NUM_AGES\n","NUM_ZIPS = dataset.NUM_ZIPS"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nDDAOXjTTxzR","executionInfo":{"status":"ok","timestamp":1633455089789,"user_tz":-330,"elapsed":1787668,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}},"outputId":"0350d0a6-9c9a-4542-87d6-6eda111736e5"},"source":["simulator_path = os.path.join(filepath, \"simulator.pt\")\n","if os.path.exists(simulator_path):\n","    simulator = ImpressionSimulator(use_impression_feats=True)\n","    simulator.load_state_dict(torch.load(simulator_path))\n","    simulator.to(device)\n","    simulator.eval()\n","else:\n","    # train a simulator model on the original ML-1M dataset\n","    # the simulator will be used to generate synthetic labels later\n","\n","    torch.manual_seed(0)\n","    torch.cuda.manual_seed(0)\n","\n","    num_samples = len(dataset)\n","    train_loader = DataLoader(\n","        Subset(dataset, list(range(num_samples * 3 // 5))),\n","        batch_size=128,\n","        shuffle=True)\n","    val_loader = DataLoader(\n","        Subset(dataset,\n","               list(range(num_samples * 3 // 5, num_samples * 4 // 5))),\n","        batch_size=128)\n","    test_loader = DataLoader(\n","        Subset(dataset, list(range(num_samples * 4 // 5, num_samples))),\n","        batch_size=128)\n","\n","    simulator = ImpressionSimulator(use_impression_feats=True).to(device)\n","    opt = torch.optim.Adagrad(\n","        simulator.parameters(), lr=0.05, weight_decay=1e-4)\n","    criterion = nn.BCEWithLogitsLoss()\n","\n","    simulator.train()\n","    for epoch in range(4):\n","        print(\"---epoch {}---\".format(epoch))\n","        for step, batch in enumerate(train_loader):\n","            feats, labels = batch\n","            logits = simulator(**feats)\n","            loss = criterion(logits, labels)\n","\n","            opt.zero_grad()\n","            loss.backward()\n","            opt.step()\n","\n","            if (step + 1) % 500 == 0:\n","                with torch.no_grad():\n","                    simulator.eval()\n","                    auc = AUCMeter()\n","                    for feats, labels in val_loader:\n","                        outputs = torch.sigmoid(simulator(**feats))\n","                        auc.add(outputs, labels)\n","                    print(step, auc.value()[0])\n","                    if auc.value()[0] > 0.735:\n","                        break\n","                simulator.train()\n","\n","    simulator.to(\"cpu\")\n","    torch.save(simulator.state_dict(), simulator_path)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["---epoch 0---\n","499 0.6932387523620361\n","999 0.7135402368767171\n","1499 0.7215369956250887\n","1999 0.7251487885185414\n","2499 0.7250320801426405\n","2999 0.7272670074151073\n","3499 0.7281266442311184\n","3999 0.7294850286875231\n","4499 0.7296389614963857\n","---epoch 1---\n","499 0.7283547770191316\n","999 0.7295152100721607\n","1499 0.7300909216769196\n","1999 0.7302949709078976\n","2499 0.7307115513456678\n","2999 0.7303182560548731\n","3499 0.729832712732132\n","3999 0.7313203021611654\n","4499 0.7318606932964403\n","---epoch 2---\n","499 0.7313658487455917\n","999 0.731270933721071\n","1499 0.7307394064320014\n","1999 0.7323340285603053\n","2499 0.7322074757365146\n","2999 0.7321213073333425\n","3499 0.7330514084885155\n","3999 0.7328071902744475\n","4499 0.7332603719115534\n","---epoch 3---\n","499 0.7324623727995114\n","999 0.7323136933519381\n","1499 0.7327187807719571\n","1999 0.7329365712368654\n","2499 0.7318917432430467\n","2999 0.733582752772605\n","3499 0.7342934273983486\n","3999 0.7341365564445322\n","4499 0.7340031962421637\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nvYcuBtBTwAT","executionInfo":{"status":"ok","timestamp":1633455306838,"user_tz":-330,"elapsed":217093,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}},"outputId":"7786e902-1ae8-4375-870c-b3b43a34ba16"},"source":["# create a torch dataset class that adopt the simulator and generate the synthetic dataset\n","synthetic_data_path = os.path.join(filepath, \"full_impression_feats.pt\")\n","syn = SyntheticMovieLensDataset(\n","    filepath, simulator_path, synthetic_data_path, device=device)\n","\n","logging_policy_path = os.path.join(filepath, \"logging_policy.pt\")\n","if os.path.exists(logging_policy_path):\n","    logging_policy = Nominator()\n","    logging_policy.load_state_dict(torch.load(logging_policy_path))\n","    logging_policy.to(device)\n","    logging_policy.eval()\n","else:\n","    # train a logging policy using the synthetic dataset\n","    num_samples = len(syn)\n","    idx_list = list(range(num_samples))\n","    rs = np.random.RandomState(0)\n","    rs.shuffle(idx_list)\n","    train_idx = idx_list[:10000]\n","    val_idx = idx_list[10000:20000]\n","    test_idx = idx_list[-100000:]\n","    train_loader = DataLoader(\n","        Subset(syn, train_idx), batch_size=128, shuffle=True)\n","    val_loader = DataLoader(Subset(syn, val_idx), batch_size=128)\n","    test_loader = DataLoader(Subset(syn, test_idx), batch_size=128)\n","\n","    logging_policy = Nominator().to(device)\n","    opt = torch.optim.Adagrad(\n","        logging_policy.parameters(), lr=0.05, weight_decay=1e-4)\n","    criterion = nn.BCEWithLogitsLoss()\n","\n","    logging_policy.train()\n","    for epoch in range(40):\n","        print(\"---epoch {}---\".format(epoch))\n","        for step, batch in enumerate(train_loader):\n","            feats, labels = batch\n","            feats = syn.to_device(feats)\n","            labels = syn.to_device(labels)\n","            logits = logging_policy(feats[\"user_feats\"], feats[\"item_feats\"])\n","            loss = criterion(logits, labels)\n","\n","            opt.zero_grad()\n","            loss.backward()\n","            opt.step()\n","\n","        with torch.no_grad():\n","            logging_policy.eval()\n","            auc = AUCMeter()\n","            for feats, labels in val_loader:\n","                feats = syn.to_device(feats)\n","                labels = syn.to_device(labels)\n","                outputs = torch.sigmoid(\n","                    logging_policy(feats[\"user_feats\"], feats[\"item_feats\"]))\n","                auc.add(outputs, labels)\n","            print(step, auc.value()[0])\n","\n","            logging_policy.train()\n","\n","    logging_policy.eval()\n","\n","    logging_policy.to(\"cpu\")\n","    torch.save(logging_policy.state_dict(), logging_policy_path)\n","    logging_policy.to(device)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["generating impression_feats\n"]},{"output_type":"stream","name":"stderr","text":["100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 46906/46906 [01:04<00:00, 726.33it/s]\n"]},{"output_type":"stream","name":"stdout","text":["saved impression_feats\n","---epoch 0---\n","78 0.7303382306338263\n","---epoch 1---\n","78 0.7581630234600971\n","---epoch 2---\n","78 0.7758875382286556\n","---epoch 3---\n","78 0.7947954816858157\n","---epoch 4---\n","78 0.8092959307857327\n","---epoch 5---\n","78 0.8209026242576345\n","---epoch 6---\n","78 0.8281561300331621\n","---epoch 7---\n","78 0.831641647559472\n","---epoch 8---\n","78 0.8359850391331326\n","---epoch 9---\n","78 0.838561799838773\n","---epoch 10---\n","78 0.8401704563395371\n","---epoch 11---\n","78 0.8411352317648504\n","---epoch 12---\n","78 0.8419039896279034\n","---epoch 13---\n","78 0.8420276300116679\n","---epoch 14---\n","78 0.8417600912427988\n","---epoch 15---\n","78 0.8424424910531916\n","---epoch 16---\n","78 0.8427186846489241\n","---epoch 17---\n","78 0.8426326119202265\n","---epoch 18---\n","78 0.8428287245904745\n","---epoch 19---\n","78 0.842670274683281\n","---epoch 20---\n","78 0.8428754226123425\n","---epoch 21---\n","78 0.8429700550599161\n","---epoch 22---\n","78 0.8431265076993719\n","---epoch 23---\n","78 0.8433620901844372\n","---epoch 24---\n","78 0.8435445073044836\n","---epoch 25---\n","78 0.8435860694950261\n","---epoch 26---\n","78 0.8435829309314381\n","---epoch 27---\n","78 0.843662155885035\n","---epoch 28---\n","78 0.8437115169305534\n","---epoch 29---\n","78 0.8438241247877666\n","---epoch 30---\n","78 0.843904205713251\n","---epoch 31---\n","78 0.843980862751185\n","---epoch 32---\n","78 0.8440994624116114\n","---epoch 33---\n","78 0.8442256707110387\n","---epoch 34---\n","78 0.8444167426579487\n","---epoch 35---\n","78 0.8444864568127943\n","---epoch 36---\n","78 0.8446461431238256\n","---epoch 37---\n","78 0.8447101507994208\n","---epoch 38---\n","78 0.8447210882179845\n","---epoch 39---\n","78 0.8447308843406981\n"]}]},{"cell_type":"code","metadata":{"id":"dRwfP8RXTuLQ"},"source":["def generate_bandit_samples(logging_policy, syn, k=5):\n","    \"\"\"Generates partial-labeled bandit samples with the logging policy.\n","    Arguments:\n","        k: The number of items to be sampled for each user.\n","    \"\"\"\n","    logging_policy.set_binary(False)\n","    with torch.no_grad():\n","        feats = {}\n","        feats[\"user_feats\"] = syn.user_feats\n","        feats[\"item_feats\"] = syn.item_feats\n","        feats = syn.to_device(feats)\n","        probs = F.softmax(logging_policy(**feats), dim=1)\n","\n","    sampled_users = []\n","    sampled_actions = []\n","    sampled_probs = []\n","    sampled_rewards = []\n","    for i in range(probs.size(0)):\n","        sampled_users.append([i] * k)\n","        sampled_actions.append(\n","            torch.multinomial(probs[i], k).cpu().numpy().tolist())\n","        sampled_probs.append(\n","            probs[i, sampled_actions[-1]].cpu().numpy().tolist())\n","        sampled_rewards.append(syn.impression_feats[\"labels\"][[\n","            i * probs.size(1) + j for j in sampled_actions[-1]\n","        ]].numpy().tolist())\n","    return np.array(sampled_users).reshape(-1), np.array(\n","        sampled_actions).reshape(-1), np.array(sampled_probs).reshape(\n","            -1), np.array(sampled_rewards).reshape(-1)\n","\n","\n","torch.manual_seed(0)\n","torch.cuda.manual_seed(0)\n","\n","u, a, p, r = generate_bandit_samples(\n","    logging_policy, syn,\n","    k=5)  # u: user, a: item, p: logging policy probability, r: reward/label\n","\n","simulator = simulator.to(device)\n","ev = Evaluator(u[r > 0], a[r > 0], simulator, syn)\n","\n","all_user_feats = syn.to_device(syn.user_feats)\n","all_item_feats = syn.to_device(syn.item_feats)\n","all_impression_feats = syn.to_device({\n","    \"real_feats\":\n","    torch.mean(\n","        syn.impression_feats[\"real_feats\"].view(NUM_USERS, NUM_ITEMS),\n","        dim=1).view(-1, 1)\n","})"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"B-uzlJDoTsGo","executionInfo":{"status":"ok","timestamp":1633456341552,"user_tz":-330,"elapsed":28192,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}},"outputId":"be5db6fb-58d9-4b29-ec10-7f277d3f46e1"},"source":["# Split validation/test users.\n","num_val_users = 2000\n","val_user_list = list(range(0, num_val_users))\n","test_user_list = list(range(num_val_users, NUM_USERS))\n","\n","test_item_feats = all_item_feats\n","test_user_feats = syn.to_device(\n","    {key: value[test_user_list]\n","     for key, value in all_user_feats.items()})\n","test_impression_feats = syn.to_device({\n","    key: value[test_user_list]\n","    for key, value in all_impression_feats.items()\n","})\n","\n","val_item_feats = all_item_feats\n","val_user_feats = syn.to_device(\n","    {key: value[val_user_list]\n","     for key, value in all_user_feats.items()})\n","val_impression_feats = syn.to_device(\n","    {key: value[val_user_list]\n","     for key, value in all_impression_feats.items()})\n","\n","ranker_path = os.path.join(filepath, \"ranker.pt\")\n","\n","if os.path.exists(ranker_path):\n","    ranker = Ranker()\n","    ranker.load_state_dict(torch.load(ranker_path))\n","    ranker.to(device)\n","    ranker.eval()\n","    ranker.set_binary(False)\n","else:\n","    # train the ranker with binary cross-entropy\n","    torch.manual_seed(0)\n","    torch.cuda.manual_seed(0)\n","\n","    batch_size = 128\n","    neg_sample_size = 29\n","\n","    ranker = Ranker().to(device)\n","    opt = torch.optim.Adagrad(ranker.parameters(), lr=0.05, weight_decay=1e-4)\n","    criterion = nn.BCEWithLogitsLoss()\n","\n","    rs = np.random.RandomState(0)\n","    ranker.train()\n","    for epoch in range(10):\n","        print(\"---epoch {}---\".format(epoch))\n","        for step in range(len(u) // batch_size):\n","            user_list = u[step * batch_size:(step + 1) * batch_size]\n","            item_list = a[step * batch_size:(step + 1) * batch_size]\n","            user_feats = syn.to_device({\n","                key: value[user_list]\n","                for key, value in syn.user_feats.items()\n","            })\n","            item_feats = syn.to_device({\n","                key: value[item_list]\n","                for key, value in syn.item_feats.items()\n","            })\n","            impression_list = [\n","                user_id * NUM_ITEMS + item_id\n","                for user_id, item_id in zip(user_list, item_list)\n","            ]\n","            impression_feats = syn.to_device({\n","                \"real_feats\":\n","                syn.impression_feats[\"real_feats\"][impression_list]\n","            })\n","\n","            labels = torch.FloatTensor(\n","                r[step * batch_size:(step + 1) * batch_size]).to(device)\n","\n","            logits = ranker(user_feats, item_feats, impression_feats)\n","            loss = criterion(logits, labels)\n","\n","            opt.zero_grad()\n","            loss.backward()\n","            opt.step()\n","\n","        with torch.no_grad():\n","            ranker.eval()\n","            ranker.set_binary(False)\n","            logits = ranker(all_user_feats, all_item_feats,\n","                            all_impression_feats)\n","\n","            print(step, ev.one_stage_eval(logits))\n","\n","            # Evaluate ranking metrics on validation users.\n","            logits = ranker(val_user_feats, val_item_feats,\n","                            val_impression_feats)\n","            print(step, ev.one_stage_ranking_eval(logits, val_user_list))\n","\n","            ranker.train()\n","            ranker.set_binary(True)\n","\n","    ranker.eval()\n","    ranker.set_binary(False)\n","\n","    ranker.to(\"cpu\")\n","    torch.save(ranker.state_dict(), ranker_path)\n","    ranker.to(device)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["---epoch 0---\n","234 0.49751657247543335\n","234 OrderedDict([('Precision@1', 0.525853157043457), ('Precision@5', 0.37435382604599), ('Precision@10', 0.3275078535079956), ('Recall@1', 0.001950422883965075), ('Recall@5', 0.006817308254539967), ('Recall@10', 0.011337128467857838), ('NDCG@5', 0.40602830052375793), ('NDCG@10', 0.36229410767555237), ('NDCG@20', 0.3150430917739868)])\n","---epoch 1---\n","234 0.5798013210296631\n","234 OrderedDict([('Precision@1', 0.6003102660179138), ('Precision@5', 0.4798329174518585), ('Precision@10', 0.42228519916534424), ('Recall@1', 0.002812947379425168), ('Recall@5', 0.011755581945180893), ('Recall@10', 0.02120870165526867), ('NDCG@5', 0.5004847645759583), ('NDCG@10', 0.45322883129119873), ('NDCG@20', 0.39284855127334595)])\n","---epoch 2---\n","234 0.5889073014259338\n","234 OrderedDict([('Precision@1', 0.6106514930725098), ('Precision@5', 0.5765244960784912), ('Precision@10', 0.5148389339447021), ('Recall@1', 0.003235821146517992), ('Recall@5', 0.016686489805579185), ('Recall@10', 0.03442412614822388), ('NDCG@5', 0.5764947533607483), ('NDCG@10', 0.5345680713653564), ('NDCG@20', 0.4556241035461426)])\n","---epoch 3---\n","234 0.628311276435852\n","234 OrderedDict([('Precision@1', 0.6411582231521606), ('Precision@5', 0.6560497283935547), ('Precision@10', 0.6184099912643433), ('Recall@1', 0.0033987725619226694), ('Recall@5', 0.0258512943983078), ('Recall@10', 0.045876603573560715), ('NDCG@5', 0.6525105237960815), ('NDCG@10', 0.6282448172569275), ('NDCG@20', 0.5369541049003601)])\n","---epoch 4---\n","234 0.7625827789306641\n","234 OrderedDict([('Precision@1', 0.7487073540687561), ('Precision@5', 0.750673770904541), ('Precision@10', 0.6952475309371948), ('Recall@1', 0.004413578659296036), ('Recall@5', 0.033956073224544525), ('Recall@10', 0.05853657424449921), ('NDCG@5', 0.7535479068756104), ('NDCG@10', 0.715246319770813), ('NDCG@20', 0.6256369352340698)])\n","---epoch 5---\n","234 0.8622516989707947\n","234 OrderedDict([('Precision@1', 0.8490175604820251), ('Precision@5', 0.8347484469413757), ('Precision@10', 0.7592080235481262), ('Recall@1', 0.009471286088228226), ('Recall@5', 0.045272961258888245), ('Recall@10', 0.07097340375185013), ('NDCG@5', 0.840373158454895), ('NDCG@10', 0.7897650003433228), ('NDCG@20', 0.717039942741394)])\n","---epoch 6---\n","234 0.9102649092674255\n","234 OrderedDict([('Precision@1', 0.9089968800544739), ('Precision@5', 0.8815937042236328), ('Precision@10', 0.8184615969657898), ('Recall@1', 0.015791630372405052), ('Recall@5', 0.053664132952690125), ('Recall@10', 0.07895129173994064), ('NDCG@5', 0.8917895555496216), ('NDCG@10', 0.8513309359550476), ('NDCG@20', 0.7841229438781738)])\n","---epoch 7---\n","234 0.9369205236434937\n","234 OrderedDict([('Precision@1', 0.9441571831703186), ('Precision@5', 0.9020691514015198), ('Precision@10', 0.8512938618659973), ('Recall@1', 0.01766125112771988), ('Recall@5', 0.05773942917585373), ('Recall@10', 0.08265078067779541), ('NDCG@5', 0.9147428274154663), ('NDCG@10', 0.8829164505004883), ('NDCG@20', 0.8230650424957275)])\n","---epoch 8---\n","234 0.9519867897033691\n","234 OrderedDict([('Precision@1', 0.9565666913986206), ('Precision@5', 0.9139613509178162), ('Precision@10', 0.8667538166046143), ('Recall@1', 0.01800968125462532), ('Recall@5', 0.060318805277347565), ('Recall@10', 0.08457545191049576), ('NDCG@5', 0.9267275929450989), ('NDCG@10', 0.8979306817054749), ('NDCG@20', 0.844747006893158)])\n","---epoch 9---\n","234 0.9541391134262085\n","234 OrderedDict([('Precision@1', 0.9601861238479614), ('Precision@5', 0.9185118079185486), ('Precision@10', 0.8744063973426819), ('Recall@1', 0.018171625211834908), ('Recall@5', 0.061439476907253265), ('Recall@10', 0.08633137494325638), ('NDCG@5', 0.9311888217926025), ('NDCG@10', 0.9044811129570007), ('NDCG@20', 0.8586583733558655)])\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GcdIHsUETncX","executionInfo":{"status":"ok","timestamp":1633456341555,"user_tz":-330,"elapsed":33,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}},"outputId":"ca461679-dff9-4f3a-e936-ae4aa5d74b3f"},"source":["u = u[r > 0]\n","a = a[r > 0]\n","p = p[r > 0]\n","\n","batch_size = 128\n","check_metric = \"Precision@10\"\n","\n","torch.manual_seed(args.seed)\n","torch.cuda.manual_seed(args.seed)\n","\n","nominator = Nominator().to(device)\n","nominator.set_binary(False)\n","\n","opt = torch.optim.Adagrad(\n","    nominator.parameters(), lr=args.lr, weight_decay=1e-4)\n","\n","rs = np.random.RandomState(0)\n","nominator.train()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Nominator(\n","  (item_rep): ItemRep(\n","    (item_embedding): Embedding(3884, 10, padding_idx=0)\n","    (year_embedding): Embedding(81, 5)\n","    (genre_linear): Linear(in_features=18, out_features=5, bias=True)\n","  )\n","  (user_rep): UserRep(\n","    (user_embedding): Embedding(6041, 10, padding_idx=0)\n","    (gender_embedding): Embedding(2, 5)\n","    (age_embedding): Embedding(7, 5)\n","    (occup_embedding): Embedding(21, 5)\n","    (zip_embedding): Embedding(3439, 5)\n","  )\n","  (linear): Linear(in_features=30, out_features=20, bias=True)\n",")"]},"metadata":{},"execution_count":43}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"O--w0zZqTlPk","executionInfo":{"status":"ok","timestamp":1633456966028,"user_tz":-330,"elapsed":622780,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}},"outputId":"93cd4783-0faf-400e-da0d-0220295fad36"},"source":["# Init results.\n","best_epoch = 0\n","best_result = 0.0\n","val_results, test_results = [], []\n","\n","if args.loss_type == \"loss_2s\":\n","    with torch.no_grad():\n","        ranker.eval()\n","        ranker.set_binary(False)\n","        ranker_logits = ranker(all_user_feats, all_item_feats,\n","                               all_impression_feats)\n","\n","for epoch in range(20):\n","    print(\"---epoch {}---\".format(epoch))\n","    for step in range(len(u) // batch_size):\n","        item_ids = torch.LongTensor(\n","            a[step * batch_size:(step + 1) * batch_size]).to(device)\n","        item_probs = torch.FloatTensor(\n","            p[step * batch_size:(step + 1) * batch_size]).to(device)\n","\n","        user_ids = u[step * batch_size:(step + 1) * batch_size]\n","        user_feats = {\n","            key: value[user_ids]\n","            for key, value in syn.user_feats.items()\n","        }\n","        user_feats = syn.to_device(user_feats)\n","\n","        logits = nominator(user_feats, val_item_feats)\n","\n","        if args.loss_type == \"loss_ce\":\n","            loss = loss_ce(logits, item_ids, item_probs)\n","        elif args.loss_type == \"loss_ips\":\n","            loss = loss_ips(logits, item_ids, item_probs, upper_limit=10)\n","        elif args.loss_type == \"loss_2s\":\n","            batch_ranker_logits = F.embedding(\n","                torch.LongTensor(user_ids).to(device), ranker_logits)\n","            loss = loss_2s(\n","                logits,\n","                item_ids,\n","                item_probs,\n","                batch_ranker_logits,\n","                upper_limit=10,\n","                alpha=args.alpha)\n","        else:\n","            raise NotImplementedError(\n","                \"{} not supported.\".format(args.loss_type))\n","\n","        opt.zero_grad()\n","        loss.backward()\n","        opt.step()\n","\n","    with torch.no_grad():\n","        nominator.eval()\n","\n","        logits = nominator(all_user_feats, all_item_feats)\n","        print(\"1 stage\", ev.one_stage_eval(logits))\n","        print(\"2 stage\", ev.two_stage_eval(logits, ranker))\n","\n","        # Evaluate ranking metrics on validation users.\n","        logits = nominator(val_user_feats, val_item_feats)\n","        one_stage_results = ev.one_stage_ranking_eval(logits, val_user_list)\n","        print(\"1 stage (val)\", one_stage_results)\n","        two_stage_results = ev.two_stage_ranking_eval(logits, ranker,\n","                                                      val_user_list)\n","        print(\"2 stage (val)\", two_stage_results)\n","        val_results.append((one_stage_results, two_stage_results))\n","        # Log best epoch\n","        if two_stage_results[check_metric] > best_result:\n","            best_epoch = epoch\n","            best_result = two_stage_results[check_metric]\n","        # Evaluate ranking metrics on test users.\n","        logits = nominator(test_user_feats, test_item_feats)\n","        one_stage_results = ev.one_stage_ranking_eval(logits, test_user_list)\n","        print(\"1 stage (test)\", one_stage_results)\n","        two_stage_results = ev.two_stage_ranking_eval(logits, ranker,\n","                                                      test_user_list)\n","        print(\"2 stage (test)\", two_stage_results)\n","        test_results.append((one_stage_results, two_stage_results))\n","\n","        nominator.train()"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["---epoch 0---\n","1 stage 0.8389073014259338\n","2 stage 0.931456983089447\n","1 stage (val) OrderedDict([('Precision@1', 0.7761116623878479), ('Precision@5', 0.7825230956077576), ('Precision@10', 0.7693895101547241), ('Recall@1', 0.0026108406018465757), ('Recall@5', 0.01443537138402462), ('Recall@10', 0.02800210937857628), ('NDCG@5', 0.7805129289627075), ('NDCG@10', 0.7720701694488525), ('NDCG@20', 0.7773204445838928)])\n","2 stage (val) OrderedDict([('Precision@1', 0.9152016639709473), ('Precision@5', 0.8799382448196411), ('Precision@10', 0.8639603853225708), ('Recall@1', 0.010427681729197502), ('Recall@5', 0.02969161979854107), ('Recall@10', 0.04909321665763855), ('NDCG@5', 0.8888923525810242), ('NDCG@10', 0.8768721222877502), ('NDCG@20', 0.8476952910423279)])\n","1 stage (test) OrderedDict([('Precision@1', 0.894406795501709), ('Precision@5', 0.8917496204376221), ('Precision@10', 0.8819892406463623), ('Recall@1', 0.003005631733685732), ('Recall@5', 0.015431685373187065), ('Recall@10', 0.030124178156256676), ('NDCG@5', 0.8914320468902588), ('NDCG@10', 0.8846309781074524), ('NDCG@20', 0.8876312971115112)])\n","2 stage (test) OrderedDict([('Precision@1', 0.9671432375907898), ('Precision@5', 0.9465765357017517), ('Precision@10', 0.9378731846809387), ('Recall@1', 0.006223659496754408), ('Recall@5', 0.022564038634300232), ('Recall@10', 0.0406784787774086), ('NDCG@5', 0.9511790871620178), ('NDCG@10', 0.9443507790565491), ('NDCG@20', 0.9283153414726257)])\n","---epoch 1---\n","1 stage 0.8367549777030945\n","2 stage 0.9365894198417664\n","1 stage (val) OrderedDict([('Precision@1', 0.7730093002319336), ('Precision@5', 0.780661404132843), ('Precision@10', 0.7693895101547241), ('Recall@1', 0.002592692384496331), ('Recall@5', 0.01431603729724884), ('Recall@10', 0.027908695861697197), ('NDCG@5', 0.7788269519805908), ('NDCG@10', 0.7716042399406433), ('NDCG@20', 0.7761374115943909)])\n","2 stage (val) OrderedDict([('Precision@1', 0.9255428910255432), ('Precision@5', 0.883350670337677), ('Precision@10', 0.8641155958175659), ('Recall@1', 0.012946750037372112), ('Recall@5', 0.03286115825176239), ('Recall@10', 0.052004944533109665), ('NDCG@5', 0.894393801689148), ('NDCG@10', 0.8804171681404114), ('NDCG@20', 0.851220965385437)])\n","1 stage (test) OrderedDict([('Precision@1', 0.8926511406898499), ('Precision@5', 0.889492392539978), ('Precision@10', 0.8817635774612427), ('Recall@1', 0.002984470222145319), ('Recall@5', 0.01524937804788351), ('Recall@10', 0.030079031363129616), ('NDCG@5', 0.8897153735160828), ('NDCG@10', 0.8841732740402222), ('NDCG@20', 0.8868218660354614)])\n","2 stage (test) OrderedDict([('Precision@1', 0.9699021577835083), ('Precision@5', 0.9467771649360657), ('Precision@10', 0.9376724362373352), ('Recall@1', 0.007032747846096754), ('Recall@5', 0.023379521444439888), ('Recall@10', 0.04146404191851616), ('NDCG@5', 0.9522501826286316), ('NDCG@10', 0.9452073574066162), ('NDCG@20', 0.9290862083435059)])\n","---epoch 2---\n","1 stage 0.8357616066932678\n","2 stage 0.9385761618614197\n","1 stage (val) OrderedDict([('Precision@1', 0.771458089351654), ('Precision@5', 0.7803509831428528), ('Precision@10', 0.7697516083717346), ('Recall@1', 0.002546904841437936), ('Recall@5', 0.014310559257864952), ('Recall@10', 0.027927899733185768), ('NDCG@5', 0.7782995104789734), ('NDCG@10', 0.7716609239578247), ('NDCG@20', 0.7768319845199585)])\n","2 stage (val) OrderedDict([('Precision@1', 0.9281282424926758), ('Precision@5', 0.8846949934959412), ('Precision@10', 0.8650981783866882), ('Recall@1', 0.013078119605779648), ('Recall@5', 0.03309616446495056), ('Recall@10', 0.05231418088078499), ('NDCG@5', 0.895997166633606), ('NDCG@10', 0.8816695213317871), ('NDCG@20', 0.8517394661903381)])\n","1 stage (test) OrderedDict([('Precision@1', 0.8918986916542053), ('Precision@5', 0.8886397480964661), ('Precision@10', 0.8822652101516724), ('Recall@1', 0.002973585156723857), ('Recall@5', 0.015205418691039085), ('Recall@10', 0.030113695189356804), ('NDCG@5', 0.8889814615249634), ('NDCG@10', 0.8843721747398376), ('NDCG@20', 0.886855959892273)])\n","2 stage (test) OrderedDict([('Precision@1', 0.9716578722000122), ('Precision@5', 0.9481315016746521), ('Precision@10', 0.9381741285324097), ('Recall@1', 0.007616504095494747), ('Recall@5', 0.024297403171658516), ('Recall@10', 0.0423288531601429), ('NDCG@5', 0.9539521932601929), ('NDCG@10', 0.9464597702026367), ('NDCG@20', 0.9301363229751587)])\n","---epoch 3---\n","1 stage 0.8339403867721558\n","2 stage 0.940562903881073\n","1 stage (val) OrderedDict([('Precision@1', 0.7683557271957397), ('Precision@5', 0.7795237898826599), ('Precision@10', 0.7704238891601562), ('Recall@1', 0.0025028539821505547), ('Recall@5', 0.014192001894116402), ('Recall@10', 0.028007399290800095), ('NDCG@5', 0.7769649624824524), ('NDCG@10', 0.7715820074081421), ('NDCG@20', 0.7766546607017517)])\n","2 stage (val) OrderedDict([('Precision@1', 0.9332988858222961), ('Precision@5', 0.8861426711082458), ('Precision@10', 0.8660286664962769), ('Recall@1', 0.01370152272284031), ('Recall@5', 0.034078482538461685), ('Recall@10', 0.05330389738082886), ('NDCG@5', 0.8979603052139282), ('NDCG@10', 0.88340163230896), ('NDCG@20', 0.8530087471008301)])\n","1 stage (test) OrderedDict([('Precision@1', 0.8906446099281311), ('Precision@5', 0.8880879282951355), ('Precision@10', 0.8825662732124329), ('Recall@1', 0.002958007389679551), ('Recall@5', 0.015176204033195972), ('Recall@10', 0.030142122879624367), ('NDCG@5', 0.8884179592132568), ('NDCG@10', 0.884431004524231), ('NDCG@20', 0.8868438005447388)])\n","2 stage (test) OrderedDict([('Precision@1', 0.9721595048904419), ('Precision@5', 0.9488838315010071), ('Precision@10', 0.938500165939331), ('Recall@1', 0.00781310349702835), ('Recall@5', 0.02463247813284397), ('Recall@10', 0.04265395551919937), ('NDCG@5', 0.9547868967056274), ('NDCG@10', 0.9471161961555481), ('NDCG@20', 0.9306148290634155)])\n","---epoch 4---\n","1 stage 0.8331125974655151\n","2 stage 0.9425497055053711\n","1 stage (val) OrderedDict([('Precision@1', 0.7652533650398254), ('Precision@5', 0.7786965370178223), ('Precision@10', 0.7709408402442932), ('Recall@1', 0.0024512740783393383), ('Recall@5', 0.014100495725870132), ('Recall@10', 0.028065374121069908), ('NDCG@5', 0.7758020758628845), ('NDCG@10', 0.7715065479278564), ('NDCG@20', 0.7763851284980774)])\n","2 stage (val) OrderedDict([('Precision@1', 0.9358841776847839), ('Precision@5', 0.8872801661491394), ('Precision@10', 0.8661839962005615), ('Recall@1', 0.013996284455060959), ('Recall@5', 0.03461095318198204), ('Recall@10', 0.05393322929739952), ('NDCG@5', 0.8994066715240479), ('NDCG@10', 0.8842594623565674), ('NDCG@20', 0.8535211086273193)])\n","1 stage (test) OrderedDict([('Precision@1', 0.890895426273346), ('Precision@5', 0.8875863552093506), ('Precision@10', 0.8828924298286438), ('Recall@1', 0.002969894791021943), ('Recall@5', 0.015152904205024242), ('Recall@10', 0.03018353506922722), ('NDCG@5', 0.887993335723877), ('NDCG@10', 0.8845545053482056), ('NDCG@20', 0.886780858039856)])\n","2 stage (test) OrderedDict([('Precision@1', 0.9739152193069458), ('Precision@5', 0.9496864676475525), ('Precision@10', 0.9387760162353516), ('Recall@1', 0.007960631512105465), ('Recall@5', 0.024898577481508255), ('Recall@10', 0.04285132512450218), ('NDCG@5', 0.9557564854621887), ('NDCG@10', 0.9477135539054871), ('NDCG@20', 0.9310080409049988)])\n","---epoch 5---\n","1 stage 0.8336092829704285\n","2 stage 0.942218542098999\n","1 stage (val) OrderedDict([('Precision@1', 0.765770435333252), ('Precision@5', 0.7783864140510559), ('Precision@10', 0.7712510228157043), ('Recall@1', 0.0024701899383217096), ('Recall@5', 0.014151088893413544), ('Recall@10', 0.02811473049223423), ('NDCG@5', 0.7756521701812744), ('NDCG@10', 0.7717127799987793), ('NDCG@20', 0.7761114239692688)])\n","2 stage (val) OrderedDict([('Precision@1', 0.9364012479782104), ('Precision@5', 0.888003945350647), ('Precision@10', 0.8662872314453125), ('Recall@1', 0.013898404315114021), ('Recall@5', 0.03511940315365791), ('Recall@10', 0.05434870347380638), ('NDCG@5', 0.8998069763183594), ('NDCG@10', 0.8843777179718018), ('NDCG@20', 0.8532311916351318)])\n","1 stage (test) OrderedDict([('Precision@1', 0.8913970589637756), ('Precision@5', 0.887134850025177), ('Precision@10', 0.8827671408653259), ('Recall@1', 0.0029800296761095524), ('Recall@5', 0.015131834894418716), ('Recall@10', 0.03018496185541153), ('NDCG@5', 0.8876475095748901), ('NDCG@10', 0.8844181895256042), ('NDCG@20', 0.8867219686508179)])\n","2 stage (test) OrderedDict([('Precision@1', 0.9731627702713013), ('Precision@5', 0.9494857788085938), ('Precision@10', 0.9387760162353516), ('Recall@1', 0.007924147881567478), ('Recall@5', 0.024823080748319626), ('Recall@10', 0.042789481580257416), ('NDCG@5', 0.9554441571235657), ('NDCG@10', 0.9475792646408081), ('NDCG@20', 0.9307677149772644)])\n","---epoch 6---\n","1 stage 0.8346026539802551\n","2 stage 0.9417218565940857\n","1 stage (val) OrderedDict([('Precision@1', 0.7652533650398254), ('Precision@5', 0.7777659296989441), ('Precision@10', 0.7713546752929688), ('Recall@1', 0.002462433883920312), ('Recall@5', 0.014112057164311409), ('Recall@10', 0.02813573367893696), ('NDCG@5', 0.7751555442810059), ('NDCG@10', 0.7717844247817993), ('NDCG@20', 0.7759582996368408)])\n","2 stage (val) OrderedDict([('Precision@1', 0.9353671073913574), ('Precision@5', 0.8887278437614441), ('Precision@10', 0.866390585899353), ('Recall@1', 0.013995282351970673), ('Recall@5', 0.035590916872024536), ('Recall@10', 0.05474210903048515), ('NDCG@5', 0.9002852439880371), ('NDCG@10', 0.8846437931060791), ('NDCG@20', 0.8533338904380798)])\n","1 stage (test) OrderedDict([('Precision@1', 0.8931527733802795), ('Precision@5', 0.8867837190628052), ('Precision@10', 0.8830179572105408), ('Recall@1', 0.0030013115610927343), ('Recall@5', 0.01514824852347374), ('Recall@10', 0.03022931143641472), ('NDCG@5', 0.8876431584358215), ('NDCG@10', 0.8846896886825562), ('NDCG@20', 0.886621356010437)])\n","2 stage (test) OrderedDict([('Precision@1', 0.9729119539260864), ('Precision@5', 0.9496863484382629), ('Precision@10', 0.9388262033462524), ('Recall@1', 0.007614111993461847), ('Recall@5', 0.024715635925531387), ('Recall@10', 0.04267028719186783), ('NDCG@5', 0.9553412199020386), ('NDCG@10', 0.9473593235015869), ('NDCG@20', 0.9304744601249695)])\n","---epoch 7---\n","1 stage 0.8350993394851685\n","2 stage 0.9408940672874451\n","1 stage (val) OrderedDict([('Precision@1', 0.7652533650398254), ('Precision@5', 0.7769388556480408), ('Precision@10', 0.771303117275238), ('Recall@1', 0.002458712086081505), ('Recall@5', 0.014063295908272266), ('Recall@10', 0.028185289353132248), ('NDCG@5', 0.7745227813720703), ('NDCG@10', 0.7716432809829712), ('NDCG@20', 0.7753005027770996)])\n","2 stage (val) OrderedDict([('Precision@1', 0.9322647452354431), ('Precision@5', 0.8887277841567993), ('Precision@10', 0.865925133228302), ('Recall@1', 0.013905792497098446), ('Recall@5', 0.035767413675785065), ('Recall@10', 0.05483044683933258), ('NDCG@5', 0.8999837636947632), ('NDCG@10', 0.8842496871948242), ('NDCG@20', 0.8529420495033264)])\n","1 stage (test) OrderedDict([('Precision@1', 0.8939051628112793), ('Precision@5', 0.8863823413848877), ('Precision@10', 0.8830680847167969), ('Recall@1', 0.003021263051778078), ('Recall@5', 0.015123428776860237), ('Recall@10', 0.03023369051516056), ('NDCG@5', 0.887448787689209), ('NDCG@10', 0.8847446441650391), ('NDCG@20', 0.886391818523407)])\n","2 stage (test) OrderedDict([('Precision@1', 0.9731627702713013), ('Precision@5', 0.9496863484382629), ('Precision@10', 0.9387257099151611), ('Recall@1', 0.00767667219042778), ('Recall@5', 0.024805083870887756), ('Recall@10', 0.04274982586503029), ('NDCG@5', 0.9554198384284973), ('NDCG@10', 0.9473903179168701), ('NDCG@20', 0.9302816390991211)])\n","---epoch 8---\n","1 stage 0.8354305028915405\n","2 stage 0.9399006962776184\n","1 stage (val) OrderedDict([('Precision@1', 0.765770435333252), ('Precision@5', 0.7759048342704773), ('Precision@10', 0.771199643611908), ('Recall@1', 0.0024883777368813753), ('Recall@5', 0.013925631530582905), ('Recall@10', 0.028112078085541725), ('NDCG@5', 0.7737619876861572), ('NDCG@10', 0.7714693546295166), ('NDCG@20', 0.7745940685272217)])\n","2 stage (val) OrderedDict([('Precision@1', 0.9307135343551636), ('Precision@5', 0.8879006505012512), ('Precision@10', 0.8653048872947693), ('Recall@1', 0.013744794763624668), ('Recall@5', 0.03534204140305519), ('Recall@10', 0.05429835617542267), ('NDCG@5', 0.8989336490631104), ('NDCG@10', 0.8833516836166382), ('NDCG@20', 0.8518505096435547)])\n","1 stage (test) OrderedDict([('Precision@1', 0.8941559791564941), ('Precision@5', 0.8859308362007141), ('Precision@10', 0.8831183314323425), ('Recall@1', 0.0030219820328056812), ('Recall@5', 0.015090204775333405), ('Recall@10', 0.030239716172218323), ('NDCG@5', 0.8871259689331055), ('NDCG@10', 0.8847541809082031), ('NDCG@20', 0.8861094117164612)])\n","2 stage (test) OrderedDict([('Precision@1', 0.9724103212356567), ('Precision@5', 0.9492348432540894), ('Precision@10', 0.9386504292488098), ('Recall@1', 0.007493751123547554), ('Recall@5', 0.02462335117161274), ('Recall@10', 0.04261213541030884), ('NDCG@5', 0.9548900723457336), ('NDCG@10', 0.9471115469932556), ('NDCG@20', 0.9297663569450378)])\n","---epoch 9---\n","1 stage 0.8354305028915405\n","2 stage 0.9394040107727051\n","1 stage (val) OrderedDict([('Precision@1', 0.7673215866088867), ('Precision@5', 0.7746638655662537), ('Precision@10', 0.7714064717292786), ('Recall@1', 0.002514956519007683), ('Recall@5', 0.013873514719307423), ('Recall@10', 0.028209399431943893), ('NDCG@5', 0.7729977965354919), ('NDCG@10', 0.7715849876403809), ('NDCG@20', 0.7738854885101318)])\n","2 stage (val) OrderedDict([('Precision@1', 0.9312306046485901), ('Precision@5', 0.8884176015853882), ('Precision@10', 0.8655635714530945), ('Recall@1', 0.01376135554164648), ('Recall@5', 0.03537794202566147), ('Recall@10', 0.0541960671544075), ('NDCG@5', 0.8995826244354248), ('NDCG@10', 0.8838357329368591), ('NDCG@20', 0.8522109389305115)])\n","1 stage (test) OrderedDict([('Precision@1', 0.8934035897254944), ('Precision@5', 0.8860311508178711), ('Precision@10', 0.8833188414573669), ('Recall@1', 0.003013131907209754), ('Recall@5', 0.015086286701261997), ('Recall@10', 0.030243460088968277), ('NDCG@5', 0.8870205283164978), ('NDCG@10', 0.884793758392334), ('NDCG@20', 0.8854647278785706)])\n","2 stage (test) OrderedDict([('Precision@1', 0.9714070558547974), ('Precision@5', 0.9491847157478333), ('Precision@10', 0.9384247064590454), ('Recall@1', 0.00736744562163949), ('Recall@5', 0.024592455476522446), ('Recall@10', 0.04252685606479645), ('NDCG@5', 0.9546566605567932), ('NDCG@10', 0.9468095302581787), ('NDCG@20', 0.9292915463447571)])\n","---epoch 10---\n","1 stage 0.8355960249900818\n","2 stage 0.9394040107727051\n","1 stage (val) OrderedDict([('Precision@1', 0.7688727974891663), ('Precision@5', 0.7746636271476746), ('Precision@10', 0.7718202471733093), ('Recall@1', 0.002541040303185582), ('Recall@5', 0.013961467891931534), ('Recall@10', 0.028273116797208786), ('NDCG@5', 0.773000955581665), ('NDCG@10', 0.7718640565872192), ('NDCG@20', 0.7727073431015015)])\n","2 stage (val) OrderedDict([('Precision@1', 0.9317476749420166), ('Precision@5', 0.8890381455421448), ('Precision@10', 0.8653049468994141), ('Recall@1', 0.01362884882837534), ('Recall@5', 0.03554679825901985), ('Recall@10', 0.05419434234499931), ('NDCG@5', 0.8999322652816772), ('NDCG@10', 0.8837052583694458), ('NDCG@20', 0.8518556952476501)])\n","1 stage (test) OrderedDict([('Precision@1', 0.8929019570350647), ('Precision@5', 0.885880708694458), ('Precision@10', 0.8833938837051392), ('Recall@1', 0.0030145926866680384), ('Recall@5', 0.015049256384372711), ('Recall@10', 0.030261926352977753), ('NDCG@5', 0.886837363243103), ('NDCG@10', 0.8847842812538147), ('NDCG@20', 0.8847416043281555)])\n","2 stage (test) OrderedDict([('Precision@1', 0.9711562395095825), ('Precision@5', 0.9493853449821472), ('Precision@10', 0.9385502338409424), ('Recall@1', 0.007329127751290798), ('Recall@5', 0.024591417983174324), ('Recall@10', 0.04251473769545555), ('NDCG@5', 0.9547252058982849), ('NDCG@10', 0.9468643069267273), ('NDCG@20', 0.9290348887443542)])\n","---epoch 11---\n","1 stage 0.8360927104949951\n","2 stage 0.939238429069519\n","1 stage (val) OrderedDict([('Precision@1', 0.7683557271957397), ('Precision@5', 0.7737331390380859), ('Precision@10', 0.7717682719230652), ('Recall@1', 0.0025312236975878477), ('Recall@5', 0.013931073248386383), ('Recall@10', 0.02832760475575924), ('NDCG@5', 0.7724922895431519), ('NDCG@10', 0.7718040943145752), ('NDCG@20', 0.772021472454071)])\n","2 stage (val) OrderedDict([('Precision@1', 0.9317476749420166), ('Precision@5', 0.8892449736595154), ('Precision@10', 0.8650463223457336), ('Recall@1', 0.01362884882837534), ('Recall@5', 0.03550821170210838), ('Recall@10', 0.05416148528456688), ('NDCG@5', 0.900104284286499), ('NDCG@10', 0.8835929036140442), ('NDCG@20', 0.8518088459968567)])\n","1 stage (test) OrderedDict([('Precision@1', 0.8939051628112793), ('Precision@5', 0.8860311508178711), ('Precision@10', 0.8833186030387878), ('Recall@1', 0.003023584373295307), ('Recall@5', 0.015060748904943466), ('Recall@10', 0.030287474393844604), ('NDCG@5', 0.8870344161987305), ('NDCG@10', 0.8848085403442383), ('NDCG@20', 0.8842055201530457)])\n","2 stage (test) OrderedDict([('Precision@1', 0.9709054231643677), ('Precision@5', 0.949586033821106), ('Precision@10', 0.9382994174957275), ('Recall@1', 0.0073165870271623135), ('Recall@5', 0.02467793971300125), ('Recall@10', 0.04251282662153244), ('NDCG@5', 0.9548905491828918), ('NDCG@10', 0.9467594623565674), ('NDCG@20', 0.9288665652275085)])\n","---epoch 12---\n","1 stage 0.8357616066932678\n","2 stage 0.9385761618614197\n","1 stage (val) OrderedDict([('Precision@1', 0.7688727974891663), ('Precision@5', 0.7738365530967712), ('Precision@10', 0.7722853422164917), ('Recall@1', 0.002532533835619688), ('Recall@5', 0.01389816403388977), ('Recall@10', 0.028517844155430794), ('NDCG@5', 0.7726624011993408), ('NDCG@10', 0.7721626162528992), ('NDCG@20', 0.7710179686546326)])\n","2 stage (val) OrderedDict([('Precision@1', 0.9312306046485901), ('Precision@5', 0.8893485069274902), ('Precision@10', 0.8653566241264343), ('Recall@1', 0.013489209115505219), ('Recall@5', 0.03542754054069519), ('Recall@10', 0.05412572622299194), ('NDCG@5', 0.9000642895698547), ('NDCG@10', 0.8836491703987122), ('NDCG@20', 0.8513972163200378)])\n","1 stage (test) OrderedDict([('Precision@1', 0.8931527733802795), ('Precision@5', 0.8858307003974915), ('Precision@10', 0.8832935690879822), ('Recall@1', 0.003019903087988496), ('Recall@5', 0.015028968453407288), ('Recall@10', 0.030282167717814445), ('NDCG@5', 0.886849045753479), ('NDCG@10', 0.8847469687461853), ('NDCG@20', 0.8832032680511475)])\n","2 stage (test) OrderedDict([('Precision@1', 0.9701529741287231), ('Precision@5', 0.9494857788085938), ('Precision@10', 0.9382240772247314), ('Recall@1', 0.007107383105903864), ('Recall@5', 0.024620026350021362), ('Recall@10', 0.04242135211825371), ('NDCG@5', 0.9546770453453064), ('NDCG@10', 0.9465430974960327), ('NDCG@20', 0.9285589456558228)])\n","---epoch 13---\n","1 stage 0.8362582921981812\n","2 stage 0.937748372554779\n","1 stage (val) OrderedDict([('Precision@1', 0.7709410786628723), ('Precision@5', 0.7739399671554565), ('Precision@10', 0.7720269560813904), ('Recall@1', 0.0025725809391587973), ('Recall@5', 0.013920770026743412), ('Recall@10', 0.028608163818717003), ('NDCG@5', 0.77310711145401), ('NDCG@10', 0.7722665071487427), ('NDCG@20', 0.7705519795417786)])\n","2 stage (val) OrderedDict([('Precision@1', 0.9296793937683105), ('Precision@5', 0.8893485069274902), ('Precision@10', 0.8653048276901245), ('Recall@1', 0.013278146274387836), ('Recall@5', 0.03546256572008133), ('Recall@10', 0.054152097553014755), ('NDCG@5', 0.8998194932937622), ('NDCG@10', 0.8834453225135803), ('NDCG@20', 0.8509284257888794)])\n","1 stage (test) OrderedDict([('Precision@1', 0.8929019570350647), ('Precision@5', 0.8853790760040283), ('Precision@10', 0.8832684755325317), ('Recall@1', 0.0030202772468328476), ('Recall@5', 0.015010001137852669), ('Recall@10', 0.030313612893223763), ('NDCG@5', 0.8865785002708435), ('NDCG@10', 0.8847340941429138), ('NDCG@20', 0.8825224041938782)])\n","2 stage (test) OrderedDict([('Precision@1', 0.9696513414382935), ('Precision@5', 0.9491847157478333), ('Precision@10', 0.9378730654716492), ('Recall@1', 0.007022056728601456), ('Recall@5', 0.024508126080036163), ('Recall@10', 0.0422583743929863), ('NDCG@5', 0.9543265700340271), ('NDCG@10', 0.9461403489112854), ('NDCG@20', 0.9281177520751953)])\n","---epoch 14---\n","1 stage 0.8352649211883545\n","2 stage 0.937748372554779\n","1 stage (val) OrderedDict([('Precision@1', 0.7699069380760193), ('Precision@5', 0.7740433216094971), ('Precision@10', 0.7716652154922485), ('Recall@1', 0.0025672533083707094), ('Recall@5', 0.013907205313444138), ('Recall@10', 0.028547627851366997), ('NDCG@5', 0.7730266451835632), ('NDCG@10', 0.7719103097915649), ('NDCG@20', 0.7697432637214661)])\n","2 stage (val) OrderedDict([('Precision@1', 0.9286453127861023), ('Precision@5', 0.8890382647514343), ('Precision@10', 0.8652012944221497), ('Recall@1', 0.012992068193852901), ('Recall@5', 0.03513404354453087), ('Recall@10', 0.05389030650258064), ('NDCG@5', 0.8995911478996277), ('NDCG@10', 0.8832032084465027), ('NDCG@20', 0.8503848314285278)])\n","1 stage (test) OrderedDict([('Precision@1', 0.8918986916542053), ('Precision@5', 0.8851282596588135), ('Precision@10', 0.8826413154602051), ('Recall@1', 0.003015252063050866), ('Recall@5', 0.015008385293185711), ('Recall@10', 0.030310826376080513), ('NDCG@5', 0.8863389492034912), ('NDCG@10', 0.8842962384223938), ('NDCG@20', 0.8815882802009583)])\n","2 stage (test) OrderedDict([('Precision@1', 0.9701529741287231), ('Precision@5', 0.9492349028587341), ('Precision@10', 0.9378228783607483), ('Recall@1', 0.006919103674590588), ('Recall@5', 0.024281606078147888), ('Recall@10', 0.04200495406985283), ('NDCG@5', 0.9543092846870422), ('NDCG@10', 0.9460137486457825), ('NDCG@20', 0.9279131889343262)])\n","---epoch 15---\n","1 stage 0.8342715501785278\n","2 stage 0.9382450580596924\n","1 stage (val) OrderedDict([('Precision@1', 0.7688727974891663), ('Precision@5', 0.7741466164588928), ('Precision@10', 0.7720270752906799), ('Recall@1', 0.0025641624815762043), ('Recall@5', 0.013893092051148415), ('Recall@10', 0.028611961752176285), ('NDCG@5', 0.7728968262672424), ('NDCG@10', 0.7719386219978333), ('NDCG@20', 0.7689577341079712)])\n","2 stage (val) OrderedDict([('Precision@1', 0.9301964640617371), ('Precision@5', 0.8893486857414246), ('Precision@10', 0.8648910522460938), ('Recall@1', 0.012969724833965302), ('Recall@5', 0.035027049481868744), ('Recall@10', 0.0536569282412529), ('NDCG@5', 0.8999989032745361), ('NDCG@10', 0.8830557465553284), ('NDCG@20', 0.8499181270599365)])\n","1 stage (test) OrderedDict([('Precision@1', 0.890895426273346), ('Precision@5', 0.885128378868103), ('Precision@10', 0.8826413154602051), ('Recall@1', 0.0030101861339062452), ('Recall@5', 0.014987428672611713), ('Recall@10', 0.030330732464790344), ('NDCG@5', 0.8862097859382629), ('NDCG@10', 0.8841873407363892), ('NDCG@20', 0.8804370760917664)])\n","2 stage (test) OrderedDict([('Precision@1', 0.9701529741287231), ('Precision@5', 0.9493853449821472), ('Precision@10', 0.9377727508544922), ('Recall@1', 0.006855727173388004), ('Recall@5', 0.0241836067289114), ('Recall@10', 0.04189404845237732), ('NDCG@5', 0.9543688297271729), ('NDCG@10', 0.9459018707275391), ('NDCG@20', 0.9275990128517151)])\n","---epoch 16---\n","1 stage 0.8342715501785278\n","2 stage 0.939238429069519\n","1 stage (val) OrderedDict([('Precision@1', 0.7699069380760193), ('Precision@5', 0.7728022933006287), ('Precision@10', 0.7725440263748169), ('Recall@1', 0.002587593160569668), ('Recall@5', 0.01379038393497467), ('Recall@10', 0.02874572202563286), ('NDCG@5', 0.7722145915031433), ('NDCG@10', 0.7722715139389038), ('NDCG@20', 0.7682958245277405)])\n","2 stage (val) OrderedDict([('Precision@1', 0.9322647452354431), ('Precision@5', 0.8898658752441406), ('Precision@10', 0.8647358417510986), ('Recall@1', 0.01313948817551136), ('Recall@5', 0.03528093919157982), ('Recall@10', 0.053822338581085205), ('NDCG@5', 0.9008933305740356), ('NDCG@10', 0.8833855390548706), ('NDCG@20', 0.8500461578369141)])\n","1 stage (test) OrderedDict([('Precision@1', 0.8903937935829163), ('Precision@5', 0.8849779367446899), ('Precision@10', 0.8826160430908203), ('Recall@1', 0.0030037390533834696), ('Recall@5', 0.014980170875787735), ('Recall@10', 0.03035588748753071), ('NDCG@5', 0.8861247897148132), ('NDCG@10', 0.8841509819030762), ('NDCG@20', 0.8795574903488159)])\n","2 stage (test) OrderedDict([('Precision@1', 0.9706546068191528), ('Precision@5', 0.9496863484382629), ('Precision@10', 0.9377727508544922), ('Recall@1', 0.006977349519729614), ('Recall@5', 0.024342140182852745), ('Recall@10', 0.0420152023434639), ('NDCG@5', 0.9546906352043152), ('NDCG@10', 0.9460608959197998), ('NDCG@20', 0.9275720715522766)])\n","---epoch 17---\n","1 stage 0.8339403867721558\n","2 stage 0.9387417435646057\n","1 stage (val) OrderedDict([('Precision@1', 0.7688727974891663), ('Precision@5', 0.7732157707214355), ('Precision@10', 0.7721819281578064), ('Recall@1', 0.0025853486731648445), ('Recall@5', 0.013845651410520077), ('Recall@10', 0.028628256171941757), ('NDCG@5', 0.7722429633140564), ('NDCG@10', 0.7718576192855835), ('NDCG@20', 0.767686665058136)])\n","2 stage (val) OrderedDict([('Precision@1', 0.9312306046485901), ('Precision@5', 0.8897624015808105), ('Precision@10', 0.864425778388977), ('Recall@1', 0.013013952411711216), ('Recall@5', 0.035018254071474075), ('Recall@10', 0.05353393778204918), ('NDCG@5', 0.9006258249282837), ('NDCG@10', 0.8829480409622192), ('NDCG@20', 0.8497097492218018)])\n","1 stage (test) OrderedDict([('Precision@1', 0.8903937935829163), ('Precision@5', 0.8849778175354004), ('Precision@10', 0.8823150396347046), ('Recall@1', 0.0030055013485252857), ('Recall@5', 0.014995132572948933), ('Recall@10', 0.030364053323864937), ('NDCG@5', 0.8861544132232666), ('NDCG@10', 0.8839383721351624), ('NDCG@20', 0.8786713480949402)])\n","2 stage (test) OrderedDict([('Precision@1', 0.970403790473938), ('Precision@5', 0.9499371647834778), ('Precision@10', 0.9376974701881409), ('Recall@1', 0.006946822162717581), ('Recall@5', 0.024365242570638657), ('Recall@10', 0.04201309755444527), ('NDCG@5', 0.9548534750938416), ('NDCG@10', 0.9460048079490662), ('NDCG@20', 0.9273778200149536)])\n","---epoch 18---\n","1 stage 0.8332781791687012\n","2 stage 0.9384106397628784\n","1 stage (val) OrderedDict([('Precision@1', 0.7683557271957397), ('Precision@5', 0.7734225392341614), ('Precision@10', 0.7714068293571472), ('Recall@1', 0.0025810804218053818), ('Recall@5', 0.013942412100732327), ('Recall@10', 0.02855859324336052), ('NDCG@5', 0.7723387479782104), ('NDCG@10', 0.7713356614112854), ('NDCG@20', 0.767216145992279)])\n","2 stage (val) OrderedDict([('Precision@1', 0.9301964640617371), ('Precision@5', 0.889141857624054), ('Precision@10', 0.8643222451210022), ('Recall@1', 0.01304357685148716), ('Recall@5', 0.034884266555309296), ('Recall@10', 0.053411055356264114), ('NDCG@5', 0.9000440239906311), ('NDCG@10', 0.8827939033508301), ('NDCG@20', 0.8493320941925049)])\n","1 stage (test) OrderedDict([('Precision@1', 0.8896413445472717), ('Precision@5', 0.8848274350166321), ('Precision@10', 0.8820641040802002), ('Recall@1', 0.0029982151463627815), ('Recall@5', 0.015002566389739513), ('Recall@10', 0.03033376671373844), ('NDCG@5', 0.8859151601791382), ('NDCG@10', 0.883664071559906), ('NDCG@20', 0.8777598142623901)])\n","2 stage (test) OrderedDict([('Precision@1', 0.970403790473938), ('Precision@5', 0.949836790561676), ('Precision@10', 0.9376975297927856), ('Recall@1', 0.006989224348217249), ('Recall@5', 0.024378934875130653), ('Recall@10', 0.042055051773786545), ('NDCG@5', 0.9547573924064636), ('NDCG@10', 0.946016252040863), ('NDCG@20', 0.9272603392601013)])\n","---epoch 19---\n","1 stage 0.8344370722770691\n","2 stage 0.937417209148407\n","1 stage (val) OrderedDict([('Precision@1', 0.7724922299385071), ('Precision@5', 0.7734224796295166), ('Precision@10', 0.7712516784667969), ('Recall@1', 0.0026290433015674353), ('Recall@5', 0.013950219377875328), ('Recall@10', 0.028614703565835953), ('NDCG@5', 0.7729315161705017), ('NDCG@10', 0.7716161608695984), ('NDCG@20', 0.7671958208084106)])\n","2 stage (val) OrderedDict([('Precision@1', 0.9281282424926758), ('Precision@5', 0.8886248469352722), ('Precision@10', 0.8638569712638855), ('Recall@1', 0.01269526220858097), ('Recall@5', 0.03449012339115143), ('Recall@10', 0.05301033332943916), ('NDCG@5', 0.8991745114326477), ('NDCG@10', 0.8819308876991272), ('NDCG@20', 0.8481321334838867)])\n","1 stage (test) OrderedDict([('Precision@1', 0.8893905282020569), ('Precision@5', 0.8843759894371033), ('Precision@10', 0.8814369440078735), ('Recall@1', 0.0030090906657278538), ('Recall@5', 0.014989291317760944), ('Recall@10', 0.030279411002993584), ('NDCG@5', 0.8855165243148804), ('NDCG@10', 0.8831601738929749), ('NDCG@20', 0.8770545125007629)])\n","2 stage (test) OrderedDict([('Precision@1', 0.9699021577835083), ('Precision@5', 0.9497867226600647), ('Precision@10', 0.9374968409538269), ('Recall@1', 0.006969019304960966), ('Recall@5', 0.024379633367061615), ('Recall@10', 0.04202014207839966), ('NDCG@5', 0.9546409845352173), ('NDCG@10', 0.9458295702934265), ('NDCG@20', 0.9270906448364258)])\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0Vj_CNjjTgsj","executionInfo":{"status":"ok","timestamp":1633456966030,"user_tz":-330,"elapsed":41,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}},"outputId":"92c6122d-f8f8-4e0a-f77b-591064629aa9"},"source":["print(\"Best validation epoch: {}\".format(best_epoch))\n","print(\"Best validation stage results\\n 1 stage: {}\\n 2 stage: {}\".format(\n","    val_results[best_epoch][0], val_results[best_epoch][1]))\n","print(\"Best test results\\n 1 stage: {}\\n 2 stage: {}\".format(\n","    test_results[best_epoch][0], test_results[best_epoch][1]))"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Best validation epoch: 6\n","Best validation stage results\n"," 1 stage: OrderedDict([('Precision@1', 0.7652533650398254), ('Precision@5', 0.7777659296989441), ('Precision@10', 0.7713546752929688), ('Recall@1', 0.002462433883920312), ('Recall@5', 0.014112057164311409), ('Recall@10', 0.02813573367893696), ('NDCG@5', 0.7751555442810059), ('NDCG@10', 0.7717844247817993), ('NDCG@20', 0.7759582996368408)])\n"," 2 stage: OrderedDict([('Precision@1', 0.9353671073913574), ('Precision@5', 0.8887278437614441), ('Precision@10', 0.866390585899353), ('Recall@1', 0.013995282351970673), ('Recall@5', 0.035590916872024536), ('Recall@10', 0.05474210903048515), ('NDCG@5', 0.9002852439880371), ('NDCG@10', 0.8846437931060791), ('NDCG@20', 0.8533338904380798)])\n","Best test results\n"," 1 stage: OrderedDict([('Precision@1', 0.8931527733802795), ('Precision@5', 0.8867837190628052), ('Precision@10', 0.8830179572105408), ('Recall@1', 0.0030013115610927343), ('Recall@5', 0.01514824852347374), ('Recall@10', 0.03022931143641472), ('NDCG@5', 0.8876431584358215), ('NDCG@10', 0.8846896886825562), ('NDCG@20', 0.886621356010437)])\n"," 2 stage: OrderedDict([('Precision@1', 0.9729119539260864), ('Precision@5', 0.9496863484382629), ('Precision@10', 0.9388262033462524), ('Recall@1', 0.007614111993461847), ('Recall@5', 0.024715635925531387), ('Recall@10', 0.04267028719186783), ('NDCG@5', 0.9553412199020386), ('NDCG@10', 0.9473593235015869), ('NDCG@20', 0.9304744601249695)])\n"]}]},{"cell_type":"code","metadata":{"id":"LvpiLglNThxl"},"source":["pickle.dump((best_epoch, val_results, test_results),\n","            open(\"{}-a{}_{}.pkl\".format(\n","                args.loss_type.split(\"_\")[1], args.alpha, args.seed), \"wb\"))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"W-UYEG3GqQRs"},"source":["!apt-get install tree"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"79ap6knRqT49","executionInfo":{"status":"ok","timestamp":1633457638495,"user_tz":-330,"elapsed":511,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}},"outputId":"a98836ff-7257-4888-fd08-af56775c5bfa"},"source":["!tree --du -h"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":[".\n","â”œâ”€â”€ [ 20K]  ce-a0.001_0.pkl\n","â”œâ”€â”€ [651M]  ml-1m\n","â”‚Â Â  â”œâ”€â”€ [626M]  full_impression_feats.pt\n","â”‚Â Â  â”œâ”€â”€ [463K]  logging_policy.pt\n","â”‚Â Â  â”œâ”€â”€ [167K]  movies.dat\n","â”‚Â Â  â”œâ”€â”€ [463K]  ranker.pt\n","â”‚Â Â  â”œâ”€â”€ [ 23M]  ratings.dat\n","â”‚Â Â  â”œâ”€â”€ [5.4K]  README\n","â”‚Â Â  â”œâ”€â”€ [502K]  simulator.pt\n","â”‚Â Â  â””â”€â”€ [131K]  users.dat\n","â”œâ”€â”€ [5.6M]  ml-1m.zip\n","â””â”€â”€ [ 54M]  sample_data\n","    â”œâ”€â”€ [1.7K]  anscombe.json\n","    â”œâ”€â”€ [294K]  california_housing_test.csv\n","    â”œâ”€â”€ [1.6M]  california_housing_train.csv\n","    â”œâ”€â”€ [ 17M]  mnist_test.csv\n","    â”œâ”€â”€ [ 35M]  mnist_train_small.csv\n","    â””â”€â”€ [ 930]  README.md\n","\n"," 711M used in 2 directories, 16 files\n"]}]},{"cell_type":"markdown","metadata":{"id":"Bk4t2SpxogJM"},"source":["## Experimental results\n","\n","![](https://github.com/recohut/drl-recsys/raw/a8a38a68e3606557f3501c8adc85fc6aa5726bc1/docs/_images/T257798_4.png)\n","\n","## Conclusion\n","\n","- The results of 2-IPS method in both evaluations are better than 1-IPS and cross-entropy.\n","- 1-IPS performs better than the cross-entropy method in one-stage evaluation, and performs worse than the cross-entropy method in two-stage evaluation, indicating that only improving the performance of a part of the system may not necessarily improve the performance of the entire system."]}]}