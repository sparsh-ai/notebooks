{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"T938387 | PyTorch AutoML with AutoPyTorch","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyMIsOyrXLOCHUOVl8y3xoN5"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"ZWazUhOh8_3Y"},"source":["Automated machine learning (AutoML) provides methods to find the optimal neural architecture and the best hyperparameter settings for a given neural network. "]},{"cell_type":"markdown","metadata":{"id":"HWkqlzdM9X-Z"},"source":["One way to think of machine learning algorithms is that they automate the process of learning relationships between given inputs and outputs. In traditional software engineering, we would have to explicitly write/code these relationships in the form of functions that take in input and return output. In the machine learning world, machine learning models find such functions for us. Although we automate to a certain extent, there is still a lot to be done. Besides mining and cleaning data, here are a few routine tasks to be performed in order to get those functions:\n","- Choosing a machine learning model (or a model family and then a model)\n","- Deciding the model architecture (especially in the case of deep learning)\n","- Choosing hyperparameters\n","- Adjusting hyperparameters based on validation set performance\n","- Trying different models (or model families)"]},{"cell_type":"markdown","metadata":{"id":"NpLiJ4iE9hdr"},"source":["These are the kinds of tasks that justify the requirement of a human machine learning expert. Most of these steps are manual and either take a lot of time or need a lot of expertise to discount the required time, and we have far fewer machine learning experts than needed to create and deploy machine learning models that are increasingly popular, valuable, and useful across both industries and academia.\n","\n","This is where AutoML comes to the rescue. AutoML has become a discipline within the field of machine learning that aims to automate the previously listed steps and beyond."]},{"cell_type":"markdown","metadata":{"id":"bqiEvapr9In8"},"source":["we will look more broadly at the AutoML tool for PyTorch—Auto-PyTorch—which performs both neural architecture search and hyperparameter search. We will first load the dataset, then define an Auto-PyTorch model search instance, and finally run the model searching routine, which will provide us with a best-performing model.\n","\n","\n","We will also look at another AutoML tool called Optuna that performs hyperparameter search for a PyTorch model."]},{"cell_type":"code","metadata":{"id":"Xfxxe7XO9MUM"},"source":["!pip install git+https://github.com/shukon/HpBandSter.git\n","!pip install autoPyTorch==0.0.2\n","!pip install torchviz==0.0.1\n","!pip install configspace==0.4.12"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3lEXFTX692bJ","executionInfo":{"status":"ok","timestamp":1631265557564,"user_tz":-330,"elapsed":1361,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}},"outputId":"6d9f2e6f-dc66-4f50-e745-d5f1527705c4"},"source":["import torch\n","from torchviz import make_dot\n","from torchvision import datasets, transforms\n","from autoPyTorch import AutoNetClassification\n","\n","import matplotlib.pyplot as plt\n","import numpy as np"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n","  import pandas.util.testing as tm\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lhKEYr2--Rka","executionInfo":{"status":"ok","timestamp":1631265557572,"user_tz":-330,"elapsed":36,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}},"outputId":"16112d1b-cb00-453c-e24b-6ad102f6aa89"},"source":["train_ds = datasets.MNIST('../data', train=True, download=True,\n","                   transform=transforms.Compose([\n","                       transforms.ToTensor(),\n","                       transforms.Normalize((0.1302,), (0.3069,))]))\n","\n","test_ds = datasets.MNIST('../data', train=False, \n","                   transform=transforms.Compose([\n","                       transforms.ToTensor(),\n","                       transforms.Normalize((0.1302,), (0.3069,))]))"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torchvision/datasets/mnist.py:498: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /pytorch/torch/csrc/utils/tensor_numpy.cpp:180.)\n","  return torch.from_numpy(parsed.astype(m[2], copy=False)).view(*s)\n"]}]},{"cell_type":"code","metadata":{"id":"bBnjQcaN-U1Q"},"source":["X_train, X_test, y_train, y_test = train_ds.data.numpy().reshape(-1, 28*28), test_ds.data.numpy().reshape(-1, 28*28) ,train_ds.targets.numpy(), test_ds.targets.numpy()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"01t84nHd-Wbx","executionInfo":{"status":"ok","timestamp":1631266344426,"user_tz":-330,"elapsed":786876,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}},"outputId":"c7b35a47-3c65-40e6-eece-8ecc9ac6fb73"},"source":["# running Auto-PyTorch\n","autoPyTorch = AutoNetClassification(\"tiny_cs\",  # config preset\n","                                    log_level='info',\n","                                    max_runtime=2000,\n","                                    min_budget=100,\n","                                    max_budget=1500)\n","\n","autoPyTorch.fit(X_train, y_train, validation_split=0.1)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:144: FutureWarning: The sklearn.metrics.classification module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.metrics. Anything that cannot be imported from sklearn.metrics is now part of the private API.\n","  warnings.warn(message, FutureWarning)\n","09:19:15 [AutoNet] Start bohb\n","09:19:15 WORKER: start listening for jobs\n","09:19:15 DISPATCHER: started the 'discover_worker' thread\n","09:19:15 DISPATCHER: started the 'job_runner' thread\n","09:19:15 DISPATCHER: Pyro daemon running on 172.28.0.2:41259\n","09:19:15 DISPATCHER: discovered new worker, hpbandster.run_0.worker.b20b68280430.581.-1140550406272896\n","09:19:15 HBMASTER: adjusted queue size to (0, 1)\n","09:19:15 DISPATCHER: A new worker triggered discover_worker\n","09:19:15 HBMASTER: starting run at 1631265555.806195\n","09:19:15 WORKER: start processing job (0, 0, 0)\n"]},{"output_type":"stream","name":"stdout","text":["True\n"]},{"output_type":"stream","name":"stderr","text":["09:19:15 Fit optimization pipeline\n","09:19:15 [AutoNet] No validation set given and either no cross validator given or budget too low for CV. Continue by splitting 0.1 of training data.\n","09:19:15 [AutoNet] CV split 0 of 1\n","09:19:15 Reduced initial budget 166.5209392706553 to cv budget 166.51660958925882 compensate for 0.004329681396484375\n","/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","09:21:52 Finished train with budget 166.51660958925882: Preprocessing took 14s, Training took 142s, Wrap up took 0s. Total time consumption in s: 156\n","09:21:52 [AutoNet] Done with current split!\n","09:21:52 Aggregate the results across the splits\n","09:21:52 Process 1 additional result(s)\n","09:21:52 Training ['shapedresnet'] with budget 166.66666666666666 resulted in optimize-metric-loss: -86.61666666666666 took 157.09570217132568 seconds\n","09:21:52 WORKER: registered result for job (0, 0, 0) with dispatcher\n","09:21:52 WORKER: start processing job (0, 0, 1)\n","09:21:52 Fit optimization pipeline\n","09:21:53 [AutoNet] No validation set given and either no cross validator given or budget too low for CV. Continue by splitting 0.1 of training data.\n","09:21:53 [AutoNet] CV split 0 of 1\n","09:21:53 Reduced initial budget 166.4198364416758 to cv budget 166.41762653986612 compensate for 0.002209901809692383\n","/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","09:24:30 Finished train with budget 166.41762653986612: Preprocessing took 14s, Training took 141s, Wrap up took 0s. Total time consumption in s: 157\n","09:24:30 [AutoNet] Done with current split!\n","09:24:30 Aggregate the results across the splits\n","09:24:30 Process 1 additional result(s)\n","09:24:30 Training ['shapedresnet'] with budget 166.66666666666666 resulted in optimize-metric-loss: -66.33333333333333 took 157.66091442108154 seconds\n","09:24:30 WORKER: registered result for job (0, 0, 1) with dispatcher\n","09:24:30 WORKER: start processing job (0, 0, 2)\n","09:24:30 Fit optimization pipeline\n","09:24:30 [AutoNet] No validation set given and either no cross validator given or budget too low for CV. Continue by splitting 0.1 of training data.\n","09:24:30 [AutoNet] CV split 0 of 1\n","09:24:30 Reduced initial budget 166.5122815767924 to cv budget 166.50750724474588 compensate for 0.004774332046508789\n","/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","09:27:07 Finished train with budget 166.50750724474588: Preprocessing took 14s, Training took 142s, Wrap up took 0s. Total time consumption in s: 156\n","09:27:07 [AutoNet] Done with current split!\n","09:27:07 Aggregate the results across the splits\n","09:27:07 Process 1 additional result(s)\n","09:27:07 Training ['shapedresnet'] with budget 166.66666666666666 resulted in optimize-metric-loss: -87.35 took 156.95166015625 seconds\n","09:27:07 WORKER: registered result for job (0, 0, 2) with dispatcher\n","09:27:07 WORKER: start processing job (0, 0, 3)\n","09:27:07 Fit optimization pipeline\n","09:27:07 [AutoNet] No validation set given and either no cross validator given or budget too low for CV. Continue by splitting 0.1 of training data.\n","09:27:07 [AutoNet] CV split 0 of 1\n","09:27:07 Reduced initial budget 166.5046707789103 to cv budget 166.502716700236 compensate for 0.0019540786743164062\n","/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","09:29:44 Finished train with budget 166.502716700236: Preprocessing took 14s, Training took 142s, Wrap up took 0s. Total time consumption in s: 156\n","09:29:44 [AutoNet] Done with current split!\n","09:29:44 Aggregate the results across the splits\n","09:29:44 Process 1 additional result(s)\n","09:29:44 Training ['shapedresnet'] with budget 166.66666666666666 resulted in optimize-metric-loss: -95.71666666666667 took 156.95315790176392 seconds\n","09:29:44 WORKER: registered result for job (0, 0, 3) with dispatcher\n","09:29:44 HBMASTER: Timelimit reached: wait for remaining 0 jobs\n","09:29:44 DISPATCHER: Dispatcher shutting down\n","09:29:44 DISPATCHER: shut down complete\n","09:29:45 Start autonet with config:\n","{'embeddings': ['none'], 'lr_scheduler': ['cosine_annealing'], 'networks': ['shapedresnet'], 'preprocessors': ['truncated_svd'], 'target_size_strategies': ['none'], 'over_sampling_methods': ['none'], 'under_sampling_methods': ['none'], 'batch_loss_computation_techniques': ['standard'], 'imputation_strategies': ['median'], 'initialization_methods': ['default'], 'loss_modules': ['cross_entropy_weighted'], 'normalization_strategies': ['standardize'], 'optimizer': ['sgd'], 'hyperparameter_search_space_updates': <autoPyTorch.utils.hyperparameter_search_space_update.HyperparameterSearchSpaceUpdates object at 0x7fd3dde6b190>, 'log_level': 'info', 'max_runtime': 2000, 'min_budget': 100, 'max_budget': 1500, 'validation_split': 0.1, 'result_logger_dir': '.', 'categorical_features': None, 'dataset_name': None, 'run_id': '0', 'task_id': -1, 'algorithm': 'bohb', 'budget_type': 'time', 'eta': 3, 'min_workers': 1, 'working_dir': '.', 'network_interface_name': 'eth0', 'memory_limit_mb': 1000000, 'use_tensorboard_logger': False, 'run_worker_on_master_node': True, 'use_pynisher': True, 'refit_validation_split': 0.0, 'cross_validator': 'none', 'cross_validator_args': {}, 'min_budget_for_cv': 0, 'shuffle': True, 'final_activation': 'softmax', 'initializer': 'simple_initializer', 'additional_logs': [], 'optimize_metric': 'accuracy', 'additional_metrics': [], 'cuda': True, 'torch_num_threads': 1, 'full_eval_each_epoch': False, 'best_over_epochs': False, 'early_stopping_patience': inf, 'early_stopping_reset_parameters': False, 'random_seed': 271995410, 'num_iterations': inf, 'cv_splits': 1, 'increase_number_of_trained_datasets': False}\n","09:29:45 Start Refitting\n","09:29:45 [AutoNet] No validation set given and either no cross validator given or budget too low for CV. Continue by splitting 0 of training data.\n","09:29:45 [AutoNet] CV split 0 of 1\n","09:29:45 Reduced initial budget 166.59684816996256 to cv budget 166.595343987147 compensate for 0.0015041828155517578\n","/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n","  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","09:32:21 Finished train with budget 166.595343987147: Preprocessing took 15s, Training took 141s, Wrap up took 0s. Total time consumption in s: 156\n","09:32:21 [AutoNet] Done with current split!\n","09:32:21 Aggregate the results across the splits\n","09:32:21 Process 1 additional result(s)\n","09:32:22 Done Refitting\n"]},{"output_type":"execute_result","data":{"text/plain":["{'budget': 166.66666666666666,\n"," 'info': {'loss': 0.1887385893130192,\n","  'lr': 2.8504234646506757e-05,\n","  'lr_scheduler_converged': 1.0,\n","  'train_accuracy': 96.6925925925926,\n","  'val_accuracy': 96.71666666666667},\n"," 'loss': -95.71666666666667,\n"," 'optimized_hyperparameter_config': {'CreateDataLoader:batch_size': 125,\n","  'Imputation:strategy': 'median',\n","  'InitializationSelector:initialization_method': 'default',\n","  'InitializationSelector:initializer:initialize_bias': 'No',\n","  'LearningrateSchedulerSelector:cosine_annealing:T_max': 10,\n","  'LearningrateSchedulerSelector:cosine_annealing:T_mult': 2,\n","  'LearningrateSchedulerSelector:lr_scheduler': 'cosine_annealing',\n","  'LossModuleSelector:loss_module': 'cross_entropy_weighted',\n","  'NetworkSelector:network': 'shapedresnet',\n","  'NetworkSelector:shapedresnet:activation': 'relu',\n","  'NetworkSelector:shapedresnet:blocks_per_group': 3,\n","  'NetworkSelector:shapedresnet:max_units': 60,\n","  'NetworkSelector:shapedresnet:num_groups': 1,\n","  'NetworkSelector:shapedresnet:resnet_shape': 'brick',\n","  'NetworkSelector:shapedresnet:use_dropout': 0,\n","  'NetworkSelector:shapedresnet:use_shake_drop': 0,\n","  'NetworkSelector:shapedresnet:use_shake_shake': 0,\n","  'NormalizationStrategySelector:normalization_strategy': 'standardize',\n","  'OptimizerSelector:optimizer': 'sgd',\n","  'OptimizerSelector:sgd:learning_rate': 0.004630445614057412,\n","  'OptimizerSelector:sgd:momentum': 0.2752045729889645,\n","  'OptimizerSelector:sgd:weight_decay': 0.019499312150623452,\n","  'PreprocessorSelector:preprocessor': 'truncated_svd',\n","  'PreprocessorSelector:truncated_svd:target_dim': 100,\n","  'ResamplingStrategySelector:over_sampling_method': 'none',\n","  'ResamplingStrategySelector:target_size_strategy': 'none',\n","  'ResamplingStrategySelector:under_sampling_method': 'none',\n","  'TrainNode:batch_loss_computation_technique': 'standard'}}"]},"metadata":{},"execution_count":4}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uG83BUEuAQM8","executionInfo":{"status":"ok","timestamp":1631266373180,"user_tz":-330,"elapsed":1306,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}},"outputId":"5cd0cfff-9f31-4b49-8327-875f7d669a43"},"source":["y_pred = autoPyTorch.predict(X_test)\n","print(\"Accuracy score\", np.mean(y_pred.reshape(-1) == y_test))"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Accuracy score 0.9691\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yBmPCJGsAavW","executionInfo":{"status":"ok","timestamp":1631266373183,"user_tz":-330,"elapsed":28,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}},"outputId":"9c9e2870-de5a-4b31-f16f-2d7e4abd883e"},"source":["pytorch_model = autoPyTorch.get_pytorch_model()\n","print(pytorch_model)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Sequential(\n","  (0): Linear(in_features=100, out_features=100, bias=True)\n","  (1): Sequential(\n","    (0): ResBlock(\n","      (layers): Sequential(\n","        (0): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (1): ReLU()\n","        (2): Linear(in_features=100, out_features=100, bias=True)\n","        (3): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (4): ReLU()\n","        (5): Linear(in_features=100, out_features=100, bias=True)\n","      )\n","    )\n","    (1): ResBlock(\n","      (layers): Sequential(\n","        (0): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (1): ReLU()\n","        (2): Linear(in_features=100, out_features=100, bias=True)\n","        (3): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (4): ReLU()\n","        (5): Linear(in_features=100, out_features=100, bias=True)\n","      )\n","    )\n","    (2): ResBlock(\n","      (layers): Sequential(\n","        (0): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (1): ReLU()\n","        (2): Linear(in_features=100, out_features=100, bias=True)\n","        (3): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (4): ReLU()\n","        (5): Linear(in_features=100, out_features=100, bias=True)\n","      )\n","    )\n","  )\n","  (2): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","  (3): ReLU()\n","  (4): Linear(in_features=100, out_features=10, bias=True)\n",")\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"SRKun9ziAed4","executionInfo":{"status":"ok","timestamp":1631266373191,"user_tz":-330,"elapsed":30,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}},"outputId":"983aefb2-c42d-4bb9-aa3f-bcc42a59a784"},"source":["x = torch.randn(1, pytorch_model[0].in_features)\n","y = pytorch_model(x)\n","arch = make_dot(y.mean(), params=dict(pytorch_model.named_parameters()))\n","arch.format=\"pdf\"\n","arch.filename = \"convnet_arch\"\n","arch.render(view=False)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'convnet_arch.pdf'"]},"metadata":{},"execution_count":7}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oOs3P_CcAifi","executionInfo":{"status":"ok","timestamp":1631266376874,"user_tz":-330,"elapsed":432,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}},"outputId":"fcdff773-b9d4-443a-b484-40d6f823a9a3"},"source":["autoPyTorch.get_hyperparameter_search_space()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Configuration space object:\n","  Hyperparameters:\n","    CreateDataLoader:batch_size, Type: Constant, Value: 125\n","    Imputation:strategy, Type: Categorical, Choices: {median}, Default: median\n","    InitializationSelector:initialization_method, Type: Categorical, Choices: {default}, Default: default\n","    InitializationSelector:initializer:initialize_bias, Type: Constant, Value: No\n","    LearningrateSchedulerSelector:cosine_annealing:T_max, Type: Constant, Value: 10\n","    LearningrateSchedulerSelector:cosine_annealing:T_mult, Type: Constant, Value: 2\n","    LearningrateSchedulerSelector:lr_scheduler, Type: Categorical, Choices: {cosine_annealing}, Default: cosine_annealing\n","    LossModuleSelector:loss_module, Type: Categorical, Choices: {cross_entropy_weighted}, Default: cross_entropy_weighted\n","    NetworkSelector:network, Type: Categorical, Choices: {shapedresnet}, Default: shapedresnet\n","    NetworkSelector:shapedresnet:activation, Type: Constant, Value: relu\n","    NetworkSelector:shapedresnet:blocks_per_group, Type: UniformInteger, Range: [1, 4], Default: 2\n","    NetworkSelector:shapedresnet:max_units, Type: UniformInteger, Range: [10, 1024], Default: 101, on log-scale\n","    NetworkSelector:shapedresnet:num_groups, Type: UniformInteger, Range: [1, 9], Default: 5\n","    NetworkSelector:shapedresnet:resnet_shape, Type: Constant, Value: brick\n","    NetworkSelector:shapedresnet:use_dropout, Type: Constant, Value: 0\n","    NetworkSelector:shapedresnet:use_shake_drop, Type: Constant, Value: 0\n","    NetworkSelector:shapedresnet:use_shake_shake, Type: Constant, Value: 0\n","    NormalizationStrategySelector:normalization_strategy, Type: Categorical, Choices: {standardize}, Default: standardize\n","    OptimizerSelector:optimizer, Type: Categorical, Choices: {sgd}, Default: sgd\n","    OptimizerSelector:sgd:learning_rate, Type: UniformFloat, Range: [0.0001, 0.1], Default: 0.0031622777, on log-scale\n","    OptimizerSelector:sgd:momentum, Type: UniformFloat, Range: [0.1, 0.99], Default: 0.3146426545, on log-scale\n","    OptimizerSelector:sgd:weight_decay, Type: UniformFloat, Range: [1e-05, 0.1], Default: 0.050005\n","    PreprocessorSelector:preprocessor, Type: Categorical, Choices: {truncated_svd}, Default: truncated_svd\n","    PreprocessorSelector:truncated_svd:target_dim, Type: Constant, Value: 100\n","    ResamplingStrategySelector:over_sampling_method, Type: Categorical, Choices: {none}, Default: none\n","    ResamplingStrategySelector:target_size_strategy, Type: Categorical, Choices: {none}, Default: none\n","    ResamplingStrategySelector:under_sampling_method, Type: Categorical, Choices: {none}, Default: none\n","    TrainNode:batch_loss_computation_technique, Type: Categorical, Choices: {standard}, Default: standard\n","  Conditions:\n","    LearningrateSchedulerSelector:cosine_annealing:T_max | LearningrateSchedulerSelector:lr_scheduler == 'cosine_annealing'\n","    LearningrateSchedulerSelector:cosine_annealing:T_mult | LearningrateSchedulerSelector:lr_scheduler == 'cosine_annealing'\n","    NetworkSelector:shapedresnet:activation | NetworkSelector:network == 'shapedresnet'\n","    NetworkSelector:shapedresnet:blocks_per_group | NetworkSelector:network == 'shapedresnet'\n","    NetworkSelector:shapedresnet:max_units | NetworkSelector:network == 'shapedresnet'\n","    NetworkSelector:shapedresnet:num_groups | NetworkSelector:network == 'shapedresnet'\n","    NetworkSelector:shapedresnet:resnet_shape | NetworkSelector:network == 'shapedresnet'\n","    NetworkSelector:shapedresnet:use_dropout | NetworkSelector:network == 'shapedresnet'\n","    NetworkSelector:shapedresnet:use_shake_drop | NetworkSelector:network == 'shapedresnet'\n","    NetworkSelector:shapedresnet:use_shake_shake | NetworkSelector:network == 'shapedresnet'\n","    OptimizerSelector:sgd:learning_rate | OptimizerSelector:optimizer == 'sgd'\n","    OptimizerSelector:sgd:momentum | OptimizerSelector:optimizer == 'sgd'\n","    OptimizerSelector:sgd:weight_decay | OptimizerSelector:optimizer == 'sgd'\n","    PreprocessorSelector:truncated_svd:target_dim | PreprocessorSelector:preprocessor == 'truncated_svd'"]},"metadata":{},"execution_count":8}]},{"cell_type":"code","metadata":{"id":"tv6JRIg7DYhS"},"source":[""],"execution_count":null,"outputs":[]}]}