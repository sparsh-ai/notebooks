{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    },
    "colab": {
      "name": "8.PerformancesMeasure.ipynb",
      "provenance": [],
      "toc_visible": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OzGirwTvnRZc"
      },
      "source": [
        "# CF Part 8 - Evaluation\n",
        "> Collaborative Filtering on MovieLens Latest-small Part 8 - Evaluation of various methods\n",
        "\n",
        "- toc: true\n",
        "- badges: true\n",
        "- comments: true\n",
        "- categories: [movie, collaborative]\n",
        "- image:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hvZ1mZ4YgDzv"
      },
      "source": [
        "import os\n",
        "\n",
        "if not (os.path.exists(\"recsys.zip\") or os.path.exists(\"recsys\")):\n",
        "    !wget https://github.com/nzhinusoftcm/review-on-collaborative-filtering/raw/master/recsys.zip    \n",
        "    !unzip recsys.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P2LwPqpwgD14"
      },
      "source": [
        "### requirements\n",
        "\n",
        "```\n",
        "matplotlib==3.2.2\n",
        "numpy==1.19.2\n",
        "pandas==1.0.5\n",
        "python==3.7\n",
        "scikit-learn==0.24.1\n",
        "scikit-surprise==1.1.1\n",
        "scipy==1.6.2\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9i69DyAdgD1_"
      },
      "source": [
        "from recsys.memories.UserToUser import UserToUser\n",
        "from recsys.memories.ItemToItem import ItemToItem\n",
        "\n",
        "from recsys.models.MatrixFactorization import MF\n",
        "from recsys.models.ExplainableMF import EMF, explainable_score\n",
        "\n",
        "from recsys.preprocessing import normalized_ratings\n",
        "from recsys.preprocessing import train_test_split\n",
        "from recsys.preprocessing import rating_matrix\n",
        "from recsys.preprocessing import scale_ratings\n",
        "from recsys.preprocessing import mean_ratings\n",
        "from recsys.preprocessing import get_examples\n",
        "from recsys.preprocessing import ids_encoder\n",
        "\n",
        "from recsys.datasets import ml100k\n",
        "from recsys.datasets import ml1m\n",
        "\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "import os"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GJOZneMKgD2D"
      },
      "source": [
        "# 1. Results on MovieLens 100k"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fRPodc5bgD2E"
      },
      "source": [
        "## 1.1. User-based CF"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x3XkiZBzgD2G",
        "outputId": "c672e9b2-c062-4c23-d666-5a6bde4eca7d"
      },
      "source": [
        "# load data\n",
        "ratings, movies = ml100k.load()\n",
        "\n",
        "# prepare data\n",
        "ratings, uencoder, iencoder = ids_encoder(ratings)\n",
        "\n",
        "# get examples as tuples of userids and itemids and labels from normalize ratings\n",
        "raw_examples, raw_labels = get_examples(ratings, labels_column='rating')\n",
        "\n",
        "# train test split\n",
        "(x_train, x_test), (y_train, y_test) = train_test_split(examples=raw_examples, labels=raw_labels)\n",
        "\n",
        "# evaluate with Euclidean distance\n",
        "usertouser = UserToUser(ratings, movies, metric='euclidean')\n",
        "print(\"==========================\")\n",
        "usertouser.evaluate(x_test, y_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Normalize users ratings ...\n",
            "Initialize the similarity model ...\n",
            "Compute nearest neighbors ...\n",
            "User to user recommendation model created with success ...\n",
            "==========================\n",
            "Evaluate the model on 10000 test data ...\n",
            "\n",
            "MAE : 0.8125945111976461\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8125945111976461"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dYT2HviSgD2J",
        "outputId": "832278a0-5a87-4068-880c-ba72d25dbc06"
      },
      "source": [
        "# evaluate with cosine similarity\n",
        "usertouser = UserToUser(ratings, movies, metric='cosine')\n",
        "print(\"=========================\")\n",
        "usertouser.evaluate(x_test, y_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Normalize users ratings ...\n",
            "Initialize the similarity model ...\n",
            "Compute nearest neighbors ...\n",
            "User to user recommendation model created with success ...\n",
            "=========================\n",
            "Evaluate the model on 10000 test data ...\n",
            "\n",
            "MAE : 0.7505910931068639\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7505910931068639"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3o9VTWDPgD2T"
      },
      "source": [
        "## 1.2. Item-based CF"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f3C4IRlrgD2V",
        "outputId": "cc046005-b2f0-4b72-8a72-fb586069cc5f"
      },
      "source": [
        "# load data\n",
        "ratings, movies = ml100k.load()\n",
        "\n",
        "# prepare data\n",
        "ratings, uencoder, iencoder = ids_encoder(ratings)\n",
        "\n",
        "# get examples as tuples of userids and itemids and labels from normalize ratings\n",
        "raw_examples, raw_labels = get_examples(ratings, labels_column='rating')\n",
        "\n",
        "# train test split\n",
        "(x_train, x_test), (y_train, y_test) = train_test_split(examples=raw_examples, labels=raw_labels)\n",
        "\n",
        "# evaluation with cosine similarity\n",
        "itemtoitem = ItemToItem(ratings, movies, metric='cosine')\n",
        "print(\"==================\")\n",
        "itemtoitem.evaluate(x_test, y_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Normalize ratings ...\n",
            "Create the similarity model ...\n",
            "Compute nearest neighbors ...\n",
            "Item to item recommendation model created with success ...\n",
            "==================\n",
            "Evaluate the model on 10000 test data ...\n",
            "\n",
            "MAE : 0.507794195659005\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.507794195659005"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yd--F3UEgD2X"
      },
      "source": [
        "### Evaluation with Euclidean distance"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uaBmWUMOgD2Y",
        "outputId": "23b888b3-0fcc-48b5-9810-9e8b2957ab26"
      },
      "source": [
        "# evaluation with Euclidean distance\n",
        "itemtoitem = ItemToItem(ratings, movies, metric='euclidean')\n",
        "print(\"==================\")\n",
        "itemtoitem.evaluate(x_test, y_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Normalize ratings ...\n",
            "Create the similarity model ...\n",
            "Compute nearest neighbors ...\n",
            "Item to item recommendation model created with success ...\n",
            "==================\n",
            "Evaluate the model on 10000 test data ...\n",
            "\n",
            "MAE : 0.8277111416143341\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8277111416143341"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B3I2hmNfgD2Z"
      },
      "source": [
        "## 1.3. Matrix Factorization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EhXhkFB-gD2Z"
      },
      "source": [
        "epochs = 10"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eToztgR3gD2a",
        "outputId": "b45a0070-496f-40c0-d928-1a49d49ca03f"
      },
      "source": [
        "# load the ml100k dataset\n",
        "ratings, movies = ml100k.load()\n",
        "\n",
        "ratings, uencoder, iencoder = ids_encoder(ratings)\n",
        "\n",
        "m = ratings.userid.nunique()   # total number of users\n",
        "n = ratings.itemid.nunique()   # total number of items\n",
        "\n",
        "# get examples as tuples of userids and itemids and labels from normalize ratings\n",
        "raw_examples, raw_labels = get_examples(ratings)\n",
        "\n",
        "# train test split\n",
        "(x_train, x_test), (y_train, y_test) = train_test_split(examples=raw_examples, labels=raw_labels)\n",
        "\n",
        "# create and train the model\n",
        "mf = MF(m, n, k=10, alpha=0.01, lamb=1.5)\n",
        "\n",
        "# fit the model on the training set\n",
        "history = mf.fit(x_train, y_train, epochs=epochs, validation_data=(x_test, y_test))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training Matrix Factorization Model ...\n",
            "k=10 \t alpha=0.01 \t lambda=1.5\n",
            "epoch 1/10 - loss : 2.734 - val_loss : 2.779\n",
            "epoch 2/10 - loss : 1.764 - val_loss : 1.794\n",
            "epoch 3/10 - loss : 1.592 - val_loss : 1.614\n",
            "epoch 4/10 - loss : 1.538 - val_loss : 1.556\n",
            "epoch 5/10 - loss : 1.515 - val_loss : 1.531\n",
            "epoch 6/10 - loss : 1.503 - val_loss : 1.517\n",
            "epoch 7/10 - loss : 1.496 - val_loss : 1.509\n",
            "epoch 8/10 - loss : 1.491 - val_loss : 1.504\n",
            "epoch 9/10 - loss : 1.488 - val_loss : 1.5\n",
            "epoch 10/10 - loss : 1.486 - val_loss : 1.497\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xluvzLf0gD2d"
      },
      "source": [
        "## 1.4. Non-negative Matrix Factorization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mNqyp6nogD2i",
        "outputId": "751d3eaa-6bc2-4740-fc58-31e74801cea7"
      },
      "source": [
        "from surprise import NMF\n",
        "from surprise import Dataset\n",
        "from surprise.model_selection import cross_validate\n",
        "\n",
        "# Load the movielens-100k dataset (download it if needed).\n",
        "data = Dataset.load_builtin('ml-100k')\n",
        "\n",
        "# Use the NMF algorithm.\n",
        "nmf = NMF(n_factors=10, n_epochs=10)\n",
        "\n",
        "# Run 5-fold cross-validation and print results.\n",
        "history = cross_validate(nmf, data, measures=['MAE'], cv=5, verbose=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluating MAE of algorithm NMF on 5 split(s).\n",
            "\n",
            "                  Fold 1  Fold 2  Fold 3  Fold 4  Fold 5  Mean    Std     \n",
            "MAE (testset)     0.9615  0.9501  0.9548  0.9582  0.9675  0.9584  0.0059  \n",
            "Fit time          0.67    0.72    0.79    0.72    0.72    0.72    0.04    \n",
            "Test time         0.15    0.09    0.16    0.10    0.14    0.13    0.03    \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IKK2gUjNgD2k"
      },
      "source": [
        "## 1.5. Explainable Matrix Factorization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ld82I8nhgD2l",
        "outputId": "66e8271e-6f30-4af8-890d-706b47091511"
      },
      "source": [
        "# load data\n",
        "ratings, movies = ml100k.load()\n",
        "\n",
        "# encode users and items ids\n",
        "ratings, uencoder, iencoder = ids_encoder(ratings)\n",
        "\n",
        "users = sorted(ratings.userid.unique())\n",
        "items = sorted(ratings.itemid.unique())\n",
        "\n",
        "m = len(users)\n",
        "n = len(items)\n",
        "\n",
        "# get examples as tuples of userids and itemids and labels from normalize ratings\n",
        "raw_examples, raw_labels = get_examples(ratings)\n",
        "\n",
        "# train test split\n",
        "(x_train, x_test), (y_train, y_test) = train_test_split(examples=raw_examples, labels=raw_labels)\n",
        "\n",
        "# create the user to user model for similarity measure\n",
        "usertouser = UserToUser(ratings, movies)\n",
        "\n",
        "# compute explainable score\n",
        "W = explainable_score(usertouser, users, items)\n",
        "\n",
        "print(\"===================\")\n",
        "# initialize the model\n",
        "emf = EMF(m, n, W, alpha=0.01, beta=0.4, lamb=0.01, k=10)\n",
        "\n",
        "history = emf.fit(x_train, y_train, epochs=epochs, validation_data=(x_test, y_test))\n",
        "\n",
        "print(\"===================\")\n",
        "emf.evaluate(x_test, y_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Normalize users ratings ...\n",
            "Initialize the similarity model ...\n",
            "Compute nearest neighbors ...\n",
            "User to user recommendation model created with success ...\n",
            "Compute explainable scores ...\n",
            "===================\n",
            "Training EMF\n",
            "k=10 \t alpha=0.01 \t beta=0.4 \t lambda=0.01\n",
            "epoch 1/10 - loss : 0.922 - val_loss : 1.036\n",
            "epoch 2/10 - loss : 0.79 - val_loss : 0.873\n",
            "epoch 3/10 - loss : 0.766 - val_loss : 0.837\n",
            "epoch 4/10 - loss : 0.757 - val_loss : 0.822\n",
            "epoch 5/10 - loss : 0.753 - val_loss : 0.814\n",
            "epoch 6/10 - loss : 0.751 - val_loss : 0.808\n",
            "epoch 7/10 - loss : 0.749 - val_loss : 0.805\n",
            "epoch 8/10 - loss : 0.748 - val_loss : 0.802\n",
            "epoch 9/10 - loss : 0.746 - val_loss : 0.799\n",
            "epoch 10/10 - loss : 0.745 - val_loss : 0.797\n",
            "===================\n",
            "MAE : 0.797\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7973478247232839"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ff9GWpCDgD2o"
      },
      "source": [
        "# 3. Results on MovieLens 1M (ML-1M)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oko1CEP3gD2p"
      },
      "source": [
        "## 3.1. User-based CF"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uv2JOR4ngD2p",
        "outputId": "bef58168-43ba-48aa-bf79-b99f1e69c2f6"
      },
      "source": [
        "# load ml100k ratings\n",
        "ratings, movies = ml1m.load()\n",
        "\n",
        "# prepare data\n",
        "ratings, uencoder, iencoder = ids_encoder(ratings)\n",
        "\n",
        "# get examples as tuples of userids and itemids and labels from normalize ratings\n",
        "raw_examples, raw_labels = get_examples(ratings, labels_column='rating')\n",
        "\n",
        "# train test split\n",
        "(x_train, x_test), (y_train, y_test) = train_test_split(examples=raw_examples, labels=raw_labels)\n",
        "\n",
        "# metric : cosine\n",
        "\n",
        "# create the user-based CF\n",
        "usertouser = UserToUser(ratings, movies, k=20, metric='cosine')\n",
        "\n",
        "# evaluate the user-based CF on the ml1m test data\n",
        "print(\"==========================\")\n",
        "usertouser.evaluate(x_test, y_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Normalize users ratings ...\n",
            "Initialize the similarity model ...\n",
            "Compute nearest neighbors ...\n",
            "User to user recommendation model created with success ...\n",
            "==========================\n",
            "Evaluate the model on 100021 test data ...\n",
            "\n",
            "MAE : 0.732267005840993\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.732267005840993"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qXrhLr03gD3E",
        "outputId": "0186ba28-e514-4866-e0d1-68ccd1285fe8"
      },
      "source": [
        "# metric : euclidean\n",
        "\n",
        "# create the user-based CF\n",
        "usertouser = UserToUser(ratings, movies, k=20, metric='euclidean')\n",
        "\n",
        "# evaluate the user-based CF on the ml1m test data\n",
        "print(\"==========================\")\n",
        "usertouser.evaluate(x_test, y_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Normalize users ratings ...\n",
            "Initialize the similarity model ...\n",
            "Compute nearest neighbors ...\n",
            "User to user recommendation model created with success ...\n",
            "==========================\n",
            "Evaluate the model on 100021 test data ...\n",
            "\n",
            "MAE : 0.8069332535426615\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8069332535426615"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Aw4Zdq0gD3F"
      },
      "source": [
        "## 3.2. Item-based CF"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CnEbPSdzgD6L"
      },
      "source": [
        "### Cosine similarity"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z8SN5rHEgD6M",
        "outputId": "30a5baa2-4ce3-47c0-8a2e-4bebb0d243cf"
      },
      "source": [
        "itemtoitem = ItemToItem(ratings, movies, metric='cosine')\n",
        "print(\"==========================\")\n",
        "itemtoitem.evaluate(x_test, y_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Normalize ratings ...\n",
            "Create the similarity model ...\n",
            "Compute nearest neighbors ...\n",
            "Item to item recommendation model created with success ...\n",
            "==========================\n",
            "Evaluate the model on 100021 test data ...\n",
            "\n",
            "MAE : 0.42514728655396045\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.42514728655396045"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kMHfDnlsgD6N"
      },
      "source": [
        "### Euclidean distance"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GSddwJXFgD6O",
        "outputId": "de4bab3d-5723-434b-a71a-6241c25e2f06"
      },
      "source": [
        "itemtoitem = ItemToItem(ratings, movies, metric='euclidean')\n",
        "print(\"==========================\")\n",
        "itemtoitem.evaluate(x_test, y_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Normalize ratings ...\n",
            "Create the similarity model ...\n",
            "Compute nearest neighbors ...\n",
            "Item to item recommendation model created with success ...\n",
            "==========================\n",
            "Evaluate the model on 100021 test data ...\n",
            "\n",
            "MAE : 0.82502173206615\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.82502173206615"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VPjmbrCigD6P"
      },
      "source": [
        "## 3.3. Matrix Factorization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kLtNjj5YgD6P",
        "outputId": "ba2ba26e-ae3a-478c-b14b-fd28a5fa1c21"
      },
      "source": [
        "# load the ml1m dataset\n",
        "ratings, movies = ml1m.load()\n",
        "\n",
        "ratings, uencoder, iencoder = ids_encoder(ratings)\n",
        "\n",
        "m = ratings.userid.nunique()   # total number of users\n",
        "n = ratings.itemid.nunique()   # total number of items\n",
        "\n",
        "# get examples as tuples of userids and itemids and labels from normalize ratings\n",
        "raw_examples, raw_labels = get_examples(ratings)\n",
        "\n",
        "# train test split\n",
        "(x_train, x_test), (y_train, y_test) = train_test_split(examples=raw_examples, labels=raw_labels)\n",
        "\n",
        "# create the model\n",
        "model = MF(m, n, k=10, alpha=0.01, lamb=1.5)\n",
        "\n",
        "# fit the model on the training set\n",
        "history = model.fit(x_train, y_train, epochs=epochs, validation_data=(x_test, y_test))\n",
        "\n",
        "print(\"===================\")\n",
        "model.evaluate(x_test, y_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training Matrix Factorization Model ...\n",
            "k=10 \t alpha=0.01 \t lambda=1.5\n",
            "epoch 1/10 - loss : 1.713 - val_loss : 1.718\n",
            "epoch 2/10 - loss : 1.523 - val_loss : 1.526\n",
            "epoch 3/10 - loss : 1.496 - val_loss : 1.498\n",
            "epoch 4/10 - loss : 1.489 - val_loss : 1.489\n",
            "epoch 5/10 - loss : 1.485 - val_loss : 1.486\n",
            "epoch 6/10 - loss : 1.484 - val_loss : 1.484\n",
            "epoch 7/10 - loss : 1.483 - val_loss : 1.483\n",
            "epoch 8/10 - loss : 1.483 - val_loss : 1.483\n",
            "epoch 9/10 - loss : 1.482 - val_loss : 1.482\n",
            "epoch 10/10 - loss : 1.482 - val_loss : 1.482\n",
            "===================\n",
            "validation error : 1.482\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.4820034560467208"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Km6USAATgD6Q"
      },
      "source": [
        "## 3.4. Non-negative Matrix Factorization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mvTGQF7AgD6R",
        "outputId": "324b982a-fd24-492f-c6b2-99835d688848"
      },
      "source": [
        "from surprise import NMF\n",
        "from surprise import Dataset\n",
        "from surprise.model_selection import cross_validate\n",
        "\n",
        "# Load the movielens-100k dataset (download it if needed).\n",
        "data = Dataset.load_builtin('ml-1m')\n",
        "\n",
        "# Use the NMF algorithm.\n",
        "nmf = NMF(n_factors=10, n_epochs=10)\n",
        "\n",
        "# Run 5-fold cross-validation and print results.\n",
        "history = cross_validate(nmf, data, measures=['MAE'], cv=5, verbose=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluating MAE of algorithm NMF on 5 split(s).\n",
            "\n",
            "                  Fold 1  Fold 2  Fold 3  Fold 4  Fold 5  Mean    Std     \n",
            "MAE (testset)     0.9435  0.9456  0.9527  0.9546  0.9524  0.9498  0.0044  \n",
            "Fit time          6.35    6.51    6.52    6.52    6.55    6.49    0.07    \n",
            "Test time         1.20    1.47    1.46    1.47    1.47    1.42    0.11    \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "umKwEF4kgD6T"
      },
      "source": [
        "## 3.5. Explainable Matrix Factorization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l-M9dCdQgD6U",
        "outputId": "f3607a0f-ffc5-4565-cc79-addb9a802aa9"
      },
      "source": [
        "# load data\n",
        "ratings, movies = ml1m.load()\n",
        "\n",
        "# encode users and items ids\n",
        "ratings, uencoder, iencoder = ids_encoder(ratings)\n",
        "\n",
        "users = sorted(ratings.userid.unique())\n",
        "items = sorted(ratings.itemid.unique())\n",
        "\n",
        "m = len(users)\n",
        "n = len(items)\n",
        "\n",
        "# get examples as tuples of userids and itemids and labels from normalize ratings\n",
        "raw_examples, raw_labels = get_examples(ratings)\n",
        "\n",
        "# train test split\n",
        "(x_train, x_test), (y_train, y_test) = train_test_split(examples=raw_examples, labels=raw_labels)\n",
        "\n",
        "# create the user to user model for similarity measure\n",
        "usertouser = UserToUser(ratings, movies)\n",
        "\n",
        "# compute explainable score\n",
        "W = explainable_score(usertouser, users, items)\n",
        "\n",
        "# initialize the model\n",
        "emf = EMF(m, n, W, alpha=0.01, beta=0.4, lamb=0.01, k=10)\n",
        "\n",
        "history = emf.fit(x_train, y_train, epochs=epochs, validation_data=(x_test, y_test))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Normalize users ratings ...\n",
            "Initialize the similarity model ...\n",
            "Compute nearest neighbors ...\n",
            "User to user recommendation model created with success ...\n",
            "Compute explainable scores ...\n",
            "Training EMF\n",
            "k=10 \t alpha=0.01 \t beta=0.4 \t lambda=0.01\n",
            "epoch 1/10 - loss : 0.782 - val_loss : 0.807\n",
            "epoch 2/10 - loss : 0.762 - val_loss : 0.781\n",
            "epoch 3/10 - loss : 0.76 - val_loss : 0.775\n",
            "epoch 4/10 - loss : 0.758 - val_loss : 0.771\n",
            "epoch 5/10 - loss : 0.757 - val_loss : 0.769\n",
            "epoch 6/10 - loss : 0.756 - val_loss : 0.767\n",
            "epoch 7/10 - loss : 0.754 - val_loss : 0.764\n",
            "epoch 8/10 - loss : 0.752 - val_loss : 0.762\n",
            "epoch 9/10 - loss : 0.751 - val_loss : 0.761\n",
            "epoch 10/10 - loss : 0.75 - val_loss : 0.76\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7I741g3dgD6V"
      },
      "source": [
        "## Summary\n",
        "\n",
        "<center> <b> MAE comparison between User-based and Item-based CF </b> </center>\n",
        "\n",
        "|   Metric  | Dataset | User-based | Item-based |\n",
        "|:---------:|:-------:|:----------:|:----------:|\n",
        "| Euclidean | ML-100k |    0.81    |    0.83    |\n",
        "| Euclidean |  ML-1M  |    0.81    |    0.82    |\n",
        "|   Cosine  | ML-100k |    0.75    |    0.51    |\n",
        "|   Cosine  |  ML-1M  |    0.73    |    0.42    |\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "<center> <b> MAE comparison between MF, NMF and EMF </b> </center>\n",
        "\n",
        "|  Preprocessing  | Dataset |   MF  |   NMF  | EMF   |\n",
        "|:---------------:|:-------:|:-----:|:------:|-------|\n",
        "|     Raw data    | ML-100k | 1.497 |  0.951 | 0.797 |\n",
        "|     Raw data    |  ML-1M  | 1.482 | 0.9567 | 0.76  |\n",
        "| Normalized data | ML-100k | 0.828 |   ---  | 0.783 |\n",
        "| Normalized data |  ML-1M  | 0.825 |   ---  | 0.758 |\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZvYJxBBpgD6X"
      },
      "source": [
        "## Author\n",
        "\n",
        "[Carmel WENGA](https://www.linkedin.com/in/carmel-wenga-871876178/), <br>\n",
        "PhD student at Université de la Polynésie Française, <br> \n",
        "Applied Machine Learning Research Engineer, <br>\n",
        "[ShoppingList](https://shoppinglist.cm), NzhinuSoft."
      ]
    }
  ]
}