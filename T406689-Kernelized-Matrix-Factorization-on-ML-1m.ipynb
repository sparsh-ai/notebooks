{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"T406689 | Kernelized Matrix Factorization on ML-1m","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyN4rQbicugrJ6UXlp6EMMFi"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"vWwjDznLzXt-"},"source":["# Kernelized Matrix Factorization on ML-1m"]},{"cell_type":"markdown","metadata":{"id":"HMubLCIT8iqZ"},"source":["A kernel function allows to transform the product of the factor matrices. Kernels like the s-shaped logistic function allow to impose bounds on the prediction (e.g. one to five stars) while still being differentiable.\n","\n","The matrix factorization can be expressed as:\n","\n","$$\\hat{r}_{ui} = b_{u,i} + \\sum_{f=1}^kw_{u,f}h_{i,f}$$\n","\n","Like matrix factorization, kernel matrix factorization (KMF) uses two feature matrices that contain the features for users and items, respectively. But the interactions between the feature vector $w_u$ of a user and the feature vector $h_i$ of an item are kernelized:\n","\n","$$\\hat{r}_{ui} = a + c\\ \\cdot \\ K(w_u,h_i)$$\n","\n","The terms $a$ and $c$ are introduced to allow re-scaling the predictions. For the kernel $K : \\mathbb{R}^k × \\mathbb{R}^k → \\mathbb{R}$ one can use any of the well-known kernels like linear, polynomial, RBF, logistic etc.\n","\n","It is obvious that normal matrix factorization can be expressed with $a = b_{u,i}$ and $c = 1$ and the linear kernel $K_l$."]},{"cell_type":"markdown","metadata":{"id":"PVtlpKAOaung"},"source":["<p><center><figure><img src='_images/T406689_1.png'><figcaption>An illustration of kernelized low-rank matrix factorization for predicting the unobserved ratings. The two latent factor matrices U and V are embedded into a high-dimensional Hilbert feature space by a linear combination of kernel functions. The products of the two latent factor matrices reconstruct the rating matrix in original space nonlinearly.</figcaption></figure></center></p>"]},{"cell_type":"markdown","metadata":{"id":"pL1thLlnle-0"},"source":["## Setup"]},{"cell_type":"code","metadata":{"id":"TO9TKft_lgIL"},"source":["from abc import ABCMeta, abstractmethod\n","from typing import Any, Tuple, Union\n","\n","import os\n","import random\n","import sys\n","\n","import math\n","import numpy as np\n","import pandas as pd\n","import numba as nb\n","\n","from sklearn.metrics import mean_squared_error\n","from sklearn.base import BaseEstimator, RegressorMixin\n","from sklearn.model_selection import train_test_split"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hnhSVMtvr1Ve"},"source":["from IPython.core.interactiveshell import InteractiveShell\n","InteractiveShell.ast_node_interactivity = \"all\"\n","\n","pd.options.display.max_rows = 100\n","    \n","rand_seed = 2\n","np.random.seed(rand_seed)\n","random.seed(rand_seed)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"B7wgnEbHn1Ry"},"source":["## Utils"]},{"cell_type":"code","metadata":{"id":"bZCre4zkn2tz"},"source":["def train_update_test_split(\n","    X: pd.DataFrame, frac_new_users: float\n",") -> Tuple[pd.DataFrame, pd.Series, pd.DataFrame, pd.Series, pd.DataFrame, pd.Series]:\n","    \"\"\"\n","    Split data into 3 parts (train_initial, train_update, test_update) for testing performance of model update for new users. First, a set of new\n","    users is set and all ratings corresponding to all other users is assigned to train_initial. Then, for each new user half of their ratings are\n","    stored in train_update and half are stored in test_update. \n","    To use the three sets returned:\n","        1. Fit your model to train_update set.\n","        2. Update your model with train_update \n","        3. Calculate predictions on test_update and compare with their actual ratings\n","    Args:\n","        X (pd.DataFrame): Data frame containing columns user_id, item_id\n","        frac_new_users (float): Fraction of users to not include in train_initial\n","    Returns:\n","        X_train_initial [pd.DataFrame]: Training set user_ids and item_ids for initial model fitting\n","        y_train_initial [pd.Series]: Corresponding ratings for X_train_initial\n","        X_train_update [pd.DataFrame]: Training set user_ids and item_ids for model updating. Contains users that are not in train_initial\n","        y_train_initial [pd.Series]: Corresponding ratings for X_train_update\n","        X_test_update [pd.DataFrame]: Testing set user_ids and item_ids for model updating. Contains same users as train_update\n","        y_test_update [pd.Series]: Corresponding ratings for X_test_update\n","    \"\"\"\n","    users = X[\"user_id\"].unique()\n","\n","    # Users that won't be included in the initial training\n","    users_update = np.random.choice(\n","        users, size=round(frac_new_users * len(users)), replace=False\n","    )\n","\n","    # Initial training matrix\n","    train_initial = X.query(\"user_id not in @users_update\").sample(\n","        frac=1, replace=False\n","    )\n","\n","    # Train and test sets for updating model. For each new user split their ratings into two sets, one for update and one for test\n","    data_update = X.query(\"user_id in @users_update\")\n","    train_update, test_update = train_test_split(\n","        data_update, stratify=data_update[\"user_id\"], test_size=0.5\n","    )\n","\n","    # Split into X and y\n","    X_train_initial, y_train_initial = (\n","        train_initial[[\"user_id\", \"item_id\"]],\n","        train_initial[\"rating\"],\n","    )\n","    X_train_update, y_train_update = (\n","        train_update[[\"user_id\", \"item_id\"]],\n","        train_update[\"rating\"],\n","    )\n","    X_test_update, y_test_update = (\n","        test_update[[\"user_id\", \"item_id\"]],\n","        test_update[\"rating\"],\n","    )\n","\n","    return (\n","        X_train_initial,\n","        y_train_initial,\n","        X_train_update,\n","        y_train_update,\n","        X_test_update,\n","        y_test_update,\n","    )"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"u5KMHkjQmpER"},"source":["## Kernels"]},{"cell_type":"markdown","metadata":{"id":"gyFaAXoQm2mo"},"source":["### Linear"]},{"cell_type":"code","metadata":{"id":"cYYO-veDm2kA"},"source":["@nb.njit()\n","def kernel_linear(\n","    global_mean: float,\n","    user_bias: float,\n","    item_bias: float,\n","    user_feature_vec: np.ndarray,\n","    item_feature_vec: np.ndarray,\n",") -> float:\n","    \"\"\"\n","    Calculates result with a linear kernel which is essentially just the dot product\n","    Args:\n","        global_mean (float): Global mean\n","        user_bias (float): User bias\n","        item_bias (float): Item bias\n","        user_feature_vec (np.ndarray): Vector of user latent features \n","        item_feature_vec (np.ndarray): Vector of item latent features\n","    Returns:\n","        [float]: Linear kernel result\n","    \"\"\"\n","    result = (\n","        global_mean + item_bias + user_bias + np.dot(user_feature_vec, item_feature_vec)\n","    )\n","    return result"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"auaG0RuOnJvO"},"source":["@nb.njit()\n","def kernel_linear_sgd_update(\n","    user_id: int,\n","    item_id: int,\n","    rating: float,\n","    global_mean: float,\n","    user_biases: np.ndarray,\n","    item_biases: np.ndarray,\n","    user_features: np.ndarray,\n","    item_features: np.ndarray,\n","    lr: float,\n","    reg: float,\n","    update_user_params: bool = True,\n","    update_item_params: bool = True,\n","):\n","    \"\"\"\n","    Performs a single update using stochastic gradient descent for a linear kernel given a user and item. \n","    Similar to https://github.com/gbolmier/funk-svd and https://github.com/NicolasHug/Surprise we iterate over each factor manually for a given \n","    user/item instead of indexing by a row such as user_feature[user] since it has shown to be much faster. We have also tested with representing\n","    user_features and item_features as 1D arrays but that also is much slower. Using parallel turned on in numba gives much worse performance as well.\n","    Args:\n","        user_id (int): User id \n","        item_id (int): Item id\n","        rating (float): Rating for user and item\n","        global_mean {float} -- Global mean of all ratings\n","        user_biases {numpy array} -- User biases vector of shape (n_users, 1)\n","        item_biases {numpy array} -- Item biases vector of shape (n_items, 1)\n","        user_features {numpy array} -- Matrix P of user features of shape (n_users, n_factors)\n","        item_features {numpy array} -- Matrix Q of item features of shape (n_items, n_factors)\n","        lr (float): Learning rate alpha\n","        reg {float} -- Regularization parameter lambda for Frobenius norm\n","        update_user_params {bool} -- Whether to update user parameters or not. Default is True.\n","        update_item_params {bool} -- Whether to update item parameters or not. Default is True.\n","    \"\"\"\n","    n_factors = user_features.shape[1]\n","    user_bias = user_biases[user_id]\n","    item_bias = item_biases[item_id]\n","\n","    # Compute predicted rating\n","    rating_pred = (\n","        global_mean\n","        + item_bias\n","        + user_bias\n","        + np.dot(user_features[user_id, :], item_features[item_id, :])\n","    )\n","\n","    # Compute error\n","    error = rating_pred - rating\n","\n","    # Update bias parameters\n","    if update_user_params:\n","        user_biases[user_id] -= lr * (error + reg * user_bias)\n","\n","    if update_item_params:\n","        item_biases[item_id] -= lr * (error + reg * item_bias)\n","\n","    # Update user and item features\n","    for f in range(n_factors):\n","        user_feature_f = user_features[user_id, f]\n","        item_feature_f = item_features[item_id, f]\n","\n","        if update_user_params:\n","            user_features[user_id, f] -= lr * (\n","                error * item_feature_f + reg * user_feature_f\n","            )\n","\n","        if update_item_params:\n","            item_features[item_id, f] -= lr * (\n","                error * user_feature_f + reg * item_feature_f\n","            )\n","\n","    return"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kcOmMCHMm2hT"},"source":["### Sigmoid"]},{"cell_type":"code","metadata":{"id":"2paoITeSm2eF"},"source":["@nb.njit()\n","def sigmoid(x: float) -> float:\n","    \"\"\"\n","    Calculates sigmoid function at x\n","    Args:\n","        x (float): Input x\n","    Returns:\n","        [float]: Sigmoid at x\n","    \"\"\"\n","    result = 1 / (1 + math.exp(-x))\n","    return result"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4654SVIRnDjA"},"source":["@nb.njit()\n","def kernel_sigmoid(\n","    global_mean: float,\n","    user_bias: float,\n","    item_bias: float,\n","    user_feature_vec: np.ndarray,\n","    item_feature_vec: np.ndarray,\n","    a: float,\n","    c: float,\n","):\n","    \"\"\"\n","    Calculates result with sigmoid kernel\n","    Args:\n","        global_mean (float): Global mean\n","        user_bias (float): User bias\n","        item_bias (float): Item bias\n","        user_feature_vec (np.ndarray): Vector of user latent features\n","        item_feature_vec (np.ndarray): Vector of item latent features\n","        a (float): Rescaling parameter for a + c * K(u, i)\n","        c (float): Rescaling parameter for a + c * K(u, i)\n","    Returns:\n","        [float]: Sigmoid kernel result\n","    \"\"\"\n","    linear_sum = (\n","        global_mean + user_bias + item_bias + np.dot(user_feature_vec, item_feature_vec)\n","    )\n","    sigmoid_result = sigmoid(linear_sum)\n","    result = a + c * sigmoid_result\n","    return result"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mroVX6tRnNYr"},"source":["@nb.njit()\n","def kernel_sigmoid_sgd_update(\n","    user_id: int,\n","    item_id: int,\n","    rating: float,\n","    global_mean: float,\n","    user_biases: np.ndarray,\n","    item_biases: np.ndarray,\n","    user_features: np.ndarray,\n","    item_features: np.ndarray,\n","    lr: float,\n","    reg: float,\n","    a: float,\n","    c: float,\n","    update_user_params: bool = True,\n","    update_item_params: bool = True,\n","):\n","    \"\"\"\n","    Performs a single update using stochastic gradient descent for a sigmoid kernel given a user and item. \n","    Args:\n","        user_id (int): User id \n","        item_id (int): Item id\n","        rating (float): Rating for user and item\n","        global_mean {float} -- Global mean of all ratings\n","        user_biases {numpy array} -- User biases vector of shape (n_users, 1)\n","        item_biases {numpy array} -- Item biases vector of shape (n_items, 1)\n","        user_features {numpy array} -- Matrix P of user features of shape (n_users, n_factors)\n","        item_features {numpy array} -- Matrix Q of item features of shape (n_items, n_factors)\n","        lr (float): Learning rate alpha\n","        reg {float} -- Regularization parameter lambda for Frobenius norm\n","        a (float): Rescaling parameter for a + c * K(u, i)\n","        c (float): Rescaling parameter for a + c * K(u, i)\n","        update_user_params {bool} -- Whether to update user parameters or not. Default is True.\n","        update_item_params {bool} -- Whether to update item parameters or not. Default is True.\n","    \"\"\"\n","    n_factors = user_features.shape[1]\n","    user_bias = user_biases[user_id]\n","    item_bias = item_biases[item_id]\n","    user_feature_vec = user_features[user_id, :]\n","    item_feature_vec = item_features[item_id, :]\n","\n","    # Compute predicted rating\n","    linear_sum = (\n","        global_mean + user_bias + item_bias + np.dot(user_feature_vec, item_feature_vec)\n","    )\n","    sigmoid_result = sigmoid(linear_sum)\n","    rating_pred = a + c * sigmoid_result\n","\n","    # Compute error\n","    error = rating_pred - rating\n","\n","    # Common term shared between all partial derivatives\n","    deriv_base = (sigmoid_result ** 2) * math.exp(-linear_sum)\n","\n","    # Update bias parameters\n","    if update_user_params:\n","        opt_deriv = error * deriv_base + reg * user_bias\n","        user_biases[user_id] -= lr * opt_deriv\n","\n","    if update_item_params:\n","        opt_deriv = error * deriv_base + reg * item_bias\n","        item_biases[item_id] -= lr * opt_deriv\n","\n","    # Update user and item features\n","    for i in range(n_factors):\n","        user_feature_f = user_features[user_id, i]\n","        item_feature_f = item_features[item_id, i]\n","\n","        if update_user_params:\n","            user_feature_deriv = item_feature_f * deriv_base\n","            opt_deriv = error * user_feature_deriv + reg * user_feature_f\n","            user_features[user_id, i] -= lr * opt_deriv\n","\n","        if update_item_params:\n","            item_feature_deriv = user_feature_f * deriv_base\n","            opt_deriv = error * item_feature_deriv + reg * item_feature_f\n","            item_features[item_id, i] -= lr * opt_deriv\n","\n","    return"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qoenuG_ym2Y4"},"source":["### RBF"]},{"cell_type":"code","metadata":{"id":"G9ooHUvinGG7"},"source":["@nb.njit()\n","def kernel_rbf(\n","    user_feature_vec: np.ndarray,\n","    item_feature_vec: np.ndarray,\n","    gamma: float,\n","    a: float,\n","    c: float,\n","):\n","    \"\"\"\n","    Calculates result with Radial basis function kernel\n","    Args:\n","        user_feature_vec (np.ndarray): Vector of user latent features\n","        item_feature_vec (np.ndarray): Vector of item latent features\n","        gamma (float): Kernel coefficient\n","        a (float): Rescaling parameter for a + c * K(u, i)\n","        c (float): Rescaling parameter for a + c * K(u, i)\n","    Returns:\n","        [float]: RBF kernel result \n","    \"\"\"\n","    power = -gamma * np.sum(np.square(user_feature_vec - item_feature_vec))\n","    exp_result = math.exp(power)\n","    result = a + c * exp_result\n","    return result"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Cj-9sH6FmwgX"},"source":["@nb.njit()\n","def kernel_rbf_sgd_update(\n","    user_id: int,\n","    item_id: int,\n","    rating: float,\n","    user_features: np.ndarray,\n","    item_features: np.ndarray,\n","    lr: float,\n","    reg: float,\n","    gamma: float,\n","    a: float,\n","    c: float,\n","    update_user_params: bool = True,\n","    update_item_params: bool = True,\n","):\n","    \"\"\"\n","    Performs a single update using stochastic gradient descent for a sigmoid kernel given a user and item. \n","    Args:\n","        user_id (int): User id \n","        item_id (int): Item id\n","        rating (float): Rating for user and item\n","        user_features {numpy array} -- Matrix P of user features of shape (n_users, n_factors)\n","        item_features {numpy array} -- Matrix Q of item features of shape (n_items, n_factors)\n","        lr (float): Learning rate alpha\n","        reg {float} -- Regularization parameter lambda for Frobenius norm\n","        gamma (float): Kernel coefficient\n","        a (float): Rescaling parameter for a + c * K(u, i)\n","        c (float): Rescaling parameter for a + c * K(u, i)\n","        update_user_params {bool} -- Whether to update user parameters or not. Default is True.\n","        update_item_params {bool} -- Whether to update item parameters or not. Default is True.\n","    \"\"\"\n","    n_factors = user_features.shape[1]\n","    user_feature_vec = user_features[user_id, :]\n","    item_feature_vec = item_features[item_id, :]\n","\n","    # Compute predicted rating\n","    power = -gamma * np.sum(np.square(user_feature_vec - item_feature_vec))\n","    exp_result = math.exp(power)\n","    rating_pred = a + c * exp_result\n","\n","    # Compute error\n","    error = rating_pred - rating\n","\n","    # Common term shared between partial derivatives\n","    deriv_base = 2 * exp_result * gamma\n","\n","    # Update user and item features params\n","    for i in range(n_factors):\n","        user_feature_f = user_features[user_id, i]\n","        item_feature_f = item_features[item_id, i]\n","\n","        if update_user_params:\n","            user_feature_deriv = deriv_base * (item_feature_f - user_feature_f)\n","            opt_deriv = error * user_feature_deriv + reg * user_feature_f\n","            user_features[user_id, i] -= lr * opt_deriv\n","\n","        if update_item_params:\n","            item_feature_deriv = deriv_base * (user_feature_f - item_feature_f)\n","            opt_deriv = error * item_feature_deriv + reg * item_feature_f\n","            item_features[item_id, i] -= lr * opt_deriv\n","\n","    return"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9XbUoTramZiZ"},"source":["## Models"]},{"cell_type":"markdown","metadata":{"id":"C87yj9ygmauo"},"source":["### Abstract"]},{"cell_type":"code","metadata":{"id":"EtLevERbmE6k"},"source":["class RecommenderBase(BaseEstimator, RegressorMixin, metaclass=ABCMeta):\n","    \"\"\"\n","    Abstract base class for all recommender models.\n","    All subclasses should implement the fit() and predict() methods\n","    Arguments:\n","        min_rating {int} -- Smallest rating possible (default: {0})\n","        max_rating {int} -- Largest rating possible (default: {5})\n","        verbose {str} -- Verbosity when fitting. Values possible are 0 to not print anything, 1 to print fitting model (default: {1})\n","    Attributes:\n","        n_users {int} -- Number of users\n","        n_items {int} -- Number of items\n","        global_mean {float} -- Global mean of all ratings\n","        user_id_map {dict} -- Mapping of user ids to assigned integer ids\n","        item_id_map {dict} -- Mapping of item ids to assigned integer ids\n","        known_users {set} -- Set of known user_ids\n","        known_items {set} -- Set of known item_ids\n","    \"\"\"\n","\n","    @abstractmethod\n","    def __init__(self, min_rating: float = 0, max_rating: float = 5, verbose: int = 0):\n","        self.min_rating = min_rating\n","        self.max_rating = max_rating\n","        self.verbose = verbose\n","        return\n","\n","    @property\n","    def known_users(self):\n","        \"\"\"\n","        List of known user_ids\n","        \"\"\"\n","        return set(self.user_id_map.keys())\n","\n","    @property\n","    def known_items(self):\n","        \"\"\"\n","        List of known item_ids\n","        \"\"\"\n","        return set(self.item_id_map.keys())\n","\n","    def contains_user(self, user_id: Any) -> bool:\n","        \"\"\"\n","        Checks if model was trained on data containing given user_id\n","        Args:\n","            user_id (any): User id\n","        Returns:\n","            bool: If user_id is known\n","        \"\"\"\n","        return user_id in self.known_users\n","\n","    def contains_item(self, item_id: Any) -> bool:\n","        \"\"\"\n","        Checks if model was trained on data containing given item_id\n","        Args:\n","            item_id (any): Item id\n","        Returns:\n","            bool: If item_id is known\n","        \"\"\"\n","        return item_id in self.known_items\n","\n","    def _preprocess_data(\n","        self, X: pd.DataFrame, y: pd.Series = None, type: str = \"fit\"\n","    ) -> Union[pd.DataFrame, Tuple[pd.DataFrame, list, list]]:\n","        \"\"\"\n","        Preprocessing steps before doing fit, update or predict\n","        Arguments:\n","            X {pd.DataFrame} -- Dataframe containing columns user_id, item_id\n","            y {pd.Series} -- Series containing rating\n","            type {str} -- The type of preprocessing to do. Allowed options are ('fit', 'predict', 'update'). Defaults to 'fit'\n","        Returns:\n","            X [pd.DataFrame] -- Dataframe with columns user_id, item_id and rating\n","            known_users [list, 'on update only'] -- List containing already known users in X. Only returned for type update\n","            new_users [list, 'on update only'] -- List containing new users in X. Only returned for type update\n","        \"\"\"\n","        X = X.loc[:, [\"user_id\", \"item_id\"]]\n","\n","        if type != \"predict\":\n","            X[\"rating\"] = y\n","\n","        if type in (\"fit\", \"update\"):\n","            # Check for duplicate user-item ratings\n","            if X.duplicated(subset=[\"user_id\", \"item_id\"]).sum() != 0:\n","                raise ValueError(\"Duplicate user-item ratings in matrix\")\n","\n","            # Shuffle rows\n","            X = X.sample(frac=1, replace=False)\n","\n","        if type == \"fit\":\n","            # Create mapping of user_id and item_id to assigned integer ids\n","            user_ids = X[\"user_id\"].unique()\n","            item_ids = X[\"item_id\"].unique()\n","            self.user_id_map = {user_id: i for (i, user_id) in enumerate(user_ids)}\n","            self.item_id_map = {item_id: i for (i, item_id) in enumerate(item_ids)}\n","            self.n_users = len(user_ids)\n","            self.n_items = len(item_ids)\n","\n","        elif type == \"update\":\n","            # Keep only item ratings for which the item is already known\n","            items = self.item_id_map.keys()\n","            X = X.query(\"item_id in @items\").copy()\n","\n","            # Add information on new users\n","            new_users, known_users = [], []\n","            users = X[\"user_id\"].unique()\n","            new_user_id = max(self.user_id_map.values()) + 1\n","\n","            for user in users:\n","                if user in self.user_id_map.keys():\n","                    known_users.append(user)\n","                    continue\n","\n","                # Add to user id mapping\n","                new_users.append(user)\n","                self.user_id_map[user] = new_user_id\n","                new_user_id += 1\n","\n","        # Remap user id and item id to assigned integer ids\n","        X.loc[:, \"user_id\"] = X[\"user_id\"].map(self.user_id_map)\n","        X.loc[:, \"item_id\"] = X[\"item_id\"].map(self.item_id_map)\n","\n","        if type == \"predict\":\n","            # Replace missing mappings with -1\n","            X.fillna(-1, inplace=True)\n","\n","        if type == \"update\":\n","            return X, known_users, new_users\n","        else:\n","            return X\n","\n","    @abstractmethod\n","    def fit(self, X: pd.DataFrame, y: pd.Series):\n","        \"\"\"\n","        Fit model to given data\n","        Args:\n","            X {pandas DataFrame} -- Dataframe containing columns user_id, item_id\n","            y {pandas DataFrame} -- Series containing rating\n","        \"\"\"\n","        return self\n","\n","    @abstractmethod\n","    def predict(self, X: pd.DataFrame, bound_ratings: bool = True) -> list:\n","        \"\"\"\n","        Predict ratings for given users and items\n","        Args:\n","            X (pd.DataFrame): Dataframe containing columns user_id and item_id\n","            bound_ratings (bool): Whether to bound ratings in range [min_rating, max_rating] (default: True)\n","        Returns:\n","            list: List containing rating predictions of all user, items in same order as input X\n","        \"\"\"\n","        return []\n","\n","    def recommend(\n","        self,\n","        user: Any,\n","        amount: int = 10,\n","        items_known: list = None,\n","        include_user: bool = True,\n","        bound_ratings: bool = True,\n","    ) -> pd.DataFrame:\n","        \"\"\"\n","        Returns a DataFrame of recommendations of items for a given user sorted from highest to lowest.\n","        Args:\n","            user (any): User_id to get recommendations for (not assigned user_id from self.user_id_map)\n","            items_known (list, optional): List of items already known by user and to not be considered in recommendations. Defaults to None.\n","            include_user (bool, optional): Whether to include the user_id in the output DataFrame or not. Defaults to True.\n","            bound_ratings (bool): Whether to bound ratings in range [min_rating, max_rating] (default: True)\n","        Returns:\n","            pd.DataFrame: Recommendations DataFrame for user with columns user_id (optional), item_id, rating sorted from highest to lowest rating \n","        \"\"\"\n","        items = list(self.item_id_map.keys())\n","\n","        # If items_known is provided then filter by items that the user does not know\n","        if items_known is not None:\n","            items_known = list(items_known)\n","            items = [item for item in items if item not in items_known]\n","\n","        # Get rating predictions for given user and all unknown items\n","        items_recommend = pd.DataFrame({\"user_id\": user, \"item_id\": items})\n","        items_recommend[\"rating_pred\"] = self.predict(\n","            X=items_recommend, bound_ratings=False\n","        )\n","\n","        # Sort and keep top n items\n","        items_recommend.sort_values(by=\"rating_pred\", ascending=False, inplace=True)\n","        items_recommend = items_recommend.head(amount)\n","\n","        # Bound ratings\n","        if bound_ratings:\n","            items_recommend[\"rating_pred\"] = items_recommend[\"rating_pred\"].clip(\n","                lower=self.min_rating, upper=self.max_rating\n","            )\n","\n","        if not include_user:\n","            items_recommend.drop([\"user_id\"], axis=\"columns\", inplace=True)\n","\n","        return items_recommend"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7Zgz1b_SmdJL"},"source":["### Baseline"]},{"cell_type":"code","metadata":{"id":"uhi768Dame7m"},"source":["class BaselineModel(RecommenderBase):\n","    \"\"\"\n","    Simple model which models the user item rating as r_{ui} = \\mu + ubias_u + ibias_i which is sum of a global mean and the corresponding\n","    user and item biases. The global mean \\mu is estimated as the mean of all ratings. The other parameters to be estimated ubias and ibias \n","    are vectors of length n_users and n_items respectively. These two vectors are estimated using stochastic gradient descent on the RMSE \n","    with regularization.\n","    NOTE: Recommend method with this model will simply recommend the most popular items for every user. This model should mainly be used\n","          for estimating the explicit rating for a given user and item \n","    Arguments:\n","        method: {str} -- Method to estimate parameters. Can be one of 'sgd' or 'als' (default: {'sgd'})\n","        n_epochs {int} -- Number of epochs to train for (default: {100})\n","        reg {float} -- Lambda parameter for L2 regularization (default: {1})\n","        lr {float} -- Learning rate for gradient optimization step (default: {0.01})\n","        min_rating {int} -- Smallest rating possible (default: {0})\n","        max_rating {int} -- Largest rating possible (default: {5})\n","        verbose {str} -- Verbosity when fitting. 0 to not print anything, 1 to print fitting model (default: {1})\n","    Attributes:\n","        n_users {int} -- Number of users\n","        n_items {int} -- Number of items\n","        global_mean {float} -- Global mean of all ratings\n","        user_biases {numpy array} -- User bias vector of shape (n_users, 1)\n","        item_biases {numpy array} -- Item bias vector of shape (n_items, i)\n","        user_id_map {dict} -- Mapping of user ids to assigned integer ids\n","        item_id_map {dict} -- Mapping of item ids to assigned integer ids\n","        train_rmse {list} -- Training rmse values\n","        predictions_possible {list} -- Boolean vector of whether both user and item were known for prediction. Only available after calling predict\n","    \"\"\"\n","\n","    def __init__(\n","        self,\n","        method: str = \"sgd\",\n","        n_epochs: int = 100,\n","        reg: float = 1,\n","        lr: float = 0.01,\n","        min_rating: int = 0,\n","        max_rating: int = 5,\n","        verbose=1,\n","    ):\n","        # Check inputs\n","        if method not in (\"sgd\", \"als\"):\n","            raise ValueError('Method param must be either \"sgd\" or \"als\"')\n","\n","        super().__init__(min_rating=min_rating, max_rating=max_rating, verbose=verbose)\n","\n","        self.method = method\n","        self.n_epochs = n_epochs\n","        self.reg = reg\n","        self.lr = lr\n","        return\n","\n","    def fit(self, X: pd.DataFrame, y: pd.Series):\n","        \"\"\" \n","        Fits simple mean and bias model to given user item ratings\n","        Arguments:\n","            X {pandas DataFrame} -- Dataframe containing columns user_id, item_id\n","            y {pandas Series} -- Series containing rating\n","        \"\"\"\n","        X = self._preprocess_data(X=X, y=y, type=\"fit\")\n","        self.global_mean = X[\"rating\"].mean()\n","\n","        # Initialize parameters\n","        self.user_biases = np.zeros(self.n_users)\n","        self.item_biases = np.zeros(self.n_items)\n","\n","        # Run parameter estimation\n","        if self.method == \"sgd\":\n","            self.user_biases, self.item_biases, self.train_rmse = _sgd(\n","                X=X.to_numpy(),\n","                global_mean=self.global_mean,\n","                user_biases=self.user_biases,\n","                item_biases=self.item_biases,\n","                n_epochs=self.n_epochs,\n","                lr=self.lr,\n","                reg=self.reg,\n","                verbose=self.verbose,\n","            )\n","\n","        elif self.method == \"als\":\n","            self.user_biases, self.item_biases, self.train_rmse = _als(\n","                X=X.to_numpy(),\n","                global_mean=self.global_mean,\n","                user_biases=self.user_biases,\n","                item_biases=self.item_biases,\n","                n_epochs=self.n_epochs,\n","                reg=self.reg,\n","                verbose=self.verbose,\n","            )\n","\n","        return self\n","\n","    def predict(self, X: pd.DataFrame, bound_ratings: bool = True) -> list:\n","        \"\"\"\n","        Predict ratings for given users and items\n","        Arguments:\n","            X {pd.DataFrame} -- Dataframe containing columns user_id and item_id\n","            bound_ratings (bool): Whether to bound ratings in range [min_rating, max_rating] (default: True)\n","        Returns:\n","            predictions [list] -- List containing rating predictions of all user, items in same order as input X\n","        \"\"\"\n","        # If empty return empty list\n","        if X.shape[0] == 0:\n","            return []\n","\n","        X = self._preprocess_data(X=X, type=\"predict\")\n","\n","        # Get predictions\n","        predictions, predictions_possible = _predict(\n","            X=X.to_numpy(),\n","            global_mean=self.global_mean,\n","            min_rating=self.min_rating,\n","            max_rating=self.max_rating,\n","            user_biases=self.user_biases,\n","            item_biases=self.item_biases,\n","            bound_ratings=bound_ratings,\n","        )\n","\n","        self.predictions_possible = predictions_possible\n","\n","        return predictions\n","\n","    def update_users(\n","        self,\n","        X: pd.DataFrame,\n","        y: pd.Series,\n","        lr: float = 0.01,\n","        n_epochs: int = 20,\n","        verbose: int = 0,\n","    ):\n","        \"\"\"\n","        Update user biases vector with new/updated user-item ratings information using SGD. Only the user parameters corresponding for the\n","        new/updated users will be updated and item parameters will be left alone. \n","        \n","        Note: If updating old users then pass all user-item ratings for old users and not just modified ratings\n","        Args:\n","            X (pd.DataFrame): Dataframe containing columns user_id, item_id \n","            y (pd.Series): Series containing rating\n","            lr (float, optional): Learning rate alpha for gradient optimization step\n","            n_epochs (int, optional): Number of epochs to run SGD. Defaults to 20.\n","            verbose (int, optional): Verbosity when updating, 0 for nothing and 1 for training messages. Defaults to 0.\n","        \"\"\"\n","        X, known_users, new_users = self._preprocess_data(X=X, y=y, type=\"update\")\n","\n","        # Re-initialize user bias for old users\n","        for user in known_users:\n","            user_index = self.user_id_map[user]\n","            self.user_biases[user_index] = 0\n","\n","        # Add user bias param for new users\n","        self.user_biases = np.append(self.user_biases, np.zeros(len(new_users)))\n","\n","        # Estimate new bias parameter\n","        self.user_biases, _, self.train_rmse = _sgd(\n","            X=X.to_numpy(),\n","            global_mean=self.global_mean,\n","            user_biases=self.user_biases,\n","            item_biases=self.item_biases,\n","            n_epochs=n_epochs,\n","            lr=lr,\n","            reg=self.reg,\n","            verbose=verbose,\n","            update_item_params=False,\n","        )\n","\n","        return\n","\n","\n","@nb.njit()\n","def _calculate_rmse(\n","    X: np.ndarray, global_mean: float, user_biases: np.ndarray, item_biases: np.ndarray\n","):\n","    \"\"\"\n","    Calculates root mean squared error for given data and model parameters\n","    Args:\n","        X (np.ndarray): Matrix with columns user, item and rating\n","        global_mean (float): Global mean rating\n","        user_biases (np.ndarray): User biases vector of shape (n_users, 1)\n","        item_biases (np.ndarray): Item biases vector of shape (n_items, 1)\n","    Returns:\n","        rmse [float]: Root mean squared error\n","    \"\"\"\n","    n_ratings = X.shape[0]\n","    errors = np.zeros(n_ratings)\n","\n","    # Iterate through all user-item ratings\n","    for i in range(n_ratings):\n","        user_id, item_id, rating = int(X[i, 0]), int(X[i, 1]), X[i, 2]\n","\n","        # Calculate prediction and error\n","        pred = global_mean + user_biases[user_id] + item_biases[item_id]\n","        errors[i] = rating - pred\n","\n","    rmse = np.sqrt(np.square(errors).mean())\n","\n","    return rmse\n","\n","\n","@nb.njit()\n","def _sgd(\n","    X: np.ndarray,\n","    global_mean: float,\n","    user_biases: np.ndarray,\n","    item_biases: np.ndarray,\n","    n_epochs: int,\n","    lr: float,\n","    reg: float,\n","    verbose: int,\n","    update_user_params: bool = True,\n","    update_item_params: bool = True,\n",") -> Tuple[np.ndarray, np.ndarray, list]:\n","    \"\"\"\n","    Performs Stochastic Gradient Descent to estimate the user_biases and item_biases\n","    Arguments:\n","        X {numpy array} -- User-item rating matrix\n","        global_mean {float} -- Global mean of all ratings\n","        user_biases {numpy array} -- User biases vector of shape (n_users, 1)\n","        item_biases {numpy array} -- Item biases vector of shape (n_items, 1)\n","        n_epochs {int} -- Number of epochs to run\n","        lr {float} -- Learning rate alpha\n","        reg {float} -- Regularization parameter lambda for Frobenius norm\n","        verbose {int} -- Verbosity when fitting. 0 for nothing and 1 for printing epochs\n","        update_user_params {bool} -- Whether to update user bias parameters or not. Default is True.\n","        update_item_params {bool} -- Whether to update item bias parameters or not. Default is True.\n","    Returns:\n","        user_biases [np.ndarray] -- Updated user_biases vector\n","        item_biases [np.ndarray] -- Updated item_bases vector\n","        train_rmse -- Training rmse values\n","    \"\"\"\n","    train_rmse = []\n","\n","    for epoch in range(n_epochs):\n","        # Shuffle data before each epoch\n","        np.random.shuffle(X)\n","\n","        # Iterate through all user-item ratings\n","        for i in range(X.shape[0]):\n","            user_id, item_id, rating = int(X[i, 0]), int(X[i, 1]), X[i, 2]\n","\n","            # Compute error\n","            rating_pred = global_mean + user_biases[user_id] + item_biases[item_id]\n","            error = rating - rating_pred\n","\n","            # Update parameters\n","            if update_user_params:\n","                user_biases[user_id] += lr * (error - reg * user_biases[user_id])\n","            if update_item_params:\n","                item_biases[item_id] += lr * (error - reg * item_biases[item_id])\n","\n","        # Calculate error and print\n","        rmse = _calculate_rmse(\n","            X=X,\n","            global_mean=global_mean,\n","            user_biases=user_biases,\n","            item_biases=item_biases,\n","        )\n","        train_rmse.append(rmse)\n","\n","        if verbose == 1:\n","            print(\"Epoch \", epoch + 1, \"/\", n_epochs, \" -  train_rmse:\", rmse)\n","\n","    return user_biases, item_biases, train_rmse\n","\n","\n","@nb.njit()\n","def _als(\n","    X: np.ndarray,\n","    global_mean: float,\n","    user_biases: np.ndarray,\n","    item_biases: np.ndarray,\n","    n_epochs: int,\n","    reg: float,\n","    verbose: int,\n",") -> Tuple[np.ndarray, np.ndarray, list]:\n","    \"\"\"\n","    Performs Alternating Least Squares to estimate the user_biases and item_biases. For every epoch, the item biases are held constant while\n","    solving directly for the user biases parameters using a closed form equation. Then the user biases parameters is held constant and the same\n","    is done for the item biases. This can be derived easily and is given in the lecture here https://www.youtube.com/watch?v=gCaOa3W9kM0&t=32m55s\n","    which is also similar to the implementation in Surprise.\n","    Arguments:\n","        X {numpy array} -- User-item rating matrix\n","        global_mean {float} -- Global mean of all ratings\n","        user_biases {numpy array} -- User biases vector of shape (n_users, 1)\n","        item_biases {numpy array} -- Item biases vector of shape (n_items, 1)\n","        n_epochs {int} -- Number of epochs to run\n","        reg {float} -- Regularization parameter lambda for Frobenius norm\n","        verbose {int} -- Verbosity when fitting. 0 for nothing and 1 for printing epochs\n","    Returns:\n","        user_biases [np.ndarray] -- Updated user_biases vector\n","        item_biases [np.ndarray] -- Updated item_bases vector\n","        train_rmse -- Training rmse values\n","    \"\"\"\n","    n_users = user_biases.shape[0]\n","    n_items = item_biases.shape[0]\n","    train_rmse = []\n","\n","    # Get counts of all users and items\n","    user_counts = np.zeros(n_users)\n","    item_counts = np.zeros(n_items)\n","    for i in range(X.shape[0]):\n","        user_id, item_id = int(X[i, 0]), int(X[i, 1])\n","        user_counts[user_id] += 1\n","        item_counts[item_id] += 1\n","\n","    # For each epoch optimize User biases, and then Item biases\n","    for epoch in range(n_epochs):\n","\n","        # Update user bias parameters\n","        user_biases = np.zeros(n_users)\n","\n","        # Iterate through all user-item ratings\n","        for i in range(X.shape[0]):\n","            user_id, item_id, rating = int(X[i, 0]), int(X[i, 1]), X[i, 2]\n","            user_biases[user_id] += rating - global_mean - item_biases[item_id]\n","\n","        # Set user bias estimation\n","        user_biases = user_biases / (reg + user_counts)\n","\n","        # Update item bias parameters\n","        item_biases = np.zeros(n_items)\n","\n","        # Iterate through all user-item ratings\n","        for i in range(X.shape[0]):\n","            user_id, item_id, rating = int(X[i, 0]), int(X[i, 1]), X[i, 2]\n","            item_biases[item_id] += rating - global_mean - user_biases[user_id]\n","\n","        # Set item bias estimation\n","        item_biases = item_biases / (reg + item_counts)\n","\n","        # Calculate error and print\n","        rmse = _calculate_rmse(\n","            X=X,\n","            global_mean=global_mean,\n","            user_biases=user_biases,\n","            item_biases=item_biases,\n","        )\n","        train_rmse.append(rmse)\n","\n","        if verbose == 1:\n","            print(\"Epoch \", epoch + 1, \"/\", n_epochs, \" -  train_rmse:\", rmse)\n","\n","    return user_biases, item_biases, train_rmse\n","\n","\n","@nb.njit()\n","def _predict(\n","    X: np.ndarray,\n","    global_mean: float,\n","    min_rating: int,\n","    max_rating: int,\n","    user_biases: np.ndarray,\n","    item_biases: np.ndarray,\n","    bound_ratings: bool,\n",") -> Tuple[list, list]:\n","    \"\"\"\n","    Calculate predicted ratings for each user-item pair.\n","    Arguments:\n","        X {np.ndarray} -- Matrix with columns representing (user_id, item_id)\n","        global_mean {float} -- Global mean of all ratings\n","        min_rating {int} -- Lowest rating possible\n","        max_rating {int} -- Highest rating possible\n","        user_biases {np.ndarray} -- User biases vector of length n_users\n","        item_biases {np.ndarray} -- Item biases vector of length n_items\n","        bound_ratings {boolean} -- Whether to bound predictions in between range [min_rating, max_rating]\n","    Returns:\n","        predictions [np.ndarray] -- Vector containing rating predictions of all user, items in same order as input X\n","        predictions_possible [np.ndarray] -- Vector of whether both given user and item were contained in the data that the model was fitted on\n","    \"\"\"\n","\n","    predictions = []\n","    predictions_possible = []\n","\n","    for i in range(X.shape[0]):\n","        user_id, item_id = int(X[i, 0]), int(X[i, 1])\n","        user_known = user_id != -1\n","        item_known = item_id != -1\n","\n","        rating_pred = global_mean\n","\n","        if user_known:\n","            rating_pred += user_biases[user_id]\n","        if item_known:\n","            rating_pred += item_biases[item_id]\n","\n","        # Bound ratings to min and max rating range\n","        if bound_ratings:\n","            if rating_pred > max_rating:\n","                rating_pred = max_rating\n","            elif rating_pred < min_rating:\n","                rating_pred = min_rating\n","\n","        predictions.append(rating_pred)\n","        predictions_possible.append(user_known and item_known)\n","\n","    return predictions, predictions_possible"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"oXW_eWLemg5a"},"source":["### Kernel Matrix Factorization"]},{"cell_type":"code","metadata":{"id":"_7mSDmCNnmSg"},"source":["class KernelMF(RecommenderBase):\n","    \"\"\" \n","    Kernel Matrix Factorization. Finds the thin matrices P and Q such that P * Q^T give a good low rank approximation to the user-item \n","    ratings matrix A based on RMSE. This is different from SVD despite the name as unlike SVD there is no constraint for matrices P and Q to have mutually\n","    orthogonal columns. \n","    Arguments:\n","        n_factors {int} -- The number of latent factors in matrices P and Q (default: {100})\n","        n_epochs {int} -- Number of epochs to train for (default: {100})\n","        kernel {str} -- Kernel function to use between user and item features. Options are 'linear', 'logistic' or 'rbf'. (default: {'linear'})\n","        gamma {str or float} -- Kernel coefficient for 'rbf'. Ignored by other kernels. If 'auto' is used then will be set to 1/n_factors. (default: 'auto')\n","        reg {float} -- Regularization parameter lambda for Tikhonov regularization (default: {0.01})\n","        lr {float} -- Learning rate alpha for gradient optimization step (default: {0.01})\n","        init_mean {float} -- Mean of normal distribution to use for initializing parameters (default: {0})\n","        init_sd {float} -- Standard deviation of normal distribution to use for initializing parameters (default: {0.1})\n","        min_rating {int} -- Smallest rating possible (default: {0})\n","        max_rating {int} -- Largest rating possible (default: {5})\n","        verbose {str} -- Verbosity when fitting. Values possible are 0 to not print anything, 1 to print fitting model (default: {1})\n","    Attributes:\n","        n_users {int} -- Number of users\n","        n_items {int} -- Number of items\n","        global_mean {float} -- Global mean of all ratings\n","        user_biases {numpy array} -- User bias vector of shape (n_users, 1)\n","        item_biases {numpy array} -- Item bias vector of shape (n_items, i)\n","        user_features {numpy array} -- Decomposed P matrix of user features of shape (n_users, n_factors)\n","        item_features {numpy array} -- Decomposed Q matrix of item features of shape (n_items, n_factors)\n","        user_id_map {dict} -- Mapping of user ids to assigned integer ids\n","        item_id_map {dict} -- Mapping of item ids to assigned integer ids\n","        train_rmse -- Training rmse values\n","        predictions_possible {list} -- Boolean vector of whether both user and item were known for prediction. Only available after calling predict\n","    \"\"\"\n","\n","    def __init__(\n","        self,\n","        n_factors: int = 100,\n","        n_epochs: int = 100,\n","        kernel: str = \"linear\",\n","        gamma: Union[str, float] = \"auto\",\n","        reg: float = 1,\n","        lr: float = 0.01,\n","        init_mean: float = 0,\n","        init_sd: float = 0.1,\n","        min_rating: int = 0,\n","        max_rating: int = 5,\n","        verbose: int = 1,\n","    ):\n","        if kernel not in (\"linear\", \"sigmoid\", \"rbf\"):\n","            raise ValueError(\"Kernel must be one of linear, sigmoid, or rbf\")\n","\n","        super().__init__(min_rating=min_rating, max_rating=max_rating, verbose=verbose)\n","\n","        self.n_factors = n_factors\n","        self.n_epochs = n_epochs\n","        self.kernel = kernel\n","        self.gamma = 1 / n_factors if gamma == \"auto\" else gamma\n","        self.reg = reg\n","        self.lr = lr\n","        self.init_mean = init_mean\n","        self.init_sd = init_sd\n","        return\n","\n","    def fit(self, X: pd.DataFrame, y: pd.Series):\n","        \"\"\" \n","        Decompose user-item rating matrix into thin matrices P and Q along with user and item bias vectors\n","        Arguments:\n","            X {pandas DataFrame} -- Dataframe containing columns user_id, item_id \n","            y {pandas Series} -- Series containing ratings\n","        \"\"\"\n","        X = self._preprocess_data(X=X, y=y, type=\"fit\")\n","        self.global_mean = X[\"rating\"].mean()\n","\n","        # Initialize vector bias parameters\n","        self.user_biases = np.zeros(self.n_users)\n","        self.item_biases = np.zeros(self.n_items)\n","\n","        # Initialize latent factor parameters of matrices P and Q\n","        self.user_features = np.random.normal(\n","            self.init_mean, self.init_sd, (self.n_users, self.n_factors)\n","        )\n","        self.item_features = np.random.normal(\n","            self.init_mean, self.init_sd, (self.n_items, self.n_factors)\n","        )\n","\n","        # Perform stochastic gradient descent\n","        (\n","            self.user_features,\n","            self.item_features,\n","            self.user_biases,\n","            self.item_biases,\n","            self.train_rmse,\n","        ) = _sgd(\n","            X=X.to_numpy(),\n","            global_mean=self.global_mean,\n","            user_biases=self.user_biases,\n","            item_biases=self.item_biases,\n","            user_features=self.user_features,\n","            item_features=self.item_features,\n","            n_epochs=self.n_epochs,\n","            kernel=self.kernel,\n","            gamma=self.gamma,\n","            lr=self.lr,\n","            reg=self.reg,\n","            min_rating=self.min_rating,\n","            max_rating=self.max_rating,\n","            verbose=self.verbose,\n","        )\n","\n","        return self\n","\n","    def predict(self, X: pd.DataFrame, bound_ratings: bool = True) -> list:\n","        \"\"\"\n","        Predict ratings for given users and items\n","        Arguments:\n","            X {pd.DataFrame} -- Dataframe containing columns user_id and item_id\n","            bound_ratings (bool): Whether to bound ratings in range [min_rating, max_rating] (default: True)\n","        Returns:\n","            predictions [list] -- List containing rating predictions of all user, items in same order as input X\n","        \"\"\"\n","        # If empty return empty list\n","        if X.shape[0] == 0:\n","            return []\n","\n","        X = self._preprocess_data(X=X, type=\"predict\")\n","\n","        # Get predictions\n","        predictions, predictions_possible = _predict(\n","            X=X.to_numpy(),\n","            global_mean=self.global_mean,\n","            user_biases=self.user_biases,\n","            item_biases=self.item_biases,\n","            user_features=self.user_features,\n","            item_features=self.item_features,\n","            min_rating=self.min_rating,\n","            max_rating=self.max_rating,\n","            kernel=self.kernel,\n","            gamma=self.gamma,\n","            bound_ratings=bound_ratings,\n","        )\n","\n","        self.predictions_possible = predictions_possible\n","        return predictions\n","\n","    def update_users(\n","        self,\n","        X: pd.DataFrame,\n","        y: pd.Series,\n","        lr: float = 0.01,\n","        n_epochs: int = 20,\n","        verbose: int = 0,\n","    ):\n","        \"\"\"\n","        Update P user features matrix with new/updated user-item ratings information using SGD. Only the user parameters corresponding for the\n","        new/updated users will be updated and item parameters will be left alone.\n","        Note: If updating old users then pass all user-item ratings for old users and not just modified ratings\n","        Args:\n","            X (pd.DataFrame): Dataframe containing columns user_id, item_id \n","            y (pd.DataFrame): Series containing ratings\n","            lr (float, optional): Learning rate alpha for gradient optimization step\n","            n_epochs (int, optional): Number of epochs to run SGD. Defaults to 20.\n","            verbose (int, optional): Verbosity when updating, 0 for nothing and 1 for training messages. Defaults to 0.\n","        \"\"\"\n","        X, known_users, new_users = self._preprocess_data(X=X, y=y, type=\"update\")\n","        n_new_users = len(new_users)\n","\n","        # Re-initialize params for old users\n","        for user in known_users:\n","            user_index = self.user_id_map[user]\n","\n","            # Initialize bias\n","            self.user_biases[user_index] = 0\n","\n","            # Initialize latent factors vector\n","            self.user_features[user_index, :] = np.random.normal(\n","                self.init_mean, self.init_sd, (1, self.n_factors)\n","            )\n","\n","        # Add bias parameters for new users\n","        self.user_biases = np.append(self.user_biases, np.zeros(n_new_users))\n","\n","        # Add latent factor parameters for new users by adding rows to P matrix\n","        new_user_features = np.random.normal(\n","            self.init_mean, self.init_sd, (n_new_users, self.n_factors)\n","        )\n","        self.user_features = np.concatenate(\n","            (self.user_features, new_user_features), axis=0\n","        )\n","\n","        # Estimate new parameters\n","        (\n","            self.user_features,\n","            self.item_features,\n","            self.user_biases,\n","            self.item_biases,\n","            self.train_rmse,\n","        ) = _sgd(\n","            X=X.to_numpy(),\n","            global_mean=self.global_mean,\n","            user_biases=self.user_biases,\n","            item_biases=self.item_biases,\n","            user_features=self.user_features,\n","            item_features=self.item_features,\n","            n_epochs=n_epochs,\n","            kernel=self.kernel,\n","            gamma=self.gamma,\n","            lr=lr,\n","            reg=self.reg,\n","            min_rating=self.min_rating,\n","            max_rating=self.max_rating,\n","            verbose=verbose,\n","            update_item_params=False,\n","        )\n","\n","        return"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ElzugSHdnsU-"},"source":["@nb.njit()\n","def _calculate_rmse(\n","    X: np.ndarray,\n","    global_mean: float,\n","    user_biases: np.ndarray,\n","    item_biases: np.ndarray,\n","    user_features: np.ndarray,\n","    item_features: np.ndarray,\n","    min_rating: float,\n","    max_rating: float,\n","    kernel: str,\n","    gamma: float,\n","):\n","    \"\"\"\n","    Calculates root mean squared error for given data and model parameters\n","    Args:\n","        X (np.ndarray): Matrix with columns user, item and rating\n","        global_mean (float): Global mean rating\n","        user_biases (np.ndarray): User biases vector of shape (n_users, 1)\n","        item_biases (np.ndarray): Item biases vector of shape (n_items, 1)\n","        user_features (np.ndarray): User features matrix P of size (n_users, n_factors)\n","        item_features (np.ndarray): Item features matrix Q of size (n_items, n_factors)\n","        min_rating (float): Minimum possible rating\n","        max_rating (float): Maximum possible rating\n","        kernel (str): Kernel type. Possible options are \"linear\", \"sigmoid\" or \"rbf\" kernel\n","        gamma (float): Kernel coefficient only for \"rbf\" kernel\n","    Returns:\n","        rmse [float]: Root mean squared error\n","    \"\"\"\n","    n_ratings = X.shape[0]\n","    errors = np.zeros(n_ratings)\n","\n","    # Iterate through all user-item ratings and calculate error\n","    for i in range(n_ratings):\n","        user_id, item_id, rating = int(X[i, 0]), int(X[i, 1]), X[i, 2]\n","        user_bias = user_biases[user_id]\n","        item_bias = item_biases[item_id]\n","        user_feature_vec = user_features[user_id, :]\n","        item_feature_vec = item_features[item_id, :]\n","\n","        # Calculate predicted rating for given kernel\n","        if kernel == \"linear\":\n","            rating_pred = kernel_linear(\n","                global_mean=global_mean,\n","                user_bias=user_bias,\n","                item_bias=item_bias,\n","                user_feature_vec=user_feature_vec,\n","                item_feature_vec=item_feature_vec,\n","            )\n","\n","        elif kernel == \"sigmoid\":\n","            rating_pred = kernel_sigmoid(\n","                global_mean=global_mean,\n","                user_bias=user_bias,\n","                item_bias=item_bias,\n","                user_feature_vec=user_feature_vec,\n","                item_feature_vec=item_feature_vec,\n","                a=min_rating,\n","                c=max_rating - min_rating,\n","            )\n","\n","        elif kernel == \"rbf\":\n","            rating_pred = kernel_rbf(\n","                user_feature_vec=user_feature_vec,\n","                item_feature_vec=item_feature_vec,\n","                gamma=gamma,\n","                a=min_rating,\n","                c=max_rating - min_rating,\n","            )\n","\n","        # Calculate error\n","        errors[i] = rating - rating_pred\n","\n","    rmse = np.sqrt(np.square(errors).mean())\n","\n","    return rmse"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"l0RyTkrLnqaK"},"source":["@nb.njit()\n","def _sgd(\n","    X: np.ndarray,\n","    global_mean: float,\n","    user_biases: np.ndarray,\n","    item_biases: np.ndarray,\n","    user_features: np.ndarray,\n","    item_features: np.ndarray,\n","    n_epochs: int,\n","    kernel: str,\n","    gamma: float,\n","    lr: float,\n","    reg: float,\n","    min_rating: float,\n","    max_rating: float,\n","    verbose: int,\n","    update_user_params: bool = True,\n","    update_item_params: bool = True,\n",") -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray, list]:\n","    \"\"\"\n","    Performs stochastic gradient descent to estimate parameters.\n","    Arguments:\n","        X {numpy array} -- User-item ranking matrix\n","        global_mean {float} -- Global mean of all ratings\n","        user_biases {numpy array} -- User biases vector of shape (n_users, 1)\n","        item_biases {numpy array} -- Item biases vector of shape (n_items, 1)\n","        user_features {numpy array} -- Start matrix P of user features of shape (n_users, n_factors)\n","        item_features {numpy array} -- Start matrix Q of item features of shape (n_items, n_factors)\n","        n_epochs {int} -- Number of epochs to run\n","        kernel {str} -- Kernel function to use between user and item features. Options are 'linear', 'logistic', and 'rbf'. \n","        gamma {float} -- Kernel coefficient for 'rbf'. Ignored by other kernels. \n","        lr {float} -- Learning rate alpha\n","        reg {float} -- Regularization parameter lambda for Frobenius norm\n","        min_rating {float} -- Minimum possible rating\n","        max_fating {float} -- Maximum possible rating\n","        verbose {int} -- Verbosity when fitting. 0 for nothing and 1 for printing epochs\n","        update_user_params {bool} -- Whether to update user parameters or not. Default is True.\n","        update_item_params {bool} -- Whether to update item  parameters or not. Default is True.\n","    Returns:\n","        user_features [np.ndarray] -- Updated user_features matrix P\n","        item_features [np.ndarray] -- Updated item_features matrix Q\n","        user_biases [np.ndarray] -- Updated user_biases vector\n","        item_biases [np.ndarray] -- Updated item_bases vector\n","        train_rmse [list] -- Training rmse values\n","    \"\"\"\n","    train_rmse = []\n","\n","    for epoch in range(n_epochs):\n","        # Shuffle dataset before each epoch\n","        np.random.shuffle(X)\n","\n","        # Iterate through all user-item ratings\n","        for i in range(X.shape[0]):\n","            user_id, item_id, rating = int(X[i, 0]), int(X[i, 1]), X[i, 2]\n","\n","            if kernel == \"linear\":\n","                kernel_linear_sgd_update(\n","                    user_id=user_id,\n","                    item_id=item_id,\n","                    rating=rating,\n","                    global_mean=global_mean,\n","                    user_biases=user_biases,\n","                    item_biases=item_biases,\n","                    user_features=user_features,\n","                    item_features=item_features,\n","                    lr=lr,\n","                    reg=reg,\n","                    update_user_params=update_user_params,\n","                    update_item_params=update_item_params,\n","                )\n","\n","            elif kernel == \"sigmoid\":\n","                kernel_sigmoid_sgd_update(\n","                    user_id=user_id,\n","                    item_id=item_id,\n","                    rating=rating,\n","                    global_mean=global_mean,\n","                    user_biases=user_biases,\n","                    item_biases=item_biases,\n","                    user_features=user_features,\n","                    item_features=item_features,\n","                    lr=lr,\n","                    reg=reg,\n","                    a=min_rating,\n","                    c=max_rating - min_rating,\n","                    update_user_params=update_user_params,\n","                    update_item_params=update_item_params,\n","                )\n","\n","            elif kernel == \"rbf\":\n","                kernel_rbf_sgd_update(\n","                    user_id=user_id,\n","                    item_id=item_id,\n","                    rating=rating,\n","                    user_features=user_features,\n","                    item_features=item_features,\n","                    lr=lr,\n","                    reg=reg,\n","                    gamma=gamma,\n","                    a=min_rating,\n","                    c=max_rating - min_rating,\n","                    update_user_params=update_user_params,\n","                    update_item_params=update_item_params,\n","                )\n","\n","        # Calculate error and print\n","        rmse = _calculate_rmse(\n","            X=X,\n","            global_mean=global_mean,\n","            user_biases=user_biases,\n","            item_biases=item_biases,\n","            user_features=user_features,\n","            item_features=item_features,\n","            min_rating=min_rating,\n","            max_rating=max_rating,\n","            kernel=kernel,\n","            gamma=gamma,\n","        )\n","        train_rmse.append(rmse)\n","\n","        if verbose == 1:\n","            print(\"Epoch \", epoch + 1, \"/\", n_epochs, \" -  train_rmse:\", rmse)\n","\n","    return user_features, item_features, user_biases, item_biases, train_rmse"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9nWZZJJYnnnV"},"source":["@nb.njit()\n","def _predict(\n","    X: np.ndarray,\n","    global_mean: float,\n","    user_biases: np.ndarray,\n","    item_biases: np.ndarray,\n","    user_features: np.ndarray,\n","    item_features: np.ndarray,\n","    min_rating: int,\n","    max_rating: int,\n","    kernel: str,\n","    gamma: float,\n","    bound_ratings: bool,\n",") -> Tuple[list, list]:\n","    \"\"\" \n","    Calculate predicted ratings for each user-item pair.\n","    Arguments:\n","        X {np.ndarray} -- Matrix with columns representing (user_id, item_id)\n","        global_mean {float} -- Global mean of all ratings\n","        user_biases {np.ndarray} -- User biases vector of length n_users\n","        item_biases {np.ndarray} -- Item biases vector of length n_items\n","        user_features {np.ndarray} -- User features matrix P of shape (n_users, n_factors)\n","        item_features {np.ndarray} -- Item features matrix Q of shape (n_items, n_factors)\n","        min_rating {int} -- Lowest rating possible\n","        max_rating {int} -- Highest rating possible\n","        kernel {str} -- Kernel function. Options are 'linear', 'sigmoid', and 'rbf'\n","        gamma {float} -- Kernel coefficient for 'rbf' only\n","        bound_ratings (bool): Whether to bound ratings in range [min_rating, max_rating] (default: True)\n","    Returns:\n","        predictions [np.ndarray] -- Vector containing rating predictions of all user, items in same order as input X\n","        predictions_possible [np.ndarray] -- Vector of whether both given user and item were contained in the data that the model was fitted on\n","    \"\"\"\n","    n_factors = user_features.shape[1]\n","    predictions = []\n","    predictions_possible = []\n","\n","    for i in range(X.shape[0]):\n","        user_id, item_id = int(X[i, 0]), int(X[i, 1])\n","        user_known = user_id != -1\n","        item_known = item_id != -1\n","\n","        # Default values if user or item are not known\n","        user_bias = user_biases[user_id] if user_known else 0\n","        item_bias = item_biases[item_id] if item_known else 0\n","        user_feature_vec = (\n","            user_features[user_id, :] if user_known else np.zeros(n_factors)\n","        )\n","        item_feature_vec = (\n","            item_features[item_id, :] if item_known else np.zeros(n_factors)\n","        )\n","\n","        # Calculate predicted rating given kernel\n","        if kernel == \"linear\":\n","            rating_pred = kernel_linear(\n","                global_mean=global_mean,\n","                user_bias=user_bias,\n","                item_bias=item_bias,\n","                user_feature_vec=user_feature_vec,\n","                item_feature_vec=item_feature_vec,\n","            )\n","\n","        elif kernel == \"sigmoid\":\n","            rating_pred = kernel_sigmoid(\n","                global_mean=global_mean,\n","                user_bias=user_bias,\n","                item_bias=item_bias,\n","                user_feature_vec=user_feature_vec,\n","                item_feature_vec=item_feature_vec,\n","                a=min_rating,\n","                c=max_rating - min_rating,\n","            )\n","\n","        elif kernel == \"rbf\":\n","            rating_pred = kernel_rbf(\n","                user_feature_vec=user_feature_vec,\n","                item_feature_vec=item_feature_vec,\n","                gamma=gamma,\n","                a=min_rating,\n","                c=max_rating - min_rating,\n","            )\n","\n","        # Bound ratings to min and max rating range\n","        if bound_ratings:\n","            if rating_pred > max_rating:\n","                rating_pred = max_rating\n","            elif rating_pred < min_rating:\n","                rating_pred = min_rating\n","\n","        predictions.append(rating_pred)\n","        predictions_possible.append(user_known and item_known)\n","\n","    return predictions, predictions_possible"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qt1Kl76KnvqG"},"source":["## Data ML-1m"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1D3EhF19sAdE","executionInfo":{"status":"ok","timestamp":1635153432261,"user_tz":-330,"elapsed":7992,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}},"outputId":"ec9cab0f-6cf0-42d3-d850-9b5bbc54ddb8"},"source":["!wget -q --show-progress http://files.grouplens.org/datasets/movielens/ml-1m.zip\n","!unzip ml-1m.zip"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["ml-1m.zip.1         100%[===================>]   5.64M  18.4MB/s    in 0.3s    \n","Archive:  ml-1m.zip\n","replace ml-1m/movies.dat? [y]es, [n]o, [A]ll, [N]one, [r]ename: N\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":359},"id":"uaUaTV-IsGha","executionInfo":{"status":"ok","timestamp":1635152666909,"user_tz":-330,"elapsed":5712,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}},"outputId":"71ca7818-334b-4d89-a409-3d1774d1b1f2"},"source":["cols = ['user_id', 'item_id', 'rating', 'timestamp']\n","movie_data = pd.read_csv('ml-1m/ratings.dat', names = cols, sep = '::', usecols=[0, 1, 2], engine='python')\n","# movie_data = pd.read_csv('ml-100k/u.data', names = cols, sep = '\\t', usecols=[0, 1, 2], engine='python')\n","movie_data.head(10)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>user_id</th>\n","      <th>item_id</th>\n","      <th>rating</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1</td>\n","      <td>1193</td>\n","      <td>5</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1</td>\n","      <td>661</td>\n","      <td>3</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>1</td>\n","      <td>914</td>\n","      <td>3</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>1</td>\n","      <td>3408</td>\n","      <td>4</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>1</td>\n","      <td>2355</td>\n","      <td>5</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>1</td>\n","      <td>1197</td>\n","      <td>3</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>1</td>\n","      <td>1287</td>\n","      <td>5</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>1</td>\n","      <td>2804</td>\n","      <td>5</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>1</td>\n","      <td>594</td>\n","      <td>4</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>1</td>\n","      <td>919</td>\n","      <td>4</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   user_id  item_id  rating\n","0        1     1193       5\n","1        1      661       3\n","2        1      914       3\n","3        1     3408       4\n","4        1     2355       5\n","5        1     1197       3\n","6        1     1287       5\n","7        1     2804       5\n","8        1      594       4\n","9        1      919       4"]},"metadata":{},"execution_count":24}]},{"cell_type":"code","metadata":{"id":"JO4UTaVssYfL"},"source":["X = movie_data[['user_id', 'item_id']]\n","y = movie_data['rating']\n","\n","# Prepare data\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n","\n","# Prepare data for online learning\n","X_train_initial, y_train_initial, X_train_update, y_train_update, X_test_update, y_test_update = train_update_test_split(movie_data, frac_new_users=0.2)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oWdwcSksssXr","executionInfo":{"status":"ok","timestamp":1635153441002,"user_tz":-330,"elapsed":11,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}},"outputId":"79bb5b73-5c8c-45da-a50b-b0be87ad1b24"},"source":["X_train_initial.shape, y_train_initial.shape, X_train_update.shape, y_train_update.shape, X_test_update.shape, y_test_update.shape"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["((798307, 2), (798307,), (100951, 2), (100951,), (100951, 2), (100951,))"]},"metadata":{},"execution_count":47}]},{"cell_type":"markdown","metadata":{"id":"gnFmrwcxubwB"},"source":["## Simple model with global mean"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rfG77InKucNa","executionInfo":{"status":"ok","timestamp":1635153442946,"user_tz":-330,"elapsed":13,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}},"outputId":"0da38d39-acc5-4a78-ef6e-3d842cff15ab"},"source":["global_mean = y_train.mean()\n","pred = [global_mean for _ in range(y_test.shape[0])]\n","\n","rmse = mean_squared_error(y_test, pred, squared = False)\n","\n","print(f'\\nTest RMSE: {rmse:4f}')"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Test RMSE: 1.117284\n"]}]},{"cell_type":"markdown","metadata":{"id":"X01dldZFu19W"},"source":["## Matrix Factorization - Linear Kernel"]},{"cell_type":"markdown","metadata":{"id":"LZZXs-WbxvPk"},"source":["### Full data training"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dBsLU0w3xINd","executionInfo":{"status":"ok","timestamp":1635153935241,"user_tz":-330,"elapsed":22285,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}},"outputId":"1c8275dd-e989-4b18-a787-736c93b69a13"},"source":["%%time \n","matrix_fact = KernelMF(n_epochs = 20, n_factors = 100, verbose = 1, lr = 0.001, reg = 0.005)\n","matrix_fact.fit(X_train, y_train)\n","\n","pred = matrix_fact.predict(X_test)\n","rmse = mean_squared_error(y_test, pred, squared = False)\n","\n","print(f'\\nTest RMSE: {rmse:.4f}')"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch  1 / 20  -  train_rmse: 1.0131011486029673\n","Epoch  2 / 20  -  train_rmse: 0.9736795487732433\n","Epoch  3 / 20  -  train_rmse: 0.9524095013881799\n","Epoch  4 / 20  -  train_rmse: 0.9387445909724861\n","Epoch  5 / 20  -  train_rmse: 0.9289583196388383\n","Epoch  6 / 20  -  train_rmse: 0.9214328298894139\n","Epoch  7 / 20  -  train_rmse: 0.915341037862705\n","Epoch  8 / 20  -  train_rmse: 0.9102190628982334\n","Epoch  9 / 20  -  train_rmse: 0.9057863730502835\n","Epoch  10 / 20  -  train_rmse: 0.9018540753357193\n","Epoch  11 / 20  -  train_rmse: 0.898292334148862\n","Epoch  12 / 20  -  train_rmse: 0.8950170446830942\n","Epoch  13 / 20  -  train_rmse: 0.8919594523719323\n","Epoch  14 / 20  -  train_rmse: 0.8890615425998871\n","Epoch  15 / 20  -  train_rmse: 0.8862865092807949\n","Epoch  16 / 20  -  train_rmse: 0.8835982429738573\n","Epoch  17 / 20  -  train_rmse: 0.8809696427693479\n","Epoch  18 / 20  -  train_rmse: 0.8783815686975167\n","Epoch  19 / 20  -  train_rmse: 0.8758045779741251\n","Epoch  20 / 20  -  train_rmse: 0.8732249161101829\n","\n","Test RMSE: 0.9176\n","CPU times: user 21.7 s, sys: 111 ms, total: 21.8 s\n","Wall time: 21.8 s\n"]}]},{"cell_type":"markdown","metadata":{"id":"L4KER5kKxKAg"},"source":["### Recommend"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":359},"id":"zLsVCpUhxcUw","executionInfo":{"status":"ok","timestamp":1635153997658,"user_tz":-330,"elapsed":769,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}},"outputId":"671696c6-f699-4567-f0b2-e56f021ae82d"},"source":["user = 200\n","items_known = X_train.query('user_id == @user')['item_id']\n","matrix_fact.recommend(user=user, items_known=items_known)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>user_id</th>\n","      <th>item_id</th>\n","      <th>rating_pred</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>49</th>\n","      <td>200</td>\n","      <td>912</td>\n","      <td>4.821835</td>\n","    </tr>\n","    <tr>\n","      <th>851</th>\n","      <td>200</td>\n","      <td>745</td>\n","      <td>4.679905</td>\n","    </tr>\n","    <tr>\n","      <th>301</th>\n","      <td>200</td>\n","      <td>260</td>\n","      <td>4.676945</td>\n","    </tr>\n","    <tr>\n","      <th>441</th>\n","      <td>200</td>\n","      <td>904</td>\n","      <td>4.637362</td>\n","    </tr>\n","    <tr>\n","      <th>1612</th>\n","      <td>200</td>\n","      <td>2019</td>\n","      <td>4.620851</td>\n","    </tr>\n","    <tr>\n","      <th>1120</th>\n","      <td>200</td>\n","      <td>1212</td>\n","      <td>4.616918</td>\n","    </tr>\n","    <tr>\n","      <th>1820</th>\n","      <td>200</td>\n","      <td>1267</td>\n","      <td>4.605954</td>\n","    </tr>\n","    <tr>\n","      <th>1347</th>\n","      <td>200</td>\n","      <td>1207</td>\n","      <td>4.604608</td>\n","    </tr>\n","    <tr>\n","      <th>99</th>\n","      <td>200</td>\n","      <td>1198</td>\n","      <td>4.598912</td>\n","    </tr>\n","    <tr>\n","      <th>1335</th>\n","      <td>200</td>\n","      <td>3030</td>\n","      <td>4.579230</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["      user_id  item_id  rating_pred\n","49        200      912     4.821835\n","851       200      745     4.679905\n","301       200      260     4.676945\n","441       200      904     4.637362\n","1612      200     2019     4.620851\n","1120      200     1212     4.616918\n","1820      200     1267     4.605954\n","1347      200     1207     4.604608\n","99        200     1198     4.598912\n","1335      200     3030     4.579230"]},"metadata":{},"execution_count":55}]},{"cell_type":"markdown","metadata":{"id":"ZHcIU7whxeba"},"source":["### Updating with new users"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DjtFyPzrx2pA","executionInfo":{"status":"ok","timestamp":1635154119083,"user_tz":-330,"elapsed":16538,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}},"outputId":"d9e5725b-1f44-40ad-df05-238507a90495"},"source":["matrix_fact = KernelMF(n_epochs = 20, n_factors = 100, verbose = 1, lr = 0.001, reg = 0.005)\n","matrix_fact.fit(X_train_initial, y_train_initial)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch  1 / 20  -  train_rmse: 1.0057029101886787\n","Epoch  2 / 20  -  train_rmse: 0.9672849494021611\n","Epoch  3 / 20  -  train_rmse: 0.9471655099931005\n","Epoch  4 / 20  -  train_rmse: 0.9343278608798081\n","Epoch  5 / 20  -  train_rmse: 0.9251420196454406\n","Epoch  6 / 20  -  train_rmse: 0.9180591631938331\n","Epoch  7 / 20  -  train_rmse: 0.9123157968080329\n","Epoch  8 / 20  -  train_rmse: 0.9074664089003092\n","Epoch  9 / 20  -  train_rmse: 0.9032607033336629\n","Epoch  10 / 20  -  train_rmse: 0.8995106953390289\n","Epoch  11 / 20  -  train_rmse: 0.896102493656463\n","Epoch  12 / 20  -  train_rmse: 0.8929527874119412\n","Epoch  13 / 20  -  train_rmse: 0.8899892958081905\n","Epoch  14 / 20  -  train_rmse: 0.8871633107005222\n","Epoch  15 / 20  -  train_rmse: 0.8844340778966453\n","Epoch  16 / 20  -  train_rmse: 0.8817656325712866\n","Epoch  17 / 20  -  train_rmse: 0.8791306016548486\n","Epoch  18 / 20  -  train_rmse: 0.8765035456844505\n","Epoch  19 / 20  -  train_rmse: 0.8738643942101336\n","Epoch  20 / 20  -  train_rmse: 0.8711891353424065\n"]},{"output_type":"execute_result","data":{"text/plain":["KernelMF(gamma=0.01, init_mean=0, init_sd=0.1, kernel='linear', lr=0.001,\n","         max_rating=5, min_rating=0, n_epochs=20, n_factors=100, reg=0.005,\n","         verbose=1)"]},"metadata":{},"execution_count":56}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EOAMxA8dx4Y6","executionInfo":{"status":"ok","timestamp":1635154147527,"user_tz":-330,"elapsed":3396,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}},"outputId":"1de2e605-8a9a-455b-bbc3-9b8f0827506d"},"source":["matrix_fact.update_users(X_train_update, y_train_update, lr=0.001, n_epochs=20, verbose=1)\n","pred = matrix_fact.predict(X_test_update)\n","rmse = mean_squared_error(y_test_update, pred, squared = False)\n","\n","print(f'\\nTest RMSE: {rmse:.4f}')"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch  1 / 20  -  train_rmse: 0.975909582166629\n","Epoch  2 / 20  -  train_rmse: 0.959080812196047\n","Epoch  3 / 20  -  train_rmse: 0.948133564611734\n","Epoch  4 / 20  -  train_rmse: 0.9403638197164311\n","Epoch  5 / 20  -  train_rmse: 0.9345120235560737\n","Epoch  6 / 20  -  train_rmse: 0.9298684007475237\n","Epoch  7 / 20  -  train_rmse: 0.9260382783895372\n","Epoch  8 / 20  -  train_rmse: 0.9227785854943145\n","Epoch  9 / 20  -  train_rmse: 0.9199343827053205\n","Epoch  10 / 20  -  train_rmse: 0.9173991826401\n","Epoch  11 / 20  -  train_rmse: 0.9151056772123974\n","Epoch  12 / 20  -  train_rmse: 0.9129980219958644\n","Epoch  13 / 20  -  train_rmse: 0.9110404002105831\n","Epoch  14 / 20  -  train_rmse: 0.9092067235641966\n","Epoch  15 / 20  -  train_rmse: 0.9074766228087963\n","Epoch  16 / 20  -  train_rmse: 0.9058307409724526\n","Epoch  17 / 20  -  train_rmse: 0.9042586832955035\n","Epoch  18 / 20  -  train_rmse: 0.9027490928843112\n","Epoch  19 / 20  -  train_rmse: 0.9012950325382446\n","Epoch  20 / 20  -  train_rmse: 0.8998884725870293\n","\n","Test RMSE: 0.9265\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":359},"id":"4KtRC9mwyCei","executionInfo":{"status":"ok","timestamp":1635154265712,"user_tz":-330,"elapsed":554,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}},"outputId":"d5858442-3be6-4de8-a8b2-aff7522a9baa"},"source":["# Get recommendations\n","user = 200\n","items_known = X_train_initial.query(\"user_id == @user\")[\"item_id\"]\n","matrix_fact.recommend(user=user, items_known=items_known)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>user_id</th>\n","      <th>item_id</th>\n","      <th>rating_pred</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>1224</th>\n","      <td>200</td>\n","      <td>1212</td>\n","      <td>4.698401</td>\n","    </tr>\n","    <tr>\n","      <th>967</th>\n","      <td>200</td>\n","      <td>923</td>\n","      <td>4.664610</td>\n","    </tr>\n","    <tr>\n","      <th>646</th>\n","      <td>200</td>\n","      <td>3435</td>\n","      <td>4.662763</td>\n","    </tr>\n","    <tr>\n","      <th>87</th>\n","      <td>200</td>\n","      <td>318</td>\n","      <td>4.650573</td>\n","    </tr>\n","    <tr>\n","      <th>207</th>\n","      <td>200</td>\n","      <td>2019</td>\n","      <td>4.630115</td>\n","    </tr>\n","    <tr>\n","      <th>1068</th>\n","      <td>200</td>\n","      <td>908</td>\n","      <td>4.584827</td>\n","    </tr>\n","    <tr>\n","      <th>987</th>\n","      <td>200</td>\n","      <td>858</td>\n","      <td>4.575215</td>\n","    </tr>\n","    <tr>\n","      <th>1842</th>\n","      <td>200</td>\n","      <td>3134</td>\n","      <td>4.558520</td>\n","    </tr>\n","    <tr>\n","      <th>1409</th>\n","      <td>200</td>\n","      <td>1148</td>\n","      <td>4.548372</td>\n","    </tr>\n","    <tr>\n","      <th>267</th>\n","      <td>200</td>\n","      <td>904</td>\n","      <td>4.544163</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["      user_id  item_id  rating_pred\n","1224      200     1212     4.698401\n","967       200      923     4.664610\n","646       200     3435     4.662763\n","87        200      318     4.650573\n","207       200     2019     4.630115\n","1068      200      908     4.584827\n","987       200      858     4.575215\n","1842      200     3134     4.558520\n","1409      200     1148     4.548372\n","267       200      904     4.544163"]},"metadata":{},"execution_count":58}]},{"cell_type":"markdown","metadata":{"id":"WlIu3QFNymL2"},"source":["## Matrix Factorization - Other Kernels"]},{"cell_type":"markdown","metadata":{"id":"kY-bRDznyozL"},"source":["### Sigmoid Kernel"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JQng-fr3ytnl","executionInfo":{"status":"ok","timestamp":1635154346754,"user_tz":-330,"elapsed":20329,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}},"outputId":"aac68f47-b16f-436c-f4ba-06f065631c3f"},"source":["%%time \n","matrix_fact = KernelMF(n_epochs = 20, n_factors = 100, verbose = 1, lr = 0.01, reg = 0.005, kernel='sigmoid')\n","matrix_fact.fit(X_train, y_train)\n","\n","pred = matrix_fact.predict(X_test)\n","rmse = mean_squared_error(y_test, pred, squared = False)\n","\n","print(f'\\nTest RMSE: {rmse:.4f}')"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch  1 / 20  -  train_rmse: 1.6533814745011026\n","Epoch  2 / 20  -  train_rmse: 1.5339097750004382\n","Epoch  3 / 20  -  train_rmse: 1.3750357735637435\n","Epoch  4 / 20  -  train_rmse: 1.2598375283855168\n","Epoch  5 / 20  -  train_rmse: 1.1821943554638776\n","Epoch  6 / 20  -  train_rmse: 1.125262915614807\n","Epoch  7 / 20  -  train_rmse: 1.0816030874553515\n","Epoch  8 / 20  -  train_rmse: 1.0475367259773625\n","Epoch  9 / 20  -  train_rmse: 1.0204813724521518\n","Epoch  10 / 20  -  train_rmse: 0.9986280715886721\n","Epoch  11 / 20  -  train_rmse: 0.9806689645125596\n","Epoch  12 / 20  -  train_rmse: 0.9655707959176579\n","Epoch  13 / 20  -  train_rmse: 0.9526029726811531\n","Epoch  14 / 20  -  train_rmse: 0.9412931693032693\n","Epoch  15 / 20  -  train_rmse: 0.9312064214180387\n","Epoch  16 / 20  -  train_rmse: 0.9220252109543311\n","Epoch  17 / 20  -  train_rmse: 0.9136075188303109\n","Epoch  18 / 20  -  train_rmse: 0.905708941750815\n","Epoch  19 / 20  -  train_rmse: 0.898232635132515\n","Epoch  20 / 20  -  train_rmse: 0.8911009955365344\n","\n","Test RMSE: 0.9417\n","CPU times: user 19.7 s, sys: 128 ms, total: 19.9 s\n","Wall time: 19.8 s\n"]}]},{"cell_type":"markdown","metadata":{"id":"RprMwVQRytk0"},"source":["### RBF Kernel"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"brGx0uaeytic","executionInfo":{"status":"ok","timestamp":1635154419002,"user_tz":-330,"elapsed":30642,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}},"outputId":"719b08d0-68cd-44b5-9eb8-cc979cdbaf7e"},"source":["%%time \n","matrix_fact = KernelMF(n_epochs = 20, n_factors = 100, verbose = 1, lr = 0.5, reg = 0.005, kernel='rbf')\n","matrix_fact.fit(X_train, y_train)\n","\n","pred = matrix_fact.predict(X_test)\n","rmse = mean_squared_error(y_test, pred, squared = False)\n","\n","print(f'\\nTest RMSE: {rmse:.4f}')"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch  1 / 20  -  train_rmse: 1.0636570917207124\n","Epoch  2 / 20  -  train_rmse: 0.9851395907047712\n","Epoch  3 / 20  -  train_rmse: 0.9489006996158574\n","Epoch  4 / 20  -  train_rmse: 0.9297552998825539\n","Epoch  5 / 20  -  train_rmse: 0.9192067544420423\n","Epoch  6 / 20  -  train_rmse: 0.9144270881347965\n","Epoch  7 / 20  -  train_rmse: 0.9116770966382195\n","Epoch  8 / 20  -  train_rmse: 0.9106819948414826\n","Epoch  9 / 20  -  train_rmse: 0.909811580700564\n","Epoch  10 / 20  -  train_rmse: 0.9092809320293156\n","Epoch  11 / 20  -  train_rmse: 0.9086799364228837\n","Epoch  12 / 20  -  train_rmse: 0.9097652008543395\n","Epoch  13 / 20  -  train_rmse: 0.9089165464493334\n","Epoch  14 / 20  -  train_rmse: 0.9082873761282293\n","Epoch  15 / 20  -  train_rmse: 0.9084235459321134\n","Epoch  16 / 20  -  train_rmse: 0.9089233921257058\n","Epoch  17 / 20  -  train_rmse: 0.9095161885319636\n","Epoch  18 / 20  -  train_rmse: 0.9095021122808937\n","Epoch  19 / 20  -  train_rmse: 0.9094299427056528\n","Epoch  20 / 20  -  train_rmse: 0.9094549873196524\n","\n","Test RMSE: 0.9280\n","CPU times: user 30.2 s, sys: 188 ms, total: 30.4 s\n","Wall time: 30.2 s\n"]}]},{"cell_type":"markdown","metadata":{"id":"9GyvkRjtytgT"},"source":["## Hyperparameter Tuning"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_JDD-cZoy3Kt","executionInfo":{"status":"ok","timestamp":1635159661787,"user_tz":-330,"elapsed":5241109,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}},"outputId":"df80034a-88fa-451a-8b1c-82df11f5dc88"},"source":["from sklearn.model_selection import GridSearchCV, ParameterGrid\n","\n","param_grid = {\n","    'kernel': ['linear', 'sigmoid', 'rbf'],\n","    'n_factors': [10, 20, 50],\n","    'n_epochs': [10, 20, 50],\n","    'reg': [0, 0.005, 0.1]\n","}\n","\n","grid_search = GridSearchCV(KernelMF(verbose=0), scoring = 'neg_root_mean_squared_error', param_grid=param_grid, n_jobs=-1, cv=5, verbose=1)\n","grid_search.fit(X_train, y_train)\n","\n","grid_search.best_score_, grid_search.best_params_"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Fitting 5 folds for each of 81 candidates, totalling 405 fits\n"]},{"output_type":"stream","name":"stderr","text":["[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n","[Parallel(n_jobs=-1)]: Done  46 tasks      | elapsed:  3.5min\n","[Parallel(n_jobs=-1)]: Done 196 tasks      | elapsed: 28.6min\n","[Parallel(n_jobs=-1)]: Done 405 out of 405 | elapsed: 86.7min finished\n"]},{"output_type":"execute_result","data":{"text/plain":["GridSearchCV(cv=5, error_score=nan,\n","             estimator=KernelMF(gamma=0.01, init_mean=0, init_sd=0.1,\n","                                kernel='linear', lr=0.01, max_rating=5,\n","                                min_rating=0, n_epochs=100, n_factors=100,\n","                                reg=1, verbose=0),\n","             iid='deprecated', n_jobs=-1,\n","             param_grid={'kernel': ['linear', 'sigmoid', 'rbf'],\n","                         'n_epochs': [10, 20, 50], 'n_factors': [10, 20, 50],\n","                         'reg': [0, 0.005, 0.1]},\n","             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n","             scoring='neg_root_mean_squared_error', verbose=1)"]},"metadata":{},"execution_count":61},{"output_type":"execute_result","data":{"text/plain":["(-0.8688711942781527,\n"," {'kernel': 'linear', 'n_epochs': 50, 'n_factors': 50, 'reg': 0.1})"]},"metadata":{},"execution_count":61}]}]}