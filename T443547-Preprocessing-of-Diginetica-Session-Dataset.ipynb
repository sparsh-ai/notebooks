{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"T443547 | Preprocessing of Diginetica Session Dataset","provenance":[{"file_id":"1LSz56jLEBpmJUk_gy2ULaHlJukfljz0f","timestamp":1637921168838},{"file_id":"1WxYGuRXmw822MiDenu8uOQrmyCKedc-g","timestamp":1637917274293},{"file_id":"1buphX5uNmwXsebanVqUfOxSP1MkUBXsx","timestamp":1637912183864}],"collapsed_sections":[],"authorship_tag":"ABX9TyNwAi7LbNZzzfbYIicwmk/9"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"p9K5Gnt00PjY","executionInfo":{"status":"ok","timestamp":1637923636296,"user_tz":-330,"elapsed":3045,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}},"outputId":"99e115ed-cf93-4ff2-e5bf-748a92fe541a"},"source":["!wget -q --show-progress https://github.com/RecoHut-Datasets/diginetica/raw/main/train-item-views.csv"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\rtrain-item-views.cs   0%[                    ]       0  --.-KB/s               \rtrain-item-views.cs 100%[===================>]  40.69M   216MB/s    in 0.2s    \n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"C1czoR8h1-P1","executionInfo":{"status":"ok","timestamp":1637923637468,"user_tz":-330,"elapsed":15,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}},"outputId":"1a0dae49-6539-444a-c620-81aaa0d25c26"},"source":["!head train-item-views.csv"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["sessionId;userId;itemId;timeframe;eventdate\n","1;NA;81766;526309;2016-05-09\n","1;NA;31331;1031018;2016-05-09\n","1;NA;32118;243569;2016-05-09\n","1;NA;9654;75848;2016-05-09\n","1;NA;32627;1112408;2016-05-09\n","1;NA;33043;173912;2016-05-09\n","1;NA;12352;329870;2016-05-09\n","1;NA;35077;390072;2016-05-09\n","1;NA;36118;487369;2016-05-09\n"]}]},{"cell_type":"markdown","metadata":{"id":"sTS_iB_m1A48"},"source":["## Method 1"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":206},"id":"iADt8Lu03Mwy","executionInfo":{"status":"ok","timestamp":1637923751244,"user_tz":-330,"elapsed":1434,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}},"outputId":"62a73613-827c-445e-ebe0-3ca5be9f719b"},"source":["df = pd.read_csv('/content/train-item-views.csv', sep=';')\n","df.head()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>sessionId;userId;itemId;timeframe;eventdate</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1;NA;81766;526309;2016-05-09</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1;NA;31331;1031018;2016-05-09</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>1;NA;32118;243569;2016-05-09</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>1;NA;9654;75848;2016-05-09</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>1;NA;32627;1112408;2016-05-09</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["  sessionId;userId;itemId;timeframe;eventdate\n","0                1;NA;81766;526309;2016-05-09\n","1               1;NA;31331;1031018;2016-05-09\n","2                1;NA;32118;243569;2016-05-09\n","3                  1;NA;9654;75848;2016-05-09\n","4               1;NA;32627;1112408;2016-05-09"]},"metadata":{},"execution_count":7}]},{"cell_type":"code","metadata":{"id":"9mPmTptN08qx"},"source":["import time\n","import csv\n","import pickle\n","import operator\n","import datetime\n","import os"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"RHQ5V-Ke08oH"},"source":["class DigineticaDataset:\n","    def __init__(self, path='.'):\n","        self.path = path\n","\n","    def preprocess(self):\n","        dataset = os.path.join(self.path, 'train-item-views.csv')\n","        print(\"-- Starting @ %ss\" % datetime.datetime.now())\n","        with open(dataset, \"r\") as f:\n","            reader = csv.DictReader(f, delimiter=';')\n","            sess_clicks = {}\n","            sess_date = {}\n","            ctr = 0\n","            curid = -1\n","            curdate = None\n","            for data in reader:\n","                sessid = data['sessionId']\n","                if curdate and not curid == sessid:\n","                    date = ''\n","                    date = time.mktime(time.strptime(curdate, '%Y-%m-%d'))\n","                    sess_date[curid] = date\n","                curid = sessid\n","                item = data['itemId'], int(data['timeframe'])\n","                curdate = ''\n","                curdate = data['eventdate']\n","                if sessid in sess_clicks:\n","                    sess_clicks[sessid] += [item]\n","                else:\n","                    sess_clicks[sessid] = [item]\n","                ctr += 1\n","            date = ''\n","            date = time.mktime(time.strptime(curdate, '%Y-%m-%d'))\n","            for i in list(sess_clicks):\n","                sorted_clicks = sorted(sess_clicks[i], key=operator.itemgetter(1))\n","                sess_clicks[i] = [c[0] for c in sorted_clicks]\n","            sess_date[curid] = date\n","\n","        print(\"-- Reading data @ %ss\" % datetime.datetime.now())\n","\n","        # Filter out length 1 sessions\n","        for s in list(sess_clicks):\n","            if len(sess_clicks[s]) == 1:\n","                del sess_clicks[s]\n","                del sess_date[s]\n","\n","        # Count number of times each item appears\n","        iid_counts = {}\n","        for s in sess_clicks:\n","            seq = sess_clicks[s]\n","            for iid in seq:\n","                if iid in iid_counts:\n","                    iid_counts[iid] += 1\n","                else:\n","                    iid_counts[iid] = 1\n","\n","        sorted_counts = sorted(iid_counts.items(), key=operator.itemgetter(1))\n","\n","        length = len(sess_clicks)\n","        for s in list(sess_clicks):\n","            curseq = sess_clicks[s]\n","            filseq = list(filter(lambda i: iid_counts[i] >= 5, curseq))\n","            if len(filseq) < 2:\n","                del sess_clicks[s]\n","                del sess_date[s]\n","            else:\n","                sess_clicks[s] = filseq\n","\n","        # Split out test set based on dates\n","        dates = list(sess_date.items())\n","        maxdate = dates[0][1]\n","\n","        for _, date in dates:\n","            if maxdate < date:\n","                maxdate = date\n","\n","        # 7 days for test\n","        splitdate = 0\n","        splitdate = maxdate - 86400 * 7\n","\n","        print('Splitting date', splitdate)      # Yoochoose: ('Split date', 1411930799.0)\n","        tra_sess = filter(lambda x: x[1] < splitdate, dates)\n","        tes_sess = filter(lambda x: x[1] > splitdate, dates)\n","\n","        # Sort sessions by date\n","        tra_sess = sorted(tra_sess, key=operator.itemgetter(1))     # [(sessionId, timestamp), (), ]\n","        tes_sess = sorted(tes_sess, key=operator.itemgetter(1))     # [(sessionId, timestamp), (), ]\n","        print(len(tra_sess))    # 186670    # 7966257\n","        print(len(tes_sess))    # 15979     # 15324\n","        print(tra_sess[:3])\n","        print(tes_sess[:3])\n","        \n","        print(\"-- Splitting train set and test set @ %ss\" % datetime.datetime.now())\n","\n","        # Choosing item count >=5 gives approximately the same number of items as reported in paper\n","        item_dict = {}\n","        # Convert training sessions to sequences and renumber items to start from 1\n","        def obtian_tra():\n","            train_ids = []\n","            train_seqs = []\n","            train_dates = []\n","            item_ctr = 1\n","            for s, date in tra_sess:\n","                seq = sess_clicks[s]\n","                outseq = []\n","                for i in seq:\n","                    if i in item_dict:\n","                        outseq += [item_dict[i]]\n","                    else:\n","                        outseq += [item_ctr]\n","                        item_dict[i] = item_ctr\n","                        item_ctr += 1\n","                if len(outseq) < 2:  # Doesn't occur\n","                    continue\n","                train_ids += [s]\n","                train_dates += [date]\n","                train_seqs += [outseq]\n","            print(item_ctr)     # 43098, 37484\n","            return train_ids, train_dates, train_seqs\n","\n","\n","        # Convert test sessions to sequences, ignoring items that do not appear in training set\n","        def obtian_tes():\n","            test_ids = []\n","            test_seqs = []\n","            test_dates = []\n","            for s, date in tes_sess:\n","                seq = sess_clicks[s]\n","                outseq = []\n","                for i in seq:\n","                    if i in item_dict:\n","                        outseq += [item_dict[i]]\n","                if len(outseq) < 2:\n","                    continue\n","                test_ids += [s]\n","                test_dates += [date]\n","                test_seqs += [outseq]\n","            return test_ids, test_dates, test_seqs\n","\n","        tra_ids, tra_dates, tra_seqs = obtian_tra()\n","        tes_ids, tes_dates, tes_seqs = obtian_tes()\n","\n","        def process_seqs(iseqs, idates):\n","            out_seqs = []\n","            out_dates = []\n","            labs = []\n","            ids = []\n","            for id, seq, date in zip(range(len(iseqs)), iseqs, idates):\n","                for i in range(1, len(seq)):\n","                    tar = seq[-i]\n","                    labs += [tar]\n","                    out_seqs += [seq[:-i]]\n","                    out_dates += [date]\n","                    ids += [id]\n","            return out_seqs, out_dates, labs, ids\n","\n","        tr_seqs, tr_dates, tr_labs, tr_ids = process_seqs(tra_seqs, tra_dates)\n","        te_seqs, te_dates, te_labs, te_ids = process_seqs(tes_seqs, tes_dates)\n","        tra = (tr_seqs, tr_labs)\n","        tes = (te_seqs, te_labs)\n","        print(len(tr_seqs))\n","        print(len(te_seqs))\n","        print(tr_seqs[:3], tr_dates[:3], tr_labs[:3])\n","        print(te_seqs[:3], te_dates[:3], te_labs[:3])\n","        all = 0\n","\n","        for seq in tra_seqs:\n","            all += len(seq)\n","        for seq in tes_seqs:\n","            all += len(seq)\n","        print('avg length: ', all/(len(tra_seqs) + len(tes_seqs) * 1.0))\n","\n","\n","        pickle.dump(tra, open('train.txt', 'wb'))\n","        pickle.dump(tes, open('test.txt', 'wb'))\n","        pickle.dump(tra_seqs, open('all_train_seq.txt', 'wb'))\n","\n","        print('Done.')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WM2P5vMu1n6n","executionInfo":{"status":"ok","timestamp":1637923823771,"user_tz":-330,"elapsed":19703,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}},"outputId":"0ece8e8b-9489-48d5-c272-e36cce8f11a0"},"source":["yc_data = DigineticaDataset(path='.')\n","yc_data.preprocess()"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["-- Starting @ 2021-11-26 10:50:03.103134s\n","-- Reading data @ 2021-11-26 10:50:15.266838s\n","Splitting date 1464134400.0\n","186670\n","15979\n","[('4737', 1451606400.0), ('4741', 1451606400.0), ('4742', 1451606400.0)]\n","[('289', 1464220800.0), ('290', 1464220800.0), ('302', 1464220800.0)]\n","-- Splitting train set and test set @ 2021-11-26 10:50:17.767758s\n","43098\n","719470\n","60858\n","[[1], [3, 4], [3]] [1451606400.0, 1451606400.0, 1451606400.0] [2, 5, 4]\n","[[21553, 20071, 8762, 21566, 6381], [21553, 20071, 8762, 21566], [21553, 20071, 8762]] [1464220800.0, 1464220800.0, 1464220800.0] [21566, 6381, 21566]\n","avg length:  4.850942344040704\n","Done.\n"]}]},{"cell_type":"markdown","metadata":{"id":"vlXCHf3T14sM"},"source":["## Method 2"]},{"cell_type":"code","metadata":{"id":"YAe1mHdg3mOj"},"source":["import pandas as pd\n","import numpy as np"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2joR-cij3oSH"},"source":["def get_session_id(df, interval):\n","    df_prev = df.shift()\n","    is_new_session = (df.userId != df_prev.userId) | (\n","        df.timestamp - df_prev.timestamp > interval\n","    )\n","    session_id = is_new_session.cumsum() - 1\n","    return session_id\n","\n","\n","def group_sessions(df, interval):\n","    sessionId = get_session_id(df, interval)\n","    df = df.assign(sessionId=sessionId)\n","    return df\n","\n","\n","def filter_short_sessions(df, min_len=2):\n","    session_len = df.groupby('sessionId', sort=False).size()\n","    long_sessions = session_len[session_len >= min_len].index\n","    df_long = df[df.sessionId.isin(long_sessions)]\n","    return df_long\n","\n","\n","def filter_infreq_items(df, min_support=5):\n","    item_support = df.groupby('itemId', sort=False).size()\n","    freq_items = item_support[item_support >= min_support].index\n","    df_freq = df[df.itemId.isin(freq_items)]\n","    return df_freq\n","\n","\n","def filter_until_all_long_and_freq(df, min_len=2, min_support=5):\n","    while True:\n","        df_long = filter_short_sessions(df, min_len)\n","        df_freq = filter_infreq_items(df_long, min_support)\n","        if len(df_freq) == len(df):\n","            break\n","        df = df_freq\n","    return df\n","\n","\n","def truncate_long_sessions(df, max_len=20, is_sorted=False):\n","    if not is_sorted:\n","        df = df.sort_values(['sessionId', 'timestamp'])\n","    itemIdx = df.groupby('sessionId').cumcount()\n","    df_t = df[itemIdx < max_len]\n","    return df_t\n","\n","\n","def update_id(df, field):\n","    labels = pd.factorize(df[field])[0]\n","    kwargs = {field: labels}\n","    df = df.assign(**kwargs)\n","    return df\n","\n","\n","def remove_immediate_repeats(df):\n","    df_prev = df.shift()\n","    is_not_repeat = (df.sessionId != df_prev.sessionId) | (df.itemId != df_prev.itemId)\n","    df_no_repeat = df[is_not_repeat]\n","    return df_no_repeat\n","\n","\n","def reorder_sessions_by_endtime(df):\n","    endtime = df.groupby('sessionId', sort=False).timestamp.max()\n","    df_endtime = endtime.sort_values().reset_index()\n","    oid2nid = dict(zip(df_endtime.sessionId, df_endtime.index))\n","    sessionId_new = df.sessionId.map(oid2nid)\n","    df = df.assign(sessionId=sessionId_new)\n","    df = df.sort_values(['sessionId', 'timestamp'])\n","    return df\n","\n","\n","def keep_top_n_items(df, n):\n","    item_support = df.groupby('itemId', sort=False).size()\n","    top_items = item_support.nlargest(n).index\n","    df_top = df[df.itemId.isin(top_items)]\n","    return df_top\n","\n","\n","def split_by_time(df, timedelta):\n","    max_time = df.timestamp.max()\n","    end_time = df.groupby('sessionId').timestamp.max()\n","    split_time = max_time - timedelta\n","    train_sids = end_time[end_time < split_time].index\n","    df_train = df[df.sessionId.isin(train_sids)]\n","    df_test = df[~df.sessionId.isin(train_sids)]\n","    return df_train, df_test\n","\n","\n","def train_test_split(df, test_split=0.2):\n","    endtime = df.groupby('sessionId', sort=False).timestamp.max()\n","    endtime = endtime.sort_values()\n","    num_tests = int(len(endtime) * test_split)\n","    test_session_ids = endtime.index[-num_tests:]\n","    df_train = df[~df.sessionId.isin(test_session_ids)]\n","    df_test = df[df.sessionId.isin(test_session_ids)]\n","    return df_train, df_test\n","\n","\n","def save_sessions(df, filepath):\n","    df = reorder_sessions_by_endtime(df)\n","    sessions = df.groupby('sessionId').itemId.apply(lambda x: ','.join(map(str, x)))\n","    sessions.to_csv(filepath, sep='\\t', header=False, index=False)\n","\n","\n","def save_dataset(df_train, df_test):\n","    # filter items in test but not in train\n","    df_test = df_test[df_test.itemId.isin(df_train.itemId.unique())]\n","    df_test = filter_short_sessions(df_test)\n","\n","    print(f'No. of Clicks: {len(df_train) + len(df_test)}')\n","    print(f'No. of Items: {df_train.itemId.nunique()}')\n","\n","    # update itemId\n","    train_itemId_new, uniques = pd.factorize(df_train.itemId)\n","    df_train = df_train.assign(itemId=train_itemId_new)\n","    oid2nid = {oid: i for i, oid in enumerate(uniques)}\n","    test_itemId_new = df_test.itemId.map(oid2nid)\n","    df_test = df_test.assign(itemId=test_itemId_new)\n","\n","    print(f'saving dataset to {os.getcwd()}')\n","    save_sessions(df_train, 'train.txt')\n","    save_sessions(df_test, 'test.txt')\n","    num_items = len(uniques)\n","    with open('num_items.txt', 'w') as f:\n","        f.write(str(num_items))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"kndB2edY4ATb"},"source":["def preprocess_diginetica(csv_file):\n","    print(f'reading {csv_file}...')\n","    df = pd.read_csv(\n","        csv_file,\n","        usecols=[0, 2, 3, 4],\n","        delimiter=';',\n","        parse_dates=['eventdate'],\n","        infer_datetime_format=True,\n","    )\n","    print('start preprocessing')\n","    # timeframe (time since the first query in a session, in milliseconds)\n","    df['timestamp'] = pd.to_timedelta(df.timeframe, unit='ms') + df.eventdate\n","    df = df.drop(['eventdate', 'timeframe'], 1)\n","    df = df.sort_values(['sessionId', 'timestamp'])\n","    df = filter_short_sessions(df)\n","    df = truncate_long_sessions(df, is_sorted=True)\n","    df = filter_infreq_items(df)\n","    df = filter_short_sessions(df)\n","    df_train, df_test = split_by_time(df, pd.Timedelta(days=7))\n","    save_dataset(df_train, df_test)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Su2jIQKw4GnK","executionInfo":{"status":"ok","timestamp":1637924039014,"user_tz":-330,"elapsed":10467,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}},"outputId":"2208ccab-1afb-4f71-e334-e12ed108a3a4"},"source":["preprocess_diginetica('train-item-views.csv')"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["reading train-item-views.csv...\n","start preprocessing\n","No. of Clicks: 981620\n","No. of Items: 42596\n"]}]},{"cell_type":"markdown","metadata":{"id":"kRSd66q-v8bT"},"source":["## Method 3"]},{"cell_type":"markdown","metadata":{"id":"PkNByPwowEri"},"source":["https://github.com/kunwuz/DGTN/blob/master/preprocess/cikm16_org_prepro.py"]},{"cell_type":"code","metadata":{"id":"05rycybwtrgI"},"source":["import numpy as np\n","import pandas as pd\n","from datetime import datetime, timezone, timedelta\n","import os\n","import pickle\n","import time"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"TfaZ1Uw7twYw"},"source":["raw_path = 'train-item-views'\n","save_path = 'unaugment_data'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NSVO6oQHt_FD"},"source":["def load_data(file):\n","    print(\"Start load_data\")\n","    # load csv\n","    # data = pd.read_csv(file+'.csv', sep=';', header=0, usecols=[0, 2, 4], dtype={0: np.int32, 1: np.int64, 3: str})\n","    data = pd.read_csv(file + '.csv', sep=';', header=0, usecols=[0, 2, 3, 4],\n","                       dtype={0: np.int32, 1: np.int64, 2: str, 3: str})\n","    # specify header names\n","    # data.columns = ['SessionId', 'ItemId', 'Eventdate']\n","    data.columns = ['SessionId', 'ItemId', 'Timeframe', 'Eventdate']\n","    # convert time string to timestamp and remove the original column\n","    data['Time'] = data.Eventdate.apply(lambda x: datetime.strptime(x, '%Y-%m-%d').timestamp())\n","    print(data['Time'].min())\n","    print(data['Time'].max())\n","    del(data['Eventdate'])\n","\n","    # output\n","    data_start = datetime.fromtimestamp(data.Time.min(), timezone.utc)\n","    data_end = datetime.fromtimestamp(data.Time.max(), timezone.utc)\n","\n","    print('Loaded data set\\n\\tEvents: {}\\n\\tSessions: {}\\n\\tItems: {}\\n\\tSpan: {} / {}\\n\\n'.\n","          format(len(data), data.SessionId.nunique(), data.ItemId.nunique(),\n","                 data_start.date().isoformat(), data_end.date().isoformat()))\n","    return data"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"kXUhkQ7-uI0s"},"source":["def filter_data(data, min_item_support=5, min_session_length=2):\n","    print(\"Start filter_data\")\n","\n","    # y?\n","    session_lengths = data.groupby('SessionId').size()\n","    data = data[np.in1d(data.SessionId, session_lengths[session_lengths > 1].index)]\n","\n","    # filter item support\n","    item_supports = data.groupby('ItemId').size()\n","    data = data[np.in1d(data.ItemId, item_supports[item_supports >= min_item_support].index)]\n","\n","    # filter session length\n","    session_lengths = data.groupby('SessionId').size()\n","    data = data[np.in1d(data.SessionId, session_lengths[session_lengths >= min_session_length].index)]\n","    print(data['Time'].min())\n","    print(data['Time'].max())\n","    # output\n","    data_start = datetime.fromtimestamp(data.Time.astype(np.int64).min(), timezone.utc)\n","    data_end = datetime.fromtimestamp(data.Time.astype(np.int64).max(), timezone.utc)\n","\n","    print('Filtered data set\\n\\tEvents: {}\\n\\tSessions: {}\\n\\tItems: {}\\n\\tSpan: {} / {}\\n\\n'.\n","          format(len(data), data.SessionId.nunique(), data.ItemId.nunique(),\n","                 data_start.date().isoformat(), data_end.date().isoformat()))\n","    return data"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"yBqe_G2ZuLfY"},"source":["def split_train_test(data):\n","    print(\"Start split_train_test\")\n","    tmax = data.Time.max()\n","    session_max_times = data.groupby('SessionId').Time.max()\n","    session_train = session_max_times[session_max_times < tmax-7*86400].index\n","    session_test = session_max_times[session_max_times >= tmax-7*86400].index\n","    train = data[np.in1d(data.SessionId, session_train)]\n","    test = data[np.in1d(data.SessionId, session_test)]\n","    test = test[np.in1d(test.ItemId, train.ItemId)]\n","    tslength = test.groupby('SessionId').size()\n","    test = test[np.in1d(test.SessionId, tslength[tslength >= 2].index)]\n","\n","    print('Full train set\\n\\tEvents: {}\\n\\tSessions: {}\\n\\tItems: {}'.format(len(train), train.SessionId.nunique(), train.ItemId.nunique()))\n","    print('Test set\\n\\tEvents: {}\\n\\tSessions: {}\\n\\tItems: {}'.format(len(test), test.SessionId.nunique(), test.ItemId.nunique()))\n","\n","    return train, test"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"N02x1X30uNzZ"},"source":["def get_dict(data):\n","    print(\"Start get_dict\")\n","    item2idx = {}\n","    pop_scores = data.groupby('ItemId').size().sort_values(ascending=False)\n","    pop_scores = pop_scores / pop_scores[:1].values[0]\n","    items = pop_scores.index\n","    for idx, item in enumerate(items):\n","        item2idx[item] = idx+1\n","    return item2idx"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"sTwklY9OuRPm"},"source":["def process_seqs(seqs, shift):\n","    start = time.time()\n","    labs = []\n","    index = shift\n","    for count, seq in enumerate(seqs):\n","        index += (len(seq) - 1)\n","        labs += [index]\n","        end = time.time()\n","        print(\"\\rprocess_seqs: [%d/%d], %.2f, usetime: %fs, \" % (count, len(seqs), count/len(seqs) * 100, end - start),\n","              end='', flush=True)\n","    print(\"\\n\")\n","    return seqs, labs"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"tl6Zr-x-uSnk"},"source":["def get_sequence(data, item2idx, shift=-1):\n","    start = time.time()\n","    sess_ids = data.drop_duplicates('SessionId', 'first')\n","    print(sess_ids)\n","    sess_ids.sort_values(['Time'], inplace=True)\n","    sess_ids = sess_ids['SessionId'].unique()\n","    seqs = []\n","    for count, sess_id in enumerate(sess_ids):\n","        # seq = data[data['SessionId'].isin([sess_id])]\n","        seq = data[data['SessionId'].isin([sess_id])].sort_values(['Timeframe'])\n","        seq = seq['ItemId'].values\n","        outseq = []\n","        for i in seq:\n","            if i in item2idx:\n","                outseq += [item2idx[i]]\n","        seqs += [outseq]\n","        end = time.time()\n","        print(\"\\rGet_sequence: [%d/%d], %.2f , usetime: %fs\" % (count, len(sess_ids), count/len(sess_ids) * 100, end - start),\n","              end='', flush=True)\n","\n","    print(\"\\n\")\n","    # print(seqs)\n","    out_seqs, labs = process_seqs(seqs, shift)\n","    # print(out_seqs)\n","    # print(labs)\n","    print(len(out_seqs), len(labs))\n","    return out_seqs, labs"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"FrdiDVTcuWyD"},"source":["def preprocess(train, test, path=save_path):\n","    print(\"--------------\")\n","    print(\"Start preprocess cikm16\")\n","    # print(\"Start preprocess sample\")\n","    item2idx = get_dict(train)\n","    train_seqs, train_labs = get_sequence(train, item2idx)\n","    test_seqs, test_labs = get_sequence(test, item2idx, train_labs[-1])\n","    train = (train_seqs, train_labs)\n","    test = (test_seqs, test_labs)\n","    path = path + '/cikm16'\n","    # path = path + '/sample'\n","    if not os.path.exists(path):\n","        os.makedirs(path)\n","\n","    pickle.dump(test, open(path+'/unaug_test.txt', 'wb'))\n","    pickle.dump(train, open(path+'/unaug_train.txt', 'wb'))\n","    print(\"finished\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tl9tQIl0uhF_","outputId":"1080cc91-77c3-4c84-e2ae-96f5a4759e45"},"source":["data = load_data(raw_path)\n","data = filter_data(data)\n","train, test = split_train_test(data)\n","preprocess(train, test)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Start load_data\n","1451606400.0\n","1464739200.0\n","Loaded data set\n","\tEvents: 1235380\n","\tSessions: 310324\n","\tItems: 122993\n","\tSpan: 2016-01-01 / 2016-06-01\n","\n","\n","Start filter_data\n"]}]},{"cell_type":"markdown","metadata":{"id":"ajD8rxjf4s8g"},"source":["---"]},{"cell_type":"code","metadata":{"id":"QnwX1Dsl4s8j"},"source":["!pip install -q watermark\n","%reload_ext watermark\n","%watermark -a \"Sparsh A.\" -m -iv -u -t -d"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mqBQEAS14s8k"},"source":["---"]},{"cell_type":"markdown","metadata":{"id":"NxoXCcpd4s8k"},"source":["**END**"]}]}