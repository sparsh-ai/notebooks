{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"T625255 | DQN on the CartPole and Atari using Coach","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyO+nnpXWeKTcldEiyEFTl2E"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"ywbetM9kk5XQ"},"source":["# DQN on the CartPole and Atari using Coach"]},{"cell_type":"markdown","metadata":{"id":"c1_k7c_gk_zM"},"source":["The Cartpole environment is a popular simple environment with a continuous state space and a discrete action space. Nervana Systems coach provides a simple interface to experiment with a variety of algorithms and environments. In this workshop you will use coach to train an agent to balance a pole."]},{"cell_type":"markdown","metadata":{"id":"vgySGucdl911"},"source":["## Setup"]},{"cell_type":"markdown","metadata":{"id":"q9vFtLX5l5v8"},"source":["### Installations"]},{"cell_type":"code","metadata":{"id":"sa0FY3SFpfI2","executionInfo":{"status":"ok","timestamp":1634464041566,"user_tz":-330,"elapsed":773,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}}},"source":["import minio"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"m-Yq7j2rpgkE","executionInfo":{"status":"ok","timestamp":1634464048538,"user_tz":-330,"elapsed":749,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}},"outputId":"fccf643c-8f1a-4947-b5c2-68d3b87e3a96"},"source":["minio.__version__"],"execution_count":4,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'7.1.1'"]},"metadata":{},"execution_count":4}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gRAKqVcdpLZV","executionInfo":{"status":"ok","timestamp":1634463975138,"user_tz":-330,"elapsed":1282,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}},"outputId":"7a47299f-3425-4d8b-f62a-fcc7e9287c0f"},"source":["%tensorflow_version 1.x"],"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["TensorFlow 1.x selected.\n"]}]},{"cell_type":"code","metadata":{"id":"Z3X1433Ql9TQ"},"source":["!sudo -E apt-get install python3-pip cmake zlib1g-dev python3-tk python-opencv -y\n","!sudo -E apt-get install libboost-all-dev -y\n","!sudo -E apt-get install libblas-dev liblapack-dev libatlas-base-dev gfortran -y\n","!sudo -E apt-get install libsdl-dev libsdl-image1.2-dev libsdl-mixer1.2-dev libsdl-ttf2.0-dev\n","!libsmpeg-dev libportmidi-dev libavformat-dev libswscale-dev -y\n","!sudo -E apt-get install dpkg-dev build-essential python3.5-dev libjpeg-dev  libtiff-dev libsdl1.2-dev libnotify-dev \n","!freeglut3 freeglut3-dev libsm-dev libgtk2.0-dev libgtk-3-dev libwebkitgtk-dev libgtk-3-dev libwebkitgtk-3.0-dev\n","!libgstreamer-plugins-base1.0-dev -y\n","!sudo -E apt-get install libav-tools libsdl2-dev swig cmake -y\n","!pip install rl_coach"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"N8c06puFl9Cs"},"source":["!sudo -E apt-get install python3-pip cmake zlib1g-dev python3-tk python-opencv -y\n","!sudo -E apt-get install libboost-all-dev -y\n","!sudo -E apt-get install libblas-dev liblapack-dev libatlas-base-dev gfortran -y\n","!sudo -E apt-get install libsdl-dev libsdl-image1.2-dev libsdl-mixer1.2-dev libsdl-ttf2.0-dev libsmpeg-dev libportmidi-dev libavformat-dev libswscale-dev -y\n","!sudo -E apt-get install dpkg-dev build-essential python3.5-dev libjpeg-dev  libtiff-dev libsdl1.2-dev libnotify-dev freeglut3 freeglut3-dev libsm-dev libgtk2.0-dev libgtk-3-dev libwebkitgtk-dev libgtk-3-dev libwebkitgtk-3.0-dev libgstreamer-plugins-base1.0-dev -y\n","!sudo -E apt-get install libav-tools libsdl2-dev swig cmake -y\n","!pip install setuptools==41.4.0\n","!pip install rl-coach==1.0.1 gym==0.12.5\n","!pip install gym[atari]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"eVcNBfS2l5tE"},"source":["### Imports"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":510},"id":"wNgSWZTxm4aB","executionInfo":{"status":"error","timestamp":1634463980032,"user_tz":-330,"elapsed":830,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}},"outputId":"b1d82be4-664f-4522-903e-62dd4d74d0a3"},"source":["import math\n","import random\n","from collections import defaultdict\n","from typing import Union\n","import numpy as np\n","\n","from rl_coach.agents.agent import Agent\n","from rl_coach.base_parameters import AgentParameters, AlgorithmParameters\n","from rl_coach.core_types import ActionInfo, EnvironmentSteps\n","from rl_coach.exploration_policies.e_greedy import EGreedyParameters\n","from rl_coach.memories.non_episodic.experience_replay import ExperienceReplayParameters\n","from rl_coach.core_types import EnvironmentEpisodes, EnvironmentSteps\n","from rl_coach.environments.gym_environment import GymVectorEnvironment\n","from rl_coach.filters.filter import InputFilter\n","from rl_coach.filters.observation.observation_crop_filter import ObservationCropFilter\n","from rl_coach.graph_managers.basic_rl_graph_manager import BasicRLGraphManager\n","from rl_coach.graph_managers.graph_manager import ScheduleParameters\n","from rl_coach.schedules import ConstantSchedule\n","from rl_coach.agents.agent import Agent\n","from rl_coach.base_parameters import AlgorithmParameters, AgentParameters\n","from rl_coach.core_types import ActionInfo\n","from rl_coach.exploration_policies.e_greedy import EGreedyParameters\n","from rl_coach.memories.non_episodic.experience_replay import ExperienceReplayParameters\n","from rl_coach.core_types import EnvironmentEpisodes, EnvironmentSteps\n","from rl_coach.environments.gym_environment import GymVectorEnvironment\n","from rl_coach.graph_managers.basic_rl_graph_manager import BasicRLGraphManager\n","from rl_coach.graph_managers.graph_manager import ScheduleParameters\n","from rl_coach.agents.dqn_agent import DQNAgentParameters\n","from rl_coach.base_parameters import VisualizationParameters\n","from rl_coach.core_types import EnvironmentEpisodes, EnvironmentSteps\n","from rl_coach.environments.gym_environment import GymVectorEnvironment\n","from rl_coach.graph_managers.basic_rl_graph_manager import BasicRLGraphManager\n","from rl_coach.graph_managers.graph_manager import ScheduleParameters\n","from rl_coach.memories.memory import MemoryGranularity\n","from rl_coach.schedules import ConstantSchedule\n","\n","from CustomObservationFilters import ObservationRoundingFilter, ObservationScalingFilter"],"execution_count":2,"outputs":[{"output_type":"error","ename":"ImportError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)","\u001b[0;32m<ipython-input-2-bc5020016819>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mrl_coach\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemories\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnon_episodic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperience_replay\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mExperienceReplayParameters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mrl_coach\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore_types\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mEnvironmentEpisodes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEnvironmentSteps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mrl_coach\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menvironments\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgym_environment\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mGymVectorEnvironment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mrl_coach\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilters\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilter\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mInputFilter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mrl_coach\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilters\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobservation_crop_filter\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mObservationCropFilter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/rl_coach/environments/gym_environment.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndimage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mrl_coach\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph_managers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph_manager\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mScheduleParameters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mrl_coach\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlower_under_to_upper\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshort_dynamic_import\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/rl_coach/graph_managers/graph_manager.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mrl_coach\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaver\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSaverCollection\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mrl_coach\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mset_cpu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_shell_command_and_wait\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mrl_coach\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_stores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_store_impl\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_data_store\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mdata_store_creator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mrl_coach\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemories\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemory_impl\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_memory_backend\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mrl_coach\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_stores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_store\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSyncFiles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/rl_coach/data_stores/data_store_impl.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mrl_coach\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_stores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnfs_data_store\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mNFSDataStore\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNFSDataStoreParameters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mrl_coach\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_stores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ms3_data_store\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mS3DataStore\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mS3DataStoreParameters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m from rl_coach.data_stores.redis_data_store import (\n\u001b[1;32m     21\u001b[0m     \u001b[0mRedisDataStore\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/rl_coach/data_stores/s3_data_store.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mrl_coach\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_stores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheckpoint_data_store\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCheckpointDataStore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mminio\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMinio\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mminio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mResponseError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mconfigparser\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mConfigParser\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mrl_coach\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheckpoint\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCheckpointStateFile\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mImportError\u001b[0m: cannot import name 'ResponseError' from 'minio.error' (/usr/local/lib/python3.7/dist-packages/minio/error.py)","","\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"],"errorDetails":{"actions":[{"action":"open_url","actionText":"Open Examples","url":"/notebooks/snippets/importing_libraries.ipynb"}]}}]},{"cell_type":"markdown","metadata":{"id":"fPaA_9SJl3P0"},"source":["## Environment\n","The environment simulates balancing a pole on a cart. The agent can nudge the cart left or right; these are the actions. It represents the state with a position on the x-axis, the velocity of the cart, the velocity of the tip of the pole and the angle of the pole (0° is straight up). The agent receives a reward of 1 for every step taken. The episode ends when the pole angle is more than ±12°, the cart position is more than ±2.4 (the edge of the display) or the episode length is greater than 200 steps. To solve the environment you need an average reward greater than or equal to 195 over 100 consecutive trials.\n","\n","## Coach Presets\n","Coach has a concept of presets which are settings for algorithms that are known to work.\n","\n","## DQN Preset\n","The CartPole_DQN preset has a solution to solve the CartPole environment with a DQN. I took this preset and made a few alterations to leave the following parameters:\n","\n","- It copies the target weights to the online weights every 100 steps of the environment\n","- The discount factor is set to 0.99\n","- The maximum size of the memory is 40,000 experiences\n","- It uses a constant greedy schedule of 0.05 (to make the plots consistent)\n","- The NN uses a mean-squared error (MSE) based loss, rather than the default Huber loss.\n","- No environment “warmup” to pre-populate the memory (to obtain a result from the beginning)\n","\n","You can see the full DQN preset I used below:"]},{"cell_type":"code","metadata":{"id":"kVvPU_KPlarw"},"source":["# dqn_preset.py\n","# Adapted from https://github.com/NervanaSystems/coach/blob/master/rl_coach/presets/CartPole_DQN.py\n","\n","####################\n","# Graph Scheduling #\n","####################\n","\n","schedule_params = ScheduleParameters()\n","schedule_params.improve_steps = EnvironmentEpisodes(200)\n","schedule_params.steps_between_evaluation_periods = EnvironmentEpisodes(10)\n","schedule_params.evaluation_steps = EnvironmentEpisodes(1)\n","schedule_params.heatup_steps = EnvironmentSteps(0)\n","\n","#########\n","# Agent #\n","#########\n","agent_params = DQNAgentParameters()\n","\n","# DQN params\n","agent_params.algorithm.num_steps_between_copying_online_weights_to_target = EnvironmentSteps(\n","    100)\n","agent_params.algorithm.discount = 0.99\n","agent_params.algorithm.num_consecutive_playing_steps = EnvironmentSteps(1)\n","\n","# NN configuration\n","agent_params.network_wrappers['main'].learning_rate = 0.00025\n","agent_params.network_wrappers['main'].replace_mse_with_huber_loss = False\n","\n","# ER size\n","agent_params.memory.max_size = (MemoryGranularity.Transitions, 40000)\n","\n","# E-Greedy schedule\n","agent_params.exploration.epsilon_schedule = ConstantSchedule(0.05)\n","\n","################\n","#  Environment #\n","################\n","env_params = GymVectorEnvironment(level='CartPole-v0')\n","\n","graph_manager = BasicRLGraphManager(agent_params=agent_params, env_params=env_params,\n","                                    schedule_params=schedule_params, vis_params=VisualizationParameters())"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"FLAXgPFombHS"},"source":["### Q-Learning Agent and Preset\n","Coach doesn’t have a basic Q-learning algorithm or preset, so I implemented my own. You can see the code below."]},{"cell_type":"code","metadata":{"id":"N34-YiIOmzs4"},"source":["class QLearningAlgorithmParameters(AlgorithmParameters):\n","    def __init__(self):\n","        super().__init__()\n","        self.discount = 0.99\n","        self.num_consecutive_playing_steps = EnvironmentSteps(1)\n","\n","\n","class QLearningAgentParameters(AgentParameters):\n","    def __init__(self, default_q=0, alpha=0.1):\n","        super().__init__(algorithm=QLearningAlgorithmParameters(),\n","                         exploration=EGreedyParameters(),\n","                         memory=ExperienceReplayParameters(),\n","                         networks={})\n","        self.default_q = default_q\n","        self.alpha = alpha\n","\n","    @property\n","    def path(self):\n","        return 'q_learning_agent:QLearningAgent'\n","\n","\n","class QLearningAgent(Agent):\n","    def __init__(self, agent_parameters,\n","                 parent: Union['LevelManager', 'CompositeAgent'] = None):\n","        super().__init__(agent_parameters, parent)\n","        self.default_q = self.ap.default_q\n","        self.q_func = defaultdict(lambda: defaultdict(lambda: self.default_q))\n","\n","    def train(self) -> float:\n","        loss = 0\n","        if self._should_train():\n","            # Required: State, action, reward\n","            transition = self.current_episode_buffer.get_last_transition()\n","            if transition is None:\n","                return loss\n","            state = tuple(transition.state[\"observation\"])\n","            action = transition.action\n","            reward = transition.reward\n","            actions_q_values = self.get_all_q_values_for_states(\n","                transition.next_state)\n","            max_q_next_state = np.max(actions_q_values)\n","            delta = (reward + self.ap.algorithm.discount *\n","                     max_q_next_state - self.q_func[state][action])\n","            self.q_func[state][action] += self.ap.alpha * delta\n","\n","            # Coach want's me to return the total training loss, but we're not\n","            # really training. Instead, I will return the TD error.\n","            loss = np.abs(delta)\n","        return loss\n","\n","    def get_all_q_values_for_states(self, state):\n","        # This is almost a replica of the ValueIterationAgent. Probably could\n","        # be refactored to use that.\n","        state = tuple(state[\"observation\"])\n","        l = np.array([self.q_func[state][a]\n","                      for a in self.spaces.action.actions])\n","        # Add a little random noise to all q_values to prevent ties\n","        # See https://github.com/NervanaSystems/coach/issues/414\n","        l = l + np.random.normal(loc=0, scale=0.000000001, size=l.shape)\n","        return l\n","\n","    def choose_action(self, curr_state):\n","        actions_q_values = self.get_all_q_values_for_states(curr_state)\n","        action, action_probabilities = self.exploration_policy.get_action(\n","            actions_q_values)\n","        action_info = ActionInfo(action=action,\n","                                 action_value=actions_q_values[action],\n","                                 max_action_value=np.max(actions_q_values),\n","                                 all_action_probabilities=action_probabilities)\n","        return action_info"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"URKopX1qms9r"},"source":["Then the preset looks like:"]},{"cell_type":"code","metadata":{"id":"uX2l7NRtmu9E"},"source":["####################\n","# Graph Scheduling #\n","####################\n","\n","schedule_params = ScheduleParameters()\n","schedule_params.improve_steps = EnvironmentEpisodes(200)\n","schedule_params.steps_between_evaluation_periods = EnvironmentEpisodes(10)\n","schedule_params.evaluation_steps = EnvironmentEpisodes(1)\n","schedule_params.heatup_steps = EnvironmentSteps(0)\n","\n","#########\n","# Agent #\n","#########\n","agent_params = QLearningAgentParameters(alpha=0.5)\n","agent_params.algorithm.discount = 0.99\n","\n","# Simplify the observations. I want to only use the angle and angular velocity.\n","# And I want to place the continuous observations into bins. This is achieved\n","# by multiplying by 10 and rounding to an integer. This limits the total number\n","# of states to about 150.\n","agent_params.input_filter = InputFilter()\n","agent_params.input_filter.add_observation_filter(\n","    \"observation\",\n","    \"cropping\",\n","    ObservationCropFilter(crop_low=np.array([2]), crop_high=np.array([4])),\n",")\n","agent_params.input_filter.add_observation_filter(\n","    \"observation\", \"scaling\", ObservationScalingFilter(10.0)\n",")\n","\n","agent_params.input_filter.add_observation_filter(\n","    \"observation\", \"rounding\", ObservationRoundingFilter()\n",")\n","\n","# E-Greedy schedule\n","agent_params.exploration.epsilon_schedule = ConstantSchedule(0.05)\n","\n","################\n","#  Environment #\n","################\n","env_params = GymVectorEnvironment(level=\"CartPole-v0\")\n","\n","graph_manager = BasicRLGraphManager(\n","    agent_params=agent_params, env_params=env_params, schedule_params=schedule_params\n",")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3jAx1oECmkpD"},"source":["### Random Agent and Preset\n","To provide a baseline for the other algorithms, I implemented a random agent and preset because coach doesn’t provide one out of the box."]},{"cell_type":"code","metadata":{"id":"SGWbfRCPml1e"},"source":["class RandomAgentParameters(AgentParameters):\n","    def __init__(self):\n","        super().__init__(algorithm=AlgorithmParameters(),\n","                         exploration=EGreedyParameters(),\n","                         memory=ExperienceReplayParameters(),\n","                         networks={})\n","\n","    @property\n","    def path(self):\n","        return 'random_agent:RandomAgent'\n","\n","\n","class RandomAgent(Agent):\n","    def __init__(self, agent_parameters,\n","                 parent: Union['LevelManager', 'CompositeAgent'] = None):\n","        super().__init__(agent_parameters, parent)\n","\n","    def train(self):\n","        return 0\n","\n","    def choose_action(self, curr_state):\n","        action_info = ActionInfo(\n","            action=self.exploration_policy.action_space.sample())\n","        return action_info"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"JTY9G2RGmc7y"},"source":["And the preset:"]},{"cell_type":"code","metadata":{"id":"O8DC62AvmeBW"},"source":["####################\n","# Graph Scheduling #\n","####################\n","\n","schedule_params = ScheduleParameters()\n","schedule_params.improve_steps = EnvironmentEpisodes(200)\n","schedule_params.steps_between_evaluation_periods = EnvironmentEpisodes(200)\n","schedule_params.evaluation_steps = EnvironmentEpisodes(1)\n","schedule_params.heatup_steps = EnvironmentSteps(0)\n","\n","#########\n","# Agent #\n","#########\n","agent_params = RandomAgentParameters()\n","\n","################\n","#  Environment #\n","################\n","env_params = GymVectorEnvironment(level='CartPole-v0')\n","graph_manager = BasicRLGraphManager(agent_params=agent_params, env_params=env_params,\n","                                    schedule_params=schedule_params)"],"execution_count":null,"outputs":[]}]}