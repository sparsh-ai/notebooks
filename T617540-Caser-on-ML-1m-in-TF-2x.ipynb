{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"T617540 | Caser on ML-1m in TF 2.x","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyOXSGI/ojPwiEBQrjnL46dR"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"id":"cafo0FHzZf96"},"source":["!pip install tensorflow==2.5.0"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dlmK8Cv3ZqRU","executionInfo":{"status":"ok","timestamp":1637060541417,"user_tz":-330,"elapsed":1441,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}},"outputId":"48365c23-c151-4416-e8d9-2d1efab9e47e"},"source":["!wget -q --show-progress https://files.grouplens.org/datasets/movielens/ml-1m.zip\n","!unzip ml-1m.zip"],"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["ml-1m.zip           100%[===================>]   5.64M  10.8MB/s    in 0.5s    \n","Archive:  ml-1m.zip\n","   creating: ml-1m/\n","  inflating: ml-1m/movies.dat        \n","  inflating: ml-1m/ratings.dat       \n","  inflating: ml-1m/README            \n","  inflating: ml-1m/users.dat         \n"]}]},{"cell_type":"code","metadata":{"id":"4pyUJOqTZoA-","executionInfo":{"status":"ok","timestamp":1637060562489,"user_tz":-330,"elapsed":2421,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}}},"source":["import pandas as pd\n","import numpy as np\n","import random\n","from tqdm import tqdm\n","import os\n","from time import time\n","\n","import tensorflow as tf\n","from tensorflow.keras import Model\n","from tensorflow.keras.optimizers import Adam\n","from tensorflow.keras.regularizers import l2\n","from tensorflow.keras.losses import BinaryCrossentropy\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from tensorflow.keras.layers import Embedding, Input, Conv1D, GlobalMaxPooling1D, Dense, Dropout"],"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"id":"R-tNVTLpaLQX","executionInfo":{"status":"ok","timestamp":1637060562492,"user_tz":-330,"elapsed":18,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}}},"source":["os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n","os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n","\n","file = 'ml-1m/ratings.dat'\n","trans_score = 1\n","maxlen = 200\n","\n","embed_dim = 50  # 32\n","hor_n = 8\n","hor_h = 2\n","ver_n = 4\n","dropout = 0.2\n","activation = 'relu'\n","embed_reg = 1e-6\n","K = 10\n","\n","learning_rate = 0.001\n","epochs = 10\n","batch_size = 512"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"id":"YNaRlOMlZsuP","executionInfo":{"status":"ok","timestamp":1637060562494,"user_tz":-330,"elapsed":16,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}}},"source":["def sparseFeature(feat, feat_num, embed_dim=4):\n","    \"\"\"\n","    create dictionary for sparse feature\n","    :param feat: feature name\n","    :param feat_num: the total number of sparse features that do not repeat\n","    :param embed_dim: embedding dimension\n","    :return:\n","    \"\"\"\n","    return {'feat': feat, 'feat_num': feat_num, 'embed_dim': embed_dim}"],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"id":"R-UK1cSTZu--","executionInfo":{"status":"ok","timestamp":1637060563295,"user_tz":-330,"elapsed":7,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}}},"source":["def create_implicit_ml_1m_dataset(file, trans_score=2, embed_dim=8, maxlen=40):\n","    \"\"\"\n","    :param file: A string. dataset path.\n","    :param trans_score: A scalar. Greater than it is 1, and less than it is 0.\n","    :param embed_dim: A scalar. latent factor.\n","    :param maxlen: A scalar. maxlen.\n","    :return: user_num, item_num, train_df, test_df\n","    \"\"\"\n","    print('==========Data Preprocess Start=============')\n","    data_df = pd.read_csv(file, sep=\"::\", engine='python',\n","                          names=['user_id', 'item_id', 'label', 'Timestamp'])\n","    # implicit dataset\n","    data_df = data_df[data_df.label >= trans_score]\n","\n","    # sort\n","    data_df = data_df.sort_values(by=['user_id', 'Timestamp'])\n","\n","    train_data, val_data, test_data = [], [], []\n","\n","    item_id_max = data_df['item_id'].max()\n","    for user_id, df in tqdm(data_df[['user_id', 'item_id']].groupby('user_id')):\n","        pos_list = df['item_id'].tolist()\n","\n","        def gen_neg():\n","            neg = pos_list[0]\n","            while neg in pos_list:\n","                neg = random.randint(1, item_id_max)\n","            return neg\n","\n","        neg_list = [gen_neg() for i in range(len(pos_list) + 100)]\n","        for i in range(1, len(pos_list)):\n","            hist_i = pos_list[:i]\n","            if i == len(pos_list) - 1:\n","                test_data.append([user_id, hist_i, pos_list[i], 1])\n","                for neg in neg_list[i:]:\n","                    test_data.append([user_id, hist_i, neg, 0])\n","            elif i == len(pos_list) - 2:\n","                val_data.append([user_id, hist_i, pos_list[i], 1])\n","                val_data.append([user_id, hist_i, neg_list[i], 0])\n","            else:\n","                train_data.append([user_id, hist_i, pos_list[i], 1])\n","                train_data.append([user_id, hist_i, neg_list[i], 0])\n","    # item feature columns\n","    user_num, item_num = data_df['user_id'].max() + 1, data_df['item_id'].max() + 1\n","    feature_columns = [sparseFeature('user_id', user_num, embed_dim),\n","                       sparseFeature('item_id', item_num, embed_dim)]\n","\n","    # shuffle\n","    random.shuffle(train_data)\n","    random.shuffle(val_data)\n","    # random.shuffle(test_data)\n","\n","    # create dataframe\n","    train = pd.DataFrame(train_data, columns=['user_id', 'hist', 'target_item', 'label'])\n","    val = pd.DataFrame(val_data, columns=['user_id', 'hist', 'target_item', 'label'])\n","    test = pd.DataFrame(test_data, columns=['user_id', 'hist', 'target_item', 'label'])\n","\n","    print('==================Padding===================')\n","    train_X = [train['user_id'].values, pad_sequences(train['hist'], maxlen=maxlen), train['target_item'].values]\n","    train_y = train['label'].values\n","    val_X = [val['user_id'].values, pad_sequences(val['hist'], maxlen=maxlen), val['target_item'].values]\n","    val_y = val['label'].values\n","    test_X = [test['user_id'].values, pad_sequences(test['hist'], maxlen=maxlen), test['target_item'].values]\n","    test_y = test['label'].values.tolist()\n","    print('============Data Preprocess End=============')\n","    return feature_columns, (train_X, train_y), (val_X, val_y), (test_X, test_y)"],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"id":"xK7G8OWQZx-z","executionInfo":{"status":"ok","timestamp":1637060564057,"user_tz":-330,"elapsed":11,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}}},"source":["class Caser(Model):\n","    def __init__(self, feature_columns, maxlen=40, hor_n=2, hor_h=8, ver_n=8, dropout=0.5, activation='relu', embed_reg=1e-6):\n","        \"\"\"\n","        AttRec\n","        :param feature_columns: A feature columns list. user + seq\n","        :param maxlen: A scalar. In the paper, maxlen is L, the number of latest items.\n","        :param hor_n: A scalar. The number of horizontal filters.\n","        :param hor_h: A scalar. Height of horizontal filters.\n","        :param ver_n: A scalar. The number of vertical filters.\n","        :param dropout: A scalar. The number of dropout.\n","        :param activation: A string. 'relu', 'sigmoid' or 'tanh'.\n","        :param embed_reg: A scalar. The regularizer of embedding.\n","        \"\"\"\n","        super(Caser, self).__init__()\n","        # maxlen\n","        self.maxlen = maxlen\n","        # feature columns\n","        self.user_fea_col, self.item_fea_col = feature_columns\n","        # embed_dim\n","        self.embed_dim = self.item_fea_col['embed_dim']\n","        # total number of item set\n","        self.total_item = self.item_fea_col['feat_num']\n","        # horizontal filters\n","        self.hor_n = hor_n\n","        self.hor_h = hor_h if hor_h <= self.maxlen else self.maxlen\n","        # vertical filters\n","        self.ver_n = ver_n\n","        self.ver_w = 1\n","        # user embedding\n","        self.user_embedding = Embedding(input_dim=self.user_fea_col['feat_num'],\n","                                        input_length=1,\n","                                        output_dim=self.user_fea_col['embed_dim'],\n","                                        mask_zero=False,\n","                                        embeddings_initializer='random_normal',\n","                                        embeddings_regularizer=l2(embed_reg))\n","        # item embedding\n","        self.item_embedding = Embedding(input_dim=self.item_fea_col['feat_num'],\n","                                        input_length=1,\n","                                        output_dim=self.item_fea_col['embed_dim'],\n","                                        mask_zero=True,\n","                                        embeddings_initializer='random_normal',\n","                                        embeddings_regularizer=l2(embed_reg))\n","        # item2 embedding\n","        self.item2_embedding = Embedding(input_dim=self.item_fea_col['feat_num'],\n","                                        input_length=1,\n","                                        output_dim=self.item_fea_col['embed_dim'] * 2,\n","                                        mask_zero=True,\n","                                        embeddings_initializer='random_normal',\n","                                        embeddings_regularizer=l2(embed_reg))\n","        # horizontal conv\n","        self.hor_conv = Conv1D(filters=self.hor_n, kernel_size=self.hor_h)\n","        # vertical conv, should transpose\n","        self.ver_conv = Conv1D(filters=self.ver_n, kernel_size=self.ver_w)\n","        # max_pooling\n","        self.pooling = GlobalMaxPooling1D()\n","        # dense\n","        self.dense = Dense(self.embed_dim, activation=activation)\n","        self.dropout = Dropout(dropout)\n","\n","    def call(self, inputs):\n","        # input\n","        user_inputs, seq_inputs, item_inputs = inputs\n","        # user info\n","        user_embed = self.user_embedding(tf.squeeze(user_inputs, axis=-1))  # (None, dim)\n","        # seq info\n","        seq_embed = self.item_embedding(seq_inputs)  # (None, maxlen, dim)\n","        # horizontal conv (None, (maxlen - kernel_size + 2 * pad) / stride +1, hor_n)\n","        hor_info = self.hor_conv(seq_embed)\n","        hor_info = self.pooling(hor_info)  # (None, hor_n)\n","        # vertical conv  (None, (dim - 1 + 2 * pad) / stride + 1, ver_n)\n","        ver_info = self.ver_conv(tf.transpose(seq_embed, perm=(0, 2, 1)))\n","        ver_info = tf.reshape(ver_info, shape=(-1, ver_info.shape[1] * ver_info.shape[2]))  # (None, ?)\n","        # info\n","        seq_info = self.dense(tf.concat([hor_info, ver_info], axis=-1))  # (None, d)\n","        seq_info = self.dropout(seq_info)\n","        # concat\n","        info = tf.concat([seq_info, user_embed], axis=-1)  # (None, 2 * d)\n","        # item info\n","        item_embed = self.item2_embedding(tf.squeeze(item_inputs, axis=-1))  # (None, dim)\n","        # predict\n","        outputs = tf.nn.sigmoid(tf.reduce_sum(tf.multiply(info, item_embed), axis=1, keepdims=True))\n","        return outputs\n","\n","    def summary(self):\n","        seq_inputs = Input(shape=(self.maxlen,), dtype=tf.int32)\n","        user_inputs = Input(shape=(1, ), dtype=tf.int32)\n","        item_inputs = Input(shape=(1,), dtype=tf.int32)\n","        Model(inputs=[user_inputs, seq_inputs, item_inputs],\n","              outputs=self.call([user_inputs, seq_inputs, item_inputs])).summary()"],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"id":"Jkv5hwHmZx73","executionInfo":{"status":"ok","timestamp":1637060564802,"user_tz":-330,"elapsed":13,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}}},"source":["def test_model():\n","    user_features = {'feat': 'user_id', 'feat_num': 100, 'embed_dim': 8}\n","    seq_features = {'feat': 'item_id', 'feat_num': 100, 'embed_dim': 8}\n","\n","    features = [user_features, seq_features]\n","    model = Caser(features)\n","    model.summary()"],"execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"id":"OlM4xkuzZx42","executionInfo":{"status":"ok","timestamp":1637060564806,"user_tz":-330,"elapsed":15,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}}},"source":["def getHit(df):\n","    \"\"\"\n","    calculate hit rate\n","    :return:\n","    \"\"\"\n","    df = df.sort_values('pred_y', ascending=False).reset_index()\n","    if df[df.true_y == 1].index.tolist()[0] < _K:\n","        return 1\n","    else:\n","        return 0\n","\n","\n","def getNDCG(df):\n","    \"\"\"\n","    calculate NDCG\n","    :return:\n","    \"\"\"\n","    df = df.sort_values('pred_y', ascending=False).reset_index()\n","    i = df[df.true_y == 1].index.tolist()[0]\n","    if i < _K:\n","        return np.log(2) / np.log(i+2)\n","    else:\n","        return 0.\n","\n","\n","def evaluate_model(model, test, K):\n","    \"\"\"\n","    evaluate model\n","    :param model: model\n","    :param test: test set\n","    :param K: top K\n","    :return: hit rate, ndcg\n","    \"\"\"\n","    global _K\n","    _K = K\n","    test_X, test_y = test\n","    pred_y = model.predict(test_X)\n","    test_df = pd.DataFrame(test_y, columns=['true_y'])\n","    test_df['user_id'] = test_X[0]\n","    test_df['pred_y'] = pred_y\n","    tg = test_df.groupby('user_id')\n","    hit_rate = tg.apply(getHit).mean()\n","    ndcg = tg.apply(getNDCG).mean()\n","    return hit_rate, ndcg"],"execution_count":8,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"03hx2SFsaSEP","executionInfo":{"status":"ok","timestamp":1637066167282,"user_tz":-330,"elapsed":5594061,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}},"outputId":"58df5e73-71c2-4728-b22d-e02842b72c44"},"source":["# ========================== Create dataset =======================\n","feature_columns, train, val, test = create_implicit_ml_1m_dataset(file, trans_score, embed_dim, maxlen)\n","train_X, train_y = train\n","val_X, val_y = val\n","\n","# ============================Build Model==========================\n","model = Caser(feature_columns, maxlen, hor_n, hor_h, ver_n, dropout, activation, embed_reg)\n","model.summary()\n","# =========================Compile============================\n","model.compile(loss=BinaryCrossentropy(), optimizer=Adam(learning_rate=learning_rate))\n","\n","results = []\n","for epoch in range(1, epochs + 1):\n","    # ===========================Fit==============================\n","    t1 = time()\n","    model.fit(\n","        train_X,\n","        train_y,\n","        validation_data=(val_X, val_y),\n","        epochs=1,\n","        batch_size=batch_size,\n","    )\n","    # ===========================Test==============================\n","    t2 = time()\n","    if epoch % 5 == 0:\n","        hit_rate, ndcg = evaluate_model(model, test, K)\n","        print('Iteration %d Fit [%.1f s], Evaluate [%.1f s]: HR = %.4f, NDCG= %.4f'\n","                % (epoch, t2 - t1, time() - t2, hit_rate, ndcg))\n","        results.append([epoch + 1, t2 - t1, time() - t2, hit_rate, ndcg])\n","# ============================Write============================\n","pd.DataFrame(results, columns=['Iteration', 'fit_time', 'evaluate_time', 'hit_rate', 'ndcg']).\\\n","    to_csv('Caser_log_maxlen_{}_dim_{}_hor_n_{}_ver_n_{}_K_{}_.csv'.\n","            format(maxlen, embed_dim, hor_n, ver_n, K), index=False)"],"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["==========Data Preprocess Start=============\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 6040/6040 [00:27<00:00, 217.52it/s]\n"]},{"output_type":"stream","name":"stdout","text":["==================Padding===================\n","============Data Preprocess End=============\n","Model: \"model\"\n","__________________________________________________________________________________________________\n","Layer (type)                    Output Shape         Param #     Connected to                     \n","==================================================================================================\n","input_1 (InputLayer)            [(None, 200)]        0                                            \n","__________________________________________________________________________________________________\n","embedding_1 (Embedding)         (None, 200, 50)      197650      input_1[0][0]                    \n","__________________________________________________________________________________________________\n","tf.compat.v1.transpose (TFOpLam (None, 50, 200)      0           embedding_1[0][0]                \n","__________________________________________________________________________________________________\n","conv1d (Conv1D)                 (None, 199, 8)       808         embedding_1[0][0]                \n","__________________________________________________________________________________________________\n","conv1d_1 (Conv1D)               (None, 50, 4)        804         tf.compat.v1.transpose[0][0]     \n","__________________________________________________________________________________________________\n","global_max_pooling1d (GlobalMax (None, 8)            0           conv1d[0][0]                     \n","__________________________________________________________________________________________________\n","tf.reshape (TFOpLambda)         (None, 200)          0           conv1d_1[0][0]                   \n","__________________________________________________________________________________________________\n","tf.concat (TFOpLambda)          (None, 208)          0           global_max_pooling1d[0][0]       \n","                                                                 tf.reshape[0][0]                 \n","__________________________________________________________________________________________________\n","input_2 (InputLayer)            [(None, 1)]          0                                            \n","__________________________________________________________________________________________________\n","dense (Dense)                   (None, 50)           10450       tf.concat[0][0]                  \n","__________________________________________________________________________________________________\n","tf.compat.v1.squeeze (TFOpLambd (None,)              0           input_2[0][0]                    \n","__________________________________________________________________________________________________\n","input_3 (InputLayer)            [(None, 1)]          0                                            \n","__________________________________________________________________________________________________\n","dropout (Dropout)               (None, 50)           0           dense[0][0]                      \n","__________________________________________________________________________________________________\n","embedding (Embedding)           (None, 50)           302050      tf.compat.v1.squeeze[0][0]       \n","__________________________________________________________________________________________________\n","tf.compat.v1.squeeze_1 (TFOpLam (None,)              0           input_3[0][0]                    \n","__________________________________________________________________________________________________\n","tf.concat_1 (TFOpLambda)        (None, 100)          0           dropout[0][0]                    \n","                                                                 embedding[0][0]                  \n","__________________________________________________________________________________________________\n","embedding_2 (Embedding)         (None, 100)          395300      tf.compat.v1.squeeze_1[0][0]     \n","__________________________________________________________________________________________________\n","tf.math.multiply (TFOpLambda)   (None, 100)          0           tf.concat_1[0][0]                \n","                                                                 embedding_2[0][0]                \n","__________________________________________________________________________________________________\n","tf.math.reduce_sum (TFOpLambda) (None, 1)            0           tf.math.multiply[0][0]           \n","__________________________________________________________________________________________________\n","tf.math.sigmoid (TFOpLambda)    (None, 1)            0           tf.math.reduce_sum[0][0]         \n","==================================================================================================\n","Total params: 907,062\n","Trainable params: 907,062\n","Non-trainable params: 0\n","__________________________________________________________________________________________________\n","3837/3837 [==============================] - 483s 126ms/step - loss: 0.3770 - val_loss: 0.3562\n","3837/3837 [==============================] - 491s 128ms/step - loss: 0.2703 - val_loss: 0.3282\n","3837/3837 [==============================] - 505s 132ms/step - loss: 0.2382 - val_loss: 0.3197\n","3837/3837 [==============================] - 513s 134ms/step - loss: 0.2216 - val_loss: 0.3228\n","3837/3837 [==============================] - 499s 130ms/step - loss: 0.2104 - val_loss: 0.3251\n","Iteration 5 Fit [502.0 s], Evaluate [73.0 s]: HR = 0.8119, NDCG= 0.5656\n","3837/3837 [==============================] - 502s 131ms/step - loss: 0.2018 - val_loss: 0.3271\n","3837/3837 [==============================] - 508s 132ms/step - loss: 0.1950 - val_loss: 0.3372\n","3837/3837 [==============================] - 523s 136ms/step - loss: 0.1898 - val_loss: 0.3394\n","3837/3837 [==============================] - 542s 141ms/step - loss: 0.1855 - val_loss: 0.3477\n","3837/3837 [==============================] - 535s 139ms/step - loss: 0.1818 - val_loss: 0.3506\n","Iteration 10 Fit [562.0 s], Evaluate [101.5 s]: HR = 0.8030, NDCG= 0.5607\n"]}]},{"cell_type":"code","metadata":{"id":"eTbvyzKFadMa"},"source":[""],"execution_count":null,"outputs":[]}]}