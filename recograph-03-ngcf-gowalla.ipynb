{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"recograph-03-ngcf-gowalla.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyNW+gLoCCrWxDQ2H+DCmfTg"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qLK7OOF3cyjk","executionInfo":{"status":"ok","timestamp":1621241408113,"user_tz":-330,"elapsed":1917,"user":{"displayName":"Sparsh Agarwal","photoUrl":"","userId":"13037694610922482904"}},"outputId":"364434f4-9b0b-47c2-d057-ec146dc73bdd"},"source":["!wget https://s3.us-west-2.amazonaws.com/dgl-data/dataset/gowalla.zip\n","!unzip gowalla.zip"],"execution_count":1,"outputs":[{"output_type":"stream","text":["--2021-05-17 08:50:11--  https://s3.us-west-2.amazonaws.com/dgl-data/dataset/gowalla.zip\n","Resolving s3.us-west-2.amazonaws.com (s3.us-west-2.amazonaws.com)... 52.218.225.64\n","Connecting to s3.us-west-2.amazonaws.com (s3.us-west-2.amazonaws.com)|52.218.225.64|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 2985934 (2.8M) [application/zip]\n","Saving to: ‘gowalla.zip’\n","\n","gowalla.zip         100%[===================>]   2.85M  6.95MB/s    in 0.4s    \n","\n","2021-05-17 08:50:12 (6.95 MB/s) - ‘gowalla.zip’ saved [2985934/2985934]\n","\n","Archive:  gowalla.zip\n","   creating: gowalla/\n","  inflating: gowalla/user_list.txt   \n","  inflating: gowalla/item_list.txt   \n","  inflating: gowalla/test.txt        \n","  inflating: gowalla/README.md       \n","  inflating: gowalla/train.txt       \n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"h8mBkow3dF7o","executionInfo":{"status":"ok","timestamp":1621241413829,"user_tz":-330,"elapsed":7595,"user":{"displayName":"Sparsh Agarwal","photoUrl":"","userId":"13037694610922482904"}},"outputId":"c7c73d05-0b5e-4ac8-9efe-7861a384e6a5"},"source":["!pip install -q dgl"],"execution_count":2,"outputs":[{"output_type":"stream","text":["\u001b[K     |████████████████████████████████| 4.4MB 5.4MB/s \n","\u001b[?25h"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"ws05UY1zeoXL","executionInfo":{"status":"ok","timestamp":1621241413832,"user_tz":-330,"elapsed":7577,"user":{"displayName":"Sparsh Agarwal","photoUrl":"","userId":"13037694610922482904"}}},"source":["!mkdir -p utility"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Xja7qUtLeyGn","executionInfo":{"status":"ok","timestamp":1621241413833,"user_tz":-330,"elapsed":7574,"user":{"displayName":"Sparsh Agarwal","photoUrl":"","userId":"13037694610922482904"}},"outputId":"fa32d3d7-91e2-426e-e98f-36cb7a80f65d"},"source":["%%writefile utility/load_data.py\n","\n","# This file is based on the NGCF author's implementation\n","# <https://github.com/xiangwang1223/neural_graph_collaborative_filtering/blob/master/NGCF/utility/load_data.py>.\n","# It implements the data processing and graph construction.\n","import numpy as np\n","import random as rd\n","import dgl\n","\n","class Data(object):\n","    def __init__(self, path, batch_size):\n","        self.path = path\n","        self.batch_size = batch_size\n","\n","        train_file = path + '/train.txt'\n","        test_file = path + '/test.txt'\n","\n","        #get number of users and items\n","        self.n_users, self.n_items = 0, 0\n","        self.n_train, self.n_test = 0, 0\n","        self.exist_users = []\n","\n","        user_item_src = []\n","        user_item_dst = []\n","\n","        with open(train_file) as f:\n","            for l in f.readlines():\n","                if len(l) > 0:\n","                    l = l.strip('\\n').split(' ')\n","                    items = [int(i) for i in l[1:]]\n","                    uid = int(l[0])\n","                    self.exist_users.append(uid)\n","                    self.n_items = max(self.n_items, max(items))\n","                    self.n_users = max(self.n_users, uid)\n","                    self.n_train += len(items)\n","                    for i in l[1:]:\n","                        user_item_src.append(uid)\n","                        user_item_dst.append(int(i))\n","\n","        with open(test_file) as f:\n","            for l in f.readlines():\n","                if len(l) > 0:\n","                    l = l.strip('\\n')\n","                    try:\n","                        items = [int(i) for i in l.split(' ')[1:]]\n","                    except Exception:\n","                        continue\n","                    self.n_items = max(self.n_items, max(items))\n","                    self.n_test += len(items)\n","        self.n_items += 1\n","        self.n_users += 1\n","\n","        self.print_statistics()\n","\n","        #training positive items corresponding to each user; testing positive items corresponding to each user\n","        self.train_items, self.test_set = {}, {}\n","        with open(train_file) as f_train:\n","            with open(test_file) as f_test:\n","                for l in f_train.readlines():\n","                    if len(l) == 0:\n","                        break\n","                    l = l.strip('\\n')\n","                    items = [int(i) for i in l.split(' ')]\n","                    uid, train_items = items[0], items[1:]\n","                    self.train_items[uid] = train_items\n","\n","                for l in f_test.readlines():\n","                    if len(l) == 0: break\n","                    l = l.strip('\\n')\n","                    try:\n","                        items = [int(i) for i in l.split(' ')]\n","                    except Exception:\n","                        continue\n","\n","                    uid, test_items = items[0], items[1:]\n","                    self.test_set[uid] = test_items\n","        \n","        #construct graph from the train data and add self-loops\n","        user_selfs = [ i for i in range(self.n_users)]\n","        item_selfs = [ i for i in range(self.n_items)]\n","        \n","        data_dict = {\n","                ('user', 'user_self', 'user') : (user_selfs, user_selfs),\n","                ('item', 'item_self', 'item') : (item_selfs, item_selfs),\n","                ('user', 'ui', 'item') : (user_item_src, user_item_dst),\n","                ('item', 'iu', 'user') : (user_item_dst, user_item_src)\n","                }\n","        num_dict = {\n","            'user': self.n_users, 'item': self.n_items\n","        }\n","\n","        self.g = dgl.heterograph(data_dict, num_nodes_dict=num_dict)\n","\n","\n","    def sample(self):\n","        if self.batch_size <= self.n_users:\n","            users = rd.sample(self.exist_users, self.batch_size)\n","        else:\n","            users = [rd.choice(self.exist_users) for _ in range(self.batch_size)]\n","\n","        def sample_pos_items_for_u(u, num):\n","            # sample num pos items for u-th user\n","            pos_items = self.train_items[u]\n","            n_pos_items = len(pos_items)\n","            pos_batch = []\n","            while True:\n","                if len(pos_batch) == num:\n","                    break\n","                pos_id = np.random.randint(low=0, high=n_pos_items, size=1)[0]\n","                pos_i_id = pos_items[pos_id]\n","\n","                if pos_i_id not in pos_batch:\n","                    pos_batch.append(pos_i_id)\n","            return pos_batch\n","\n","        def sample_neg_items_for_u(u, num):\n","            # sample num neg items for u-th user\n","            neg_items = []\n","            while True:\n","                if len(neg_items) == num:\n","                    break\n","                neg_id = np.random.randint(low=0, high=self.n_items,size=1)[0]\n","                if neg_id not in self.train_items[u] and neg_id not in neg_items:\n","                    neg_items.append(neg_id)\n","            return neg_items\n","\n","\n","        pos_items, neg_items = [], []\n","        for u in users:\n","            pos_items += sample_pos_items_for_u(u, 1)\n","            neg_items += sample_neg_items_for_u(u, 1)\n","\n","        return users, pos_items, neg_items\n","\n","    def get_num_users_items(self):\n","        return self.n_users, self.n_items\n","\n","    def print_statistics(self):\n","        print('n_users=%d, n_items=%d' % (self.n_users, self.n_items))\n","        print('n_interactions=%d' % (self.n_train + self.n_test))\n","        print('n_train=%d, n_test=%d, sparsity=%.5f' % (self.n_train, self.n_test, (self.n_train + self.n_test)/(self.n_users * self.n_items)))"],"execution_count":4,"outputs":[{"output_type":"stream","text":["Writing utility/load_data.py\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"48eSz3I9exya","executionInfo":{"status":"ok","timestamp":1621241413835,"user_tz":-330,"elapsed":7570,"user":{"displayName":"Sparsh Agarwal","photoUrl":"","userId":"13037694610922482904"}},"outputId":"4140d1d0-cdaa-45c6-ba50-e5056b282a1f"},"source":["%%writefile utility/metrics.py\n","\n","# This file is copied from the NGCF author's implementation\n","# <https://github.com/xiangwang1223/neural_graph_collaborative_filtering/blob/master/NGCF/utility/metrics.py>.\n","# It implements the metrics.\n","'''\n","Created on Oct 10, 2018\n","Tensorflow Implementation of Neural Graph Collaborative Filtering (NGCF) model in:\n","Wang Xiang et al. Neural Graph Collaborative Filtering. In SIGIR 2019.\n","@author: Xiang Wang (xiangwang@u.nus.edu)\n","'''\n","\n","import numpy as np\n","from sklearn.metrics import roc_auc_score\n","\n","def recall(rank, ground_truth, N):\n","    return len(set(rank[:N]) & set(ground_truth)) / float(len(set(ground_truth)))\n","\n","\n","def precision_at_k(r, k):\n","    \"\"\"Score is precision @ k\n","    Relevance is binary (nonzero is relevant).\n","    Returns:\n","        Precision @ k\n","    Raises:\n","        ValueError: len(r) must be >= k\n","    \"\"\"\n","    assert k >= 1\n","    r = np.asarray(r)[:k]\n","    return np.mean(r)\n","\n","\n","def average_precision(r,cut):\n","    \"\"\"Score is average precision (area under PR curve)\n","    Relevance is binary (nonzero is relevant).\n","    Returns:\n","        Average precision\n","    \"\"\"\n","    r = np.asarray(r)\n","    out = [precision_at_k(r, k + 1) for k in range(cut) if r[k]]\n","    if not out:\n","        return 0.\n","    return np.sum(out)/float(min(cut, np.sum(r)))\n","\n","\n","def mean_average_precision(rs):\n","    \"\"\"Score is mean average precision\n","    Relevance is binary (nonzero is relevant).\n","    Returns:\n","        Mean average precision\n","    \"\"\"\n","    return np.mean([average_precision(r) for r in rs])\n","\n","\n","def dcg_at_k(r, k, method=1):\n","    \"\"\"Score is discounted cumulative gain (dcg)\n","    Relevance is positive real values.  Can use binary\n","    as the previous methods.\n","    Returns:\n","        Discounted cumulative gain\n","    \"\"\"\n","    r = np.asfarray(r)[:k]\n","    if r.size:\n","        if method == 0:\n","            return r[0] + np.sum(r[1:] / np.log2(np.arange(2, r.size + 1)))\n","        elif method == 1:\n","            return np.sum(r / np.log2(np.arange(2, r.size + 2)))\n","        else:\n","            raise ValueError('method must be 0 or 1.')\n","    return 0.\n","\n","\n","def ndcg_at_k(r, k, method=1):\n","    \"\"\"Score is normalized discounted cumulative gain (ndcg)\n","    Relevance is positive real values.  Can use binary\n","    as the previous methods.\n","    Returns:\n","        Normalized discounted cumulative gain\n","    \"\"\"\n","    dcg_max = dcg_at_k(sorted(r, reverse=True), k, method)\n","    if not dcg_max:\n","        return 0.\n","    return dcg_at_k(r, k, method) / dcg_max\n","\n","\n","def recall_at_k(r, k, all_pos_num):\n","    r = np.asfarray(r)[:k]\n","    return np.sum(r) / all_pos_num\n","\n","\n","def hit_at_k(r, k):\n","    r = np.array(r)[:k]\n","    if np.sum(r) > 0:\n","        return 1.\n","    else:\n","        return 0.\n","\n","def F1(pre, rec):\n","    if pre + rec > 0:\n","        return (2.0 * pre * rec) / (pre + rec)\n","    else:\n","        return 0.\n","\n","def auc(ground_truth, prediction):\n","    try:\n","        res = roc_auc_score(y_true=ground_truth, y_score=prediction)\n","    except Exception:\n","        res = 0.\n","    return res"],"execution_count":5,"outputs":[{"output_type":"stream","text":["Writing utility/metrics.py\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-VWXIW6BgAkc","executionInfo":{"status":"ok","timestamp":1621241413836,"user_tz":-330,"elapsed":7566,"user":{"displayName":"Sparsh Agarwal","photoUrl":"","userId":"13037694610922482904"}},"outputId":"5ee53cda-dc57-4dd2-855b-eb1c8a2726c2"},"source":["%%writefile utility/helper.py\n","\n","# This file is copied from the NGCF author's implementation\n","# <https://github.com/xiangwang1223/neural_graph_collaborative_filtering/blob/master/NGCF/utility/helper.py>.\n","# It implements the helper functions.\n","'''\n","Created on Aug 19, 2016\n","@author: Xiang Wang (xiangwang@u.nus.edu)\n","'''\n","__author__ = \"xiangwang\"\n","import os\n","import re\n","\n","def txt2list(file_src):\n","    orig_file = open(file_src, \"r\")\n","    lines = orig_file.readlines()\n","    return lines\n","\n","def ensureDir(dir_path):\n","    d = os.path.dirname(dir_path)\n","    if not os.path.exists(d):\n","        os.makedirs(d)\n","\n","def uni2str(unicode_str):\n","    return str(unicode_str.encode('ascii', 'ignore')).replace('\\n', '').strip()\n","\n","def hasNumbers(inputString):\n","    return bool(re.search(r'\\d', inputString))\n","\n","def delMultiChar(inputString, chars):\n","    for ch in chars:\n","        inputString = inputString.replace(ch, '')\n","    return inputString\n","\n","def merge_two_dicts(x, y):\n","    z = x.copy()   # start with x's keys and values\n","    z.update(y)    # modifies z with y's keys and values & returns None\n","    return z\n","\n","def early_stopping(log_value, best_value, stopping_step, expected_order='acc', flag_step=100):\n","    # early stopping strategy:\n","    assert expected_order in ['acc', 'dec']\n","\n","    if (expected_order == 'acc' and log_value >= best_value) or (expected_order == 'dec' and log_value <= best_value):\n","        stopping_step = 0\n","        best_value = log_value\n","    else:\n","        stopping_step += 1\n","\n","    if stopping_step >= flag_step:\n","        print(\"Early stopping is trigger at step: {} log:{}\".format(flag_step, log_value))\n","        should_stop = True\n","    else:\n","        should_stop = False\n","    return best_value, stopping_step, should_stop"],"execution_count":6,"outputs":[{"output_type":"stream","text":["Writing utility/helper.py\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"GRsRMAjQjj7R","executionInfo":{"status":"ok","timestamp":1621244667266,"user_tz":-330,"elapsed":1138,"user":{"displayName":"Sparsh Agarwal","photoUrl":"","userId":"13037694610922482904"}}},"source":["# This file is based on the NGCF author's implementation\n","# <https://github.com/xiangwang1223/neural_graph_collaborative_filtering/blob/master/NGCF/utility/parser.py>.\n","\n","import argparse\n","\n","def parse_args():\n","    parser = argparse.ArgumentParser(description=\"Run NGCF.\")\n","    parser.add_argument('--weights_path', nargs='?', default='./',\n","                        help='Store model path.')\n","    parser.add_argument('--data_path', nargs='?', default='./',\n","                        help='Input data path.')\n","    parser.add_argument('--model_name', type=str, default='NGCF.pkl',\n","                        help='Saved model name.')\n","\n","\n","    parser.add_argument('--dataset', nargs='?', default='gowalla',\n","                        help='Choose a dataset from {gowalla, yelp2018, amazon-book}')\n","    parser.add_argument('--verbose', type=int, default=1,\n","                        help='Interval of evaluation.')\n","    parser.add_argument('--epoch', type=int, default=400,\n","                        help='Number of epoch.')\n","\n","    parser.add_argument('--embed_size', type=int, default=64,\n","                        help='Embedding size.')\n","    parser.add_argument('--layer_size', nargs='?', default='[64,64,64]',\n","                        help='Output sizes of every layer')\n","    parser.add_argument('--batch_size', type=int, default=1024,\n","                        help='Batch size.')\n","\n","    parser.add_argument('--regs', nargs='?', default='[1e-5]',\n","                        help='Regularizations.')\n","    parser.add_argument('--lr', type=float, default=0.0001,\n","                        help='Learning rate.')\n","\n","\n","    parser.add_argument('--gpu', type=int, default=0,\n","                        help='0 for NAIS_prod, 1 for NAIS_concat')\n","\n","    parser.add_argument('--mess_dropout', nargs='?', default='[0.1,0.1,0.1]',\n","                        help='Keep probability w.r.t. message dropout (i.e., 1-dropout_ratio) for each deep layer. 1: no dropout.')\n","\n","    parser.add_argument('--Ks', nargs='?', default='[20, 40]',\n","                        help='Output sizes of every layer')\n","\n","    parser.add_argument('--save_flag', type=int, default=1,\n","                        help='0: Disable model saver, 1: Activate model saver')\n","\n","    parser.add_argument('--test_flag', nargs='?', default='part',\n","                        help='Specify the test type from {part, full}, indicating whether the reference is done in mini-batch')\n","\n","    parser.add_argument('--report', type=int, default=0,\n","                        help='0: Disable performance report w.r.t. sparsity levels, 1: Show performance report w.r.t. sparsity levels')\n","    return parser.parse_known_args()"],"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4gCHaVRfdrIJ","executionInfo":{"status":"ok","timestamp":1621243364490,"user_tz":-330,"elapsed":1283,"user":{"displayName":"Sparsh Agarwal","photoUrl":"","userId":"13037694610922482904"}},"outputId":"8659c80d-80d0-46be-dfd7-98b1047e5993"},"source":["%%writefile model.py\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import dgl.function as fn\n","\n","class NGCFLayer(nn.Module):\n","    def __init__(self, in_size, out_size, norm_dict, dropout):\n","        super(NGCFLayer, self).__init__()\n","        self.in_size = in_size\n","        self.out_size = out_size\n","\n","        #weights for different types of messages\n","        self.W1 = nn.Linear(in_size, out_size, bias = True)\n","        self.W2 = nn.Linear(in_size, out_size, bias = True)\n","\n","        #leaky relu\n","        self.leaky_relu = nn.LeakyReLU(0.2)\n","\n","        #dropout layer\n","        self.dropout = nn.Dropout(dropout)\n","\n","        #initialization\n","        torch.nn.init.xavier_uniform_(self.W1.weight)\n","        torch.nn.init.constant_(self.W1.bias, 0)\n","        torch.nn.init.xavier_uniform_(self.W2.weight)\n","        torch.nn.init.constant_(self.W2.bias, 0)\n","\n","        #norm\n","        self.norm_dict = norm_dict\n","\n","    def forward(self, g, feat_dict):\n","\n","        funcs = {} #message and reduce functions dict\n","        #for each type of edges, compute messages and reduce them all\n","        for srctype, etype, dsttype in g.canonical_etypes:\n","            if srctype == dsttype: #for self loops\n","                messages = self.W1(feat_dict[srctype])\n","                g.nodes[srctype].data[etype] = messages   #store in ndata\n","                funcs[(srctype, etype, dsttype)] = (fn.copy_u(etype, 'm'), fn.sum('m', 'h'))  #define message and reduce functions\n","            else:\n","                src, dst = g.edges(etype=(srctype, etype, dsttype))\n","                norm = self.norm_dict[(srctype, etype, dsttype)]\n","                messages = norm * (self.W1(feat_dict[srctype][src]) + self.W2(feat_dict[srctype][src]*feat_dict[dsttype][dst])) #compute messages\n","                g.edges[(srctype, etype, dsttype)].data[etype] = messages  #store in edata\n","                funcs[(srctype, etype, dsttype)] = (fn.copy_e(etype, 'm'), fn.sum('m', 'h'))  #define message and reduce functions\n","\n","        g.multi_update_all(funcs, 'sum') #update all, reduce by first type-wisely then across different types\n","        feature_dict={}\n","        for ntype in g.ntypes:\n","            h = self.leaky_relu(g.nodes[ntype].data['h']) #leaky relu\n","            h = self.dropout(h) #dropout\n","            h = F.normalize(h,dim=1,p=2) #l2 normalize\n","            feature_dict[ntype] = h\n","        return feature_dict\n","\n","class NGCF(nn.Module):\n","    def __init__(self, g, in_size, layer_size, dropout, lmbd=1e-5):\n","        super(NGCF, self).__init__()\n","        self.lmbd = lmbd\n","        self.norm_dict = dict()\n","        for srctype, etype, dsttype in g.canonical_etypes:\n","            src, dst = g.edges(etype=(srctype, etype, dsttype))\n","            dst_degree = g.in_degrees(dst, etype=(srctype, etype, dsttype)).float() #obtain degrees\n","            src_degree = g.out_degrees(src, etype=(srctype, etype, dsttype)).float()\n","            norm = torch.pow(src_degree * dst_degree, -0.5).unsqueeze(1) #compute norm\n","            self.norm_dict[(srctype, etype, dsttype)] = norm\n","\n","        self.layers = nn.ModuleList()\n","        self.layers.append(\n","            NGCFLayer(in_size, layer_size[0], self.norm_dict, dropout[0])\n","        )\n","        self.num_layers = len(layer_size)\n","        for i in range(self.num_layers-1):\n","            self.layers.append(\n","                NGCFLayer(layer_size[i], layer_size[i+1], self.norm_dict, dropout[i+1])\n","            )\n","        self.initializer = nn.init.xavier_uniform_\n","\n","        #embeddings for different types of nodes\n","        self.feature_dict = nn.ParameterDict({\n","            ntype: nn.Parameter(self.initializer(torch.empty(g.num_nodes(ntype), in_size))) for ntype in g.ntypes\n","        })\n","\n","    def create_bpr_loss(self, users, pos_items, neg_items):\n","        pos_scores = (users * pos_items).sum(1)\n","        neg_scores = (users * neg_items).sum(1)\n","\n","        mf_loss = nn.LogSigmoid()(pos_scores - neg_scores).mean()\n","        mf_loss = -1 * mf_loss\n","\n","        regularizer = (torch.norm(users) ** 2 + torch.norm(pos_items) ** 2 + torch.norm(neg_items) ** 2) / 2\n","        emb_loss = self.lmbd * regularizer / users.shape[0]\n","\n","        return mf_loss + emb_loss, mf_loss, emb_loss\n","\n","    def rating(self, u_g_embeddings, pos_i_g_embeddings):\n","        return torch.matmul(u_g_embeddings, pos_i_g_embeddings.t())\n","\n","    def forward(self, g,user_key, item_key, users, pos_items, neg_items):\n","        h_dict = {ntype : self.feature_dict[ntype] for ntype in g.ntypes}\n","        #obtain features of each layer and concatenate them all\n","        user_embeds = []\n","        item_embeds = []\n","        user_embeds.append(h_dict[user_key])\n","        item_embeds.append(h_dict[item_key])\n","        for layer in self.layers:\n","            h_dict = layer(g, h_dict)\n","            user_embeds.append(h_dict[user_key])\n","            item_embeds.append(h_dict[item_key])\n","        user_embd = torch.cat(user_embeds, 1)\n","        item_embd = torch.cat(item_embeds, 1)\n","\n","        u_g_embeddings = user_embd[users, :]\n","        pos_i_g_embeddings = item_embd[pos_items, :]\n","        neg_i_g_embeddings = item_embd[neg_items, :]\n","\n","        return u_g_embeddings, pos_i_g_embeddings, neg_i_g_embeddings"],"execution_count":36,"outputs":[{"output_type":"stream","text":["Overwriting model.py\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"UrPPVUb9ktx3","executionInfo":{"status":"ok","timestamp":1621244712262,"user_tz":-330,"elapsed":918,"user":{"displayName":"Sparsh Agarwal","photoUrl":"","userId":"13037694610922482904"}}},"source":["# This file is based on the NGCF author's implementation\n","# <https://github.com/xiangwang1223/neural_graph_collaborative_filtering/blob/master/NGCF/utility/batch_test.py>.\n","# It implements the batch test.\n","\n","import utility.metrics as metrics\n","from utility.load_data import *\n","import multiprocessing\n","import heapq"],"execution_count":14,"outputs":[]},{"cell_type":"code","metadata":{"id":"mUvhoJ86kwDj","executionInfo":{"status":"ok","timestamp":1621244679290,"user_tz":-330,"elapsed":5929,"user":{"displayName":"Sparsh Agarwal","photoUrl":"","userId":"13037694610922482904"}}},"source":["cores = multiprocessing.cpu_count()"],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"id":"oAl-MEIKkzoI","executionInfo":{"status":"ok","timestamp":1621244679292,"user_tz":-330,"elapsed":5736,"user":{"displayName":"Sparsh Agarwal","photoUrl":"","userId":"13037694610922482904"}}},"source":["args, unknown = parse_args()\n","Ks = eval(args.Ks)"],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3jVnlhvsk16t","executionInfo":{"status":"ok","timestamp":1621244748440,"user_tz":-330,"elapsed":2737,"user":{"displayName":"Sparsh Agarwal","photoUrl":"","userId":"13037694610922482904"}},"outputId":"1343e0c0-9b33-41fa-b222-741b73b2d1bf"},"source":["data_generator = Data(path=args.data_path + args.dataset, batch_size=args.batch_size)\n","USR_NUM, ITEM_NUM = data_generator.n_users, data_generator.n_items\n","N_TRAIN, N_TEST = data_generator.n_train, data_generator.n_test\n","BATCH_SIZE = args.batch_size"],"execution_count":16,"outputs":[{"output_type":"stream","text":["n_users=29858, n_items=40981\n","n_interactions=1027370\n","n_train=810128, n_test=217242, sparsity=0.00084\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"dSpkZGjMk13H","executionInfo":{"status":"ok","timestamp":1621244755168,"user_tz":-330,"elapsed":1434,"user":{"displayName":"Sparsh Agarwal","photoUrl":"","userId":"13037694610922482904"}}},"source":["def ranklist_by_heapq(user_pos_test, test_items, rating, Ks):\n","    item_score = {}\n","    for i in test_items:\n","        item_score[i] = rating[i]\n","\n","    K_max = max(Ks)\n","    K_max_item_score = heapq.nlargest(K_max, item_score, key=item_score.get)\n","\n","    r = []\n","    for i in K_max_item_score:\n","        if i in user_pos_test:\n","            r.append(1)\n","        else:\n","            r.append(0)\n","    auc = 0.\n","    return r, auc"],"execution_count":17,"outputs":[]},{"cell_type":"code","metadata":{"id":"qNJI8Yu7lBI6","executionInfo":{"status":"ok","timestamp":1621244756255,"user_tz":-330,"elapsed":2361,"user":{"displayName":"Sparsh Agarwal","photoUrl":"","userId":"13037694610922482904"}}},"source":["def get_auc(item_score, user_pos_test):\n","    item_score = sorted(item_score.items(), key=lambda kv: kv[1])\n","    item_score.reverse()\n","    item_sort = [x[0] for x in item_score]\n","    posterior = [x[1] for x in item_score]\n","\n","    r = []\n","    for i in item_sort:\n","        if i in user_pos_test:\n","            r.append(1)\n","        else:\n","            r.append(0)\n","    auc = metrics.auc(ground_truth=r, prediction=posterior)\n","    return auc"],"execution_count":18,"outputs":[]},{"cell_type":"code","metadata":{"id":"2rsJSP1XlDgs","executionInfo":{"status":"ok","timestamp":1621244756258,"user_tz":-330,"elapsed":2219,"user":{"displayName":"Sparsh Agarwal","photoUrl":"","userId":"13037694610922482904"}}},"source":["def ranklist_by_sorted(user_pos_test, test_items, rating, Ks):\n","    item_score = {}\n","    for i in test_items:\n","        item_score[i] = rating[i]\n","\n","    K_max = max(Ks)\n","    K_max_item_score = heapq.nlargest(K_max, item_score, key=item_score.get)\n","\n","    r = []\n","    for i in K_max_item_score:\n","        if i in user_pos_test:\n","            r.append(1)\n","        else:\n","            r.append(0)\n","    auc = get_auc(item_score, user_pos_test)\n","    return r, auc"],"execution_count":19,"outputs":[]},{"cell_type":"code","metadata":{"id":"uLYLa2eXlE-9","executionInfo":{"status":"ok","timestamp":1621244756260,"user_tz":-330,"elapsed":2087,"user":{"displayName":"Sparsh Agarwal","photoUrl":"","userId":"13037694610922482904"}}},"source":["def get_performance(user_pos_test, r, auc, Ks):\n","    precision, recall, ndcg, hit_ratio = [], [], [], []\n","\n","    for K in Ks:\n","        precision.append(metrics.precision_at_k(r, K))\n","        recall.append(metrics.recall_at_k(r, K, len(user_pos_test)))\n","        ndcg.append(metrics.ndcg_at_k(r, K))\n","        hit_ratio.append(metrics.hit_at_k(r, K))\n","\n","    return {'recall': np.array(recall), 'precision': np.array(precision),\n","            'ndcg': np.array(ndcg), 'hit_ratio': np.array(hit_ratio), 'auc': auc}"],"execution_count":20,"outputs":[]},{"cell_type":"code","metadata":{"id":"BIPxCEThlHg3","executionInfo":{"status":"ok","timestamp":1621244756996,"user_tz":-330,"elapsed":1094,"user":{"displayName":"Sparsh Agarwal","photoUrl":"","userId":"13037694610922482904"}}},"source":["def test_one_user(x):\n","    # user u's ratings for user u\n","    rating = x[0]\n","    #uid\n","    u = x[1]\n","    #user u's items in the training set\n","    try:\n","        training_items = data_generator.train_items[u]\n","    except Exception:\n","        training_items = []\n","    #user u's items in the test set\n","    user_pos_test = data_generator.test_set[u]\n","\n","    all_items = set(range(ITEM_NUM))\n","\n","    test_items = list(all_items - set(training_items))\n","\n","    if args.test_flag == 'part':\n","        r, auc = ranklist_by_heapq(user_pos_test, test_items, rating, Ks)\n","    else:\n","        r, auc = ranklist_by_sorted(user_pos_test, test_items, rating, Ks)\n","\n","    return get_performance(user_pos_test, r, auc, Ks)"],"execution_count":21,"outputs":[]},{"cell_type":"code","metadata":{"id":"49cf5Y8LeE1-","executionInfo":{"status":"ok","timestamp":1621244756997,"user_tz":-330,"elapsed":855,"user":{"displayName":"Sparsh Agarwal","photoUrl":"","userId":"13037694610922482904"}}},"source":["def test(model, g, users_to_test, batch_test_flag=False):\n","    result = {'precision': np.zeros(len(Ks)), 'recall': np.zeros(len(Ks)), 'ndcg': np.zeros(len(Ks)),\n","              'hit_ratio': np.zeros(len(Ks)), 'auc': 0.}\n","\n","    pool = multiprocessing.Pool(cores)\n","\n","    u_batch_size = 5000\n","    i_batch_size = BATCH_SIZE\n","\n","    test_users = users_to_test\n","    n_test_users = len(test_users)\n","    n_user_batchs = n_test_users // u_batch_size + 1\n","\n","    count = 0\n","\n","    for u_batch_id in range(n_user_batchs):\n","        start = u_batch_id * u_batch_size\n","        end = (u_batch_id + 1) * u_batch_size\n","\n","        user_batch = test_users[start: end]\n","\n","        if batch_test_flag:\n","            # batch-item test\n","            n_item_batchs = ITEM_NUM // i_batch_size + 1\n","            rate_batch = np.zeros(shape=(len(user_batch), ITEM_NUM))\n","\n","            i_count = 0\n","            for i_batch_id in range(n_item_batchs):\n","                i_start = i_batch_id * i_batch_size\n","                i_end = min((i_batch_id + 1) * i_batch_size, ITEM_NUM)\n","\n","                item_batch = range(i_start, i_end)\n","\n","                u_g_embeddings, pos_i_g_embeddings, _ = model(g, 'user', 'item',user_batch, item_batch, [])\n","                i_rate_batch = model.rating(u_g_embeddings, pos_i_g_embeddings).detach().cpu()\n","\n","                rate_batch[:, i_start: i_end] = i_rate_batch\n","                i_count += i_rate_batch.shape[1]\n","\n","            assert i_count == ITEM_NUM\n","\n","        else:\n","            # all-item test\n","            item_batch = range(ITEM_NUM)\n","            u_g_embeddings, pos_i_g_embeddings, _ = model(g, 'user', 'item',user_batch, item_batch, [])\n","            rate_batch = model.rating(u_g_embeddings, pos_i_g_embeddings).detach().cpu()\n","\n","        user_batch_rating_uid = zip(rate_batch.numpy(), user_batch)\n","        batch_result = pool.map(test_one_user, user_batch_rating_uid)\n","        count += len(batch_result)\n","\n","        for re in batch_result:\n","            result['precision'] += re['precision']/n_test_users\n","            result['recall'] += re['recall']/n_test_users\n","            result['ndcg'] += re['ndcg']/n_test_users\n","            result['hit_ratio'] += re['hit_ratio']/n_test_users\n","            result['auc'] += re['auc']/n_test_users\n","\n","\n","    assert count == n_test_users\n","    pool.close()\n","    return result"],"execution_count":22,"outputs":[]},{"cell_type":"code","metadata":{"id":"IOsb7AqbgvXy","executionInfo":{"status":"ok","timestamp":1621244759331,"user_tz":-330,"elapsed":827,"user":{"displayName":"Sparsh Agarwal","photoUrl":"","userId":"13037694610922482904"}}},"source":["import torch\n","import torch.optim as optim\n","from model import NGCF\n","from utility.helper import early_stopping\n","from time import time\n","import os"],"execution_count":23,"outputs":[]},{"cell_type":"code","metadata":{"id":"BtnwxwbwnLws","executionInfo":{"status":"ok","timestamp":1621244761149,"user_tz":-330,"elapsed":1580,"user":{"displayName":"Sparsh Agarwal","photoUrl":"","userId":"13037694610922482904"}}},"source":["# Step 1: Prepare graph data and device ================================================================= #\n","if args.gpu >= 0 and torch.cuda.is_available():\n","    device = 'cuda:{}'.format(args.gpu)\n","else:\n","    device = 'cpu'\n","g=data_generator.g\n","g=g.to(device)"],"execution_count":24,"outputs":[]},{"cell_type":"code","metadata":{"id":"ENA17frtnrYk","executionInfo":{"status":"ok","timestamp":1621244764165,"user_tz":-330,"elapsed":1196,"user":{"displayName":"Sparsh Agarwal","photoUrl":"","userId":"13037694610922482904"}}},"source":["if not os.path.exists(args.weights_path):\n","    os.makedirs(args.weights_path)\n","args.mess_dropout = eval(args.mess_dropout)\n","args.layer_size = eval(args.layer_size)\n","args.regs = eval(args.regs)"],"execution_count":25,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BatinFK7rhIO","executionInfo":{"status":"ok","timestamp":1621244764781,"user_tz":-330,"elapsed":1641,"user":{"displayName":"Sparsh Agarwal","photoUrl":"","userId":"13037694610922482904"}},"outputId":"7dcbb385-9711-4078-8b61-dcb36ced448d"},"source":["print(args)"],"execution_count":26,"outputs":[{"output_type":"stream","text":["Namespace(Ks='[20, 40]', batch_size=1024, data_path='./', dataset='gowalla', embed_size=64, epoch=400, gpu=0, layer_size=[64, 64, 64], lr=0.0001, mess_dropout=[0.1, 0.1, 0.1], model_name='NGCF.pkl', regs=[1e-05], report=0, save_flag=1, test_flag='part', verbose=1, weights_path='./')\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"2RlATJDZnT8f","executionInfo":{"status":"ok","timestamp":1621244764784,"user_tz":-330,"elapsed":1505,"user":{"displayName":"Sparsh Agarwal","photoUrl":"","userId":"13037694610922482904"}}},"source":["# Step 2: Create model and training components=========================================================== #\n","model = NGCF(g, args.embed_size, args.layer_size, args.mess_dropout, args.regs[0]).to(device)\n","optimizer = optim.Adam(model.parameters(), lr=args.lr)"],"execution_count":27,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PZ0Km0PCrzQH","executionInfo":{"status":"ok","timestamp":1621244766172,"user_tz":-330,"elapsed":1149,"user":{"displayName":"Sparsh Agarwal","photoUrl":"","userId":"13037694610922482904"}},"outputId":"490cae6e-46ba-4900-9961-19a0ed2f1cc8"},"source":["[len(data_generator.sample()[i]) for i in range(len(data_generator.sample()))]"],"execution_count":28,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[1024, 1024, 1024]"]},"metadata":{"tags":[]},"execution_count":28}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IVg5nsofdwfN","outputId":"c960e760-1e7b-44ba-d29b-bc22d2fbaf76"},"source":["# Step 3: training epoches ============================================================================== #\n","n_batch = data_generator.n_train // args.batch_size + 1\n","t0 = time()\n","cur_best_pre_0, stopping_step = 0, 0\n","loss_loger, pre_loger, rec_loger, ndcg_loger, hit_loger = [], [], [], [], []\n","for epoch in range(args.epoch):\n","    t1 = time()\n","    loss, mf_loss, emb_loss = 0., 0., 0.\n","    for idx in range(n_batch):\n","        users, pos_items, neg_items = data_generator.sample()\n","        u_g_embeddings, pos_i_g_embeddings, neg_i_g_embeddings = model(g, 'user', 'item', users,\n","                                                                        pos_items,\n","                                                                        neg_items)\n","\n","        batch_loss, batch_mf_loss, batch_emb_loss = model.create_bpr_loss(u_g_embeddings,\n","                                                                          pos_i_g_embeddings,\n","                                                                          neg_i_g_embeddings)\n","        optimizer.zero_grad()\n","        batch_loss.backward()\n","        optimizer.step()\n","\n","        loss += batch_loss\n","        mf_loss += batch_mf_loss\n","        emb_loss += batch_emb_loss\n","        \n","\n","    if (epoch + 1) % 10 != 0:\n","        if args.verbose > 0 and epoch % args.verbose == 0:\n","            perf_str = 'Epoch %d [%.1fs]: train==[%.5f=%.5f + %.5f]' % (\n","                epoch, time() - t1, loss, mf_loss, emb_loss)\n","            print(perf_str)\n","        continue #end the current epoch and move to the next epoch, let the following evaluation run every 10 epoches\n","\n","    #evaluate the model every 10 epoches\n","    t2 = time()\n","    users_to_test = list(data_generator.test_set.keys())\n","    ret = test(model, g, users_to_test)\n","    t3 = time()\n","\n","    loss_loger.append(loss)\n","    rec_loger.append(ret['recall'])\n","    pre_loger.append(ret['precision'])\n","    ndcg_loger.append(ret['ndcg'])\n","    hit_loger.append(ret['hit_ratio'])\n","\n","    if args.verbose > 0:\n","        perf_str = 'Epoch %d [%.1fs + %.1fs]: train==[%.5f=%.5f + %.5f], recall=[%.5f, %.5f], ' \\\n","                    'precision=[%.5f, %.5f], hit=[%.5f, %.5f], ndcg=[%.5f, %.5f]' % \\\n","                    (epoch, t2 - t1, t3 - t2, loss, mf_loss, emb_loss, ret['recall'][0], ret['recall'][-1],\n","                    ret['precision'][0], ret['precision'][-1], ret['hit_ratio'][0], ret['hit_ratio'][-1],\n","                    ret['ndcg'][0], ret['ndcg'][-1])\n","        print(perf_str)\n","\n","    cur_best_pre_0, stopping_step, should_stop = early_stopping(ret['recall'][0], cur_best_pre_0,\n","                                                                stopping_step, expected_order='acc', flag_step=5)\n","\n","    # early stop\n","    if should_stop == True:\n","        break\n","\n","    if ret['recall'][0] == cur_best_pre_0 and args.save_flag == 1:\n","        torch.save(model.state_dict(), args.weights_path + args.model_name)\n","        print('save the weights in path: ', args.weights_path + args.model_name)\n","\n","recs = np.array(rec_loger)\n","pres = np.array(pre_loger)\n","ndcgs = np.array(ndcg_loger)\n","hit = np.array(hit_loger)\n","\n","best_rec_0 = max(recs[:, 0])\n","idx = list(recs[:, 0]).index(best_rec_0)\n","\n","final_perf = \"Best Iter=[%d]@[%.1f]\\trecall=[%s], precision=[%s], hit=[%s], ndcg=[%s]\" % \\\n","              (idx, time() - t0, '\\t'.join(['%.5f' % r for r in recs[idx]]),\n","              '\\t'.join(['%.5f' % r for r in pres[idx]]),\n","              '\\t'.join(['%.5f' % r for r in hit[idx]]),\n","              '\\t'.join(['%.5f' % r for r in ndcgs[idx]]))\n","print(final_perf)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Epoch 0 [10845.4s]: train==[229.65080=229.61519 + 0.03567]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"nCrV8a6it_0Z"},"source":[""],"execution_count":null,"outputs":[]}]}