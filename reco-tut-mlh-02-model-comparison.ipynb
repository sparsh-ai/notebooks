{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "project_name = \"reco-tut-mlh\"; branch = \"main\"; account = \"sparsh-ai\"\n",
    "project_path = os.path.join('/content', project_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(project_path):\n",
    "    !cp /content/drive/MyDrive/mykeys.py /content\n",
    "    import mykeys\n",
    "    !rm /content/mykeys.py\n",
    "    path = \"/content/\" + project_name; \n",
    "    !mkdir \"{path}\"\n",
    "    %cd \"{path}\"\n",
    "    import sys; sys.path.append(path)\n",
    "    !git config --global user.email \"recotut@recohut.com\"\n",
    "    !git config --global user.name  \"reco-tut\"\n",
    "    !git init\n",
    "    !git remote add origin https://\"{mykeys.git_token}\":x-oauth-basic@github.com/\"{account}\"/\"{project_name}\".git\n",
    "    !git pull origin \"{branch}\"\n",
    "    !git checkout main\n",
    "else:\n",
    "    %cd \"{project_path}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git add . && git commit -m 'commit' && git push origin \"{branch}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, './code')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collaborative Filtering Comparison\n",
    "\n",
    "In this notebook we compare different recommendation systems starting with the state-of-the-art LightGCN and going back to the winning algorithm for 2009's Netflix Prize competition, SVD++.\n",
    "\n",
    "Models include in order are LightGCN, NGCF, SVAE, SVD++, and SVD. Each model has their own individual notebooks where we go more indepth, especially LightGCN and NGCF, where we implemented them from scratch in Tensorflow. \n",
    "\n",
    "The last cell compares the performance of the different models using ranking metrics:\n",
    "\n",
    "\n",
    "*   Precision@k\n",
    "*   Recall@k\n",
    "*   Mean Average Precision (MAP)\n",
    "*   Normalized Discounted Cumulative Gain (NDCG)\n",
    "\n",
    "where $k=10$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q surprise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import random\n",
    "import requests\n",
    "import scipy.sparse as sp\n",
    "import surprise\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.python.framework.ops import disable_eager_execution\n",
    "from tqdm import tqdm\n",
    "\n",
    "from utils import stratified_split, numpy_stratified_split\n",
    "import build_features\n",
    "import metrics\n",
    "from models import SVAE\n",
    "from models.GCN import LightGCN, NGCF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fp = os.path.join('./data/bronze', 'u.data')\n",
    "raw_data = pd.read_csv(fp, sep='\\t', names=['userId', 'movieId', 'rating', 'timestamp'])\n",
    "print(f'Shape: {raw_data.shape}')\n",
    "raw_data.sample(10, random_state=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load movie titles.\n",
    "fp = os.path.join('./data/bronze', 'u.item')\n",
    "movie_titles = pd.read_csv(fp, sep='|', names=['movieId', 'title'], usecols = range(2), encoding='iso-8859-1')\n",
    "print(f'Shape: {movie_titles.shape}')\n",
    "movie_titles.sample(10, random_state=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = 0.75\n",
    "train, test = stratified_split(raw_data, 'userId', train_size)\n",
    "\n",
    "print(f'Train Shape: {train.shape}')\n",
    "print(f'Test Shape: {test.shape}')\n",
    "print(f'Do they have the same users?: {set(train.userId) == set(test.userId)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined = train.append(test)\n",
    "\n",
    "n_users = combined['userId'].nunique()\n",
    "print('Number of users:', n_users)\n",
    "\n",
    "n_movies = combined['movieId'].nunique()\n",
    "print('Number of movies:', n_movies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataFrame with reset index of 0-n_movies.\n",
    "movie_new = combined[['movieId']].drop_duplicates()\n",
    "movie_new['movieId_new'] = np.arange(len(movie_new))\n",
    "\n",
    "train_reindex = pd.merge(train, movie_new, on='movieId', how='left')\n",
    "# Reset index to 0-n_users.\n",
    "train_reindex['userId_new'] = train_reindex['userId'] - 1  \n",
    "train_reindex = train_reindex[['userId_new', 'movieId_new', 'rating']]\n",
    "\n",
    "test_reindex = pd.merge(test, movie_new, on='movieId', how='left')\n",
    "# Reset index to 0-n_users.\n",
    "test_reindex['userId_new'] = test_reindex['userId'] - 1\n",
    "test_reindex = test_reindex[['userId_new', 'movieId_new', 'rating']]\n",
    "\n",
    "# Create dictionaries so we can convert to and from indexes\n",
    "item2id = dict(zip(movie_new['movieId'], movie_new['movieId_new']))\n",
    "id2item = dict(zip(movie_new['movieId_new'], movie_new['movieId']))\n",
    "user2id = dict(zip(train['userId'], train_reindex['userId_new']))\n",
    "id2user = dict(zip(train_reindex['userId_new'], train['userId']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create user-item graph (sparse matix where users are rows and movies are columns.\n",
    "# 1 if a user reviewed that movie, 0 if they didn't).\n",
    "R = sp.dok_matrix((n_users, n_movies), dtype=np.float32)\n",
    "R[train_reindex['userId_new'], train_reindex['movieId_new']] = 1\n",
    "\n",
    "# Create the adjaceny matrix with the user-item graph.\n",
    "adj_mat = sp.dok_matrix((n_users + n_movies, n_users + n_movies), dtype=np.float32)\n",
    "\n",
    "# List of lists.\n",
    "adj_mat.tolil()\n",
    "R = R.tolil()\n",
    "\n",
    "# Put together adjacency matrix. Movies and users are nodes/vertices.\n",
    "# 1 if the movie and user are connected.\n",
    "adj_mat[:n_users, n_users:] = R\n",
    "adj_mat[n_users:, :n_users] = R.T\n",
    "\n",
    "adj_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate degree matrix D (for every row count the number of nonzero entries)\n",
    "D_values = np.array(adj_mat.sum(1))\n",
    "\n",
    "# Square root and inverse.\n",
    "D_inv_values = np.power(D_values  + 1e-9, -0.5).flatten()\n",
    "D_inv_values[np.isinf(D_inv_values)] = 0.0\n",
    "\n",
    " # Create sparse matrix with the values of D^(-0.5) are the diagonals.\n",
    "D_inv_sq_root = sp.diags(D_inv_values)\n",
    "\n",
    "# Eval (D^-0.5 * A * D^-0.5).\n",
    "norm_adj_mat = D_inv_sq_root.dot(adj_mat).dot(D_inv_sq_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to COOrdinate format first ((row, column), data)\n",
    "coo = norm_adj_mat.tocoo().astype(np.float32)\n",
    "\n",
    "# create an index that will tell SparseTensor where the non-zero points are\n",
    "indices = np.mat([coo.row, coo.col]).transpose()\n",
    "\n",
    "# covert to sparse tensor\n",
    "A_tilde = tf.SparseTensor(indices, coo.data, coo.shape)\n",
    "A_tilde"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graph Convoultional Networks (GCNs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Light Graph Convolution Network (LightGCN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "light_model = LightGCN(A_tilde,\n",
    "                 n_users = n_users,\n",
    "                 n_items = n_movies,\n",
    "                 n_layers = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-2)\n",
    "light_model.fit(epochs=25, batch_size=1024, optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Graph Collaborative Filtering (NGCF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngcf_model = NGCF(A_tilde,\n",
    "                  n_users = n_users,\n",
    "                  n_items = n_movies,\n",
    "                  n_layers = 3\n",
    "                  )\n",
    "\n",
    "ngcf_model.fit(epochs=25, batch_size=1024, optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recommend with LightGCN and NGCF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert test user ids to the new ids\n",
    "users = np.array([user2id[x] for x in test['userId'].unique()])\n",
    "\n",
    "recs = []\n",
    "for model in [light_model, ngcf_model]:\n",
    "    recommendations = model.recommend(users, k=10)\n",
    "    recommendations = recommendations.replace({'userId': id2user, 'movieId': id2item})\n",
    "    recommendations = recommendations.merge(movie_titles,\n",
    "                                                    how='left',\n",
    "                                                    on='movieId'\n",
    "                                                    )[['userId', 'movieId', 'title', 'prediction']]\n",
    "\n",
    "    # Create column with the predicted movie's rank for each user \n",
    "    top_k = recommendations.copy()\n",
    "    top_k['rank'] = recommendations.groupby('userId', sort=False).cumcount() + 1  # For each user, only include movies recommendations that are also in the test set\n",
    "\n",
    "    recs.append(top_k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standard Variational Autoencoder (SVAE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Binarize the data (only keep ratings >= 4)\n",
    "df_preferred = raw_data[raw_data['rating'] > 3.5]\n",
    "df_low_rating = raw_data[raw_data['rating'] <= 3.5]\n",
    "\n",
    "df = df_preferred.groupby('userId').filter(lambda x: len(x) >= 5)\n",
    "df = df.groupby('movieId').filter(lambda x: len(x) >= 1)\n",
    "\n",
    "# Obtain both usercount and itemcount after filtering\n",
    "usercount = df[['userId']].groupby('userId', as_index = False).size()\n",
    "itemcount = df[['movieId']].groupby('movieId', as_index = False).size()\n",
    "\n",
    "unique_users =sorted(df.userId.unique())\n",
    "np.random.seed(123)\n",
    "unique_users = np.random.permutation(unique_users)\n",
    "\n",
    "HELDOUT_USERS = 200\n",
    "\n",
    "# Create train/validation/test users\n",
    "n_users = len(unique_users)\n",
    "train_users = unique_users[:(n_users - HELDOUT_USERS * 2)]\n",
    "val_users = unique_users[(n_users - HELDOUT_USERS * 2) : (n_users - HELDOUT_USERS)]\n",
    "test_users = unique_users[(n_users - HELDOUT_USERS):]\n",
    "\n",
    "train_set = df.loc[df['userId'].isin(train_users)]\n",
    "val_set = df.loc[df['userId'].isin(val_users)]\n",
    "test_set = df.loc[df['userId'].isin(test_users)]\n",
    "unique_train_items = pd.unique(train_set['movieId'])\n",
    "val_set = val_set.loc[val_set['movieId'].isin(unique_train_items)]\n",
    "test_set = test_set.loc[test_set['movieId'].isin(unique_train_items)]\n",
    "\n",
    "# Instantiate the sparse matrix generation for train, validation and test sets\n",
    "# use list of unique items from training set for all sets\n",
    "am_train = build_features.AffinityMatrix(df=train_set, items_list=unique_train_items)\n",
    "am_val = build_features.AffinityMatrix(df=val_set, items_list=unique_train_items)\n",
    "am_test = build_features.AffinityMatrix(df=test_set, items_list=unique_train_items)\n",
    "\n",
    "# Obtain the sparse matrix for train, validation and test sets\n",
    "train_data, _, _ = am_train.gen_affinity_matrix()\n",
    "val_data, val_map_users, val_map_items = am_val.gen_affinity_matrix()\n",
    "test_data, test_map_users, test_map_items = am_test.gen_affinity_matrix()\n",
    "\n",
    "# Split validation and test data into training and testing parts\n",
    "val_data_tr, val_data_te = numpy_stratified_split(val_data, ratio=0.75, seed=123)\n",
    "test_data_tr, test_data_te = numpy_stratified_split(test_data, ratio=0.75, seed=123)\n",
    "\n",
    "# Binarize train, validation and test data\n",
    "train_data = np.where(train_data > 3.5, 1.0, 0.0)\n",
    "val_data = np.where(val_data > 3.5, 1.0, 0.0)\n",
    "test_data = np.where(test_data > 3.5, 1.0, 0.0)\n",
    "\n",
    "# Binarize validation data\n",
    "val_data_tr = np.where(val_data_tr > 3.5, 1.0, 0.0)\n",
    "val_data_te_ratings = val_data_te.copy()\n",
    "val_data_te = np.where(val_data_te > 3.5, 1.0, 0.0)\n",
    "\n",
    "# Binarize test data: training part \n",
    "test_data_tr = np.where(test_data_tr > 3.5, 1.0, 0.0)\n",
    "\n",
    "# Binarize test data: testing part (save non-binary version in the separate object, will be used for calculating NDCG)\n",
    "test_data_te_ratings = test_data_te.copy()\n",
    "test_data_te = np.where(test_data_te > 3.5, 1.0, 0.0)\n",
    "\n",
    "# retrieve real ratings from initial dataset \n",
    "test_data_te_ratings=pd.DataFrame(test_data_te_ratings)\n",
    "val_data_te_ratings=pd.DataFrame(val_data_te_ratings)\n",
    "\n",
    "for index,i in df_low_rating.iterrows():\n",
    "    user_old= i['userId'] # old value \n",
    "    item_old=i['movieId'] # old value \n",
    "\n",
    "    if (test_map_users.get(user_old) is not None)  and (test_map_items.get(item_old) is not None) :\n",
    "        user_new=test_map_users.get(user_old) # new value \n",
    "        item_new=test_map_items.get(item_old) # new value \n",
    "        rating=i['rating'] \n",
    "        test_data_te_ratings.at[user_new,item_new]= rating   \n",
    "\n",
    "    if (val_map_users.get(user_old) is not None)  and (val_map_items.get(item_old) is not None) :\n",
    "        user_new=val_map_users.get(user_old) # new value \n",
    "        item_new=val_map_items.get(item_old) # new value \n",
    "        rating=i['rating'] \n",
    "        val_data_te_ratings.at[user_new,item_new]= rating   \n",
    "\n",
    "\n",
    "val_data_te_ratings=val_data_te_ratings.to_numpy()    \n",
    "test_data_te_ratings=test_data_te_ratings.to_numpy()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disable_eager_execution()\n",
    "svae_model = SVAE.StandardVAE(n_users=train_data.shape[0],\n",
    "                                   original_dim=train_data.shape[1], \n",
    "                                   intermediate_dim=200, \n",
    "                                   latent_dim=64, \n",
    "                                   n_epochs=400, \n",
    "                                   batch_size=100, \n",
    "                                   k=10,\n",
    "                                   verbose=0,\n",
    "                                   seed=123,\n",
    "                                   drop_encoder=0.5,\n",
    "                                   drop_decoder=0.5,\n",
    "                                   annealing=False,\n",
    "                                   beta=1.0\n",
    "                                   )\n",
    "\n",
    "svae_model.fit(x_train=train_data,\n",
    "          x_valid=val_data,\n",
    "          x_val_tr=val_data_tr,\n",
    "          x_val_te=val_data_te_ratings,\n",
    "          mapper=am_val\n",
    "          )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recommend with SVAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model prediction on the training part of test set \n",
    "top_k =  svae_model.recommend_k_items(x=test_data_tr,k=10,remove_seen=True)\n",
    "\n",
    "# Convert sparse matrix back to df\n",
    "recommendations = am_test.map_back_sparse(top_k, kind='prediction')\n",
    "test_df = am_test.map_back_sparse(test_data_te_ratings, kind='ratings') # use test_data_te_, with the original ratings\n",
    "\n",
    "# Create column with the predicted movie's rank for each user \n",
    "top_k = recommendations.copy()\n",
    "top_k['rank'] = recommendations.groupby('userId', sort=False).cumcount() + 1  # For each user, only include movies recommendations that are also in the test set\n",
    "\n",
    "recs.append(top_k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Singular Value Decomposition (SVD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVD++"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "surprise_train = surprise.Dataset.load_from_df(train.drop('timestamp', axis=1), reader=surprise.Reader('ml-100k')).build_full_trainset()\n",
    "svdpp = surprise.SVDpp(random_state=0, n_factors=64, n_epochs=10, verbose=True)\n",
    "svdpp.fit(surprise_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svd = surprise.SVD(random_state=0, n_factors=64, n_epochs=10, verbose=True)\n",
    "svd.fit(surprise_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recommend with SVD++ and SVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model in [svdpp, svd]:\n",
    "    predictions = []\n",
    "    users = train['userId'].unique()\n",
    "    items = train['movieId'].unique()\n",
    "\n",
    "    for user in users:\n",
    "            for item in items:\n",
    "                predictions.append([user, item, model.predict(user, item).est])\n",
    "\n",
    "    predictions = pd.DataFrame(predictions, columns=['userId', 'movieId', 'prediction'])\n",
    "\n",
    "    # Remove movies already seen by users\n",
    "    # Create column of all 1s\n",
    "    temp = train[['userId', 'movieId']].copy()\n",
    "    temp['seen'] = 1\n",
    "\n",
    "    # Outer join and remove movies that have alread been seen (seen=1)\n",
    "    merged = pd.merge(temp, predictions, on=['userId', 'movieId'], how=\"outer\")\n",
    "    merged = merged[merged['seen'].isnull()].drop('seen', axis=1)\n",
    "\n",
    "    # Create filter for users that appear in both the train and test set\n",
    "    common_users = set(test['userId']).intersection(set(predictions['userId']))\n",
    "\n",
    "    # Filter the test and predictions so they have the same users between them\n",
    "    test_common = test[test['userId'].isin(common_users)]\n",
    "    svd_pred_common = merged[merged['userId'].isin(common_users)]\n",
    "\n",
    "    if len(set(merged['userId'])) != len(set(test['userId'])):\n",
    "        print('Number of users in train and test are NOT equal')\n",
    "        print(f\"# of users in train and test respectively: {len(set(merged['userId']))}, {len(set(test['userId']))}\")\n",
    "        print(f\"# of users in BOTH train and test: {len(set(svd_pred_common['userId']))}\")\n",
    "        continue\n",
    "        \n",
    "    # From the predictions, we want only the top k for each user,\n",
    "    # not all the recommendations.\n",
    "    # Extract the top k recommendations from the predictions\n",
    "    top_movies = svd_pred_common.groupby('userId', as_index=False).apply(lambda x: x.nlargest(10, 'prediction')).reset_index(drop=True)\n",
    "    top_movies['rank'] = top_movies.groupby('userId', sort=False).cumcount() + 1\n",
    "    \n",
    "    top_k = top_movies.copy()\n",
    "    top_k['rank'] = top_movies.groupby('userId', sort=False).cumcount() + 1  # For each user, only include movies recommendations that are also in the test set\n",
    "    \n",
    "    recs.append(top_k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at all 5 of our models, we can see that the state-of-the-art model LightGCN vastly outperforms all other models. When compared to SVD++, a widely used algorithm during the Netflix Prize competition, LightGCN achieves an increase in **Percision@k by 29%, Recall@k by 18%, MAP by 12%, and NDCG by 35%**.\n",
    "\n",
    "NGCF is the older sister model to LightGCN, but only by a single year. We can see how LightGCN improves in ranking metrics compared to NGCF by simply removing unnecessary operations. \n",
    "\n",
    "In conclusion, this demonstrates how far recommendation systems have advanced since 2009, and how new model architectures with notable performance increases can be developed in the span of just 1-2 years."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_names = ['LightGCN', 'NGCF', 'SVAE', 'SVD++', 'SVD']\n",
    "comparison = pd.DataFrame(columns=['Algorithm', 'Precision@k', 'Recall@k', 'MAP', 'NDCG'])\n",
    "\n",
    "# Convert test user ids to the new ids\n",
    "users = np.array([user2id[x] for x in test['userId'].unique()])\n",
    "\n",
    "for rec, name in zip(recs, model_names):\n",
    "    tester = test_df if name == 'SVAE' else test\n",
    "\n",
    "    pak = metrics.precision_at_k(rec, tester, 'userId', 'movieId', 'rank')\n",
    "    rak = metrics.recall_at_k(rec, tester, 'userId', 'movieId', 'rank')\n",
    "    map = metrics.mean_average_precision(rec, tester, 'userId', 'movieId', 'rank')\n",
    "    ndcg = metrics.ndcg(rec, tester, 'userId', 'movieId', 'rank')\n",
    "\n",
    "    comparison.loc[len(comparison)] = [name, pak, rak, map, ndcg]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References:\n",
    "\n",
    "1.   Xiangnan He, Kuan Deng, Xiang Wang, Yan Li, Yongdong Zhang & Meng Wang, LightGCN: Simplifying and Powering Graph Convolution Network for Recommendation, 2020, https://arxiv.org/abs/2002.02126\n",
    "2.   Xiang Wang, Xiangnan He, Meng Wang, Fuli Feng, & Tata-Seng Chua, Neural Graph Collaorative Filtering, 2019, https://arxiv.org/abs/1905.08108\n",
    "3.   Microsoft SVAE implementation: https://github.com/microsoft/recommenders/blob/main/examples/02_model_collaborative_filtering/standard_vae_deep_dive.ipynb\n",
    "4. Simon Gower, Netflix Prize and SVD, 2014, https://www.semanticscholar.org/paper/Netflix-Prize-and-SVD-Gower/ce7b81b46939d7852dbb30538a7796e69fdd407c\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyNZQ/Aq0zpCkV6LLQZnuS6x",
   "collapsed_sections": [],
   "mount_file_id": "1qal5YmJsYrefA9qpvL6Bv-DxT-9Ay7eK",
   "name": "reco-tut-mlh-02-model-comparison.ipynb",
   "provenance": [
    {
     "file_id": "1WgX5gqwulFU6zMuRI-sXbyIlSPj58LBh",
     "timestamp": 1628676317350
    },
    {
     "file_id": "1AqVI1vgtpjVbcx5XdPgFnkJCmWXMAg7G",
     "timestamp": 1628676023810
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
