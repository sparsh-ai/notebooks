{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"T851476 | Multi-Task Learning","provenance":[{"file_id":"https://github.com/recohut/notebooks/blob/master/_notebooks/2021-07-11-multi-task-learning-income-census-data.ipynb","timestamp":1634753674253}],"collapsed_sections":[],"authorship_tag":"ABX9TyPqr6mZlmjdNYK2BVYTWy7E"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"o5AaP597b2Y3"},"source":["# Multi-Task Learning\n","\n","> Training 5 models on census data taking salary and marital status as proxies for CTR and CTCVR proxy. Objective is to learn about different models and how they work."]},{"cell_type":"markdown","metadata":{"id":"jTBvw75NcuQp"},"source":["## Setup"]},{"cell_type":"code","metadata":{"id":"IZjIdtVyPqK-"},"source":["!pip install deepctr[cpu]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_2NnJXRUQDY8"},"source":["# !wget -q --show-progress http://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data\n","# !wget -q --show-progress http://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.test"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"WV5ZuOGTLGaD","executionInfo":{"status":"ok","timestamp":1635311782621,"user_tz":-330,"elapsed":1687,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}}},"source":["!git clone -q https://github.com/sparsh-ai/multiobjective-optimizations.git\n","!cp multiobjective-optimizations/data/adult.data .\n","!cp multiobjective-optimizations/data/adult.test ."],"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"id":"4KwQhlFbVLtv","executionInfo":{"status":"ok","timestamp":1635311815541,"user_tz":-330,"elapsed":2484,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}}},"source":["import numpy as np\n","import pandas as pd\n","from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n","from sklearn.metrics import roc_auc_score\n","\n","import tensorflow as tf\n","\n","from deepctr.feature_column import build_input_features, input_from_feature_columns\n","from deepctr.feature_column import SparseFeat, DenseFeat, get_feature_names\n","\n","from deepctr.layers.core import PredictionLayer, DNN\n","from deepctr.layers.utils import combined_dnn_input \n","\n","import warnings\n","warnings.filterwarnings('ignore')"],"execution_count":1,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hrtoxsdZZvTc"},"source":["## Tasks"]},{"cell_type":"markdown","metadata":{"id":"Z_8ZpKvGZw4q"},"source":["| Task | Goal | Proxy |\n","| - | -:| ---:|\n","| Task 1 | Predict whether the income exceeds 50K | ctr |\n","| Task 2 | Predict whether this personâ€™s marital status is never married | ctcvr |"]},{"cell_type":"markdown","metadata":{"id":"e3fReRLqZ8uX"},"source":["## Data"]},{"cell_type":"code","metadata":{"id":"s8eRPP47Z-Te","executionInfo":{"status":"ok","timestamp":1635311815546,"user_tz":-330,"elapsed":11,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}}},"source":["CENSUS_COLUMNS = ['age','workclass','fnlwgt','education','education_num',\n","                  'marital_status','occupation','relationship','race','gender',\n","                  'capital_gain','capital_loss','hours_per_week','native_country',\n","                  'income_bracket']\n","\n","df_train = pd.read_csv('adult.data',header=None,names=CENSUS_COLUMNS)\n","df_test = pd.read_csv('adult.test',header=None,names=CENSUS_COLUMNS)\n","\n","data = pd.concat([df_train, df_test], axis=0)\n","\n","#take task1 as ctr task, take task2 as ctcvr task.\n","data['ctr_label'] = data['income_bracket'].map({' >50K.':1, ' >50K':1, ' <=50K.':0, ' <=50K':0})\n","data['ctcvr_label'] = data['marital_status'].apply(lambda x: 1 if x==' Never-married' else 0)\n","data.drop(labels=['marital_status', 'income_bracket'], axis=1, inplace=True)"],"execution_count":2,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GkJkFfoXawZs"},"source":["## Features"]},{"cell_type":"code","metadata":{"id":"hZEZteUgayC9","executionInfo":{"status":"ok","timestamp":1635312052503,"user_tz":-330,"elapsed":645,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}}},"source":["#define dense and sparse features\n","columns = data.columns.values.tolist()\n","dense_features = ['fnlwgt', 'education_num', 'capital_gain', 'capital_loss', 'hours_per_week']\n","sparse_features = [col for col in columns if col not in dense_features and col not in ['ctr_label', 'ctcvr_label']]\n","\n","data[sparse_features] = data[sparse_features].fillna('-1', )\n","data[dense_features] = data[dense_features].fillna(0, )\n","mms = MinMaxScaler(feature_range=(0, 1))\n","data[dense_features] = mms.fit_transform(data[dense_features])\n","    \n","for feat in sparse_features:\n","    lbe = LabelEncoder()\n","    data[feat] = data[feat].astype('str')\n","    data[feat] = lbe.fit_transform(data[feat])\n","    \n","fixlen_feature_columns = [SparseFeat(feat, data[feat].max()+1, embedding_dim=16)for feat in sparse_features] \\\n","+ [DenseFeat(feat, 1,) for feat in dense_features]\n","\n","dnn_feature_columns = fixlen_feature_columns\n","\n","feature_names = get_feature_names(dnn_feature_columns)"],"execution_count":4,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WCt1FpDLa4kL"},"source":["## Train & Test Data"]},{"cell_type":"code","metadata":{"id":"o-oz915Fa7f6","executionInfo":{"status":"ok","timestamp":1635312057210,"user_tz":-330,"elapsed":500,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}}},"source":["#train test split\n","n_train = df_train.shape[0]\n","train = data[:n_train]\n","test = data[n_train:]\n","train_model_input = {name: train[name] for name in feature_names}\n","test_model_input = {name: test[name] for name in feature_names}"],"execution_count":5,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"nPWtKmlVQcys"},"source":["## Shared Bottom"]},{"cell_type":"code","metadata":{"id":"o4dK-MS4QR9A","executionInfo":{"status":"ok","timestamp":1635312060969,"user_tz":-330,"elapsed":471,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}}},"source":["# import tensorflow as tf\n","\n","# from deepctr.feature_column import build_input_features, input_from_feature_columns\n","# from deepctr.layers.core import PredictionLayer, DNN\n","# from deepctr.layers.utils import combined_dnn_input \n","\n","def Shared_Bottom(dnn_feature_columns, num_tasks, task_types, task_names,\n","                  bottom_dnn_units=[128, 128], tower_dnn_units_lists=[[64,32], [64,32]],\n","                  l2_reg_embedding=0.00001, l2_reg_dnn=0, seed=1024, dnn_dropout=0,\n","                 dnn_activation='relu', dnn_use_bn=False):\n","    \"\"\"Instantiates the Shared_Bottom multi-task learning Network architecture.\n","    \n","    :param dnn_feature_columns: An iterable containing all the features used by deep part of the model.\n","    :param num_tasks:  integer, number of tasks, equal to number of outputs, must be greater than 1.\n","    :param task_types: list of str, indicating the loss of each tasks, ``\"binary\"`` for  binary logloss or  ``\"regression\"`` for regression loss. e.g. ['binary', 'regression']\n","    :param task_names: list of str, indicating the predict target of each tasks\n","    :param bottom_dnn_units: list,list of positive integer or empty list, the layer number and units in each layer of shared-bottom DNN\n","    :param tower_dnn_units_lists: list, list of positive integer list, its length must be euqal to num_tasks, the layer number and units in each layer of task-specific DNN\n","    :param l2_reg_embedding: float. L2 regularizer strength applied to embedding vector\n","    :param l2_reg_dnn: float. L2 regularizer strength applied to DNN\n","    :param seed: integer ,to use as random seed.\n","    :param dnn_dropout: float in [0,1), the probability we will drop out a given DNN coordinate.\n","    :param dnn_activation: Activation function to use in DNN\n","    :param dnn_use_bn: bool. Whether use BatchNormalization before activation or not in DNN\n","    :return: A Keras model instance.\n","    \"\"\"\n","    if num_tasks <= 1:\n","        raise ValueError(\"num_tasks must be greater than 1\")\n","    if len(task_types) != num_tasks:\n","        raise ValueError(\"num_tasks must be equal to the length of task_types\")\n","        \n","    for task_type in task_types:\n","        if task_type not in ['binary', 'regression']:\n","            raise ValueError(\"task must be binary or regression, {} is illegal\".format(task_type))\n","            \n","    if num_tasks != len(tower_dnn_units_lists):\n","        raise ValueError(\"the length of tower_dnn_units_lists must be euqal to num_tasks\")\n","\n","    features = build_input_features(dnn_feature_columns)\n","    inputs_list = list(features.values())\n","    \n","    sparse_embedding_list, dense_value_list = input_from_feature_columns(features, dnn_feature_columns, l2_reg_embedding,seed)\n","\n","    dnn_input = combined_dnn_input(sparse_embedding_list, dense_value_list)\n","    shared_bottom_output = DNN(bottom_dnn_units, dnn_activation, l2_reg_dnn, dnn_dropout, dnn_use_bn, seed=seed)(dnn_input)\n","\n","    tasks_output = []\n","    for task_type, task_name, tower_dnn in zip(task_types, task_names, tower_dnn_units_lists):\n","        tower_output = DNN(tower_dnn, dnn_activation, l2_reg_dnn, dnn_dropout, dnn_use_bn, seed=seed, name='tower_'+task_name)(shared_bottom_output)\n","        \n","        logit = tf.keras.layers.Dense(1, use_bias=False, activation=None)(tower_output)\n","        output = PredictionLayer(task_type, name=task_name)(logit) #regression->keep, binary classification->sigmoid\n","        tasks_output.append(output)\n","\n","    model = tf.keras.models.Model(inputs=inputs_list, outputs=tasks_output)\n","    return model"],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"t4ysNDKebMdz","executionInfo":{"status":"ok","timestamp":1635312425191,"user_tz":-330,"elapsed":8743,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}},"outputId":"e9764e29-6613-451a-82ac-10c9089b7fff"},"source":["task_names = ['income', 'marital']\n","model = Shared_Bottom(dnn_feature_columns, num_tasks=2, task_types= ['binary', 'binary'], task_names=task_names, bottom_dnn_units=[128, 128], tower_dnn_units_lists=[[64,32], [64,32]])\n","\n","model.compile(\"adam\", loss=[\"binary_crossentropy\", \"binary_crossentropy\"], metrics=['AUC'])\n","history = model.fit(train_model_input, [train['ctr_label'].values, train['ctcvr_label'].values],batch_size=256, epochs=5, verbose=2, validation_split=0.0 )\n","\n","pred_ans = model.predict(test_model_input, batch_size=256)"],"execution_count":18,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/5\n","128/128 - 4s - loss: 0.7891 - income_loss: 0.4193 - marital_loss: 0.3698 - income_auc: 0.8152 - marital_auc_1: 0.8995\n","Epoch 2/5\n","128/128 - 1s - loss: 0.5544 - income_loss: 0.3231 - marital_loss: 0.2313 - income_auc: 0.9043 - marital_auc_1: 0.9617\n","Epoch 3/5\n","128/128 - 1s - loss: 0.5435 - income_loss: 0.3154 - marital_loss: 0.2281 - income_auc: 0.9088 - marital_auc_1: 0.9627\n","Epoch 4/5\n","128/128 - 1s - loss: 0.5393 - income_loss: 0.3132 - marital_loss: 0.2261 - income_auc: 0.9101 - marital_auc_1: 0.9632\n","Epoch 5/5\n","128/128 - 1s - loss: 0.5345 - income_loss: 0.3104 - marital_loss: 0.2241 - income_auc: 0.9117 - marital_auc_1: 0.9639\n"]}]},{"cell_type":"markdown","metadata":{"id":"4LZ8mzYeVVUc"},"source":["## ESSM"]},{"cell_type":"code","metadata":{"id":"rJd4euhoVVUx","executionInfo":{"status":"ok","timestamp":1635312450778,"user_tz":-330,"elapsed":776,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}}},"source":["# import tensorflow as tf\n","\n","# from deepctr.feature_column import build_input_features, input_from_feature_columns\n","# from deepctr.layers.core import PredictionLayer, DNN\n","# from deepctr.layers.utils import combined_dnn_input \n","\n","\n","def ESSM(dnn_feature_columns, task_type='binary', task_names=['ctr', 'ctcvr'],\n","        tower_dnn_units_lists=[[128, 128],[128, 128]], l2_reg_embedding=0.00001, l2_reg_dnn=0, \n","         seed=1024, dnn_dropout=0,dnn_activation='relu', dnn_use_bn=False):\n","    \"\"\"Instantiates the Entire Space Multi-Task Model architecture.\n","    \n","    :param dnn_feature_columns: An iterable containing all the features used by deep part of the model.\n","    :param task_type:  str, indicating the loss of each tasks, ``\"binary\"`` for  binary logloss or  ``\"regression\"`` for regression loss.\n","    :param task_names: list of str, indicating the predict target of each tasks. default value is ['ctr', 'ctcvr']\n","    :param tower_dnn_units_lists: list, list of positive integer, the length must be equal to 2, the layer number and units in each layer of task-specific DNN\n","    :param l2_reg_embedding: float. L2 regularizer strength applied to embedding vector\n","    :param l2_reg_dnn: float. L2 regularizer strength applied to DNN\n","    :param seed: integer ,to use as random seed.\n","    :param dnn_dropout: float in [0,1), the probability we will drop out a given DNN coordinate.\n","    :param dnn_activation: Activation function to use in DNN\n","    :param dnn_use_bn: bool. Whether use BatchNormalization before activation or not in DNN\n","    :return: A Keras model instance.\n","    \"\"\"\n","    if len(task_names)!=2:\n","        raise ValueError(\"the length of task_names must be equal to 2\")\n","    \n","    if len(tower_dnn_units_lists)!=2:\n","        raise ValueError(\"the length of tower_dnn_units_lists must be equal to 2\")\n","\n","    features = build_input_features(dnn_feature_columns)\n","    inputs_list = list(features.values())\n","    \n","    sparse_embedding_list, dense_value_list = input_from_feature_columns(features, dnn_feature_columns, l2_reg_embedding,seed)\n","\n","    dnn_input = combined_dnn_input(sparse_embedding_list, dense_value_list)\n","    \n","    ctr_output = DNN(tower_dnn_units_lists[0], dnn_activation, l2_reg_dnn, dnn_dropout, dnn_use_bn, seed=seed)(dnn_input)\n","    cvr_output = DNN(tower_dnn_units_lists[1], dnn_activation, l2_reg_dnn, dnn_dropout, dnn_use_bn, seed=seed)(dnn_input)\n","    \n","    ctr_logit = tf.keras.layers.Dense(1, use_bias=False, activation=None)(ctr_output)\n","    cvr_logit = tf.keras.layers.Dense(1, use_bias=False, activation=None)(cvr_output)\n","    \n","    ctr_pred = PredictionLayer(task_type, name=task_names[0])(ctr_logit) \n","    cvr_pred = PredictionLayer(task_type)(cvr_logit) \n","    \n","    ctcvr_pred = tf.keras.layers.Multiply(name=task_names[1])([ctr_pred, cvr_pred]) #CTCVR = CTR * CVR\n","\n","    model = tf.keras.models.Model(inputs=inputs_list, outputs=[ctr_pred, ctcvr_pred])    \n","    return model"],"execution_count":20,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IX6jiuBLZf56","executionInfo":{"status":"ok","timestamp":1635312457674,"user_tz":-330,"elapsed":6105,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}},"outputId":"fa967eca-c267-456e-eb9b-952ebe12aaf0"},"source":["model = ESSM(dnn_feature_columns, task_type='binary', task_names=['ctr', 'ctcvr'],\n","        tower_dnn_units_lists=[[64, 64],[64, 64]])\n","model.compile(\"adam\", loss=[\"binary_crossentropy\", \"binary_crossentropy\"],\n","              metrics=['AUC'])\n","\n","history = model.fit(train_model_input, [train['ctr_label'].values, train['ctcvr_label'].values],batch_size=256, epochs=5, verbose=2, validation_split=0.0 )\n","\n","pred_ans = model.predict(test_model_input, batch_size=256)"],"execution_count":21,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/5\n","128/128 - 3s - loss: 1.0421 - ctr_loss: 0.5653 - ctcvr_loss: 0.4768 - ctr_auc: 0.6201 - ctcvr_auc_1: 0.8950\n","Epoch 2/5\n","128/128 - 1s - loss: 0.8699 - ctr_loss: 0.5029 - ctcvr_loss: 0.3670 - ctr_auc: 0.7766 - ctcvr_auc_1: 0.9556\n","Epoch 3/5\n","128/128 - 1s - loss: 0.8588 - ctr_loss: 0.4938 - ctcvr_loss: 0.3650 - ctr_auc: 0.7898 - ctcvr_auc_1: 0.9564\n","Epoch 4/5\n","128/128 - 1s - loss: 0.8530 - ctr_loss: 0.4883 - ctcvr_loss: 0.3647 - ctr_auc: 0.7946 - ctcvr_auc_1: 0.9567\n","Epoch 5/5\n","128/128 - 1s - loss: 0.8502 - ctr_loss: 0.4855 - ctcvr_loss: 0.3646 - ctr_auc: 0.7964 - ctcvr_auc_1: 0.9565\n"]}]},{"cell_type":"markdown","metadata":{"id":"xVOqqp7JXG8Y"},"source":["## MMoE"]},{"cell_type":"code","metadata":{"id":"6p2MP7eaXG8i","executionInfo":{"status":"ok","timestamp":1635312468060,"user_tz":-330,"elapsed":820,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}}},"source":["# import tensorflow as tf\n","\n","# from deepctr.feature_column import build_input_features, input_from_feature_columns\n","# from deepctr.layers.core import PredictionLayer, DNN\n","# from deepctr.layers.utils import combined_dnn_input\n","\n","def MMOE(dnn_feature_columns, num_tasks, task_types, task_names, num_experts=4, \n","          expert_dnn_units=[32,32],  gate_dnn_units=[16,16], tower_dnn_units_lists=[[16,8],[16,8]],\n","          l2_reg_embedding=1e-5, l2_reg_dnn=0, seed=1024, dnn_dropout=0, dnn_activation='relu', dnn_use_bn=False):\n","    \"\"\"Instantiates the Multi-gate Mixture-of-Experts multi-task learning architecture.\n","    \n","    :param dnn_feature_columns: An iterable containing all the features used by deep part of the model.\n","    :param num_tasks: integer, number of tasks, equal to number of outputs, must be greater than 1.\n","    :param task_types: list of str, indicating the loss of each tasks, ``\"binary\"`` for  binary logloss, ``\"regression\"`` for regression loss. e.g. ['binary', 'regression']\n","    :param task_names: list of str, indicating the predict target of each tasks\n","    \n","    :param num_experts: integer, number of experts.\n","    :param expert_dnn_units: list, list of positive integer, its length must be greater than 1, the layer number and units in each layer of expert DNN\n","    :param gate_dnn_units: list, list of positive integer, its length must be greater than 1, the layer number and units in each layer of gate DNN\n","    :param tower_dnn_units_lists: list, list of positive integer list, its length must be euqal to num_tasks, the layer number and units in each layer of task-specific DNN\n","    \n","    :param l2_reg_embedding: float. L2 regularizer strength applied to embedding vector\n","    :param l2_reg_dnn: float. L2 regularizer strength applied to DNN\n","    :param seed: integer ,to use as random seed.\n","    :param dnn_dropout: float in [0,1), the probability we will drop out a given DNN coordinate.\n","    :param dnn_activation: Activation function to use in DNN\n","    :param dnn_use_bn: bool. Whether use BatchNormalization before activation or not in DNN\n","    :return: a Keras model instance\n","    \"\"\"\n","    \n","    if num_tasks <= 1:\n","        raise ValueError(\"num_tasks must be greater than 1\")\n","    if len(task_types) != num_tasks:\n","        raise ValueError(\"num_tasks must be equal to the length of task_types\")\n","        \n","    for task_type in task_types:\n","        if task_type not in ['binary', 'regression']:\n","            raise ValueError(\"task must be binary or regression, {} is illegal\".format(task_type))\n","            \n","    if num_tasks != len(tower_dnn_units_lists):\n","        raise ValueError(\"the length of tower_dnn_units_lists must be euqal to num_tasks\")\n","\n","    features = build_input_features(dnn_feature_columns)\n","\n","    inputs_list = list(features.values())\n","\n","    sparse_embedding_list, dense_value_list = input_from_feature_columns(features, dnn_feature_columns,\n","                                                                         l2_reg_embedding, seed)\n","    dnn_input = combined_dnn_input(sparse_embedding_list, dense_value_list)\n","    \n","    #build expert layer\n","    expert_outs = []\n","    for i in range(num_experts):\n","        expert_network = DNN(expert_dnn_units, dnn_activation, l2_reg_dnn, dnn_dropout, dnn_use_bn, seed=seed, name='expert_'+str(i))(dnn_input)\n","        expert_outs.append(expert_network)\n","    expert_concat = tf.keras.layers.concatenate(expert_outs, axis=1, name='expert_concat')\n","    expert_concat = tf.keras.layers.Reshape([num_experts, expert_dnn_units[-1]], name='expert_reshape')(expert_concat) #(num_experts, output dim of expert_network)\n","\n","    mmoe_outs = []\n","    for i in range(num_tasks): #one mmoe layer: nums_tasks = num_gates\n","        #build gate layers\n","        gate_network = DNN(gate_dnn_units, dnn_activation, l2_reg_dnn, dnn_dropout, dnn_use_bn, seed=seed, name='gate_'+task_names[i])(dnn_input)\n","        gate_out = tf.keras.layers.Dense(num_experts, use_bias=False, activation='softmax', name='gate_softmax_'+task_names[i])(gate_network)\n","        gate_out = tf.tile(tf.expand_dims(gate_out, axis=-1), [1, 1, expert_dnn_units[-1]]) #let the shape of gate_out be (num_experts, output dim of expert_network)\n","\n","        #gate multiply the expert\n","        gate_mul_expert = tf.keras.layers.Multiply(name='gate_mul_expert_'+task_names[i])([expert_concat, gate_out]) \n","        gate_mul_expert = tf.math.reduce_sum(gate_mul_expert, axis=1) #sum pooling in the expert ndim\n","        mmoe_outs.append(gate_mul_expert)\n","    \n","    task_outs = []\n","    for task_type, task_name, tower_dnn, mmoe_out in zip(task_types, task_names, tower_dnn_units_lists, mmoe_outs):\n","        #build tower layer\n","        tower_output = DNN(tower_dnn, dnn_activation, l2_reg_dnn, dnn_dropout, dnn_use_bn, seed=seed, name='tower_'+task_name)(mmoe_out)\n","        \n","        logit = tf.keras.layers.Dense(1, use_bias=False, activation=None)(tower_output)\n","        output = PredictionLayer(task_type, name=task_name)(logit) \n","        task_outs.append(output)\n","        \n","    model = tf.keras.models.Model(inputs=inputs_list, outputs=task_outs)\n","    return model"],"execution_count":23,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"37XlaPF2bQ3t","executionInfo":{"status":"ok","timestamp":1635312486435,"user_tz":-330,"elapsed":17542,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}},"outputId":"28e1d482-add5-4baa-ba7a-fd4ae5c9f477"},"source":["model = MMOE(dnn_feature_columns, num_tasks=2, task_types=['binary', 'binary'], task_names=task_names, \n","num_experts=8, expert_dnn_units=[64,64], gate_dnn_units=[32,32], tower_dnn_units_lists=[[32,32],[32,32]])\n","model.compile(\"adam\", loss=[\"binary_crossentropy\", \"binary_crossentropy\"], metrics=['AUC'])\n","\n","history = model.fit(train_model_input, [train['ctr_label'].values, train['ctcvr_label'].values], batch_size=256, epochs=5, verbose=2, validation_split=0.0 )\n","\n","pred_ans = model.predict(test_model_input, batch_size=256)"],"execution_count":24,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/5\n","128/128 - 8s - loss: 0.8699 - income_loss: 0.4430 - marital_loss: 0.4269 - income_auc: 0.7902 - marital_auc_1: 0.8515\n","Epoch 2/5\n","128/128 - 2s - loss: 0.5630 - income_loss: 0.3321 - marital_loss: 0.2308 - income_auc: 0.8990 - marital_auc_1: 0.9617\n","Epoch 3/5\n","128/128 - 2s - loss: 0.5476 - income_loss: 0.3207 - marital_loss: 0.2268 - income_auc: 0.9061 - marital_auc_1: 0.9630\n","Epoch 4/5\n","128/128 - 2s - loss: 0.5412 - income_loss: 0.3165 - marital_loss: 0.2247 - income_auc: 0.9084 - marital_auc_1: 0.9637\n","Epoch 5/5\n","128/128 - 2s - loss: 0.5340 - income_loss: 0.3111 - marital_loss: 0.2228 - income_auc: 0.9115 - marital_auc_1: 0.9643\n"]}]},{"cell_type":"markdown","metadata":{"id":"YGUyMNPgVV3e"},"source":["## CGC"]},{"cell_type":"code","metadata":{"id":"Roq2o3zzVV3w","executionInfo":{"status":"ok","timestamp":1635312486436,"user_tz":-330,"elapsed":13,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}}},"source":["# import tensorflow as tf\n","\n","# from deepctr.feature_column import build_input_features, input_from_feature_columns\n","# from deepctr.layers.core import PredictionLayer, DNN\n","# from deepctr.layers.utils import combined_dnn_input \n","\n","def PLE_CGC(dnn_feature_columns, num_tasks, task_types, task_names, num_experts_specific=8, num_experts_shared=4,\n","          expert_dnn_units=[64,64],  gate_dnn_units=[16,16], tower_dnn_units_lists=[[16,16],[16,16]],\n","          l2_reg_embedding=1e-5, l2_reg_dnn=0, seed=1024, dnn_dropout=0, dnn_activation='relu', dnn_use_bn=False):\n","    \"\"\"Instantiates the Customized Gate Control block of Progressive Layered Extraction architecture.\n","    \n","    :param dnn_feature_columns: An iterable containing all the features used by deep part of the model.\n","    :param num_tasks: integer, number of tasks, equal to number of outputs, must be greater than 1.\n","    :param task_types: list of str, indicating the loss of each tasks, ``\"binary\"`` for  binary logloss, ``\"regression\"`` for regression loss. e.g. ['binary', 'regression']\n","    :param task_names: list of str, indicating the predict target of each tasks\n","    \n","    :param num_experts_specific: integer, number of task-specific experts.\n","    :param num_experts_shared: integer, number of task-shared experts.\n","    :param expert_dnn_units: list, list of positive integer, its length must be greater than 1, the layer number and units in each layer of expert DNN\n","    :param gate_dnn_units: list, list of positive integer, its length must be greater than 1, the layer number and units in each layer of gate DNN\n","    :param tower_dnn_units_lists: list, list of positive integer list, its length must be euqal to num_tasks, the layer number and units in each layer of task-specific DNN\n","    \n","    :param l2_reg_embedding: float. L2 regularizer strength applied to embedding vector\n","    :param l2_reg_dnn: float. L2 regularizer strength applied to DNN\n","    :param seed: integer ,to use as random seed.\n","    :param dnn_dropout: float in [0,1), the probability we will drop out a given DNN coordinate.\n","    :param dnn_activation: Activation function to use in DNN\n","    :param dnn_use_bn: bool. Whether use BatchNormalization before activation or not in DNN\n","    :return: a Keras model instance\n","    \"\"\"\n","    \n","    if num_tasks <= 1:\n","        raise ValueError(\"num_tasks must be greater than 1\")\n","    if len(task_types) != num_tasks:\n","        raise ValueError(\"num_tasks must be equal to the length of task_types\")\n","        \n","    for task_type in task_types:\n","        if task_type not in ['binary', 'regression']:\n","            raise ValueError(\"task must be binary or regression, {} is illegal\".format(task_type))\n","            \n","    if num_tasks != len(tower_dnn_units_lists):\n","        raise ValueError(\"the length of tower_dnn_units_lists must be euqal to num_tasks\")\n","\n","    features = build_input_features(dnn_feature_columns)\n","\n","    inputs_list = list(features.values())\n","\n","    sparse_embedding_list, dense_value_list = input_from_feature_columns(features, dnn_feature_columns,\n","                                                                         l2_reg_embedding, seed)\n","    dnn_input = combined_dnn_input(sparse_embedding_list, dense_value_list)\n","    \n","    expert_outputs = []\n","    #build task-specific expert layer\n","    for i in range(num_tasks):\n","        for j in range(num_experts_specific):\n","            expert_network = DNN(expert_dnn_units, dnn_activation, l2_reg_dnn, dnn_dropout, dnn_use_bn, seed=seed, name='task_'+task_names[i]+'_expert_specific_'+str(j))(dnn_input)\n","            expert_outputs.append(expert_network)\n","\n","    #build task-shared expert layer\n","    for i in range(num_experts_shared):\n","        expert_network = DNN(expert_dnn_units, dnn_activation, l2_reg_dnn, dnn_dropout, dnn_use_bn, seed=seed, name='expert_shared_'+str(i))(dnn_input)\n","        expert_outputs.append(expert_network)\n","        \n","\n","\n","    cgc_outs = []\n","    for i in range(num_tasks): \n","        #concat task-specific expert and task-shared expert\n","        cur_expert_num = num_experts_specific + num_experts_shared\n","        cur_experts = expert_outputs[i * num_experts_specific:(i + 1) * num_experts_specific] + expert_outputs[-int(num_experts_shared):] #task_specific + task_shared\n","        expert_concat = tf.keras.layers.concatenate(cur_experts, axis=1, name='expert_concat_'+task_names[i])\n","        expert_concat = tf.keras.layers.Reshape([cur_expert_num, expert_dnn_units[-1]], name='expert_reshape_'+task_names[i])(expert_concat)\n","        \n","        #build gate layers\n","        gate_network = DNN(gate_dnn_units, dnn_activation, l2_reg_dnn, dnn_dropout, dnn_use_bn, seed=seed, name='gate_'+task_names[i])(dnn_input)\n","        gate_out = tf.keras.layers.Dense(cur_expert_num, use_bias=False, activation='softmax', name='gate_softmax_'+task_names[i])(gate_network)\n","        gate_out = tf.tile(tf.expand_dims(gate_out, axis=-1), [1, 1, expert_dnn_units[-1]]) \n","        \n","        #gate multiply the expert\n","        gate_mul_expert = tf.keras.layers.Multiply(name='gate_mul_expert_'+task_names[i])([expert_concat, gate_out]) \n","        gate_mul_expert = tf.math.reduce_sum(gate_mul_expert, axis=1) #sum pooling in the expert ndim\n","        cgc_outs.append(gate_mul_expert)\n","    \n","    task_outs = []\n","    for task_type, task_name, tower_dnn, cgc_out in zip(task_types, task_names, tower_dnn_units_lists, cgc_outs):\n","        #build tower layer\n","        tower_output = DNN(tower_dnn, dnn_activation, l2_reg_dnn, dnn_dropout, dnn_use_bn, seed=seed, name='tower_'+task_name)(cgc_out)\n","        logit = tf.keras.layers.Dense(1, use_bias=False, activation=None)(tower_output)\n","        output = PredictionLayer(task_type, name=task_name)(logit) \n","        task_outs.append(output)\n","        \n","    model = tf.keras.models.Model(inputs=inputs_list, outputs=task_outs)\n","    return model"],"execution_count":25,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PGDbvHTMbWXd","executionInfo":{"status":"ok","timestamp":1635312517472,"user_tz":-330,"elapsed":31047,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}},"outputId":"79dab04c-55f4-4680-cc8a-8f7fae0d5dbc"},"source":["model = PLE_CGC(dnn_feature_columns, num_tasks=2, task_types=['binary', 'binary'], task_names=task_names, \n","num_experts_specific=8, num_experts_shared=4, expert_dnn_units=[64,64],  gate_dnn_units=[16,16], tower_dnn_units_lists=[[32,32],[32,32]])\n","model.compile(\"adam\", loss=[\"binary_crossentropy\", \"binary_crossentropy\"], metrics=['AUC'])\n","\n","history = model.fit(train_model_input, [train['ctr_label'].values, train['ctcvr_label'].values], batch_size=256, epochs=5, verbose=2, validation_split=0.0 )\n","\n","pred_ans = model.predict(test_model_input, batch_size=256)"],"execution_count":26,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/5\n","128/128 - 12s - loss: 0.8895 - income_loss: 0.4479 - marital_loss: 0.4415 - income_auc: 0.7814 - marital_auc_1: 0.8322\n","Epoch 2/5\n","128/128 - 4s - loss: 0.5638 - income_loss: 0.3326 - marital_loss: 0.2311 - income_auc: 0.8980 - marital_auc_1: 0.9616\n","Epoch 3/5\n","128/128 - 4s - loss: 0.5487 - income_loss: 0.3196 - marital_loss: 0.2290 - income_auc: 0.9063 - marital_auc_1: 0.9623\n","Epoch 4/5\n","128/128 - 4s - loss: 0.5402 - income_loss: 0.3148 - marital_loss: 0.2253 - income_auc: 0.9092 - marital_auc_1: 0.9635\n","Epoch 5/5\n","128/128 - 4s - loss: 0.5371 - income_loss: 0.3119 - marital_loss: 0.2251 - income_auc: 0.9109 - marital_auc_1: 0.9636\n"]}]},{"cell_type":"markdown","metadata":{"id":"oNCTMODkVWGg"},"source":["## PLE"]},{"cell_type":"code","metadata":{"id":"58llgjkxVWGs","executionInfo":{"status":"ok","timestamp":1635312519416,"user_tz":-330,"elapsed":738,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}}},"source":["# import tensorflow as tf\n","\n","# from deepctr.feature_column import build_input_features, input_from_feature_columns\n","# from deepctr.layers.core import PredictionLayer, DNN\n","# from deepctr.layers.utils import combined_dnn_input \n","\n","def PLE(dnn_feature_columns, num_tasks, task_types, task_names, num_levels=1, num_experts_specific=8, num_experts_shared=4,\n","          expert_dnn_units=[64,64],  gate_dnn_units=[16,16], tower_dnn_units_lists=[[16,16],[16,16]],\n","          l2_reg_embedding=1e-5, l2_reg_dnn=0, seed=1024, dnn_dropout=0, dnn_activation='relu', dnn_use_bn=False):\n","    \"\"\"Instantiates the multi level of Customized Gate Control of Progressive Layered Extraction architecture.\n","    \n","    :param dnn_feature_columns: An iterable containing all the features used by deep part of the model.\n","    :param num_tasks: integer, number of tasks, equal to number of outputs, must be greater than 1.\n","    :param task_types: list of str, indicating the loss of each tasks, ``\"binary\"`` for  binary logloss, ``\"regression\"`` for regression loss. e.g. ['binary', 'regression']\n","    :param task_names: list of str, indicating the predict target of each tasks\n","    \n","    :param num_levels: integer, number of CGC levels.\n","    :param num_experts_specific: integer, number of task-specific experts.\n","    :param num_experts_shared: integer, number of task-shared experts.\n","    :param expert_dnn_units: list, list of positive integer, its length must be greater than 1, the layer number and units in each layer of expert DNN\n","    :param gate_dnn_units: list, list of positive integer, its length must be greater than 1, the layer number and units in each layer of gate DNN\n","    :param tower_dnn_units_lists: list, list of positive integer list, its length must be euqal to num_tasks, the layer number and units in each layer of task-specific DNN\n","    \n","    :param l2_reg_embedding: float. L2 regularizer strength applied to embedding vector\n","    :param l2_reg_dnn: float. L2 regularizer strength applied to DNN\n","    :param seed: integer ,to use as random seed.\n","    :param dnn_dropout: float in [0,1), the probability we will drop out a given DNN coordinate.\n","    :param dnn_activation: Activation function to use in DNN\n","    :param dnn_use_bn: bool. Whether use BatchNormalization before activation or not in DNN\n","    :return: a Keras model instance\n","    \"\"\"\n","    \n","    if num_tasks <= 1:\n","        raise ValueError(\"num_tasks must be greater than 1\")\n","    if len(task_types) != num_tasks:\n","        raise ValueError(\"num_tasks must be equal to the length of task_types\")\n","        \n","    for task_type in task_types:\n","        if task_type not in ['binary', 'regression']:\n","            raise ValueError(\"task must be binary or regression, {} is illegal\".format(task_type))\n","            \n","    if num_tasks != len(tower_dnn_units_lists):\n","        raise ValueError(\"the length of tower_dnn_units_lists must be euqal to num_tasks\")\n","\n","    features = build_input_features(dnn_feature_columns)\n","\n","    inputs_list = list(features.values())\n","\n","    sparse_embedding_list, dense_value_list = input_from_feature_columns(features, dnn_feature_columns,\n","                                                                         l2_reg_embedding, seed)\n","    dnn_input = combined_dnn_input(sparse_embedding_list, dense_value_list)\n"," \n","    #single cgc layer\n","    def cgc_net(inputs, level_name, is_last=False):\n","        #inputs: [task1, task2, ... taskn, shared task]\n","        \n","        expert_outputs = []\n","        #build task-specific expert layer\n","        for i in range(num_tasks):\n","            for j in range(num_experts_specific):\n","                expert_network = DNN(expert_dnn_units, dnn_activation, l2_reg_dnn, dnn_dropout, dnn_use_bn, seed=seed, name=level_name+'task_'+task_names[i]+'_expert_specific_'+str(j))(inputs[i])\n","                expert_outputs.append(expert_network)\n","\n","        #build task-shared expert layer\n","        for i in range(num_experts_shared):\n","            expert_network = DNN(expert_dnn_units, dnn_activation, l2_reg_dnn, dnn_dropout, dnn_use_bn, seed=seed, name=level_name+'expert_shared_'+str(i))(inputs[-1]) \n","            expert_outputs.append(expert_network)\n","\n","        #task_specific gate (count = num_tasks)\n","        cgc_outs = []\n","        for i in range(num_tasks): \n","            #concat task-specific expert and task-shared expert\n","            cur_expert_num = num_experts_specific + num_experts_shared\n","            cur_experts = expert_outputs[i * num_experts_specific:(i + 1) * num_experts_specific] + expert_outputs[-int(num_experts_shared):] #task_specific + task_shared\n","            \n","            expert_concat = tf.keras.layers.concatenate(cur_experts, axis=1, name=level_name+'expert_concat_specific_'+task_names[i])\n","            expert_concat = tf.keras.layers.Reshape([cur_expert_num, expert_dnn_units[-1]], name=level_name+'expert_reshape_specific_'+task_names[i])(expert_concat)\n","\n","            #build gate layers\n","            gate_network = DNN(gate_dnn_units, dnn_activation, l2_reg_dnn, dnn_dropout, dnn_use_bn, seed=seed, name=level_name+'gate_specific_'+task_names[i])(inputs[i]) #gate[i] for task input[i]\n","            gate_out = tf.keras.layers.Dense(cur_expert_num, use_bias=False, activation='softmax', name=level_name+'gate_softmax_specific_'+task_names[i])(gate_network)\n","            gate_out = tf.tile(tf.expand_dims(gate_out, axis=-1), [1, 1, expert_dnn_units[-1]]) \n","\n","            #gate multiply the expert\n","            gate_mul_expert = tf.keras.layers.Multiply(name=level_name+'gate_mul_expert_specific_'+task_names[i])([expert_concat, gate_out]) \n","            gate_mul_expert = tf.math.reduce_sum(gate_mul_expert, axis=1) #sum pooling in the expert ndim\n","            cgc_outs.append(gate_mul_expert)\n","        \n","        #task_shared gate, if the level not in last, add one shared gate\n","        if not is_last:\n","            cur_expert_num = num_tasks * num_experts_specific + num_experts_shared\n","            cur_experts = expert_outputs #all the expert include task-specific expert and task-shared expert\n","            \n","            expert_concat = tf.keras.layers.concatenate(cur_experts, axis=1, name=level_name+'expert_concat_shared_'+task_names[i])\n","            expert_concat = tf.keras.layers.Reshape([cur_expert_num, expert_dnn_units[-1]], name=level_name+'expert_reshape_shared_'+task_names[i])(expert_concat)\n","            \n","            #build gate layers\n","            gate_network = DNN(gate_dnn_units, dnn_activation, l2_reg_dnn, dnn_dropout, dnn_use_bn, seed=seed, name=level_name+'gate_shared_'+str(i))(inputs[-1]) #gate for shared task input\n","            gate_out = tf.keras.layers.Dense(cur_expert_num, use_bias=False, activation='softmax', name=level_name+'gate_softmax_shared_'+str(i))(gate_network)\n","            gate_out = tf.tile(tf.expand_dims(gate_out, axis=-1), [1, 1, expert_dnn_units[-1]]) \n","\n","            #gate multiply the expert\n","            gate_mul_expert = tf.keras.layers.Multiply(name=level_name+'gate_mul_expert_shared_'+task_names[i])([expert_concat, gate_out]) \n","            gate_mul_expert = tf.math.reduce_sum(gate_mul_expert, axis=1) #sum pooling in the expert ndim\n","            cgc_outs.append(gate_mul_expert)\n","        return cgc_outs\n","    \n","    ple_inputs = [dnn_input]*(num_tasks+1) #[task1, task2, ... taskn, shared task]\n","    ple_outputs = []\n","    for i in range(num_levels):\n","        if i == num_levels-1: #the last level\n","            ple_outputs = cgc_net(inputs=ple_inputs, level_name='level_'+str(i)+'_', is_last=True)\n","            break\n","        else:\n","            ple_outputs = cgc_net(inputs=ple_inputs, level_name='level_'+str(i)+'_', is_last=False)\n","            ple_inputs = ple_outputs\n","            \n","    task_outs = []\n","    for task_type, task_name, tower_dnn, ple_out in zip(task_types, task_names, tower_dnn_units_lists, ple_outputs):\n","        #build tower layer\n","        tower_output = DNN(tower_dnn, dnn_activation, l2_reg_dnn, dnn_dropout, dnn_use_bn, seed=seed, name='tower_'+task_name)(ple_out)\n","        logit = tf.keras.layers.Dense(1, use_bias=False, activation=None)(tower_output)\n","        output = PredictionLayer(task_type, name=task_name)(logit) \n","        task_outs.append(output)\n","        \n","    model = tf.keras.models.Model(inputs=inputs_list, outputs=task_outs)\n","    return model"],"execution_count":27,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Gx0FBpalbcvA","executionInfo":{"status":"ok","timestamp":1635312574946,"user_tz":-330,"elapsed":55538,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}},"outputId":"251c116a-82b9-4b19-ef23-de0dc2de2aac"},"source":["#Test PLE Model\n","\n","model = PLE(dnn_feature_columns, num_tasks=2, task_types=['binary', 'binary'], task_names=task_names, \n","num_levels=2, num_experts_specific=8, num_experts_shared=4, expert_dnn_units=[64,64],  gate_dnn_units=[16,16], tower_dnn_units_lists=[[32,32],[32,32]])\n","model.compile(\"adam\", loss=[\"binary_crossentropy\", \"binary_crossentropy\"], metrics=['AUC'])\n","\n","history = model.fit(train_model_input, [train['ctr_label'].values, train['ctcvr_label'].values], batch_size=256, epochs=5, verbose=2, validation_split=0.0 )\n","\n","pred_ans = model.predict(test_model_input, batch_size=256)"],"execution_count":28,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/5\n","128/128 - 22s - loss: 0.8928 - income_loss: 0.4675 - marital_loss: 0.4253 - income_auc: 0.7553 - marital_auc_1: 0.8548\n","Epoch 2/5\n","128/128 - 7s - loss: 0.5642 - income_loss: 0.3352 - marital_loss: 0.2290 - income_auc: 0.8969 - marital_auc_1: 0.9622\n","Epoch 3/5\n","128/128 - 7s - loss: 0.5450 - income_loss: 0.3193 - marital_loss: 0.2257 - income_auc: 0.9066 - marital_auc_1: 0.9633\n","Epoch 4/5\n","128/128 - 7s - loss: 0.5368 - income_loss: 0.3128 - marital_loss: 0.2240 - income_auc: 0.9102 - marital_auc_1: 0.9639\n","Epoch 5/5\n","128/128 - 7s - loss: 0.5328 - income_loss: 0.3104 - marital_loss: 0.2223 - income_auc: 0.9117 - marital_auc_1: 0.9645\n"]}]},{"cell_type":"markdown","metadata":{"id":"dby3KYwfOatU"},"source":["## Watermark"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1OXidUrYOc3i","executionInfo":{"status":"ok","timestamp":1635312610609,"user_tz":-330,"elapsed":4202,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}},"outputId":"835a3c4f-c399-4a33-c83c-51cdaebd8064"},"source":["!pip install -q watermark\n","%reload_ext watermark\n","%watermark -m -iv -u -t -d"],"execution_count":29,"outputs":[{"output_type":"stream","name":"stdout","text":["Last updated: 2021-10-27 05:30:19\n","\n","Compiler    : GCC 7.5.0\n","OS          : Linux\n","Release     : 5.4.104+\n","Machine     : x86_64\n","Processor   : x86_64\n","CPU cores   : 2\n","Architecture: 64bit\n","\n","pandas    : 1.1.5\n","tensorflow: 2.4.3\n","numpy     : 1.19.5\n","IPython   : 5.5.0\n","\n"]}]}]}