{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "2021-07-05-mf-movielens-pytorch.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyO/2SiJ8nwNILq9HM82oyWg"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5uDaQrFWMDif"
      },
      "source": [
        "# Matrix Factorization based Movie Recommender built in PyTorch\n",
        "> Simple PyTorch based Matrix Factorization models on movielens-100k dataset - implicit, explicit and hogwild variant\n",
        "\n",
        "- toc: true\n",
        "- badges: true\n",
        "- comments: true\n",
        "- categories: [PyTorch, Movie, MF, Factorization]\n",
        "- author: \"<a href='https://github.com/EthanRosenthal/torchmf'>Ethan Rosenthal</a>\"\n",
        "- image:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2jqJNaQhK8ji"
      },
      "source": [
        "## utils.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vsby4vwWGlWJ",
        "outputId": "35f3b79f-4242-426d-9ace-10adc6f5fb69"
      },
      "source": [
        "%%writefile utils.py\n",
        "\n",
        "import os\n",
        "import requests\n",
        "import zipfile\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import scipy.sparse as sp\n",
        "\n",
        "\"\"\"\n",
        "Shamelessly stolen from\n",
        "https://github.com/maciejkula/triplet_recommendations_keras\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "def train_test_split(interactions, n=10):\n",
        "    \"\"\"\n",
        "    Split an interactions matrix into training and test sets.\n",
        "    Parameters\n",
        "    ----------\n",
        "    interactions : np.ndarray\n",
        "    n : int (default=10)\n",
        "        Number of items to select / row to place into test.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    train : np.ndarray\n",
        "    test : np.ndarray\n",
        "    \"\"\"\n",
        "    test = np.zeros(interactions.shape)\n",
        "    train = interactions.copy()\n",
        "    for user in range(interactions.shape[0]):\n",
        "        if interactions[user, :].nonzero()[0].shape[0] > n:\n",
        "            test_interactions = np.random.choice(interactions[user, :].nonzero()[0],\n",
        "                                                 size=n,\n",
        "                                                 replace=False)\n",
        "            train[user, test_interactions] = 0.\n",
        "            test[user, test_interactions] = interactions[user, test_interactions]\n",
        "\n",
        "    # Test and training are truly disjoint\n",
        "    assert(np.all((train * test) == 0))\n",
        "    return train, test\n",
        "\n",
        "\n",
        "def _get_data_path():\n",
        "    \"\"\"\n",
        "    Get path to the movielens dataset file.\n",
        "    \"\"\"\n",
        "    data_path = os.path.join(os.path.dirname(os.path.abspath(__file__)),\n",
        "                        'data')\n",
        "    if not os.path.exists(data_path):\n",
        "        print('Making data path')\n",
        "        os.mkdir(data_path)\n",
        "    return data_path\n",
        "\n",
        "\n",
        "def _download_movielens(dest_path):\n",
        "    \"\"\"\n",
        "    Download the dataset.\n",
        "    \"\"\"\n",
        "\n",
        "    url = 'http://files.grouplens.org/datasets/movielens/ml-100k.zip'\n",
        "    req = requests.get(url, stream=True)\n",
        "\n",
        "    print('Downloading MovieLens data')\n",
        "\n",
        "    with open(os.path.join(dest_path, 'ml-100k.zip'), 'wb') as fd:\n",
        "        for chunk in req.iter_content(chunk_size=None):\n",
        "            fd.write(chunk)\n",
        "\n",
        "    with zipfile.ZipFile(os.path.join(dest_path, 'ml-100k.zip'), 'r') as z:\n",
        "        z.extractall(dest_path)\n",
        "\n",
        "\n",
        "def read_movielens_df():\n",
        "    path = _get_data_path()\n",
        "    zipfile = os.path.join(path, 'ml-100k.zip')\n",
        "    if not os.path.isfile(zipfile):\n",
        "        _download_movielens(path)\n",
        "    fname = os.path.join(path, 'ml-100k', 'u.data')\n",
        "    names = ['user_id', 'item_id', 'rating', 'timestamp']\n",
        "    df = pd.read_csv(fname, sep='\\t', names=names)\n",
        "    return df\n",
        "\n",
        "\n",
        "def get_movielens_interactions():\n",
        "    df = read_movielens_df()\n",
        "\n",
        "    n_users = df.user_id.unique().shape[0]\n",
        "    n_items = df.item_id.unique().shape[0]\n",
        "\n",
        "    interactions = np.zeros((n_users, n_items))\n",
        "    for row in df.itertuples():\n",
        "        interactions[row[1] - 1, row[2] - 1] = row[3]\n",
        "    return interactions\n",
        "\n",
        "\n",
        "def get_movielens_train_test_split(implicit=False):\n",
        "    interactions = get_movielens_interactions()\n",
        "    if implicit:\n",
        "        interactions = (interactions >= 4).astype(np.float32)\n",
        "    train, test = train_test_split(interactions)\n",
        "    train = sp.coo_matrix(train)\n",
        "    test = sp.coo_matrix(test)\n",
        "    return train, test"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Writing utils.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d9wghzkqLR2i"
      },
      "source": [
        "## metrics.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EWmOo0AqKiEN",
        "outputId": "4362643e-6e76-4913-9e5d-75afa8ec3753"
      },
      "source": [
        "%%writefile metrics.py\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from torch import multiprocessing as mp\n",
        "import torch\n",
        "\n",
        "\n",
        "def get_row_indices(row, interactions):\n",
        "    start = interactions.indptr[row]\n",
        "    end = interactions.indptr[row + 1]\n",
        "    return interactions.indices[start:end]\n",
        "\n",
        "\n",
        "def auc(model, interactions, num_workers=1):\n",
        "    aucs = []\n",
        "    processes = []\n",
        "    n_users = interactions.shape[0]\n",
        "    mp_batch = int(np.ceil(n_users / num_workers))\n",
        "\n",
        "    queue = mp.Queue()\n",
        "    rows = np.arange(n_users)\n",
        "    np.random.shuffle(rows)\n",
        "    for rank in range(num_workers):\n",
        "        start = rank * mp_batch\n",
        "        end = np.min((start + mp_batch,  n_users))\n",
        "        p = mp.Process(target=batch_auc,\n",
        "                       args=(queue, rows[start:end], interactions, model))\n",
        "        p.start()\n",
        "        processes.append(p)\n",
        "\n",
        "    while True:\n",
        "        is_alive = False\n",
        "        for p in processes:\n",
        "            if p.is_alive():\n",
        "                is_alive = True\n",
        "                break\n",
        "        if not is_alive and queue.empty():\n",
        "            break\n",
        "\n",
        "        while not queue.empty():\n",
        "            aucs.append(queue.get())\n",
        "\n",
        "    queue.close()\n",
        "    for p in processes:\n",
        "        p.join()\n",
        "    return np.mean(aucs)\n",
        "\n",
        "\n",
        "def batch_auc(queue, rows, interactions, model):\n",
        "    n_items = interactions.shape[1]\n",
        "    items = torch.arange(0, n_items).long()\n",
        "    users_init = torch.ones(n_items).long()\n",
        "    for row in rows:\n",
        "        row = int(row)\n",
        "        users = users_init.fill_(row)\n",
        "\n",
        "        preds = model.predict(users, items)\n",
        "        actuals = get_row_indices(row, interactions)\n",
        "\n",
        "        if len(actuals) == 0:\n",
        "            continue\n",
        "        y_test = np.zeros(n_items)\n",
        "        y_test[actuals] = 1\n",
        "        queue.put(roc_auc_score(y_test, preds.data.numpy()))\n",
        "\n",
        "\n",
        "def patk(model, interactions, num_workers=1, k=5):\n",
        "    patks = []\n",
        "    processes = []\n",
        "    n_users = interactions.shape[0]\n",
        "    mp_batch = int(np.ceil(n_users / num_workers))\n",
        "\n",
        "    queue = mp.Queue()\n",
        "    rows = np.arange(n_users)\n",
        "    np.random.shuffle(rows)\n",
        "    for rank in range(num_workers):\n",
        "        start = rank * mp_batch\n",
        "        end = np.min((start + mp_batch, n_users))\n",
        "        p = mp.Process(target=batch_patk,\n",
        "                       args=(queue, rows[start:end], interactions, model),\n",
        "                       kwargs={'k': k})\n",
        "        p.start()\n",
        "        processes.append(p)\n",
        "\n",
        "    while True:\n",
        "        is_alive = False\n",
        "        for p in processes:\n",
        "            if p.is_alive():\n",
        "                is_alive = True\n",
        "                break\n",
        "        if not is_alive and queue.empty():\n",
        "            break\n",
        "\n",
        "        while not queue.empty():\n",
        "            patks.append(queue.get())\n",
        "\n",
        "    queue.close()\n",
        "    for p in processes:\n",
        "        p.join()\n",
        "    return np.mean(patks)\n",
        "\n",
        "\n",
        "def batch_patk(queue, rows, interactions, model, k=5):\n",
        "    n_items = interactions.shape[1]\n",
        "\n",
        "    items = torch.arange(0, n_items).long()\n",
        "    users_init = torch.ones(n_items).long()\n",
        "    for row in rows:\n",
        "        row = int(row)\n",
        "        users = users_init.fill_(row)\n",
        "\n",
        "        preds = model.predict(users, items)\n",
        "        actuals = get_row_indices(row, interactions)\n",
        "\n",
        "        if len(actuals) == 0:\n",
        "            continue\n",
        "\n",
        "        top_k = np.argpartition(-np.squeeze(preds.data.numpy()), k)\n",
        "        top_k = set(top_k[:k])\n",
        "        true_pids = set(actuals)\n",
        "        if true_pids:\n",
        "            queue.put(len(top_k & true_pids) / float(k))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Writing metrics.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7N1Rl15-LJAj"
      },
      "source": [
        "## torchmf.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NUlsa6LeLKqu",
        "outputId": "1c82c38f-467e-4bfc-9280-4a053e3d924a"
      },
      "source": [
        "%%writefile torchmf.py\n",
        "\n",
        "import collections\n",
        "import os\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.metrics import roc_auc_score\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.multiprocessing as mp\n",
        "import torch.utils.data as data\n",
        "from tqdm import tqdm\n",
        "\n",
        "import metrics\n",
        "\n",
        "\n",
        "# Models\n",
        "# Interactions Dataset => Singular Iter => Singular Loss\n",
        "# Pairwise Datasets => Pairwise Iter => Pairwise Loss\n",
        "# Pairwise Iters\n",
        "# Loss Functions\n",
        "# Optimizers\n",
        "# Metric callbacks\n",
        "\n",
        "# Serve up users, items (and items could be pos_items, neg_items)\n",
        "# In this case, the iteration remains the same. Pass both items into a model\n",
        "# which is a concat of the base model. it handles the pos and neg_items\n",
        "# accordingly. define the loss after.\n",
        "\n",
        "\n",
        "class Interactions(data.Dataset):\n",
        "    \"\"\"\n",
        "    Hold data in the form of an interactions matrix.\n",
        "    Typical use-case is like a ratings matrix:\n",
        "    - Users are the rows\n",
        "    - Items are the columns\n",
        "    - Elements of the matrix are the ratings given by a user for an item.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, mat):\n",
        "        self.mat = mat.astype(np.float32).tocoo()\n",
        "        self.n_users = self.mat.shape[0]\n",
        "        self.n_items = self.mat.shape[1]\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        row = self.mat.row[index]\n",
        "        col = self.mat.col[index]\n",
        "        val = self.mat.data[index]\n",
        "        return (row, col), val\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.mat.nnz\n",
        "\n",
        "\n",
        "class PairwiseInteractions(data.Dataset):\n",
        "    \"\"\"\n",
        "    Sample data from an interactions matrix in a pairwise fashion. The row is\n",
        "    treated as the main dimension, and the columns are sampled pairwise.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, mat):\n",
        "        self.mat = mat.astype(np.float32).tocoo()\n",
        "\n",
        "        self.n_users = self.mat.shape[0]\n",
        "        self.n_items = self.mat.shape[1]\n",
        "\n",
        "        self.mat_csr = self.mat.tocsr()\n",
        "        if not self.mat_csr.has_sorted_indices:\n",
        "            self.mat_csr.sort_indices()\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        row = self.mat.row[index]\n",
        "        found = False\n",
        "\n",
        "        while not found:\n",
        "            neg_col = np.random.randint(self.n_items)\n",
        "            if self.not_rated(row, neg_col, self.mat_csr.indptr,\n",
        "                              self.mat_csr.indices):\n",
        "                found = True\n",
        "\n",
        "        pos_col = self.mat.col[index]\n",
        "        val = self.mat.data[index]\n",
        "\n",
        "        return (row, (pos_col, neg_col)), val\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.mat.nnz\n",
        "\n",
        "    @staticmethod\n",
        "    def not_rated(row, col, indptr, indices):\n",
        "        # similar to use of bsearch in lightfm\n",
        "        start = indptr[row]\n",
        "        end = indptr[row + 1]\n",
        "        searched = np.searchsorted(indices[start:end], col, 'right')\n",
        "        if searched >= (end - start):\n",
        "            # After the array\n",
        "            return False\n",
        "        return col != indices[searched]  # Not found\n",
        "\n",
        "    def get_row_indices(self, row):\n",
        "        start = self.mat_csr.indptr[row]\n",
        "        end = self.mat_csr.indptr[row + 1]\n",
        "        return self.mat_csr.indices[start:end]\n",
        "\n",
        "\n",
        "class BaseModule(nn.Module):\n",
        "    \"\"\"\n",
        "    Base module for explicit matrix factorization.\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self,\n",
        "                 n_users,\n",
        "                 n_items,\n",
        "                 n_factors=40,\n",
        "                 dropout_p=0,\n",
        "                 sparse=False):\n",
        "        \"\"\"\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        n_users : int\n",
        "            Number of users\n",
        "        n_items : int\n",
        "            Number of items\n",
        "        n_factors : int\n",
        "            Number of latent factors (or embeddings or whatever you want to\n",
        "            call it).\n",
        "        dropout_p : float\n",
        "            p in nn.Dropout module. Probability of dropout.\n",
        "        sparse : bool\n",
        "            Whether or not to treat embeddings as sparse. NOTE: cannot use\n",
        "            weight decay on the optimizer if sparse=True. Also, can only use\n",
        "            Adagrad.\n",
        "        \"\"\"\n",
        "        super(BaseModule, self).__init__()\n",
        "        self.n_users = n_users\n",
        "        self.n_items = n_items\n",
        "        self.n_factors = n_factors\n",
        "        self.user_biases = nn.Embedding(n_users, 1, sparse=sparse)\n",
        "        self.item_biases = nn.Embedding(n_items, 1, sparse=sparse)\n",
        "        self.user_embeddings = nn.Embedding(n_users, n_factors, sparse=sparse)\n",
        "        self.item_embeddings = nn.Embedding(n_items, n_factors, sparse=sparse)\n",
        "        \n",
        "        self.dropout_p = dropout_p\n",
        "        self.dropout = nn.Dropout(p=self.dropout_p)\n",
        "\n",
        "        self.sparse = sparse\n",
        "        \n",
        "    def forward(self, users, items):\n",
        "        \"\"\"\n",
        "        Forward pass through the model. For a single user and item, this\n",
        "        looks like:\n",
        "\n",
        "        user_bias + item_bias + user_embeddings.dot(item_embeddings)\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        users : np.ndarray\n",
        "            Array of user indices\n",
        "        items : np.ndarray\n",
        "            Array of item indices\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        preds : np.ndarray\n",
        "            Predicted ratings.\n",
        "\n",
        "        \"\"\"\n",
        "        ues = self.user_embeddings(users)\n",
        "        uis = self.item_embeddings(items)\n",
        "\n",
        "        preds = self.user_biases(users)\n",
        "        preds += self.item_biases(items)\n",
        "        preds += (self.dropout(ues) * self.dropout(uis)).sum(dim=1, keepdim=True)\n",
        "\n",
        "        return preds.squeeze()\n",
        "    \n",
        "    def __call__(self, *args):\n",
        "        return self.forward(*args)\n",
        "\n",
        "    def predict(self, users, items):\n",
        "        return self.forward(users, items)\n",
        "\n",
        "\n",
        "def bpr_loss(preds, vals):\n",
        "    sig = nn.Sigmoid()\n",
        "    return (1.0 - sig(preds)).pow(2).sum()\n",
        "\n",
        "\n",
        "class BPRModule(nn.Module):\n",
        "    \n",
        "    def __init__(self,\n",
        "                 n_users,\n",
        "                 n_items,\n",
        "                 n_factors=40,\n",
        "                 dropout_p=0,\n",
        "                 sparse=False,\n",
        "                 model=BaseModule):\n",
        "        super(BPRModule, self).__init__()\n",
        "\n",
        "        self.n_users = n_users\n",
        "        self.n_items = n_items\n",
        "        self.n_factors = n_factors\n",
        "        self.dropout_p = dropout_p\n",
        "        self.sparse = sparse\n",
        "        self.pred_model = model(\n",
        "            self.n_users,\n",
        "            self.n_items,\n",
        "            n_factors=n_factors,\n",
        "            dropout_p=dropout_p,\n",
        "            sparse=sparse\n",
        "        )\n",
        "\n",
        "    def forward(self, users, items):\n",
        "        assert isinstance(items, tuple), \\\n",
        "            'Must pass in items as (pos_items, neg_items)'\n",
        "        # Unpack\n",
        "        (pos_items, neg_items) = items\n",
        "        pos_preds = self.pred_model(users, pos_items)\n",
        "        neg_preds = self.pred_model(users, neg_items)\n",
        "        return pos_preds - neg_preds\n",
        "\n",
        "    def predict(self, users, items):\n",
        "        return self.pred_model(users, items)\n",
        "\n",
        "\n",
        "class BasePipeline:\n",
        "    \"\"\"\n",
        "    Class defining a training pipeline. Instantiates data loaders, model,\n",
        "    and optimizer. Handles training for multiple epochs and keeping track of\n",
        "    train and test loss.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self,\n",
        "                 train,\n",
        "                 test=None,\n",
        "                 model=BaseModule,\n",
        "                 n_factors=40,\n",
        "                 batch_size=32,\n",
        "                 dropout_p=0.02,\n",
        "                 sparse=False,\n",
        "                 lr=0.01,\n",
        "                 weight_decay=0.,\n",
        "                 optimizer=torch.optim.Adam,\n",
        "                 loss_function=nn.MSELoss(reduction='sum'),\n",
        "                 n_epochs=10,\n",
        "                 verbose=False,\n",
        "                 random_seed=None,\n",
        "                 interaction_class=Interactions,\n",
        "                 hogwild=False,\n",
        "                 num_workers=0,\n",
        "                 eval_metrics=None,\n",
        "                 k=5):\n",
        "        self.train = train\n",
        "        self.test = test\n",
        "\n",
        "        if hogwild:\n",
        "            num_loader_workers = 0\n",
        "        else:\n",
        "            num_loader_workers = num_workers\n",
        "        self.train_loader = data.DataLoader(\n",
        "            interaction_class(train), batch_size=batch_size, shuffle=True,\n",
        "            num_workers=num_loader_workers)\n",
        "        if self.test is not None:\n",
        "            self.test_loader = data.DataLoader(\n",
        "                interaction_class(test), batch_size=batch_size, shuffle=True,\n",
        "                num_workers=num_loader_workers)\n",
        "        self.num_workers = num_workers\n",
        "        self.n_users = self.train.shape[0]\n",
        "        self.n_items = self.train.shape[1]\n",
        "        self.n_factors = n_factors\n",
        "        self.batch_size = batch_size\n",
        "        self.dropout_p = dropout_p\n",
        "        self.lr = lr\n",
        "        self.weight_decay = weight_decay\n",
        "        self.loss_function = loss_function\n",
        "        self.n_epochs = n_epochs\n",
        "        if sparse:\n",
        "            assert weight_decay == 0.0\n",
        "        self.model = model(self.n_users,\n",
        "                           self.n_items,\n",
        "                           n_factors=self.n_factors,\n",
        "                           dropout_p=self.dropout_p,\n",
        "                           sparse=sparse)\n",
        "        self.optimizer = optimizer(self.model.parameters(),\n",
        "                                   lr=self.lr,\n",
        "                                   weight_decay=self.weight_decay)\n",
        "        self.warm_start = False\n",
        "        self.losses = collections.defaultdict(list)\n",
        "        self.verbose = verbose\n",
        "        self.hogwild = hogwild\n",
        "        if random_seed is not None:\n",
        "            if self.hogwild:\n",
        "                random_seed += os.getpid()\n",
        "            torch.manual_seed(random_seed)\n",
        "            np.random.seed(random_seed)\n",
        "\n",
        "        if eval_metrics is None:\n",
        "            eval_metrics = []\n",
        "        self.eval_metrics = eval_metrics\n",
        "        self.k = k\n",
        "\n",
        "    def break_grads(self):\n",
        "        for param in self.model.parameters():\n",
        "            # Break gradient sharing\n",
        "            if param.grad is not None:\n",
        "                param.grad.data = param.grad.data.clone()\n",
        "\n",
        "    def fit(self):\n",
        "        for epoch in range(1, self.n_epochs + 1):\n",
        "\n",
        "            if self.hogwild:\n",
        "                self.model.share_memory()\n",
        "                processes = []\n",
        "                train_losses = []\n",
        "                queue = mp.Queue()\n",
        "                for rank in range(self.num_workers):\n",
        "                    p = mp.Process(target=self._fit_epoch,\n",
        "                                   kwargs={'epoch': epoch,\n",
        "                                           'queue': queue})\n",
        "                    p.start()\n",
        "                    processes.append(p)\n",
        "                for p in processes:\n",
        "                    p.join()\n",
        "\n",
        "                while True:\n",
        "                    is_alive = False\n",
        "                    for p in processes:\n",
        "                        if p.is_alive():\n",
        "                            is_alive = True\n",
        "                            break\n",
        "                    if not is_alive and queue.empty():\n",
        "                        break\n",
        "\n",
        "                    while not queue.empty():\n",
        "                        train_losses.append(queue.get())\n",
        "                queue.close()\n",
        "                train_loss = np.mean(train_losses)\n",
        "            else:\n",
        "                train_loss = self._fit_epoch(epoch)\n",
        "\n",
        "            self.losses['train'].append(train_loss)\n",
        "            row = 'Epoch: {0:^3}  train: {1:^10.5f}'.format(epoch, self.losses['train'][-1])\n",
        "            if self.test is not None:\n",
        "                self.losses['test'].append(self._validation_loss())\n",
        "                row += 'val: {0:^10.5f}'.format(self.losses['test'][-1])\n",
        "                for metric in self.eval_metrics:\n",
        "                    func = getattr(metrics, metric)\n",
        "                    res = func(self.model, self.test_loader.dataset.mat_csr,\n",
        "                               num_workers=self.num_workers)\n",
        "                    self.losses['eval-{}'.format(metric)].append(res)\n",
        "                    row += 'eval-{0}: {1:^10.5f}'.format(metric, res)\n",
        "            self.losses['epoch'].append(epoch)\n",
        "            if self.verbose:\n",
        "                print(row)\n",
        "\n",
        "    def _fit_epoch(self, epoch=1, queue=None):\n",
        "        if self.hogwild:\n",
        "            self.break_grads()\n",
        "\n",
        "        self.model.train()\n",
        "        total_loss = torch.Tensor([0])\n",
        "        pbar = tqdm(enumerate(self.train_loader),\n",
        "                    total=len(self.train_loader),\n",
        "                    desc='({0:^3})'.format(epoch))\n",
        "        for batch_idx, ((row, col), val) in pbar:\n",
        "            self.optimizer.zero_grad()\n",
        "\n",
        "            row = row.long()\n",
        "            # TODO: turn this into a collate_fn like the data_loader\n",
        "            if isinstance(col, list):\n",
        "                col = tuple(c.long() for c in col)\n",
        "            else:\n",
        "                col = col.long()\n",
        "            val = val.float()\n",
        "\n",
        "            preds = self.model(row, col)\n",
        "            loss = self.loss_function(preds, val)\n",
        "            loss.backward()\n",
        "\n",
        "            self.optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            batch_loss = loss.item() / row.size()[0]\n",
        "            pbar.set_postfix(train_loss=batch_loss)\n",
        "        total_loss /= self.train.nnz\n",
        "        if queue is not None:\n",
        "            queue.put(total_loss[0])\n",
        "        else:\n",
        "            return total_loss[0]\n",
        "\n",
        "    def _validation_loss(self):\n",
        "        self.model.eval()\n",
        "        total_loss = torch.Tensor([0])\n",
        "        for batch_idx, ((row, col), val) in enumerate(self.test_loader):\n",
        "            row = row.long()\n",
        "            if isinstance(col, list):\n",
        "                col = tuple(c.long() for c in col)\n",
        "            else:\n",
        "                col = col.long()\n",
        "            val = val.float()\n",
        "\n",
        "            preds = self.model(row, col)\n",
        "            loss = self.loss_function(preds, val)\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        total_loss /= self.test.nnz\n",
        "        return total_loss[0]"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Writing torchmf.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5KpaDgMwLNI5"
      },
      "source": [
        "## run.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HxKI9a2dLDDy",
        "outputId": "d7b903c2-6adb-4afd-dcd3-f1e5d49af5f6"
      },
      "source": [
        "%%writefile run.py\n",
        "\n",
        "import argparse\n",
        "import pickle\n",
        "\n",
        "import torch\n",
        "\n",
        "from torchmf import (BaseModule, BPRModule, BasePipeline,\n",
        "                     bpr_loss, PairwiseInteractions)\n",
        "import utils\n",
        "\n",
        "\n",
        "def explicit():\n",
        "    train, test = utils.get_movielens_train_test_split()\n",
        "    pipeline = BasePipeline(train, test=test, model=BaseModule,\n",
        "                            n_factors=10, batch_size=1024, dropout_p=0.02,\n",
        "                            lr=0.02, weight_decay=0.1,\n",
        "                            optimizer=torch.optim.Adam, n_epochs=40,\n",
        "                            verbose=True, random_seed=2017)\n",
        "    pipeline.fit()\n",
        "\n",
        "\n",
        "def implicit():\n",
        "    train, test = utils.get_movielens_train_test_split(implicit=True)\n",
        "\n",
        "    pipeline = BasePipeline(train, test=test, verbose=True,\n",
        "                           batch_size=1024, num_workers=4,\n",
        "                           n_factors=20, weight_decay=0,\n",
        "                           dropout_p=0., lr=.2, sparse=True,\n",
        "                           optimizer=torch.optim.SGD, n_epochs=40,\n",
        "                           random_seed=2017, loss_function=bpr_loss,\n",
        "                           model=BPRModule,\n",
        "                           interaction_class=PairwiseInteractions,\n",
        "                           eval_metrics=('auc', 'patk'))\n",
        "    pipeline.fit()\n",
        "\n",
        "\n",
        "def hogwild():\n",
        "    train, test = utils.get_movielens_train_test_split(implicit=True)\n",
        "\n",
        "    pipeline = BasePipeline(train, test=test, verbose=True,\n",
        "                            batch_size=1024, num_workers=4,\n",
        "                            n_factors=20, weight_decay=0,\n",
        "                            dropout_p=0., lr=.2, sparse=True,\n",
        "                            optimizer=torch.optim.SGD, n_epochs=40,\n",
        "                            random_seed=2017, loss_function=bpr_loss,\n",
        "                            model=BPRModule, hogwild=True,\n",
        "                            interaction_class=PairwiseInteractions,\n",
        "                            eval_metrics=('auc', 'patk'))\n",
        "    pipeline.fit()\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    parser = argparse.ArgumentParser(description='torchmf')\n",
        "    parser.add_argument('--example',\n",
        "                        help='explicit, implicit, or hogwild')\n",
        "    args = parser.parse_args()\n",
        "    if args.example == 'explicit':\n",
        "        explicit()\n",
        "    elif args.example == 'implicit':\n",
        "        implicit()\n",
        "    elif args.example == 'hogwild':\n",
        "        hogwild()\n",
        "    else:\n",
        "        print('example must be explicit, implicit, or hogwild')"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Writing run.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "40lNybzWLtRP"
      },
      "source": [
        "## explicit run"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0i4BoW9HLHSb",
        "outputId": "e648ac4f-928a-4d50-ab8a-3d3015ba82a3"
      },
      "source": [
        "!python run.py --example explicit"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading MovieLens data\n",
            "( 1 ): 100% 89/89 [00:01<00:00, 71.00it/s, train_loss=8.19]\n",
            "Epoch:  1   train:  14.64144 val:  8.70737  \n",
            "( 2 ): 100% 89/89 [00:00<00:00, 97.51it/s, train_loss=3.03]\n",
            "Epoch:  2   train:  4.25602  val:  4.06002  \n",
            "( 3 ): 100% 89/89 [00:00<00:00, 96.01it/s, train_loss=1.47]\n",
            "Epoch:  3   train:  1.90983  val:  2.49680  \n",
            "( 4 ): 100% 89/89 [00:00<00:00, 96.26it/s, train_loss=1.11]\n",
            "Epoch:  4   train:  1.24095  val:  1.85090  \n",
            "( 5 ): 100% 89/89 [00:00<00:00, 94.35it/s, train_loss=0.972]\n",
            "Epoch:  5   train:  0.99838  val:  1.54036  \n",
            "( 6 ): 100% 89/89 [00:00<00:00, 93.74it/s, train_loss=0.936]\n",
            "Epoch:  6   train:  0.89663  val:  1.36695  \n",
            "( 7 ): 100% 89/89 [00:00<00:00, 96.31it/s, train_loss=0.906]\n",
            "Epoch:  7   train:  0.84003  val:  1.26457  \n",
            "( 8 ): 100% 89/89 [00:00<00:00, 95.79it/s, train_loss=0.864]\n",
            "Epoch:  8   train:  0.80580  val:  1.19734  \n",
            "( 9 ): 100% 89/89 [00:00<00:00, 97.95it/s, train_loss=0.809]\n",
            "Epoch:  9   train:  0.77795  val:  1.15100  \n",
            "(10 ): 100% 89/89 [00:00<00:00, 97.03it/s, train_loss=0.805]\n",
            "Epoch: 10   train:  0.75526  val:  1.11836  \n",
            "(11 ): 100% 89/89 [00:00<00:00, 94.96it/s, train_loss=0.762]\n",
            "Epoch: 11   train:  0.73437  val:  1.09048  \n",
            "(12 ): 100% 89/89 [00:00<00:00, 97.37it/s, train_loss=0.717]\n",
            "Epoch: 12   train:  0.71592  val:  1.07447  \n",
            "(13 ): 100% 89/89 [00:00<00:00, 96.82it/s, train_loss=0.721]\n",
            "Epoch: 13   train:  0.70062  val:  1.05621  \n",
            "(14 ): 100% 89/89 [00:00<00:00, 97.52it/s, train_loss=0.672]\n",
            "Epoch: 14   train:  0.68740  val:  1.04526  \n",
            "(15 ): 100% 89/89 [00:00<00:00, 97.48it/s, train_loss=0.725]\n",
            "Epoch: 15   train:  0.67550  val:  1.03545  \n",
            "(16 ): 100% 89/89 [00:00<00:00, 103.46it/s, train_loss=0.676]\n",
            "Epoch: 16   train:  0.66732  val:  1.02976  \n",
            "(17 ): 100% 89/89 [00:00<00:00, 99.98it/s, train_loss=0.634]\n",
            "Epoch: 17   train:  0.65880  val:  1.02630  \n",
            "(18 ): 100% 89/89 [00:00<00:00, 102.10it/s, train_loss=0.684]\n",
            "Epoch: 18   train:  0.65355  val:  1.02109  \n",
            "(19 ): 100% 89/89 [00:00<00:00, 101.78it/s, train_loss=0.678]\n",
            "Epoch: 19   train:  0.64726  val:  1.01894  \n",
            "(20 ): 100% 89/89 [00:00<00:00, 100.12it/s, train_loss=0.646]\n",
            "Epoch: 20   train:  0.64413  val:  1.01405  \n",
            "(21 ): 100% 89/89 [00:00<00:00, 97.41it/s, train_loss=0.724]\n",
            "Epoch: 21   train:  0.64022  val:  1.01139  \n",
            "(22 ): 100% 89/89 [00:00<00:00, 102.92it/s, train_loss=0.653]\n",
            "Epoch: 22   train:  0.63731  val:  1.00706  \n",
            "(23 ): 100% 89/89 [00:00<00:00, 96.05it/s, train_loss=0.588] \n",
            "Epoch: 23   train:  0.63537  val:  1.00648  \n",
            "(24 ): 100% 89/89 [00:00<00:00, 96.94it/s, train_loss=0.677] \n",
            "Epoch: 24   train:  0.63058  val:  1.00690  \n",
            "(25 ): 100% 89/89 [00:00<00:00, 90.87it/s, train_loss=0.712]\n",
            "Epoch: 25   train:  0.63242  val:  1.01011  \n",
            "(26 ): 100% 89/89 [00:00<00:00, 95.66it/s, train_loss=0.666]\n",
            "Epoch: 26   train:  0.62927  val:  1.01176  \n",
            "(27 ): 100% 89/89 [00:00<00:00, 94.58it/s, train_loss=0.594]\n",
            "Epoch: 27   train:  0.63036  val:  1.00783  \n",
            "(28 ): 100% 89/89 [00:00<00:00, 92.47it/s, train_loss=0.703]\n",
            "Epoch: 28   train:  0.62958  val:  1.00737  \n",
            "(29 ): 100% 89/89 [00:00<00:00, 92.86it/s, train_loss=0.669]\n",
            "Epoch: 29   train:  0.62973  val:  1.00857  \n",
            "(30 ): 100% 89/89 [00:00<00:00, 95.10it/s, train_loss=0.723]\n",
            "Epoch: 30   train:  0.62967  val:  1.01080  \n",
            "(31 ): 100% 89/89 [00:00<00:00, 95.64it/s, train_loss=0.726]\n",
            "Epoch: 31   train:  0.62821  val:  1.00937  \n",
            "(32 ): 100% 89/89 [00:00<00:00, 94.79it/s, train_loss=0.674]\n",
            "Epoch: 32   train:  0.63007  val:  1.00693  \n",
            "(33 ): 100% 89/89 [00:00<00:00, 96.29it/s, train_loss=0.65]\n",
            "Epoch: 33   train:  0.62933  val:  1.00872  \n",
            "(34 ): 100% 89/89 [00:00<00:00, 95.44it/s, train_loss=0.669]\n",
            "Epoch: 34   train:  0.62964  val:  1.00822  \n",
            "(35 ): 100% 89/89 [00:00<00:00, 92.46it/s, train_loss=0.711]\n",
            "Epoch: 35   train:  0.62926  val:  1.01489  \n",
            "(36 ): 100% 89/89 [00:00<00:00, 93.36it/s, train_loss=0.689]\n",
            "Epoch: 36   train:  0.63266  val:  1.01679  \n",
            "(37 ): 100% 89/89 [00:00<00:00, 95.29it/s, train_loss=0.695]\n",
            "Epoch: 37   train:  0.62937  val:  1.00905  \n",
            "(38 ): 100% 89/89 [00:00<00:00, 94.00it/s, train_loss=0.653]\n",
            "Epoch: 38   train:  0.63059  val:  1.01712  \n",
            "(39 ): 100% 89/89 [00:00<00:00, 95.39it/s, train_loss=0.71]\n",
            "Epoch: 39   train:  0.63048  val:  1.01576  \n",
            "(40 ): 100% 89/89 [00:00<00:00, 96.14it/s, train_loss=0.761]\n",
            "Epoch: 40   train:  0.63232  val:  1.01292  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JxqWyDE4LxPb"
      },
      "source": [
        "## implicit"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IBXdHOUXLdPE",
        "outputId": "62bd667d-56d6-4969-e390-8cab4842353e"
      },
      "source": [
        "!python run.py --example implicit"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "( 1 ): 100% 46/46 [00:01<00:00, 28.55it/s, train_loss=0.382]\n",
            "Epoch:  1   train:  0.41578  val:  0.39289  eval-auc:  0.55840  eval-patk:  0.00913  \n",
            "( 2 ): 100% 46/46 [00:01<00:00, 28.86it/s, train_loss=0.323]\n",
            "Epoch:  2   train:  0.34652  val:  0.34228  eval-auc:  0.61282  eval-patk:  0.01507  \n",
            "( 3 ): 100% 46/46 [00:01<00:00, 30.01it/s, train_loss=0.273]\n",
            "Epoch:  3   train:  0.27728  val:  0.31357  eval-auc:  0.65768  eval-patk:  0.02215  \n",
            "( 4 ): 100% 46/46 [00:01<00:00, 29.36it/s, train_loss=0.226]\n",
            "Epoch:  4   train:  0.23051  val:  0.29723  eval-auc:  0.69258  eval-patk:  0.02991  \n",
            "( 5 ): 100% 46/46 [00:01<00:00, 29.57it/s, train_loss=0.198]\n",
            "Epoch:  5   train:  0.20115  val:  0.28018  eval-auc:  0.71729  eval-patk:  0.03539  \n",
            "( 6 ): 100% 46/46 [00:01<00:00, 28.66it/s, train_loss=0.152]\n",
            "Epoch:  6   train:  0.17812  val:  0.26524  eval-auc:  0.73440  eval-patk:  0.03607  \n",
            "( 7 ): 100% 46/46 [00:01<00:00, 30.65it/s, train_loss=0.15]\n",
            "Epoch:  7   train:  0.16726  val:  0.25652  eval-auc:  0.74640  eval-patk:  0.03813  \n",
            "( 8 ): 100% 46/46 [00:01<00:00, 29.89it/s, train_loss=0.172]\n",
            "Epoch:  8   train:  0.15538  val:  0.24975  eval-auc:  0.75780  eval-patk:  0.03950  \n",
            "( 9 ): 100% 46/46 [00:01<00:00, 29.88it/s, train_loss=0.133]\n",
            "Epoch:  9   train:  0.14574  val:  0.24520  eval-auc:  0.76651  eval-patk:  0.04498  \n",
            "(10 ): 100% 46/46 [00:01<00:00, 30.02it/s, train_loss=0.14]\n",
            "Epoch: 10   train:  0.13953  val:  0.22739  eval-auc:  0.77529  eval-patk:  0.04749  \n",
            "(11 ): 100% 46/46 [00:01<00:00, 29.70it/s, train_loss=0.151]\n",
            "Epoch: 11   train:  0.13218  val:  0.22872  eval-auc:  0.78306  eval-patk:  0.04749  \n",
            "(12 ): 100% 46/46 [00:01<00:00, 29.81it/s, train_loss=0.13]\n",
            "Epoch: 12   train:  0.12857  val:  0.22756  eval-auc:  0.78880  eval-patk:  0.04840  \n",
            "(13 ): 100% 46/46 [00:01<00:00, 30.26it/s, train_loss=0.13]\n",
            "Epoch: 13   train:  0.12364  val:  0.21565  eval-auc:  0.79382  eval-patk:  0.05114  \n",
            "(14 ): 100% 46/46 [00:01<00:00, 30.80it/s, train_loss=0.0979]\n",
            "Epoch: 14   train:  0.11943  val:  0.21567  eval-auc:  0.79833  eval-patk:  0.05479  \n",
            "(15 ): 100% 46/46 [00:01<00:00, 30.27it/s, train_loss=0.109]\n",
            "Epoch: 15   train:  0.11619  val:  0.21074  eval-auc:  0.80249  eval-patk:  0.05548  \n",
            "(16 ): 100% 46/46 [00:01<00:00, 29.81it/s, train_loss=0.129]\n",
            "Epoch: 16   train:  0.11254  val:  0.21105  eval-auc:  0.80617  eval-patk:  0.05890  \n",
            "(17 ): 100% 46/46 [00:01<00:00, 30.27it/s, train_loss=0.111]\n",
            "Epoch: 17   train:  0.10796  val:  0.20284  eval-auc:  0.80958  eval-patk:  0.05890  \n",
            "(18 ): 100% 46/46 [00:01<00:00, 30.48it/s, train_loss=0.1]\n",
            "Epoch: 18   train:  0.10627  val:  0.19820  eval-auc:  0.81167  eval-patk:  0.06119  \n",
            "(19 ): 100% 46/46 [00:01<00:00, 29.63it/s, train_loss=0.132]\n",
            "Epoch: 19   train:  0.10392  val:  0.20573  eval-auc:  0.81511  eval-patk:  0.06370  \n",
            "(20 ): 100% 46/46 [00:01<00:00, 29.22it/s, train_loss=0.106]\n",
            "Epoch: 20   train:  0.10310  val:  0.20031  eval-auc:  0.81784  eval-patk:  0.06393  \n",
            "(21 ): 100% 46/46 [00:01<00:00, 29.44it/s, train_loss=0.084]\n",
            "Epoch: 21   train:  0.10323  val:  0.19672  eval-auc:  0.82062  eval-patk:  0.06530  \n",
            "(22 ): 100% 46/46 [00:01<00:00, 28.61it/s, train_loss=0.123]\n",
            "Epoch: 22   train:  0.10163  val:  0.19164  eval-auc:  0.82266  eval-patk:  0.06986  \n",
            "(23 ): 100% 46/46 [00:01<00:00, 29.98it/s, train_loss=0.109]\n",
            "Epoch: 23   train:  0.09932  val:  0.18622  eval-auc:  0.82489  eval-patk:  0.06849  \n",
            "(24 ): 100% 46/46 [00:01<00:00, 30.33it/s, train_loss=0.125]\n",
            "Epoch: 24   train:  0.09856  val:  0.18985  eval-auc:  0.82689  eval-patk:  0.06941  \n",
            "(25 ): 100% 46/46 [00:01<00:00, 30.46it/s, train_loss=0.0867]\n",
            "Epoch: 25   train:  0.09591  val:  0.18680  eval-auc:  0.82851  eval-patk:  0.07100  \n",
            "(26 ): 100% 46/46 [00:01<00:00, 29.23it/s, train_loss=0.0945]\n",
            "Epoch: 26   train:  0.09670  val:  0.18181  eval-auc:  0.83038  eval-patk:  0.07009  \n",
            "(27 ): 100% 46/46 [00:01<00:00, 29.79it/s, train_loss=0.0699]\n",
            "Epoch: 27   train:  0.09253  val:  0.18122  eval-auc:  0.83169  eval-patk:  0.06667  \n",
            "(28 ): 100% 46/46 [00:01<00:00, 30.00it/s, train_loss=0.0759]\n",
            "Epoch: 28   train:  0.09226  val:  0.18196  eval-auc:  0.83282  eval-patk:  0.06826  \n",
            "(29 ): 100% 46/46 [00:01<00:00, 29.22it/s, train_loss=0.0822]\n",
            "Epoch: 29   train:  0.09307  val:  0.18249  eval-auc:  0.83441  eval-patk:  0.07648  \n",
            "(30 ): 100% 46/46 [00:01<00:00, 30.18it/s, train_loss=0.114]\n",
            "Epoch: 30   train:  0.09162  val:  0.18411  eval-auc:  0.83504  eval-patk:  0.07648  \n",
            "(31 ): 100% 46/46 [00:01<00:00, 29.39it/s, train_loss=0.086]\n",
            "Epoch: 31   train:  0.08987  val:  0.17815  eval-auc:  0.83631  eval-patk:  0.07374  \n",
            "(32 ): 100% 46/46 [00:01<00:00, 29.27it/s, train_loss=0.0911]\n",
            "Epoch: 32   train:  0.08841  val:  0.18399  eval-auc:  0.83683  eval-patk:  0.07306  \n",
            "(33 ): 100% 46/46 [00:01<00:00, 29.72it/s, train_loss=0.0876]\n",
            "Epoch: 33   train:  0.09061  val:  0.17719  eval-auc:  0.83845  eval-patk:  0.07489  \n",
            "(34 ): 100% 46/46 [00:01<00:00, 29.93it/s, train_loss=0.0647]\n",
            "Epoch: 34   train:  0.08688  val:  0.18095  eval-auc:  0.83955  eval-patk:  0.06918  \n",
            "(35 ): 100% 46/46 [00:01<00:00, 30.05it/s, train_loss=0.0928]\n",
            "Epoch: 35   train:  0.08915  val:  0.17626  eval-auc:  0.84050  eval-patk:  0.07215  \n",
            "(36 ): 100% 46/46 [00:01<00:00, 29.89it/s, train_loss=0.111]\n",
            "Epoch: 36   train:  0.08683  val:  0.17530  eval-auc:  0.84146  eval-patk:  0.07420  \n",
            "(37 ): 100% 46/46 [00:01<00:00, 29.70it/s, train_loss=0.0915]\n",
            "Epoch: 37   train:  0.08663  val:  0.16717  eval-auc:  0.84286  eval-patk:  0.07215  \n",
            "(38 ): 100% 46/46 [00:01<00:00, 29.93it/s, train_loss=0.0765]\n",
            "Epoch: 38   train:  0.08452  val:  0.16749  eval-auc:  0.84417  eval-patk:  0.07763  \n",
            "(39 ): 100% 46/46 [00:01<00:00, 30.19it/s, train_loss=0.0737]\n",
            "Epoch: 39   train:  0.08514  val:  0.16763  eval-auc:  0.84427  eval-patk:  0.07443  \n",
            "(40 ): 100% 46/46 [00:01<00:00, 29.84it/s, train_loss=0.0934]\n",
            "Epoch: 40   train:  0.08454  val:  0.16994  eval-auc:  0.84553  eval-patk:  0.07283  \n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}