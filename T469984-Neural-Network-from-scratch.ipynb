{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"T469984 | Neural Network from scratch","provenance":[{"file_id":"1e7ntofI-u6uODEmSnkOXVyasq1igsoNG","timestamp":1630856714821}],"collapsed_sections":[],"authorship_tag":"ABX9TyN1OZp5rYxbsOBbeZ0ZUoS5"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"cellView":"form","id":"yW9y5ZQMJuoZ"},"source":["#@markdown Linear Algebra\n","\n","from typing import List\n","\n","Vector = List[float]\n","\n","height_weight_age = [70,  # inches,\n","                     170, # pounds,\n","                     40 ] # years\n","\n","grades = [95,   # exam1\n","          80,   # exam2\n","          75,   # exam3\n","          62 ]  # exam4\n","\n","def add(v: Vector, w: Vector) -> Vector:\n","    \"\"\"Adds corresponding elements\"\"\"\n","    assert len(v) == len(w), \"vectors must be the same length\"\n","\n","    return [v_i + w_i for v_i, w_i in zip(v, w)]\n","\n","assert add([1, 2, 3], [4, 5, 6]) == [5, 7, 9]\n","\n","def subtract(v: Vector, w: Vector) -> Vector:\n","    \"\"\"Subtracts corresponding elements\"\"\"\n","    assert len(v) == len(w), \"vectors must be the same length\"\n","\n","    return [v_i - w_i for v_i, w_i in zip(v, w)]\n","\n","assert subtract([5, 7, 9], [4, 5, 6]) == [1, 2, 3]\n","\n","def vector_sum(vectors: List[Vector]) -> Vector:\n","    \"\"\"Sums all corresponding elements\"\"\"\n","    # Check that vectors is not empty\n","    assert vectors, \"no vectors provided!\"\n","\n","    # Check the vectors are all the same size\n","    num_elements = len(vectors[0])\n","    assert all(len(v) == num_elements for v in vectors), \"different sizes!\"\n","\n","    # the i-th element of the result is the sum of every vector[i]\n","    return [sum(vector[i] for vector in vectors)\n","            for i in range(num_elements)]\n","\n","assert vector_sum([[1, 2], [3, 4], [5, 6], [7, 8]]) == [16, 20]\n","\n","def scalar_multiply(c: float, v: Vector) -> Vector:\n","    \"\"\"Multiplies every element by c\"\"\"\n","    return [c * v_i for v_i in v]\n","\n","assert scalar_multiply(2, [1, 2, 3]) == [2, 4, 6]\n","\n","def vector_mean(vectors: List[Vector]) -> Vector:\n","    \"\"\"Computes the element-wise average\"\"\"\n","    n = len(vectors)\n","    return scalar_multiply(1/n, vector_sum(vectors))\n","\n","assert vector_mean([[1, 2], [3, 4], [5, 6]]) == [3, 4]\n","\n","def dot(v: Vector, w: Vector) -> float:\n","    \"\"\"Computes v_1 * w_1 + ... + v_n * w_n\"\"\"\n","    assert len(v) == len(w), \"vectors must be same length\"\n","\n","    return sum(v_i * w_i for v_i, w_i in zip(v, w))\n","\n","assert dot([1, 2, 3], [4, 5, 6]) == 32  # 1 * 4 + 2 * 5 + 3 * 6\n","\n","def sum_of_squares(v: Vector) -> float:\n","    \"\"\"Returns v_1 * v_1 + ... + v_n * v_n\"\"\"\n","    return dot(v, v)\n","\n","assert sum_of_squares([1, 2, 3]) == 14  # 1 * 1 + 2 * 2 + 3 * 3\n","\n","import math\n","\n","def magnitude(v: Vector) -> float:\n","    \"\"\"Returns the magnitude (or length) of v\"\"\"\n","    return math.sqrt(sum_of_squares(v))   # math.sqrt is square root function\n","\n","assert magnitude([3, 4]) == 5\n","\n","def squared_distance(v: Vector, w: Vector) -> float:\n","    \"\"\"Computes (v_1 - w_1) ** 2 + ... + (v_n - w_n) ** 2\"\"\"\n","    return sum_of_squares(subtract(v, w))\n","\n","def distance(v: Vector, w: Vector) -> float:\n","    \"\"\"Computes the distance between v and w\"\"\"\n","    return math.sqrt(squared_distance(v, w))\n","\n","\n","def distance(v: Vector, w: Vector) -> float:  # type: ignore\n","    return magnitude(subtract(v, w))\n","\n","# Another type alias\n","Matrix = List[List[float]]\n","\n","A = [[1, 2, 3],  # A has 2 rows and 3 columns\n","     [4, 5, 6]]\n","\n","B = [[1, 2],     # B has 3 rows and 2 columns\n","     [3, 4],\n","     [5, 6]]\n","\n","from typing import Tuple\n","\n","def shape(A: Matrix) -> Tuple[int, int]:\n","    \"\"\"Returns (# of rows of A, # of columns of A)\"\"\"\n","    num_rows = len(A)\n","    num_cols = len(A[0]) if A else 0   # number of elements in first row\n","    return num_rows, num_cols\n","\n","assert shape([[1, 2, 3], [4, 5, 6]]) == (2, 3)  # 2 rows, 3 columns\n","\n","def get_row(A: Matrix, i: int) -> Vector:\n","    \"\"\"Returns the i-th row of A (as a Vector)\"\"\"\n","    return A[i]             # A[i] is already the ith row\n","\n","def get_column(A: Matrix, j: int) -> Vector:\n","    \"\"\"Returns the j-th column of A (as a Vector)\"\"\"\n","    return [A_i[j]          # jth element of row A_i\n","            for A_i in A]   # for each row A_i\n","\n","from typing import Callable\n","\n","def make_matrix(num_rows: int,\n","                num_cols: int,\n","                entry_fn: Callable[[int, int], float]) -> Matrix:\n","    \"\"\"\n","    Returns a num_rows x num_cols matrix\n","    whose (i,j)-th entry is entry_fn(i, j)\n","    \"\"\"\n","    return [[entry_fn(i, j)             # given i, create a list\n","             for j in range(num_cols)]  #   [entry_fn(i, 0), ... ]\n","            for i in range(num_rows)]   # create one list for each i\n","\n","def identity_matrix(n: int) -> Matrix:\n","    \"\"\"Returns the n x n identity matrix\"\"\"\n","    return make_matrix(n, n, lambda i, j: 1 if i == j else 0)\n","\n","assert identity_matrix(5) == [[1, 0, 0, 0, 0],\n","                              [0, 1, 0, 0, 0],\n","                              [0, 0, 1, 0, 0],\n","                              [0, 0, 0, 1, 0],\n","                              [0, 0, 0, 0, 1]]\n","\n","data = [[70, 170, 40],\n","        [65, 120, 26],\n","        [77, 250, 19],\n","        # ....\n","       ]\n","\n","friendships = [(0, 1), (0, 2), (1, 2), (1, 3), (2, 3), (3, 4),\n","               (4, 5), (5, 6), (5, 7), (6, 8), (7, 8), (8, 9)]\n","\n","#            user 0  1  2  3  4  5  6  7  8  9\n","#\n","friend_matrix = [[0, 1, 1, 0, 0, 0, 0, 0, 0, 0],  # user 0\n","                 [1, 0, 1, 1, 0, 0, 0, 0, 0, 0],  # user 1\n","                 [1, 1, 0, 1, 0, 0, 0, 0, 0, 0],  # user 2\n","                 [0, 1, 1, 0, 1, 0, 0, 0, 0, 0],  # user 3\n","                 [0, 0, 0, 1, 0, 1, 0, 0, 0, 0],  # user 4\n","                 [0, 0, 0, 0, 1, 0, 1, 1, 0, 0],  # user 5\n","                 [0, 0, 0, 0, 0, 1, 0, 0, 1, 0],  # user 6\n","                 [0, 0, 0, 0, 0, 1, 0, 0, 1, 0],  # user 7\n","                 [0, 0, 0, 0, 0, 0, 1, 1, 0, 1],  # user 8\n","                 [0, 0, 0, 0, 0, 0, 0, 0, 1, 0]]  # user 9\n","\n","assert friend_matrix[0][2] == 1, \"0 and 2 are friends\"\n","assert friend_matrix[0][8] == 0, \"0 and 8 are not friends\"\n","\n","# only need to look at one row\n","friends_of_five = [i\n","                   for i, is_friend in enumerate(friend_matrix[5])\n","                   if is_friend]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"L9BD9IJfpM4h"},"source":["## Perceptron"]},{"cell_type":"code","metadata":{"id":"KtMEfc8upJI1"},"source":["def step_function(x: float) -> float:\n","    return 1.0 if x >= 0 else 0.0\n","\n","def perceptron_output(weights: Vector, bias: float, x: Vector) -> float:\n","    \"\"\"Returns 1 if the perceptron 'fires', 0 if not\"\"\"\n","    calculation = dot(weights, x) + bias\n","    return step_function(calculation)\n","\n","and_weights = [2., 2]\n","and_bias = -3.\n","\n","assert perceptron_output(and_weights, and_bias, [1, 1]) == 1\n","assert perceptron_output(and_weights, and_bias, [0, 1]) == 0\n","assert perceptron_output(and_weights, and_bias, [1, 0]) == 0\n","assert perceptron_output(and_weights, and_bias, [0, 0]) == 0\n","\n","or_weights = [2., 2]\n","or_bias = -1.\n","\n","assert perceptron_output(or_weights, or_bias, [1, 1]) == 1\n","assert perceptron_output(or_weights, or_bias, [0, 1]) == 1\n","assert perceptron_output(or_weights, or_bias, [1, 0]) == 1\n","assert perceptron_output(or_weights, or_bias, [0, 0]) == 0\n","\n","not_weights = [-2.]\n","not_bias = 1.\n","\n","assert perceptron_output(not_weights, not_bias, [0]) == 1\n","assert perceptron_output(not_weights, not_bias, [1]) == 0"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rXMGoQMupYJc"},"source":["## Neuron calculation"]},{"cell_type":"code","metadata":{"id":"ujMdFm82pXi0"},"source":["import math\n","\n","def sigmoid(t: float) -> float:\n","    return 1 / (1 + math.exp(-t))\n","\n","def neuron_output(weights: Vector, inputs: Vector) -> float:\n","    # weights includes the bias term, inputs includes a 1\n","    return sigmoid(dot(weights, inputs))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"H3CMQOc7qAX-"},"source":["## Feed-forward"]},{"cell_type":"code","metadata":{"id":"1AXlmjUVp_hP"},"source":["from typing import List\n","\n","def feed_forward(neural_network: List[List[Vector]],\n","                 input_vector: Vector) -> List[Vector]:\n","    \"\"\"\n","    Feeds the input vector through the neural network.\n","    Returns the outputs of all layers (not just the last one).\n","    \"\"\"\n","    outputs: List[Vector] = []\n","\n","    for layer in neural_network:\n","        input_with_bias = input_vector + [1]              # Add a constant.\n","        output = [neuron_output(neuron, input_with_bias)  # Compute the output\n","                  for neuron in layer]                    # for each neuron.\n","        outputs.append(output)                            # Add to results.\n","\n","        # Then the input to the next layer is the output of this one\n","        input_vector = output\n","\n","    return outputs\n","\n","xor_network = [# hidden layer\n","               [[20., 20, -30],      # 'and' neuron\n","                [20., 20, -10]],     # 'or'  neuron\n","               # output layer\n","               [[-60., 60, -30]]]    # '2nd input but not 1st input' neuron\n","\n","# feed_forward returns the outputs of all layers, so the [-1] gets the\n","# final output, and the [0] gets the value out of the resulting vector\n","assert 0.000 < feed_forward(xor_network, [0, 0])[-1][0] < 0.001\n","assert 0.999 < feed_forward(xor_network, [1, 0])[-1][0] < 1.000\n","assert 0.999 < feed_forward(xor_network, [0, 1])[-1][0] < 1.000\n","assert 0.000 < feed_forward(xor_network, [1, 1])[-1][0] < 0.001"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"JzR7jUQeqWrr"},"source":["### Back-propagation gradient calculation"]},{"cell_type":"code","metadata":{"id":"_5GM5t4bqV1C"},"source":["def sqerror_gradients(network: List[List[Vector]],\n","                      input_vector: Vector,\n","                      target_vector: Vector) -> List[List[Vector]]:\n","    \"\"\"\n","    Given a neural network, an input vector, and a target vector,\n","    make a prediction and compute the gradient of the squared error\n","    loss with respect to the neuron weights.\n","    \"\"\"\n","    # forward pass\n","    hidden_outputs, outputs = feed_forward(network, input_vector)\n","\n","    # gradients with respect to output neuron pre-activation outputs\n","    output_deltas = [output * (1 - output) * (output - target)\n","                     for output, target in zip(outputs, target_vector)]\n","\n","    # gradients with respect to output neuron weights\n","    output_grads = [[output_deltas[i] * hidden_output\n","                     for hidden_output in hidden_outputs + [1]]\n","                    for i, output_neuron in enumerate(network[-1])]\n","\n","    # gradients with respect to hidden neuron pre-activation outputs\n","    hidden_deltas = [hidden_output * (1 - hidden_output) *\n","                         dot(output_deltas, [n[i] for n in network[-1]])\n","                     for i, hidden_output in enumerate(hidden_outputs)]\n","\n","    # gradients with respect to hidden neuron weights\n","    hidden_grads = [[hidden_deltas[i] * input for input in input_vector + [1]]\n","                    for i, hidden_neuron in enumerate(network[0])]\n","\n","    return [hidden_grads, output_grads]\n","\n","[   # hidden layer\n","    [[7, 7, -3],     # computes OR\n","     [5, 5, -8]],    # computes AND\n","    # output layer\n","    [[11, -12, -5]]  # computes \"first but not second\"\n","]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GwwwAmDzrGBh"},"source":["## Training XOR neural network from scratch"]},{"cell_type":"code","metadata":{"id":"ZmQpp95Tr-Xi"},"source":["import tqdm\n","import random\n","\n","random.seed(0)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"WbDXdOOVr_yO"},"source":["def gradient_step(v: Vector, gradient: Vector, step_size: float) -> Vector:\n","    \"\"\"Moves `step_size` in the `gradient` direction from `v`\"\"\"\n","    assert len(v) == len(gradient)\n","    step = scalar_multiply(step_size, gradient)\n","    return add(v, step)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"yy2lrAYKrobt"},"source":["# training data\n","xs = [[0., 0], [0., 1], [1., 0], [1., 1]]\n","ys = [[0.], [1.], [1.], [0.]]\n","\n","# start with random weights\n","network = [ # hidden layer: 2 inputs -> 2 outputs\n","            [[random.random() for _ in range(2 + 1)],   # 1st hidden neuron\n","                [random.random() for _ in range(2 + 1)]],  # 2nd hidden neuron\n","            # output layer: 2 inputs -> 1 output\n","            [[random.random() for _ in range(2 + 1)]]   # 1st output neuron\n","            ]\n","\n","\n","learning_rate = 1.0\n","\n","for epoch in tqdm.trange(20000, desc=\"neural net for xor\"):\n","    for x, y in zip(xs, ys):\n","        gradients = sqerror_gradients(network, x, y)\n","\n","        # Take a gradient step for each neuron in each layer\n","        network = [[gradient_step(neuron, grad, -learning_rate)\n","                    for neuron, grad in zip(layer, layer_grad)]\n","                    for layer, layer_grad in zip(network, gradients)]\n","\n","# check that it learned XOR\n","assert feed_forward(network, [0, 0])[-1][0] < 0.01\n","assert feed_forward(network, [0, 1])[-1][0] > 0.99\n","assert feed_forward(network, [1, 0])[-1][0] > 0.99\n","assert feed_forward(network, [1, 1])[-1][0] < 0.01"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"n4MIqS4WsQsr"},"source":["## Fizbuzz neural network\n","Binary encoding to Fizbuzz encoding mapping function approximation by training a neural network from scratch"]},{"cell_type":"code","metadata":{"id":"qm0OtMCiqi-S"},"source":["def binary_encode(x: int) -> Vector:\n","    binary: List[float] = []\n","\n","    for i in range(10):\n","        binary.append(x % 2)\n","        x = x // 2\n","\n","    return binary\n","\n","#                             1  2  4  8 16 32 64 128 256 512\n","assert binary_encode(0)   == [0, 0, 0, 0, 0, 0, 0, 0,  0,  0]\n","assert binary_encode(1)   == [1, 0, 0, 0, 0, 0, 0, 0,  0,  0]\n","assert binary_encode(10)  == [0, 1, 0, 1, 0, 0, 0, 0,  0,  0]\n","assert binary_encode(101) == [1, 0, 1, 0, 0, 1, 1, 0,  0,  0]\n","assert binary_encode(999) == [1, 1, 1, 0, 0, 1, 1, 1,  1,  1]\n","\n","\n","def argmax(xs: list) -> int:\n","    \"\"\"Returns the index of the largest value\"\"\"\n","    return max(range(len(xs)), key=lambda i: xs[i])\n","\n","assert argmax([0, -1]) == 0               # items[0] is largest\n","assert argmax([-1, 0]) == 1               # items[1] is largest\n","assert argmax([-1, 10, 5, 20, -3]) == 3   # items[3] is largest\n","\n","\n","def fizz_buzz_encode(x: int) -> Vector:\n","    if x % 15 == 0:\n","        return [0, 0, 0, 1]\n","    elif x % 5 == 0:\n","        return [0, 0, 1, 0]\n","    elif x % 3 == 0:\n","        return [0, 1, 0, 0]\n","    else:\n","        return [1, 0, 0, 0]\n","\n","assert fizz_buzz_encode(2) == [1, 0, 0, 0]\n","assert fizz_buzz_encode(6) == [0, 1, 0, 0]\n","assert fizz_buzz_encode(10) == [0, 0, 1, 0]\n","assert fizz_buzz_encode(30) == [0, 0, 0, 1]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ydjAgVMAo0Iz","executionInfo":{"status":"ok","timestamp":1630857545543,"user_tz":-330,"elapsed":180766,"user":{"displayName":"Sparsh Agarwal","photoUrl":"","userId":"13037694610922482904"}},"outputId":"5567c510-c1d2-4219-ec09-e990173d7c9f"},"source":["xs = [binary_encode(n) for n in range(101, 1024)]\n","ys = [fizz_buzz_encode(n) for n in range(101, 1024)]\n","\n","NUM_HIDDEN = 25\n","\n","network = [\n","    # hidden layer: 10 inputs -> NUM_HIDDEN outputs\n","    [[random.random() for _ in range(10 + 1)] for _ in range(NUM_HIDDEN)],\n","\n","    # output_layer: NUM_HIDDEN inputs -> 4 outputs\n","    [[random.random() for _ in range(NUM_HIDDEN + 1)] for _ in range(4)]\n","]\n","\n","learning_rate = 1.0\n","\n","with tqdm.trange(500) as t:\n","    for epoch in t:\n","        epoch_loss = 0.0\n","\n","        for x, y in zip(xs, ys):\n","            predicted = feed_forward(network, x)[-1]\n","            epoch_loss += squared_distance(predicted, y)\n","            gradients = sqerror_gradients(network, x, y)\n","\n","            # Take a gradient step for each neuron in each layer\n","            network = [[gradient_step(neuron, grad, -learning_rate)\n","                        for neuron, grad in zip(layer, layer_grad)]\n","                    for layer, layer_grad in zip(network, gradients)]\n","\n","        t.set_description(f\"fizz buzz (loss: {epoch_loss:.2f})\")\n","\n","num_correct = 0\n","\n","for n in range(1, 101):\n","    x = binary_encode(n)\n","    predicted = argmax(feed_forward(network, x)[-1])\n","    actual = argmax(fizz_buzz_encode(n))\n","    labels = [str(n), \"fizz\", \"buzz\", \"fizzbuzz\"]\n","    print(n, labels[predicted], labels[actual])\n","\n","    if predicted == actual:\n","        num_correct += 1\n","\n","print(num_correct, \"/\", 100)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["neural net for xor: 100%|██████████| 20000/20000 [00:01<00:00, 10978.04it/s]\n","fizz buzz (loss: 29.68): 100%|██████████| 500/500 [02:57<00:00,  2.81it/s]"]},{"output_type":"stream","name":"stdout","text":["1 1 1\n","2 2 2\n","3 fizz fizz\n","4 4 4\n","5 buzz buzz\n","6 fizz fizz\n","7 7 7\n","8 8 8\n","9 fizz fizz\n","10 buzz buzz\n","11 11 11\n","12 fizz fizz\n","13 13 13\n","14 14 14\n","15 fizzbuzz fizzbuzz\n","16 16 16\n","17 17 17\n","18 fizz fizz\n","19 19 19\n","20 20 buzz\n","21 fizz fizz\n","22 22 22\n","23 23 23\n","24 fizz fizz\n","25 buzz buzz\n","26 26 26\n","27 fizz fizz\n","28 28 28\n","29 29 29\n","30 fizzbuzz fizzbuzz\n","31 31 31\n","32 32 32\n","33 fizz fizz\n","34 34 34\n","35 buzz buzz\n","36 fizz fizz\n","37 37 37\n","38 38 38\n","39 fizz fizz\n","40 buzz buzz\n","41 41 41\n","42 fizz fizz\n","43 43 43\n","44 44 44\n","45 fizzbuzz fizzbuzz\n","46 46 46\n","47 47 47\n","48 fizz fizz\n","49 49 49\n","50 buzz buzz\n","51 fizz fizz\n","52 52 52\n","53 53 53\n","54 fizz fizz\n","55 buzz buzz\n","56 56 56\n","57 fizz fizz\n","58 58 58\n","59 59 59\n","60 fizzbuzz fizzbuzz\n","61 61 61\n","62 62 62\n","63 fizz fizz\n","64 64 64\n","65 buzz buzz\n","66 fizz fizz\n","67 67 67\n","68 68 68\n","69 fizz fizz\n","70 buzz buzz\n","71 71 71\n","72 fizz fizz\n","73 73 73\n","74 74 74\n","75 fizzbuzz fizzbuzz\n","76 76 76\n","77 77 77\n","78 fizz fizz\n","79 79 79\n","80 80 buzz\n","81 fizz fizz\n","82 82 82\n","83 83 83\n","84 fizz fizz\n","85 fizz buzz\n","86 86 86\n","87 fizz fizz\n","88 88 88\n","89 89 89\n","90 fizzbuzz fizzbuzz\n","91 91 91\n","92 92 92\n","93 fizz fizz\n","94 94 94\n","95 fizz buzz\n","96 fizz fizz\n","97 97 97\n","98 98 98\n","99 fizz fizz\n","100 buzz buzz\n","96 / 100\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}]},{"cell_type":"code","metadata":{"id":"MVbmHBe2rIGp"},"source":[""],"execution_count":null,"outputs":[]}]}