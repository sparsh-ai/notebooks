{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp utils.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text utils\n",
    "> Implementation of text utilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev.showdoc import *\n",
    "from fastcore.nb_imports import *\n",
    "from fastcore.test import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "nltk.download('stopwords', quiet=True)\n",
    "stopword = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## text_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def text_clean(txt):\n",
    "  txt = re.sub('[^A-Za-z0-9]+', ' ', txt)\n",
    "  txt = txt.lower()\n",
    "  pattern = re.compile(r'\\b(' + r'|'.join(stopword) + r')\\b\\s*')\n",
    "  txt = pattern.sub(' ', txt)\n",
    "  txt = ' '.join(txt.split())\n",
    "  return txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'guns n roses popular card tee shirt pretty lady xxl red'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_clean(\"Guns N' Roses Popular Card Tee Shirt for Pretty Lady XXL Red\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk import pos_tag\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.corpus.reader.wordnet import NOUN, VERB, ADJ, ADV\n",
    "from nltk.corpus import brown, stopwords\n",
    "from nltk.cluster.util import cosine_distance\n",
    "from operator import itemgetter\n",
    "\n",
    "class TextCleaner():\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.stop_words = set(stopwords.words(\"english\"))\n",
    "        self.punctuations = set(string.punctuation)\n",
    "        self.pos_tags = {\n",
    "                NOUN: ['NN', 'NNS', 'NNP', 'NNPS', 'PRP', 'PRP$', 'WP', 'WP$'],\n",
    "                VERB: ['VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ'],\n",
    "                ADJ: ['JJ', 'JJR', 'JJS'],\n",
    "                ADV: ['RB', 'RBR', 'RBS', 'WRB']\n",
    "        }\n",
    "\n",
    "\n",
    "    def _remove_stop_words(self, words):\n",
    "        return [w for w in words if w not in self.stop_words]\n",
    "     \n",
    "    \n",
    "    def _remove_regex(self):\n",
    "        self.input_sent = \" \".join([w.lower() for w in self.input_sent])\n",
    "        self.input_sent = re.sub(r\"i'm\", \"i am\", self.input_sent)\n",
    "        self.input_sent = re.sub(r\"he's\", \"he is\", self.input_sent)\n",
    "        self.input_sent = re.sub(r\"she's\", \"she is\", self.input_sent)\n",
    "        self.input_sent = re.sub(r\"that's\", \"that is\", self.input_sent)\n",
    "        self.input_sent = re.sub(r\"what's\", \"what is\", self.input_sent)\n",
    "        self.input_sent = re.sub(r\"where's\", \"where is\", self.input_sent)\n",
    "        self.input_sent = re.sub(r\"\\'ll\", \" will\", self.input_sent)\n",
    "        self.input_sent = re.sub(r\"\\'ve\", \" have\", self.input_sent)\n",
    "        self.input_sent = re.sub(r\"\\'re\", \" are\", self.input_sent)\n",
    "        self.input_sent = re.sub(r\"\\'d\", \" would\", self.input_sent)\n",
    "        self.input_sent = re.sub(r\"won't\", \"will not\", self.input_sent)\n",
    "        self.input_sent = re.sub(r\"can't\", \"cannot\", self.input_sent)\n",
    "        self.input_sent = re.sub(r\"don't\", \"do not\", self.input_sent)\n",
    "        patterns = re.finditer(\"#[\\w]*\", self.input_sent)\n",
    "        for pattern in patterns:\n",
    "            self.input_sent = re.sub(pattern.group().strip(), \"\", self.input_sent)\n",
    "        self.input_sent = \"\".join(ch for ch in self.input_sent if ch not in self.punctuations)\n",
    "    \n",
    "    \n",
    "    def _tokenize(self):\n",
    "        return word_tokenize(self.input_sent)\n",
    "    \n",
    "    \n",
    "    def _process_content_for_pos(self, words):\n",
    "        tagged_words = pos_tag(words)\n",
    "        pos_words = []\n",
    "        for word in tagged_words:\n",
    "            flag = False\n",
    "            for key, value in self.pos_tags.items():\n",
    "                if word[1] in value:\n",
    "                    pos_words.append((word[0], key))\n",
    "                    flag = True\n",
    "                    break\n",
    "            if not flag:\n",
    "                pos_words.append((word[0], NOUN))\n",
    "        return pos_words\n",
    "       \n",
    "                 \n",
    "    def _remove_noise(self):\n",
    "        self._remove_regex()\n",
    "        words = self._tokenize()\n",
    "        noise_free_words = self._remove_stop_words(words)\n",
    "        return noise_free_words\n",
    "    \n",
    "    \n",
    "    def _normalize_text(self, words):\n",
    "        lem = WordNetLemmatizer()\n",
    "        pos_words = self._process_content_for_pos(words)\n",
    "        normalized_words = [lem.lemmatize(w, pos=p) for w, p in pos_words]\n",
    "        return normalized_words\n",
    "    \n",
    "    \n",
    "    def clean_up(self, input_sent):\n",
    "        self.input_sent = input_sent\n",
    "        cleaned_words = self._remove_noise()\n",
    "        cleaned_words = self._normalize_text(cleaned_words)\n",
    "        return cleaned_words"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
