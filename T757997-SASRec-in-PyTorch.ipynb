{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"T757997 | SASRec in PyTorch","provenance":[],"collapsed_sections":[],"toc_visible":true,"authorship_tag":"ABX9TyP2iX4DiFBZ+NSQ2/9TEc0X"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"ulksN0pl0xTh"},"source":["# SASRec in PyTorch"]},{"cell_type":"markdown","metadata":{"id":"SLja7CCaLzE0"},"source":["## Setup"]},{"cell_type":"markdown","metadata":{"id":"Iv0h05cHL0GP"},"source":["### Imports"]},{"cell_type":"code","metadata":{"id":"ZGKhq2pgL1jw"},"source":["import os\n","import sys\n","import copy\n","import time\n","import random\n","import numpy as np\n","from collections import defaultdict\n","from multiprocessing import Process, Queue\n","\n","import torch\n","\n","import argparse"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"M07jAbLyMNSr"},"source":["### Params"]},{"cell_type":"code","metadata":{"id":"9-aT_taXMNGI"},"source":["parser = argparse.ArgumentParser()\n","# parser.add_argument('--dataset', default='ml1m')\n","parser.add_argument('--dataset', default='beauty')\n","# parser.add_argument('--dataset', default='steam')\n","# parser.add_argument('--dataset', default='wikipedia')\n","# parser.add_argument('--dataset', default='video')\n","parser.add_argument('--train_dir', default='train')\n","parser.add_argument('--batch_size', default=128, type=int)\n","parser.add_argument('--lr', default=0.001, type=float)\n","parser.add_argument('--maxlen', default=50, type=int)\n","parser.add_argument('--hidden_units', default=50, type=int)\n","parser.add_argument('--num_blocks', default=2, type=int)\n","parser.add_argument('--num_epochs', default=201, type=int)\n","parser.add_argument('--num_heads', default=1, type=int)\n","parser.add_argument('--dropout_rate', default=0.5, type=float)\n","parser.add_argument('--l2_emb', default=0.0, type=float)\n","parser.add_argument('--device', default='cuda', type=str)\n","parser.add_argument('--inference_only', default=False, type=str2bool)\n","parser.add_argument('--state_dict_path', default=None, type=str)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"YvmfvYVLL9kg"},"source":["### Data downloads"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qFP3HO1zqj6G","executionInfo":{"status":"ok","timestamp":1633257376191,"user_tz":-330,"elapsed":4827,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}},"outputId":"e75a5122-aad3-4996-88fc-c33c8f50b6a7"},"source":["!mkdir -p data\n","!wget -O data/ml1m.txt -q --show-progress https://github.com/pmixer/SASRec.pytorch/raw/master/data/ml-1m.txt\n","!wget -O data/beauty.txt -q --show-progress https://github.com/pmixer/SASRec.pytorch/raw/master/data/Beauty.txt\n","!wget -O data/steam.txt -q --show-progress https://github.com/pmixer/SASRec.pytorch/raw/master/data/Steam.txt\n","!wget -O data/video.txt -q --show-progress https://github.com/pmixer/SASRec.pytorch/raw/master/data/Video.txt\n","!wget -O data/wikipedia.txt -q --show-progress https://github.com/pmixer/SASRec.pytorch/raw/master/data/wikipedia.txt"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["data/ml1m.txt       100%[===================>]   8.63M  --.-KB/s    in 0.1s    \n","data/beauty.txt     100%[===================>]   4.33M  --.-KB/s    in 0.08s   \n","data/steam.txt      100%[===================>]  40.57M   156MB/s    in 0.3s    \n","data/video.txt      100%[===================>]   3.04M  --.-KB/s    in 0.07s   \n","data/wikipedia.txt  100%[===================>]   1.26M  --.-KB/s    in 0.05s   \n"]}]},{"cell_type":"markdown","metadata":{"id":"Ew2yGjvsq1xF"},"source":["##  Utils"]},{"cell_type":"code","metadata":{"id":"RIHzzWNSq6YJ"},"source":["# sampler for batch generation\n","def random_neq(l, r, s):\n","    t = np.random.randint(l, r)\n","    while t in s:\n","        t = np.random.randint(l, r)\n","    return t\n","\n","\n","def sample_function(user_train, usernum, itemnum, batch_size, maxlen, result_queue, SEED):\n","    def sample():\n","\n","        user = np.random.randint(1, usernum + 1)\n","        while len(user_train[user]) <= 1: user = np.random.randint(1, usernum + 1)\n","\n","        seq = np.zeros([maxlen], dtype=np.int32)\n","        pos = np.zeros([maxlen], dtype=np.int32)\n","        neg = np.zeros([maxlen], dtype=np.int32)\n","        nxt = user_train[user][-1]\n","        idx = maxlen - 1\n","\n","        ts = set(user_train[user])\n","        for i in reversed(user_train[user][:-1]):\n","            seq[idx] = i\n","            pos[idx] = nxt\n","            if nxt != 0: neg[idx] = random_neq(1, itemnum + 1, ts)\n","            nxt = i\n","            idx -= 1\n","            if idx == -1: break\n","\n","        return (user, seq, pos, neg)\n","\n","    np.random.seed(SEED)\n","    while True:\n","        one_batch = []\n","        for i in range(batch_size):\n","            one_batch.append(sample())\n","\n","        result_queue.put(zip(*one_batch))\n","\n","\n","class WarpSampler(object):\n","    def __init__(self, User, usernum, itemnum, batch_size=64, maxlen=10, n_workers=1):\n","        self.result_queue = Queue(maxsize=n_workers * 10)\n","        self.processors = []\n","        for i in range(n_workers):\n","            self.processors.append(\n","                Process(target=sample_function, args=(User,\n","                                                      usernum,\n","                                                      itemnum,\n","                                                      batch_size,\n","                                                      maxlen,\n","                                                      self.result_queue,\n","                                                      np.random.randint(2e9)\n","                                                      )))\n","            self.processors[-1].daemon = True\n","            self.processors[-1].start()\n","\n","    def next_batch(self):\n","        return self.result_queue.get()\n","\n","    def close(self):\n","        for p in self.processors:\n","            p.terminate()\n","            p.join()\n","\n","\n","# train/val/test data generation\n","def data_partition(fname):\n","    usernum = 0\n","    itemnum = 0\n","    User = defaultdict(list)\n","    user_train = {}\n","    user_valid = {}\n","    user_test = {}\n","    # assume user/item index starting from 1\n","    f = open('data/%s.txt' % fname, 'r')\n","    for line in f:\n","        u, i = line.rstrip().split(' ')\n","        u = int(u)\n","        i = int(i)\n","        usernum = max(u, usernum)\n","        itemnum = max(i, itemnum)\n","        User[u].append(i)\n","\n","    for user in User:\n","        nfeedback = len(User[user])\n","        if nfeedback < 3:\n","            user_train[user] = User[user]\n","            user_valid[user] = []\n","            user_test[user] = []\n","        else:\n","            user_train[user] = User[user][:-2]\n","            user_valid[user] = []\n","            user_valid[user].append(User[user][-2])\n","            user_test[user] = []\n","            user_test[user].append(User[user][-1])\n","    return [user_train, user_valid, user_test, usernum, itemnum]\n","\n","# TODO: merge evaluate functions for test and val set\n","# evaluate on test set\n","def evaluate(model, dataset, args):\n","    [train, valid, test, usernum, itemnum] = copy.deepcopy(dataset)\n","\n","    NDCG = 0.0\n","    HT = 0.0\n","    valid_user = 0.0\n","\n","    if usernum>10000:\n","        users = random.sample(range(1, usernum + 1), 10000)\n","    else:\n","        users = range(1, usernum + 1)\n","    for u in users:\n","\n","        if len(train[u]) < 1 or len(test[u]) < 1: continue\n","\n","        seq = np.zeros([args.maxlen], dtype=np.int32)\n","        idx = args.maxlen - 1\n","        seq[idx] = valid[u][0]\n","        idx -= 1\n","        for i in reversed(train[u]):\n","            seq[idx] = i\n","            idx -= 1\n","            if idx == -1: break\n","        rated = set(train[u])\n","        rated.add(0)\n","        item_idx = [test[u][0]]\n","        for _ in range(100):\n","            t = np.random.randint(1, itemnum + 1)\n","            while t in rated: t = np.random.randint(1, itemnum + 1)\n","            item_idx.append(t)\n","\n","        predictions = -model.predict(*[np.array(l) for l in [[u], [seq], item_idx]])\n","        predictions = predictions[0] # - for 1st argsort DESC\n","\n","        rank = predictions.argsort().argsort()[0].item()\n","\n","        valid_user += 1\n","\n","        if rank < 10:\n","            NDCG += 1 / np.log2(rank + 2)\n","            HT += 1\n","        if valid_user % 100 == 0:\n","            print('.', end=\"\")\n","            sys.stdout.flush()\n","\n","    return NDCG / valid_user, HT / valid_user\n","\n","\n","# evaluate on val set\n","def evaluate_valid(model, dataset, args):\n","    [train, valid, test, usernum, itemnum] = copy.deepcopy(dataset)\n","\n","    NDCG = 0.0\n","    valid_user = 0.0\n","    HT = 0.0\n","    if usernum>10000:\n","        users = random.sample(range(1, usernum + 1), 10000)\n","    else:\n","        users = range(1, usernum + 1)\n","    for u in users:\n","        if len(train[u]) < 1 or len(valid[u]) < 1: continue\n","\n","        seq = np.zeros([args.maxlen], dtype=np.int32)\n","        idx = args.maxlen - 1\n","        for i in reversed(train[u]):\n","            seq[idx] = i\n","            idx -= 1\n","            if idx == -1: break\n","\n","        rated = set(train[u])\n","        rated.add(0)\n","        item_idx = [valid[u][0]]\n","        for _ in range(100):\n","            t = np.random.randint(1, itemnum + 1)\n","            while t in rated: t = np.random.randint(1, itemnum + 1)\n","            item_idx.append(t)\n","\n","        predictions = -model.predict(*[np.array(l) for l in [[u], [seq], item_idx]])\n","        predictions = predictions[0]\n","\n","        rank = predictions.argsort().argsort()[0].item()\n","\n","        valid_user += 1\n","\n","        if rank < 10:\n","            NDCG += 1 / np.log2(rank + 2)\n","            HT += 1\n","        if valid_user % 100 == 0:\n","            print('.', end=\"\")\n","            sys.stdout.flush()\n","\n","    return NDCG / valid_user, HT / valid_user"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"bTUUobKxq9xE"},"source":["##  Model"]},{"cell_type":"code","metadata":{"id":"zDiAUNiurAKq"},"source":["class PointWiseFeedForward(torch.nn.Module):\n","    def __init__(self, hidden_units, dropout_rate):\n","\n","        super(PointWiseFeedForward, self).__init__()\n","\n","        self.conv1 = torch.nn.Conv1d(hidden_units, hidden_units, kernel_size=1)\n","        self.dropout1 = torch.nn.Dropout(p=dropout_rate)\n","        self.relu = torch.nn.ReLU()\n","        self.conv2 = torch.nn.Conv1d(hidden_units, hidden_units, kernel_size=1)\n","        self.dropout2 = torch.nn.Dropout(p=dropout_rate)\n","\n","    def forward(self, inputs):\n","        outputs = self.dropout2(self.conv2(self.relu(self.dropout1(self.conv1(inputs.transpose(-1, -2))))))\n","        outputs = outputs.transpose(-1, -2) # as Conv1D requires (N, C, Length)\n","        outputs += inputs\n","        return outputs\n","\n","# pls use the following self-made multihead attention layer\n","# in case your pytorch version is below 1.16 or for other reasons\n","# https://github.com/pmixer/TiSASRec.pytorch/blob/master/model.py\n","\n","class SASRec(torch.nn.Module):\n","    def __init__(self, user_num, item_num, args):\n","        super(SASRec, self).__init__()\n","\n","        self.user_num = user_num\n","        self.item_num = item_num\n","        self.dev = args.device\n","\n","        # TODO: loss += args.l2_emb for regularizing embedding vectors during training\n","        # https://stackoverflow.com/questions/42704283/adding-l1-l2-regularization-in-pytorch\n","        self.item_emb = torch.nn.Embedding(self.item_num+1, args.hidden_units, padding_idx=0)\n","        self.pos_emb = torch.nn.Embedding(args.maxlen, args.hidden_units) # TO IMPROVE\n","        self.emb_dropout = torch.nn.Dropout(p=args.dropout_rate)\n","\n","        self.attention_layernorms = torch.nn.ModuleList() # to be Q for self-attention\n","        self.attention_layers = torch.nn.ModuleList()\n","        self.forward_layernorms = torch.nn.ModuleList()\n","        self.forward_layers = torch.nn.ModuleList()\n","\n","        self.last_layernorm = torch.nn.LayerNorm(args.hidden_units, eps=1e-8)\n","\n","        for _ in range(args.num_blocks):\n","            new_attn_layernorm = torch.nn.LayerNorm(args.hidden_units, eps=1e-8)\n","            self.attention_layernorms.append(new_attn_layernorm)\n","\n","            new_attn_layer =  torch.nn.MultiheadAttention(args.hidden_units,\n","                                                            args.num_heads,\n","                                                            args.dropout_rate)\n","            self.attention_layers.append(new_attn_layer)\n","\n","            new_fwd_layernorm = torch.nn.LayerNorm(args.hidden_units, eps=1e-8)\n","            self.forward_layernorms.append(new_fwd_layernorm)\n","\n","            new_fwd_layer = PointWiseFeedForward(args.hidden_units, args.dropout_rate)\n","            self.forward_layers.append(new_fwd_layer)\n","\n","            # self.pos_sigmoid = torch.nn.Sigmoid()\n","            # self.neg_sigmoid = torch.nn.Sigmoid()\n","\n","    def log2feats(self, log_seqs):\n","        seqs = self.item_emb(torch.LongTensor(log_seqs).to(self.dev))\n","        seqs *= self.item_emb.embedding_dim ** 0.5\n","        positions = np.tile(np.array(range(log_seqs.shape[1])), [log_seqs.shape[0], 1])\n","        seqs += self.pos_emb(torch.LongTensor(positions).to(self.dev))\n","        seqs = self.emb_dropout(seqs)\n","\n","        timeline_mask = torch.BoolTensor(log_seqs == 0).to(self.dev)\n","        seqs *= ~timeline_mask.unsqueeze(-1) # broadcast in last dim\n","\n","        tl = seqs.shape[1] # time dim len for enforce causality\n","        attention_mask = ~torch.tril(torch.ones((tl, tl), dtype=torch.bool, device=self.dev))\n","\n","        for i in range(len(self.attention_layers)):\n","            seqs = torch.transpose(seqs, 0, 1)\n","            Q = self.attention_layernorms[i](seqs)\n","            mha_outputs, _ = self.attention_layers[i](Q, seqs, seqs, \n","                                            attn_mask=attention_mask)\n","                                            # key_padding_mask=timeline_mask\n","                                            # need_weights=False) this arg do not work?\n","            seqs = Q + mha_outputs\n","            seqs = torch.transpose(seqs, 0, 1)\n","\n","            seqs = self.forward_layernorms[i](seqs)\n","            seqs = self.forward_layers[i](seqs)\n","            seqs *=  ~timeline_mask.unsqueeze(-1)\n","\n","        log_feats = self.last_layernorm(seqs) # (U, T, C) -> (U, -1, C)\n","\n","        return log_feats\n","\n","    def forward(self, user_ids, log_seqs, pos_seqs, neg_seqs): # for training        \n","        log_feats = self.log2feats(log_seqs) # user_ids hasn't been used yet\n","\n","        pos_embs = self.item_emb(torch.LongTensor(pos_seqs).to(self.dev))\n","        neg_embs = self.item_emb(torch.LongTensor(neg_seqs).to(self.dev))\n","\n","        pos_logits = (log_feats * pos_embs).sum(dim=-1)\n","        neg_logits = (log_feats * neg_embs).sum(dim=-1)\n","\n","        # pos_pred = self.pos_sigmoid(pos_logits)\n","        # neg_pred = self.neg_sigmoid(neg_logits)\n","\n","        return pos_logits, neg_logits # pos_pred, neg_pred\n","\n","    def predict(self, user_ids, log_seqs, item_indices): # for inference\n","        log_feats = self.log2feats(log_seqs) # user_ids hasn't been used yet\n","\n","        final_feat = log_feats[:, -1, :] # only use last QKV classifier, a waste\n","\n","        item_embs = self.item_emb(torch.LongTensor(item_indices).to(self.dev)) # (U, I, C)\n","\n","        logits = item_embs.matmul(final_feat.unsqueeze(-1)).squeeze(-1)\n","\n","        # preds = self.pos_sigmoid(logits) # rank same item list for different users\n","\n","        return logits # preds # (U, I)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6n6aFbT4rCKk"},"source":["## Run"]},{"cell_type":"code","metadata":{"id":"0_2TzzI2rPa2"},"source":["def str2bool(s):\n","    if s not in {'false', 'true'}:\n","        raise ValueError('Not a valid boolean string')\n","    return s == 'true'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2Pryb4aFrVk1"},"source":["args = parser.parse_args(args={})\n","if not os.path.isdir(args.dataset + '_' + args.train_dir):\n","    os.makedirs(args.dataset + '_' + args.train_dir)\n","with open(os.path.join(args.dataset + '_' + args.train_dir, 'args.txt'), 'w') as f:\n","    f.write('\\n'.join([str(k) + ',' + str(v) for k, v in sorted(vars(args).items(), key=lambda x: x[0])]))\n","f.close()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7xPTLOyjr8NI","executionInfo":{"status":"ok","timestamp":1633260825293,"user_tz":-330,"elapsed":3232953,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}},"outputId":"cce4c9c8-b2db-4dec-c3dc-ae07b80332e0"},"source":["if __name__ == '__main__':\n","    # global dataset\n","    dataset = data_partition(args.dataset)\n","\n","    [user_train, user_valid, user_test, usernum, itemnum] = dataset\n","    num_batch = len(user_train) // args.batch_size # tail? + ((len(user_train) % args.batch_size) != 0)\n","    cc = 0.0\n","    for u in user_train:\n","        cc += len(user_train[u])\n","    print('average sequence length: %.2f' % (cc / len(user_train)))\n","    \n","    f = open(os.path.join(args.dataset + '_' + args.train_dir, 'log.txt'), 'w')\n","    \n","    sampler = WarpSampler(user_train, usernum, itemnum, batch_size=args.batch_size, maxlen=args.maxlen, n_workers=3)\n","    model = SASRec(usernum, itemnum, args).to(args.device) # no ReLU activation in original SASRec implementation?\n","    \n","    for name, param in model.named_parameters():\n","        try:\n","            torch.nn.init.xavier_normal_(param.data)\n","        except:\n","            pass # just ignore those failed init layers\n","    \n","    # this fails embedding init 'Embedding' object has no attribute 'dim'\n","    # model.apply(torch.nn.init.xavier_uniform_)\n","    \n","    model.train() # enable model training\n","    \n","    epoch_start_idx = 1\n","    if args.state_dict_path is not None:\n","        try:\n","            model.load_state_dict(torch.load(args.state_dict_path, map_location=torch.device(args.device)))\n","            tail = args.state_dict_path[args.state_dict_path.find('epoch=') + 6:]\n","            epoch_start_idx = int(tail[:tail.find('.')]) + 1\n","        except: # in case your pytorch version is not 1.6 etc., pls debug by pdb if load weights failed\n","            print('failed loading state_dicts, pls check file path: ', end=\"\")\n","            print(args.state_dict_path)\n","            print('pdb enabled for your quick check, pls type exit() if you do not need it')\n","            import pdb; pdb.set_trace()\n","            \n","    \n","    if args.inference_only:\n","        model.eval()\n","        t_test = evaluate(model, dataset, args)\n","        print('test (NDCG@10: %.4f, HR@10: %.4f)' % (t_test[0], t_test[1]))\n","    \n","    # ce_criterion = torch.nn.CrossEntropyLoss()\n","    # https://github.com/NVIDIA/pix2pixHD/issues/9 how could an old bug appear again...\n","    bce_criterion = torch.nn.BCEWithLogitsLoss() # torch.nn.BCELoss()\n","    adam_optimizer = torch.optim.Adam(model.parameters(), lr=args.lr, betas=(0.9, 0.98))\n","    \n","    T = 0.0\n","    t0 = time.time()\n","    \n","    for epoch in range(epoch_start_idx, args.num_epochs + 1):\n","        print('Epoch {}'.format(epoch))\n","        if args.inference_only: break # just to decrease identition\n","        for step in range(num_batch): # tqdm(range(num_batch), total=num_batch, ncols=70, leave=False, unit='b'):\n","            u, seq, pos, neg = sampler.next_batch() # tuples to ndarray\n","            u, seq, pos, neg = np.array(u), np.array(seq), np.array(pos), np.array(neg)\n","            pos_logits, neg_logits = model(u, seq, pos, neg)\n","            pos_labels, neg_labels = torch.ones(pos_logits.shape, device=args.device), torch.zeros(neg_logits.shape, device=args.device)\n","            # print(\"\\neye ball check raw_logits:\"); print(pos_logits); print(neg_logits) # check pos_logits > 0, neg_logits < 0\n","            adam_optimizer.zero_grad()\n","            indices = np.where(pos != 0)\n","            loss = bce_criterion(pos_logits[indices], pos_labels[indices])\n","            loss += bce_criterion(neg_logits[indices], neg_labels[indices])\n","            for param in model.item_emb.parameters(): loss += args.l2_emb * torch.norm(param)\n","            loss.backward()\n","            adam_optimizer.step()\n","            print(\"loss in epoch {} iteration {}: {}\".format(epoch, step, loss.item())) # expected 0.4~0.6 after init few epochs\n","    \n","        if epoch % 20 == 0:\n","            model.eval()\n","            t1 = time.time() - t0\n","            T += t1\n","            print('Evaluating', end='')\n","            t_test = evaluate(model, dataset, args)\n","            t_valid = evaluate_valid(model, dataset, args)\n","            print('epoch:%d, time: %f(s), valid (NDCG@10: %.4f, HR@10: %.4f), test (NDCG@10: %.4f, HR@10: %.4f)'\n","                    % (epoch, T, t_valid[0], t_valid[1], t_test[0], t_test[1]))\n","    \n","            f.write(str(t_valid) + ' ' + str(t_test) + '\\n')\n","            f.flush()\n","            t0 = time.time()\n","            model.train()\n","    \n","        if epoch == args.num_epochs:\n","            folder = args.dataset + '_' + args.train_dir\n","            fname = 'SASRec.epoch={}.lr={}.layer={}.head={}.hidden={}.maxlen={}.pth'\n","            fname = fname.format(args.num_epochs, args.lr, args.num_blocks, args.num_heads, args.hidden_units, args.maxlen)\n","            torch.save(model.state_dict(), os.path.join(folder, fname))\n","    \n","    f.close()\n","    sampler.close()\n","    print(\"Done\")"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["average sequence length: 5.63\n","Epoch 1\n","Epoch 2\n","Epoch 3\n","Epoch 4\n","Epoch 5\n","Epoch 6\n","Epoch 7\n","Epoch 8\n","Epoch 9\n","Epoch 10\n","Epoch 11\n","Epoch 12\n","Epoch 13\n","Epoch 14\n","Epoch 15\n","Epoch 16\n","Epoch 17\n","Epoch 18\n","Epoch 19\n","Epoch 20\n","Evaluating................................................................................................................................................................................................epoch:20, time: 236.749498(s), valid (NDCG@10: 0.2822, HR@10: 0.4481), test (NDCG@10: 0.2575, HR@10: 0.4166)\n","Epoch 21\n","Epoch 22\n","Epoch 23\n","Epoch 24\n","Epoch 25\n","Epoch 26\n","Epoch 27\n","Epoch 28\n","Epoch 29\n","Epoch 30\n","Epoch 31\n","Epoch 32\n","Epoch 33\n","Epoch 34\n","Epoch 35\n","Epoch 36\n","Epoch 37\n","Epoch 38\n","Epoch 39\n","Epoch 40\n","Evaluating................................................................................................................................................................................................epoch:40, time: 472.672947(s), valid (NDCG@10: 0.2954, HR@10: 0.4634), test (NDCG@10: 0.2695, HR@10: 0.4326)\n","Epoch 41\n","Epoch 42\n","Epoch 43\n","Epoch 44\n","Epoch 45\n","Epoch 46\n","Epoch 47\n","Epoch 48\n","Epoch 49\n","Epoch 50\n","Epoch 51\n","Epoch 52\n","Epoch 53\n","Epoch 54\n","Epoch 55\n","Epoch 56\n","Epoch 57\n","Epoch 58\n","Epoch 59\n","Epoch 60\n","Evaluating................................................................................................................................................................................................epoch:60, time: 709.644598(s), valid (NDCG@10: 0.2972, HR@10: 0.4589), test (NDCG@10: 0.2783, HR@10: 0.4371)\n","Epoch 61\n","Epoch 62\n","Epoch 63\n","Epoch 64\n","Epoch 65\n","Epoch 66\n","Epoch 67\n","Epoch 68\n","Epoch 69\n","Epoch 70\n","Epoch 71\n","Epoch 72\n","Epoch 73\n","Epoch 74\n","Epoch 75\n","Epoch 76\n","Epoch 77\n","Epoch 78\n","Epoch 79\n","Epoch 80\n","Evaluating................................................................................................................................................................................................epoch:80, time: 946.620109(s), valid (NDCG@10: 0.3067, HR@10: 0.4731), test (NDCG@10: 0.2757, HR@10: 0.4314)\n","Epoch 81\n","Epoch 82\n","Epoch 83\n","Epoch 84\n","Epoch 85\n","Epoch 86\n","Epoch 87\n","Epoch 88\n","Epoch 89\n","Epoch 90\n","Epoch 91\n","Epoch 92\n","Epoch 93\n","Epoch 94\n","Epoch 95\n","Epoch 96\n","Epoch 97\n","Epoch 98\n","Epoch 99\n","Epoch 100\n","Evaluating.................................................................................................................................................................................................epoch:100, time: 1183.069700(s), valid (NDCG@10: 0.3107, HR@10: 0.4775), test (NDCG@10: 0.2806, HR@10: 0.4368)\n","Epoch 101\n","Epoch 102\n","Epoch 103\n","Epoch 104\n","Epoch 105\n","Epoch 106\n","Epoch 107\n","Epoch 108\n","Epoch 109\n","Epoch 110\n","Epoch 111\n","Epoch 112\n","Epoch 113\n","Epoch 114\n","Epoch 115\n","Epoch 116\n","Epoch 117\n","Epoch 118\n","Epoch 119\n","Epoch 120\n","Evaluating................................................................................................................................................................................................epoch:120, time: 1419.415009(s), valid (NDCG@10: 0.3213, HR@10: 0.4836), test (NDCG@10: 0.2928, HR@10: 0.4532)\n","Epoch 121\n","Epoch 122\n","Epoch 123\n","Epoch 124\n","Epoch 125\n","Epoch 126\n","Epoch 127\n","Epoch 128\n","Epoch 129\n","Epoch 130\n","Epoch 131\n","Epoch 132\n","Epoch 133\n","Epoch 134\n","Epoch 135\n","Epoch 136\n","Epoch 137\n","Epoch 138\n","Epoch 139\n","Epoch 140\n","Evaluating................................................................................................................................................................................................epoch:140, time: 1655.250550(s), valid (NDCG@10: 0.3208, HR@10: 0.4852), test (NDCG@10: 0.2990, HR@10: 0.4550)\n","Epoch 141\n","Epoch 142\n","Epoch 143\n","Epoch 144\n","Epoch 145\n","Epoch 146\n","Epoch 147\n","Epoch 148\n","Epoch 149\n","Epoch 150\n","Epoch 151\n","Epoch 152\n","Epoch 153\n","Epoch 154\n","Epoch 155\n","Epoch 156\n","Epoch 157\n","Epoch 158\n","Epoch 159\n","Epoch 160\n","Evaluating................................................................................................................................................................................................epoch:160, time: 1889.764914(s), valid (NDCG@10: 0.3333, HR@10: 0.4954), test (NDCG@10: 0.2929, HR@10: 0.4466)\n","Epoch 161\n","Epoch 162\n","Epoch 163\n","Epoch 164\n","Epoch 165\n","Epoch 166\n","Epoch 167\n","Epoch 168\n","Epoch 169\n","Epoch 170\n","Epoch 171\n","Epoch 172\n","Epoch 173\n","Epoch 174\n","Epoch 175\n","Epoch 176\n","Epoch 177\n","Epoch 178\n","Epoch 179\n","Epoch 180\n","Evaluating................................................................................................................................................................................................epoch:180, time: 2124.906773(s), valid (NDCG@10: 0.3362, HR@10: 0.5021), test (NDCG@10: 0.2895, HR@10: 0.4457)\n","Epoch 181\n","Epoch 182\n","Epoch 183\n","Epoch 184\n","Epoch 185\n","Epoch 186\n","Epoch 187\n","Epoch 188\n","Epoch 189\n","Epoch 190\n","Epoch 191\n","Epoch 192\n","Epoch 193\n","Epoch 194\n","Epoch 195\n","Epoch 196\n","Epoch 197\n","Epoch 198\n","Epoch 199\n","Epoch 200\n","Evaluating................................................................................................................................................................................................epoch:200, time: 2361.204898(s), valid (NDCG@10: 0.3295, HR@10: 0.4943), test (NDCG@10: 0.3000, HR@10: 0.4609)\n","Epoch 201\n","Done\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_gzzHYSusG8f","executionInfo":{"status":"ok","timestamp":1633261166513,"user_tz":-330,"elapsed":6897,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}},"outputId":"8508d294-6709-4252-ea93-3fc1d5b48a66"},"source":["!apt-get install tree"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Reading package lists... Done\n","Building dependency tree       \n","Reading state information... Done\n","The following NEW packages will be installed:\n","  tree\n","0 upgraded, 1 newly installed, 0 to remove and 37 not upgraded.\n","Need to get 40.7 kB of archives.\n","After this operation, 105 kB of additional disk space will be used.\n","Get:1 http://archive.ubuntu.com/ubuntu bionic/universe amd64 tree amd64 1.7.0-5 [40.7 kB]\n","Fetched 40.7 kB in 0s (108 kB/s)\n","Selecting previously unselected package tree.\n","(Reading database ... 155047 files and directories currently installed.)\n","Preparing to unpack .../tree_1.7.0-5_amd64.deb ...\n","Unpacking tree (1.7.0-5) ...\n","Setting up tree (1.7.0-5) ...\n","Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_E3siaoE84DR","executionInfo":{"status":"ok","timestamp":1633261199077,"user_tz":-330,"elapsed":686,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}},"outputId":"4b28aa89-21d1-451d-ddc8-08a8372af21c"},"source":["!tree --du -h ./beauty_train"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["./beauty_train\n","├── [ 202]  args.txt\n","├── [ 833]  log.txt\n","└── [ 11M]  SASRec.epoch=201.lr=0.001.layer=2.head=1.hidden=50.maxlen=50.pth\n","\n","  11M used in 0 directories, 3 files\n"]}]}]}