{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp datasets.tmall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tmall dataset\n",
    "> Tmall dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev.showdoc import *\n",
    "from fastcore.nb_imports import *\n",
    "from fastcore.test import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "import pickle\n",
    "\n",
    "from recohut.datasets.bases.session_graph import SessionGraphDataset\n",
    "from recohut.datasets.bases.common import Dataset\n",
    "from recohut.utils.common_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class TmallDataset(SessionGraphDataset):\n",
    "    train_url = \"https://github.com/RecoHut-Datasets/tmall/raw/v1/train.txt\"\n",
    "    test_url = \"https://github.com/RecoHut-Datasets/tmall/raw/v1/test.txt\"\n",
    "    all_train_seq_url = \"https://github.com/RecoHut-Datasets/tmall/raw/v1/all_train_seq.txt\"\n",
    "\n",
    "    def __init__(self, root, shuffle=False, n_node=40727, is_train=True):\n",
    "        self.n_node = n_node\n",
    "        self.shuffle = shuffle\n",
    "        self.is_train = is_train\n",
    "        super().__init__(root, shuffle, n_node)\n",
    "\n",
    "    @property\n",
    "    def raw_file_names(self) -> str:\n",
    "        if self.is_train:\n",
    "            return ['train.txt', 'all_train_seq.txt']\n",
    "        return ['test.txt', 'all_train_seq.txt']\n",
    "\n",
    "    def download(self):\n",
    "        download_url(self.all_train_seq_url, self.raw_dir)\n",
    "        if self.is_train:\n",
    "            download_url(self.train_url, self.raw_dir)\n",
    "        else:\n",
    "            download_url(self.test_url, self.raw_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = '/content/tmall'\n",
    "\n",
    "train_data = TmallDataset(root=root, shuffle=True, is_train=True)\n",
    "test_data = TmallDataset(root=root, shuffle=False, is_train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class TmallDataset_v2(Dataset):\n",
    "    def __init__(self, data_dir):\n",
    "        self.behs = ['pv', 'fav', 'cart', 'buy']\n",
    "        super().__init__(data_dir)\n",
    "\n",
    "        self._process()\n",
    "\n",
    "    @property\n",
    "    def raw_file_names(self):\n",
    "        return ['trn_buy','trn_cart','trn_pv','trn_fav','tst_int']\n",
    "    \n",
    "    @property\n",
    "    def processed_file_names(self):\n",
    "        return 'data.zip'\n",
    "\n",
    "    def download(self):\n",
    "        urls = ['https://github.com/RecoHut-Datasets/tmall/raw/v3/trn_buy',\n",
    "                'https://github.com/RecoHut-Datasets/tmall/raw/v3/trn_cart',\n",
    "                'https://github.com/RecoHut-Datasets/tmall/raw/v3/trn_pv',\n",
    "                'https://github.com/RecoHut-Datasets/tmall/raw/v3/trn_fav',\n",
    "                'https://github.com/RecoHut-Datasets/tmall/raw/v3/tst_int']\n",
    "        for url in urls:\n",
    "            _ = download_url(url, self.raw_dir)\n",
    "\n",
    "    def process(self):\n",
    "        trnMats = list()\n",
    "        for path in self.raw_paths[:-1]:\n",
    "            with open(path, 'rb') as fs:\n",
    "                mat = (pickle.load(fs) != 0).astype(np.float32)\n",
    "            trnMats.append(mat)\n",
    "        # test set\n",
    "        path = self.raw_paths[-1]\n",
    "        with open(path, 'rb') as fs:\n",
    "            tstInt = np.array(pickle.load(fs))\n",
    "        tstStat = (tstInt != None)\n",
    "        tstUsrs = np.reshape(np.argwhere(tstStat != False), [-1])\n",
    "        self.trnMats = trnMats\n",
    "        self.tstInt = tstInt\n",
    "        self.tstUsrs = tstUsrs\n",
    "        self.user, self.item = self.trnMats[0].shape\n",
    "        self.behNum = len(self.behs)\n",
    "\n",
    "        adj = 0\n",
    "        for i in range(self.behNum):\n",
    "            adj = adj + self.trnMats[i]\n",
    "        adj = (adj != 0).astype(np.float32)\n",
    "        self.labelP = np.squeeze(np.array(np.sum(adj, axis=0)))\n",
    "        tpadj = self.transpose(adj)\n",
    "        adjNorm = np.reshape(np.array(np.sum(adj, axis=1)), [-1])\n",
    "        tpadjNorm = np.reshape(np.array(np.sum(tpadj, axis=1)), [-1])\n",
    "        for i in range(adj.shape[0]):\n",
    "            for j in range(adj.indptr[i], adj.indptr[i+1]):\n",
    "                adj.data[j] /= adjNorm[i]\n",
    "        for i in range(tpadj.shape[0]):\n",
    "            for j in range(tpadj.indptr[i], tpadj.indptr[i+1]):\n",
    "                tpadj.data[j] /= tpadjNorm[i]\n",
    "        self.adj = adj\n",
    "        self.tpadj = tpadj\n",
    "\n",
    "    @staticmethod\n",
    "    def transpose(mat):\n",
    "        coomat = sp.coo_matrix(mat)\n",
    "        return sp.csr_matrix(coomat.transpose())\n",
    "\n",
    "    @staticmethod\n",
    "    def make_mask(nodes, size):\n",
    "        mask = np.ones(size)\n",
    "        if not nodes is None:\n",
    "            mask[nodes] = 0.0\n",
    "        return mask\n",
    "\n",
    "    @staticmethod\n",
    "    def update_bdgt(adj, nodes):\n",
    "        if nodes is None:\n",
    "            return 0\n",
    "        tembat = 1000\n",
    "        ret = 0\n",
    "        for i in range(int(np.ceil(len(nodes) / tembat))):\n",
    "            st = tembat * i\n",
    "            ed = min((i+1) * tembat, len(nodes))\n",
    "            temNodes = nodes[st: ed]\n",
    "            ret += np.sum(adj[temNodes], axis=0)\n",
    "        return ret\n",
    "\n",
    "    @staticmethod\n",
    "    def sample(budget, mask, sampNum):\n",
    "        score = (mask * np.reshape(np.array(budget), [-1])) ** 2\n",
    "        norm = np.sum(score)\n",
    "        if norm == 0:\n",
    "            return np.random.choice(len(score), 1), sampNum - 1\n",
    "        score = list(score / norm)\n",
    "        arrScore = np.array(score)\n",
    "        posNum = np.sum(np.array(score)!=0)\n",
    "        if posNum < sampNum:\n",
    "            pckNodes1 = np.squeeze(np.argwhere(arrScore!=0))\n",
    "            # pckNodes2 = np.random.choice(np.squeeze(np.argwhere(arrScore==0.0)), min(len(score) - posNum, sampNum - posNum), replace=False)\n",
    "            # pckNodes = np.concatenate([pckNodes1, pckNodes2], axis=0)\n",
    "            pckNodes = pckNodes1\n",
    "        else:\n",
    "            pckNodes = np.random.choice(len(score), sampNum, p=score, replace=False)\n",
    "        return pckNodes, max(sampNum - posNum, 0)\n",
    "\n",
    "    @staticmethod\n",
    "    def transToLsts(mat, mask=False, norm=False):\n",
    "        shape = [mat.shape[0], mat.shape[1]]\n",
    "        coomat = sp.coo_matrix(mat)\n",
    "        indices = np.array(list(map(list, zip(coomat.row, coomat.col))), dtype=np.int32)\n",
    "        data = coomat.data.astype(np.float32)\n",
    "\n",
    "        if norm:\n",
    "            rowD = np.squeeze(np.array(1 / (np.sqrt(np.sum(mat, axis=1) + 1e-8) + 1e-8)))\n",
    "            colD = np.squeeze(np.array(1 / (np.sqrt(np.sum(mat, axis=0) + 1e-8) + 1e-8)))\n",
    "            for i in range(len(data)):\n",
    "                row = indices[i, 0]\n",
    "                col = indices[i, 1]\n",
    "                data[i] = data[i] * rowD[row] * colD[col]\n",
    "        # half mask\n",
    "        if mask:\n",
    "            spMask = (np.random.uniform(size=data.shape) > 0.5) * 1.0\n",
    "            data = data * spMask\n",
    "\n",
    "        if indices.shape[0] == 0:\n",
    "            indices = np.array([[0, 0]], dtype=np.int32)\n",
    "            data = np.array([0.0], np.float32)\n",
    "        return indices, data, shape\n",
    "\n",
    "    def construct_data(self, adjs, usrs, itms):\n",
    "        pckAdjs = []\n",
    "        pckTpAdjs = []\n",
    "        for i in range(len(adjs)):\n",
    "            pckU = adjs[i][usrs]\n",
    "            tpPckI = self.transpose(pckU)[itms]\n",
    "            pckTpAdjs.append(tpPckI)\n",
    "            pckAdjs.append(self.transpose(tpPckI))\n",
    "        return pckAdjs, pckTpAdjs, usrs, itms\n",
    "\n",
    "    def sample_large_graph(self, pckUsrs, pckItms=None, sampDepth=2, sampNum=1e3, preSamp=False):\n",
    "        adj = self.adj\n",
    "        tpadj = self.tpadj\n",
    "        usrMask = self.make_mask(pckUsrs, adj.shape[0])\n",
    "        itmMask = self.make_mask(pckItms, adj.shape[1])\n",
    "        itmBdgt = self.update_bdgt(adj, pckUsrs)\n",
    "        if pckItms is None:\n",
    "            pckItms, _ = self.sample(itmBdgt, itmMask, len(pckUsrs))\n",
    "            itmMask = itmMask * self.make_mask(pckItms, adj.shape[1])\n",
    "        usrBdgt = self.update_bdgt(tpadj, pckItms)\n",
    "        uSampRes = 0\n",
    "        iSampRes = 0\n",
    "        for i in range(sampDepth + 1):\n",
    "            uSamp = uSampRes + (sampNum if i < sampDepth else 0)\n",
    "            iSamp = iSampRes + (sampNum if i < sampDepth else 0)\n",
    "            newUsrs, uSampRes = self.sample(usrBdgt, usrMask, uSamp)\n",
    "            usrMask = usrMask * self.make_mask(newUsrs, adj.shape[0])\n",
    "            newItms, iSampRes = self.sample(itmBdgt, itmMask, iSamp)\n",
    "            itmMask = itmMask * self.make_mask(newItms, adj.shape[1])\n",
    "            if i == sampDepth or i == sampDepth and uSampRes == 0 and iSampRes == 0:\n",
    "                break\n",
    "            usrBdgt += self.update_bdgt(tpadj, newItms)\n",
    "            itmBdgt += self.update_bdgt(adj, newUsrs)\n",
    "        usrs = np.reshape(np.argwhere(usrMask==0), [-1])\n",
    "        itms = np.reshape(np.argwhere(itmMask==0), [-1])\n",
    "        return self.construct_data(usrs, itms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "ds = TmallDataset_v2(data_dir='/content/data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<147894x99037 sparse matrix of type '<class 'numpy.float32'>'\n",
       "\twith 642916 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds.trnMats[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Author: Sparsh A.\n",
      "\n",
      "Last updated: 2021-12-30 05:00:21\n",
      "\n",
      "recohut: 0.0.8\n",
      "\n",
      "Compiler    : GCC 7.5.0\n",
      "OS          : Linux\n",
      "Release     : 5.4.144+\n",
      "Machine     : x86_64\n",
      "Processor   : x86_64\n",
      "CPU cores   : 2\n",
      "Architecture: 64bit\n",
      "\n",
      "recohut: 0.0.8\n",
      "IPython: 5.5.0\n",
      "numpy  : 1.19.5\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "%reload_ext watermark\n",
    "%watermark -a \"Sparsh A.\" -m -iv -u -t -d -p recohut"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
