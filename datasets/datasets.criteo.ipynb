{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp datasets.criteo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Criteo\n",
    "> Criteo dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev.showdoc import *\n",
    "from fastcore.nb_imports import *\n",
    "from fastcore.test import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Criteo dataset and datamodule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from recohut.datasets.bases.ctr import *\n",
    "from recohut.utils.common_utils import download_url\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from datetime import datetime, date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class CriteoDataset(CTRDataset):\n",
    "\n",
    "    feature_cols = [{'name': ['I1','I2','I3','I4','I5','I6','I7','I8','I9','I10','I11','I12','I13'], \n",
    "                     'active': True, 'dtype': float, 'type': 'categorical', 'preprocess': 'convert_to_bucket', 'na_value': 0},\n",
    "                    {'name': ['C1','C2','C3','C4','C5','C6','C7','C8','C9','C10','C11','C12','C13','C14','C15','C16','C17',\n",
    "                              'C18','C19','C20','C21','C22','C23','C24','C25','C26'],\n",
    "                     'active': True, 'dtype': str, 'type': 'categorical', 'na_value': \"\"}]\n",
    "                        \n",
    "    label_col = {'name': 'Label', 'dtype': float}\n",
    "\n",
    "    train_url = \"https://github.com/RecoHut-Datasets/criteo/raw/v2/train.csv\"\n",
    "    valid_url = \"https://github.com/RecoHut-Datasets/criteo/raw/v2/valid.csv\"\n",
    "    test_url = \"https://github.com/RecoHut-Datasets/criteo/raw/v2/test.csv\"\n",
    "\n",
    "    @property\n",
    "    def raw_file_names(self):\n",
    "        return ['train.csv',\n",
    "                'valid.csv',\n",
    "                'test.csv']\n",
    "\n",
    "    def download(self):\n",
    "        download_url(self.train_url, self.raw_dir)\n",
    "        download_url(self.valid_url, self.raw_dir)\n",
    "        download_url(self.test_url, self.raw_dir)\n",
    "\n",
    "    def convert_to_bucket(self, df, col_name):\n",
    "        def _convert_to_bucket(value):\n",
    "            if value > 2:\n",
    "                value = int(np.floor(np.log(value) ** 2))\n",
    "            else:\n",
    "                value = int(value)\n",
    "            return value\n",
    "        return df[col_name].map(_convert_to_bucket).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class CriteoDataModule(CTRDataModule):\n",
    "    dataset_cls = CriteoDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'model_id': 'DCN_demo',\n",
    "              'data_dir': '/content/data',\n",
    "              'model_root': './checkpoints/',\n",
    "              'dnn_hidden_units': [64, 64],\n",
    "              'dnn_activations': \"relu\",\n",
    "              'crossing_layers': 3,\n",
    "              'learning_rate': 1e-3,\n",
    "              'net_dropout': 0,\n",
    "              'batch_norm': False,\n",
    "              'optimizer': 'adamw',\n",
    "              'task': 'binary_classification',\n",
    "              'loss': 'binary_crossentropy',\n",
    "              'metrics': ['logloss', 'AUC'],\n",
    "              'embedding_dim': 10,\n",
    "              'batch_size': 64,\n",
    "              'epochs': 3,\n",
    "              'shuffle': True,\n",
    "              'seed': 2019,\n",
    "              'use_hdf5': True,\n",
    "              'workers': 1,\n",
    "              'verbose': 0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/pytorch_lightning/core/datamodule.py:74: LightningDeprecationWarning: DataModule property `train_transforms` was deprecated in v1.5 and will be removed in v1.7.\n",
      "  \"DataModule property `train_transforms` was deprecated in v1.5 and will be removed in v1.7.\"\n",
      "Downloading https://github.com/RecoHut-Datasets/criteo/raw/v2/train.csv\n",
      "Downloading https://github.com/RecoHut-Datasets/criteo/raw/v2/valid.csv\n",
      "Downloading https://github.com/RecoHut-Datasets/criteo/raw/v2/test.csv\n",
      "Processing...\n",
      "Done!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[  4.,   1.,   2.,  ...,   2.,   0.,   0.],\n",
      "        [  0.,  16.,   0.,  ...,  52.,   0.,   0.],\n",
      "        [  0.,   2.,   0.,  ...,   5.,   0.,   0.],\n",
      "        ...,\n",
      "        [  0.,  22.,   4.,  ...,   1.,   0.,   0.],\n",
      "        [  0.,   6.,   1.,  ...,   3.,   1., 202.],\n",
      "        [  0.,   5.,   2.,  ...,  12.,   2.,   6.]], dtype=torch.float64), tensor([1., 1., 0., 0., 1., 1., 0., 1., 0., 0., 0., 1., 1., 1., 1., 1., 1., 0.,\n",
      "        1., 1., 0., 1., 0., 0., 0., 1., 0., 1., 0., 1., 1., 0., 1., 1., 0., 1.,\n",
      "        0., 0., 0., 0., 1., 1., 0., 0., 0., 1., 0., 1., 1., 1., 0., 0., 1., 1.,\n",
      "        1., 0., 0., 0., 0., 1., 0., 1., 1., 1.], dtype=torch.float64)]\n"
     ]
    }
   ],
   "source": [
    "!rm -r /content/data\n",
    "ds = CriteoDataModule(**params)\n",
    "ds.prepare_data()\n",
    "ds.setup()\n",
    "\n",
    "for batch in ds.train_dataloader():\n",
    "    print(batch)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from recohut.models.deepcrossing import DeepCrossing\n",
    "\n",
    "params = {'model_id': 'DeepCrossing',\n",
    "              'data_dir': '/content/data',\n",
    "              'model_root': './checkpoints/',\n",
    "              'dnn_hidden_units': [64, 64],\n",
    "              'dnn_activations': \"relu\",\n",
    "              'learning_rate': 1e-3,\n",
    "              'net_dropout': 0,\n",
    "              'batch_norm': False,\n",
    "              'optimizer': 'adamw',\n",
    "              'use_residual': True,\n",
    "              'residual_blocks': [500, 500, 500],\n",
    "              'task': 'binary_classification',\n",
    "              'loss': 'binary_crossentropy',\n",
    "              'metrics': ['logloss', 'AUC'],\n",
    "              'embedding_dim': 10,\n",
    "              'batch_size': 64,\n",
    "              'epochs': 3,\n",
    "              'shuffle': True,\n",
    "              'seed': 2019,\n",
    "              'use_hdf5': True,\n",
    "              'workers': 1,\n",
    "              'verbose': 0}\n",
    "\n",
    "model = DeepCrossing(ds.dataset.feature_map, **params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "/usr/local/lib/python3.7/dist-packages/pytorch_lightning/core/datamodule.py:470: LightningDeprecationWarning: DataModule.setup has already been called, so it will not be called again. In v1.6 this behavior will change to always call DataModule.setup.\n",
      "  f\"DataModule.{name} has already been called, so it will not be called again. \"\n",
      "\n",
      "  | Name              | Type           | Params\n",
      "-----------------------------------------------------\n",
      "0 | embedding_layer   | EmbeddingLayer | 136 K \n",
      "1 | crossing_layer    | Sequential     | 1.2 M \n",
      "2 | output_activation | Sigmoid        | 0     \n",
      "-----------------------------------------------------\n",
      "1.3 M     Trainable params\n",
      "0         Non-trainable params\n",
      "1.3 M     Total params\n",
      "5.238     Total estimated model params size (MB)\n",
      "/usr/local/lib/python3.7/dist-packages/pytorch_lightning/callbacks/model_checkpoint.py:631: UserWarning: Checkpoint directory /content exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/data_loading.py:429: UserWarning: The number of training samples (32) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  f\"The number of training samples ({self.num_training_batches}) is smaller than the logging interval\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "887b754ea4d3466b98930e5d49d91b65",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08309c38d71e42f59fed7ff97ed6dbba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Metrics] logloss: 1.923407 - AUC: 0.512695\n",
      "[Metrics] logloss: 2.335812 - AUC: 0.374510\n",
      "[Metrics] logloss: 1.951746 - AUC: 0.510511\n",
      "[Metrics] logloss: 1.656238 - AUC: 0.495951\n",
      "[Metrics] logloss: 1.599615 - AUC: 0.457490\n",
      "[Metrics] logloss: 2.074813 - AUC: 0.432063\n",
      "[Metrics] logloss: 1.315625 - AUC: 0.600586\n",
      "[Metrics] logloss: 1.596338 - AUC: 0.548039\n",
      "[Metrics] logloss: 1.768354 - AUC: 0.580296\n",
      "[Metrics] logloss: 2.074194 - AUC: 0.433040\n",
      "[Metrics] logloss: 1.842569 - AUC: 0.550342\n",
      "[Metrics] logloss: 2.395853 - AUC: 0.432617\n",
      "[Metrics] logloss: 1.774030 - AUC: 0.462564\n",
      "[Metrics] logloss: 1.734166 - AUC: 0.557185\n",
      "[Metrics] logloss: 1.963507 - AUC: 0.422287\n",
      "[Metrics] logloss: 1.737491 - AUC: 0.504433\n",
      "[Metrics] logloss: 1.519305 - AUC: 0.607843\n",
      "[Metrics] logloss: 1.669249 - AUC: 0.491903\n",
      "[Metrics] logloss: 1.663352 - AUC: 0.552941\n",
      "[Metrics] logloss: 1.668330 - AUC: 0.539216\n",
      "[Metrics] logloss: 2.353048 - AUC: 0.385142\n",
      "[Metrics] logloss: 1.886832 - AUC: 0.502930\n",
      "[Metrics] logloss: 1.787228 - AUC: 0.501538\n",
      "[Metrics] logloss: 1.813736 - AUC: 0.549754\n",
      "[Metrics] logloss: 1.948717 - AUC: 0.507843\n",
      "[Metrics] logloss: 1.364824 - AUC: 0.584008\n",
      "[Metrics] logloss: 2.089619 - AUC: 0.460784\n",
      "[Metrics] logloss: 2.286810 - AUC: 0.390693\n",
      "[Metrics] logloss: 1.533276 - AUC: 0.622549\n",
      "[Metrics] logloss: 1.852024 - AUC: 0.415385\n",
      "[Metrics] logloss: 1.757749 - AUC: 0.507843\n",
      "[Metrics] logloss: 1.660756 - AUC: 0.620000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/pytorch_lightning/core/datamodule.py:470: LightningDeprecationWarning: DataModule.setup has already been called, so it will not be called again. In v1.6 this behavior will change to always call DataModule.setup.\n",
      "  f\"DataModule.{name} has already been called, so it will not be called again. \"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ff425dcf4e24a44a692887c6485b8fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Metrics] logloss: 1.830594 - AUC: 0.558162\n",
      "[Metrics] logloss: 2.161489 - AUC: 0.407738\n",
      "[Metrics] logloss: 1.679642 - AUC: 0.486275\n",
      "[Metrics] logloss: 1.562141 - AUC: 0.551320\n",
      "[Metrics] logloss: 1.594444 - AUC: 0.567460\n",
      "[Metrics] logloss: 1.626753 - AUC: 0.592118\n",
      "[Metrics] logloss: 1.529520 - AUC: 0.557635\n",
      "[Metrics] logloss: 1.329303 - AUC: 0.599600\n",
      "[Metrics] logloss: 2.305318 - AUC: 0.466010\n",
      "[Metrics] logloss: 1.942118 - AUC: 0.477539\n",
      "[Metrics] logloss: 2.328542 - AUC: 0.436453\n",
      "[Metrics] logloss: 2.221586 - AUC: 0.467647\n",
      "[Metrics] logloss: 1.445153 - AUC: 0.582353\n",
      "[Metrics] logloss: 1.402739 - AUC: 0.618768\n",
      "[Metrics] logloss: 2.210657 - AUC: 0.413086\n",
      "[Metrics] logloss: 1.112972 - AUC: 0.677734\n",
      "[Metrics] logloss: 1.882559 - AUC: 0.485826\n",
      "[Metrics] logloss: 1.917402 - AUC: 0.472656\n",
      "[Metrics] logloss: 2.366392 - AUC: 0.401760\n",
      "[Metrics] logloss: 1.604059 - AUC: 0.507843\n",
      "[Metrics] logloss: 2.733787 - AUC: 0.312831\n",
      "[Metrics] logloss: 1.982845 - AUC: 0.459113\n",
      "[Metrics] logloss: 2.498603 - AUC: 0.507602\n",
      "[Metrics] logloss: 1.936384 - AUC: 0.543500\n",
      "[Metrics] logloss: 1.396205 - AUC: 0.631476\n",
      "[Metrics] logloss: 1.669453 - AUC: 0.540887\n",
      "[Metrics] logloss: 1.669070 - AUC: 0.610390\n",
      "[Metrics] logloss: 2.800417 - AUC: 0.288542\n",
      "[Metrics] logloss: 1.862424 - AUC: 0.484375\n",
      "[Metrics] logloss: 1.917967 - AUC: 0.483135\n",
      "[Metrics] logloss: 1.751082 - AUC: 0.488866\n",
      "[Metrics] logloss: 3.960979 - AUC: 0.296296\n",
      "--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{'Test Metrics': {'AUC': tensor(0.5042), 'logloss': tensor(1.8953)}}\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'Test Metrics': {'AUC': tensor(0.5042), 'logloss': tensor(1.8953)}}]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from recohut.trainers.pl_trainer import pl_trainer\n",
    "\n",
    "pl_trainer(model, ds, max_epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Criteo sample dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import math\n",
    "import shutil\n",
    "import struct\n",
    "from collections import defaultdict\n",
    "from functools import lru_cache\n",
    "from pathlib import Path\n",
    "\n",
    "import lmdb\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.utils.data\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, TensorDataset\n",
    "\n",
    "from recohut.datasets.bases.common import Dataset\n",
    "from recohut.utils.common_utils import download_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class CriteoSampleDataset(Dataset):\n",
    "    \"\"\"Criteo Sample Dataset\n",
    "\n",
    "    Reference:\n",
    "        1. https://github.com/huangjunheng/recommendation_model/blob/master/DCN/dcn.py\n",
    "    \"\"\"\n",
    "    url = 'https://github.com/RecoHut-Datasets/criteo/raw/v1/dac_sample.txt'\n",
    "\n",
    "    def __init__(self, root, test_size=0.2, random_seed=42):\n",
    "        super().__init__(root)\n",
    "        self.test_size = test_size\n",
    "        self.random_seed = random_seed\n",
    "\n",
    "        self._process()\n",
    "\n",
    "    @property\n",
    "    def raw_file_names(self) -> str:\n",
    "        return 'dac_sample.txt'\n",
    "\n",
    "    @property\n",
    "    def processed_file_names(self) -> str:\n",
    "        return ['train.pt', 'test.pt']\n",
    "\n",
    "    def download(self):\n",
    "        path = download_url(self.url, self.raw_dir)\n",
    "\n",
    "    def process(self):\n",
    "        sparse_feature = ['C' + str(i) for i in range(1, 27)]\n",
    "        dense_feature = ['I' + str(i) for i in range(1, 14)]\n",
    "        col_names = ['label'] + dense_feature + sparse_feature\n",
    "        data = pd.read_csv(self.raw_paths[0], names=col_names, sep='\\t')\n",
    "\n",
    "        data[sparse_feature] = data[sparse_feature].fillna('-1', )\n",
    "        data[dense_feature] = data[dense_feature].fillna('0',)\n",
    "\n",
    "        feat_sizes = {}\n",
    "        feat_sizes_dense = {feat:1 for feat in dense_feature}\n",
    "        feat_sizes_sparse = {feat:len(data[feat].unique()) for feat in sparse_feature}\n",
    "        feat_sizes.update(feat_sizes_dense)\n",
    "        feat_sizes.update(feat_sizes_sparse)\n",
    "        self.feat_sizes = feat_sizes\n",
    "\n",
    "        from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
    "        for feat in sparse_feature:\n",
    "            lbe = LabelEncoder()\n",
    "            data[feat] = lbe.fit_transform(data[feat])\n",
    "        nms = MinMaxScaler(feature_range=(0, 1))\n",
    "        data[dense_feature] = nms.fit_transform(data[dense_feature])\n",
    "\n",
    "        fixlen_feature_columns = [(feat,'sparse') for feat in sparse_feature ]  + [(feat,'dense') for feat in dense_feature]\n",
    "        self.dnn_feature_columns = fixlen_feature_columns\n",
    "        self.linear_feature_columns = fixlen_feature_columns\n",
    "\n",
    "        from sklearn.model_selection import train_test_split\n",
    "        train, test = train_test_split(data, test_size=self.test_size, random_state=self.random_seed)\n",
    "\n",
    "        train_label = pd.DataFrame(train['label'])\n",
    "        train = train.drop(columns=['label'])\n",
    "        train_tensor_data = TensorDataset(torch.from_numpy(np.array(train)), torch.from_numpy(np.array(train_label)))\n",
    "        torch.save(train_tensor_data, self.processed_paths[0])\n",
    "\n",
    "        test_label = pd.DataFrame(test['label'])\n",
    "        test = test.drop(columns=['label'])\n",
    "        test_tensor_data = TensorDataset(torch.from_numpy(np.array(test)), torch.from_numpy(np.array(test_label)))\n",
    "        torch.save(test_tensor_data, self.processed_paths[1])\n",
    "\n",
    "    def load(self):\n",
    "        train_tensor_data = torch.load(self.processed_paths[0])\n",
    "        test_tensor_data = torch.load(self.processed_paths[1])\n",
    "        return train_tensor_data, test_tensor_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing...\n",
      "Done!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch/epoches: 0/20, train loss: 0.536, test auc: 0.679\n",
      "epoch/epoches: 1/20, train loss: 0.499, test auc: 0.702\n",
      "epoch/epoches: 2/20, train loss: 0.486, test auc: 0.722\n",
      "epoch/epoches: 3/20, train loss: 0.477, test auc: 0.740\n",
      "epoch/epoches: 4/20, train loss: 0.468, test auc: 0.745\n",
      "epoch/epoches: 5/20, train loss: 0.463, test auc: 0.752\n",
      "epoch/epoches: 6/20, train loss: 0.460, test auc: 0.757\n",
      "epoch/epoches: 7/20, train loss: 0.458, test auc: 0.753\n",
      "epoch/epoches: 8/20, train loss: 0.455, test auc: 0.758\n",
      "epoch/epoches: 9/20, train loss: 0.452, test auc: 0.759\n",
      "epoch/epoches: 10/20, train loss: 0.450, test auc: 0.757\n",
      "epoch/epoches: 11/20, train loss: 0.449, test auc: 0.758\n",
      "epoch/epoches: 12/20, train loss: 0.444, test auc: 0.758\n",
      "epoch/epoches: 13/20, train loss: 0.438, test auc: 0.757\n",
      "epoch/epoches: 14/20, train loss: 0.428, test auc: 0.753\n",
      "epoch/epoches: 15/20, train loss: 0.413, test auc: 0.750\n",
      "epoch/epoches: 16/20, train loss: 0.395, test auc: 0.744\n",
      "epoch/epoches: 17/20, train loss: 0.381, test auc: 0.735\n",
      "epoch/epoches: 18/20, train loss: 0.365, test auc: 0.737\n",
      "epoch/epoches: 19/20, train loss: 0.354, test auc: 0.735\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import log_loss, roc_auc_score\n",
    "\n",
    "from recohut.models.dcn import DCNv2 as DCN\n",
    "\n",
    "\n",
    "def get_auc(loader, model):\n",
    "    pred, target = [], []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            x, y = x.to(device).float(), y.to(device).float()\n",
    "            y_hat = model(x)\n",
    "            pred += list(y_hat.cpu().numpy())\n",
    "            target += list(y.cpu().numpy())\n",
    "    auc = roc_auc_score(target, pred)\n",
    "    return auc\n",
    "\n",
    "\n",
    "root = '/content/data'\n",
    "batch_size = 1024\n",
    "lr = 1e-2\n",
    "wd = 1e-3\n",
    "epoches = 20\n",
    "seed = 2022\n",
    "embedding_size = 4\n",
    "device = 'cpu'\n",
    "\n",
    "ds = CriteoSampleDataset(root=root)\n",
    "train_tensor_data, test_tensor_data = ds.load()\n",
    "train_loader = DataLoader(train_tensor_data, shuffle=True, batch_size=batch_size)\n",
    "test_loader = DataLoader(test_tensor_data, batch_size=batch_size)\n",
    "\n",
    "model = DCN(ds.feat_sizes, embedding_size, ds.linear_feature_columns, ds.dnn_feature_columns).to(device)\n",
    "loss_func = nn.BCELoss(reduction='mean')\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=wd)\n",
    "\n",
    "for epoch in range(epoches):\n",
    "    total_loss_epoch = 0.0\n",
    "    total_tmp = 0\n",
    "    model.train()\n",
    "    for index, (x, y) in enumerate(train_loader):\n",
    "        x, y = x.to(device).float(), y.to(device).float()\n",
    "        y_hat = model(x)\n",
    "        optimizer.zero_grad()\n",
    "        loss = loss_func(y_hat, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss_epoch += loss.item()\n",
    "        total_tmp += 1\n",
    "    auc = get_auc(test_loader, model)\n",
    "    print('epoch/epoches: {}/{}, train loss: {:.3f}, test auc: {:.3f}'.format(epoch, epoches, total_loss_epoch / total_tmp, auc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Criteo dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #export\n",
    "# class CriteoDataset(torch.utils.data.Dataset):\n",
    "#     \"\"\"\n",
    "#     Criteo Display Advertising Challenge Dataset\n",
    "#     Data prepration:\n",
    "#         * Remove the infrequent features (appearing in less than threshold instances) and treat them as a single feature\n",
    "#         * Discretize numerical values by log2 transformation which is proposed by the winner of Criteo Competition\n",
    "#     :param dataset_path: criteo train.txt path.\n",
    "#     :param cache_path: lmdb cache path.\n",
    "#     :param rebuild_cache: If True, lmdb cache is refreshed.\n",
    "#     :param min_threshold: infrequent feature threshold.\n",
    "#     Reference:\n",
    "#         https://labs.criteo.com/2014/02/kaggle-display-advertising-challenge-dataset\n",
    "#         https://www.csie.ntu.edu.tw/~r01922136/kaggle-2014-criteo.pdf\n",
    "#     \"\"\"\n",
    "\n",
    "#     def __init__(self, dataset_path=None, cache_path='.criteo', rebuild_cache=False, min_threshold=10):\n",
    "#         self.NUM_FEATS = 39\n",
    "#         self.NUM_INT_FEATS = 13\n",
    "#         self.min_threshold = min_threshold\n",
    "#         if rebuild_cache or not Path(cache_path).exists():\n",
    "#             shutil.rmtree(cache_path, ignore_errors=True)\n",
    "#             if dataset_path is None:\n",
    "#                 raise ValueError('create cache: failed: dataset_path is None')\n",
    "#             self.__build_cache(dataset_path, cache_path)\n",
    "#         self.env = lmdb.open(cache_path, create=False, lock=False, readonly=True)\n",
    "#         with self.env.begin(write=False) as txn:\n",
    "#             self.length = txn.stat()['entries'] - 1\n",
    "#             self.field_dims = np.frombuffer(txn.get(b'field_dims'), dtype=np.uint32)\n",
    "\n",
    "#     def __getitem__(self, index):\n",
    "#         with self.env.begin(write=False) as txn:\n",
    "#             np_array = np.frombuffer(\n",
    "#                 txn.get(struct.pack('>I', index)), dtype=np.uint32).astype(dtype=np.long)\n",
    "#         return np_array[1:], np_array[0]\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return self.length\n",
    "\n",
    "#     def __build_cache(self, path, cache_path):\n",
    "#         feat_mapper, defaults = self.__get_feat_mapper(path)\n",
    "#         with lmdb.open(cache_path, map_size=int(1e11)) as env:\n",
    "#             field_dims = np.zeros(self.NUM_FEATS, dtype=np.uint32)\n",
    "#             for i, fm in feat_mapper.items():\n",
    "#                 field_dims[i - 1] = len(fm) + 1\n",
    "#             with env.begin(write=True) as txn:\n",
    "#                 txn.put(b'field_dims', field_dims.tobytes())\n",
    "#             for buffer in self.__yield_buffer(path, feat_mapper, defaults):\n",
    "#                 with env.begin(write=True) as txn:\n",
    "#                     for key, value in buffer:\n",
    "#                         txn.put(key, value)\n",
    "\n",
    "#     def __get_feat_mapper(self, path):\n",
    "#         feat_cnts = defaultdict(lambda: defaultdict(int))\n",
    "#         with open(path) as f:\n",
    "#             pbar = tqdm(f, mininterval=1, smoothing=0.1)\n",
    "#             pbar.set_description('Create criteo dataset cache: counting features')\n",
    "#             for line in pbar:\n",
    "#                 values = line.rstrip('\\n').split('\\t')\n",
    "#                 if len(values) != self.NUM_FEATS + 1:\n",
    "#                     continue\n",
    "#                 for i in range(1, self.NUM_INT_FEATS + 1):\n",
    "#                     feat_cnts[i][convert_numeric_feature(values[i])] += 1\n",
    "#                 for i in range(self.NUM_INT_FEATS + 1, self.NUM_FEATS + 1):\n",
    "#                     feat_cnts[i][values[i]] += 1\n",
    "#         feat_mapper = {i: {feat for feat, c in cnt.items() if c >= self.min_threshold} for i, cnt in feat_cnts.items()}\n",
    "#         feat_mapper = {i: {feat: idx for idx, feat in enumerate(cnt)} for i, cnt in feat_mapper.items()}\n",
    "#         defaults = {i: len(cnt) for i, cnt in feat_mapper.items()}\n",
    "#         return feat_mapper, defaults\n",
    "\n",
    "#     def __yield_buffer(self, path, feat_mapper, defaults, buffer_size=int(1e5)):\n",
    "#         item_idx = 0\n",
    "#         buffer = list()\n",
    "#         with open(path) as f:\n",
    "#             pbar = tqdm(f, mininterval=1, smoothing=0.1)\n",
    "#             pbar.set_description('Create criteo dataset cache: setup lmdb')\n",
    "#             for line in pbar:\n",
    "#                 values = line.rstrip('\\n').split('\\t')\n",
    "#                 if len(values) != self.NUM_FEATS + 1:\n",
    "#                     continue\n",
    "#                 np_array = np.zeros(self.NUM_FEATS + 1, dtype=np.uint32)\n",
    "#                 np_array[0] = int(values[0])\n",
    "#                 for i in range(1, self.NUM_INT_FEATS + 1):\n",
    "#                     np_array[i] = feat_mapper[i].get(convert_numeric_feature(values[i]), defaults[i])\n",
    "#                 for i in range(self.NUM_INT_FEATS + 1, self.NUM_FEATS + 1):\n",
    "#                     np_array[i] = feat_mapper[i].get(values[i], defaults[i])\n",
    "#                 buffer.append((struct.pack('>I', item_idx), np_array.tobytes()))\n",
    "#                 item_idx += 1\n",
    "#                 if item_idx % buffer_size == 0:\n",
    "#                     yield buffer\n",
    "#                     buffer.clear()\n",
    "#             yield buffer\n",
    "\n",
    "\n",
    "# @lru_cache(maxsize=None)\n",
    "# def convert_numeric_feature(val: str):\n",
    "#     if val == '':\n",
    "#         return 'NULL'\n",
    "#     v = int(val)\n",
    "#     if v > 2:\n",
    "#         return str(int(math.log(v) ** 2))\n",
    "#     else:\n",
    "#         return str(v - 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Criteo Dataset Transformation\n",
    "> Implementation of transformation functions specific to criteo ad-display dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder, KBinsDiscretizer\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def sparseFeature(feat, feat_num, embed_dim=4):\n",
    "    \"\"\"\n",
    "    create dictionary for sparse feature\n",
    "    :param feat: feature name\n",
    "    :param feat_num: the total number of sparse features that do not repeat\n",
    "    :param embed_dim: embedding dimension\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    return {'feat_name': feat, 'feat_num': feat_num, 'embed_dim': embed_dim}\n",
    "\n",
    "\n",
    "def denseFeature(feat):\n",
    "    \"\"\"\n",
    "    create dictionary for dense feature\n",
    "    :param feat: dense feature name\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    return {'feat_name': feat}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def create_criteo_dataset(file, embed_dim=8, read_part=True, sample_num=100000, test_size=0.2):\n",
    "    \"\"\"\n",
    "    a example about creating criteo dataset\n",
    "    :param file: dataset's path\n",
    "    :param embed_dim: the embedding dimension of sparse features\n",
    "    :param read_part: whether to read part of it\n",
    "    :param sample_num: the number of instances if read_part is True\n",
    "    :param test_size: ratio of test dataset\n",
    "    :return: feature columns, train, test\n",
    "    \"\"\"\n",
    "    names = ['label', 'I1', 'I2', 'I3', 'I4', 'I5', 'I6', 'I7', 'I8', 'I9', 'I10', 'I11',\n",
    "             'I12', 'I13', 'C1', 'C2', 'C3', 'C4', 'C5', 'C6', 'C7', 'C8', 'C9', 'C10', 'C11',\n",
    "             'C12', 'C13', 'C14', 'C15', 'C16', 'C17', 'C18', 'C19', 'C20', 'C21', 'C22',\n",
    "             'C23', 'C24', 'C25', 'C26']\n",
    "\n",
    "    if read_part:\n",
    "        data_df = pd.read_csv(file, sep='\\t', iterator=True, header=None,\n",
    "                          names=names)\n",
    "        data_df = data_df.get_chunk(sample_num)\n",
    "\n",
    "    else:\n",
    "        data_df = pd.read_csv(file, sep='\\t', header=None, names=names)\n",
    "\n",
    "    sparse_features = ['C' + str(i) for i in range(1, 27)]\n",
    "    dense_features = ['I' + str(i) for i in range(1, 14)]\n",
    "    features = sparse_features + dense_features\n",
    "\n",
    "    data_df[sparse_features] = data_df[sparse_features].fillna('-1')\n",
    "    data_df[dense_features] = data_df[dense_features].fillna(0)\n",
    "\n",
    "    # Bin continuous data into intervals.\n",
    "    est = KBinsDiscretizer(n_bins=100, encode='ordinal', strategy='uniform')\n",
    "    data_df[dense_features] = est.fit_transform(data_df[dense_features])\n",
    "\n",
    "    for feat in sparse_features:\n",
    "        le = LabelEncoder()\n",
    "        data_df[feat] = le.fit_transform(data_df[feat])\n",
    "\n",
    "    # ==============Feature Engineering===================\n",
    "\n",
    "    # ====================================================\n",
    "    feature_columns = [sparseFeature(feat, int(data_df[feat].max()) + 1, embed_dim=embed_dim)\n",
    "                        for feat in features]\n",
    "    train, test = train_test_split(data_df, test_size=test_size)\n",
    "\n",
    "    train_X = train[features].values.astype('int32')\n",
    "    train_y = train['label'].values.astype('int32')\n",
    "    test_X = test[features].values.astype('int32')\n",
    "    test_y = test['label'].values.astype('int32')\n",
    "\n",
    "    return feature_columns, (train_X, train_y), (test_X, test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting kaggle\n",
      "  Downloading kaggle-1.5.12.tar.gz (58 kB)\n",
      "\u001b[?25l\r\u001b[K     |█████▋                          | 10 kB 19.0 MB/s eta 0:00:01\r\u001b[K     |███████████▏                    | 20 kB 20.9 MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 30 kB 14.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▎         | 40 kB 10.5 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▉    | 51 kB 4.5 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 58 kB 2.7 MB/s \n",
      "\u001b[?25hBuilding wheels for collected packages: kaggle\n",
      "  Building wheel for kaggle (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for kaggle: filename=kaggle-1.5.12-py3-none-any.whl size=73051 sha256=a09d2576937c68b6341e6bce9eeefa020563e125d97e69548f4d591568008b5f\n",
      "  Stored in directory: /root/.cache/pip/wheels/62/d6/58/5853130f941e75b2177d281eb7e44b4a98ed46dd155f556dc5\n",
      "Successfully built kaggle\n",
      "Installing collected packages: kaggle\n",
      "  Attempting uninstall: kaggle\n",
      "    Found existing installation: kaggle 1.5.12\n",
      "    Uninstalling kaggle-1.5.12:\n",
      "      Successfully uninstalled kaggle-1.5.12\n",
      "Successfully installed kaggle-1.5.12\n",
      "Downloading criteo-dataset.zip to /content\n",
      "100% 4.31G/4.31G [01:20<00:00, 58.4MB/s]\n",
      "100% 4.31G/4.31G [01:20<00:00, 57.4MB/s]\n",
      "Archive:  criteo-dataset.zip\n",
      "  inflating: dac/readme.txt          \n",
      "  inflating: dac/test.txt            \n",
      "  inflating: dac/train.txt           \n"
     ]
    }
   ],
   "source": [
    "# !pip install -q -U kaggle\n",
    "# !pip install --upgrade --force-reinstall --no-deps kaggle\n",
    "# !mkdir ~/.kaggle\n",
    "# !cp /content/drive/MyDrive/kaggle.json ~/.kaggle/\n",
    "# !chmod 600 ~/.kaggle/kaggle.json\n",
    "# !kaggle datasets download -d mrkmakr/criteo-dataset\n",
    "# !unzip criteo-dataset.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = 'dac/train.txt'\n",
    "read_part = True\n",
    "sample_num = 10000\n",
    "test_size = 0.2\n",
    "\n",
    "feature_columns, train, test = create_criteo_dataset(file=file,\n",
    "                                        read_part=read_part,\n",
    "                                        sample_num=sample_num,\n",
    "                                        test_size=test_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'embed_dim': 8, 'feat_name': 'C1', 'feat_num': 175},\n",
       " {'embed_dim': 8, 'feat_name': 'C2', 'feat_num': 386},\n",
       " {'embed_dim': 8, 'feat_name': 'C3', 'feat_num': 5521},\n",
       " {'embed_dim': 8, 'feat_name': 'C4', 'feat_num': 4033},\n",
       " {'embed_dim': 8, 'feat_name': 'C5', 'feat_num': 56},\n",
       " {'embed_dim': 8, 'feat_name': 'C6', 'feat_num': 8},\n",
       " {'embed_dim': 8, 'feat_name': 'C7', 'feat_num': 3184},\n",
       " {'embed_dim': 8, 'feat_name': 'C8', 'feat_num': 93},\n",
       " {'embed_dim': 8, 'feat_name': 'C9', 'feat_num': 3},\n",
       " {'embed_dim': 8, 'feat_name': 'C10', 'feat_num': 2986},\n",
       " {'embed_dim': 8, 'feat_name': 'C11', 'feat_num': 2084},\n",
       " {'embed_dim': 8, 'feat_name': 'C12', 'feat_num': 5284},\n",
       " {'embed_dim': 8, 'feat_name': 'C13', 'feat_num': 1725},\n",
       " {'embed_dim': 8, 'feat_name': 'C14', 'feat_num': 24},\n",
       " {'embed_dim': 8, 'feat_name': 'C15', 'feat_num': 2035},\n",
       " {'embed_dim': 8, 'feat_name': 'C16', 'feat_num': 4724},\n",
       " {'embed_dim': 8, 'feat_name': 'C17', 'feat_num': 9},\n",
       " {'embed_dim': 8, 'feat_name': 'C18', 'feat_num': 1149},\n",
       " {'embed_dim': 8, 'feat_name': 'C19', 'feat_num': 547},\n",
       " {'embed_dim': 8, 'feat_name': 'C20', 'feat_num': 4},\n",
       " {'embed_dim': 8, 'feat_name': 'C21', 'feat_num': 5037},\n",
       " {'embed_dim': 8, 'feat_name': 'C22', 'feat_num': 8},\n",
       " {'embed_dim': 8, 'feat_name': 'C23', 'feat_num': 12},\n",
       " {'embed_dim': 8, 'feat_name': 'C24', 'feat_num': 2525},\n",
       " {'embed_dim': 8, 'feat_name': 'C25', 'feat_num': 40},\n",
       " {'embed_dim': 8, 'feat_name': 'C26', 'feat_num': 1939},\n",
       " {'embed_dim': 8, 'feat_name': 'I1', 'feat_num': 100},\n",
       " {'embed_dim': 8, 'feat_name': 'I2', 'feat_num': 100},\n",
       " {'embed_dim': 8, 'feat_name': 'I3', 'feat_num': 100},\n",
       " {'embed_dim': 8, 'feat_name': 'I4', 'feat_num': 100},\n",
       " {'embed_dim': 8, 'feat_name': 'I5', 'feat_num': 100},\n",
       " {'embed_dim': 8, 'feat_name': 'I6', 'feat_num': 100},\n",
       " {'embed_dim': 8, 'feat_name': 'I7', 'feat_num': 100},\n",
       " {'embed_dim': 8, 'feat_name': 'I8', 'feat_num': 100},\n",
       " {'embed_dim': 8, 'feat_name': 'I9', 'feat_num': 100},\n",
       " {'embed_dim': 8, 'feat_name': 'I10', 'feat_num': 100},\n",
       " {'embed_dim': 8, 'feat_name': 'I11', 'feat_num': 100},\n",
       " {'embed_dim': 8, 'feat_name': 'I12', 'feat_num': 100},\n",
       " {'embed_dim': 8, 'feat_name': 'I13', 'feat_num': 100}]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[   1,  293, 2491, ...,    0,    0,    1],\n",
       "        [   1,   88,    0, ...,    1,    0,    1],\n",
       "        [   1,   17, 5197, ...,    1,    0,    0],\n",
       "        ...,\n",
       "        [   1,  355, 4284, ...,    3,    0,    0],\n",
       "        [   1,  192,   56, ...,    1,    0,    0],\n",
       "        [  75,   18, 2613, ...,    3,    0,    0]], dtype=int32),\n",
       " array([0, 0, 0, ..., 0, 0, 0], dtype=int32))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[ 111,  105,  695, ...,    3,    0,    0],\n",
       "        [ 102,  337, 2613, ...,    0,    0,    1],\n",
       "        [  75,  301,  155, ...,    1,    0,    0],\n",
       "        ...,\n",
       "        [  75,   86,  507, ...,    1,    1,    1],\n",
       "        [   1,  347, 2205, ...,    2,    1,    1],\n",
       "        [ 102,  125,    5, ...,    1,    1,    0]], dtype=int32),\n",
       " array([1, 0, 1, ..., 0, 0, 0], dtype=int32))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **References:-**\n",
    "- https://github.com/rixwew/pytorch-fm/blob/master/torchfm/dataset/criteo.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Author: Sparsh A.\n",
      "\n",
      "Last updated: 2022-01-07 08:45:18\n",
      "\n",
      "recohut: 0.0.9\n",
      "\n",
      "Compiler    : GCC 7.5.0\n",
      "OS          : Linux\n",
      "Release     : 5.4.144+\n",
      "Machine     : x86_64\n",
      "Processor   : x86_64\n",
      "CPU cores   : 2\n",
      "Architecture: 64bit\n",
      "\n",
      "matplotlib: 3.2.2\n",
      "pandas    : 1.1.5\n",
      "lmdb      : 0.99\n",
      "PIL       : 7.1.2\n",
      "IPython   : 5.5.0\n",
      "numpy     : 1.19.5\n",
      "torch     : 1.10.0+cu111\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%reload_ext watermark\n",
    "%watermark -a \"Sparsh A.\" -m -iv -u -t -d -p recohut"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
