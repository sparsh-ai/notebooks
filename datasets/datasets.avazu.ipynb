{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp datasets.avazu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Avazu\n",
    "> Avazu dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev.showdoc import *\n",
    "from fastcore.nb_imports import *\n",
    "from fastcore.test import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from recohut.datasets.bases.ctr import *\n",
    "from recohut.utils.common_utils import download_url\n",
    "\n",
    "from datetime import date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class AvazuDataset(CTRDataset):\n",
    "\n",
    "    feature_cols = [{'name': 'id', 'active': False, 'dtype': 'str', 'type': 'categorical'},\n",
    "         {'name': 'hour', 'active': True, 'dtype': 'str', 'type': 'categorical', 'preprocess': 'convert_hour'},\n",
    "         {'name': ['C1','banner_pos','site_id','site_domain','site_category','app_id','app_domain','app_category','device_id',\n",
    "                   'device_ip','device_model','device_type','device_conn_type','C14','C15','C16','C17','C18','C19','C20','C21'], \n",
    "           'active': True, 'dtype': 'str', 'type': 'categorical'},\n",
    "         {'name': 'weekday', 'active': True, 'dtype': 'str', 'type': 'categorical', 'preprocess': 'convert_weekday'},\n",
    "         {'name': 'weekend', 'active': True, 'dtype': 'str', 'type': 'categorical', 'preprocess': 'convert_weekend'}]\n",
    "                        \n",
    "    label_col = {'name': 'click', 'dtype': float}\n",
    "\n",
    "    train_url = \"https://github.com/RecoHut-Datasets/avazu/raw/v1/train.csv\"\n",
    "    valid_url = \"https://github.com/RecoHut-Datasets/avazu/raw/v1/valid.csv\"\n",
    "    test_url = \"https://github.com/RecoHut-Datasets/avazu/raw/v1/test.csv\"\n",
    "\n",
    "    @property\n",
    "    def raw_file_names(self):\n",
    "        return ['train.csv',\n",
    "                'valid.csv',\n",
    "                'test.csv']\n",
    "\n",
    "    def download(self):\n",
    "        download_url(self.train_url, self.raw_dir)\n",
    "        download_url(self.valid_url, self.raw_dir)\n",
    "        download_url(self.test_url, self.raw_dir)\n",
    "\n",
    "    def convert_weekday(self, df, col_name):\n",
    "        def _convert_weekday(timestamp):\n",
    "            dt = date(int('20' + timestamp[0:2]), int(timestamp[2:4]), int(timestamp[4:6]))\n",
    "            return int(dt.strftime('%w'))\n",
    "        return df['hour'].apply(_convert_weekday)\n",
    "\n",
    "    def convert_weekend(self, df, col_name):\n",
    "        def _convert_weekend(timestamp):\n",
    "            dt = date(int('20' + timestamp[0:2]), int(timestamp[2:4]), int(timestamp[4:6]))\n",
    "            return 1 if dt.strftime('%w') in ['6', '0'] else 0\n",
    "        return df['hour'].apply(_convert_weekend)\n",
    "\n",
    "    def convert_hour(self, df, col_name):\n",
    "        return df['hour'].apply(lambda x: int(x[6:8]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class AvazuDataModule(CTRDataModule):\n",
    "    dataset_cls = AvazuDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'model_id': 'DCN_demo',\n",
    "              'data_dir': '/content/data',\n",
    "              'model_root': './checkpoints/',\n",
    "              'dnn_hidden_units': [64, 64],\n",
    "              'dnn_activations': \"relu\",\n",
    "              'crossing_layers': 3,\n",
    "              'learning_rate': 1e-3,\n",
    "              'net_dropout': 0,\n",
    "              'batch_norm': False,\n",
    "              'optimizer': 'adamw',\n",
    "              'task': 'binary_classification',\n",
    "              'loss': 'binary_crossentropy',\n",
    "              'metrics': ['logloss', 'AUC'],\n",
    "              'embedding_dim': 10,\n",
    "              'batch_size': 64,\n",
    "              'epochs': 3,\n",
    "              'shuffle': True,\n",
    "              'seed': 2019,\n",
    "              'use_hdf5': True,\n",
    "              'workers': 1,\n",
    "              'verbose': 0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/pytorch_lightning/core/datamodule.py:74: LightningDeprecationWarning: DataModule property `train_transforms` was deprecated in v1.5 and will be removed in v1.7.\n",
      "  \"DataModule property `train_transforms` was deprecated in v1.5 and will be removed in v1.7.\"\n",
      "Downloading https://github.com/RecoHut-Datasets/avazu/raw/v1/train.csv\n",
      "Downloading https://github.com/RecoHut-Datasets/avazu/raw/v1/valid.csv\n",
      "Downloading https://github.com/RecoHut-Datasets/avazu/raw/v1/test.csv\n",
      "Processing...\n",
      "Done!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[1., 1., 2.,  ..., 6., 1., 1.],\n",
      "        [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "        [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "        ...,\n",
      "        [1., 1., 2.,  ..., 2., 1., 1.],\n",
      "        [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "        [1., 1., 1.,  ..., 4., 1., 1.]], dtype=torch.float64), tensor([0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 1., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 0.], dtype=torch.float64)]\n"
     ]
    }
   ],
   "source": [
    "!rm -r /content/data\n",
    "ds = AvazuDataModule(**params)\n",
    "ds.prepare_data()\n",
    "ds.setup()\n",
    "\n",
    "for batch in ds.train_dataloader():\n",
    "    print(batch)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from recohut.models.deepcrossing import DeepCrossing\n",
    "\n",
    "params = {'model_id': 'DeepCrossing',\n",
    "              'data_dir': '/content/data',\n",
    "              'model_root': './checkpoints/',\n",
    "              'dnn_hidden_units': [64, 64],\n",
    "              'dnn_activations': \"relu\",\n",
    "              'learning_rate': 1e-3,\n",
    "              'net_dropout': 0,\n",
    "              'batch_norm': False,\n",
    "              'optimizer': 'adamw',\n",
    "              'use_residual': True,\n",
    "              'residual_blocks': [500, 500, 500],\n",
    "              'task': 'binary_classification',\n",
    "              'loss': 'binary_crossentropy',\n",
    "              'metrics': ['logloss', 'AUC'],\n",
    "              'embedding_dim': 10,\n",
    "              'batch_size': 64,\n",
    "              'epochs': 3,\n",
    "              'shuffle': True,\n",
    "              'seed': 2019,\n",
    "              'use_hdf5': True,\n",
    "              'workers': 1,\n",
    "              'verbose': 0}\n",
    "\n",
    "model = DeepCrossing(ds.dataset.feature_map, **params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "/usr/local/lib/python3.7/dist-packages/pytorch_lightning/core/datamodule.py:470: LightningDeprecationWarning: DataModule.setup has already been called, so it will not be called again. In v1.6 this behavior will change to always call DataModule.setup.\n",
      "  f\"DataModule.{name} has already been called, so it will not be called again. \"\n",
      "\n",
      "  | Name              | Type           | Params\n",
      "-----------------------------------------------------\n",
      "0 | embedding_layer   | EmbeddingLayer | 12.6 K\n",
      "1 | crossing_layer    | Sequential     | 722 K \n",
      "2 | output_activation | Sigmoid        | 0     \n",
      "-----------------------------------------------------\n",
      "735 K     Trainable params\n",
      "0         Non-trainable params\n",
      "735 K     Total params\n",
      "2.940     Total estimated model params size (MB)\n",
      "/usr/local/lib/python3.7/dist-packages/pytorch_lightning/callbacks/model_checkpoint.py:631: UserWarning: Checkpoint directory /content exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/data_loading.py:429: UserWarning: The number of training samples (47) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  f\"The number of training samples ({self.num_training_batches}) is smaller than the logging interval\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a251da889274457fa0e3625bf7cb16a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "393968cbaf7e404fa8f7aebb5219c6cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Metrics] logloss: 0.000000 - AUC: 1.000000\n",
      "[Metrics] logloss: 0.000000 - AUC: 1.000000\n",
      "[Metrics] logloss: 0.000000 - AUC: 1.000000\n",
      "[Metrics] logloss: 0.000000 - AUC: 1.000000\n",
      "[Metrics] logloss: 0.000000 - AUC: 1.000000\n",
      "[Metrics] logloss: 0.000000 - AUC: 1.000000\n",
      "[Metrics] logloss: 0.000000 - AUC: 1.000000\n",
      "[Metrics] logloss: 0.000000 - AUC: 1.000000\n",
      "[Metrics] logloss: 0.000000 - AUC: 1.000000\n",
      "[Metrics] logloss: 0.000000 - AUC: 1.000000\n",
      "[Metrics] logloss: 0.000000 - AUC: 1.000000\n",
      "[Metrics] logloss: 0.000000 - AUC: 1.000000\n",
      "[Metrics] logloss: 0.000000 - AUC: 1.000000\n",
      "[Metrics] logloss: 0.000000 - AUC: 1.000000\n",
      "[Metrics] logloss: 0.000000 - AUC: 1.000000\n",
      "[Metrics] logloss: 0.000000 - AUC: 1.000000\n",
      "[Metrics] logloss: 0.000000 - AUC: 1.000000\n",
      "[Metrics] logloss: 0.000000 - AUC: 1.000000\n",
      "[Metrics] logloss: 0.000000 - AUC: 1.000000\n",
      "[Metrics] logloss: 0.000000 - AUC: 1.000000\n",
      "[Metrics] logloss: 0.000000 - AUC: 1.000000\n",
      "[Metrics] logloss: 0.000000 - AUC: 1.000000\n",
      "[Metrics] logloss: 0.000000 - AUC: 1.000000\n",
      "[Metrics] logloss: 0.000000 - AUC: 1.000000\n",
      "[Metrics] logloss: 0.000000 - AUC: 1.000000\n",
      "[Metrics] logloss: 0.000000 - AUC: 1.000000\n",
      "[Metrics] logloss: 0.000000 - AUC: 1.000000\n",
      "[Metrics] logloss: 0.000000 - AUC: 1.000000\n",
      "[Metrics] logloss: 0.000000 - AUC: 1.000000\n",
      "[Metrics] logloss: 0.000000 - AUC: 1.000000\n",
      "[Metrics] logloss: 0.000000 - AUC: 1.000000\n",
      "[Metrics] logloss: 0.000000 - AUC: 1.000000\n",
      "[Metrics] logloss: 0.000000 - AUC: 1.000000\n",
      "[Metrics] logloss: 0.000000 - AUC: 1.000000\n",
      "[Metrics] logloss: 0.000000 - AUC: 1.000000\n",
      "[Metrics] logloss: 0.000000 - AUC: 1.000000\n",
      "[Metrics] logloss: 0.000000 - AUC: 1.000000\n",
      "[Metrics] logloss: 0.000000 - AUC: 1.000000\n",
      "[Metrics] logloss: 0.000000 - AUC: 1.000000\n",
      "[Metrics] logloss: 0.000000 - AUC: 1.000000\n",
      "[Metrics] logloss: 0.000000 - AUC: 1.000000\n",
      "[Metrics] logloss: 0.000000 - AUC: 1.000000\n",
      "[Metrics] logloss: 0.000000 - AUC: 1.000000\n",
      "[Metrics] logloss: 0.000000 - AUC: 1.000000\n",
      "[Metrics] logloss: 0.000000 - AUC: 1.000000\n",
      "[Metrics] logloss: 0.000000 - AUC: 1.000000\n",
      "[Metrics] logloss: 0.000000 - AUC: 1.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/pytorch_lightning/core/datamodule.py:470: LightningDeprecationWarning: DataModule.setup has already been called, so it will not be called again. In v1.6 this behavior will change to always call DataModule.setup.\n",
      "  f\"DataModule.{name} has already been called, so it will not be called again. \"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d559374f05874adf9e94b504a3cd4a90",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Metrics] logloss: 0.000000 - AUC: 1.000000\n",
      "[Metrics] logloss: 0.000000 - AUC: 1.000000\n",
      "[Metrics] logloss: 0.000000 - AUC: 1.000000\n",
      "[Metrics] logloss: 0.000000 - AUC: 1.000000\n",
      "[Metrics] logloss: 0.000000 - AUC: 1.000000\n",
      "[Metrics] logloss: 0.000000 - AUC: 1.000000\n",
      "[Metrics] logloss: 0.000000 - AUC: 1.000000\n",
      "[Metrics] logloss: 0.000000 - AUC: 1.000000\n",
      "[Metrics] logloss: 0.000000 - AUC: 1.000000\n",
      "[Metrics] logloss: 0.000000 - AUC: 1.000000\n",
      "[Metrics] logloss: 0.000000 - AUC: 1.000000\n",
      "[Metrics] logloss: 0.000000 - AUC: 1.000000\n",
      "[Metrics] logloss: 0.000000 - AUC: 1.000000\n",
      "[Metrics] logloss: 0.000000 - AUC: 1.000000\n",
      "[Metrics] logloss: 0.000000 - AUC: 1.000000\n",
      "[Metrics] logloss: 0.000000 - AUC: 1.000000\n",
      "[Metrics] logloss: 0.000000 - AUC: 1.000000\n",
      "[Metrics] logloss: 0.000000 - AUC: 1.000000\n",
      "[Metrics] logloss: 0.000000 - AUC: 1.000000\n",
      "[Metrics] logloss: 0.000000 - AUC: 1.000000\n",
      "[Metrics] logloss: 0.000000 - AUC: 1.000000\n",
      "[Metrics] logloss: 0.000000 - AUC: 1.000000\n",
      "[Metrics] logloss: 0.000000 - AUC: 1.000000\n",
      "[Metrics] logloss: 0.000000 - AUC: 1.000000\n",
      "[Metrics] logloss: 0.000000 - AUC: 1.000000\n",
      "[Metrics] logloss: 0.000000 - AUC: 1.000000\n",
      "[Metrics] logloss: 0.000000 - AUC: 1.000000\n",
      "[Metrics] logloss: 0.000000 - AUC: 1.000000\n",
      "[Metrics] logloss: 0.000000 - AUC: 1.000000\n",
      "[Metrics] logloss: 0.000000 - AUC: 1.000000\n",
      "[Metrics] logloss: 0.000000 - AUC: 1.000000\n",
      "[Metrics] logloss: 0.000000 - AUC: 1.000000\n",
      "[Metrics] logloss: 0.000000 - AUC: 1.000000\n",
      "[Metrics] logloss: 0.000000 - AUC: 1.000000\n",
      "[Metrics] logloss: 0.000000 - AUC: 1.000000\n",
      "[Metrics] logloss: 0.000000 - AUC: 1.000000\n",
      "[Metrics] logloss: 0.000000 - AUC: 1.000000\n",
      "[Metrics] logloss: 0.000000 - AUC: 1.000000\n",
      "[Metrics] logloss: 0.000000 - AUC: 1.000000\n",
      "[Metrics] logloss: 0.000000 - AUC: 1.000000\n",
      "[Metrics] logloss: 0.000000 - AUC: 1.000000\n",
      "[Metrics] logloss: 0.000000 - AUC: 1.000000\n",
      "[Metrics] logloss: 0.000000 - AUC: 1.000000\n",
      "[Metrics] logloss: 0.000000 - AUC: 1.000000\n",
      "[Metrics] logloss: 0.000000 - AUC: 1.000000\n",
      "[Metrics] logloss: 0.000000 - AUC: 1.000000\n",
      "[Metrics] logloss: 0.000000 - AUC: 1.000000\n",
      "--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{'Test Metrics': {'AUC': tensor(1.), 'logloss': tensor(1.0680e-07)}}\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'Test Metrics': {'AUC': tensor(1.), 'logloss': tensor(1.0680e-07)}}]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from recohut.trainers.pl_trainer import pl_trainer\n",
    "\n",
    "pl_trainer(model, ds, max_epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #export\n",
    "# import shutil\n",
    "# import struct\n",
    "# from collections import defaultdict\n",
    "# from pathlib import Path\n",
    "\n",
    "# import lmdb\n",
    "# import numpy as np\n",
    "# import torch.utils.data\n",
    "# from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #export\n",
    "# class AvazuDataset(torch.utils.data.Dataset):\n",
    "#     \"\"\"\n",
    "#     Avazu Click-Through Rate Prediction Dataset\n",
    "#     Dataset preparation\n",
    "#         Remove the infrequent features (appearing in less than threshold instances) and treat them as a single feature\n",
    "#     :param dataset_path: avazu train path\n",
    "#     :param cache_path: lmdb cache path\n",
    "#     :param rebuild_cache: If True, lmdb cache is refreshed\n",
    "#     :param min_threshold: infrequent feature threshold\n",
    "#     Reference\n",
    "#         https://www.kaggle.com/c/avazu-ctr-prediction\n",
    "#     \"\"\"\n",
    "\n",
    "#     def __init__(self, dataset_path=None, cache_path='.avazu', rebuild_cache=False, min_threshold=4):\n",
    "#         self.NUM_FEATS = 22\n",
    "#         self.min_threshold = min_threshold\n",
    "#         if rebuild_cache or not Path(cache_path).exists():\n",
    "#             shutil.rmtree(cache_path, ignore_errors=True)\n",
    "#             if dataset_path is None:\n",
    "#                 raise ValueError('create cache: failed: dataset_path is None')\n",
    "#             self.__build_cache(dataset_path, cache_path)\n",
    "#         self.env = lmdb.open(cache_path, create=False, lock=False, readonly=True)\n",
    "#         with self.env.begin(write=False) as txn:\n",
    "#             self.length = txn.stat()['entries'] - 1\n",
    "#             self.field_dims = np.frombuffer(txn.get(b'field_dims'), dtype=np.uint32)\n",
    "\n",
    "#     def __getitem__(self, index):\n",
    "#         with self.env.begin(write=False) as txn:\n",
    "#             np_array = np.frombuffer(\n",
    "#                 txn.get(struct.pack('>I', index)), dtype=np.uint32).astype(dtype=np.long)\n",
    "#         return np_array[1:], np_array[0]\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return self.length\n",
    "\n",
    "#     def __build_cache(self, path, cache_path):\n",
    "#         feat_mapper, defaults = self.__get_feat_mapper(path)\n",
    "#         with lmdb.open(cache_path, map_size=int(1e11)) as env:\n",
    "#             field_dims = np.zeros(self.NUM_FEATS, dtype=np.uint32)\n",
    "#             for i, fm in feat_mapper.items():\n",
    "#                 field_dims[i - 1] = len(fm) + 1\n",
    "#             with env.begin(write=True) as txn:\n",
    "#                 txn.put(b'field_dims', field_dims.tobytes())\n",
    "#             for buffer in self.__yield_buffer(path, feat_mapper, defaults):\n",
    "#                 with env.begin(write=True) as txn:\n",
    "#                     for key, value in buffer:\n",
    "#                         txn.put(key, value)\n",
    "\n",
    "#     def __get_feat_mapper(self, path):\n",
    "#         feat_cnts = defaultdict(lambda: defaultdict(int))\n",
    "#         with open(path) as f:\n",
    "#             f.readline()\n",
    "#             pbar = tqdm(f, mininterval=1, smoothing=0.1)\n",
    "#             pbar.set_description('Create avazu dataset cache: counting features')\n",
    "#             for line in pbar:\n",
    "#                 values = line.rstrip('\\n').split(',')\n",
    "#                 if len(values) != self.NUM_FEATS + 2:\n",
    "#                     continue\n",
    "#                 for i in range(1, self.NUM_FEATS + 1):\n",
    "#                     feat_cnts[i][values[i + 1]] += 1\n",
    "#         feat_mapper = {i: {feat for feat, c in cnt.items() if c >= self.min_threshold} for i, cnt in feat_cnts.items()}\n",
    "#         feat_mapper = {i: {feat: idx for idx, feat in enumerate(cnt)} for i, cnt in feat_mapper.items()}\n",
    "#         defaults = {i: len(cnt) for i, cnt in feat_mapper.items()}\n",
    "#         return feat_mapper, defaults\n",
    "\n",
    "#     def __yield_buffer(self, path, feat_mapper, defaults, buffer_size=int(1e5)):\n",
    "#         item_idx = 0\n",
    "#         buffer = list()\n",
    "#         with open(path) as f:\n",
    "#             f.readline()\n",
    "#             pbar = tqdm(f, mininterval=1, smoothing=0.1)\n",
    "#             pbar.set_description('Create avazu dataset cache: setup lmdb')\n",
    "#             for line in pbar:\n",
    "#                 values = line.rstrip('\\n').split(',')\n",
    "#                 if len(values) != self.NUM_FEATS + 2:\n",
    "#                     continue\n",
    "#                 np_array = np.zeros(self.NUM_FEATS + 1, dtype=np.uint32)\n",
    "#                 np_array[0] = int(values[1])\n",
    "#                 for i in range(1, self.NUM_FEATS + 1):\n",
    "#                     np_array[i] = feat_mapper[i].get(values[i+1], defaults[i])\n",
    "#                 buffer.append((struct.pack('>I', item_idx), np_array.tobytes()))\n",
    "#                 item_idx += 1\n",
    "#                 if item_idx % buffer_size == 0:\n",
    "#                     yield buffer\n",
    "#                     buffer.clear()\n",
    "#             yield buffer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **References:-**\n",
    "- https://github.com/rixwew/pytorch-fm/blob/master/torchfm/dataset/avazu.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Author: Sparsh A.\n",
      "\n",
      "Last updated: 2022-01-11 22:08:24\n",
      "\n",
      "recohut          : 0.0.11\n",
      "pytorch_lightning: 1.5.8\n",
      "\n",
      "Compiler    : GCC 7.5.0\n",
      "OS          : Linux\n",
      "Release     : 5.4.144+\n",
      "Machine     : x86_64\n",
      "Processor   : x86_64\n",
      "CPU cores   : 2\n",
      "Architecture: 64bit\n",
      "\n",
      "IPython: 5.5.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "%reload_ext watermark\n",
    "%watermark -a \"Sparsh A.\" -m -iv -u -t -d -p recohut,pytorch_lightning"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
