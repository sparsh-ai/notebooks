{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"2022-01-23-sasrec-paddlepaddle.ipynb","provenance":[{"file_id":"https://github.com/recohut/nbs/blob/main/raw/T225287%20%7C%20SASRec%20on%20ML-1m%20in%20PaddlePaddle.ipynb","timestamp":1644663414516}],"collapsed_sections":[],"authorship_tag":"ABX9TyMF8Mg9MSYmYfSu7NHjv9oV"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"uDw7iV-2ynqJ"},"source":["# SASRec on ML-1m in PaddlePaddle"]},{"cell_type":"markdown","metadata":{"id":"UjLxWVPkKyhF"},"source":["## Setup"]},{"cell_type":"markdown","metadata":{"id":"vIWlbC_jKzeL"},"source":["### Installations"]},{"cell_type":"code","metadata":{"id":"S0fgO4r4w7nD"},"source":["!pip install -q paddlepaddle"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"v89X-JBeK1cl"},"source":["### Downloads"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"x4f-Rv89woy0","executionInfo":{"status":"ok","timestamp":1633207647131,"user_tz":-330,"elapsed":454,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}},"outputId":"b9d9191b-e730-467b-ee7b-0ab95b1e6261"},"source":["!wget -q --show-progress https://github.com/paddorch/SASRec.paddle/raw/main/data/preprocessed/ml-1m.txt"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\rml-1m.txt             0%[                    ]       0  --.-KB/s               \rml-1m.txt           100%[===================>]   8.63M  --.-KB/s    in 0.04s   \n"]}]},{"cell_type":"markdown","metadata":{"id":"ABOGAwyyK26M"},"source":["### Imports"]},{"cell_type":"code","metadata":{"id":"ZQOjPPXyK4C_"},"source":["import os\n","import sys\n","import copy\n","import random\n","import numpy as np\n","from multiprocessing import Process, Queue\n","from collections import defaultdict\n","\n","import random\n","from tqdm import tqdm\n","\n","import paddle\n","import paddle.nn as nn\n","from paddle import optimizer\n","import paddle.nn.functional as F\n","\n","import argparse"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jBWlW-NsLNYP"},"source":["### Params"]},{"cell_type":"code","metadata":{"id":"xmkjSjaJLPcN"},"source":["set_seed(42)\n","\n","parser = argparse.ArgumentParser(description='SASRec training')\n","# data\n","parser.add_argument('--dataset_path', metavar='DIR',\n","                    default='ml-1m.txt')\n","# learning\n","learn = parser.add_argument_group('Learning options')\n","learn.add_argument('--lr', type=float, default=0.001, help='initial learning rate [default: 0.01]')\n","learn.add_argument('--epochs', type=int, default=100, help='number of epochs for train')\n","learn.add_argument('--batch_size', type=int, default=128, help='batch size for training')\n","learn.add_argument('--optimizer', default='AdamW',\n","                   help='Type of optimizer. Adagrad|Adam|AdamW are supported [default: Adagrad]')\n","# model\n","model_cfg = parser.add_argument_group('Model options')\n","model_cfg.add_argument('--hidden_units', type=int, default=50,\n","                       help='hidden size of LSTM [default: 300]')\n","model_cfg.add_argument('--maxlen', type=int, default=200,\n","                       help='hidden size of LSTM [default: 300]')\n","model_cfg.add_argument('--dropout', type=float, default=0.2, help='the probability for dropout')\n","model_cfg.add_argument('--l2_emb', type=float, default=0.0, help='penalty term coefficient')\n","model_cfg.add_argument('--num_blocks', type=int, default=2,\n","                       help='d_a size [default: 150]')\n","model_cfg.add_argument('--num_heads', type=int, default=1,\n","                       help='row size of sentence embedding [default: 30]')\n","# device\n","device = parser.add_argument_group('Device options')\n","device.add_argument('--num_workers', default=8, type=int, help='Number of workers used in data-loading')\n","device.add_argument('--cuda', action='store_true', default=True, help='enable the gpu')\n","device.add_argument('--device', type=int, default=None)\n","\n","# experiment options\n","experiment = parser.add_argument_group('Experiment options')\n","experiment.add_argument('--continue_from', default='', help='Continue from checkpoint model')\n","experiment.add_argument('--checkpoint', dest='checkpoint', default=True, action='store_true',\n","                        help='Enables checkpoint saving of model')\n","experiment.add_argument('--checkpoint_per_batch', default=10000, type=int,\n","                        help='Save checkpoint per batch. 0 means never save [default: 10000]')\n","experiment.add_argument('--save_folder', default='output/',\n","                        help='Location to save epoch models, training configurations and results.')\n","experiment.add_argument('--log_config', default=True, action='store_true', help='Store experiment configuration')\n","experiment.add_argument('--log_result', default=True, action='store_true', help='Store experiment result')\n","experiment.add_argument('--log_interval', type=int, default=30,\n","                        help='how many steps to wait before logging training status')\n","experiment.add_argument('--val_interval', type=int, default=800,\n","                        help='how many steps to wait before vaidation')\n","experiment.add_argument('--val_start_batch', type=int, default=8000,\n","                        help='how many steps to wait before vaidation')\n","experiment.add_argument('--save_interval', type=int, default=20,\n","                        help='how many epochs to wait before saving')\n","experiment.add_argument('--test', type=bool, default=False, help='test only')\n","experiment.add_argument('--model_path', type=str, default=False, help='test only')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"n6BMp5aLwsFD"},"source":["## Data"]},{"cell_type":"code","metadata":{"id":"cDS6GBujw0OU"},"source":["# sampler for batch generation\n","def random_neq(l, r, s):\n","    t = np.random.randint(l, r)\n","    while t in s:\n","        t = np.random.randint(l, r)\n","    return t\n","\n","\n","def sample_function(user_train, usernum, itemnum, batch_size, maxlen, result_queue=None, SEED=42):\n","    def sample():\n","        user = np.random.randint(1, usernum + 1)\n","        while len(user_train[user]) <= 1:\n","            user = np.random.randint(1, usernum + 1)\n","\n","        seq = np.zeros([maxlen], dtype=np.int32)\n","        pos = np.zeros([maxlen], dtype=np.int32)\n","        neg = np.zeros([maxlen], dtype=np.int32)\n","        nxt = user_train[user][-1]\n","        idx = maxlen - 1\n","\n","        ts = set(user_train[user])\n","        for i in reversed(user_train[user][:-1]):\n","            seq[idx] = i\n","            pos[idx] = nxt\n","            if nxt != 0: neg[idx] = random_neq(1, itemnum + 1, ts)\n","            nxt = i\n","            idx -= 1\n","            if idx == -1: break\n","\n","        return (user, seq, pos, neg)  # TODO\n","\n","    if result_queue is None:\n","        np.random.seed(SEED)\n","        one_batch = []\n","        for i in range(batch_size):\n","            one_batch.append(sample())\n","        return zip(*one_batch)\n","    else:\n","        np.random.seed(SEED)\n","        while True:\n","            one_batch = []\n","            for i in range(batch_size):\n","                one_batch.append(sample())\n","\n","            result_queue.put(zip(*one_batch))\n","\n","\n","class WarpSampler(object):\n","    def __init__(self, User, usernum, itemnum, batch_size=64, maxlen=10, n_workers=1):\n","        self.n_workers = n_workers\n","        if self.n_workers != 0:\n","            self.result_queue = Queue(maxsize=n_workers * 10)\n","            self.processors = []\n","            for i in range(n_workers):\n","                self.processors.append(\n","                    Process(target=sample_function, args=(User,\n","                                                          usernum,\n","                                                          itemnum,\n","                                                          batch_size,\n","                                                          maxlen,\n","                                                          self.result_queue,\n","                                                          np.random.randint(2e9)\n","                                                          )))\n","                self.processors[-1].daemon = True\n","                self.processors[-1].start()\n","        else:\n","            self.User = User\n","            self.usernum = usernum\n","            self.itemnum = itemnum\n","            self.batch_size = batch_size\n","            self.maxlen = maxlen\n","\n","    def next_batch(self):\n","        if self.n_workers != 0:\n","            return self.result_queue.get()\n","        return sample_function(self.User,\n","                               self.usernum,\n","                               self.itemnum,\n","                               self.batch_size,\n","                               self.maxlen,\n","                               None,\n","                               np.random.randint(2e9))\n","\n","    def close(self):\n","        for p in self.processors:\n","            p.terminate()\n","            p.join()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"P1EgnHMlxVh-"},"source":["## Utils"]},{"cell_type":"code","metadata":{"id":"YVTtmt4ww0kj"},"source":["def set_seed(seed):\n","    random.seed(seed)\n","    np.random.seed(seed)\n","    paddle.seed(seed)\n","\n","\n","# train/val/test data generation\n","def data_partition(fname):\n","    usernum = 0\n","    itemnum = 0\n","    User = defaultdict(list)\n","    user_train = {}\n","    user_valid = {}\n","    user_test = {}\n","    # assume user/item index starting from 1\n","    with open(fname, 'r') as f:\n","        for line in f:\n","            u, i = line.rstrip().split(' ')\n","            u = int(u)\n","            i = int(i)\n","            usernum = max(u, usernum)\n","            itemnum = max(i, itemnum)\n","            User[u].append(i)\n","\n","    for user in User:\n","        nfeedback = len(User[user])\n","        if nfeedback < 3:\n","            user_train[user] = User[user]\n","            user_valid[user] = []\n","            user_test[user] = []\n","        else:\n","            user_train[user] = User[user][:-2]\n","            user_valid[user] = []\n","            user_valid[user].append(User[user][-2])\n","            user_test[user] = []\n","            user_test[user].append(User[user][-1])\n","    return [user_train, user_valid, user_test, usernum, itemnum]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9Lt_z2cZxXY4"},"source":["## Model"]},{"cell_type":"code","metadata":{"id":"JiURfKinw6FR"},"source":["class SASRec(paddle.nn.Layer):\n","    def __init__(self, item_num, args):\n","        super(SASRec, self).__init__()\n","        self.item_emb = nn.Embedding(item_num + 1, args.hidden_units)  # [pad] is 0\n","        self.pos_emb = nn.Embedding(args.maxlen, args.hidden_units)\n","        self.emb_dropout = paddle.nn.Dropout(p=args.dropout)\n","\n","        self.subsequent_mask = (paddle.triu(paddle.ones((args.maxlen, args.maxlen))) == 0)\n","        self.encoder_layer = nn.TransformerEncoderLayer(d_model=args.hidden_units,\n","                                                        nhead=args.num_heads,\n","                                                        dim_feedforward=args.hidden_units,\n","                                                        dropout=args.dropout)\n","        self.encoder = nn.TransformerEncoder(encoder_layer=self.encoder_layer, num_layers=args.num_blocks)\n","\n","    def position_encoding(self, seqs):\n","        seqs_embed = self.item_emb(seqs)  # (batch_size, max_len, embed_size)\n","        positions = np.tile(np.array(range(seqs.shape[1])), [seqs.shape[0], 1])\n","        position_embed = self.pos_emb(paddle.to_tensor(positions, dtype='int64'))\n","        return self.emb_dropout(seqs_embed + position_embed)\n","\n","    def forward(self, log_seqs, pos_seqs, neg_seqs):\n","        # all input seqs: (batch_size, seq_len)\n","        seqs_embed = self.position_encoding(log_seqs)  # (batch_size, seq_len, embed_size)\n","        log_feats = self.encoder(seqs_embed, self.subsequent_mask)  # (batch_size, seq_len, embed_size)\n","\n","        pos_embed = self.item_emb(pos_seqs)  # (batch_size, seq_len, embed_size)\n","        neg_embed = self.item_emb(neg_seqs)\n","\n","        pos_logits = (log_feats * pos_embed).sum(axis=-1)\n","        neg_logits = (log_feats * neg_embed).sum(axis=-1)\n","\n","        return pos_logits, neg_logits\n","\n","    def predict(self, log_seqs, item_indices):  # for inference\n","        seqs = self.position_encoding(log_seqs)\n","        log_feats = self.encoder(seqs, self.subsequent_mask)  # (batch_size, seq_len, embed_size)\n","\n","        final_feat = log_feats[:, -1, :]\n","        item_embs = self.item_emb(paddle.to_tensor(item_indices, dtype='int64'))\n","\n","        logits = item_embs.matmul(final_feat.unsqueeze(-1)).squeeze(-1)\n","        return logits"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"18NJ5h3mxQfD"},"source":["## Eval"]},{"cell_type":"code","metadata":{"id":"-h0TdKVbxZVK"},"source":["def evaluate(dataset, model, epoch_train, batch_train, args, is_val=True):\n","    model.eval()\n","    [train, valid, test, usernum, itemnum] = copy.deepcopy(dataset)\n","\n","    before = train\n","    now = valid if is_val else test\n","\n","    NDCG = 0.0\n","    HT = 0.0\n","    valid_user = 0.0\n","\n","    if usernum > 10000:\n","        users = random.sample(range(1, usernum + 1), 10000)\n","    else:\n","        users = range(1, usernum + 1)\n","\n","    for u in tqdm(users):\n","        if len(before[u]) < 1 or len(now[u]) < 1:\n","            continue\n","        seq = np.zeros([args.maxlen], dtype=np.int32)\n","        idx = args.maxlen - 1\n","        if not is_val:\n","            seq[idx] = valid[u][0]\n","            idx -= 1\n","        for i in reversed(before[u]):\n","            seq[idx] = i\n","            idx -= 1\n","            if idx == -1: break\n","        rated = set(before[u])\n","        rated.add(0)\n","        item_idx = [now[u][0]]\n","        for _ in range(100):\n","            t = np.random.randint(1, itemnum + 1)\n","            while t in rated:\n","                t = np.random.randint(1, itemnum + 1)\n","            item_idx.append(t)\n","        predictions = -model.predict(*[paddle.to_tensor(l) for l in [[seq], item_idx]])\n","        predictions = predictions[0]  # - for 1st argsort DESC\n","\n","        rank = predictions.argsort().argsort()[0].item()\n","\n","        valid_user += 1\n","        if rank < 10:\n","            NDCG += 1 / np.log2(rank + 2)\n","            HT += 1\n","\n","    NDCG /= valid_user\n","    HT /= valid_user\n","\n","    model.train()\n","    print('\\nEpoch {} Evaluation - NDCG: {:.4f}  HIT@10: {:.4f}'.format(epoch_train,  NDCG, HT))\n","    if args.log_result and is_val:\n","        with open(os.path.join(args.save_folder, 'result.csv'), 'a') as r:\n","            r.write('\\n{:d},{:d},{:.4f},{:.4f}'.format(epoch_train, batch_train, NDCG, HT))\n","    return (HT, NDCG)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"bCjflfkfxbLJ"},"source":["## Train"]},{"cell_type":"code","metadata":{"id":"Kq7KJBoAxc5L"},"source":["class MyBCEWithLogitLoss(paddle.nn.Layer):\n","    def __init__(self):\n","        super(MyBCEWithLogitLoss, self).__init__()\n","\n","    def forward(self, pos_logits, neg_logits, labels):\n","        return paddle.sum(\n","            - paddle.log(F.sigmoid(pos_logits) + 1e-24) * labels -\n","            paddle.log(1 - F.sigmoid(neg_logits) + 1e-24) * labels,\n","            axis=(0, 1)\n","        ) / paddle.sum(labels, axis=(0, 1))\n","\n","\n","def train(sampler, model, args, num_batch, dataset):\n","    clip = None\n","    # optimization scheme\n","    if args.optimizer == 'Adam':\n","        optim = optimizer.Adam(parameters=model.parameters(), learning_rate=args.lr, grad_clip=clip)\n","    elif args.optimizer == 'Adagrad':\n","        optim = optimizer.Adagrad(parameters=model.parameters(), learning_rate=args.lr, grad_clip=clip)\n","    elif args.optimizer == 'AdamW':\n","        optim = optimizer.AdamW(parameters=model.parameters(), learning_rate=args.lr, grad_clip=clip)\n","\n","    # loss\n","    # criterion = nn.BCEWithLogitsLoss()\n","    criterion = MyBCEWithLogitLoss()\n","\n","    # continue training from checkpoint model\n","    if args.continue_from:\n","        print(\"=> loading checkpoint from '{}'\".format(args.continue_from))\n","        assert os.path.isfile(args.continue_from), \"=> no checkpoint found at '{}'\".format(args.continue_from)\n","        checkpoint = paddle.load(args.continue_from)\n","        start_epoch = checkpoint['epoch']\n","        best_pair = checkpoint.get('best_pair', None)\n","        model.set_state_dict(checkpoint['state_dict'])\n","    else:\n","        start_epoch = 1\n","        best_pair = None\n","\n","    model.train()\n","\n","    tot_batch = 0\n","    for epoch in range(start_epoch, args.epochs + 1):\n","        epoch_loss = 0\n","        for i_batch in range(num_batch):\n","            tot_batch += 1\n","            u, seq, pos, neg = sampler.next_batch()  # tuples to ndarray\n","            u, seq, pos, neg = paddle.to_tensor(u, dtype='int64'), paddle.to_tensor(seq,\n","                                                                                    dtype='int64'), paddle.to_tensor(\n","                pos), paddle.to_tensor(neg)\n","            pos_logits, neg_logits = model(seq, pos, neg)  # ()\n","\n","            targets = (pos != 0).astype(dtype='int32')\n","            # targets = targets.reshape((args.batch_size*args.maxlen, -1))\n","            loss = criterion(pos_logits, neg_logits, targets)\n","            for param in model.item_emb.parameters():\n","                loss += args.l2_emb * paddle.norm(param)\n","            loss.backward()\n","            epoch_loss += loss.numpy()[0]\n","            optim.step()\n","            optim.clear_grad()\n","\n","            # validation\n","            if tot_batch >= args.val_start_batch and tot_batch % args.val_interval == 0 and i_batch != 0:\n","                valid_pair = evaluate(dataset, model, epoch, i_batch, args, is_val=True)\n","                if best_pair is None or valid_pair > best_pair:\n","                    best_pair = valid_pair\n","                    file_path = '%s/SASRec_best.pth.tar' % (args.save_folder)\n","                    print(\"=> found better validated model, saving to %s\" % file_path)\n","                    save_checkpoint(model,\n","                                    {'epoch': epoch,\n","                                     'optimizer': optim.state_dict(),\n","                                     'best_pair': best_pair},\n","                                    file_path)\n","\n","        print('Epoch {:3} - loss: {:.4f}  lr: {:.5f}'.format(epoch,\n","                                                             epoch_loss / num_batch,\n","                                                             optim._learning_rate,\n","                                                             ))\n","\n","        if args.checkpoint and epoch % args.save_interval == 0:\n","            file_path = '%s/SASRec_epoch_%d.pth.tar' % (args.save_folder, epoch)\n","            print(\"\\r=> saving checkpoint model to %s\" % file_path)\n","            save_checkpoint(model, {'epoch': epoch,\n","                                    'optimizer': optim.state_dict(),\n","                                    'best_pair': best_pair},\n","                            file_path)\n","\n","\n","def save_checkpoint(model, state, filename):\n","    state['state_dict'] = model.state_dict()\n","    paddle.save(state, filename)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"XqlsIzOqxgTk"},"source":["## Run"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kceOYcKvxk3O","outputId":"bb13e1c5-a0ed-4231-a096-a64ebf93e052"},"source":["def main():\n","    print(paddle.__version__)\n","    args = parser.parse_args(args={})\n","\n","    # gpu\n","    if args.cuda and args.device:\n","        paddle.set_device(f\"gpu:{args.device}\")\n","    print(paddle.get_device())\n","\n","    dataset = data_partition(args.dataset_path)\n","\n","    [user_train, _, _, usernum, itemnum] = dataset\n","    num_batch = len(user_train) // args.batch_size  # tail? + ((len(user_train) % args.batch_size) != 0)\n","    print(\"batches / epoch:\", num_batch)\n","\n","    seq_len = 0.0\n","    for u in user_train:\n","        seq_len += len(user_train[u])\n","    print('\\nAverage sequence length: %.2f' % (seq_len / len(user_train)))\n","\n","    # make save folder\n","    if not os.path.exists(args.save_folder):\n","        os.makedirs(args.save_folder)\n","\n","    # configuration\n","    print(\"\\nConfiguration:\")\n","    for attr, value in sorted(args.__dict__.items()):\n","        print(\"\\t{}:\".format(attr.capitalize().replace('_', ' ')).ljust(25) + \"{}\".format(value))\n","\n","    # log result\n","    if args.log_result:\n","        with open(os.path.join(args.save_folder, 'result.csv'), 'w') as r:\n","            r.write('{:s},{:s},{:s},{:s},{:s}'.format('epoch', 'batch', 'loss', 'acc', 'lr'))\n","\n","    # model\n","    model = SASRec(itemnum, args)\n","    print(model)\n","\n","    if not args.test:  # train\n","        # dataloader\n","        sampler = WarpSampler(user_train, usernum, itemnum, batch_size=args.batch_size, maxlen=args.maxlen,\n","                              n_workers=args.num_workers)\n","        train(sampler, model, args, num_batch, dataset)\n","        sampler.close()\n","    else:  # test\n","        print(\"=> loading weights from '{}'\".format(args.model_path))\n","        assert os.path.isfile(args.model_path), \"=> no checkpoint found at '{}'\".format(args.model_path)\n","        checkpoint = paddle.load(args.model_path)\n","        model.set_state_dict(checkpoint['state_dict'])\n","        evaluate(dataset, model, checkpoint['epoch'], 0, args, is_val=False)\n","\n","\n","if __name__ == '__main__':\n","    main()"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["2.1.3\n","cpu\n","batches / epoch: 47\n","\n","Average sequence length: 163.50\n","\n","Configuration:\n","\tBatch size:             128\n","\tCheckpoint:             True\n","\tCheckpoint per batch:   10000\n","\tContinue from:          \n","\tCuda:                   True\n","\tDataset path:           ml-1m.txt\n","\tDevice:                 None\n","\tDropout:                0.2\n","\tEpochs:                 1000\n","\tHidden units:           50\n","\tL2 emb:                 0.0\n","\tLog config:             True\n","\tLog interval:           30\n","\tLog result:             True\n","\tLr:                     0.001\n","\tMaxlen:                 200\n","\tModel path:             False\n","\tNum blocks:             2\n","\tNum heads:              1\n","\tNum workers:            8\n","\tOptimizer:              AdamW\n","\tSave folder:            output/\n","\tSave interval:          20\n","\tTest:                   False\n","\tVal interval:           800\n","\tVal start batch:        8000\n","SASRec(\n","  (item_emb): Embedding(3417, 50, sparse=False)\n","  (pos_emb): Embedding(200, 50, sparse=False)\n","  (emb_dropout): Dropout(p=0.2, axis=None, mode=upscale_in_train)\n","  (encoder_layer): TransformerEncoderLayer(\n","    (self_attn): MultiHeadAttention(\n","      (q_proj): Linear(in_features=50, out_features=50, dtype=float32)\n","      (k_proj): Linear(in_features=50, out_features=50, dtype=float32)\n","      (v_proj): Linear(in_features=50, out_features=50, dtype=float32)\n","      (out_proj): Linear(in_features=50, out_features=50, dtype=float32)\n","    )\n","    (linear1): Linear(in_features=50, out_features=50, dtype=float32)\n","    (dropout): Dropout(p=0.2, axis=None, mode=upscale_in_train)\n","    (linear2): Linear(in_features=50, out_features=50, dtype=float32)\n","    (norm1): LayerNorm(normalized_shape=[50], epsilon=1e-05)\n","    (norm2): LayerNorm(normalized_shape=[50], epsilon=1e-05)\n","    (dropout1): Dropout(p=0.2, axis=None, mode=upscale_in_train)\n","    (dropout2): Dropout(p=0.2, axis=None, mode=upscale_in_train)\n","  )\n","  (encoder): TransformerEncoder(\n","    (layers): LayerList(\n","      (0): TransformerEncoderLayer(\n","        (self_attn): MultiHeadAttention(\n","          (q_proj): Linear(in_features=50, out_features=50, dtype=float32)\n","          (k_proj): Linear(in_features=50, out_features=50, dtype=float32)\n","          (v_proj): Linear(in_features=50, out_features=50, dtype=float32)\n","          (out_proj): Linear(in_features=50, out_features=50, dtype=float32)\n","        )\n","        (linear1): Linear(in_features=50, out_features=50, dtype=float32)\n","        (dropout): Dropout(p=0.2, axis=None, mode=upscale_in_train)\n","        (linear2): Linear(in_features=50, out_features=50, dtype=float32)\n","        (norm1): LayerNorm(normalized_shape=[50], epsilon=1e-05)\n","        (norm2): LayerNorm(normalized_shape=[50], epsilon=1e-05)\n","        (dropout1): Dropout(p=0.2, axis=None, mode=upscale_in_train)\n","        (dropout2): Dropout(p=0.2, axis=None, mode=upscale_in_train)\n","      )\n","      (1): TransformerEncoderLayer(\n","        (self_attn): MultiHeadAttention(\n","          (q_proj): Linear(in_features=50, out_features=50, dtype=float32)\n","          (k_proj): Linear(in_features=50, out_features=50, dtype=float32)\n","          (v_proj): Linear(in_features=50, out_features=50, dtype=float32)\n","          (out_proj): Linear(in_features=50, out_features=50, dtype=float32)\n","        )\n","        (linear1): Linear(in_features=50, out_features=50, dtype=float32)\n","        (dropout): Dropout(p=0.2, axis=None, mode=upscale_in_train)\n","        (linear2): Linear(in_features=50, out_features=50, dtype=float32)\n","        (norm1): LayerNorm(normalized_shape=[50], epsilon=1e-05)\n","        (norm2): LayerNorm(normalized_shape=[50], epsilon=1e-05)\n","        (dropout1): Dropout(p=0.2, axis=None, mode=upscale_in_train)\n","        (dropout2): Dropout(p=0.2, axis=None, mode=upscale_in_train)\n","      )\n","    )\n","  )\n",")\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/paddle/fluid/dygraph/math_op_patch.py:239: UserWarning: The dtype of left and right variables are not the same, left dtype is paddle.float32, but right dtype is paddle.int32, the right dtype will convert to paddle.float32\n","  format(lhs_dtype, rhs_dtype, lhs_dtype))\n"]},{"output_type":"stream","name":"stdout","text":["Epoch   1 - loss: 1.1634  lr: 0.00100\n","Epoch   2 - loss: 1.0174  lr: 0.00100\n","Epoch   3 - loss: 1.0029  lr: 0.00100\n","Epoch   4 - loss: 0.9717  lr: 0.00100\n","Epoch   5 - loss: 0.9300  lr: 0.00100\n"]}]},{"cell_type":"code","metadata":{"id":"9F65suc6xxuO"},"source":["%%writefile train.sh\n","python run.py \\\n","  --dataset_path=data/preprocessed/ml-1m.txt \\\n","  --hidden_units=50\n","  --dropout=0.2\n","  --num_blocks=2\n","  --num_heads=1\n","  --device=0"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"07Ej6jTDyAzk"},"source":["%%writefile eval.sh\n","python run.py \\\n","  --dataset_path=data/preprocessed/ml-1m.txt \\\n","  --hidden_units=50 \\\n","  --num_blocks=2 \\\n","  --num_heads=1 \\\n","  --device=0 \\\n","  --test=True\\\n","  --model_path=output/SASRec_epoch_420.pth.tar"],"execution_count":null,"outputs":[]}]}