{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"reco-tutorials-prod-neural-matrix-factorization-from-scratch-in-pytorch.ipynb","provenance":[{"file_id":"1JaNgnfSp-uA1DZzhAI9usSS_rSk3JqfI","timestamp":1622022444063},{"file_id":"1ARqzPS9EuVZL3Xroqlz_6AqDLUsWAivx","timestamp":1622018870681},{"file_id":"1shu4X-Zz6pPN8bOBDKrDSbjNym1uy3ml","timestamp":1621379458916},{"file_id":"1SaddF89o-d87pl1ecWL_xL0TmHAkDwVd","timestamp":1621377490593}],"collapsed_sections":[],"toc_visible":true,"mount_file_id":"1_rp4g6-pyprSUlnrWhYy4cDvUKunUIrD","authorship_tag":"ABX9TyNK30mS5WSTO/O4V01fB7nF"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"VTHU63hYgwzT"},"source":["summary: A tutorial to understand the process of building a Neural Matrix Factorization model from scratch in PyTorch on MovieLens-1M dataset.\n","id: neural-matrix-factorization-from-scratch-in-pytorch\n","categories: Pytorch\n","tags: scratch, movielens\n","status: Published \n","authors: Sparsh A.\n","Feedback Link: https://form.jotform.com/211377288388469"]},{"cell_type":"markdown","metadata":{"id":"uC6AkZdChapT"},"source":["# Neural Matrix Factorization from scratch in PyTorch"]},{"cell_type":"markdown","metadata":{"id":"-67Oh2k3uCIW"},"source":["<!-- ------------------------ -->\n","## What you'll learn\n","Duration: 2\n","\n","- Create movielens dataset class in Pytorch\n","- Setting the evaluation criteria\n","- Architecture of neural matrix factorization model\n","- Train and evaluating a neural matrix factorization model"]},{"cell_type":"markdown","metadata":{"id":"slD2OYQvIfhG"},"source":["<!-- ------------------------ -->\n","## Dataset\n","Duration: 5\n","\n","After downloading and expanding the movielens-1m dataset, we will create the dataset class as the first step."]},{"cell_type":"code","metadata":{"id":"kEYXkACmIe-B"},"source":["class Rating_Datset(torch.utils.data.Dataset):\n","\tdef __init__(self, user_list, item_list, rating_list):\n","\t\tsuper(Rating_Datset, self).__init__()\n","\t\tself.user_list = user_list\n","\t\tself.item_list = item_list\n","\t\tself.rating_list = rating_list\n","\n","\tdef __len__(self):\n","\t\treturn len(self.user_list)\n","\n","\tdef __getitem__(self, idx):\n","\t\tuser = self.user_list[idx]\n","\t\titem = self.item_list[idx]\n","\t\trating = self.rating_list[idx]\n","\t\t\n","\t\treturn (\n","\t\t\ttorch.tensor(user, dtype=torch.long),\n","\t\t\ttorch.tensor(item, dtype=torch.long),\n","\t\t\ttorch.tensor(rating, dtype=torch.float)\n","\t\t\t)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HOyBk8riI66H"},"source":["The name of our class is *Rating_Dataset* and it is getting inherited from PyTorch *Dataset* base class. The *__getitem__* method is helping us in 2 ways: 1) It is reinforcing the type to [long, long, float] and returning the tensor version of the tuple for the given index id.\n","\n","We are also creating a helper dataset class to put all the data processing functions under a single umbrella. This class contains 5 methods:\n","\n","- *_reindex*: process dataset to reindex userID and itemID, also set rating as binary feedback\n","- *_leave_one_out*: leave-one-out evaluation protocol in paper [https://www.comp.nus.edu.sg/~xiangnan/papers/ncf.pdf](https://www.comp.nus.edu.sg/~xiangnan/papers/ncf.pdf)\n","- *_negative_sampling*: randomly selects n negative examples for each positive one\n","- *get_train_instance*: merge the examples of train data with negative samples and return the PyTorch dataloader object\n","- *get_test_instance*: merge the examples of test data with negative samples and return the PyTorch dataloader object"]},{"cell_type":"markdown","metadata":{"id":"71Gd05RMI-5i"},"source":["<!-- ------------------------ -->\n","## Evaluation criteria\n","Duration: 5\n","\n","Next, we are defining evaluation metrics. We are using Hit Rate and NDCG as our evaluation metrics."]},{"cell_type":"code","metadata":{"id":"VVd3dzFVI3Tw"},"source":["def hit(ng_item, pred_items):\n","\tif ng_item in pred_items:\n","\t\treturn 1\n","\treturn 0\n","\n","\n","def ndcg(ng_item, pred_items):\n","\tif ng_item in pred_items:\n","\t\tindex = pred_items.index(ng_item)\n","\t\treturn np.reciprocal(np.log2(index+2))\n","\treturn 0\n","\n","\n","def metrics(model, test_loader, top_k, device):\n","\tHR, NDCG = [], []\n","\n","\tfor user, item, label in test_loader:\n","\t\tuser = user.to(device)\n","\t\titem = item.to(device)\n","\n","\t\tpredictions = model(user, item)\n","\t\t_, indices = torch.topk(predictions, top_k)\n","\t\trecommends = torch.take(\n","\t\t\t\titem, indices).cpu().numpy().tolist()\n","\n","\t\tng_item = item[0].item() # leave one-out evaluation has only one item per user\n","\t\tHR.append(hit(ng_item, recommends))\n","\t\tNDCG.append(ndcg(ng_item, recommends))\n","\n","\treturn np.mean(HR), np.mean(NDCG)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ieH6c6OKJVrq"},"source":["The metrics function is first loading the user and item variables to the right device (e.g. to the GPU if it is enabled), then getting predictions from the model and then finally calculating (& returning) the hit_rate_at_k and ndcg_at_k values."]},{"cell_type":"markdown","metadata":{"id":"kzkunJLrJeae"},"source":["<!-- ------------------------ -->\n","## Defining Model Architectures\n","Duration: 10\n","\n","After defining the dataset class and evaluation function, it is time to define the model architecture.\n","\n","We are going to use *Neural Collaborative Filtering for Personalized Ranking*. This model leverages the flexibility and non-linearity of neural networks to replace dot products of matrix factorization, aiming at enhancing the model expressiveness. In specific, this model is structured with two subnetworks including generalized matrix factorization (GMF) and MLP and models the interactions from two pathways instead of simple inner products. The outputs of these two networks are concatenated for the final prediction scores calculation.\n","\n","![nmf_architecture](img/nmf_architecture.png)\n","\n","In this architecture, we are first creating the user and item embedding layers for both MLP and MF architectures, and with the help of PyTorch ModuleList, we are creating MLP architecture. Then, in the forward method, we are passing user and item indices list in the embedding layers and then concatenating and multiplying the MLP and MF embedding layers respectively. And finally, concatenating the MLP and MF feature layers and a logistic activation at the end."]},{"cell_type":"code","metadata":{"id":"2Ywo9NU-K7yy"},"source":["class NeuMF(nn.Module):\n","    def __init__(self, args, num_users, num_items):\n","        super(NeuMF, self).__init__()\n","        self.num_users = num_users\n","        self.num_items = num_items\n","        self.factor_num_mf = args.factor_num\n","        self.factor_num_mlp =  int(args.layers[0]/2)\n","        self.layers = args.layers\n","        self.dropout = args.dropout\n","\n","        self.embedding_user_mlp = nn.Embedding(num_embeddings=self.num_users, embedding_dim=self.factor_num_mlp)\n","        self.embedding_item_mlp = nn.Embedding(num_embeddings=self.num_items, embedding_dim=self.factor_num_mlp)\n","\n","        self.embedding_user_mf = nn.Embedding(num_embeddings=self.num_users, embedding_dim=self.factor_num_mf)\n","        self.embedding_item_mf = nn.Embedding(num_embeddings=self.num_items, embedding_dim=self.factor_num_mf)\n","\n","        self.fc_layers = nn.ModuleList()\n","        for idx, (in_size, out_size) in enumerate(zip(args.layers[:-1], args.layers[1:])):\n","            self.fc_layers.append(torch.nn.Linear(in_size, out_size))\n","            self.fc_layers.append(nn.ReLU())\n","\n","        self.affine_output = nn.Linear(in_features=args.layers[-1] + self.factor_num_mf, out_features=1)\n","        self.logistic = nn.Sigmoid()\n","        self.init_weight()\n","\n","    def init_weight(self):\n","        nn.init.normal_(self.embedding_user_mlp.weight, std=0.01)\n","        nn.init.normal_(self.embedding_item_mlp.weight, std=0.01)\n","        nn.init.normal_(self.embedding_user_mf.weight, std=0.01)\n","        nn.init.normal_(self.embedding_item_mf.weight, std=0.01)\n","        \n","        for m in self.fc_layers:\n","            if isinstance(m, nn.Linear):\n","                nn.init.xavier_uniform_(m.weight)\n","                \n","        nn.init.xavier_uniform_(self.affine_output.weight)\n","\n","        for m in self.modules():\n","            if isinstance(m, nn.Linear) and m.bias is not None:\n","                m.bias.data.zero_()\n","\n","    def forward(self, user_indices, item_indices):\n","        user_embedding_mlp = self.embedding_user_mlp(user_indices)\n","        item_embedding_mlp = self.embedding_item_mlp(item_indices)\n","\n","        user_embedding_mf = self.embedding_user_mf(user_indices)\n","        item_embedding_mf = self.embedding_item_mf(item_indices)\n","\n","        mlp_vector = torch.cat([user_embedding_mlp, item_embedding_mlp], dim=-1)\n","        mf_vector =torch.mul(user_embedding_mf, item_embedding_mf)\n","\n","        for idx, _ in enumerate(range(len(self.fc_layers))):\n","            mlp_vector = self.fc_layers[idx](mlp_vector)\n","\n","        vector = torch.cat([mlp_vector, mf_vector], dim=-1)\n","        logits = self.affine_output(vector)\n","        rating = self.logistic(logits)\n","        return rating.squeeze()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"CcyotvWVLiBo"},"source":["<!-- ------------------------ -->\n","## Training and evaluation\n","Duration: 10\n","\n","We are using following hyperparameters to train the model:\n","- Learning rate is 0.001\n","- Dropout rate is 0.2\n","- Running for 10 epochs\n","- HitRate@10 and NDCG@10\n","- 4 negative samples for each positive one"]},{"cell_type":"code","metadata":{"id":"lMKJ2FJrL27U"},"source":["args = parser.parse_args(\"\")\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","writer = SummaryWriter()\n","\n","# seed for Reproducibility\n","seed_everything(args.seed)\n","\n","# load data\n","ml_1m = pd.read_csv(\n","\tDATA_PATH, \n","\tsep=\"::\", \n","\tnames = ['user_id', 'item_id', 'rating', 'timestamp'], \n","\tengine='python')\n","\n","# set the num_users, items\n","num_users = ml_1m['user_id'].nunique()+1\n","num_items = ml_1m['item_id'].nunique()+1\n","\n","# construct the train and test datasets\n","data = NCF_Data(args, ml_1m)\n","train_loader = data.get_train_instance()\n","test_loader = data.get_test_instance()\n","\n","# set model and loss, optimizer\n","model = NeuMF(args, num_users, num_items)\n","model = model.to(device)\n","loss_function = nn.BCELoss()\n","optimizer = optim.Adam(model.parameters(), lr=args.lr)\n","\n","# train, evaluation\n","best_hr = 0\n","for epoch in range(1, args.epochs+1):\n","\tmodel.train() # Enable dropout (if have).\n","\tstart_time = time.time()\n","\n","\tfor user, item, label in train_loader:\n","\t\tuser = user.to(device)\n","\t\titem = item.to(device)\n","\t\tlabel = label.to(device)\n","\n","\t\toptimizer.zero_grad()\n","\t\tprediction = model(user, item)\n","\t\tloss = loss_function(prediction, label)\n","\t\tloss.backward()\n","\t\toptimizer.step()\n","\t\twriter.add_scalar('loss/Train_loss', loss.item(), epoch)\n","\n","\tmodel.eval()\n","\tHR, NDCG = metrics(model, test_loader, args.top_k, device)\n","\twriter.add_scalar('Perfomance/HR@10', HR, epoch)\n","\twriter.add_scalar('Perfomance/NDCG@10', NDCG, epoch)\n","\n","\telapsed_time = time.time() - start_time\n","\tprint(\"The time elapse of epoch {:03d}\".format(epoch) + \" is: \" + \n","\t\t\ttime.strftime(\"%H: %M: %S\", time.gmtime(elapsed_time)))\n","\tprint(\"HR: {:.3f}\\tNDCG: {:.3f}\".format(np.mean(HR), np.mean(NDCG)))\n","\n","\tif HR > best_hr:\n","\t\tbest_hr, best_ndcg, best_epoch = HR, NDCG, epoch\n","\t\tif args.out:\n","\t\t\tif not os.path.exists(MODEL_PATH):\n","\t\t\t\tos.mkdir(MODEL_PATH)\n","\t\t\ttorch.save(model, \n","\t\t\t\t'{}{}.pth'.format(MODEL_PATH, MODEL))\n","\n","writer.close()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"a09fgGhEMALC"},"source":["Average epoch time is 90 seconds on Nvidia T4 GPU. Both hit_rate and ndcg values improves initially for first 4 epochs and then converged to a local (or global, I hope) minima."]},{"cell_type":"markdown","metadata":{"id":"Hq-Tn1PsMXXq"},"source":["<!-- ------------------------ -->\n","## Congratulations\n","Duration: 2\n","\n","Congratulations! We covered a lot of content and hopefully you have a better understanding of the working of neural matrix factorization model by now.\n","\n","### What we've covered\n","- Create movielens dataset class in Pytorch\n","- Setting the evaluation criteria\n","- Architecture of neural matrix factorization model\n","- Train and evaluating a neural matrix factorization model\n","\n","### Resources\n","- [Colab notebook](https://sparsh-ai.github.io/rec-tutorials/matrixfactorization%20movielens%20pytorch%20scratch/2021/04/21/rec-algo-ncf-pytorch-pyy0715.html)\n","\n","### Next Steps\n","- Notebook based tutorials [here](https://sparsh-ai.github.io/rec-tutorials/)\n","- Read NMF Paper on [Arxiv](https://arxiv.org/abs/1511.06443)\n","- Continue learning by following [this](https://medium.com/@lz2576/a-first-look-at-recommendation-system-with-matrix-factorization-and-neural-nets-7e21e54295c) medium post\n","\n","#### Have a Question?\n","- https://form.jotform.com/211377288388469\n","\n","#### Github Issues\n","- https://github.com/sparsh-ai/reco-tutorials/issues"]}]}