{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"T874693 | Evaluating Standard Off-Policy Estimators with Small Sample Open-Bandit Dataset","provenance":[],"collapsed_sections":[],"toc_visible":true,"mount_file_id":"1AYSeQTpwN8gnX2UlPwTH_sU11WG0kOHj","authorship_tag":"ABX9TyPxjXiMEYU21hrrJcGFPNa1"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"5N3T7I8gMpUz"},"source":["# Evaluating Standard Off-Policy Estimators with Small Sample Open-Bandit Dataset"]},{"cell_type":"markdown","metadata":{"id":"YIFTbouJ7ou_"},"source":["## Overview"]},{"cell_type":"markdown","metadata":{"id":"DkER8cqYhOBP"},"source":["**Evaluating 3 standard off-policy estimators (DirectMethod, DoublyRobust, and InverseProbabilityWeighting) on small sample open-bandit dataset**.\n","\n","These OPE estimators will estimate the performance of BernoulliTS policy (counterfactual/evaluation policy) using data generated by Random policy (behavior policy). "]},{"cell_type":"markdown","metadata":{"id":"T95drMPRKjIr"},"source":["Imports"]},{"cell_type":"code","metadata":{"id":"wCK4RHz7nG9T"},"source":["import argparse\n","from pathlib import Path\n","\n","from joblib import delayed\n","from joblib import Parallel\n","import numpy as np\n","from pandas import DataFrame\n","from sklearn.ensemble import GradientBoostingClassifier\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.linear_model import LogisticRegression\n","import yaml"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dOlWZUldnJ-l"},"source":["## Dataloader"]},{"cell_type":"markdown","metadata":{"id":"EFhh5oX9oava"},"source":["Abstract Base Class for Logged Bandit Feedback."]},{"cell_type":"code","metadata":{"id":"t8CSGqAqoqdV"},"source":["from abc import ABCMeta\n","from abc import abstractmethod"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"24jN9OXmoonK"},"source":["class BaseBanditDataset(metaclass=ABCMeta):\n","    \"\"\"Base Class for Synthetic Bandit Dataset.\"\"\"\n","\n","    @abstractmethod\n","    def obtain_batch_bandit_feedback(self) -> None:\n","        \"\"\"Obtain batch logged bandit feedback.\"\"\"\n","        raise NotImplementedError"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NXKutk-in_8U"},"source":["class BaseRealBanditDataset(BaseBanditDataset):\n","    \"\"\"Base Class for Real-World Bandit Dataset.\"\"\"\n","\n","    @abstractmethod\n","    def load_raw_data(self) -> None:\n","        \"\"\"Load raw dataset.\"\"\"\n","        raise NotImplementedError\n","\n","    @abstractmethod\n","    def pre_process(self) -> None:\n","        \"\"\"Preprocess raw dataset.\"\"\"\n","        raise NotImplementedError"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Ooe-aPK8nu80"},"source":["Dataset Class for Real-World Logged Bandit Feedback."]},{"cell_type":"code","metadata":{"id":"zvKs8F2unxf6"},"source":["from dataclasses import dataclass\n","from logging import basicConfig\n","from logging import getLogger\n","from logging import INFO\n","from pathlib import Path\n","from typing import Optional\n","from typing import Tuple\n","from typing import Union\n","from typing import Dict\n","\n","import numpy as np\n","import pandas as pd\n","from scipy.stats import rankdata\n","from sklearn.preprocessing import LabelEncoder\n","from sklearn.utils import check_random_state\n","from sklearn.utils import check_scalar"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"IICUDW5xoUfd"},"source":["# dataset type\n","BanditFeedback = Dict[str, Union[int, np.ndarray]]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"vb887BtroVYL"},"source":["logger = getLogger(__name__)\n","basicConfig(level=INFO)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Q9SHoGzC2Qy_"},"source":["OBD_DATA_PATH = '/content/zr-obp/obd'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"43D2q4iqog--"},"source":["@dataclass\n","class OpenBanditDataset(BaseRealBanditDataset):\n","    \"\"\"Class for loading and preprocessing Open Bandit Dataset.\n","    Note\n","    -----\n","    Users are free to implement their own feature engineering by overriding the `pre_process` method.\n","    Parameters\n","    -----------\n","    behavior_policy: str\n","        Name of the behavior policy that generated the logged bandit feedback data.\n","        Must be either 'random' or 'bts'.\n","    campaign: str\n","        One of the three possible campaigns considered in ZOZOTOWN.\n","        Must be one of \"all\", \"men\", or \"women\".\n","    data_path: str or Path, default=None\n","        Path where the Open Bandit Dataset is stored.\n","    dataset_name: str, default='obd'\n","        Name of the dataset.\n","    References\n","    ------------\n","    Yuta Saito, Shunsuke Aihara, Megumi Matsutani, Yusuke Narita.\n","    \"Open Bandit Dataset and Pipeline: Towards Realistic and Reproducible Off-Policy Evaluation.\", 2020.\n","    \"\"\"\n","\n","    behavior_policy: str\n","    campaign: str\n","    data_path: Optional[Union[str, Path]] = None\n","    dataset_name: str = \"obd\"\n","\n","    def __post_init__(self) -> None:\n","        \"\"\"Initialize Open Bandit Dataset Class.\"\"\"\n","        if self.behavior_policy not in [\n","            \"bts\",\n","            \"random\",\n","        ]:\n","            raise ValueError(\n","                f\"behavior_policy must be either of 'bts' or 'random', but {self.behavior_policy} is given\"\n","            )\n","\n","        if self.campaign not in [\n","            \"all\",\n","            \"men\",\n","            \"women\",\n","        ]:\n","            raise ValueError(\n","                f\"campaign must be one of 'all', 'men', or 'women', but {self.campaign} is given\"\n","            )\n","\n","        if self.data_path is None:\n","            self.data_path = Path(OBD_DATA_PATH)\n","        else:\n","            if isinstance(self.data_path, Path):\n","                pass\n","            elif isinstance(self.data_path, str):\n","                self.data_path = Path(self.data_path)\n","            else:\n","                raise ValueError(\"data_path must be a string or Path\")\n","        self.data_path = self.data_path / self.behavior_policy / self.campaign\n","        self.raw_data_file = f\"{self.campaign}.csv\"\n","\n","        self.load_raw_data()\n","        self.pre_process()\n","\n","    @property\n","    def n_rounds(self) -> int:\n","        \"\"\"Total number of rounds contained in the logged bandit dataset.\"\"\"\n","        return self.data.shape[0]\n","\n","    @property\n","    def n_actions(self) -> int:\n","        \"\"\"Number of actions.\"\"\"\n","        return int(self.action.max() + 1)\n","\n","    @property\n","    def dim_context(self) -> int:\n","        \"\"\"Dimensions of context vectors.\"\"\"\n","        return self.context.shape[1]\n","\n","    @property\n","    def len_list(self) -> int:\n","        \"\"\"Length of recommendation lists.\"\"\"\n","        return int(self.position.max() + 1)\n","\n","    @classmethod\n","    def calc_on_policy_policy_value_estimate(\n","        cls,\n","        behavior_policy: str,\n","        campaign: str,\n","        data_path: Optional[Path] = None,\n","        test_size: float = 0.3,\n","        is_timeseries_split: bool = False,\n","    ) -> float:\n","        \"\"\"Calculate on-policy policy value estimate (used as a ground-truth policy value).\n","        Parameters\n","        ----------\n","        behavior_policy: str\n","            Name of the behavior policy that generated the log data.\n","            Must be either 'random' or 'bts'.\n","        campaign: str\n","            One of the three possible campaigns considered in ZOZOTOWN (i.e., \"all\", \"men\", and \"women\").\n","        data_path: Path, default=None\n","            Path where the Open Bandit Dataset exists.\n","        test_size: float, default=0.3\n","            Proportion of the dataset included in the test split.\n","            If float, should be between 0.0 and 1.0.\n","            This argument matters only when `is_timeseries_split=True` (the out-sample case).\n","        is_timeseries_split: bool, default=False\n","            If true, split the original logged bandit feedback data by time series.\n","        Returns\n","        ---------\n","        on_policy_policy_value_estimate: float\n","            Policy value of the behavior policy estimated by on-policy estimation, i.e., :math:`\\\\mathbb{E}_{\\\\mathcal{D}} [r_t]`.\n","            where :math:`\\\\mathbb{E}_{\\\\mathcal{D}}[\\\\cdot]` is the empirical average over :math:`T` observations in :math:`\\\\mathcal{D}`.\n","            This parameter is used as a ground-truth policy value in the evaluation of OPE estimators.\n","        \"\"\"\n","        bandit_feedback = cls(\n","            behavior_policy=behavior_policy, campaign=campaign, data_path=data_path\n","        ).obtain_batch_bandit_feedback(\n","            test_size=test_size, is_timeseries_split=is_timeseries_split\n","        )\n","        if is_timeseries_split:\n","            bandit_feedback_test = bandit_feedback[1]\n","        else:\n","            bandit_feedback_test = bandit_feedback\n","        return bandit_feedback_test[\"reward\"].mean()\n","\n","    def load_raw_data(self) -> None:\n","        \"\"\"Load raw open bandit dataset.\"\"\"\n","        self.data = pd.read_csv(self.data_path / self.raw_data_file, index_col=0)\n","        self.item_context = pd.read_csv(\n","            self.data_path / \"item_context.csv\", index_col=0\n","        )\n","        self.data.sort_values(\"timestamp\", inplace=True)\n","        self.action = self.data[\"item_id\"].values\n","        self.position = (rankdata(self.data[\"position\"].values, \"dense\") - 1).astype(\n","            int\n","        )\n","        self.reward = self.data[\"click\"].values\n","        self.pscore = self.data[\"propensity_score\"].values\n","\n","    def pre_process(self) -> None:\n","        \"\"\"Preprocess raw open bandit dataset.\n","        Note\n","        -----\n","        This is the default feature engineering and please override this method to\n","        implement your own preprocessing.\n","        see https://github.com/st-tech/zr-obp/blob/master/examples/examples_with_obd/custom_dataset.py for example.\n","        \"\"\"\n","        user_cols = self.data.columns.str.contains(\"user_feature\")\n","        self.context = pd.get_dummies(\n","            self.data.loc[:, user_cols], drop_first=True\n","        ).values\n","        item_feature_0 = self.item_context[\"item_feature_0\"]\n","        item_feature_cat = self.item_context.drop(\"item_feature_0\", 1).apply(\n","            LabelEncoder().fit_transform\n","        )\n","        self.action_context = pd.concat([item_feature_cat, item_feature_0], 1).values\n","\n","    def obtain_batch_bandit_feedback(\n","        self, test_size: float = 0.3, is_timeseries_split: bool = False\n","    ) -> Union[BanditFeedback, Tuple[BanditFeedback, BanditFeedback]]:\n","        \"\"\"Obtain batch logged bandit feedback.\n","        Parameters\n","        -----------\n","        test_size: float, default=0.3\n","            Proportion of the dataset included in the test split.\n","            If float, should be between 0.0 and 1.0.\n","            This argument matters only when `is_timeseries_split=True` (the out-sample case).\n","        is_timeseries_split: bool, default=False\n","            If true, split the original logged bandit feedback data into train and test sets based on time series.\n","        Returns\n","        --------\n","        bandit_feedback: BanditFeedback\n","            A dictionary containing batch logged bandit feedback data collected by a behavior policy.\n","            The keys of the dictionary are as follows.\n","            - n_rounds: number of rounds (size) of the logged bandit data\n","            - n_actions: number of actions (:math:`|\\mathcal{A}|`)\n","            - action: action variables sampled by a behavior policy\n","            - position: positions where actions are recommended\n","            - reward: reward variables\n","            - pscore: action choice probabilities by a behavior policy\n","            - context: context vectors such as user-related features and user-item affinity scores\n","            - action_context: item-related context vectors\n","        \"\"\"\n","        if not isinstance(is_timeseries_split, bool):\n","            raise TypeError(\n","                f\"`is_timeseries_split` must be a bool, but {type(is_timeseries_split)} is given\"\n","            )\n","\n","        if is_timeseries_split:\n","            check_scalar(\n","                test_size,\n","                name=\"target_size\",\n","                target_type=(float),\n","                min_val=0.0,\n","                max_val=1.0,\n","            )\n","            n_rounds_train = np.int(self.n_rounds * (1.0 - test_size))\n","            bandit_feedback_train = dict(\n","                n_rounds=n_rounds_train,\n","                n_actions=self.n_actions,\n","                action=self.action[:n_rounds_train],\n","                position=self.position[:n_rounds_train],\n","                reward=self.reward[:n_rounds_train],\n","                pscore=self.pscore[:n_rounds_train],\n","                context=self.context[:n_rounds_train],\n","                action_context=self.action_context,\n","            )\n","            bandit_feedback_test = dict(\n","                n_rounds=(self.n_rounds - n_rounds_train),\n","                n_actions=self.n_actions,\n","                action=self.action[n_rounds_train:],\n","                position=self.position[n_rounds_train:],\n","                reward=self.reward[n_rounds_train:],\n","                pscore=self.pscore[n_rounds_train:],\n","                context=self.context[n_rounds_train:],\n","                action_context=self.action_context,\n","            )\n","            return bandit_feedback_train, bandit_feedback_test\n","        else:\n","            return dict(\n","                n_rounds=self.n_rounds,\n","                n_actions=self.n_actions,\n","                action=self.action,\n","                position=self.position,\n","                reward=self.reward,\n","                pscore=self.pscore,\n","                context=self.context,\n","                action_context=self.action_context,\n","            )\n","\n","    def sample_bootstrap_bandit_feedback(\n","        self,\n","        sample_size: Optional[int] = None,\n","        test_size: float = 0.3,\n","        is_timeseries_split: bool = False,\n","        random_state: Optional[int] = None,\n","    ) -> BanditFeedback:\n","        \"\"\"Obtain bootstrap logged bandit feedback.\n","        Parameters\n","        -----------\n","        sample_size: int, default=None\n","            Number of data sampled by bootstrap.\n","            When None is given, the original data size (n_rounds) is used as `sample_size`.\n","            The value must be smaller than the original data size.\n","        test_size: float, default=0.3\n","            Proportion of the dataset included in the test split.\n","            If float, should be between 0.0 and 1.0.\n","            This argument matters only when `is_timeseries_split=True` (the out-sample case).\n","        is_timeseries_split: bool, default=False\n","            If true, split the original logged bandit feedback data into train and test sets based on time series.\n","        random_state: int, default=None\n","            Controls the random seed in bootstrap sampling.\n","        Returns\n","        --------\n","        bandit_feedback: BanditFeedback\n","            A dictionary containing logged bandit feedback data sampled independently from the original data with replacement.\n","            The keys of the dictionary are as follows.\n","            - n_rounds: number of rounds (size) of the logged bandit data\n","            - n_actions: number of actions\n","            - action: action variables sampled by a behavior policy\n","            - position: positions where actions are recommended by a behavior policy\n","            - reward: reward variables\n","            - pscore: action choice probabilities by a behavior policy\n","            - context: context vectors such as user-related features and user-item affinity scores\n","            - action_context: item-related context vectors\n","        \"\"\"\n","        if is_timeseries_split:\n","            bandit_feedback = self.obtain_batch_bandit_feedback(\n","                test_size=test_size, is_timeseries_split=is_timeseries_split\n","            )[0]\n","        else:\n","            bandit_feedback = self.obtain_batch_bandit_feedback(\n","                test_size=test_size, is_timeseries_split=is_timeseries_split\n","            )\n","        n_rounds = bandit_feedback[\"n_rounds\"]\n","        if sample_size is None:\n","            sample_size = bandit_feedback[\"n_rounds\"]\n","        else:\n","            check_scalar(\n","                sample_size,\n","                name=\"sample_size\",\n","                target_type=(int),\n","                min_val=0,\n","                max_val=n_rounds,\n","            )\n","        random_ = check_random_state(random_state)\n","        bootstrap_idx = random_.choice(\n","            np.arange(n_rounds), size=sample_size, replace=True\n","        )\n","        for key_ in [\"action\", \"position\", \"reward\", \"pscore\", \"context\"]:\n","            bandit_feedback[key_] = bandit_feedback[key_][bootstrap_idx]\n","        bandit_feedback[\"n_rounds\"] = sample_size\n","        return bandit_feedback"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Wnu9ngfUnMTq"},"source":["## OPE Estimators"]},{"cell_type":"markdown","metadata":{"id":"4hTFxj81pLEW"},"source":["### Utils"]},{"cell_type":"code","metadata":{"id":"ExEdhjf8pjtl"},"source":["from abc import ABCMeta\n","from abc import abstractmethod\n","from dataclasses import dataclass\n","from typing import Dict\n","from typing import Optional\n","from typing import Union\n","\n","import numpy as np\n","import pandas as pd\n","from sklearn.utils import check_random_state\n","from sklearn.utils import check_scalar\n","import torch"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QCVreKJQqRZM"},"source":["def check_array(\n","    array: np.ndarray,\n","    name: str,\n","    expected_dim: int = 1,\n",") -> ValueError:\n","    \"\"\"Input validation on an array.\n","    Parameters\n","    -------------\n","    array: object\n","        Input object to check.\n","    name: str\n","        Name of the input array.\n","    expected_dim: int, default=1\n","        Expected dimension of the input array.\n","    \"\"\"\n","    if not isinstance(array, np.ndarray):\n","        raise ValueError(f\"{name} must be {expected_dim}D array, but got {type(array)}\")\n","    if array.ndim != expected_dim:\n","        raise ValueError(\n","            f\"{name} must be {expected_dim}D array, but got {array.ndim}D array\"\n","        )"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"v8OJMwMdsUro"},"source":["def check_confidence_interval_arguments(\n","    alpha: float = 0.05,\n","    n_bootstrap_samples: int = 10000,\n","    random_state: Optional[int] = None,\n",") -> Optional[ValueError]:\n","    \"\"\"Check confidence interval arguments.\n","    Parameters\n","    ----------\n","    alpha: float, default=0.05\n","        Significance level.\n","    n_bootstrap_samples: int, default=10000\n","        Number of resampling performed in the bootstrap procedure.\n","    random_state: int, default=None\n","        Controls the random seed in bootstrap sampling.\n","    Returns\n","    ----------\n","    estimated_confidence_interval: Dict[str, float]\n","        Dictionary storing the estimated mean and upper-lower confidence bounds.\n","    \"\"\"\n","    check_random_state(random_state)\n","    check_scalar(alpha, \"alpha\", float, min_val=0.0, max_val=1.0)\n","    check_scalar(n_bootstrap_samples, \"n_bootstrap_samples\", int, min_val=1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zLZoBWRFqDBG"},"source":["def estimate_confidence_interval_by_bootstrap(\n","    samples: np.ndarray,\n","    alpha: float = 0.05,\n","    n_bootstrap_samples: int = 10000,\n","    random_state: Optional[int] = None,\n",") -> Dict[str, float]:\n","    \"\"\"Estimate confidence interval by nonparametric bootstrap-like procedure.\n","    Parameters\n","    ----------\n","    samples: array-like\n","        Empirical observed samples to be used to estimate cumulative distribution function.\n","    alpha: float, default=0.05\n","        Significance level.\n","    n_bootstrap_samples: int, default=10000\n","        Number of resampling performed in the bootstrap procedure.\n","    random_state: int, default=None\n","        Controls the random seed in bootstrap sampling.\n","    Returns\n","    ----------\n","    estimated_confidence_interval: Dict[str, float]\n","        Dictionary storing the estimated mean and upper-lower confidence bounds.\n","    \"\"\"\n","    check_confidence_interval_arguments(\n","        alpha=alpha, n_bootstrap_samples=n_bootstrap_samples, random_state=random_state\n","    )\n","\n","    boot_samples = list()\n","    random_ = check_random_state(random_state)\n","    for _ in np.arange(n_bootstrap_samples):\n","        boot_samples.append(np.mean(random_.choice(samples, size=samples.shape[0])))\n","    lower_bound = np.percentile(boot_samples, 100 * (alpha / 2))\n","    upper_bound = np.percentile(boot_samples, 100 * (1.0 - alpha / 2))\n","    return {\n","        \"mean\": np.mean(boot_samples),\n","        f\"{100 * (1. - alpha)}% CI (lower)\": lower_bound,\n","        f\"{100 * (1. - alpha)}% CI (upper)\": upper_bound,\n","    }"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"lJq8KBjYqe1p"},"source":["def check_ope_inputs(\n","    action_dist: np.ndarray,\n","    position: Optional[np.ndarray] = None,\n","    action: Optional[np.ndarray] = None,\n","    reward: Optional[np.ndarray] = None,\n","    pscore: Optional[np.ndarray] = None,\n","    estimated_rewards_by_reg_model: Optional[np.ndarray] = None,\n",") -> Optional[ValueError]:\n","    \"\"\"Check inputs for ope.\n","    Parameters\n","    -----------\n","    action_dist: array-like, shape (n_rounds, n_actions, len_list)\n","        Action choice probabilities of evaluation policy (can be deterministic), i.e., :math:`\\\\pi_e(a_t|x_t)`.\n","    position: array-like, shape (n_rounds,), default=None\n","        Position of recommendation interface where action was presented in each round of the given logged bandit data.\n","    action: array-like, shape (n_rounds,), default=None\n","        Action sampled by behavior policy in each round of the logged bandit feedback, i.e., :math:`a_t`.\n","    reward: array-like, shape (n_rounds,), default=None\n","        Observed rewards (or outcome) in each round, i.e., :math:`r_t`.\n","    pscore: array-like, shape (n_rounds,), default=None\n","        Propensity scores, the probability of selecting each action by behavior policy,\n","        in the given logged bandit data.\n","    estimated_rewards_by_reg_model: array-like, shape (n_rounds, n_actions, len_list), default=None\n","        Expected rewards given context, action, and position estimated by regression model, i.e., :math:`\\\\hat{q}(x_t,a_t)`.\n","    \"\"\"\n","    # action_dist\n","    check_array(array=action_dist, name=\"action_dist\", expected_dim=3)\n","    if not np.allclose(action_dist.sum(axis=1), 1):\n","        raise ValueError(\"action_dist must be a probability distribution\")\n","\n","    # position\n","    if position is not None:\n","        check_array(array=position, name=\"position\", expected_dim=1)\n","        if not (position.shape[0] == action_dist.shape[0]):\n","            raise ValueError(\n","                \"Expected `position.shape[0] == action_dist.shape[0]`, but found it False\"\n","            )\n","        if not (np.issubdtype(position.dtype, np.integer) and position.min() >= 0):\n","            raise ValueError(\"position elements must be non-negative integers\")\n","        if position.max() >= action_dist.shape[2]:\n","            raise ValueError(\n","                \"position elements must be smaller than `action_dist.shape[2]`\"\n","            )\n","    elif action_dist.shape[2] > 1:\n","        raise ValueError(\n","            \"position elements must be given when `action_dist.shape[2] > 1`\"\n","        )\n","\n","    # estimated_rewards_by_reg_model\n","    if estimated_rewards_by_reg_model is not None:\n","        if estimated_rewards_by_reg_model.shape != action_dist.shape:\n","            raise ValueError(\n","                \"Expected `estimated_rewards_by_reg_model.shape == action_dist.shape`, but found it False\"\n","            )\n","\n","    # action, reward\n","    if action is not None or reward is not None:\n","        check_array(array=action, name=\"action\", expected_dim=1)\n","        check_array(array=reward, name=\"reward\", expected_dim=1)\n","        if not (action.shape[0] == reward.shape[0]):\n","            raise ValueError(\n","                \"Expected `action.shape[0] == reward.shape[0]`, but found it False\"\n","            )\n","        if not (np.issubdtype(action.dtype, np.integer) and action.min() >= 0):\n","            raise ValueError(\"action elements must be non-negative integers\")\n","        if action.max() >= action_dist.shape[1]:\n","            raise ValueError(\n","                \"action elements must be smaller than `action_dist.shape[1]`\"\n","            )\n","\n","    # pscore\n","    if pscore is not None:\n","        if pscore.ndim != 1:\n","            raise ValueError(\"pscore must be 1-dimensional\")\n","        if not (action.shape[0] == reward.shape[0] == pscore.shape[0]):\n","            raise ValueError(\n","                \"Expected `action.shape[0] == reward.shape[0] == pscore.shape[0]`, but found it False\"\n","            )\n","        if np.any(pscore <= 0):\n","            raise ValueError(\"pscore must be positive\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Q_yhHvbtqtaC"},"source":["def estimate_bias_in_ope(\n","    reward: np.ndarray,\n","    iw: np.ndarray,\n","    iw_hat: np.ndarray,\n","    q_hat: Optional[np.ndarray] = None,\n",") -> float:\n","    \"\"\"Helper to estimate a bias in OPE.\n","    Parameters\n","    ----------\n","    reward: array-like, shape (n_rounds,)\n","        Reward observed in each round of the logged bandit feedback, i.e., :math:`r_t`.\n","    iw: array-like, shape (n_rounds,)\n","        Importance weight in each round of the logged bandit feedback, i.e., :math:`w(x,a)=\\\\pi_e(a|x)/ \\\\pi_b(a|x)`.\n","    iw_hat: array-like, shape (n_rounds,)\n","        Importance weight (IW) modified by a hyparpareter. How IW is modified depends on the estimator as follows.\n","            - clipping: :math:`\\\\hat{w}(x,a) := \\\\min \\\\{ \\\\lambda, w(x,a) \\\\}`\n","            - switching: :math:`\\\\hat{w}(x,a) := w(x,a) \\\\cdot \\\\mathbb{I} \\\\{ w(x,a) < \\\\lambda \\\\}`\n","            - shrinkage: :math:`\\\\hat{w}(x,a) := (\\\\lambda w(x,a)) / (\\\\lambda + w^2(x,a))`\n","        where :math:`\\\\lambda` is a hyperparameter value.\n","    q_hat: array-like, shape (n_rounds,), default=None\n","        Estimated expected reward given context :math:`x_t` and action :math:`a_t`.\n","    Returns\n","    ----------\n","    estimated_bias: float\n","        Estimated the bias in OPE.\n","        This is based on the direct bias estimation stated on page 17 of Su et al.(2020).\n","    References\n","    ----------\n","    Yi Su, Maria Dimakopoulou, Akshay Krishnamurthy, and Miroslav Dudik.\n","    \"Doubly Robust Off-Policy Evaluation with Shrinkage.\", 2020.\n","    \"\"\"\n","    n_rounds = reward.shape[0]\n","    if q_hat is None:\n","        q_hat = np.zeros(n_rounds)\n","    estimated_bias_arr = (iw - iw_hat) * (reward - q_hat)\n","    estimated_bias = np.abs(estimated_bias_arr.mean())\n","\n","    return estimated_bias"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"P1_jQjDvqERd"},"source":["### Abstract Class"]},{"cell_type":"code","metadata":{"id":"stB6HUE1psIm"},"source":["@dataclass\n","class BaseOffPolicyEstimator(metaclass=ABCMeta):\n","    \"\"\"Base class for OPE estimators.\"\"\"\n","\n","    @abstractmethod\n","    def _estimate_round_rewards(self) -> np.ndarray:\n","        \"\"\"Estimate round-wise (or sample-wise) rewards.\"\"\"\n","        raise NotImplementedError\n","\n","    @abstractmethod\n","    def estimate_policy_value(self) -> float:\n","        \"\"\"Estimate the policy value of evaluation policy.\"\"\"\n","        raise NotImplementedError\n","\n","    @abstractmethod\n","    def estimate_interval(self) -> Dict[str, float]:\n","        \"\"\"Estimate confidence interval of policy value by nonparametric bootstrap procedure.\"\"\"\n","        raise NotImplementedError"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qPeYSo7wpMv9"},"source":["### Direct Method"]},{"cell_type":"code","metadata":{"id":"tj-yXjd8pRDN"},"source":["@dataclass\n","class DirectMethod(BaseOffPolicyEstimator):\n","    \"\"\"Direct Method (DM).\n","    Note\n","    -------\n","    DM first learns a supervised machine learning model, such as ridge regression and gradient boosting,\n","    to estimate the mean reward function (:math:`q(x,a) = \\\\mathbb{E}[r|x,a]`).\n","    It then uses it to estimate the policy value as follows.\n","    .. math::\n","        \\\\hat{V}_{\\\\mathrm{DM}} (\\\\pi_e; \\\\mathcal{D}, \\\\hat{q})\n","        &:= \\\\mathbb{E}_{\\\\mathcal{D}} \\\\left[ \\\\sum_{a \\\\in \\\\mathcal{A}} \\\\hat{q} (x_t,a) \\\\pi_e(a|x_t) \\\\right],    \\\\\\\\\n","        & =  \\\\mathbb{E}_{\\\\mathcal{D}}[\\\\hat{q} (x_t,\\\\pi_e)],\n","    where :math:`\\\\mathcal{D}=\\\\{(x_t,a_t,r_t)\\\\}_{t=1}^{T}` is logged bandit feedback data with :math:`T` rounds collected by\n","    a behavior policy :math:`\\\\pi_b`. :math:`\\\\mathbb{E}_{\\\\mathcal{D}}[\\\\cdot]` is the empirical average over :math:`T` observations in :math:`\\\\mathcal{D}`.\n","    :math:`\\\\hat{q} (x,a)` is an estimated expected reward given :math:`x` and :math:`a`.\n","    :math:`\\\\hat{q} (x_t,\\\\pi):= \\\\mathbb{E}_{a \\\\sim \\\\pi(a|x)}[\\\\hat{q}(x,a)]` is the expectation of the estimated reward function over :math:`\\\\pi`.\n","    To estimate the mean reward function, please use `obp.ope.regression_model.RegressionModel`, which supports several fitting methods specific to OPE.\n","    If the regression model (:math:`\\\\hat{q}`) is a good approximation to the true mean reward function,\n","    this estimator accurately estimates the policy value of the evaluation policy.\n","    If the regression function fails to approximate the mean reward function well,\n","    however, the final estimator is no longer consistent.\n","    Parameters\n","    ----------\n","    estimator_name: str, default='dm'.\n","        Name of the estimator.\n","    References\n","    ----------\n","    Alina Beygelzimer and John Langford.\n","    \"The offset tree for learning with partial labels.\", 2009.\n","    Miroslav Dudík, Dumitru Erhan, John Langford, and Lihong Li.\n","    \"Doubly Robust Policy Evaluation and Optimization.\", 2014.\n","    \"\"\"\n","\n","    estimator_name: str = \"dm\"\n","\n","    def _estimate_round_rewards(\n","        self,\n","        action_dist: np.ndarray,\n","        estimated_rewards_by_reg_model: np.ndarray,\n","        position: Optional[np.ndarray] = None,\n","        **kwargs,\n","    ) -> np.ndarray:\n","        \"\"\"Estimate the policy value of evaluation policy.\n","        Parameters\n","        ----------\n","        action_dist: array-like or Tensor, shape (n_rounds, n_actions, len_list)\n","            Action choice probabilities of evaluation policy (can be deterministic), i.e., :math:`\\\\pi_e(a_t|x_t)`.\n","        estimated_rewards_by_reg_model: array-like or Tensor, shape (n_rounds, n_actions, len_list)\n","            Expected rewards given context, action, and position estimated by regression model, i.e., :math:`\\\\hat{q}(x_t,a_t)`.\n","        position: array-like or Tensor, shape (n_rounds,), default=None\n","            Position of recommendation interface where action was presented in each round of the given logged bandit data.\n","            When None is given, the effect of position on the reward will be ignored.\n","            (If only one action is chosen and there is no posion, then you can just ignore this argument.)\n","        Returns\n","        ----------\n","        estimated_rewards: array-like or Tensor, shape (n_rounds,)\n","            Rewards of each round estimated by the DM estimator.\n","        \"\"\"\n","        if position is None:\n","            position = np.zeros(action_dist.shape[0], dtype=int)\n","        n_rounds = position.shape[0]\n","        q_hat_at_position = estimated_rewards_by_reg_model[\n","            np.arange(n_rounds), :, position\n","        ]\n","        pi_e_at_position = action_dist[np.arange(n_rounds), :, position]\n","\n","        if isinstance(action_dist, np.ndarray):\n","            return np.average(\n","                q_hat_at_position,\n","                weights=pi_e_at_position,\n","                axis=1,\n","            )\n","        else:\n","            raise ValueError(\"action must be 1D array\")\n","\n","    def estimate_policy_value(\n","        self,\n","        action_dist: np.ndarray,\n","        estimated_rewards_by_reg_model: np.ndarray,\n","        position: Optional[np.ndarray] = None,\n","        **kwargs,\n","    ) -> float:\n","        \"\"\"Estimate the policy value of evaluation policy.\n","        Parameters\n","        ----------\n","        action_dist: array-like, shape (n_rounds, n_actions, len_list)\n","            Action choice probabilities of evaluation policy (can be deterministic), i.e., :math:`\\\\pi_e(a_t|x_t)`.\n","        estimated_rewards_by_reg_model: array-like, shape (n_rounds, n_actions, len_list)\n","            Expected rewards given context, action, and position estimated by regression model, i.e., :math:`\\\\hat{q}(x_t,a_t)`.\n","        position: array-like, shape (n_rounds,), default=None\n","            Position of recommendation interface where action was presented in each round of the given logged bandit data.\n","            When None is given, the effect of position on the reward will be ignored.\n","            (If only one action is chosen and there is no posion, then you can just ignore this argument.)\n","        Returns\n","        ----------\n","        V_hat: float\n","            Estimated policy value (performance) of a given evaluation policy.\n","        \"\"\"\n","        check_array(\n","            array=estimated_rewards_by_reg_model,\n","            name=\"estimated_rewards_by_reg_model\",\n","            expected_dim=3,\n","        )\n","        check_ope_inputs(\n","            action_dist=action_dist,\n","            estimated_rewards_by_reg_model=estimated_rewards_by_reg_model,\n","            position=position,\n","        )\n","        if position is None:\n","            position = np.zeros(action_dist.shape[0], dtype=int)\n","\n","        return self._estimate_round_rewards(\n","            position=position,\n","            estimated_rewards_by_reg_model=estimated_rewards_by_reg_model,\n","            action_dist=action_dist,\n","        ).mean()\n","\n","    def estimate_interval(\n","        self,\n","        action_dist: np.ndarray,\n","        estimated_rewards_by_reg_model: np.ndarray,\n","        position: Optional[np.ndarray] = None,\n","        alpha: float = 0.05,\n","        n_bootstrap_samples: int = 10000,\n","        random_state: Optional[int] = None,\n","        **kwargs,\n","    ) -> Dict[str, float]:\n","        \"\"\"Estimate confidence interval of policy value by nonparametric bootstrap procedure.\n","        Parameters\n","        ----------\n","        action_dist: array-like, shape (n_rounds, n_actions, len_list)\n","            Action choice probabilities of evaluation policy (can be deterministic), i.e., :math:`\\\\pi_e(a_t|x_t)`.\n","        estimated_rewards_by_reg_model: array-like, shape (n_rounds, n_actions, len_list)\n","            Expected rewards given context, action, and position estimated by regression model, i.e., :math:`\\\\hat{q}(x_t,a_t)`.\n","        position: array-like, shape (n_rounds,), default=None\n","            Position of recommendation interface where action was presented in each round of the given logged bandit data.\n","            When None is given, the effect of position on the reward will be ignored.\n","            (If only one action is chosen and there is no posion, then you can just ignore this argument.)\n","        alpha: float, default=0.05\n","            Significance level.\n","        n_bootstrap_samples: int, default=10000\n","            Number of resampling performed in the bootstrap procedure.\n","        random_state: int, default=None\n","            Controls the random seed in bootstrap sampling.\n","        Returns\n","        ----------\n","        estimated_confidence_interval: Dict[str, float]\n","            Dictionary storing the estimated mean and upper-lower confidence bounds.\n","        \"\"\"\n","        check_array(\n","            array=estimated_rewards_by_reg_model,\n","            name=\"estimated_rewards_by_reg_model\",\n","            expected_dim=3,\n","        )\n","        check_ope_inputs(\n","            action_dist=action_dist,\n","            estimated_rewards_by_reg_model=estimated_rewards_by_reg_model,\n","            position=position,\n","        )\n","        if position is None:\n","            position = np.zeros(action_dist.shape[0], dtype=int)\n","\n","        estimated_round_rewards = self._estimate_round_rewards(\n","            position=position,\n","            estimated_rewards_by_reg_model=estimated_rewards_by_reg_model,\n","            action_dist=action_dist,\n","        )\n","        return estimate_confidence_interval_by_bootstrap(\n","            samples=estimated_round_rewards,\n","            alpha=alpha,\n","            n_bootstrap_samples=n_bootstrap_samples,\n","            random_state=random_state,\n","        )"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"QVlZL9ZPpRAN"},"source":["### Doubly Robust"]},{"cell_type":"code","metadata":{"id":"twarEBTMpQ9S"},"source":["@dataclass\n","class DoublyRobust(BaseOffPolicyEstimator):\n","    \"\"\"Doubly Robust (DR) Estimator.\n","    Note\n","    -------\n","    Similar to DM, DR first learns a supervised machine learning model, such as ridge regression and gradient boosting,\n","    to estimate the mean reward function (:math:`q(x,a) = \\\\mathbb{E}[r|x,a]`).\n","    It then uses it to estimate the policy value as follows.\n","    .. math::\n","        \\\\hat{V}_{\\\\mathrm{DR}} (\\\\pi_e; \\\\mathcal{D}, \\\\hat{q})\n","        := \\\\mathbb{E}_{\\\\mathcal{D}}[\\\\hat{q}(x_t,\\\\pi_e) +  w(x_t,a_t) (r_t - \\\\hat{q}(x_t,a_t))],\n","    where :math:`\\\\mathcal{D}=\\\\{(x_t,a_t,r_t)\\\\}_{t=1}^{T}` is logged bandit feedback data with :math:`T` rounds collected by\n","    a behavior policy :math:`\\\\pi_b`.\n","    :math:`w(x,a):=\\\\pi_e (a|x)/\\\\pi_b (a|x)` is the importance weight given :math:`x` and :math:`a`.\n","    :math:`\\\\mathbb{E}_{\\\\mathcal{D}}[\\\\cdot]` is the empirical average over :math:`T` observations in :math:`\\\\mathcal{D}`.\n","    :math:`\\\\hat{q} (x,a)` is an estimated expected reward given :math:`x` and :math:`a`.\n","    :math:`\\\\hat{q} (x_t,\\\\pi):= \\\\mathbb{E}_{a \\\\sim \\\\pi(a|x)}[\\\\hat{q}(x,a)]` is the expectation of the estimated reward function over :math:`\\\\pi`.\n","    When the weight-clipping is applied, a large importance weight is clipped as :math:`\\\\hat{w}(x,a) := \\\\min \\\\{ \\\\lambda, w(x,a) \\\\}`\n","    where :math:`\\\\lambda (>0)` is a hyperparameter that decides a maximum allowed importance weight.\n","    To estimate the mean reward function, please use `obp.ope.regression_model.RegressionModel`,\n","    which supports several fitting methods specific to OPE such as *more robust doubly robust*.\n","    DR mimics IPW to use a weighted version of rewards, but DR also uses the estimated mean reward\n","    function (the regression model) as a control variate to decrease the variance.\n","    It preserves the consistency of IPW if either the importance weight or\n","    the mean reward estimator is accurate (a property called double robustness).\n","    Moreover, DR is semiparametric efficient when the mean reward estimator is correctly specified.\n","    Parameters\n","    ----------\n","    lambda_: float, default=np.inf\n","        A maximum possible value of the importance weight.\n","        When a positive finite value is given, importance weights larger than `lambda_` will be clipped.\n","        DoublyRobust with a finite positive `lambda_` corresponds to Doubly Robust with Pessimistic Shrinkage of Su et al.(2020) or CAB-DR of Su et al.(2019).\n","    estimator_name: str, default='dr'.\n","        Name of the estimator.\n","    References\n","    ----------\n","    Miroslav Dudík, Dumitru Erhan, John Langford, and Lihong Li.\n","    \"Doubly Robust Policy Evaluation and Optimization.\", 2014.\n","    Mehrdad Farajtabar, Yinlam Chow, and Mohammad Ghavamzadeh.\n","    \"More Robust Doubly Robust Off-policy Evaluation.\", 2018.\n","    Yi Su, Lequn Wang, Michele Santacatterina, and Thorsten Joachims.\n","    \"CAB: Continuous Adaptive Blending Estimator for Policy Evaluation and Learning\", 2019.\n","    Yi Su, Maria Dimakopoulou, Akshay Krishnamurthy, and Miroslav Dudík.\n","    \"Doubly robust off-policy evaluation with shrinkage.\", 2020.\n","    \"\"\"\n","\n","    lambda_: float = np.inf\n","    estimator_name: str = \"dr\"\n","\n","    def __post_init__(self) -> None:\n","        \"\"\"Initialize Class.\"\"\"\n","        check_scalar(\n","            self.lambda_,\n","            name=\"lambda_\",\n","            target_type=(int, float),\n","            min_val=0.0,\n","        )\n","        if self.lambda_ != self.lambda_:\n","            raise ValueError(\"lambda_ must not be nan\")\n","\n","    def _estimate_round_rewards(\n","        self,\n","        reward: np.ndarray,\n","        action: np.ndarray,\n","        pscore: np.ndarray,\n","        action_dist: np.ndarray,\n","        estimated_rewards_by_reg_model: np.ndarray,\n","        position: Optional[np.ndarray] = None,\n","        **kwargs,\n","    ) -> np.ndarray:\n","        \"\"\"Estimate round-wise (or sample-wise) rewards.\n","        Parameters\n","        ----------\n","        reward: array-like or Tensor, shape (n_rounds,)\n","            Reward observed in each round of the logged bandit feedback, i.e., :math:`r_t`.\n","        action: array-like or Tensor, shape (n_rounds,)\n","            Action sampled by behavior policy in each round of the logged bandit feedback, i.e., :math:`a_t`.\n","        pscore: array-like or Tensor, shape (n_rounds,)\n","            Action choice probabilities of behavior policy (propensity scores), i.e., :math:`\\\\pi_b(a_t|x_t)`.\n","        action_dist: array-like or Tensor, shape (n_rounds, n_actions, len_list)\n","            Action choice probabilities of evaluation policy (can be deterministic), i.e., :math:`\\\\pi_e(a_t|x_t)`.\n","        estimated_rewards_by_reg_model or Tensor: array-like, shape (n_rounds, n_actions, len_list)\n","            Expected rewards given context, action, and position estimated by regression model, i.e., :math:`\\\\hat{q}(x_t,a_t)`.\n","        position: array-like or Tensor, shape (n_rounds,), default=None\n","            Position of recommendation interface where action was presented in each round of the given logged bandit data.\n","            When None is given, the effect of position on the reward will be ignored.\n","            (If only one action is chosen and there is no posion, then you can just ignore this argument.)\n","        Returns\n","        ----------\n","        estimated_rewards: array-like or Tensor, shape (n_rounds,)\n","            Rewards of each round estimated by the DR estimator.\n","        \"\"\"\n","        if position is None:\n","            position = np.zeros(action_dist.shape[0], dtype=int)\n","        n_rounds = action.shape[0]\n","        iw = action_dist[np.arange(n_rounds), action, position] / pscore\n","        # weight clipping\n","        if isinstance(iw, np.ndarray):\n","            iw = np.minimum(iw, self.lambda_)\n","        q_hat_at_position = estimated_rewards_by_reg_model[\n","            np.arange(n_rounds), :, position\n","        ]\n","        q_hat_factual = estimated_rewards_by_reg_model[\n","            np.arange(n_rounds), action, position\n","        ]\n","        pi_e_at_position = action_dist[np.arange(n_rounds), :, position]\n","\n","        if isinstance(reward, np.ndarray):\n","            estimated_rewards = np.average(\n","                q_hat_at_position,\n","                weights=pi_e_at_position,\n","                axis=1,\n","            )\n","        else:\n","            raise ValueError(\"reward must be 1D array\")\n","\n","        estimated_rewards += iw * (reward - q_hat_factual)\n","        return estimated_rewards\n","\n","    def estimate_policy_value(\n","        self,\n","        reward: np.ndarray,\n","        action: np.ndarray,\n","        pscore: np.ndarray,\n","        action_dist: np.ndarray,\n","        estimated_rewards_by_reg_model: np.ndarray,\n","        position: Optional[np.ndarray] = None,\n","    ) -> float:\n","        \"\"\"Estimate the policy value of evaluation policy.\n","        Parameters\n","        ----------\n","        reward: array-like, shape (n_rounds,)\n","            Reward observed in each round of the logged bandit feedback, i.e., :math:`r_t`.\n","        action: array-like, shape (n_rounds,)\n","            Action sampled by behavior policy in each round of the logged bandit feedback, i.e., :math:`a_t`.\n","        pscore: array-like, shape (n_rounds,)\n","            Action choice probabilities of behavior policy (propensity scores), i.e., :math:`\\\\pi_b(a_t|x_t)`.\n","        action_dist: array-like, shape (n_rounds, n_actions, len_list)\n","            Action choice probabilities of evaluation policy (can be deterministic), i.e., :math:`\\\\pi_e(a_t|x_t)`.\n","        estimated_rewards_by_reg_model: array-like, shape (n_rounds, n_actions, len_list)\n","            Expected rewards given context, action, and position estimated by regression model, i.e., :math:`\\\\hat{q}(x_t,a_t)`.\n","        position: array-like, shape (n_rounds,), default=None\n","            Position of recommendation interface where action was presented in each round of the given logged bandit data.\n","            When None is given, the effect of position on the reward will be ignored.\n","            (If only one action is chosen and there is no posion, then you can just ignore this argument.)\n","        Returns\n","        ----------\n","        V_hat: float\n","            Policy value estimated by the DR estimator.\n","        \"\"\"\n","        check_array(\n","            array=estimated_rewards_by_reg_model,\n","            name=\"estimated_rewards_by_reg_model\",\n","            expected_dim=3,\n","        )\n","        check_array(array=reward, name=\"reward\", expected_dim=1)\n","        check_array(array=action, name=\"action\", expected_dim=1)\n","        check_array(array=pscore, name=\"pscore\", expected_dim=1)\n","        check_ope_inputs(\n","            action_dist=action_dist,\n","            position=position,\n","            action=action,\n","            reward=reward,\n","            pscore=pscore,\n","            estimated_rewards_by_reg_model=estimated_rewards_by_reg_model,\n","        )\n","        if position is None:\n","            position = np.zeros(action_dist.shape[0], dtype=int)\n","\n","        return self._estimate_round_rewards(\n","            reward=reward,\n","            action=action,\n","            position=position,\n","            pscore=pscore,\n","            action_dist=action_dist,\n","            estimated_rewards_by_reg_model=estimated_rewards_by_reg_model,\n","        ).mean()\n","\n","    def estimate_interval(\n","        self,\n","        reward: np.ndarray,\n","        action: np.ndarray,\n","        pscore: np.ndarray,\n","        action_dist: np.ndarray,\n","        estimated_rewards_by_reg_model: np.ndarray,\n","        position: Optional[np.ndarray] = None,\n","        alpha: float = 0.05,\n","        n_bootstrap_samples: int = 10000,\n","        random_state: Optional[int] = None,\n","        **kwargs,\n","    ) -> Dict[str, float]:\n","        \"\"\"Estimate confidence interval of policy value by nonparametric bootstrap procedure.\n","        Parameters\n","        ----------\n","        reward: array-like, shape (n_rounds,)\n","            Reward observed in each round of the logged bandit feedback, i.e., :math:`r_t`.\n","        action: array-like, shape (n_rounds,)\n","            Action sampled by behavior policy in each round of the logged bandit feedback, i.e., :math:`a_t`.\n","        pscore: array-like, shape (n_rounds,)\n","            Action choice probabilities of behavior policy (propensity scores), i.e., :math:`\\\\pi_b(a_t|x_t)`.\n","        action_dist: array-like, shape (n_rounds, n_actions, len_list)\n","            Action choice probabilities of evaluation policy (can be deterministic), i.e., :math:`\\\\pi_e(a_t|x_t)`.\n","        estimated_rewards_by_reg_model: array-like, shape (n_rounds, n_actions, len_list)\n","            Expected rewards given context, action, and position estimated by regression model, i.e., :math:`\\\\hat{q}(x_t,a_t)`.\n","        position: array-like, shape (n_rounds,), default=None\n","            Position of recommendation interface where action was presented in each round of the given logged bandit data.\n","            When None is given, the effect of position on the reward will be ignored.\n","            (If only one action is chosen and there is no posion, then you can just ignore this argument.)\n","        alpha: float, default=0.05\n","            Significance level.\n","        n_bootstrap_samples: int, default=10000\n","            Number of resampling performed in the bootstrap procedure.\n","        random_state: int, default=None\n","            Controls the random seed in bootstrap sampling.\n","        Returns\n","        ----------\n","        estimated_confidence_interval: Dict[str, float]\n","            Dictionary storing the estimated mean and upper-lower confidence bounds.\n","        \"\"\"\n","        check_array(\n","            array=estimated_rewards_by_reg_model,\n","            name=\"estimated_rewards_by_reg_model\",\n","            expected_dim=3,\n","        )\n","        check_array(array=reward, name=\"reward\", expected_dim=1)\n","        check_array(array=action, name=\"action\", expected_dim=1)\n","        check_array(array=pscore, name=\"pscore\", expected_dim=1)\n","        check_ope_inputs(\n","            action_dist=action_dist,\n","            position=position,\n","            action=action,\n","            reward=reward,\n","            pscore=pscore,\n","            estimated_rewards_by_reg_model=estimated_rewards_by_reg_model,\n","        )\n","        if position is None:\n","            position = np.zeros(action_dist.shape[0], dtype=int)\n","\n","        estimated_round_rewards = self._estimate_round_rewards(\n","            reward=reward,\n","            action=action,\n","            position=position,\n","            pscore=pscore,\n","            action_dist=action_dist,\n","            estimated_rewards_by_reg_model=estimated_rewards_by_reg_model,\n","        )\n","        return estimate_confidence_interval_by_bootstrap(\n","            samples=estimated_round_rewards,\n","            alpha=alpha,\n","            n_bootstrap_samples=n_bootstrap_samples,\n","            random_state=random_state,\n","        )\n","\n","    def _estimate_mse_score(\n","        self,\n","        reward: np.ndarray,\n","        action: np.ndarray,\n","        pscore: np.ndarray,\n","        action_dist: np.ndarray,\n","        estimated_rewards_by_reg_model: np.ndarray,\n","        position: Optional[np.ndarray] = None,\n","        use_bias_upper_bound: bool = True,\n","        delta: float = 0.05,\n","    ) -> float:\n","        \"\"\"Estimate the MSE score of a given clipping hyperparameter to conduct hyperparameter tuning.\n","        Parameters\n","        ----------\n","        reward: array-like, shape (n_rounds,)\n","            Reward observed in each round of the logged bandit feedback, i.e., :math:`r_t`.\n","        action: array-like, shape (n_rounds,)\n","            Action sampled by behavior policy in each round of the logged bandit feedback, i.e., :math:`a_t`.\n","        pscore: array-like, shape (n_rounds,)\n","            Action choice probabilities of behavior policy (propensity scores), i.e., :math:`\\\\pi_b(a_t|x_t)`.\n","        action_dist: array-like, shape (n_rounds, n_actions, len_list)\n","            Action choice probabilities of evaluation policy (can be deterministic), i.e., :math:`\\\\pi_e(a_t|x_t)`.\n","        position: array-like, shape (n_rounds,), default=None\n","            Position of recommendation interface where action was presented in each round of the given logged bandit data.\n","            When None is given, the effect of position on the reward will be ignored.\n","            (If only one action is chosen and there is no posion, then you can just ignore this argument.)\n","        estimated_rewards_by_reg_model: array-like, shape (n_rounds, n_actions, len_list)\n","            Expected rewards given context, action, and position estimated by regression model, i.e., :math:`\\\\hat{q}(x_t,a_t)`.\n","        use_bias_upper_bound: bool, default=True\n","            Whether to use bias upper bound in hyperparameter tuning.\n","            If False, direct bias estimator is used to estimate the MSE.\n","        delta: float, default=0.05\n","            A confidence delta to construct a high probability upper bound based on the Bernstein’s inequality.\n","        Returns\n","        ----------\n","        estimated_mse_score: float\n","            Estimated MSE score of a given clipping hyperparameter `lambda_`.\n","            MSE score is the sum of (high probability) upper bound of bias and the sample variance.\n","            This is estimated using the automatic hyperparameter tuning procedure\n","            based on Section 5 of Su et al.(2020).\n","        \"\"\"\n","        n_rounds = reward.shape[0]\n","        # estimate the sample variance of DR with clipping\n","        sample_variance = np.var(\n","            self._estimate_round_rewards(\n","                reward=reward,\n","                action=action,\n","                pscore=pscore,\n","                action_dist=action_dist,\n","                estimated_rewards_by_reg_model=estimated_rewards_by_reg_model,\n","                position=position,\n","            )\n","        )\n","        sample_variance /= n_rounds\n","\n","        # estimate the (high probability) upper bound of the bias of DR with clipping\n","        iw = action_dist[np.arange(n_rounds), action, position] / pscore\n","        if use_bias_upper_bound:\n","            bias_term = estimate_high_probability_upper_bound_bias(\n","                reward=reward,\n","                iw=iw,\n","                iw_hat=np.minimum(iw, self.lambda_),\n","                q_hat=estimated_rewards_by_reg_model[\n","                    np.arange(n_rounds), action, position\n","                ],\n","                delta=delta,\n","            )\n","        else:\n","            bias_term = estimate_bias_in_ope(\n","                reward=reward,\n","                iw=iw,\n","                iw_hat=np.minimum(iw, self.lambda_),\n","                q_hat=estimated_rewards_by_reg_model[\n","                    np.arange(n_rounds), action, position\n","                ],\n","            )\n","        estimated_mse_score = sample_variance + (bias_term ** 2)\n","\n","        return estimated_mse_score"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rK0m1w6dpQ4w"},"source":["### Inverse Probability Weighting"]},{"cell_type":"code","metadata":{"id":"f3Uj8nvOpQ0m"},"source":["@dataclass\n","class InverseProbabilityWeighting(BaseOffPolicyEstimator):\n","    \"\"\"Inverse Probability Weighting (IPW) Estimator.\n","    Note\n","    -------\n","    Inverse Probability Weighting (IPW) estimates the policy value of evaluation policy :math:`\\\\pi_e` by\n","    .. math::\n","        \\\\hat{V}_{\\\\mathrm{IPW}} (\\\\pi_e; \\\\mathcal{D}) := \\\\mathbb{E}_{\\\\mathcal{D}} [ w(x_t,a_t) r_t],\n","    where :math:`\\\\mathcal{D}=\\\\{(x_t,a_t,r_t)\\\\}_{t=1}^{T}` is logged bandit feedback data with :math:`T` rounds collected by\n","    a behavior policy :math:`\\\\pi_b`. :math:`w(x,a):=\\\\pi_e (a|x)/\\\\pi_b (a|x)` is the importance weight given :math:`x` and :math:`a`.\n","    :math:`\\\\mathbb{E}_{\\\\mathcal{D}}[\\\\cdot]` is the empirical average over :math:`T` observations in :math:`\\\\mathcal{D}`.\n","    When the weight-clipping is applied, a large importance weight is clipped as :math:`\\\\hat{w}(x,a) := \\\\min \\\\{ \\\\lambda, w(x,a) \\\\}`\n","    where :math:`\\\\lambda (>0)` is a hyperparameter that decides a maximum allowed importance weight.\n","    IPW re-weights the rewards by the ratio of the evaluation policy and behavior policy (importance weight).\n","    When the behavior policy is known, IPW is unbiased and consistent for the true policy value.\n","    However, it can have a large variance, especially when the evaluation policy significantly deviates from the behavior policy.\n","    Parameters\n","    ------------\n","    lambda_: float, default=np.inf\n","        A maximum possible value of the importance weight.\n","        When a positive finite value is given, importance weights larger than `lambda_` will be clipped.\n","    estimator_name: str, default='ipw'.\n","        Name of the estimator.\n","    References\n","    ------------\n","    Alex Strehl, John Langford, Lihong Li, and Sham M Kakade.\n","    \"Learning from Logged Implicit Exploration Data\"., 2010.\n","    Miroslav Dudík, Dumitru Erhan, John Langford, and Lihong Li.\n","    \"Doubly Robust Policy Evaluation and Optimization.\", 2014.\n","    Yi Su, Maria Dimakopoulou, Akshay Krishnamurthy, and Miroslav Dudik.\n","    \"Doubly Robust Off-Policy Evaluation with Shrinkage.\", 2020.\n","    \"\"\"\n","\n","    lambda_: float = np.inf\n","    estimator_name: str = \"ipw\"\n","\n","    def __post_init__(self) -> None:\n","        \"\"\"Initialize Class.\"\"\"\n","        check_scalar(\n","            self.lambda_,\n","            name=\"lambda_\",\n","            target_type=(int, float),\n","            min_val=0.0,\n","        )\n","        if self.lambda_ != self.lambda_:\n","            raise ValueError(\"lambda_ must not be nan\")\n","\n","    def _estimate_round_rewards(\n","        self,\n","        reward: np.ndarray,\n","        action: np.ndarray,\n","        pscore: np.ndarray,\n","        action_dist: np.ndarray,\n","        position: Optional[np.ndarray] = None,\n","        **kwargs,\n","    ) -> np.ndarray:\n","        \"\"\"Estimate round-wise (or sample-wise) rewards.\n","        Parameters\n","        ----------\n","        reward: array-like or Tensor, shape (n_rounds,)\n","            Reward observed in each round of the logged bandit feedback, i.e., :math:`r_t`.\n","        action: array-like or Tensor, shape (n_rounds,)\n","            Action sampled by behavior policy in each round of the logged bandit feedback, i.e., :math:`a_t`.\n","        pscore: array-like or Tensor, shape (n_rounds,)\n","            Action choice probabilities of behavior policy (propensity scores), i.e., :math:`\\\\pi_b(a_t|x_t)`.\n","        action_dist: array-like or Tensor, shape (n_rounds, n_actions, len_list)\n","            Action choice probabilities of evaluation policy (can be deterministic), i.e., :math:`\\\\pi_e(a_t|x_t)`.\n","        position: array-like or Tensor, shape (n_rounds,), default=None\n","            Position of recommendation interface where action was presented in each round of the given logged bandit data.\n","            When None is given, the effect of position on the reward will be ignored.\n","            (If only one action is chosen and there is no posion, then you can just ignore this argument.)\n","        Returns\n","        ----------\n","        estimated_rewards: array-like or Tensor, shape (n_rounds,)\n","            Rewards of each round estimated by IPW.\n","        \"\"\"\n","        if position is None:\n","            position = np.zeros(action_dist.shape[0], dtype=int)\n","        iw = action_dist[np.arange(action.shape[0]), action, position] / pscore\n","        # weight clipping\n","        if isinstance(iw, np.ndarray):\n","            iw = np.minimum(iw, self.lambda_)\n","        return reward * iw\n","\n","    def estimate_policy_value(\n","        self,\n","        reward: np.ndarray,\n","        action: np.ndarray,\n","        pscore: np.ndarray,\n","        action_dist: np.ndarray,\n","        position: Optional[np.ndarray] = None,\n","        **kwargs,\n","    ) -> np.ndarray:\n","        \"\"\"Estimate the policy value of evaluation policy.\n","        Parameters\n","        ----------\n","        reward: array-like, shape (n_rounds,)\n","            Reward observed in each round of the logged bandit feedback, i.e., :math:`r_t`.\n","        action: array-like, shape (n_rounds,)\n","            Action sampled by behavior policy in each round of the logged bandit feedback, i.e., :math:`a_t`.\n","        pscore: array-like, shape (n_rounds,)\n","            Action choice probabilities of behavior policy (propensity scores), i.e., :math:`\\\\pi_b(a_t|x_t)`.\n","        action_dist: array-like, shape (n_rounds, n_actions, len_list)\n","            Action choice probabilities of evaluation policy (can be deterministic), i.e., :math:`\\\\pi_e(a_t|x_t)`.\n","        position: array-like, shape (n_rounds,), default=None\n","            Position of recommendation interface where action was presented in each round of the given logged bandit data.\n","            When None is given, the effect of position on the reward will be ignored.\n","            (If only one action is chosen and there is no posion, then you can just ignore this argument.)\n","        Returns\n","        ----------\n","        V_hat: float\n","            Estimated policy value (performance) of a given evaluation policy.\n","        \"\"\"\n","        check_array(array=reward, name=\"reward\", expected_dim=1)\n","        check_array(array=action, name=\"action\", expected_dim=1)\n","        check_array(array=pscore, name=\"pscore\", expected_dim=1)\n","        check_ope_inputs(\n","            action_dist=action_dist,\n","            position=position,\n","            action=action,\n","            reward=reward,\n","            pscore=pscore,\n","        )\n","        if position is None:\n","            position = np.zeros(action_dist.shape[0], dtype=int)\n","\n","        return self._estimate_round_rewards(\n","            reward=reward,\n","            action=action,\n","            position=position,\n","            pscore=pscore,\n","            action_dist=action_dist,\n","        ).mean()\n","\n","    def estimate_interval(\n","        self,\n","        reward: np.ndarray,\n","        action: np.ndarray,\n","        pscore: np.ndarray,\n","        action_dist: np.ndarray,\n","        position: Optional[np.ndarray] = None,\n","        alpha: float = 0.05,\n","        n_bootstrap_samples: int = 10000,\n","        random_state: Optional[int] = None,\n","        **kwargs,\n","    ) -> Dict[str, float]:\n","        \"\"\"Estimate confidence interval of policy value by nonparametric bootstrap procedure.\n","        Parameters\n","        ----------\n","        reward: array-like, shape (n_rounds,)\n","            Reward observed in each round of the logged bandit feedback, i.e., :math:`r_t`.\n","        action: array-like, shape (n_rounds,)\n","            Action sampled by behavior policy in each round of the logged bandit feedback, i.e., :math:`a_t`.\n","        pscore: array-like, shape (n_rounds,)\n","            Action choice probabilities of behavior policy (propensity scores), i.e., :math:`\\\\pi_b(a_t|x_t)`.\n","        action_dist: array-like, shape (n_rounds, n_actions, len_list)\n","            Action choice probabilities of evaluation policy (can be deterministic), i.e., :math:`\\\\pi_e(a_t|x_t)`.\n","        position: array-like, shape (n_rounds,), default=None\n","            Position of recommendation interface where action was presented in each round of the given logged bandit data.\n","            When None is given, the effect of position on the reward will be ignored.\n","            (If only one action is chosen and there is no posion, then you can just ignore this argument.)\n","        alpha: float, default=0.05\n","            Significance level.\n","        n_bootstrap_samples: int, default=10000\n","            Number of resampling performed in the bootstrap procedure.\n","        random_state: int, default=None\n","            Controls the random seed in bootstrap sampling.\n","        Returns\n","        ----------\n","        estimated_confidence_interval: Dict[str, float]\n","            Dictionary storing the estimated mean and upper-lower confidence bounds.\n","        \"\"\"\n","        check_array(array=reward, name=\"reward\", expected_dim=1)\n","        check_array(array=action, name=\"action\", expected_dim=1)\n","        check_array(array=pscore, name=\"pscore\", expected_dim=1)\n","        check_ope_inputs(\n","            action_dist=action_dist,\n","            position=position,\n","            action=action,\n","            reward=reward,\n","            pscore=pscore,\n","        )\n","        if position is None:\n","            position = np.zeros(action_dist.shape[0], dtype=int)\n","\n","        estimated_round_rewards = self._estimate_round_rewards(\n","            reward=reward,\n","            action=action,\n","            position=position,\n","            pscore=pscore,\n","            action_dist=action_dist,\n","        )\n","        return estimate_confidence_interval_by_bootstrap(\n","            samples=estimated_round_rewards,\n","            alpha=alpha,\n","            n_bootstrap_samples=n_bootstrap_samples,\n","            random_state=random_state,\n","        )\n","\n","    def _estimate_mse_score(\n","        self,\n","        reward: np.ndarray,\n","        action: np.ndarray,\n","        pscore: np.ndarray,\n","        action_dist: np.ndarray,\n","        position: Optional[np.ndarray] = None,\n","        use_bias_upper_bound: bool = True,\n","        delta: float = 0.05,\n","        **kwargs,\n","    ) -> float:\n","        \"\"\"Estimate the MSE score of a given clipping hyperparameter to conduct hyperparameter tuning.\n","        Parameters\n","        ----------\n","        reward: array-like, shape (n_rounds,)\n","            Reward observed in each round of the logged bandit feedback, i.e., :math:`r_t`.\n","        action: array-like, shape (n_rounds,)\n","            Action sampled by behavior policy in each round of the logged bandit feedback, i.e., :math:`a_t`.\n","        pscore: array-like, shape (n_rounds,)\n","            Action choice probabilities of behavior policy (propensity scores), i.e., :math:`\\\\pi_b(a_t|x_t)`.\n","        action_dist: array-like, shape (n_rounds, n_actions, len_list)\n","            Action choice probabilities of evaluation policy (can be deterministic), i.e., :math:`\\\\pi_e(a_t|x_t)`.\n","        position: array-like, shape (n_rounds,), default=None\n","            Position of recommendation interface where action was presented in each round of the given logged bandit data.\n","        use_bias_upper_bound: bool, default=True\n","            Whether to use bias upper bound in hyperparameter tuning.\n","            If False, direct bias estimator is used to estimate the MSE.\n","        delta: float, default=0.05\n","            A confidence delta to construct a high probability upper bound based on the Bernstein’s inequality.\n","        Returns\n","        ----------\n","        estimated_mse_score: float\n","            Estimated MSE score of a given clipping hyperparameter `lambda_`.\n","            MSE score is the sum of (high probability) upper bound of bias and the sample variance.\n","            This is estimated using the automatic hyperparameter tuning procedure\n","            based on Section 5 of Su et al.(2020).\n","        \"\"\"\n","        n_rounds = reward.shape[0]\n","        # estimate the sample variance of IPW with clipping\n","        sample_variance = np.var(\n","            self._estimate_round_rewards(\n","                reward=reward,\n","                action=action,\n","                pscore=pscore,\n","                action_dist=action_dist,\n","                position=position,\n","            )\n","        )\n","        sample_variance /= n_rounds\n","\n","        # estimate the (high probability) upper bound of the bias of IPW with clipping\n","        iw = action_dist[np.arange(n_rounds), action, position] / pscore\n","        if use_bias_upper_bound:\n","            bias_term = estimate_high_probability_upper_bound_bias(\n","                reward=reward, iw=iw, iw_hat=np.minimum(iw, self.lambda_), delta=delta\n","            )\n","        else:\n","            bias_term = estimate_bias_in_ope(\n","                reward=reward,\n","                iw=iw,\n","                iw_hat=np.minimum(iw, self.lambda_),\n","            )\n","        estimated_mse_score = sample_variance + (bias_term ** 2)\n","\n","        return estimated_mse_score"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"yIT3K3iprCt8"},"source":["## Off-Policy Evaluation Class"]},{"cell_type":"markdown","metadata":{"id":"TPrOdZ98r7SU"},"source":["Off-Policy Evaluation Class to Streamline OPE."]},{"cell_type":"code","metadata":{"id":"-M_ANO2FrCl7"},"source":["from dataclasses import dataclass\n","from logging import getLogger\n","from pathlib import Path\n","from typing import Dict\n","from typing import List\n","from typing import Optional\n","from typing import Tuple\n","from typing import Union\n","\n","import matplotlib.pyplot as plt\n","import numpy as np\n","from pandas import DataFrame\n","import seaborn as sns\n","from sklearn.utils import check_scalar"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Z7tu8XTUsrgs"},"source":["logger = getLogger(__name__)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"J_oCM8rXsArc"},"source":["@dataclass\n","class OffPolicyEvaluation:\n","    \"\"\"Class to conduct OPE by multiple estimators simultaneously.\n","    Parameters\n","    -----------\n","    bandit_feedback: BanditFeedback\n","        Logged bandit feedback data used to conduct OPE.\n","    ope_estimators: List[BaseOffPolicyEstimator]\n","        List of OPE estimators used to evaluate the policy value of evaluation policy.\n","        Estimators must follow the interface of `obp.ope.BaseOffPolicyEstimator`.\n","    Examples\n","    ----------\n","    .. code-block:: python\n","        # a case for implementing OPE of the BernoulliTS policy\n","        # using log data generated by the Random policy\n","        >>> from obp.dataset import OpenBanditDataset\n","        >>> from obp.policy import BernoulliTS\n","        >>> from obp.ope import OffPolicyEvaluation, InverseProbabilityWeighting as IPW\n","        # (1) Data loading and preprocessing\n","        >>> dataset = OpenBanditDataset(behavior_policy='random', campaign='all')\n","        >>> bandit_feedback = dataset.obtain_batch_bandit_feedback()\n","        >>> bandit_feedback.keys()\n","        dict_keys(['n_rounds', 'n_actions', 'action', 'position', 'reward', 'pscore', 'context', 'action_context'])\n","        # (2) Off-Policy Learning\n","        >>> evaluation_policy = BernoulliTS(\n","            n_actions=dataset.n_actions,\n","            len_list=dataset.len_list,\n","            is_zozotown_prior=True, # replicate the policy in the ZOZOTOWN production\n","            campaign=\"all\",\n","            random_state=12345\n","        )\n","        >>> action_dist = evaluation_policy.compute_batch_action_dist(\n","            n_sim=100000, n_rounds=bandit_feedback[\"n_rounds\"]\n","        )\n","        # (3) Off-Policy Evaluation\n","        >>> ope = OffPolicyEvaluation(bandit_feedback=bandit_feedback, ope_estimators=[IPW()])\n","        >>> estimated_policy_value = ope.estimate_policy_values(action_dist=action_dist)\n","        >>> estimated_policy_value\n","        {'ipw': 0.004553...}\n","        # policy value improvement of BernoulliTS over the Random policy estimated by IPW\n","        >>> estimated_policy_value_improvement = estimated_policy_value['ipw'] / bandit_feedback['reward'].mean()\n","        # our OPE procedure suggests that BernoulliTS improves Random by 19.81%\n","        >>> print(estimated_policy_value_improvement)\n","        1.198126...\n","    \"\"\"\n","\n","    bandit_feedback: BanditFeedback\n","    ope_estimators: List[BaseOffPolicyEstimator]\n","\n","    def __post_init__(self) -> None:\n","        \"\"\"Initialize class.\"\"\"\n","        for key_ in [\"action\", \"position\", \"reward\", \"pscore\"]:\n","            if key_ not in self.bandit_feedback:\n","                raise RuntimeError(f\"Missing key of {key_} in 'bandit_feedback'.\")\n","        self.ope_estimators_ = dict()\n","        self.is_model_dependent = False\n","        for estimator in self.ope_estimators:\n","            self.ope_estimators_[estimator.estimator_name] = estimator\n","            if isinstance(estimator, DirectMethod) or isinstance(estimator, DoublyRobust):\n","                self.is_model_dependent = True\n","\n","    def _create_estimator_inputs(\n","        self,\n","        action_dist: np.ndarray,\n","        estimated_rewards_by_reg_model: Optional[\n","            Union[np.ndarray, Dict[str, np.ndarray]]\n","        ] = None,\n","    ) -> Dict[str, Dict[str, np.ndarray]]:\n","        \"\"\"Create input dictionary to estimate policy value using subclasses of `BaseOffPolicyEstimator`\"\"\"\n","        check_array(array=action_dist, name=\"action_dist\", expected_dim=3)\n","        if estimated_rewards_by_reg_model is None:\n","            pass\n","        elif isinstance(estimated_rewards_by_reg_model, dict):\n","            for estimator_name, value in estimated_rewards_by_reg_model.items():\n","                check_array(\n","                    array=value,\n","                    name=f\"estimated_rewards_by_reg_model[{estimator_name}]\",\n","                    expected_dim=3,\n","                )\n","                if value.shape != action_dist.shape:\n","                    raise ValueError(\n","                        f\"Expected `estimated_rewards_by_reg_model[{estimator_name}].shape == action_dist.shape`, but found it False.\"\n","                    )\n","        elif estimated_rewards_by_reg_model.shape != action_dist.shape:\n","            raise ValueError(\n","                \"Expected `estimated_rewards_by_reg_model.shape == action_dist.shape`, but found it False\"\n","            )\n","        estimator_inputs = {\n","            estimator_name: {\n","                input_: self.bandit_feedback[input_]\n","                for input_ in [\"reward\", \"action\", \"position\", \"pscore\"]\n","            }\n","            for estimator_name in self.ope_estimators_\n","        }\n","\n","        for estimator_name in self.ope_estimators_:\n","            estimator_inputs[estimator_name][\"action_dist\"] = action_dist\n","            if isinstance(estimated_rewards_by_reg_model, dict):\n","                if estimator_name in estimated_rewards_by_reg_model:\n","                    estimator_inputs[estimator_name][\n","                        \"estimated_rewards_by_reg_model\"\n","                    ] = estimated_rewards_by_reg_model[estimator_name]\n","                else:\n","                    estimator_inputs[estimator_name][\n","                        \"estimated_rewards_by_reg_model\"\n","                    ] = None\n","            else:\n","                estimator_inputs[estimator_name][\n","                    \"estimated_rewards_by_reg_model\"\n","                ] = estimated_rewards_by_reg_model\n","\n","        return estimator_inputs\n","\n","    def estimate_policy_values(\n","        self,\n","        action_dist: np.ndarray,\n","        estimated_rewards_by_reg_model: Optional[\n","            Union[np.ndarray, Dict[str, np.ndarray]]\n","        ] = None,\n","    ) -> Dict[str, float]:\n","        \"\"\"Estimate the policy value of evaluation policy.\n","        Parameters\n","        ------------\n","        action_dist: array-like, shape (n_rounds, n_actions, len_list)\n","            Action choice probabilities of evaluation policy (can be deterministic), i.e., :math:`\\\\pi_e(a_t|x_t)`.\n","        estimated_rewards_by_reg_model: array-like, shape (n_rounds, n_actions, len_list) or Dict[str, array-like], default=None\n","            Expected rewards given each round, action, and position estimated by regression model, i.e., :math:`\\\\hat{q}(x_t,a_t)`.\n","            When an array-like is given, all OPE estimators use it.\n","            When a dict is given, if the dict has the name of a estimator as a key, the corresponding value is used.\n","            When it is not given, model-dependent estimators such as DM and DR cannot be used.\n","        Returns\n","        ----------\n","        policy_value_dict: Dict[str, float]\n","            Dictionary containing estimated policy values by OPE estimators.\n","        \"\"\"\n","        if self.is_model_dependent:\n","            if estimated_rewards_by_reg_model is None:\n","                raise ValueError(\n","                    \"When model dependent estimators such as DM or DR are used, `estimated_rewards_by_reg_model` must be given\"\n","                )\n","\n","        policy_value_dict = dict()\n","        estimator_inputs = self._create_estimator_inputs(\n","            action_dist=action_dist,\n","            estimated_rewards_by_reg_model=estimated_rewards_by_reg_model,\n","        )\n","        for estimator_name, estimator in self.ope_estimators_.items():\n","            policy_value_dict[estimator_name] = estimator.estimate_policy_value(\n","                **estimator_inputs[estimator_name]\n","            )\n","\n","        return policy_value_dict\n","\n","    def estimate_intervals(\n","        self,\n","        action_dist: np.ndarray,\n","        estimated_rewards_by_reg_model: Optional[\n","            Union[np.ndarray, Dict[str, np.ndarray]]\n","        ] = None,\n","        alpha: float = 0.05,\n","        n_bootstrap_samples: int = 100,\n","        random_state: Optional[int] = None,\n","    ) -> Dict[str, Dict[str, float]]:\n","        \"\"\"Estimate confidence intervals of policy values using nonparametric bootstrap procedure.\n","        Parameters\n","        ------------\n","        action_dist: array-like, shape (n_rounds, n_actions, len_list)\n","            Action choice probabilities of evaluation policy (can be deterministic), i.e., :math:`\\\\pi_e(a_t|x_t)`.\n","        estimated_rewards_by_reg_model: array-like, shape (n_rounds, n_actions, len_list) or Dict[str, array-like], default=None\n","            Expected rewards given context, action, and position estimated by regression model, i.e., :math:`\\\\hat{q}(x_t,a_t)`.\n","            When an array-like is given, all OPE estimators use it.\n","            When a dict is given, if the dict has the name of a estimator as a key, the corresponding value is used.\n","            When it is not given, model-dependent estimators such as DM and DR cannot be used.\n","        alpha: float, default=0.05\n","            Significance level.\n","        n_bootstrap_samples: int, default=100\n","            Number of resampling performed in the bootstrap procedure.\n","        random_state: int, default=None\n","            Controls the random seed in bootstrap sampling.\n","        Returns\n","        ----------\n","        policy_value_interval_dict: Dict[str, Dict[str, float]]\n","            Dictionary containing confidence intervals of estimated policy value estimated\n","            using nonparametric bootstrap procedure.\n","        \"\"\"\n","        if self.is_model_dependent:\n","            if estimated_rewards_by_reg_model is None:\n","                raise ValueError(\n","                    \"When model dependent estimators such as DM or DR are used, `estimated_rewards_by_reg_model` must be given\"\n","                )\n","\n","        check_confidence_interval_arguments(\n","            alpha=alpha,\n","            n_bootstrap_samples=n_bootstrap_samples,\n","            random_state=random_state,\n","        )\n","        policy_value_interval_dict = dict()\n","        estimator_inputs = self._create_estimator_inputs(\n","            action_dist=action_dist,\n","            estimated_rewards_by_reg_model=estimated_rewards_by_reg_model,\n","        )\n","        for estimator_name, estimator in self.ope_estimators_.items():\n","            policy_value_interval_dict[estimator_name] = estimator.estimate_interval(\n","                **estimator_inputs[estimator_name],\n","                alpha=alpha,\n","                n_bootstrap_samples=n_bootstrap_samples,\n","                random_state=random_state,\n","            )\n","\n","        return policy_value_interval_dict\n","\n","    def summarize_off_policy_estimates(\n","        self,\n","        action_dist: np.ndarray,\n","        estimated_rewards_by_reg_model: Optional[\n","            Union[np.ndarray, Dict[str, np.ndarray]]\n","        ] = None,\n","        alpha: float = 0.05,\n","        n_bootstrap_samples: int = 100,\n","        random_state: Optional[int] = None,\n","    ) -> Tuple[DataFrame, DataFrame]:\n","        \"\"\"Summarize policy values and their confidence intervals estimated by OPE estimators.\n","        Parameters\n","        ------------\n","        action_dist: array-like, shape (n_rounds, n_actions, len_list)\n","            Action choice probabilities of evaluation policy (can be deterministic), i.e., :math:`\\\\pi_e(a_t|x_t)`.\n","        estimated_rewards_by_reg_model: array-like, shape (n_rounds, n_actions, len_list) or Dict[str, array-like], default=None\n","            Expected rewards given each round, action, and position estimated by regression model, i.e., :math:`\\\\hat{q}(x_t,a_t)`.\n","            When an array-like is given, all OPE estimators use it.\n","            When a dict is given, if the dict has the name of a estimator as a key, the corresponding value is used.\n","            When it is not given, model-dependent estimators such as DM and DR cannot be used.\n","        alpha: float, default=0.05\n","            Significance level.\n","        n_bootstrap_samples: int, default=100\n","            Number of resampling performed in the bootstrap procedure.\n","        random_state: int, default=None\n","            Controls the random seed in bootstrap sampling.\n","        Returns\n","        ----------\n","        (policy_value_df, policy_value_interval_df): Tuple[DataFrame, DataFrame]\n","            Policy values and their confidence intervals Estimated by OPE estimators.\n","        \"\"\"\n","        policy_value_df = DataFrame(\n","            self.estimate_policy_values(\n","                action_dist=action_dist,\n","                estimated_rewards_by_reg_model=estimated_rewards_by_reg_model,\n","            ),\n","            index=[\"estimated_policy_value\"],\n","        )\n","        policy_value_interval_df = DataFrame(\n","            self.estimate_intervals(\n","                action_dist=action_dist,\n","                estimated_rewards_by_reg_model=estimated_rewards_by_reg_model,\n","                alpha=alpha,\n","                n_bootstrap_samples=n_bootstrap_samples,\n","                random_state=random_state,\n","            )\n","        )\n","        policy_value_of_behavior_policy = self.bandit_feedback[\"reward\"].mean()\n","        policy_value_df = policy_value_df.T\n","        if policy_value_of_behavior_policy <= 0:\n","            logger.warning(\n","                f\"Policy value of the behavior policy is {policy_value_of_behavior_policy} (<=0); relative estimated policy value is set to np.nan\"\n","            )\n","            policy_value_df[\"relative_estimated_policy_value\"] = np.nan\n","        else:\n","            policy_value_df[\"relative_estimated_policy_value\"] = (\n","                policy_value_df.estimated_policy_value / policy_value_of_behavior_policy\n","            )\n","        return policy_value_df, policy_value_interval_df.T\n","\n","    def visualize_off_policy_estimates(\n","        self,\n","        action_dist: np.ndarray,\n","        estimated_rewards_by_reg_model: Optional[\n","            Union[np.ndarray, Dict[str, np.ndarray]]\n","        ] = None,\n","        alpha: float = 0.05,\n","        is_relative: bool = False,\n","        n_bootstrap_samples: int = 100,\n","        random_state: Optional[int] = None,\n","        fig_dir: Optional[Path] = None,\n","        fig_name: str = \"estimated_policy_value.png\",\n","    ) -> None:\n","        \"\"\"Visualize policy values estimated by OPE estimators.\n","        Parameters\n","        ----------\n","        action_dist: array-like, shape (n_rounds, n_actions, len_list)\n","            Action choice probabilities of evaluation policy (can be deterministic), i.e., :math:`\\\\pi_e(a_t|x_t)`.\n","        estimated_rewards_by_reg_model: array-like, shape (n_rounds, n_actions, len_list) or Dict[str, array-like], default=None\n","            Expected rewards given context, action, and position estimated by regression model, i.e., :math:`\\\\hat{q}(x_t,a_t)`.\n","            When an array-like is given, all OPE estimators use it.\n","            When a dict is given, if the dict has the name of a estimator as a key, the corresponding value is used.\n","            When it is not given, model-dependent estimators such as DM and DR cannot be used.\n","        alpha: float, default=0.05\n","            Significance level.\n","        n_bootstrap_samples: int, default=100\n","            Number of resampling performed in the bootstrap procedure.\n","        random_state: int, default=None\n","            Controls the random seed in bootstrap sampling.\n","        is_relative: bool, default=False,\n","            If True, the method visualizes the estimated policy values of evaluation policy\n","            relative to the ground-truth policy value of behavior policy.\n","        fig_dir: Path, default=None\n","            Path to store the bar figure.\n","            If 'None' is given, the figure will not be saved.\n","        fig_name: str, default=\"estimated_policy_value.png\"\n","            Name of the bar figure.\n","        \"\"\"\n","        if fig_dir is not None:\n","            assert isinstance(fig_dir, Path), \"fig_dir must be a Path\"\n","        if fig_name is not None:\n","            assert isinstance(fig_name, str), \"fig_dir must be a string\"\n","\n","        estimated_round_rewards_dict = dict()\n","        estimator_inputs = self._create_estimator_inputs(\n","            action_dist=action_dist,\n","            estimated_rewards_by_reg_model=estimated_rewards_by_reg_model,\n","        )\n","        for estimator_name, estimator in self.ope_estimators_.items():\n","            estimated_round_rewards_dict[\n","                estimator_name\n","            ] = estimator._estimate_round_rewards(**estimator_inputs[estimator_name])\n","        estimated_round_rewards_df = DataFrame(estimated_round_rewards_dict)\n","        estimated_round_rewards_df.rename(\n","            columns={key: key.upper() for key in estimated_round_rewards_dict.keys()},\n","            inplace=True,\n","        )\n","        if is_relative:\n","            estimated_round_rewards_df /= self.bandit_feedback[\"reward\"].mean()\n","\n","        plt.style.use(\"ggplot\")\n","        fig, ax = plt.subplots(figsize=(8, 6))\n","        sns.barplot(\n","            data=estimated_round_rewards_df,\n","            ax=ax,\n","            ci=100 * (1 - alpha),\n","            n_boot=n_bootstrap_samples,\n","            seed=random_state,\n","        )\n","        plt.xlabel(\"OPE Estimators\", fontsize=25)\n","        plt.ylabel(\n","            f\"Estimated Policy Value (± {np.int(100*(1 - alpha))}% CI)\", fontsize=20\n","        )\n","        plt.yticks(fontsize=15)\n","        plt.xticks(fontsize=25 - 2 * len(self.ope_estimators))\n","\n","        if fig_dir:\n","            fig.savefig(str(fig_dir / fig_name))\n","\n","    def evaluate_performance_of_estimators(\n","        self,\n","        ground_truth_policy_value: float,\n","        action_dist: np.ndarray,\n","        estimated_rewards_by_reg_model: Optional[\n","            Union[np.ndarray, Dict[str, np.ndarray]]\n","        ] = None,\n","        metric: str = \"relative-ee\",\n","    ) -> Dict[str, float]:\n","        \"\"\"Evaluate estimation performance of OPE estimators.\n","        Note\n","        ------\n","        Evaluate the estimation performance of OPE estimators by relative estimation error (relative-EE) or squared error (SE):\n","        .. math ::\n","            \\\\text{Relative-EE} (\\\\hat{V}; \\\\mathcal{D}) = \\\\left|  \\\\frac{\\\\hat{V}(\\\\pi; \\\\mathcal{D}) - V(\\\\pi)}{V(\\\\pi)} \\\\right|,\n","        .. math ::\n","            \\\\text{SE} (\\\\hat{V}; \\\\mathcal{D}) = \\\\left(\\\\hat{V}(\\\\pi; \\\\mathcal{D}) - V(\\\\pi) \\\\right)^2,\n","        where :math:`V({\\\\pi})` is the ground-truth policy value of the evalation policy :math:`\\\\pi_e` (often estimated using on-policy estimation).\n","        :math:`\\\\hat{V}(\\\\pi; \\\\mathcal{D})` is an estimated policy value by an OPE estimator :math:`\\\\hat{V}` and logged bandit feedback :math:`\\\\mathcal{D}`.\n","        Parameters\n","        ----------\n","        ground_truth policy value: float\n","            Ground_truth policy value of evaluation policy, i.e., :math:`V(\\\\pi_e)`.\n","            With Open Bandit Dataset, we use an on-policy estimate of the policy value as its ground-truth.\n","        action_dist: array-like, shape (n_rounds, n_actions, len_list)\n","            Action choice probabilities of evaluation policy (can be deterministic), i.e., :math:`\\\\pi_e(a_t|x_t)`.\n","        estimated_rewards_by_reg_model: array-like, shape (n_rounds, n_actions, len_list) or Dict[str, array-like], default=None\n","            Expected rewards given context, action, and position estimated by regression model, i.e., :math:`\\\\hat{q}(x_t,a_t)`.\n","            When an array-like is given, all OPE estimators use it.\n","            When a dict is given, if the dict has the name of a estimator as a key, the corresponding value is used.\n","            When it is not given, model-dependent estimators such as DM and DR cannot be used.\n","        metric: str, default=\"relative-ee\"\n","            Evaluation metric used to evaluate and compare the estimation performance of OPE estimators.\n","            Must be \"relative-ee\" or \"se\".\n","        Returns\n","        ----------\n","        eval_metric_ope_dict: Dict[str, float]\n","            Dictionary containing evaluation metric for evaluating the estimation performance of OPE estimators.\n","        \"\"\"\n","        check_scalar(\n","            ground_truth_policy_value,\n","            \"ground_truth_policy_value\",\n","            float,\n","        )\n","        if metric not in [\"relative-ee\", \"se\"]:\n","            raise ValueError(\n","                f\"metric must be either 'relative-ee' or 'se', but {metric} is given\"\n","            )\n","        if metric == \"relative-ee\" and ground_truth_policy_value == 0.0:\n","            raise ValueError(\n","                \"ground_truth_policy_value must be non-zero when metric is relative-ee\"\n","            )\n","\n","        eval_metric_ope_dict = dict()\n","        estimator_inputs = self._create_estimator_inputs(\n","            action_dist=action_dist,\n","            estimated_rewards_by_reg_model=estimated_rewards_by_reg_model,\n","        )\n","        for estimator_name, estimator in self.ope_estimators_.items():\n","            estimated_policy_value = estimator.estimate_policy_value(\n","                **estimator_inputs[estimator_name]\n","            )\n","            if metric == \"relative-ee\":\n","                relative_ee_ = estimated_policy_value - ground_truth_policy_value\n","                relative_ee_ /= ground_truth_policy_value\n","                eval_metric_ope_dict[estimator_name] = np.abs(relative_ee_)\n","            elif metric == \"se\":\n","                se_ = (estimated_policy_value - ground_truth_policy_value) ** 2\n","                eval_metric_ope_dict[estimator_name] = se_\n","        return eval_metric_ope_dict\n","\n","    def summarize_estimators_comparison(\n","        self,\n","        ground_truth_policy_value: float,\n","        action_dist: np.ndarray,\n","        estimated_rewards_by_reg_model: Optional[\n","            Union[np.ndarray, Dict[str, np.ndarray]]\n","        ] = None,\n","        metric: str = \"relative-ee\",\n","    ) -> DataFrame:\n","        \"\"\"Summarize performance comparisons of OPE estimators.\n","        Parameters\n","        ----------\n","        ground_truth policy value: float\n","            Ground_truth policy value of evaluation policy, i.e., :math:`V(\\\\pi_e)`.\n","            With Open Bandit Dataset, we use an on-policy estimate of the policy value as ground-truth.\n","        action_dist: array-like, shape (n_rounds, n_actions, len_list)\n","            Action choice probabilities of evaluation policy (can be deterministic), i.e., :math:`\\\\pi_e(a_t|x_t)`.\n","        estimated_rewards_by_reg_model: array-like, shape (n_rounds, n_actions, len_list), default=None\n","            Expected rewards given context, action, and position estimated by regression model, i.e., :math:`\\\\hat{q}(x_t,a_t)`.\n","            When it is not given, model-dependent estimators such as DM and DR cannot be used.\n","        metric: str, default=\"relative-ee\"\n","            Evaluation metric used to evaluate and compare the estimation performance of OPE estimators.\n","            Must be either \"relative-ee\" or \"se\".\n","        Returns\n","        ----------\n","        eval_metric_ope_df: DataFrame\n","            Evaluation metric to evaluate and compare the estimation performance of OPE estimators.\n","        \"\"\"\n","        eval_metric_ope_df = DataFrame(\n","            self.evaluate_performance_of_estimators(\n","                ground_truth_policy_value=ground_truth_policy_value,\n","                action_dist=action_dist,\n","                estimated_rewards_by_reg_model=estimated_rewards_by_reg_model,\n","                metric=metric,\n","            ),\n","            index=[metric],\n","        )\n","        return eval_metric_ope_df.T\n","\n","    def visualize_off_policy_estimates_of_multiple_policies(\n","        self,\n","        policy_name_list: List[str],\n","        action_dist_list: List[np.ndarray],\n","        estimated_rewards_by_reg_model: Optional[\n","            Union[np.ndarray, Dict[str, np.ndarray]]\n","        ] = None,\n","        alpha: float = 0.05,\n","        is_relative: bool = False,\n","        n_bootstrap_samples: int = 100,\n","        random_state: Optional[int] = None,\n","        fig_dir: Optional[Path] = None,\n","        fig_name: str = \"estimated_policy_value.png\",\n","    ) -> None:\n","        \"\"\"Visualize policy values estimated by OPE estimators.\n","        Parameters\n","        ----------\n","        policy_name_list: List[str]\n","            List of the names of evaluation policies.\n","        action_dist_list: List[array-like, shape (n_rounds, n_actions, len_list)]\n","            List of action choice probabilities by the evaluation policies (can be deterministic), i.e., :math:`\\\\pi_e(a_t|x_t)`.\n","        estimated_rewards_by_reg_model: array-like, shape (n_rounds, n_actions, len_list) or Dict[str, array-like], default=None\n","            Expected rewards given context, action, and position estimated by regression model, i.e., :math:`\\\\hat{q}(x_t,a_t)`.\n","            When an array-like is given, all OPE estimators use it.\n","            When a dict is given, if the dict has the name of an estimator as a key, the corresponding value is used.\n","            When it is not given, model-dependent estimators such as DM and DR cannot be used.\n","        alpha: float, default=0.05\n","            Significance level.\n","        n_bootstrap_samples: int, default=100\n","            Number of resampling performed in the bootstrap procedure.\n","        random_state: int, default=None\n","            Controls the random seed in bootstrap sampling.\n","        is_relative: bool, default=False,\n","            If True, the method visualizes the estimated policy values of evaluation policy\n","            relative to the ground-truth policy value of behavior policy.\n","        fig_dir: Path, default=None\n","            Path to store the bar figure.\n","            If 'None' is given, the figure will not be saved.\n","        fig_name: str, default=\"estimated_policy_value.png\"\n","            Name of the bar figure.\n","        \"\"\"\n","        if len(policy_name_list) != len(action_dist_list):\n","            raise ValueError(\n","                \"the length of policy_name_list must be the same as action_dist_list\"\n","            )\n","        if fig_dir is not None:\n","            assert isinstance(fig_dir, Path), \"fig_dir must be a Path\"\n","        if fig_name is not None:\n","            assert isinstance(fig_name, str), \"fig_dir must be a string\"\n","\n","        estimated_round_rewards_dict = {\n","            estimator_name: {} for estimator_name in self.ope_estimators_\n","        }\n","\n","        for policy_name, action_dist in zip(policy_name_list, action_dist_list):\n","            estimator_inputs = self._create_estimator_inputs(\n","                action_dist=action_dist,\n","                estimated_rewards_by_reg_model=estimated_rewards_by_reg_model,\n","            )\n","            for estimator_name, estimator in self.ope_estimators_.items():\n","                estimated_round_rewards_dict[estimator_name][\n","                    policy_name\n","                ] = estimator._estimate_round_rewards(\n","                    **estimator_inputs[estimator_name]\n","                )\n","\n","        plt.style.use(\"ggplot\")\n","        fig = plt.figure(figsize=(8, 6.2 * len(self.ope_estimators_)))\n","\n","        for i, estimator_name in enumerate(self.ope_estimators_):\n","            estimated_round_rewards_df = DataFrame(\n","                estimated_round_rewards_dict[estimator_name]\n","            )\n","            if is_relative:\n","                estimated_round_rewards_df /= self.bandit_feedback[\"reward\"].mean()\n","\n","            ax = fig.add_subplot(len(action_dist_list), 1, i + 1)\n","            sns.barplot(\n","                data=estimated_round_rewards_df,\n","                ax=ax,\n","                ci=100 * (1 - alpha),\n","                n_boot=n_bootstrap_samples,\n","                seed=random_state,\n","            )\n","            ax.set_title(estimator_name.upper(), fontsize=20)\n","            ax.set_ylabel(\n","                f\"Estimated Policy Value (± {np.int(100*(1 - alpha))}% CI)\", fontsize=20\n","            )\n","            plt.yticks(fontsize=15)\n","            plt.xticks(fontsize=25 - 2 * len(policy_name_list))\n","\n","        if fig_dir:\n","            fig.savefig(str(fig_dir / fig_name))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"TtKCr6UBsyOR"},"source":["## Base Models"]},{"cell_type":"markdown","metadata":{"id":"v7-Buj6EtDUE"},"source":["### Utils"]},{"cell_type":"code","metadata":{"id":"xNIdOK3UtGFJ"},"source":["def check_bandit_feedback_inputs(\n","    context: np.ndarray,\n","    action: np.ndarray,\n","    reward: np.ndarray,\n","    expected_reward: Optional[np.ndarray] = None,\n","    position: Optional[np.ndarray] = None,\n","    pscore: Optional[np.ndarray] = None,\n","    action_context: Optional[np.ndarray] = None,\n",") -> Optional[ValueError]:\n","    \"\"\"Check inputs for bandit learning or simulation.\n","    Parameters\n","    -----------\n","    context: array-like, shape (n_rounds, dim_context)\n","        Context vectors in each round, i.e., :math:`x_t`.\n","    action: array-like, shape (n_rounds,)\n","        Action sampled by behavior policy in each round of the logged bandit feedback, i.e., :math:`a_t`.\n","    reward: array-like, shape (n_rounds,)\n","        Observed rewards (or outcome) in each round, i.e., :math:`r_t`.\n","    expected_reward: array-like, shape (n_rounds, n_actions), default=None\n","        Expected rewards (or outcome) in each round, i.e., :math:`\\\\mathbb{E}[r_t]`.\n","    position: array-like, shape (n_rounds,), default=None\n","        Position of recommendation interface where action was presented in each round of the given logged bandit data.\n","    pscore: array-like, shape (n_rounds,), default=None\n","        Propensity scores, the probability of selecting each action by behavior policy,\n","        in the given logged bandit data.\n","    action_context: array-like, shape (n_actions, dim_action_context)\n","        Context vectors characterizing each action.\n","    \"\"\"\n","    check_array(array=context, name=\"context\", expected_dim=2)\n","    check_array(array=action, name=\"action\", expected_dim=1)\n","    check_array(array=reward, name=\"reward\", expected_dim=1)\n","    if not (np.issubdtype(action.dtype, np.integer) and action.min() >= 0):\n","        raise ValueError(\"action elements must be non-negative integers\")\n","\n","    if expected_reward is not None:\n","        check_array(array=expected_reward, name=\"expected_reward\", expected_dim=2)\n","        if not (\n","            context.shape[0]\n","            == action.shape[0]\n","            == reward.shape[0]\n","            == expected_reward.shape[0]\n","        ):\n","            raise ValueError(\n","                \"Expected `context.shape[0] == action.shape[0] == reward.shape[0] == expected_reward.shape[0]`\"\n","                \", but found it False\"\n","            )\n","        if action.max() >= expected_reward.shape[1]:\n","            raise ValueError(\n","                \"action elements must be smaller than `expected_reward.shape[1]`\"\n","            )\n","    if pscore is not None:\n","        check_array(array=pscore, name=\"pscore\", expected_dim=1)\n","        if not (\n","            context.shape[0] == action.shape[0] == reward.shape[0] == pscore.shape[0]\n","        ):\n","            raise ValueError(\n","                \"Expected `context.shape[0] == action.shape[0] == reward.shape[0] == pscore.shape[0]`\"\n","                \", but found it False\"\n","            )\n","        if np.any(pscore <= 0):\n","            raise ValueError(\"pscore must be positive\")\n","\n","    if position is not None:\n","        check_array(array=position, name=\"position\", expected_dim=1)\n","        if not (\n","            context.shape[0] == action.shape[0] == reward.shape[0] == position.shape[0]\n","        ):\n","            raise ValueError(\n","                \"Expected `context.shape[0] == action.shape[0] == reward.shape[0] == position.shape[0]`\"\n","                \", but found it False\"\n","            )\n","        if not (np.issubdtype(position.dtype, np.integer) and position.min() >= 0):\n","            raise ValueError(\"position elements must be non-negative integers\")\n","    else:\n","        if not (context.shape[0] == action.shape[0] == reward.shape[0]):\n","            raise ValueError(\n","                \"Expected `context.shape[0] == action.shape[0] == reward.shape[0]`\"\n","                \", but found it False\"\n","            )\n","    if action_context is not None:\n","        check_array(array=action_context, name=\"action_context\", expected_dim=2)\n","        if action.max() >= action_context.shape[0]:\n","            raise ValueError(\n","                \"action elements must be smaller than `action_context.shape[0]`\"\n","            )"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"pVZVogaFrIig"},"source":["### Regression Model"]},{"cell_type":"markdown","metadata":{"id":"stLAFisWs39g"},"source":["Regression Model Class for Estimating Mean Reward Functions."]},{"cell_type":"code","metadata":{"id":"SP8ZL-gsI9wF"},"source":["from dataclasses import dataclass\n","from typing import Optional\n","\n","import numpy as np\n","from sklearn.base import BaseEstimator\n","from sklearn.base import clone\n","from sklearn.base import is_classifier\n","from sklearn.model_selection import KFold\n","from sklearn.utils import check_random_state\n","from sklearn.utils import check_scalar"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4V_uwIAztJM_"},"source":["@dataclass\n","class RegressionModel(BaseEstimator):\n","    \"\"\"Machine learning model to estimate the mean reward function (:math:`q(x,a):= \\\\mathbb{E}[r|x,a]`).\n","    Note\n","    -------\n","    Reward (or outcome) :math:`r` must be either binary or continuous.\n","    Parameters\n","    ------------\n","    base_model: BaseEstimator\n","        A machine learning model used to estimate the mean reward function.\n","    n_actions: int\n","        Number of actions.\n","    len_list: int, default=1\n","        Length of a list of actions recommended in each impression.\n","        When Open Bandit Dataset is used, 3 should be set.\n","    action_context: array-like, shape (n_actions, dim_action_context), default=None\n","        Context vector characterizing action (i.e., vector representation of each action).\n","        If not given, one-hot encoding of the action variable is used as default.\n","    fitting_method: str, default='normal'\n","        Method to fit the regression model.\n","        Must be one of ['normal', 'iw', 'mrdr'] where 'iw' stands for importance weighting and\n","        'mrdr' stands for more robust doubly robust.\n","    References\n","    -----------\n","    Mehrdad Farajtabar, Yinlam Chow, and Mohammad Ghavamzadeh.\n","    \"More Robust Doubly Robust Off-policy Evaluation.\", 2018.\n","    Yi Su, Maria Dimakopoulou, Akshay Krishnamurthy, and Miroslav Dudik.\n","    \"Doubly Robust Off-Policy Evaluation with Shrinkage.\", 2020.\n","    Yusuke Narita, Shota Yasui, and Kohei Yata.\n","    \"Off-policy Bandit and Reinforcement Learning.\", 2020.\n","    \"\"\"\n","\n","    base_model: BaseEstimator\n","    n_actions: int\n","    len_list: int = 1\n","    action_context: Optional[np.ndarray] = None\n","    fitting_method: str = \"normal\"\n","\n","    def __post_init__(self) -> None:\n","        \"\"\"Initialize Class.\"\"\"\n","        check_scalar(self.n_actions, \"n_actions\", int, min_val=2)\n","        check_scalar(self.len_list, \"len_list\", int, min_val=1)\n","        if not (\n","            isinstance(self.fitting_method, str)\n","            and self.fitting_method in [\"normal\", \"iw\", \"mrdr\"]\n","        ):\n","            raise ValueError(\n","                f\"fitting_method must be one of 'normal', 'iw', or 'mrdr', but {self.fitting_method} is given\"\n","            )\n","        if not isinstance(self.base_model, BaseEstimator):\n","            raise ValueError(\n","                \"base_model must be BaseEstimator or a child class of BaseEstimator\"\n","            )\n","\n","        self.base_model_list = [\n","            clone(self.base_model) for _ in np.arange(self.len_list)\n","        ]\n","        if self.action_context is None:\n","            self.action_context = np.eye(self.n_actions, dtype=int)\n","\n","    def fit(\n","        self,\n","        context: np.ndarray,\n","        action: np.ndarray,\n","        reward: np.ndarray,\n","        pscore: Optional[np.ndarray] = None,\n","        position: Optional[np.ndarray] = None,\n","        action_dist: Optional[np.ndarray] = None,\n","    ) -> None:\n","        \"\"\"Fit the regression model on given logged bandit feedback data.\n","        Parameters\n","        ----------\n","        context: array-like, shape (n_rounds, dim_context)\n","            Context vectors observed in each round of the logged bandit feedback, i.e., :math:`x_t`.\n","        action: array-like, shape (n_rounds,)\n","            Action sampled by behavior policy in each round of the logged bandit feedback, i.e., :math:`a_t`.\n","        reward: array-like, shape (n_rounds,)\n","            Reward observed in each round of the logged bandit feedback, i.e., :math:`r_t`.\n","        pscore: array-like, shape (n_rounds,)\n","            Action choice probabilities of behavior policy (propensity scores), i.e., :math:`\\\\pi_b(a_t|x_t)`.\n","            When None is given, behavior policy is assumed to be uniform.\n","        position: array-like, shape (n_rounds,), default=None\n","            Position of recommendation interface where action was presented in each round of the given logged bandit data.\n","            If None is set, a regression model assumes that there is only one position.\n","            When `len_list` > 1, this position argument has to be set.\n","        action_dist: array-like, shape (n_rounds, n_actions, len_list), default=None\n","            Action choice probabilities of evaluation policy (can be deterministic), i.e., :math:`\\\\pi_e(a_t|x_t)`.\n","            When either of 'iw' or 'mrdr' is used as the 'fitting_method' argument, then `action_dist` must be given.\n","        \"\"\"\n","        check_bandit_feedback_inputs(\n","            context=context,\n","            action=action,\n","            reward=reward,\n","            pscore=pscore,\n","            position=position,\n","            action_context=self.action_context,\n","        )\n","        n_rounds = context.shape[0]\n","\n","        if position is None or self.len_list == 1:\n","            position = np.zeros_like(action)\n","        else:\n","            if position.max() >= self.len_list:\n","                raise ValueError(\n","                    f\"position elements must be smaller than len_list, but the maximum value is {position.max()} (>= {self.len_list})\"\n","                )\n","        if self.fitting_method in [\"iw\", \"mrdr\"]:\n","            if not (isinstance(action_dist, np.ndarray) and action_dist.ndim == 3):\n","                raise ValueError(\n","                    \"when fitting_method is either 'iw' or 'mrdr', action_dist (a 3-dimensional ndarray) must be given\"\n","                )\n","            if action_dist.shape != (n_rounds, self.n_actions, self.len_list):\n","                raise ValueError(\n","                    f\"shape of action_dist must be (n_rounds, n_actions, len_list)=({n_rounds, self.n_actions, self.len_list}), but is {action_dist.shape}\"\n","                )\n","            if not np.allclose(action_dist.sum(axis=1), 1):\n","                raise ValueError(\"action_dist must be a probability distribution\")\n","        if pscore is None:\n","            pscore = np.ones_like(action) / self.n_actions\n","\n","        for position_ in np.arange(self.len_list):\n","            idx = position == position_\n","            X = self._pre_process_for_reg_model(\n","                context=context[idx],\n","                action=action[idx],\n","                action_context=self.action_context,\n","            )\n","            if X.shape[0] == 0:\n","                raise ValueError(f\"No training data at position {position_}\")\n","            # train the base model according to the given `fitting method`\n","            if self.fitting_method == \"normal\":\n","                self.base_model_list[position_].fit(X, reward[idx])\n","            else:\n","                action_dist_at_position = action_dist[\n","                    np.arange(n_rounds),\n","                    action,\n","                    position_ * np.ones(n_rounds, dtype=int),\n","                ][idx]\n","                if self.fitting_method == \"iw\":\n","                    sample_weight = action_dist_at_position / pscore[idx]\n","                    self.base_model_list[position_].fit(\n","                        X, reward[idx], sample_weight=sample_weight\n","                    )\n","                elif self.fitting_method == \"mrdr\":\n","                    sample_weight = action_dist_at_position\n","                    sample_weight *= 1.0 - pscore[idx]\n","                    sample_weight /= pscore[idx] ** 2\n","                    self.base_model_list[position_].fit(\n","                        X, reward[idx], sample_weight=sample_weight\n","                    )\n","\n","    def predict(self, context: np.ndarray) -> np.ndarray:\n","        \"\"\"Predict the mean reward function.\n","        Parameters\n","        -----------\n","        context: array-like, shape (n_rounds_of_new_data, dim_context)\n","            Context vectors of new data.\n","        Returns\n","        -----------\n","        estimated_rewards_by_reg_model: array-like, shape (n_rounds_of_new_data, n_actions, len_list)\n","            Expected rewards of new data estimated by the regression model.\n","        \"\"\"\n","        n_rounds_of_new_data = context.shape[0]\n","        ones_n_rounds_arr = np.ones(n_rounds_of_new_data, int)\n","        estimated_rewards_by_reg_model = np.zeros(\n","            (n_rounds_of_new_data, self.n_actions, self.len_list)\n","        )\n","        for action_ in np.arange(self.n_actions):\n","            for position_ in np.arange(self.len_list):\n","                X = self._pre_process_for_reg_model(\n","                    context=context,\n","                    action=action_ * ones_n_rounds_arr,\n","                    action_context=self.action_context,\n","                )\n","                estimated_rewards_ = (\n","                    self.base_model_list[position_].predict_proba(X)[:, 1]\n","                    if is_classifier(self.base_model_list[position_])\n","                    else self.base_model_list[position_].predict(X)\n","                )\n","                estimated_rewards_by_reg_model[\n","                    np.arange(n_rounds_of_new_data),\n","                    action_ * ones_n_rounds_arr,\n","                    position_ * ones_n_rounds_arr,\n","                ] = estimated_rewards_\n","        return estimated_rewards_by_reg_model\n","\n","    def fit_predict(\n","        self,\n","        context: np.ndarray,\n","        action: np.ndarray,\n","        reward: np.ndarray,\n","        pscore: Optional[np.ndarray] = None,\n","        position: Optional[np.ndarray] = None,\n","        action_dist: Optional[np.ndarray] = None,\n","        n_folds: int = 1,\n","        random_state: Optional[int] = None,\n","    ) -> np.ndarray:\n","        \"\"\"Fit the regression model on given logged bandit feedback data and predict the reward function of the same data.\n","        Note\n","        ------\n","        When `n_folds` is larger than 1, then the cross-fitting procedure is applied.\n","        See the reference for the details about the cross-fitting technique.\n","        Parameters\n","        ----------\n","        context: array-like, shape (n_rounds, dim_context)\n","            Context vectors observed in each round of the logged bandit feedback, i.e., :math:`x_t`.\n","        action: array-like, shape (n_rounds,)\n","            Action sampled by behavior policy in each round of the logged bandit feedback, i.e., :math:`a_t`.\n","        reward: array-like, shape (n_rounds,)\n","            Observed rewards (or outcome) in each round, i.e., :math:`r_t`.\n","        pscore: array-like, shape (n_rounds,), default=None\n","            Action choice probabilities (propensity score) of a behavior policy\n","            in the training logged bandit feedback.\n","            When None is given, the the behavior policy is assumed to be a uniform one.\n","        position: array-like, shape (n_rounds,), default=None\n","            Position of recommendation interface where action was presented in each round of the given logged bandit data.\n","            If None is set, a regression model assumes that there is only one position.\n","            When `len_list` > 1, this position argument has to be set.\n","        action_dist: array-like, shape (n_rounds, n_actions, len_list), default=None\n","            Action choice probabilities of evaluation policy (can be deterministic), i.e., :math:`\\\\pi_e(a_t|x_t)`.\n","            When either of 'iw' or 'mrdr' is used as the 'fitting_method' argument, then `action_dist` must be given.\n","        n_folds: int, default=1\n","            Number of folds in the cross-fitting procedure.\n","            When 1 is given, the regression model is trained on the whole logged bandit feedback data.\n","            Please refer to https://arxiv.org/abs/2002.08536 about the details of the cross-fitting procedure.\n","        random_state: int, default=None\n","            `random_state` affects the ordering of the indices, which controls the randomness of each fold.\n","            See https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.KFold.html for the details.\n","        Returns\n","        -----------\n","        estimated_rewards_by_reg_model: array-like, shape (n_rounds, n_actions, len_list)\n","            Expected rewards of new data estimated by the regression model.\n","        \"\"\"\n","        check_bandit_feedback_inputs(\n","            context=context,\n","            action=action,\n","            reward=reward,\n","            pscore=pscore,\n","            position=position,\n","            action_context=self.action_context,\n","        )\n","        n_rounds = context.shape[0]\n","\n","        check_scalar(n_folds, \"n_folds\", int, min_val=1)\n","        check_random_state(random_state)\n","\n","        if position is None or self.len_list == 1:\n","            position = np.zeros_like(action)\n","        else:\n","            if position.max() >= self.len_list:\n","                raise ValueError(\n","                    f\"position elements must be smaller than len_list, but the maximum value is {position.max()} (>= {self.len_list})\"\n","                )\n","        if self.fitting_method in [\"iw\", \"mrdr\"]:\n","            if not (isinstance(action_dist, np.ndarray) and action_dist.ndim == 3):\n","                raise ValueError(\n","                    \"when fitting_method is either 'iw' or 'mrdr', action_dist (a 3-dimensional ndarray) must be given\"\n","                )\n","            if action_dist.shape != (n_rounds, self.n_actions, self.len_list):\n","                raise ValueError(\n","                    f\"shape of action_dist must be (n_rounds, n_actions, len_list)=({n_rounds, self.n_actions, self.len_list}), but is {action_dist.shape}\"\n","                )\n","        if pscore is None:\n","            pscore = np.ones_like(action) / self.n_actions\n","\n","        if n_folds == 1:\n","            self.fit(\n","                context=context,\n","                action=action,\n","                reward=reward,\n","                pscore=pscore,\n","                position=position,\n","                action_dist=action_dist,\n","            )\n","            return self.predict(context=context)\n","        else:\n","            estimated_rewards_by_reg_model = np.zeros(\n","                (n_rounds, self.n_actions, self.len_list)\n","            )\n","        kf = KFold(n_splits=n_folds, shuffle=True, random_state=random_state)\n","        kf.get_n_splits(context)\n","        for train_idx, test_idx in kf.split(context):\n","            action_dist_tr = (\n","                action_dist[train_idx] if action_dist is not None else action_dist\n","            )\n","            self.fit(\n","                context=context[train_idx],\n","                action=action[train_idx],\n","                reward=reward[train_idx],\n","                pscore=pscore[train_idx],\n","                position=position[train_idx],\n","                action_dist=action_dist_tr,\n","            )\n","            estimated_rewards_by_reg_model[test_idx, :, :] = self.predict(\n","                context=context[test_idx]\n","            )\n","        return estimated_rewards_by_reg_model\n","\n","    def _pre_process_for_reg_model(\n","        self,\n","        context: np.ndarray,\n","        action: np.ndarray,\n","        action_context: np.ndarray,\n","    ) -> np.ndarray:\n","        \"\"\"Preprocess feature vectors to train a regression model.\n","        Note\n","        -----\n","        Please override this method if you want to use another feature enginnering\n","        for training the regression model.\n","        Parameters\n","        -----------\n","        context: array-like, shape (n_rounds,)\n","            Context vectors observed in each round of the logged bandit feedback, i.e., :math:`x_t`.\n","        action: array-like, shape (n_rounds,)\n","            Action sampled by behavior policy in each round of the logged bandit feedback, i.e., :math:`a_t`.\n","        action_context: array-like, shape shape (n_actions, dim_action_context)\n","            Context vector characterizing action (i.e., vector representation of each action).\n","        \"\"\"\n","        return np.c_[context, action_context[action]]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"TkJk1PKQnSx3"},"source":["## Policies"]},{"cell_type":"code","metadata":{"id":"30YB9wGCu0K6"},"source":["from dataclasses import dataclass\n","import os\n","from typing import Optional\n","  \n","import enum\n","\n","import numpy as np\n","from sklearn.utils import check_scalar\n","\n","# import pkg_resources\n","import yaml\n","\n","from abc import ABCMeta\n","from abc import abstractmethod\n","from dataclasses import dataclass\n","from typing import Optional\n","\n","import numpy as np\n","from sklearn.utils import check_random_state\n","from sklearn.utils import check_scalar"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ldpa49SYv-gA","executionInfo":{"status":"ok","timestamp":1631966001357,"user_tz":-330,"elapsed":19,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}},"outputId":"676d6dde-1ec9-4a39-b28d-d357a66dc2da"},"source":["%%writefile prior_bts.yaml\n","all:\n","  alpha:\n","    - 47.0\n","    - 8.0\n","    - 62.0\n","    - 142.0\n","    - 3.0\n","    - 14.0\n","    - 7.0\n","    - 857.0\n","    - 12.0\n","    - 15.0\n","    - 6.0\n","    - 100.0\n","    - 48.0\n","    - 23.0\n","    - 71.0\n","    - 61.0\n","    - 13.0\n","    - 16.0\n","    - 518.0\n","    - 30.0\n","    - 7.0\n","    - 4.0\n","    - 23.0\n","    - 8.0\n","    - 10.0\n","    - 11.0\n","    - 11.0\n","    - 18.0\n","    - 121.0\n","    - 11.0\n","    - 11.0\n","    - 10.0\n","    - 14.0\n","    - 9.0\n","    - 204.0\n","    - 58.0\n","    - 3.0\n","    - 19.0\n","    - 42.0\n","    - 1013.0\n","    - 2.0\n","    - 328.0\n","    - 15.0\n","    - 31.0\n","    - 14.0\n","    - 138.0\n","    - 45.0\n","    - 55.0\n","    - 23.0\n","    - 38.0\n","    - 10.0\n","    - 401.0\n","    - 52.0\n","    - 6.0\n","    - 3.0\n","    - 6.0\n","    - 5.0\n","    - 32.0\n","    - 35.0\n","    - 133.0\n","    - 52.0\n","    - 820.0\n","    - 43.0\n","    - 195.0\n","    - 8.0\n","    - 42.0\n","    - 40.0\n","    - 4.0\n","    - 32.0\n","    - 30.0\n","    - 9.0\n","    - 22.0\n","    - 6.0\n","    - 23.0\n","    - 5.0\n","    - 54.0\n","    - 8.0\n","    - 22.0\n","    - 65.0\n","    - 246.0\n","  beta:\n","    - 12198.0\n","    - 3566.0\n","    - 15993.0\n","    - 35522.0\n","    - 2367.0\n","    - 4609.0\n","    - 3171.0\n","    - 181745.0\n","    - 4372.0\n","    - 4951.0\n","    - 3100.0\n","    - 24665.0\n","    - 13210.0\n","    - 7061.0\n","    - 18061.0\n","    - 17449.0\n","    - 5644.0\n","    - 6787.0\n","    - 111326.0\n","    - 8776.0\n","    - 3334.0\n","    - 2271.0\n","    - 7389.0\n","    - 2659.0\n","    - 3665.0\n","    - 4724.0\n","    - 3561.0\n","    - 5085.0\n","    - 27407.0\n","    - 4601.0\n","    - 4756.0\n","    - 4120.0\n","    - 4736.0\n","    - 3788.0\n","    - 45292.0\n","    - 14719.0\n","    - 2189.0\n","    - 5589.0\n","    - 11995.0\n","    - 222255.0\n","    - 2308.0\n","    - 70034.0\n","    - 4801.0\n","    - 8274.0\n","    - 5421.0\n","    - 31912.0\n","    - 12213.0\n","    - 13576.0\n","    - 6230.0\n","    - 10382.0\n","    - 4141.0\n","    - 85731.0\n","    - 12811.0\n","    - 2707.0\n","    - 2250.0\n","    - 2668.0\n","    - 2886.0\n","    - 9581.0\n","    - 9465.0\n","    - 28336.0\n","    - 12062.0\n","    - 162793.0\n","    - 12107.0\n","    - 41240.0\n","    - 3162.0\n","    - 11604.0\n","    - 10818.0\n","    - 2923.0\n","    - 8897.0\n","    - 8654.0\n","    - 4000.0\n","    - 6580.0\n","    - 3174.0\n","    - 6766.0\n","    - 2602.0\n","    - 14506.0\n","    - 3968.0\n","    - 7523.0\n","    - 16532.0\n","    - 51964.0\n","men:\n","  alpha:\n","    - 47.0\n","    - 8.0\n","    - 62.0\n","    - 142.0\n","    - 3.0\n","    - 6.0\n","    - 100.0\n","    - 48.0\n","    - 23.0\n","    - 71.0\n","    - 61.0\n","    - 13.0\n","    - 16.0\n","    - 518.0\n","    - 30.0\n","    - 7.0\n","    - 4.0\n","    - 23.0\n","    - 8.0\n","    - 10.0\n","    - 11.0\n","    - 11.0\n","    - 18.0\n","    - 121.0\n","    - 11.0\n","    - 4.0\n","    - 32.0\n","    - 30.0\n","    - 9.0\n","    - 22.0\n","    - 6.0\n","    - 23.0\n","    - 5.0\n","    - 54.0\n","  beta:\n","    - 12198.0\n","    - 3566.0\n","    - 15993.0\n","    - 35522.0\n","    - 2367.0\n","    - 3100.0\n","    - 24665.0\n","    - 13210.0\n","    - 7061.0\n","    - 18061.0\n","    - 17449.0\n","    - 5644.0\n","    - 6787.0\n","    - 111326.0\n","    - 8776.0\n","    - 3334.0\n","    - 2271.0\n","    - 7389.0\n","    - 2659.0\n","    - 3665.0\n","    - 4724.0\n","    - 3561.0\n","    - 5085.0\n","    - 27407.0\n","    - 4601.0\n","    - 2923.0\n","    - 8897.0\n","    - 8654.0\n","    - 4000.0\n","    - 6580.0\n","    - 3174.0\n","    - 6766.0\n","    - 2602.0\n","    - 14506.0\n","women:\n","  alpha:\n","    - 12.0\n","    - 7.0\n","    - 984.0\n","    - 13.0\n","    - 15.0\n","    - 15.0\n","    - 11.0\n","    - 14.0\n","    - 9.0\n","    - 200.0\n","    - 72.0\n","    - 3.0\n","    - 14.0\n","    - 49.0\n","    - 1278.0\n","    - 3.0\n","    - 325.0\n","    - 14.0\n","    - 27.0\n","    - 14.0\n","    - 169.0\n","    - 48.0\n","    - 47.0\n","    - 18.0\n","    - 40.0\n","    - 12.0\n","    - 447.0\n","    - 46.0\n","    - 5.0\n","    - 3.0\n","    - 5.0\n","    - 7.0\n","    - 35.0\n","    - 34.0\n","    - 99.0\n","    - 30.0\n","    - 880.0\n","    - 51.0\n","    - 182.0\n","    - 6.0\n","    - 45.0\n","    - 39.0\n","    - 10.0\n","    - 24.0\n","    - 72.0\n","    - 229.0\n","  beta:\n","    - 3612.0\n","    - 3173.0\n","    - 204484.0\n","    - 4517.0\n","    - 4765.0\n","    - 5331.0\n","    - 4131.0\n","    - 4728.0\n","    - 4028.0\n","    - 44280.0\n","    - 17918.0\n","    - 2309.0\n","    - 4339.0\n","    - 12922.0\n","    - 270771.0\n","    - 2480.0\n","    - 68475.0\n","    - 5129.0\n","    - 7367.0\n","    - 5819.0\n","    - 38026.0\n","    - 13047.0\n","    - 11604.0\n","    - 5394.0\n","    - 10912.0\n","    - 4439.0\n","    - 94485.0\n","    - 10700.0\n","    - 2679.0\n","    - 2319.0\n","    - 2578.0\n","    - 3288.0\n","    - 9566.0\n","    - 9775.0\n","    - 20120.0\n","    - 7317.0\n","    - 172026.0\n","    - 13673.0\n","    - 37329.0\n","    - 3365.0\n","    - 10911.0\n","    - 10734.0\n","    - 4278.0\n","    - 7574.0\n","    - 16826.0\n","    - 47462.0"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Writing prior_bts.yaml\n"]}]},{"cell_type":"code","metadata":{"id":"V0p-uwZquz_a"},"source":["# configurations to replicate the Bernoulli Thompson Sampling policy used in ZOZOTOWN production\n","prior_bts_file = \"prior_bts.yaml\"\n","with open(prior_bts_file, \"rb\") as f:\n","    production_prior_for_bts = yaml.safe_load(f)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"rlzGNao1vzus"},"source":["class PolicyType(enum.Enum):\n","    \"\"\"Policy type.\n","    Attributes\n","    ----------\n","    CONTEXT_FREE:\n","        The policy type is contextfree.\n","    CONTEXTUAL:\n","        The policy type is contextual.\n","    OFFLINE:\n","        The policy type is offline.\n","    \"\"\"\n","\n","    CONTEXT_FREE = enum.auto()\n","    CONTEXTUAL = enum.auto()\n","    OFFLINE = enum.auto()\n","\n","    def __repr__(self) -> str:\n","\n","        return str(self)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dglvKky_vq1w"},"source":["### Base Context Free Policy"]},{"cell_type":"code","metadata":{"id":"owf3bFVPvih7"},"source":["@dataclass\n","class BaseContextFreePolicy(metaclass=ABCMeta):\n","    \"\"\"Base class for context-free bandit policies.\n","    Parameters\n","    ----------\n","    n_actions: int\n","        Number of actions.\n","    len_list: int, default=1\n","        Length of a list of actions recommended in each impression.\n","        When Open Bandit Dataset is used, 3 should be set.\n","    batch_size: int, default=1\n","        Number of samples used in a batch parameter update.\n","    random_state: int, default=None\n","        Controls the random seed in sampling actions.\n","    \"\"\"\n","\n","    n_actions: int\n","    len_list: int = 1\n","    batch_size: int = 1\n","    random_state: Optional[int] = None\n","\n","    def __post_init__(self) -> None:\n","        \"\"\"Initialize Class.\"\"\"\n","        check_scalar(self.n_actions, \"n_actions\", int, min_val=2)\n","        check_scalar(self.len_list, \"len_list\", int, min_val=1, max_val=self.n_actions)\n","        check_scalar(self.batch_size, \"batch_size\", int, min_val=1)\n","        self.n_trial = 0\n","        self.random_ = check_random_state(self.random_state)\n","        self.action_counts = np.zeros(self.n_actions, dtype=int)\n","        self.action_counts_temp = np.zeros(self.n_actions, dtype=int)\n","        self.reward_counts_temp = np.zeros(self.n_actions)\n","        self.reward_counts = np.zeros(self.n_actions)\n","\n","    @property\n","    def policy_type(self) -> PolicyType:\n","        \"\"\"Type of the bandit policy.\"\"\"\n","        return PolicyType.CONTEXT_FREE\n","\n","    def initialize(self) -> None:\n","        \"\"\"Initialize Parameters.\"\"\"\n","        self.n_trial = 0\n","        self.random_ = check_random_state(self.random_state)\n","        self.action_counts = np.zeros(self.n_actions, dtype=int)\n","        self.action_counts_temp = np.zeros(self.n_actions, dtype=int)\n","        self.reward_counts_temp = np.zeros(self.n_actions)\n","        self.reward_counts = np.zeros(self.n_actions)\n","\n","    @abstractmethod\n","    def select_action(self) -> np.ndarray:\n","        \"\"\"Select a list of actions.\"\"\"\n","        raise NotImplementedError\n","\n","    @abstractmethod\n","    def update_params(self, action: int, reward: float) -> None:\n","        \"\"\"Update policy parameters.\"\"\"\n","        raise NotImplementedError"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"l6X1EJOmvEi0"},"source":["### Epsilon Greedy"]},{"cell_type":"code","metadata":{"id":"Yo60uqpkuz4x"},"source":["@dataclass\n","class EpsilonGreedy(BaseContextFreePolicy):\n","    \"\"\"Epsilon Greedy policy.\n","    Parameters\n","    ----------\n","    n_actions: int\n","        Number of actions.\n","    len_list: int, default=1\n","        Length of a list of actions recommended in each impression.\n","        When Open Bandit Dataset is used, 3 should be set.\n","    batch_size: int, default=1\n","        Number of samples used in a batch parameter update.\n","    random_state: int, default=None\n","        Controls the random seed in sampling actions.\n","    epsilon: float, default=1.\n","        Exploration hyperparameter that must take value in the range of [0., 1.].\n","    policy_name: str, default=f'egreedy_{epsilon}'.\n","        Name of bandit policy.\n","    \"\"\"\n","\n","    epsilon: float = 1.0\n","\n","    def __post_init__(self) -> None:\n","        \"\"\"Initialize Class.\"\"\"\n","        check_scalar(self.epsilon, \"epsilon\", float, min_val=0.0, max_val=1.0)\n","        self.policy_name = f\"egreedy_{self.epsilon}\"\n","        super().__post_init__()\n","\n","    def select_action(self) -> np.ndarray:\n","        \"\"\"Select a list of actions.\n","        Returns\n","        ----------\n","        selected_actions: array-like, shape (len_list, )\n","            List of selected actions.\n","        \"\"\"\n","        if (self.random_.rand() > self.epsilon) and (self.action_counts.min() > 0):\n","            predicted_rewards = self.reward_counts / self.action_counts\n","            return predicted_rewards.argsort()[::-1][: self.len_list]\n","        else:\n","            return self.random_.choice(\n","                self.n_actions, size=self.len_list, replace=False\n","            )\n","\n","    def update_params(self, action: int, reward: float) -> None:\n","        \"\"\"Update policy parameters.\n","        Parameters\n","        ----------\n","        action: int\n","            Selected action by the policy.\n","        reward: float\n","            Observed reward for the chosen action and position.\n","        \"\"\"\n","        self.n_trial += 1\n","        self.action_counts_temp[action] += 1\n","        self.reward_counts_temp[action] += reward\n","        if self.n_trial % self.batch_size == 0:\n","            self.action_counts = np.copy(self.action_counts_temp)\n","            self.reward_counts = np.copy(self.reward_counts_temp)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_HP-Jx8ovC42"},"source":["### Random"]},{"cell_type":"code","metadata":{"id":"a5UzjtLWvPUC"},"source":["@dataclass\n","class Random(EpsilonGreedy):\n","    \"\"\"Random policy\n","    Parameters\n","    ----------\n","    n_actions: int\n","        Number of actions.\n","    len_list: int, default=1\n","        Length of a list of actions recommended in each impression.\n","        When Open Bandit Dataset is used, 3 should be set.\n","    batch_size: int, default=1\n","        Number of samples used in a batch parameter update.\n","    random_state: int, default=None\n","        Controls the random seed in sampling actions.\n","    epsilon: float, default=1.\n","        Exploration hyperparameter that must take value in the range of [0., 1.].\n","    policy_name: str, default='random'.\n","        Name of bandit policy.\n","    \"\"\"\n","\n","    policy_name: str = \"random\"\n","\n","    def compute_batch_action_dist(\n","        self,\n","        n_rounds: int = 1,\n","    ) -> np.ndarray:\n","        \"\"\"Compute the distribution over actions by Monte Carlo simulation.\n","        Parameters\n","        ----------\n","        n_rounds: int, default=1\n","            Number of rounds in the distribution over actions.\n","            (the size of the first axis of `action_dist`)\n","        Returns\n","        ----------\n","        action_dist: array-like, shape (n_rounds, n_actions, len_list)\n","            Probability estimates of each arm being the best one for each sample, action, and position.\n","        \"\"\"\n","        action_dist = np.ones((n_rounds, self.n_actions, self.len_list)) * (\n","            1 / self.n_actions\n","        )\n","        return action_dist"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VJXABgMqvRL3"},"source":["### BernoulliTS"]},{"cell_type":"code","metadata":{"id":"NG9hGDDSvMpJ"},"source":["@dataclass\n","class BernoulliTS(BaseContextFreePolicy):\n","    \"\"\"Bernoulli Thompson Sampling Policy\n","    Parameters\n","    ----------\n","    n_actions: int\n","        Number of actions.\n","    len_list: int, default=1\n","        Length of a list of actions recommended in each impression.\n","        When Open Bandit Dataset is used, 3 should be set.\n","    batch_size: int, default=1\n","        Number of samples used in a batch parameter update.\n","    random_state: int, default=None\n","        Controls the random seed in sampling actions.\n","    alpha: array-like, shape (n_actions, ), default=None\n","        Prior parameter vector for Beta distributions.\n","    beta: array-like, shape (n_actions, ), default=None\n","        Prior parameter vector for Beta distributions.\n","    is_zozotown_prior: bool, default=False\n","        Whether to use hyperparameters for the beta distribution used\n","        at the start of the data collection period in ZOZOTOWN.\n","    campaign: str, default=None\n","        One of the three possible campaigns considered in ZOZOTOWN, \"all\", \"men\", and \"women\".\n","    policy_name: str, default='bts'\n","        Name of bandit policy.\n","    \"\"\"\n","\n","    alpha: Optional[np.ndarray] = None\n","    beta: Optional[np.ndarray] = None\n","    is_zozotown_prior: bool = False\n","    campaign: Optional[str] = None\n","    policy_name: str = \"bts\"\n","\n","    def __post_init__(self) -> None:\n","        \"\"\"Initialize class.\"\"\"\n","        super().__post_init__()\n","        if self.is_zozotown_prior:\n","            if self.campaign is None:\n","                raise Exception(\n","                    \"`campaign` must be specified when `is_zozotown_prior` is True.\"\n","                )\n","            self.alpha = production_prior_for_bts[self.campaign][\"alpha\"]\n","            self.beta = production_prior_for_bts[self.campaign][\"beta\"]\n","        else:\n","            self.alpha = np.ones(self.n_actions) if self.alpha is None else self.alpha\n","            self.beta = np.ones(self.n_actions) if self.beta is None else self.beta\n","\n","    def select_action(self) -> np.ndarray:\n","        \"\"\"Select a list of actions.\n","        Returns\n","        ----------\n","        selected_actions: array-like, shape (len_list, )\n","            List of selected actions.\n","        \"\"\"\n","        predicted_rewards = self.random_.beta(\n","            a=self.reward_counts + self.alpha,\n","            b=(self.action_counts - self.reward_counts) + self.beta,\n","        )\n","        return predicted_rewards.argsort()[::-1][: self.len_list]\n","\n","    def update_params(self, action: int, reward: float) -> None:\n","        \"\"\"Update policy parameters.\n","        Parameters\n","        ----------\n","        action: int\n","            Selected action by the policy.\n","        reward: float\n","            Observed reward for the chosen action and position.\n","        \"\"\"\n","        self.n_trial += 1\n","        self.action_counts_temp[action] += 1\n","        self.reward_counts_temp[action] += reward\n","        if self.n_trial % self.batch_size == 0:\n","            self.action_counts = np.copy(self.action_counts_temp)\n","            self.reward_counts = np.copy(self.reward_counts_temp)\n","\n","    def compute_batch_action_dist(\n","        self,\n","        n_rounds: int = 1,\n","        n_sim: int = 100000,\n","    ) -> np.ndarray:\n","        \"\"\"Compute the distribution over actions by Monte Carlo simulation.\n","        Parameters\n","        ----------\n","        n_rounds: int, default=1\n","            Number of rounds in the distribution over actions.\n","            (the size of the first axis of `action_dist`)\n","        n_sim: int, default=100000\n","            Number of simulations in the Monte Carlo simulation to compute the distribution over actions.\n","        Returns\n","        ----------\n","        action_dist: array-like, shape (n_rounds, n_actions, len_list)\n","            Probability estimates of each arm being the best one for each sample, action, and position.\n","        \"\"\"\n","        action_count = np.zeros((self.n_actions, self.len_list))\n","        for _ in np.arange(n_sim):\n","            selected_actions = self.select_action()\n","            for pos in np.arange(self.len_list):\n","                action_count[selected_actions[pos], pos] += 1\n","        action_dist = np.tile(\n","            action_count / n_sim,\n","            (n_rounds, 1, 1),\n","        )\n","        return action_dist"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"48QcS56Gnd7R"},"source":["## Run"]},{"cell_type":"markdown","metadata":{"id":"U9fYM9YxKlE-"},"source":["Policies"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0uWnAUctwelL","executionInfo":{"status":"ok","timestamp":1631966069604,"user_tz":-330,"elapsed":620,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}},"outputId":"211717fe-e2ae-484e-817d-52df6fa78348"},"source":["evaluation_policy_dict = dict(bts=BernoulliTS, random=Random)\n","evaluation_policy_dict"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'bts': __main__.BernoulliTS, 'random': __main__.Random}"]},"metadata":{},"execution_count":63}]},{"cell_type":"markdown","metadata":{"id":"-NDCdaSJKnJ8"},"source":["Base models"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"N8RXXGS2J6sS","executionInfo":{"status":"ok","timestamp":1631966077101,"user_tz":-330,"elapsed":385,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}},"outputId":"56e1870c-a2ef-4218-dde5-1791647e31fd"},"source":["%%writefile hyperparams.yaml\n","lightgbm:\n","    n_estimators: 30\n","    learning_rate: 0.01\n","    max_depth: 5\n","    min_samples_leaf: 10\n","    random_state: 12345\n","logistic_regression:\n","    max_iter: 10000\n","    C: 100\n","    random_state: 12345\n","random_forest:\n","    n_estimators: 30\n","    max_depth: 5\n","    min_samples_leaf: 10\n","    random_state: 12345"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Overwriting hyperparams.yaml\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1lQoBhOzKCdF","executionInfo":{"status":"ok","timestamp":1631966081302,"user_tz":-330,"elapsed":554,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}},"outputId":"41a3bf67-89bd-4aa5-a6ef-2e380d3fb59d"},"source":["# hyperparameters of the regression model used in model dependent OPE estimators\n","with open(\"hyperparams.yaml\", \"rb\") as f:\n","    hyperparams = yaml.safe_load(f)\n","hyperparams"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'lightgbm': {'learning_rate': 0.01,\n","  'max_depth': 5,\n","  'min_samples_leaf': 10,\n","  'n_estimators': 30,\n","  'random_state': 12345},\n"," 'logistic_regression': {'C': 100, 'max_iter': 10000, 'random_state': 12345},\n"," 'random_forest': {'max_depth': 5,\n","  'min_samples_leaf': 10,\n","  'n_estimators': 30,\n","  'random_state': 12345}}"]},"metadata":{},"execution_count":65}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wNR8qGA7KSMk","executionInfo":{"status":"ok","timestamp":1631966092039,"user_tz":-330,"elapsed":527,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}},"outputId":"2cb62dda-8866-4171-ada6-3cf82427fc9c"},"source":["base_model_dict = dict(\n","    logistic_regression=LogisticRegression,\n","    lightgbm=GradientBoostingClassifier,\n","    random_forest=RandomForestClassifier,\n",")\n","base_model_dict"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'lightgbm': sklearn.ensemble._gb.GradientBoostingClassifier,\n"," 'logistic_regression': sklearn.linear_model._logistic.LogisticRegression,\n"," 'random_forest': sklearn.ensemble._forest.RandomForestClassifier}"]},"metadata":{},"execution_count":66}]},{"cell_type":"markdown","metadata":{"id":"Dbr3KwvSKpoc"},"source":["OPE estimators"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UPvzieZNwrQ_","executionInfo":{"status":"ok","timestamp":1631966135960,"user_tz":-330,"elapsed":475,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}},"outputId":"d7f01f1e-4117-49cd-9a71-9d2883fa6be4"},"source":["ope_estimators = [DirectMethod(), InverseProbabilityWeighting(), DoublyRobust()]\n","ope_estimators"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[DirectMethod(estimator_name='dm'),\n"," InverseProbabilityWeighting(lambda_=inf, estimator_name='ipw'),\n"," DoublyRobust(lambda_=inf, estimator_name='dr')]"]},"metadata":{},"execution_count":67}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gONqRsIHK8r_","executionInfo":{"status":"ok","timestamp":1631966155420,"user_tz":-330,"elapsed":686,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}},"outputId":"25cb45e0-b26c-4bc8-c0c1-a336ab778f7c"},"source":["parser = argparse.ArgumentParser(description=\"evaluate off-policy estimators.\")\n","parser.add_argument(\n","    \"--n_runs\",\n","    type=int,\n","    default=1,\n","    help=\"number of bootstrap sampling in the experiment.\",\n",")\n","parser.add_argument(\n","    \"--evaluation_policy\",\n","    type=str,\n","    choices=[\"bts\", \"random\"],\n","    default='bts',\n","    help=\"evaluation policy, bts or random.\",\n",")\n","parser.add_argument(\n","    \"--base_model\",\n","    type=str,\n","    choices=[\"logistic_regression\", \"lightgbm\", \"random_forest\"],\n","    default='lightgbm',\n","    help=\"base ML model for regression model, logistic_regression, random_forest or lightgbm.\",\n",")\n","parser.add_argument(\n","    \"--behavior_policy\",\n","    type=str,\n","    choices=[\"bts\", \"random\"],\n","    default='random',\n","    help=\"behavior policy, bts or random.\",\n",")\n","parser.add_argument(\n","    \"--campaign\",\n","    type=str,\n","    choices=[\"all\", \"men\", \"women\"],\n","    default='all',\n","    help=\"campaign name, men, women, or all.\",\n",")\n","parser.add_argument(\n","    \"--n_sim_to_compute_action_dist\",\n","    type=float,\n","    default=1000000,\n","    help=\"number of monte carlo simulation to compute the action distribution of bts.\",\n",")\n","parser.add_argument(\n","    \"--n_jobs\",\n","    type=int,\n","    default=1,\n","    help=\"the maximum number of concurrently running jobs.\",\n",")\n","parser.add_argument(\"--random_state\", type=int, default=12345)\n","\n","args = parser.parse_args(args={})\n","print(args)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Namespace(base_model='lightgbm', behavior_policy='random', campaign='all', evaluation_policy='bts', n_jobs=1, n_runs=1, n_sim_to_compute_action_dist=1000000, random_state=12345)\n"]}]},{"cell_type":"code","metadata":{"id":"nLzdrdMBKyGi"},"source":["# configurations\n","n_runs = args.n_runs\n","base_model = args.base_model\n","evaluation_policy = args.evaluation_policy\n","behavior_policy = args.behavior_policy\n","campaign = args.campaign\n","n_sim_to_compute_action_dist = args.n_sim_to_compute_action_dist\n","n_jobs = args.n_jobs\n","random_state = args.random_state\n","np.random.seed(random_state)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YMwGllH6yayF","executionInfo":{"status":"ok","timestamp":1631967017681,"user_tz":-330,"elapsed":3680,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}},"outputId":"c62a0a10-a327-4f05-dd14-57b322ffe7ae"},"source":["!git clone https://github.com/st-tech/zr-obp.git"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Cloning into 'zr-obp'...\n","remote: Enumerating objects: 4993, done.\u001b[K\n","remote: Counting objects: 100% (2007/2007), done.\u001b[K\n","remote: Compressing objects: 100% (860/860), done.\u001b[K\n","remote: Total 4993 (delta 1404), reused 1661 (delta 1135), pack-reused 2986\u001b[K\n","Receiving objects: 100% (4993/4993), 27.54 MiB | 29.23 MiB/s, done.\n","Resolving deltas: 100% (3306/3306), done.\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8Cf2xmS5w5-B","executionInfo":{"status":"ok","timestamp":1631967043481,"user_tz":-330,"elapsed":496,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}},"outputId":"b8c142db-f932-420b-bfe1-e94f9db94280"},"source":["obd = OpenBanditDataset(behavior_policy=behavior_policy, campaign=campaign, data_path=Path('/content/zr-obp/obd'))\n","obd"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["OpenBanditDataset(behavior_policy='random', campaign='all', data_path=PosixPath('/content/zr-obp/obd/random/all'), dataset_name='obd')"]},"metadata":{},"execution_count":78}]},{"cell_type":"markdown","metadata":{"id":"-UXvvdCeN46a"},"source":["Compute action distribution by evaluation policy"]},{"cell_type":"code","metadata":{"id":"COzp35bXNXYj"},"source":["kwargs = dict(\n","    n_actions=obd.n_actions, len_list=obd.len_list, random_state=random_state\n",")\n","\n","if evaluation_policy == \"bts\":\n","    kwargs[\"is_zozotown_prior\"] = True\n","    kwargs[\"campaign\"] = campaign\n","\n","policy = evaluation_policy_dict[evaluation_policy](**kwargs)\n","\n","action_dist_single_round = policy.compute_batch_action_dist(\n","    n_sim=n_sim_to_compute_action_dist\n",")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"55mQ8tpZOLhY"},"source":["Ground-truth policy value of an evaluation policy, which is estimated with factual (observed) rewards (on-policy estimation)"]},{"cell_type":"code","metadata":{"id":"35a15U14N78j"},"source":["ground_truth_policy_value = OpenBanditDataset.calc_on_policy_policy_value_estimate(\n","    behavior_policy=evaluation_policy,\n","    campaign=campaign,\n",")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ziedh4N6OSSP"},"source":["def process(b: int):\n","    # sample bootstrap from batch logged bandit feedback\n","    bandit_feedback = obd.sample_bootstrap_bandit_feedback(random_state=b)\n","    # estimate the mean reward function with an ML model\n","    regression_model = RegressionModel(\n","        n_actions=obd.n_actions,\n","        len_list=obd.len_list,\n","        action_context=obd.action_context,\n","        base_model=base_model_dict[base_model](**hyperparams[base_model]),\n","    )\n","    estimated_rewards_by_reg_model = regression_model.fit_predict(\n","        context=bandit_feedback[\"context\"],\n","        action=bandit_feedback[\"action\"],\n","        reward=bandit_feedback[\"reward\"],\n","        position=bandit_feedback[\"position\"],\n","        pscore=bandit_feedback[\"pscore\"],\n","        n_folds=3,  # 3-fold cross-fitting\n","        random_state=random_state,\n","    )\n","    # evaluate estimators' performances using relative estimation error (relative-ee)\n","    ope = OffPolicyEvaluation(\n","        bandit_feedback=bandit_feedback,\n","        ope_estimators=ope_estimators,\n","    )\n","    action_dist = np.tile(\n","        action_dist_single_round, (bandit_feedback[\"n_rounds\"], 1, 1)\n","    )\n","    relative_ee_b = ope.evaluate_performance_of_estimators(\n","        ground_truth_policy_value=ground_truth_policy_value,\n","        action_dist=action_dist,\n","        estimated_rewards_by_reg_model=estimated_rewards_by_reg_model,\n","    )\n","\n","    return relative_ee_b"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IQEKco8T3CzY","executionInfo":{"status":"ok","timestamp":1631967958429,"user_tz":-330,"elapsed":3527,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}},"outputId":"aff02db7-a1cd-4f76-fbb9-123f30ab4225"},"source":["processed = Parallel(n_jobs=n_jobs, verbose=50)([delayed(process)(i) for i in np.arange(n_runs)])\n","\n","relative_ee_dict = {est.estimator_name: dict() for est in ope_estimators}\n","\n","for b, relative_ee_b in enumerate(processed):\n","    for (estimator_name, relative_ee_) in relative_ee_b.items():\n","        relative_ee_dict[estimator_name][b] = relative_ee_"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n","[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    3.0s remaining:    0.0s\n","[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    3.0s finished\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"E5KHoKu03wd2","executionInfo":{"status":"ok","timestamp":1631967974688,"user_tz":-330,"elapsed":395,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}},"outputId":"2212f4b5-065a-4a86-87eb-0c43163574b7"},"source":["relative_ee_df = DataFrame(relative_ee_dict).describe().T.round(6)\n","\n","print(\"=\" * 30)\n","print(f\"random_state={random_state}\")\n","print(\"-\" * 30)\n","print(relative_ee_df[[\"mean\", \"std\"]])\n","print(\"=\" * 30)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["==============================\n","random_state=12345\n","------------------------------\n","         mean  std\n","dm   0.034354  NaN\n","ipw  0.100573  NaN\n","dr   0.096567  NaN\n","==============================\n"]}]},{"cell_type":"code","metadata":{"id":"zH69t50wPBEM"},"source":["# save results of the evaluation of off-policy estimators in './logs' directory.\n","log_path = Path(\"./logs\") / behavior_policy / campaign\n","log_path.mkdir(exist_ok=True, parents=True)\n","relative_ee_df.to_csv(log_path / \"relative_ee_of_ope_estimators.csv\")"],"execution_count":null,"outputs":[]}]}