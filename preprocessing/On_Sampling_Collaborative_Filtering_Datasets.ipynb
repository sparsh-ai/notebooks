{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U networkit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_path(*args, **kwargs):\n",
    "    return '/content/recohut/datasets/ml-100k'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Torch utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "is_cuda_available = torch.cuda.is_available()\n",
    "\n",
    "if is_cuda_available: \n",
    "    print(\"Using CUDA...\\n\")\n",
    "    LongTensor = torch.cuda.LongTensor\n",
    "    FloatTensor = torch.cuda.FloatTensor\n",
    "    BoolTensor = torch.cuda.BoolTensor\n",
    "else:\n",
    "    LongTensor = torch.LongTensor\n",
    "    FloatTensor = torch.FloatTensor\n",
    "    BoolTensor = torch.BoolTensor\n",
    "\n",
    "# def get_model_class(hyper_params):\n",
    "#     from pytorch_models import MF, MVAE, SASRec, SVAE\n",
    "\n",
    "#     return {\n",
    "#         \"bias_only\": MF.MF,\n",
    "#         \"MF_dot\": MF.MF,\n",
    "#         \"MF\": MF.MF,\n",
    "#         \"MVAE\": MVAE.MVAE,\n",
    "#         \"SVAE\": SVAE.SVAE,\n",
    "#         \"SASRec\": SASRec.SASRec,\n",
    "#     }[hyper_params['model_type']]\n",
    "\n",
    "def get_model_class(hyper_params):\n",
    "\n",
    "    return {\n",
    "        \"bias_only\":MF,\n",
    "        \"MF_dot\": MF,\n",
    "        \"MF\": MF,\n",
    "    }[hyper_params['model_type']]\n",
    "\n",
    "\n",
    "def xavier_init(model):\n",
    "    for _, param in model.named_parameters():\n",
    "        try: torch.nn.init.xavier_uniform_(param.data)\n",
    "        except: pass # just ignore those failed init layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INF = float(1e6)\n",
    "\n",
    "# def get_data_loader_class(hyper_params):\n",
    "#     from data_loaders import MF, MVAE, SASRec, SVAE\n",
    "\n",
    "#     return {\n",
    "#         \"pop_rec\": (MF.TrainDataset, MF.TestDataset),\n",
    "#         \"bias_only\": (MF.TrainDataset, MF.TestDataset),\n",
    "#         \"MF_dot\": (MF.TrainDataset, MF.TestDataset),\n",
    "#         \"MF\": (MF.TrainDataset, MF.TestDataset),\n",
    "#         \"NeuMF\": (MF.TrainDataset, MF.TestDataset),\n",
    "#         \"MVAE\": (MVAE.TrainDataset, MVAE.TestDataset),\n",
    "#         \"SVAE\": (SVAE.TrainDataset, SVAE.TestDataset),\n",
    "#         \"SASRec\": (SASRec.TrainDataset, SASRec.TestDataset),\n",
    "#     }[hyper_params['model_type']]\n",
    "\n",
    "def get_data_loader_class(hyper_params):\n",
    "\n",
    "    return {\n",
    "        \"pop_rec\": (TrainDataset, TestDataset),\n",
    "        \"bias_only\": (TrainDataset, TestDataset),\n",
    "        \"MF_dot\": (TrainDataset, TestDataset),\n",
    "        \"MF\": (TrainDataset, TestDataset),\n",
    "        \"NeuMF\": (TrainDataset, TestDataset),\n",
    "        \"MVAE\": (TrainDataset, TestDataset),\n",
    "        \"SVAE\": (TrainDataset, TestDataset),\n",
    "        \"SASRec\": (TrainDataset, TestDataset),\n",
    "    }[hyper_params['model_type']]\n",
    "\n",
    "def valid_hyper_params(hyper_params):\n",
    "    ## Check if the methods and task match\n",
    "    valid_tasks = {\n",
    "        \"pop_rec\":      [             'implicit', 'sequential' ],\n",
    "        \"bias_only\":    [ 'explicit', 'implicit', 'sequential' ],\n",
    "        \"MF_dot\":       [ 'explicit', 'implicit', 'sequential' ],\n",
    "        \"MF\":           [ 'explicit', 'implicit', 'sequential' ],\n",
    "        \"NeuMF\":        [ 'explicit', 'implicit', 'sequential' ],\n",
    "        \"MVAE\":         [             'implicit', 'sequential' ],\n",
    "        \"SVAE\":         [                         'sequential' ],\n",
    "        \"SASRec\":       [                         'sequential' ],\n",
    "    }[hyper_params['model_type']]\n",
    "\n",
    "    return hyper_params['task'] in valid_tasks\n",
    "\n",
    "def get_common_path(hyper_params, star_match = False):\n",
    "    ## E.g. Running SASRec on an explicit/implicit feedback task.\n",
    "    if not valid_hyper_params(hyper_params): return None\n",
    "\n",
    "    # To avoid writing hyper_params[key] everytime\n",
    "    def get(key): \n",
    "        if star_match: return hyper_params.get(key, \".*\")\n",
    "        return hyper_params[key]\n",
    "\n",
    "    common_path = \"{}_{}\".format(get('dataset'), get('task'))\n",
    "\n",
    "    if get('sampling')[:3] == 'svp':\n",
    "        common_path += \"_{}_{}_perc_{}\".format(get('sampling'), get('sampling_percent'), get('sampling_svp'))\n",
    "    elif get('sampling') == 'complete_data': common_path += \"_complete_data\"\n",
    "    else: common_path += \"_{}_perc_{}\".format(get('sampling_percent'), get('sampling'))\n",
    "    \n",
    "    common_path += \"_{}\".format(get('model_type')) + {\n",
    "        \".*\":        lambda: \"\",\n",
    "        \"pop_rec\":   lambda: \"\",\n",
    "        \"bias_only\": lambda: \"\",\n",
    "        \"MF_dot\":    lambda: \"_latent_size_{}_dropout_{}\".format(get('latent_size'), get('dropout')),\n",
    "        \"MF\":        lambda: \"_latent_size_{}_dropout_{}\".format(get('latent_size'), get('dropout')),\n",
    "        \"NeuMF\":     lambda: \"_latent_size_{}_dropout_{}\".format(get('latent_size'), get('dropout')),\n",
    "        \"MVAE\":      lambda: \"_latent_size_{}_dropout_{}\".format(get('latent_size'), get('dropout')),\n",
    "        \"SVAE\":      lambda: \"_latent_size_{}_dropout_{}_next_{}\".format(get('latent_size'), get('dropout'), get('num_next')),\n",
    "        \"SASRec\":    lambda: \"_latent_size_{}_dropout_{}_heads_{}_blocks_{}\".format(get('latent_size'), get('dropout'), get('num_heads'), get('num_blocks')),\n",
    "    }[get('model_type')]() # lambda to ensure lazy evaluation\n",
    "\n",
    "    if get('task') in [ 'implicit', 'sequential' ]:\n",
    "        common_path += \"_trn_negs_{}_tst_negs_{}\".format(get('num_train_negs'), get('num_test_negs'))\n",
    "\n",
    "    common_path += \"_wd_{}_lr_{}\".format(get('weight_decay'), get('lr'))\n",
    "\n",
    "    return common_path\n",
    "\n",
    "def remap_items(data):\n",
    "    item_map = {}\n",
    "    for user_data in data:\n",
    "        for item, rating, time in user_data:\n",
    "            if item not in item_map: item_map[item] = len(item_map) + 1\n",
    "\n",
    "    for u in range(len(data)):\n",
    "        data[u] = list(map(lambda x: [ item_map[x[0]], x[1], x[2] ], data[u]))\n",
    "\n",
    "    return data\n",
    "\n",
    "def file_write(log_file, s, dont_print=False):\n",
    "    if dont_print == False: print(s)\n",
    "    f = open(log_file, 'a')\n",
    "    f.write(s+'\\n')\n",
    "    f.close()\n",
    "\n",
    "def clear_log_file(log_file):\n",
    "    f = open(log_file, 'w')\n",
    "    f.write('')\n",
    "    f.close()\n",
    "\n",
    "def pretty_print(h):\n",
    "    print(\"{\")\n",
    "    for key in h:\n",
    "        print(' ' * 4 + str(key) + ': ' + h[key])\n",
    "    print('}\\n')\n",
    "\n",
    "def log_end_epoch(hyper_params, metrics, epoch, time_elpased, metrics_on = '(VAL)', dont_print = False):\n",
    "    string2 = \"\"\n",
    "    for m in metrics: string2 += \" | \" + m + ' = ' + str(metrics[m])\n",
    "    string2 += ' ' + metrics_on\n",
    "\n",
    "    ss  = '-' * 89\n",
    "    ss += '\\n| end of epoch {} | time = {:5.2f}'.format(epoch, time_elpased)\n",
    "    ss += string2\n",
    "    ss += '\\n'\n",
    "    ss += '-' * 89\n",
    "    file_write(hyper_params['log_file'], ss, dont_print = dont_print)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Sampling experiments' constants\n",
    "BASE_SAMPLING_PATH = \"./experiments/sampling_runs/\"\n",
    "\n",
    "# Data-genie experiments' constants\n",
    "BASE_DATA_GENIE_PATH = \"./experiments/data_genie/\"\n",
    "\n",
    "def get_svp_log_file_path(hyper_params):\n",
    "    return BASE_SAMPLING_PATH + \"/results/logs/SVP/{}.txt\".format(get_common_path(hyper_params))\n",
    "\n",
    "def get_svp_model_file_path(hyper_params):\n",
    "    return BASE_SAMPLING_PATH + \"/results/models/SVP/{}.pt\".format(get_common_path(hyper_params))\n",
    "\n",
    "def get_log_base_path():\n",
    "\treturn BASE_SAMPLING_PATH + \"/results/logs/trained/\"\n",
    "\n",
    "def get_log_file_path(hyper_params):\n",
    "\treturn get_log_base_path() + get_common_path(hyper_params) + \".txt\"\n",
    "\n",
    "def get_model_file_path(hyper_params):\n",
    "\treturn BASE_SAMPLING_PATH + \"/results/models/trained/\" + get_common_path(hyper_params) + \".pt\"\n",
    "\n",
    "def get_data_path(hyper_params):\n",
    "    dataset = hyper_params\n",
    "    if type(dataset) != str: dataset = hyper_params['dataset']\n",
    "    return \"./datasets/{}/\".format(dataset)\n",
    "\n",
    "def get_index_path(hyper_params):\n",
    "    train_test_split = {\n",
    "        'explicit':    '20_percent_hist',\n",
    "        'implicit':    '20_percent_hist',\n",
    "        'sequential':  'leave_2',\n",
    "    }[hyper_params['task']]\n",
    "\n",
    "    ret = get_data_path(hyper_params['dataset']) + \"/{}/\".format(train_test_split)\n",
    "\n",
    "    if hyper_params['sampling'][:3] == 'svp':\n",
    "        ret += \"{}_{}/{}_perc_{}/\".format(\n",
    "            hyper_params['sampling'], hyper_params['task'],\n",
    "            hyper_params['sampling_percent'], hyper_params['sampling_svp']\n",
    "        )\n",
    "    elif hyper_params['sampling'] == 'complete_data':\n",
    "        ret += \"complete_data/\"\n",
    "    else:\n",
    "        ret += \"{}_perc_{}/\".format(\n",
    "            hyper_params['sampling_percent'], hyper_params['sampling'],\n",
    "        )\n",
    "\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# from torch_utils import LongTensor, FloatTensor\n",
    "\n",
    "class BaseMF(nn.Module):\n",
    "    def __init__(self, hyper_params, keep_gamma = True):\n",
    "        super(BaseMF, self).__init__()\n",
    "        self.hyper_params = hyper_params\n",
    "\n",
    "        # Declaring alpha, beta, gamma\n",
    "        self.global_bias = nn.Parameter(FloatTensor([ 4.0 if hyper_params['task'] == 'explicit' else 0.5 ]))\n",
    "        self.user_bias = nn.Parameter(FloatTensor([ 0.0 for _ in range(hyper_params['total_users']) ]))\n",
    "        self.item_bias = nn.Parameter(FloatTensor([ 0.0 for _ in range(hyper_params['total_items']) ]))\n",
    "        if keep_gamma:\n",
    "            self.user_embedding = nn.Embedding(hyper_params['total_users'], hyper_params['latent_size'])\n",
    "            self.item_embedding = nn.Embedding(hyper_params['total_items'], hyper_params['latent_size'])\n",
    "\n",
    "        # For faster evaluation\n",
    "        self.all_items_vector = LongTensor(\n",
    "            list(range(hyper_params['total_items']))\n",
    "        )\n",
    "\n",
    "    def get_score(self, data):\n",
    "        pass # Virtual function, implement in all sub-classes\n",
    "\n",
    "    def forward(self, data, eval = False):\n",
    "        user_id, pos_item_id, neg_items = data\n",
    "\n",
    "        # Evaluation -- Rank all items\n",
    "        if pos_item_id is None: \n",
    "            ret = []\n",
    "            for b in range(user_id.shape[0]):\n",
    "                ret.append(self.get_score(\n",
    "                    user_id[b].unsqueeze(-1).repeat(1, self.hyper_params['total_items']).view(-1), \n",
    "                    self.all_items_vector.view(-1)\n",
    "                ).view(1, -1))\n",
    "            return torch.cat(ret)\n",
    "        \n",
    "        # Explicit feedback\n",
    "        if neg_items is None: return self.get_score(user_id, pos_item_id.squeeze(-1))\n",
    "        \n",
    "        # Implicit feedback\n",
    "        return self.get_score(\n",
    "            user_id.unsqueeze(-1).repeat(1, pos_item_id.shape[1]).view(-1), \n",
    "            pos_item_id.view(-1)\n",
    "        ).view(pos_item_id.shape), self.get_score(\n",
    "            user_id.unsqueeze(-1).repeat(1, neg_items.shape[1]).view(-1), \n",
    "            neg_items.view(-1)\n",
    "        ).view(neg_items.shape)\n",
    "\n",
    "class MF(BaseMF):\n",
    "    def __init__(self, hyper_params):\n",
    "        keep_gamma = hyper_params['model_type'] != 'bias_only'\n",
    "\n",
    "        super(MF, self).__init__(hyper_params, keep_gamma = keep_gamma)\n",
    "        if keep_gamma: self.dropout = nn.Dropout(hyper_params['dropout'])\n",
    "\n",
    "        if hyper_params['model_type'] == 'MF':\n",
    "            latent_size = hyper_params['latent_size']\n",
    "\n",
    "            self.projection = nn.Sequential(\n",
    "                nn.Dropout(hyper_params['dropout']),\n",
    "                nn.Linear(2 * latent_size, latent_size),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(latent_size, latent_size)\n",
    "            )\n",
    "            for m in self.projection:\n",
    "                if isinstance(m, nn.Linear): nn.init.xavier_uniform_(m.weight)\n",
    "\n",
    "            self.final = nn.Linear(2 * latent_size, 1)\n",
    "            self.sigmoid = nn.Sigmoid()\n",
    "            self.relu = nn.ReLU()\n",
    "\n",
    "    def get_score(self, user_id, item_id):\n",
    "        # For the FM\n",
    "        user_bias = self.user_bias.gather(0, user_id.view(-1)).view(user_id.shape)\n",
    "        item_bias = self.item_bias.gather(0, item_id.view(-1)).view(item_id.shape)\n",
    "\n",
    "        if self.hyper_params['model_type'] == 'bias_only': \n",
    "            return user_bias + item_bias + self.global_bias\n",
    "\n",
    "        # Embed Latent space\n",
    "        user = self.dropout(self.user_embedding(user_id.view(-1))) # [bsz x 32]\n",
    "        item = self.dropout(self.item_embedding(item_id.view(-1))) # [bsz x 32]\n",
    "\n",
    "        # Dot product\n",
    "        if self.hyper_params['model_type'] == 'MF_dot':\n",
    "            rating = torch.sum(user * item, dim = -1).view(user_id.shape)\n",
    "            return user_bias + item_bias + self.global_bias + rating\n",
    "\n",
    "        mf_vector = user * item\n",
    "        cat = torch.cat([ user, item ], dim = -1)\n",
    "        mlp_vector = self.projection(cat)\n",
    "\n",
    "        # Concatenate and get single score\n",
    "        cat = torch.cat([ mlp_vector, mf_vector ], dim = -1)\n",
    "        rating = self.final(cat)[:, 0].view(user_id.shape) # [bsz]\n",
    "\n",
    "        return user_bias + item_bias + self.global_bias + rating"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graph Sampler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ForestFire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "\n",
    "class ForestFireSampler:\n",
    "    \"\"\"An implementation of forest fire sampling. The procedure is a stochastic\n",
    "    snowball sampling method where the expansion is proportional to the burning probability. \n",
    "    `\"For details about the algorithm see this paper.\" <https://cs.stanford.edu/people/jure/pubs/sampling-kdd06.pdf>`_\n",
    "    Inspiration credit: \n",
    "        littleballoffur\n",
    "        https://github.com/benedekrozemberczki/littleballoffur\n",
    "    Args:\n",
    "        number_of_nodes (int): Number of sampled nodes. Default is 100.\n",
    "        p (float): Burning probability. Default is 0.4.\n",
    "        seed (int): Random seed. Default is 42.\n",
    "    \"\"\"\n",
    "    def __init__(self, number_of_nodes: int=100, p: float=0.4, seed: int=42, max_visited_nodes_backlog: int=100,\n",
    "                 restart_hop_size: int = 10):\n",
    "        self.number_of_nodes = number_of_nodes\n",
    "        self.p = p\n",
    "        self.seed = seed\n",
    "        self._set_seed() \n",
    "        self.restart_hop_size = restart_hop_size\n",
    "        self.max_visited_nodes_backlog = max_visited_nodes_backlog\n",
    "\n",
    "    def _set_seed(self):\n",
    "        random.seed(self.seed)\n",
    "        np.random.seed(self.seed)\n",
    "\n",
    "    def _create_node_sets(self, graph):\n",
    "        \"\"\"\n",
    "        Create a starting set of nodes.\n",
    "        \"\"\"\n",
    "        self._sampled_nodes = set()\n",
    "        self._set_of_nodes = set(range(graph.number_of_nodes()))\n",
    "        self._visited_nodes = deque(maxlen=self.max_visited_nodes_backlog)\n",
    "\n",
    "    def get_neighbors(self, graph, node):\n",
    "        return list(graph.neighbors(node))\n",
    "\n",
    "    def _start_a_fire(self, graph):\n",
    "        \"\"\"\n",
    "        Starting a forest fire from a single node.\n",
    "        \"\"\"\n",
    "        remaining_nodes = list(self._set_of_nodes.difference(self._sampled_nodes))\n",
    "        seed_node = random.choice(remaining_nodes)\n",
    "        self._sampled_nodes.add(seed_node)\n",
    "        node_queue = deque([seed_node])\n",
    "        while len(self._sampled_nodes) < self.number_of_nodes:\n",
    "            if len(node_queue) == 0:\n",
    "                node_queue = deque([self._visited_nodes.popleft()\n",
    "                              for k in range(min(self.restart_hop_size, len(self._visited_nodes)))])\n",
    "                if len(node_queue) == 0:\n",
    "                    # print('Warning: could not collect the required number of nodes. The fire could not find enough nodes to burn.')\n",
    "                    break\n",
    "            top_node = node_queue.popleft()\n",
    "            self._sampled_nodes.add(top_node)\n",
    "            neighbors = set(self.get_neighbors(graph, top_node))\n",
    "            unvisited_neighbors = neighbors.difference(self._sampled_nodes)\n",
    "            score = np.random.geometric(self.p)\n",
    "            count = min(len(unvisited_neighbors), score)\n",
    "            burned_neighbors = random.sample(unvisited_neighbors, count)\n",
    "            self._visited_nodes.extendleft(unvisited_neighbors.difference(set(burned_neighbors)))\n",
    "            for neighbor in burned_neighbors:\n",
    "                if len(self._sampled_nodes) >= self.number_of_nodes:\n",
    "                    break\n",
    "                node_queue.extend([neighbor])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RandomWalk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "class RandomWalkWithRestartSampler:\n",
    "    \"\"\"An implementation of node sampling by random walks with restart. The \n",
    "    process is a discrete random walker on nodes which teleports back to the\n",
    "    staring node with a fixed probability. This results in a connected subsample\n",
    "    from the original input graph. `\"For details about the algorithm see this \n",
    "    paper.\" <https://cs.stanford.edu/people/jure/pubs/sampling-kdd06.pdf>`_\n",
    "    Inspiration credit: \n",
    "        littleballoffur\n",
    "        https://github.com/benedekrozemberczki/littleballoffur\n",
    "    Args:\n",
    "        number_of_nodes (int): Number of nodes. Default is 100.\n",
    "        seed (int): Random seed. Default is 42.\n",
    "        p (float): Restart probability. Default is 0.1.\n",
    "    \"\"\"\n",
    "    def __init__(self, number_of_nodes: int=100, seed: int=42, p: float=0.1):\n",
    "        self.number_of_nodes = number_of_nodes\n",
    "        self.seed = seed\n",
    "        self.p = p\n",
    "        self._set_seed()\n",
    "\n",
    "    def _set_seed(self):\n",
    "        random.seed(self.seed)\n",
    "\n",
    "    def get_neighbors(self, graph, node):\n",
    "        return list(graph.neighbors(node))\n",
    "\n",
    "    def get_random_neighbor(self, graph, node):\n",
    "        return random.choice(self.get_neighbors(graph, node))\n",
    "\n",
    "    def get_nodes(self, graph):\n",
    "        return list(graph.nodes)\n",
    "\n",
    "    def get_number_of_nodes(self, graph):\n",
    "        return graph.number_of_nodes()\n",
    "\n",
    "    def _create_initial_node_set(self, graph, start_node):\n",
    "        \"\"\"\n",
    "        Choosing an initial node.\n",
    "        \"\"\"\n",
    "        self._set_of_nodes = set(self.get_nodes(graph))\n",
    "\n",
    "        if start_node is not None:\n",
    "            if start_node >= 0 and start_node < self.get_number_of_nodes(graph):\n",
    "                self._current_node = start_node\n",
    "                self._sampled_nodes = set([self._current_node])\n",
    "            else:\n",
    "                raise ValueError(\"Starting node index is out of range.\")\n",
    "        else:\n",
    "            self._current_node = random.choice(range(self.get_number_of_nodes(graph)))\n",
    "            self._sampled_nodes = set([self._current_node])\n",
    "        self._initial_node = self._current_node\n",
    "\n",
    "    def _do_a_step(self, graph):\n",
    "        \"\"\"\n",
    "        Doing a single random walk step.\n",
    "        \"\"\"\n",
    "        score = random.uniform(0, 1)\n",
    "        if score < self.p:\n",
    "            self._current_node = self._initial_node\n",
    "        else:\n",
    "            new_node = self.get_random_neighbor(graph, self._current_node)\n",
    "            self._sampled_nodes.add(new_node)\n",
    "            self._current_node = new_node"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MF Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from torch.multiprocessing import Process, Queue, Event\n",
    "\n",
    "class CombinedBase:\n",
    "    def __init__(self): pass\n",
    "\n",
    "    def __len__(self): return (self.num_interactions // self.batch_size) + 1\n",
    "\n",
    "    def __del__(self):\n",
    "        try:\n",
    "            self.p.terminate() ; self.p.join()\n",
    "        except: pass\n",
    "\n",
    "    def make_user_history(self, data):\n",
    "        user_history = [ [] for _ in range(self.num_users) ]\n",
    "        for u, i, r in data: user_history[u].append(i)\n",
    "        return user_history\n",
    "\n",
    "    def pad(self, arr, max_len = None, pad_with = -1, side = 'right'):\n",
    "        seq_len = max_len if max_len is not None else max(map(len, arr))\n",
    "        seq_len = min(seq_len, 200) # You don't need more than this\n",
    "\n",
    "        for i in range(len(arr)):\n",
    "            while len(arr[i]) < seq_len: \n",
    "                pad_elem = arr[i][-1] if len(arr[i]) > 0 else 0\n",
    "                pad_elem = pad_elem if pad_with == -1 else pad_with\n",
    "                if side == 'right': arr[i].append(pad_elem)\n",
    "                else: arr[i] = [ pad_elem ] + arr[i]\n",
    "            arr[i] = arr[i][-seq_len:] # Keep last `seq_len` items\n",
    "        return arr\n",
    "\n",
    "    def sequential_pad(self, arr, hyper_params):\n",
    "        # Padding left side so that we can simply take out [:, -1, :] in the output\n",
    "        return self.pad(\n",
    "            arr, max_len = hyper_params['max_seq_len'], \n",
    "            pad_with = hyper_params['total_items'], side = 'left'\n",
    "        )\n",
    "\n",
    "    def scatter(self, batch, tensor_kind, last_dimension):\n",
    "        ret = tensor_kind(len(batch), last_dimension).zero_()\n",
    "\n",
    "        if not torch.is_tensor(batch):\n",
    "            if ret.is_cuda: batch = torch.cuda.LongTensor(batch)\n",
    "            else: batch = torch.LongTensor(batch)\n",
    "\n",
    "        return ret.scatter_(1, batch, 1)\n",
    "\n",
    "    # NOTE: is_negative(user, item) is a function which tells \n",
    "    # if the item is a negative item for the user\n",
    "    def sample_negatives(self, num_points, num_negs, is_negative):\n",
    "        # Sample all the random numbers you need at once as this is much faster than \n",
    "        # calling random.randint() once everytime\n",
    "        random_numbers = np.random.randint(\n",
    "            self.num_items, \n",
    "            size = int(num_points * num_negs * 1.5)\n",
    "        )\n",
    "\n",
    "        negatives, at = [], 0\n",
    "        for u in range(num_points):\n",
    "            temp_negatives = []\n",
    "            while len(temp_negatives) < num_negs:\n",
    "                ## Negatives not possible\n",
    "                if at >= len(random_numbers):\n",
    "                    temp_negatives.append(0)\n",
    "                    continue\n",
    "\n",
    "                random_item = random_numbers[at] ; at += 1\n",
    "                if is_negative(u, random_item):\n",
    "                    # allowing duplicates, rare possibility\n",
    "                    temp_negatives.append(random_item)\n",
    "            negatives.append(temp_negatives)\n",
    "\n",
    "        return negatives\n",
    "\n",
    "    # So that training, GPU copying etc. \n",
    "    # doesn't have to wait for negative sampling\n",
    "    def init_background_sampler(self, function):\n",
    "        self.event = Event()\n",
    "        self.result_queue = Queue(maxsize=4)\n",
    "        \n",
    "        def sample(result_queue):\n",
    "            try:\n",
    "                while True:\n",
    "                    result_queue.put(function())\n",
    "                    self.event.wait()\n",
    "            except KeyboardInterrupt: pass\n",
    "        self.p = Process(target = sample, args=(self.result_queue, ))\n",
    "        self.p.daemon = True ; self.p.start()\n",
    "\n",
    "class BaseTrainDataset(CombinedBase):\n",
    "    def __init__(self, data, hyper_params):\n",
    "        self.hyper_params = hyper_params\n",
    "        self.batch_size = hyper_params['batch_size']\n",
    "        self.implicit_task = hyper_params['task'] in [ 'implicit', 'sequential' ]\n",
    "        self.data = data\n",
    "        self.num_users, self.num_items = hyper_params['total_users'], hyper_params['total_items']\n",
    "        \n",
    "        ## Making user histories because sequential models require this\n",
    "        self.user_history = self.make_user_history(data)\n",
    "        \n",
    "        ## Making sets of history for easier finding\n",
    "        self.user_history_set = list(map(set, self.user_history))\n",
    "\n",
    "        ## For computing PSP-metrics\n",
    "        self.item_propensity = self.get_item_propensity()\n",
    "\n",
    "    def get_item_count_map(self):\n",
    "        item_count = defaultdict(int)\n",
    "        for u, i, r in self.data: item_count[i] += 1\n",
    "        return item_count\n",
    "\n",
    "    def get_item_propensity(self, A = 0.55, B = 1.5):\n",
    "        item_freq_map = self.get_item_count_map()\n",
    "        item_freq = [ item_freq_map[i] for i in range(self.num_items) ]\n",
    "        num_instances = len(self.data)\n",
    "\n",
    "        C = (np.log(num_instances)-1)*np.power(B+1, A)\n",
    "        wts = 1.0 + C*np.power(np.array(item_freq)+B, -A)\n",
    "        return np.ravel(wts)\n",
    "\n",
    "class BaseTestDataset(CombinedBase):\n",
    "    def __init__(self, data, train_data, hyper_params, val_data):\n",
    "        self.hyper_params = hyper_params\n",
    "        self.batch_size = hyper_params['batch_size']\n",
    "        self.implicit_task = hyper_params['task'] in [ 'implicit', 'sequential' ]\n",
    "        self.data, self.train_data = data, train_data\n",
    "        self.num_users, self.num_items = hyper_params['total_users'], hyper_params['total_items']\n",
    "        \n",
    "        ## Making user histories because sequential models require this\n",
    "        self.train_user_history = self.make_user_history(train_data)\n",
    "        if val_data is not None: \n",
    "            self.val_user_history = self.make_user_history(val_data)\n",
    "            for u in range(self.num_users): self.train_user_history[u] += self.val_user_history[u]\n",
    "        self.test_user_history = self.make_user_history(data)\n",
    "\n",
    "        ## Making sets of history for easier finding\n",
    "        self.train_user_history_set = list(map(set, self.train_user_history))\n",
    "        self.test_user_history_set = list(map(set, self.test_user_history))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# from data_loaders.base import BaseTrainDataset, BaseTestDataset\n",
    "# from torch_utils import LongTensor, FloatTensor, is_cuda_available\n",
    "\n",
    "class TrainDataset(BaseTrainDataset):\n",
    "    def __init__(self, data, hyper_params, track_events):\n",
    "        super(TrainDataset, self).__init__(data, hyper_params)\n",
    "        self.shuffle_allowed = not track_events\n",
    "\n",
    "        # Copying ENTIRE dataset to GPU\n",
    "        self.users_cpu = list(map(lambda x: x[0], data))\n",
    "        self.users = LongTensor(self.users_cpu)\n",
    "        self.items = LongTensor(list(map(lambda x: x[1], data)))\n",
    "        self.ratings = FloatTensor(list(map(lambda x: x[2], data)))\n",
    "\n",
    "        self.num_interactions = len(data)\n",
    "\n",
    "        self.init_background_sampler(\n",
    "            lambda : torch.LongTensor(self.sample_negatives(\n",
    "                len(self.data), self.hyper_params['num_train_negs'],\n",
    "                lambda point, random_neg: random_neg not in self.user_history_set[self.users_cpu[point]]\n",
    "            ))\n",
    "        )\n",
    "\n",
    "    def __iter__(self):\n",
    "        # Important for optimal and stable performance\n",
    "        indices = np.arange(self.num_interactions)\n",
    "        if self.shuffle_allowed: np.random.shuffle(indices)\n",
    "        temp_users = self.users[indices] ; temp_items = self.items[indices] ; temp_ratings = self.ratings[indices]\n",
    "\n",
    "        if self.implicit_task: \n",
    "            negatives = self.result_queue.get()[indices]\n",
    "            if is_cuda_available: negatives = negatives.cuda()\n",
    "            self.event.set()\n",
    "\n",
    "        for i in range(0, self.num_interactions, self.batch_size):\n",
    "            yield [ \n",
    "                temp_users[i:i+self.batch_size], \n",
    "                temp_items[i:i+self.batch_size].unsqueeze(-1), \n",
    "                negatives[i:i+self.batch_size] if self.implicit_task else None, \n",
    "            ], temp_ratings[i:i+self.batch_size]\n",
    "\n",
    "class TestDataset(BaseTestDataset):\n",
    "    def __init__(self, data, train_data, hyper_params, val_data = None, test_set = False):\n",
    "        super(TestDataset, self).__init__(data, train_data, hyper_params, val_data)\n",
    "        self.test_set = test_set\n",
    "\n",
    "        if self.implicit_task:\n",
    "            # Padding for easier scattering\n",
    "            self.test_user_history = LongTensor(self.pad(self.test_user_history))\n",
    "            self.train_user_history = list(map(lambda x: LongTensor(x), self.train_user_history))\n",
    "\n",
    "            # Copying all user-IDs to GPU\n",
    "            self.all_users = LongTensor(list(range(self.num_users)))\n",
    "\n",
    "            self.partial_eval = (not test_set) and hyper_params['partial_eval']\n",
    "\n",
    "            def one_sample():\n",
    "                negatives = self.sample_negatives(\n",
    "                    self.num_users, self.hyper_params['num_test_negs'],\n",
    "                    lambda point, random_neg: random_neg not in self.train_user_history_set[point] and \\\n",
    "                                              random_neg not in self.test_user_history_set[point]\n",
    "                )\n",
    "                if self.partial_eval: negatives = torch.LongTensor(negatives) # Sampled ranking\n",
    "                else: negatives = np.array(negatives) # Sampled AUC\n",
    "                return negatives\n",
    "\n",
    "            self.init_background_sampler(one_sample)\n",
    "\n",
    "        else:\n",
    "            self.users = LongTensor(list(map(lambda x: x[0], data)))\n",
    "            self.items = LongTensor(list(map(lambda x: x[1], data)))\n",
    "            self.ratings = FloatTensor(list(map(lambda x: x[2], data)))\n",
    "\n",
    "        self.num_interactions = self.num_users if self.implicit_task else len(data)\n",
    "\n",
    "    def __iter__(self):\n",
    "        if self.implicit_task:\n",
    "            negatives = self.result_queue.get() ; self.event.set()\n",
    "            if self.partial_eval and is_cuda_available: negatives = negatives.cuda()\n",
    "\n",
    "        for u in range(0, self.num_interactions, self.batch_size):\n",
    "            if self.implicit_task:\n",
    "                batch             = self.all_users[u:u+self.batch_size]\n",
    "                train_positive    = self.train_user_history[u:u+self.batch_size]\n",
    "                test_positive     = self.test_user_history[u:u+self.batch_size]\n",
    "                test_positive_set = self.test_user_history_set[u:u+self.batch_size]\n",
    "                test_negative     = negatives[u:u+self.batch_size]\n",
    "\n",
    "                yield [ batch, test_positive if self.partial_eval else None, test_negative ], [ \n",
    "                    train_positive,\n",
    "                    test_positive_set,\n",
    "                ]\n",
    "            else:\n",
    "                yield [ \n",
    "                    self.users[u:u+self.batch_size], \n",
    "                    self.items[u:u+self.batch_size].unsqueeze(-1), \n",
    "                    None, \n",
    "                ], self.ratings[u:u+self.batch_size]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "\n",
    "# from utils import get_data_loader_class\n",
    "# from data_path_constants import get_data_path, get_index_path\n",
    "\n",
    "def load_data(hyper_params, track_events = False):\n",
    "    rating_data_path = get_data_path(hyper_params)\n",
    "    index_path = get_index_path(hyper_params)\n",
    "\n",
    "    data_holder = DataHolder(rating_data_path, index_path)\n",
    "    print(\"# of users: {}\\n# of items: {}\".format(data_holder.num_users, data_holder.num_items))\n",
    "\n",
    "    hyper_params['total_users']  = data_holder.num_users\n",
    "    hyper_params['total_items']  = data_holder.num_items\n",
    "    # Do a partial item-space evaluation (only on the validation set)\n",
    "    # if the dataset has too many items\n",
    "    hyper_params['partial_eval'] = hyper_params['total_items'] > 1_000\n",
    "\n",
    "    train_loader_class, test_loader_class = get_data_loader_class(hyper_params)\n",
    "    \n",
    "    send_val = hyper_params['model_type'] in [ 'SASRec', 'SVAE', 'MVAE' ]\n",
    "\n",
    "    return train_loader_class(data_holder.train, hyper_params, track_events), test_loader_class(\n",
    "        data_holder.test, data_holder.train, hyper_params, test_set = True,\n",
    "        val_data = data_holder.val if send_val else None\n",
    "    ), test_loader_class(data_holder.val, data_holder.train, hyper_params), hyper_params\n",
    "\n",
    "class DataHolder:\n",
    "    def __init__(self, rating_data_path, index_path):\n",
    "        with h5py.File(rating_data_path + \"total_data.hdf5\", 'r') as f:\n",
    "            self.data = list(zip(f['user'][:], f['item'][:], f['rating'][:]))\n",
    "\n",
    "        self.index = np.load(index_path + \"index.npz\")['data']\n",
    "        self.remap()\n",
    "\n",
    "    def remap(self):\n",
    "        ## Counting number of unique users/items before\n",
    "        valid_users, valid_items = set(), set()\n",
    "        for at, (u, i, r) in enumerate(self.data):\n",
    "            if self.index[at] != -1:\n",
    "                valid_users.add(u)\n",
    "                valid_items.add(i)\n",
    "\n",
    "        ## Map creation done!\n",
    "        user_map = dict(zip(list(valid_users), list(range(len(valid_users)))))\n",
    "        item_map = dict(zip(list(valid_items), list(range(len(valid_items)))))\n",
    "\n",
    "        new_data, new_index = [], []\n",
    "        for at, (u, i, r) in enumerate(self.data):\n",
    "            if self.index[at] == -1: continue\n",
    "            new_data.append([ user_map[u], item_map[i], r ])\n",
    "            new_index.append(self.index[at])\n",
    "\n",
    "        self.data = new_data\n",
    "        self.index = new_index\n",
    "        self.num_users = len(valid_users)\n",
    "        self.num_items = len(valid_items)\n",
    "\n",
    "    def select(self, index_val):\n",
    "        ret = []\n",
    "        for at, tup in enumerate(self.data):\n",
    "            if self.index[at] == index_val: ret.append(tup)\n",
    "        return ret\n",
    "\n",
    "    @property\n",
    "    def train(self): return self.select(0)\n",
    "\n",
    "    @property\n",
    "    def val(self): return self.select(1)\n",
    "\n",
    "    @property\n",
    "    def test(self): return self.select(2)\n",
    "\n",
    "    @property\n",
    "    def num_train_interactions(self): return int(sum(map(lambda x: x == 0, self.index)))\n",
    "\n",
    "    @property\n",
    "    def num_val_interactions(self): return int(sum(map(lambda x: x == 1, self.index)))\n",
    "\n",
    "    @property\n",
    "    def num_test_interactions(self): return int(sum(map(lambda x: x == 2, self.index)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from numba import jit, float32, float64, int64\n",
    "\n",
    "# from utils import INF\n",
    "\n",
    "def evaluate(model, criterion, reader, hyper_params, item_propensity, topk = [ 10, 100 ], test = False):\n",
    "    metrics = {}\n",
    "\n",
    "    # Do a negative sampled item-space evaluation (only on the validation set)\n",
    "    # if the dataset is too big \n",
    "    partial_eval = (not test) and hyper_params['partial_eval'] \n",
    "    partial_eval = partial_eval and (hyper_params['model_type'] not in [ 'MVAE', 'SVAE', 'pop_rec' ])\n",
    "    if partial_eval: metrics['eval'] = 'partial'\n",
    "\n",
    "    if hyper_params['task'] == 'explicit': metrics['MSE'] = 0.0\n",
    "    else:\n",
    "        preds, y_binary = [], []\n",
    "        for kind in [ 'HR', 'NDCG', 'PSP' ]:\n",
    "            for k in topk: \n",
    "                metrics['{}@{}'.format(kind, k)] = 0.0\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for data, y in reader:\n",
    "            output = model(data, eval = True)\n",
    "            if hyper_params['model_type'] in [ 'MVAE', 'SVAE' ]: output, _, _ = output\n",
    "            if hyper_params['model_type'] == 'SVAE': output = output[:, -1, :]\n",
    "\n",
    "            if hyper_params['task'] == 'explicit': \n",
    "                metrics['MSE'] += torch.sum(criterion(output, y, return_mean = False).data)\n",
    "            else:\n",
    "                function = evaluate_batch_partial if partial_eval else evaluate_batch\n",
    "\n",
    "                metrics, temp_preds, temp_y = function(data, output, y, item_propensity, topk, metrics)\n",
    "                preds += temp_preds\n",
    "                y_binary += temp_y\n",
    "\n",
    "    if hyper_params['task'] == 'explicit':\n",
    "        metrics['MSE'] = round(float(metrics['MSE']) / reader.num_interactions, 4)\n",
    "    else:\n",
    "        # NOTE: sklearn's `roc_auc_score` is suuuuper slow\n",
    "        metrics['AUC'] = round(fast_auc(np.array(y_binary), np.array(preds)), 4)\n",
    "        \n",
    "        for kind in [ 'HR', 'NDCG', 'PSP' ]:\n",
    "            for k in topk: \n",
    "                metrics['{}@{}'.format(kind, k)] = round(\n",
    "                    float(100.0 * metrics['{}@{}'.format(kind, k)]) / reader.num_interactions, 4\n",
    "                )\n",
    "\n",
    "    return metrics\n",
    "\n",
    "def evaluate_batch(data, output_batch, y, item_propensity, topk, metrics):\n",
    "    # Y\n",
    "    train_positive, test_positive_set = y\n",
    "\n",
    "    # Data\n",
    "    _, _, auc_negatives = data\n",
    "\n",
    "    # AUC Stuff\n",
    "    temp_preds, temp_y = [], []\n",
    "    logits_cpu = output_batch.cpu().numpy()\n",
    "    for b in range(len(output_batch)):\n",
    "        # Validation set could have 0 positive interactions\n",
    "        if len(test_positive_set[b]) == 0: continue\n",
    "\n",
    "        temp_preds += np.take(logits_cpu[b], np.array(list(test_positive_set[b]))).tolist()\n",
    "        temp_y += [ 1.0 for _ in range(len(test_positive_set[b])) ]\n",
    "\n",
    "        temp_preds += np.take(logits_cpu[b], auc_negatives[b]).tolist()\n",
    "        temp_y += [ 0.0 for _ in range(len(auc_negatives[b])) ]\n",
    "\n",
    "    # Marking train-set consumed items as negative INF\n",
    "    for b in range(len(output_batch)): output_batch[b][ train_positive[b] ] = -INF\n",
    "\n",
    "    _, indices = torch.topk(output_batch, min(item_propensity.shape[0], max(topk)), sorted = True)\n",
    "    indices = indices.cpu().numpy().tolist()\n",
    "\n",
    "    for k in topk: \n",
    "        for b in range(len(output_batch)):\n",
    "            num_pos = float(len(test_positive_set[b]))\n",
    "            # Validation set could have 0 positive interactions after sampling\n",
    "            if num_pos == 0: continue\n",
    "\n",
    "            metrics['HR@{}'.format(k)] += float(len(set(indices[b][:k]) & test_positive_set[b])) / float(min(num_pos, k))\n",
    "\n",
    "            test_positive_sorted_psp = sorted([ item_propensity[x] for x in test_positive_set[b] ])[::-1]\n",
    "\n",
    "            dcg, idcg, psp, max_psp = 0.0, 0.0, 0.0, 0.0\n",
    "            for at, pred in enumerate(indices[b][:k]):\n",
    "                if pred in test_positive_set[b]: \n",
    "                    dcg += 1.0 / np.log2(at + 2)\n",
    "                    psp += float(item_propensity[pred]) / float(min(num_pos, k))\n",
    "                if at < num_pos: \n",
    "                    idcg += 1.0 / np.log2(at + 2)\n",
    "                    max_psp += test_positive_sorted_psp[at]\n",
    "\n",
    "            metrics['NDCG@{}'.format(k)] += dcg / idcg\n",
    "            metrics['PSP@{}'.format(k)] += psp / max_psp\n",
    "\n",
    "    return metrics, temp_preds, temp_y\n",
    "\n",
    "def evaluate_batch_partial(data, output, y, item_propensity, topk, metrics):\n",
    "    _, test_pos_items, _ = data\n",
    "    test_pos_items = test_pos_items.cpu().numpy()\n",
    "\n",
    "    pos_score, neg_score = output\n",
    "    pos_score, neg_score = pos_score.cpu().numpy(), neg_score.cpu().numpy()\n",
    "\n",
    "    temp_preds, temp_y, hr, ndcg, psp = evaluate_batch_partial_jit(\n",
    "        pos_score, neg_score, test_pos_items, np.array(item_propensity), np.array(topk)\n",
    "    )\n",
    "\n",
    "    for at_k, k in enumerate(topk): \n",
    "        metrics['HR@{}'.format(k)] += hr[at_k]\n",
    "        metrics['NDCG@{}'.format(k)] += ndcg[at_k]\n",
    "        metrics['PSP@{}'.format(k)] += psp[at_k]\n",
    "\n",
    "    return metrics, temp_preds.tolist(), temp_y.tolist()\n",
    "\n",
    "@jit('Tuple((float32[:], float32[:], float32[:], float32[:], float32[:]))(float32[:,:], float32[:,:], int64[:,:], float64[:], int64[:])')\n",
    "def evaluate_batch_partial_jit(pos_score, neg_score, test_pos_items, item_propensity, topk):\n",
    "    temp_preds = np.zeros(\n",
    "        ((pos_score.shape[0] * pos_score.shape[1]) + (neg_score.shape[0] * neg_score.shape[1])), \n",
    "        dtype = np.float32\n",
    "    )\n",
    "    temp_y = np.zeros(temp_preds.shape, dtype = np.float32)\n",
    "    at_preds = 0\n",
    "\n",
    "    hr_arr = np.zeros((len(topk)), dtype = np.float32)\n",
    "    ndcg_arr = np.zeros((len(topk)), dtype = np.float32)\n",
    "    psp_arr = np.zeros((len(topk)), dtype = np.float32)\n",
    "\n",
    "    for b in range(len(pos_score)):\n",
    "        pos, neg = pos_score[b, :], neg_score[b, :]\n",
    "\n",
    "        # pos will be padded, un-pad it\n",
    "        last_index = len(pos) - 1\n",
    "        while last_index > 0 and pos[last_index] == pos[last_index - 1]: last_index -= 1\n",
    "        pos = pos[:last_index + 1]\n",
    "\n",
    "        # Add to AUC\n",
    "        temp_preds[at_preds:at_preds+len(pos)] = pos\n",
    "        temp_y[at_preds:at_preds+len(pos)] = 1\n",
    "        at_preds += len(pos)\n",
    "\n",
    "        temp_preds[at_preds:at_preds+len(neg)] = neg\n",
    "        temp_y[at_preds:at_preds+len(neg)] = 0\n",
    "        at_preds += len(neg)\n",
    "\n",
    "        # get rank of all elements in pos\n",
    "        temp_ranks = np.argsort(- np.concatenate((pos, neg)))\n",
    "\n",
    "        # To maintain order\n",
    "        pos_ranks = np.zeros(len(pos))\n",
    "        for at, r in enumerate(temp_ranks):\n",
    "            if r < len(pos): pos_ranks[r] = at + 1\n",
    "\n",
    "        test_positive_sorted_psp = sorted([ item_propensity[x] for x in test_pos_items[b] ])[::-1]\n",
    "\n",
    "        for at_k, k in enumerate(topk): \n",
    "            num_pos = float(len(pos))\n",
    "            \n",
    "            hr_arr[at_k] += np.sum(pos_ranks <= k) / float(min(num_pos, k))\n",
    "\n",
    "            dcg, idcg, psp, max_psp = 0.0, 0.0, 0.0, 0.0\n",
    "            for at, rank in enumerate(pos_ranks):\n",
    "                if rank <= k:\n",
    "                    dcg += 1.0 / np.log2(rank + 1) # 1-based indexing\n",
    "                    psp += item_propensity[test_pos_items[b][at]] / float(min(num_pos, k))\n",
    "                idcg += 1.0 / np.log2(at + 2)\n",
    "                max_psp += test_positive_sorted_psp[at]\n",
    "            \n",
    "            ndcg_arr[at_k] += dcg / idcg\n",
    "            psp_arr[at_k] += psp / max_psp\n",
    "\n",
    "    return temp_preds[:at_preds], temp_y[:at_preds], hr_arr, ndcg_arr, psp_arr\n",
    "\n",
    "@jit(float64(float64[:], float64[:]))\n",
    "def fast_auc(y_true, y_prob):\n",
    "    y_true = y_true[np.argsort(y_prob)]\n",
    "    nfalse, auc = 0, 0\n",
    "    for i in range(len(y_true)):\n",
    "        nfalse += (1 - y_true[i])\n",
    "        auc += y_true[i] * nfalse\n",
    "    return auc / (nfalse * (len(y_true) - nfalse))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# from torch_utils import is_cuda_available\n",
    "\n",
    "class CustomLoss(torch.nn.Module):\n",
    "    def __init__(self, hyper_params):\n",
    "        super(CustomLoss, self).__init__()\n",
    "        self.forward = {\n",
    "            'explicit': self.mse,\n",
    "            'implicit': self.bpr,\n",
    "            'sequential': self.bpr,\n",
    "        }[hyper_params['task']]\n",
    "\n",
    "        if hyper_params['model_type'] == \"MVAE\": self.forward = self.vae_loss\n",
    "        if hyper_params['model_type'] == \"SVAE\": self.forward = self.svae_loss\n",
    "        if hyper_params['model_type'] == \"SASRec\": self.forward = self.bce_sasrec\n",
    "\n",
    "        self.torch_bce = torch.nn.BCEWithLogitsLoss()\n",
    "        self.anneal_val = 0.0\n",
    "        self.hyper_params = hyper_params\n",
    "\n",
    "    def mse(self, output, y, return_mean = True):\n",
    "        mse = torch.pow(output - y, 2)\n",
    "                \n",
    "        if return_mean: return torch.mean(mse)\n",
    "        return mse\n",
    "\n",
    "    def bce_sasrec(self, output, pos, return_mean = True):\n",
    "        pos_logits, neg_logits = output\n",
    "        pos_labels, neg_labels = torch.ones(pos_logits.shape), torch.zeros(neg_logits.shape)\n",
    "        if is_cuda_available: pos_labels, neg_labels = pos_labels.cuda(), neg_labels.cuda()\n",
    "\n",
    "        indices = pos != self.hyper_params['total_items']\n",
    "\n",
    "        loss = self.torch_bce(pos_logits[indices], pos_labels[indices])\n",
    "        loss += self.torch_bce(neg_logits[indices], neg_labels[indices])\n",
    "        return loss\n",
    "\n",
    "    def bpr(self, output, y, return_mean = True):\n",
    "        pos_output, neg_output = output\n",
    "        pos_output = pos_output.repeat(1, neg_output.shape[1]).view(-1)\n",
    "        neg_output = neg_output.view(-1)\n",
    "        \n",
    "        loss = -F.logsigmoid(pos_output - neg_output)\n",
    "                \n",
    "        if return_mean: return torch.mean(loss)\n",
    "        return loss\n",
    "\n",
    "    def anneal(self, step_size):\n",
    "        self.anneal_val += step_size\n",
    "        self.anneal_val = max(self.anneal_val, 0.2)\n",
    "\n",
    "    def vae_loss(self, output, y_true_s, return_mean = True):\n",
    "        decoder_output, mu_q, logvar_q = output\n",
    "\n",
    "        # Calculate KL Divergence loss\n",
    "        kld = torch.mean(torch.sum(0.5 * (-logvar_q + torch.exp(logvar_q) + mu_q**2 - 1), -1))\n",
    "    \n",
    "        # Calculate Likelihood\n",
    "        decoder_output = F.log_softmax(decoder_output, -1)\n",
    "        likelihood = torch.sum(-1.0 * y_true_s * decoder_output, -1)\n",
    "        \n",
    "        final = (self.anneal_val * kld) + (likelihood)\n",
    "        \n",
    "        if return_mean: return torch.mean(final)\n",
    "        return final\n",
    "\n",
    "    def svae_loss(self, output, y, return_mean = True):\n",
    "        decoder_output, mu_q, logvar_q = output\n",
    "        dec_shape = decoder_output.shape # [batch_size x seq_len x total_items]\n",
    "\n",
    "        # Calculate KL Divergence loss\n",
    "        kld = torch.mean(torch.sum(0.5 * (-logvar_q + torch.exp(logvar_q) + mu_q**2 - 1), -1))\n",
    "    \n",
    "        # Don't compute loss on padded items\n",
    "        y_true_s, y_indices = y\n",
    "        keep_indices = y_indices != self.hyper_params['total_items']\n",
    "        y_true_s = y_true_s[keep_indices]\n",
    "        decoder_output = decoder_output[keep_indices]\n",
    "\n",
    "        # Calculate Likelihood\n",
    "        decoder_output = F.log_softmax(decoder_output, -1)\n",
    "        likelihood = torch.sum(-1.0 * y_true_s * decoder_output)\n",
    "        likelihood = likelihood / float(dec_shape[0] * self.hyper_params['num_next'])\n",
    "        \n",
    "        final = (self.anneal_val * kld) + (likelihood)\n",
    "        \n",
    "        if return_mean: return torch.mean(final)\n",
    "        return final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, criterion, optimizer, reader, hyper_params, forgetting_events, track_events):\n",
    "    import torch\n",
    "\n",
    "    model.train()\n",
    "    \n",
    "    # Initializing metrics since we will calculate MSE on the train set on the fly\n",
    "    metrics = {}\n",
    "    \n",
    "    # Initializations\n",
    "    at = 0\n",
    "    \n",
    "    # Train for one epoch, batch-by-batch\n",
    "    loop = tqdm(reader)\n",
    "    for data, y in loop:\n",
    "        # Empty the gradients\n",
    "        model.zero_grad()\n",
    "        optimizer.zero_grad()\n",
    "    \n",
    "        # Forward pass\n",
    "        output = model(data)\n",
    "\n",
    "        # Compute per-interaction loss\n",
    "        loss = criterion(output, y, return_mean = False)\n",
    "        criterion.anneal(1.0 / float(len(reader) * hyper_params['epochs']))\n",
    "\n",
    "        # loop.set_description(\"Loss: {}\".format(round(float(loss), 4)))\n",
    "        \n",
    "        # Track forgetting events\n",
    "        if track_events:\n",
    "            with torch.no_grad():\n",
    "                if hyper_params['task'] == 'explicit': forgetting_events[at : at+data[0].shape[0]] += loss.data\n",
    "                else:\n",
    "                    pos_output, neg_output = output\n",
    "                    pos_output = pos_output.repeat(1, neg_output.shape[1])\n",
    "                    num_incorrect = torch.sum((neg_output > pos_output).float(), -1)\n",
    "                    forgetting_events[at : at+data[0].shape[0]] += num_incorrect.data\n",
    "                    \n",
    "                at += data[0].shape[0]\n",
    "\n",
    "        # Backward pass\n",
    "        loss = torch.mean(loss)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    return metrics, forgetting_events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_complete(hyper_params, train_reader, val_reader, model, model_class, track_events):\n",
    "    import torch\n",
    "\n",
    "    # from loss import CustomLoss\n",
    "    # from eval import evaluate\n",
    "    # from torch_utils import is_cuda_available\n",
    "\n",
    "    criterion = CustomLoss(hyper_params)\n",
    "    optimizer = torch.optim.Adam(\n",
    "        model.parameters(), lr=hyper_params['lr'], betas=(0.9, 0.98),\n",
    "        weight_decay=hyper_params['weight_decay']\n",
    "    )\n",
    "\n",
    "    file_write(hyper_params['log_file'], str(model))\n",
    "    file_write(hyper_params['log_file'], \"\\nModel Built!\\nStarting Training...\\n\")\n",
    "\n",
    "    try:\n",
    "        best_MSE = float(INF)\n",
    "        best_AUC = -float(INF)\n",
    "        best_HR = -float(INF)\n",
    "        decreasing_streak = 0\n",
    "        forgetting_events = None\n",
    "        if track_events: \n",
    "            forgetting_events = torch.zeros(train_reader.num_interactions).float()\n",
    "            if is_cuda_available: forgetting_events = forgetting_events.cuda()\n",
    "\n",
    "        for epoch in range(1, hyper_params['epochs'] + 1):\n",
    "            epoch_start_time = time.time()\n",
    "            \n",
    "            # Training for one epoch\n",
    "            metrics, local_forgetted_count = train(\n",
    "                model, criterion, optimizer, train_reader, hyper_params, \n",
    "                forgetting_events, track_events\n",
    "            )\n",
    "\n",
    "            # Calulating the metrics on the validation set\n",
    "            if (epoch % hyper_params['validate_every'] == 0) or (epoch == 1):\n",
    "                metrics = evaluate(model, criterion, val_reader, hyper_params, train_reader.item_propensity)\n",
    "                metrics['dataset'] = hyper_params['dataset']\n",
    "                decreasing_streak += 1\n",
    "\n",
    "                # Save best model on validation set\n",
    "                if hyper_params['task'] == 'explicit' and metrics['MSE'] < best_MSE:\n",
    "                    print(\"Saving model...\")\n",
    "                    torch.save(model.state_dict(), hyper_params['model_path'])\n",
    "                    decreasing_streak, best_MSE = 0, metrics['MSE']\n",
    "                elif hyper_params['task'] != 'explicit' and metrics['AUC'] > best_AUC:\n",
    "                    print(\"Saving model...\")\n",
    "                    torch.save(model.state_dict(), hyper_params['model_path'])\n",
    "                    decreasing_streak, best_AUC = 0, metrics['AUC']\n",
    "                elif hyper_params['task'] != 'explicit' and metrics['HR@10'] > best_HR:\n",
    "                    print(\"Saving model...\")\n",
    "                    torch.save(model.state_dict(), hyper_params['model_path'])\n",
    "                    decreasing_streak, best_HR = 0, metrics['HR@10']\n",
    "            \n",
    "            log_end_epoch(hyper_params, metrics, epoch, time.time() - epoch_start_time, metrics_on = '(VAL)')\n",
    "\n",
    "            # Check if need to early-stop\n",
    "            if 'early_stop' in hyper_params and decreasing_streak >= hyper_params['early_stop']:\n",
    "                file_write(hyper_params['log_file'], \"Early stopping..\")\n",
    "                break\n",
    "            \n",
    "    except KeyboardInterrupt: print('Exiting from training early')\n",
    "\n",
    "    # Load best model and return it for evaluation on test-set\n",
    "    if os.path.exists(hyper_params['model_path']):\n",
    "        model = model_class(hyper_params)\n",
    "        if is_cuda_available: model = model.cuda()\n",
    "        model.load_state_dict(torch.load(hyper_params['model_path']))\n",
    "    \n",
    "    model.eval()\n",
    "\n",
    "    if track_events: forgetting_events = forgetting_events.cpu().numpy() / float(hyper_params['epochs'])\n",
    "\n",
    "    return model, forgetting_events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import importlib\n",
    "import datetime as dt\n",
    "from tqdm import tqdm\n",
    "\n",
    "# from utils import file_write, log_end_epoch, INF, valid_hyper_params\n",
    "# from data_path_constants import get_log_file_path, get_model_file_path\n",
    "\n",
    "\n",
    "def main_pytorch(hyper_params, track_events = False, eval_full = True):\n",
    "    # from load_data import load_data\n",
    "    # from eval import evaluate\n",
    "    \n",
    "    # from torch_utils import is_cuda_available, xavier_init, get_model_class\n",
    "    # from loss import CustomLoss\n",
    "\n",
    "    if not valid_hyper_params(hyper_params): \n",
    "        print(\"Invalid task combination specified, exiting.\")\n",
    "        return\n",
    "\n",
    "    # Load the data readers\n",
    "    train_reader, test_reader, val_reader, hyper_params = load_data(hyper_params, track_events = track_events)\n",
    "    file_write(hyper_params['log_file'], \"\\n\\nSimulation run on: \" + str(dt.datetime.now()) + \"\\n\\n\")\n",
    "    file_write(hyper_params['log_file'], \"Data reading complete!\")\n",
    "    file_write(hyper_params['log_file'], \"Number of train batches: {:4d}\".format(len(train_reader)))\n",
    "    file_write(hyper_params['log_file'], \"Number of validation batches: {:4d}\".format(len(val_reader)))\n",
    "    file_write(hyper_params['log_file'], \"Number of test batches: {:4d}\".format(len(test_reader)))\n",
    "\n",
    "    # Initialize & train the model\n",
    "    start_time = time.time()\n",
    "\n",
    "    if hyper_params['model_type'] == 'NeuMF': \n",
    "        model, forgetting_events = train_neumf(hyper_params, train_reader, val_reader, track_events)\n",
    "    else:\n",
    "        model = get_model_class(hyper_params)(hyper_params)\n",
    "        if is_cuda_available: model = model.cuda()\n",
    "        xavier_init(model)\n",
    "        model, forgetting_events = train_complete(\n",
    "            hyper_params, train_reader, val_reader, model, get_model_class(hyper_params), track_events\n",
    "        )\n",
    "\n",
    "    metrics = {}\n",
    "    if eval_full:\n",
    "        # Calculating MSE on test-set\n",
    "        criterion = CustomLoss(hyper_params)\n",
    "        metrics = evaluate(model, criterion, test_reader, hyper_params, train_reader.item_propensity, test = True)\n",
    "        log_end_epoch(hyper_params, metrics, 'final', time.time() - start_time, metrics_on = '(TEST)')\n",
    "\n",
    "    # We have no space left for storing the models\n",
    "    os.remove(hyper_params['model_path'])\n",
    "    del model, train_reader, test_reader, val_reader\n",
    "    return metrics, forgetting_events"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVPHandler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "\n",
    "# from main import main_pytorch\n",
    "\n",
    "class SVPHandler:\n",
    "    def __init__(self, model_type, loss_type, hyper_params):\n",
    "        hyper_params['model_type'] = model_type\n",
    "        hyper_params['task'] = loss_type\n",
    "        hyper_params['num_train_negs'] = 1\n",
    "        hyper_params['num_test_negs'] = 100\n",
    "\n",
    "        hyper_params['latent_size'] = 10\n",
    "        hyper_params['dropout'] = 0.3\n",
    "        hyper_params['weight_decay'] = float(1e-6)\n",
    "        hyper_params['lr'] = 0.006\n",
    "        hyper_params['epochs'] = 50\n",
    "        hyper_params['validate_every'] = 5000\n",
    "        hyper_params['batch_size'] = 1024\n",
    "        self.hyper_params = hyper_params\n",
    "        self.hyper_params['log_file'] = self.log_file\n",
    "        self.hyper_params['model_path'] = self.model_file\n",
    "\n",
    "        self.train_model()\n",
    "\n",
    "    def train_model(self): \n",
    "        _, self.forgetted_count = main_pytorch(self.hyper_params, track_events = True, eval_full = False)\n",
    "\n",
    "    def forgetting_events(self, percent, data, index):\n",
    "        # Keep those points which have the maximum forgetted count\n",
    "        # => Remove those points which have the minimum forgetted count\n",
    "        index_map = []\n",
    "        for at, i in enumerate(index):\n",
    "            if i == 0: index_map.append(at)\n",
    "\n",
    "        split_point = int(float(len(self.forgetted_count)) * (float(percent) / 100.0))\n",
    "        order = np.argsort(self.forgetted_count)\n",
    "        order = list(map(lambda x: index_map[x], order))\n",
    "        remove_indices = order[:split_point] # If greedy\n",
    "\n",
    "        for i in remove_indices: index[i] = -1 # Remove\n",
    "        return index\n",
    "\n",
    "    def forgetting_events_user(self, percent, data, index):\n",
    "        # Keep those users which have the maximum forgetted count\n",
    "        # Remove those users which have the minimum forgetted count\n",
    "        index_map, user_map, hist, at, total = [], [], {}, 0, 0\n",
    "        for u in range(len(data)):\n",
    "            for i, r, t in data[u]:\n",
    "                if index[at] == 0:\n",
    "                    index_map.append(at)\n",
    "                    user_map.append(u)\n",
    "                    if u not in hist: hist[u] = 0\n",
    "                    hist[u] += 1\n",
    "                    total += 1\n",
    "                at += 1\n",
    "\n",
    "        user_forgetted_count = defaultdict(list)\n",
    "        for train_at, cnt in enumerate(self.forgetted_count):\n",
    "            user_forgetted_count[user_map[train_at]].append(cnt)\n",
    "        user_forgetted_count = sorted(list(dict(user_forgetted_count).items()), key = lambda x: np.mean(x[1]))\n",
    "\n",
    "        interactions_to_remove, removed, users_to_remove = total * (float(percent) / 100.0), 0, set()\n",
    "        for u, _ in user_forgetted_count:\n",
    "            if removed >= interactions_to_remove: break\n",
    "            users_to_remove.add(u)\n",
    "            removed += hist[u]\n",
    "\n",
    "        for train_at in range(len(user_map)):\n",
    "            if user_map[train_at] in users_to_remove: index[index_map[train_at]] = -1\n",
    "\n",
    "        return index\n",
    "\n",
    "    def compute_freq(self, data, index, freq_type):\n",
    "        freq, at = defaultdict(int), 0\n",
    "        for u in range(len(data)):\n",
    "            for i, r, t in data[u]:\n",
    "                if index[at] == 0:\n",
    "                    to_count = [ u, i ][freq_type]\n",
    "                    freq[to_count] += 1\n",
    "                at += 1\n",
    "\n",
    "        valid_users = list(freq.keys())\n",
    "        return list(map(lambda x: freq[x], valid_users)), dict(zip(valid_users, list(range(len(freq)))))\n",
    "\n",
    "    def compute_prop(self, freq_vector, num_instances, A = 0.55, B = 1.5):\n",
    "        C = (np.log(num_instances)-1)*np.power(B+1, A)\n",
    "        wts = 1.0 + C*np.power(np.array(freq_vector)+B, -A)\n",
    "        return np.ravel(wts)\n",
    "\n",
    "    def forgetting_events_propensity(self, percent, data, index, pooling_method = 'max'):\n",
    "        # Keep those points which have the maximum forgetted count\n",
    "        # Remove those points which have the minimum forgetted count\n",
    "\n",
    "        num_interactions = len(self.forgetted_count)\n",
    "        user_freq, user_map = self.compute_freq(data, index, 0)\n",
    "        user_propensity_vector = self.compute_prop(user_freq, num_interactions)\n",
    "        item_freq, item_map = self.compute_freq(data, index, 1)\n",
    "        item_propensity_vector = self.compute_prop(item_freq, num_interactions)\n",
    "        interaction_propensity, at = [], 0\n",
    "        freq, at = defaultdict(int), 0\n",
    "        \n",
    "        def pool(prop_u, prop_i):\n",
    "            if pooling_method == 'sum': return prop_u + prop_i\n",
    "            elif pooling_method == 'max': return max(prop_u, prop_i)\n",
    "\n",
    "        for u in range(len(data)):\n",
    "            for i, r, t in data[u]:\n",
    "                if index[at] == 0:\n",
    "                    interaction_propensity.append(\n",
    "                        pool(user_propensity_vector[user_map[u]], item_propensity_vector[item_map[i]])\n",
    "                    )\n",
    "                at += 1\n",
    "        assert len(interaction_propensity) == num_interactions\n",
    "\n",
    "        # interaction_propensity actually estimates the `inverse` propensity, hence multiply\n",
    "        updated_count = np.array(self.forgetted_count) * np.array(interaction_propensity)\n",
    "\n",
    "        index_map = []\n",
    "        for at, i in enumerate(index):\n",
    "            if i == 0: index_map.append(at)\n",
    "\n",
    "        split_point = int(float(len(updated_count)) * (float(percent) / 100.0))\n",
    "        order = np.argsort(updated_count)\n",
    "        order = list(map(lambda x: index_map[x], order))\n",
    "        remove_indices = order[:split_point] # If greedy\n",
    "\n",
    "        for i in remove_indices: index[i] = -1 # Remove\n",
    "        return index\n",
    "\n",
    "    def forgetting_events_user_propensity(self, percent, data, index):\n",
    "        # Keep those users which have the maximum forgetted count\n",
    "        # Keep those users which have the maximum propensity --> minimum frequency\n",
    "        # Remove those users which have the minimum forgetted count\n",
    "\n",
    "        num_interactions = len(self.forgetted_count)\n",
    "        user_freq, user_index_map = self.compute_freq(data, index, 0)\n",
    "        user_propensity_vector = self.compute_prop(user_freq, num_interactions)\n",
    "\n",
    "        index_map, user_map, hist, at, total = [], [], {}, 0, 0\n",
    "        for u in range(len(data)):\n",
    "            for i, r, t in data[u]:\n",
    "                if index[at] == 0:\n",
    "                    index_map.append(at)\n",
    "                    user_map.append(u)\n",
    "                    if u not in hist: hist[u] = 0\n",
    "                    hist[u] += 1\n",
    "                    total += 1\n",
    "                at += 1\n",
    "\n",
    "        user_forgetted_count = defaultdict(list)\n",
    "        for train_at, cnt in enumerate(self.forgetted_count):\n",
    "            u = user_map[train_at]\n",
    "            user_forgetted_count[u].append(cnt * user_propensity_vector[user_index_map[u]])\n",
    "        user_forgetted_count = sorted(list(dict(user_forgetted_count).items()), key = lambda x: np.mean(x[1]))\n",
    "\n",
    "        interactions_to_remove, removed, users_to_remove = total * (float(percent) / 100.0), 0, set()\n",
    "        for u, _ in user_forgetted_count:\n",
    "            if removed >= interactions_to_remove: break\n",
    "            users_to_remove.add(u)\n",
    "            removed += hist[u]\n",
    "\n",
    "        for train_at in range(len(user_map)):\n",
    "            if user_map[train_at] in users_to_remove: index[index_map[train_at]] = -1\n",
    "\n",
    "        return index\n",
    "\n",
    "    @property\n",
    "    def model_file(self): \n",
    "        return get_model_file_path(self.hyper_params)\n",
    "\n",
    "    @property\n",
    "    def log_file(self): \n",
    "        return get_log_file_path(self.hyper_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RatingData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "\t\t\t<script type=\"text/javascript\">\n",
       "\t\t\t<!--\n",
       "\t\t\t\t\n",
       "\t\t\t{\n",
       "\t\t\t\tvar element = document.getElementById('NetworKit_script');\n",
       "\t\t\t\tif (element) {\n",
       "\t\t\t\t\telement.parentNode.removeChild(element);\n",
       "\t\t\t\t}\n",
       "\t\t\t\telement = document.createElement('script');\n",
       "\t\t\t\telement.type = 'text/javascript';\n",
       "\t\t\t\telement.innerHTML = 'function NetworKit_pageEmbed(id) { var i, j; var elements; elements = document.getElementById(id).getElementsByClassName(\"Plot\"); for (i=0; i<elements.length; i++) { elements[i].id = id + \"_Plot_\" + i; var data = elements[i].getAttribute(\"data-image\").split(\"|\"); elements[i].removeAttribute(\"data-image\"); var content = \"<div class=\\\\\"Image\\\\\" id=\\\\\"\" + elements[i].id + \"_Image\\\\\" />\"; elements[i].innerHTML = content; elements[i].setAttribute(\"data-image-index\", 0); elements[i].setAttribute(\"data-image-length\", data.length); for (j=0; j<data.length; j++) { elements[i].setAttribute(\"data-image-\" + j, data[j]); } NetworKit_plotUpdate(elements[i]); elements[i].onclick = function (e) { NetworKit_overlayShow((e.target) ? e.target : e.srcElement); } } elements = document.getElementById(id).getElementsByClassName(\"HeatCell\"); for (i=0; i<elements.length; i++) { var data = parseFloat(elements[i].getAttribute(\"data-heat\")); var color = \"#00FF00\"; if (data <= 1 && data > 0) { color = \"hsla(0, 100%, 75%, \" + (data) + \")\"; } else if (data <= 0 && data >= -1) { color = \"hsla(240, 100%, 75%, \" + (-data) + \")\"; } elements[i].style.backgroundColor = color; } elements = document.getElementById(id).getElementsByClassName(\"Details\"); for (i=0; i<elements.length; i++) { elements[i].setAttribute(\"data-title\", \"-\"); NetworKit_toggleDetails(elements[i]); elements[i].onclick = function (e) { NetworKit_toggleDetails((e.target) ? e.target : e.srcElement); } } elements = document.getElementById(id).getElementsByClassName(\"MathValue\"); for (i=elements.length-1; i>=0; i--) { value = elements[i].innerHTML.trim(); if (value === \"nan\") { elements[i].parentNode.innerHTML = \"\" } } elements = document.getElementById(id).getElementsByClassName(\"SubCategory\"); for (i=elements.length-1; i>=0; i--) { value = elements[i].innerHTML.trim(); if (value === \"\") { elements[i].parentNode.removeChild(elements[i]) } } elements = document.getElementById(id).getElementsByClassName(\"Category\"); for (i=elements.length-1; i>=0; i--) { value = elements[i].innerHTML.trim(); if (value === \"\") { elements[i].parentNode.removeChild(elements[i]) } } var isFirefox = false; try { isFirefox = typeof InstallTrigger !== \"undefined\"; } catch (e) {} if (!isFirefox) { alert(\"Currently the function\\'s output is only fully supported by Firefox.\"); } } function NetworKit_plotUpdate(source) { var index = source.getAttribute(\"data-image-index\"); var data = source.getAttribute(\"data-image-\" + index); var image = document.getElementById(source.id + \"_Image\"); image.style.backgroundImage = \"url(\" + data + \")\"; } function NetworKit_showElement(id, show) { var element = document.getElementById(id); element.style.display = (show) ? \"block\" : \"none\"; } function NetworKit_overlayShow(source) { NetworKit_overlayUpdate(source); NetworKit_showElement(\"NetworKit_Overlay\", true); } function NetworKit_overlayUpdate(source) { document.getElementById(\"NetworKit_Overlay_Title\").innerHTML = source.title; var index = source.getAttribute(\"data-image-index\"); var data = source.getAttribute(\"data-image-\" + index); var image = document.getElementById(\"NetworKit_Overlay_Image\"); image.setAttribute(\"data-id\", source.id); image.style.backgroundImage = \"url(\" + data + \")\"; var link = document.getElementById(\"NetworKit_Overlay_Toolbar_Bottom_Save\"); link.href = data; link.download = source.title + \".svg\"; } function NetworKit_overlayImageShift(delta) { var image = document.getElementById(\"NetworKit_Overlay_Image\"); var source = document.getElementById(image.getAttribute(\"data-id\")); var index = parseInt(source.getAttribute(\"data-image-index\")); var length = parseInt(source.getAttribute(\"data-image-length\")); var index = (index+delta) % length; if (index < 0) { index = length + index; } source.setAttribute(\"data-image-index\", index); NetworKit_overlayUpdate(source); } function NetworKit_toggleDetails(source) { var childs = source.children; var show = false; if (source.getAttribute(\"data-title\") == \"-\") { source.setAttribute(\"data-title\", \"+\"); show = false; } else { source.setAttribute(\"data-title\", \"-\"); show = true; } for (i=0; i<childs.length; i++) { if (show) { childs[i].style.display = \"block\"; } else { childs[i].style.display = \"none\"; } } }';\n",
       "\t\t\t\telement.setAttribute('id', 'NetworKit_script');\n",
       "\t\t\t\tdocument.head.appendChild(element);\n",
       "\t\t\t}\n",
       "\t\t\n",
       "\t\t\t\t\n",
       "\t\t\t{\n",
       "\t\t\t\tvar element = document.getElementById('NetworKit_style');\n",
       "\t\t\t\tif (element) {\n",
       "\t\t\t\t\telement.parentNode.removeChild(element);\n",
       "\t\t\t\t}\n",
       "\t\t\t\telement = document.createElement('style');\n",
       "\t\t\t\telement.type = 'text/css';\n",
       "\t\t\t\telement.innerHTML = '.NetworKit_Page { font-family: Arial, Helvetica, sans-serif; font-size: 14px; } .NetworKit_Page .Value:before { font-family: Arial, Helvetica, sans-serif; font-size: 1.05em; content: attr(data-title) \":\"; margin-left: -2.5em; padding-right: 0.5em; } .NetworKit_Page .Details .Value:before { display: block; } .NetworKit_Page .Value { font-family: monospace; white-space: pre; padding-left: 2.5em; white-space: -moz-pre-wrap !important; white-space: -pre-wrap; white-space: -o-pre-wrap; white-space: pre-wrap; word-wrap: break-word; tab-size: 4; -moz-tab-size: 4; } .NetworKit_Page .Category { clear: both; padding-left: 1em; margin-bottom: 1.5em; } .NetworKit_Page .Category:before { content: attr(data-title); font-size: 1.75em; display: block; margin-left: -0.8em; margin-bottom: 0.5em; } .NetworKit_Page .SubCategory { margin-bottom: 1.5em; padding-left: 1em; } .NetworKit_Page .SubCategory:before { font-size: 1.6em; display: block; margin-left: -0.8em; margin-bottom: 0.5em; } .NetworKit_Page .SubCategory[data-title]:before { content: attr(data-title); } .NetworKit_Page .Block { display: block; } .NetworKit_Page .Block:after { content: \".\"; visibility: hidden; display: block; height: 0; clear: both; } .NetworKit_Page .Block .Thumbnail_Overview, .NetworKit_Page .Block .Thumbnail_ScatterPlot { width: 260px; float: left; } .NetworKit_Page .Block .Thumbnail_Overview img, .NetworKit_Page .Block .Thumbnail_ScatterPlot img { width: 260px; } .NetworKit_Page .Block .Thumbnail_Overview:before, .NetworKit_Page .Block .Thumbnail_ScatterPlot:before { display: block; text-align: center; font-weight: bold; } .NetworKit_Page .Block .Thumbnail_Overview:before { content: attr(data-title); } .NetworKit_Page .HeatCell { font-family: \"Courier New\", Courier, monospace; cursor: pointer; } .NetworKit_Page .HeatCell, .NetworKit_Page .HeatCellName { display: inline; padding: 0.1em; margin-right: 2px; background-color: #FFFFFF } .NetworKit_Page .HeatCellName { margin-left: 0.25em; } .NetworKit_Page .HeatCell:before { content: attr(data-heat); display: inline-block; color: #000000; width: 4em; text-align: center; } .NetworKit_Page .Measure { clear: both; } .NetworKit_Page .Measure .Details { cursor: pointer; } .NetworKit_Page .Measure .Details:before { content: \"[\" attr(data-title) \"]\"; display: block; } .NetworKit_Page .Measure .Details .Value { border-left: 1px dotted black; margin-left: 0.4em; padding-left: 3.5em; pointer-events: none; } .NetworKit_Page .Measure .Details .Spacer:before { content: \".\"; opacity: 0.0; pointer-events: none; } .NetworKit_Page .Measure .Plot { width: 440px; height: 440px; cursor: pointer; float: left; margin-left: -0.9em; margin-right: 20px; } .NetworKit_Page .Measure .Plot .Image { background-repeat: no-repeat; background-position: center center; background-size: contain; height: 100%; pointer-events: none; } .NetworKit_Page .Measure .Stat { width: 500px; float: left; } .NetworKit_Page .Measure .Stat .Group { padding-left: 1.25em; margin-bottom: 0.75em; } .NetworKit_Page .Measure .Stat .Group .Title { font-size: 1.1em; display: block; margin-bottom: 0.3em; margin-left: -0.75em; border-right-style: dotted; border-right-width: 1px; border-bottom-style: dotted; border-bottom-width: 1px; background-color: #D0D0D0; padding-left: 0.2em; } .NetworKit_Page .Measure .Stat .Group .List { -webkit-column-count: 3; -moz-column-count: 3; column-count: 3; } .NetworKit_Page .Measure .Stat .Group .List .Entry { position: relative; line-height: 1.75em; } .NetworKit_Page .Measure .Stat .Group .List .Entry[data-tooltip]:before { position: absolute; left: 0; top: -40px; background-color: #808080; color: #ffffff; height: 30px; line-height: 30px; border-radius: 5px; padding: 0 15px; content: attr(data-tooltip); white-space: nowrap; display: none; } .NetworKit_Page .Measure .Stat .Group .List .Entry[data-tooltip]:after { position: absolute; left: 15px; top: -10px; border-top: 7px solid #808080; border-left: 7px solid transparent; border-right: 7px solid transparent; content: \"\"; display: none; } .NetworKit_Page .Measure .Stat .Group .List .Entry[data-tooltip]:hover:after, .NetworKit_Page .Measure .Stat .Group .List .Entry[data-tooltip]:hover:before { display: block; } .NetworKit_Page .Measure .Stat .Group .List .Entry .MathValue { font-family: \"Courier New\", Courier, monospace; } .NetworKit_Page .Measure:after { content: \".\"; visibility: hidden; display: block; height: 0; clear: both; } .NetworKit_Page .PartitionPie { clear: both; } .NetworKit_Page .PartitionPie img { width: 600px; } #NetworKit_Overlay { left: 0px; top: 0px; display: none; position: absolute; width: 100%; height: 100%; background-color: rgba(0,0,0,0.6); z-index: 1000; } #NetworKit_Overlay_Title { position: absolute; color: white; transform: rotate(-90deg); width: 32em; height: 32em; padding-right: 0.5em; padding-top: 0.5em; text-align: right; font-size: 40px; } #NetworKit_Overlay .button { background: white; cursor: pointer; } #NetworKit_Overlay .button:before { size: 13px; display: inline-block; text-align: center; margin-top: 0.5em; margin-bottom: 0.5em; width: 1.5em; height: 1.5em; } #NetworKit_Overlay .icon-close:before { content: \"X\"; } #NetworKit_Overlay .icon-previous:before { content: \"P\"; } #NetworKit_Overlay .icon-next:before { content: \"N\"; } #NetworKit_Overlay .icon-save:before { content: \"S\"; } #NetworKit_Overlay_Toolbar_Top, #NetworKit_Overlay_Toolbar_Bottom { position: absolute; width: 40px; right: 13px; text-align: right; z-index: 1100; } #NetworKit_Overlay_Toolbar_Top { top: 0.5em; } #NetworKit_Overlay_Toolbar_Bottom { Bottom: 0.5em; } #NetworKit_Overlay_ImageContainer { position: absolute; top: 5%; left: 5%; height: 90%; width: 90%; background-repeat: no-repeat; background-position: center center; background-size: contain; } #NetworKit_Overlay_Image { height: 100%; width: 100%; background-repeat: no-repeat; background-position: center center; background-size: contain; }';\n",
       "\t\t\t\telement.setAttribute('id', 'NetworKit_style');\n",
       "\t\t\t\tdocument.head.appendChild(element);\n",
       "\t\t\t}\n",
       "\t\t\n",
       "\t\t\t\t\n",
       "\t\t\t{\n",
       "\t\t\t\tvar element = document.getElementById('NetworKit_Overlay');\n",
       "\t\t\t\tif (element) {\n",
       "\t\t\t\t\telement.parentNode.removeChild(element);\n",
       "\t\t\t\t}\n",
       "\t\t\t\telement = document.createElement('div');\n",
       "\t\t\t\telement.innerHTML = '<div id=\"NetworKit_Overlay_Toolbar_Top\"><div class=\"button icon-close\" id=\"NetworKit_Overlay_Close\" /></div><div id=\"NetworKit_Overlay_Title\" /> <div id=\"NetworKit_Overlay_ImageContainer\"> <div id=\"NetworKit_Overlay_Image\" /> </div> <div id=\"NetworKit_Overlay_Toolbar_Bottom\"> <div class=\"button icon-previous\" onclick=\"NetworKit_overlayImageShift(-1)\" /> <div class=\"button icon-next\" onclick=\"NetworKit_overlayImageShift(1)\" /> <a id=\"NetworKit_Overlay_Toolbar_Bottom_Save\"><div class=\"button icon-save\" /></a> </div>';\n",
       "\t\t\t\telement.setAttribute('id', 'NetworKit_Overlay');\n",
       "\t\t\t\tdocument.body.appendChild(element);\n",
       "\t\t\t\tdocument.getElementById('NetworKit_Overlay_Close').onclick = function (e) {\n",
       "\t\t\t\t\tdocument.getElementById('NetworKit_Overlay').style.display = 'none';\n",
       "\t\t\t\t}\n",
       "\t\t\t}\n",
       "\t\t\n",
       "\t\t\t-->\n",
       "\t\t\t</script>\n",
       "\t\t"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import h5py\n",
    "import json\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "\n",
    "import networkx as nx\n",
    "import networkit as nk\n",
    "nk.setNumberOfThreads(16)\n",
    "\n",
    "# from graph_sampling.ForestFire import ForestFireSampler\n",
    "# from graph_sampling.RW import RandomWalkWithRestartSampler\n",
    "\n",
    "class rating_data:\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "        self.index = [] # 0: train, 1: validation, 2: test, -1: removed/ignore\n",
    "        for user_data in self.data:\n",
    "            for _ in range(len(user_data)): self.index.append(42)\n",
    "\n",
    "        self.complete_data_stats = None\n",
    "\n",
    "    def train_test_split(self, split_type):\n",
    "        at = 0\n",
    "\n",
    "        for user in range(len(self.data)):\n",
    "            if split_type == \"20_percent_hist\": \n",
    "                first_split_point = int(0.8 * len(self.data[user]))\n",
    "                second_split_point = int(0.9 * len(self.data[user]))\n",
    "\n",
    "                indices = np.arange(len(self.data[user]))\n",
    "                np.random.shuffle(indices)\n",
    "\n",
    "                for timestep, (item, rating, time) in enumerate(self.data[user]):\n",
    "                    if len(self.data[user]) < 3: self.index[at] = -1\n",
    "                    else:\n",
    "                        # Force atleast one element in user history to be in test\n",
    "                        if timestep == indices[0]: self.index[at] = 2\n",
    "                        else:\n",
    "                            if timestep in indices[:first_split_point]: self.index[at] = 0\n",
    "                            elif timestep in indices[first_split_point:second_split_point]: self.index[at] = 1\n",
    "                            else: self.index[at] = 2\n",
    "                    at += 1\n",
    "            \n",
    "            elif split_type == \"leave_2\":\n",
    "                for timestep, (item, rating, time) in enumerate(self.data[user]):\n",
    "                    if len(self.data[user]) < 3: self.index[at] = -1\n",
    "                    else:\n",
    "                        if timestep <= len(self.data[user]) - 3: self.index[at] = 0\n",
    "                        elif timestep == len(self.data[user]) - 2: self.index[at] = 1\n",
    "                        else: self.index[at] = 2\n",
    "                    at += 1\n",
    "\n",
    "        assert at == len(self.index)\n",
    "        self.complete_data_stats = None\n",
    "\n",
    "    def interaction_random_sample(self, percent):\n",
    "        active, at = set(), 0\n",
    "        for u in range(len(self.data)):\n",
    "            for i, r, t in self.data[u]:\n",
    "                # NOTE: only sample on the train-set\n",
    "                if self.index[at] == 0: active.add(at)\n",
    "                at += 1\n",
    "        active = list(active)\n",
    "\n",
    "        # Remove `percent`% at random\n",
    "        remove_mask = {}\n",
    "        for i in active: remove_mask[i] = False\n",
    "        random.shuffle(active)\n",
    "        split_point = int(float(len(active)) * (float(percent) / 100.0))\n",
    "        for i in active[:split_point]: remove_mask[i] = True\n",
    "\n",
    "        at = 0\n",
    "        for u in range(len(self.data)):\n",
    "            for i, r, t in self.data[u]:\n",
    "                if remove_mask.get(at, False) and self.index[at] == 0: self.index[at] = -1\n",
    "                at += 1\n",
    "        assert at == len(self.index)\n",
    "\n",
    "    def frequency_sample(self, percent, sample_type):\n",
    "        hist, at = {}, 0\n",
    "        for u in range(len(self.data)):\n",
    "            for i, r, t in self.data[u]:\n",
    "                key = [ u, i ][sample_type]\n",
    "                if key not in hist: hist[key] = []\n",
    "                # NOTE: only sample on the train-set\n",
    "                if self.index[at] == 0: hist[key].append(at) \n",
    "                at += 1\n",
    "\n",
    "        # Remove `percent`% at random\n",
    "        remove_mask = {}\n",
    "        for key in hist:\n",
    "            interactions = hist[key]\n",
    "            random.shuffle(interactions)\n",
    "            split_point = math.ceil(float(len(interactions)) * (float(percent) / 100.0))\n",
    "            for i in interactions[:split_point]: remove_mask[i] = True\n",
    "\n",
    "        at = 0\n",
    "        for u in range(len(self.data)):\n",
    "            for i, r, t in self.data[u]:\n",
    "                if remove_mask.get(at, False) and self.index[at] == 0: self.index[at] = -1\n",
    "                at += 1\n",
    "        assert at == len(self.index)\n",
    "\n",
    "    def user_random_sample(self, percent):\n",
    "        hist, at, total = {}, 0, 0\n",
    "        for u in range(len(self.data)):\n",
    "            for i, r, t in self.data[u]:\n",
    "                # NOTE: only sample on the train-set\n",
    "                if self.index[at] == 0: \n",
    "                    if u not in hist: hist[u] = 0\n",
    "                    hist[u] += 1\n",
    "                    total += 1\n",
    "                at += 1\n",
    "\n",
    "        # Remove `percent`% at random\n",
    "        user_freqs = list(hist.items()) ; np.random.shuffle(user_freqs)\n",
    "        interactions_to_remove, removed, users_to_remove = total * (float(percent) / 100.0), 0, set()\n",
    "        for u, cnt in user_freqs:\n",
    "            if removed >= interactions_to_remove: break\n",
    "            users_to_remove.add(u)\n",
    "            removed += cnt\n",
    "\n",
    "        at = 0\n",
    "        for u in range(len(self.data)):\n",
    "            for i, r, t in self.data[u]:\n",
    "                if u in users_to_remove and self.index[at] == 0: self.index[at] = -1\n",
    "                at += 1\n",
    "        assert at == len(self.index)\n",
    "\n",
    "    def temporal_sample(self, percent):\n",
    "        hist, at = {}, 0\n",
    "        for u in range(len(self.data)):\n",
    "            for i, r, t in self.data[u]:\n",
    "                if u not in hist: hist[u] = []\n",
    "                # NOTE: only sample on the train-set\n",
    "                if self.index[at] == 0: hist[u].append(at) \n",
    "                at += 1\n",
    "\n",
    "        # Remove first `percent`% interactions for each user\n",
    "        remove_mask = {}\n",
    "        for u in hist:\n",
    "            interactions = hist[u]\n",
    "            # random.shuffle(interactions) ### No shuffling, remove first % interactions\n",
    "            split_point = math.ceil(float(len(interactions)) * (float(percent) / 100.0))\n",
    "            for i in interactions[:split_point]: remove_mask[i] = True\n",
    "\n",
    "        at = 0\n",
    "        for u in range(len(self.data)):\n",
    "            for i, r, t in self.data[u]:\n",
    "                if remove_mask.get(at, False) and self.index[at] == 0: self.index[at] = -1\n",
    "                at += 1\n",
    "        assert at == len(self.index)\n",
    "\n",
    "    def tail_user_remove(self, percent):\n",
    "        hist, at, total = {}, 0, 0\n",
    "        for u in range(len(self.data)):\n",
    "            for i, r, t in self.data[u]:\n",
    "                # NOTE: only count on the train-set\n",
    "                if self.index[at] == 0: \n",
    "                    if u not in hist: hist[u] = 0\n",
    "                    hist[u] += 1\n",
    "                    total += 1\n",
    "                at += 1\n",
    "\n",
    "        user_freqs = sorted(list(hist.items()), key = lambda x: x[1])\n",
    "        interactions_to_remove, removed, users_to_remove = total * (float(percent) / 100.0), 0, set()\n",
    "        for u, cnt in user_freqs:\n",
    "            if removed >= interactions_to_remove: break\n",
    "            users_to_remove.add(u)\n",
    "            removed += cnt\n",
    "\n",
    "        at = 0\n",
    "        for u in range(len(self.data)):\n",
    "            for i, r, t in self.data[u]:\n",
    "                if u in users_to_remove and self.index[at] == 0: self.index[at] = -1\n",
    "                at += 1\n",
    "        assert at == len(self.index)\n",
    "\n",
    "    def svp_sample(self, percent, svp_handler, sampling_type):\n",
    "        self.index = {\n",
    "            'forgetting_events': svp_handler.forgetting_events,\n",
    "            'forgetting_events_user': svp_handler.forgetting_events_user,\n",
    "            'forgetting_events_propensity': svp_handler.forgetting_events_propensity,\n",
    "            'forgetting_events_user_propensity': svp_handler.forgetting_events_user_propensity,\n",
    "        }[sampling_type](percent, self.data, self.index)\n",
    "\n",
    "    def construct_nx_graph(self):\n",
    "        # Make graph\n",
    "        g = nx.Graph()\n",
    "\n",
    "        # Add nodes & edges\n",
    "        user_map, item_map, rev_user_map, rev_item_map, at, node_num = {}, {}, {}, {}, 0, 0\n",
    "        user_actions, item_actions, total = defaultdict(list), defaultdict(list), 0\n",
    "\n",
    "        for u in range(len(self.data)):\n",
    "            for i, r, t in self.data[u]:\n",
    "                # NOTE: only sample on the train-set\n",
    "                if self.index[at] == 0: \n",
    "                    total += 1\n",
    "                    user_actions[u].append(at)\n",
    "                    item_actions[i].append(at)\n",
    "\n",
    "                    if u not in user_map:\n",
    "                        user_map[u] = node_num\n",
    "                        rev_user_map[node_num] = u\n",
    "                        g.add_node(node_num)\n",
    "                        node_num += 1\n",
    "                    if i not in item_map:\n",
    "                        item_map[i] = node_num\n",
    "                        rev_item_map[node_num] = i\n",
    "                        g.add_node(node_num)\n",
    "                        node_num += 1\n",
    "                    g.add_edge(user_map[u], item_map[i])\n",
    "                at += 1\n",
    "        assert node_num == g.number_of_nodes()\n",
    "        return g, rev_user_map, rev_item_map, user_actions, item_actions\n",
    "\n",
    "    def pagerank_sample(self, percent):\n",
    "        # networkx graph\n",
    "        g, rev_user_map, rev_item_map, user_actions, item_actions = self.construct_nx_graph()\n",
    "\n",
    "        # Convert to networkit\n",
    "        nk_g = nk.nxadapter.nx2nk(g)\n",
    "\n",
    "        # Run pagerank\n",
    "        pr = nk.centrality.PageRank(nk_g, 1e-6) ; pr.run()\n",
    "\n",
    "        # Remove `percent`% acc to pagerank scores\n",
    "        # THOUGHT: the nodes with the least pagerank scores will most probably be the tail users/items\n",
    "        interactions_to_remove, removed = nk_g.numberOfEdges() * (float(percent) / 100.0), 0\n",
    "        for node, _ in pr.ranking()[::-1]:\n",
    "            if removed >= interactions_to_remove: break\n",
    "            \n",
    "            if node in rev_user_map: \n",
    "                for at in user_actions[rev_user_map[node]]:\n",
    "                    if self.index[at] != -1: removed += 1\n",
    "                    self.index[at] = -1\n",
    "            else: \n",
    "                for at in item_actions[rev_item_map[node]]:\n",
    "                    if self.index[at] != -1: removed += 1\n",
    "                    self.index[at] = -1\n",
    "\n",
    "    def random_walk_sample(self, percent):\n",
    "        at, total = 0, 0\n",
    "        for u in range(len(self.data)):\n",
    "            for i, r, t in self.data[u]:\n",
    "                if self.index[at] == 0: total += 1\n",
    "                at += 1\n",
    "\n",
    "        interactions_to_remove, removed = float(total) * (float(percent) / 100.0), 0\n",
    "        while removed < interactions_to_remove:\n",
    "            # networkx graph\n",
    "            nx_g, rev_user_map, rev_item_map, user_actions, item_actions = self.construct_nx_graph()\n",
    "            \n",
    "            # Create sampler ## Nodes to keep\n",
    "            sampler = RandomWalkWithRestartSampler(number_of_nodes = int(nx_g.number_of_nodes() * (float(100 - percent) / 100.0)))\n",
    "            sampler._create_initial_node_set(nx_g, None)\n",
    "\n",
    "            # Sample\n",
    "            while len(sampler._sampled_nodes) < sampler.number_of_nodes:\n",
    "                sampler._do_a_step(nx_g)\n",
    "\n",
    "            # Remove from the main graph\n",
    "            ## `sampler._sampled_nodes` are the nodes that are kept, not removed\n",
    "            nodes_to_remove = list(sampler._set_of_nodes.difference(sampler._sampled_nodes))\n",
    "\n",
    "            for node in nodes_to_remove:\n",
    "                if removed >= interactions_to_remove: break\n",
    "\n",
    "                if node in rev_user_map: \n",
    "                    for at in user_actions[rev_user_map[node]]:\n",
    "                        if self.index[at] != -1: removed += 1\n",
    "                        self.index[at] = -1\n",
    "                else: \n",
    "                    for at in item_actions[rev_item_map[node]]:\n",
    "                        if self.index[at] != -1: removed += 1\n",
    "                        self.index[at] = -1\n",
    "\n",
    "    def forest_fire_sample(self, percent):\n",
    "        at, total = 0, 0\n",
    "        for u in range(len(self.data)):\n",
    "            for i, r, t in self.data[u]:\n",
    "                if self.index[at] == 0: total += 1\n",
    "                at += 1\n",
    "\n",
    "        interactions_to_remove, removed = float(total) * (float(percent) / 100.0), 0\n",
    "        while removed < interactions_to_remove:\n",
    "            # networkx graph\n",
    "            nx_g, rev_user_map, rev_item_map, user_actions, item_actions = self.construct_nx_graph()\n",
    "            \n",
    "            # Create sampler ## Nodes to keep\n",
    "            sampler = ForestFireSampler(number_of_nodes = int(nx_g.number_of_nodes() * (float(100 - percent) / 100.0)))\n",
    "            sampler._create_node_sets(nx_g)\n",
    "\n",
    "            # Sample\n",
    "            while len(sampler._sampled_nodes) < sampler.number_of_nodes: \n",
    "                sampler._start_a_fire(nx_g)\n",
    "\n",
    "            # Remove from the main graph\n",
    "            ## `sampler._sampled_nodes` are the nodes that are kept, not removed\n",
    "            nodes_to_remove = list(sampler._set_of_nodes.difference(sampler._sampled_nodes))\n",
    "\n",
    "            for node in nodes_to_remove:\n",
    "                if removed >= interactions_to_remove: break\n",
    "\n",
    "                if node in rev_user_map: \n",
    "                    for at in user_actions[rev_user_map[node]]:\n",
    "                        if self.index[at] != -1: removed += 1\n",
    "                        self.index[at] = -1\n",
    "                else: \n",
    "                    for at in item_actions[rev_item_map[node]]:\n",
    "                        if self.index[at] != -1: removed += 1\n",
    "                        self.index[at] = -1\n",
    "\n",
    "    def measure_data_stats(self):\n",
    "        num_users, num_items, num_interactions, num_test, num_val = set(), set(), 0, 0, 0\n",
    "        at = 0\n",
    "        for u in range(len(self.data)):\n",
    "            for i, _, _ in self.data[u]:\n",
    "                if self.index[at] == 0: num_interactions += 1\n",
    "                if self.index[at] == 1: num_val += 1\n",
    "                if self.index[at] == 2: num_test += 1\n",
    "\n",
    "                if self.index[at] != -1:\n",
    "                    num_users.add(u)\n",
    "                    num_items.add(i)\n",
    "                at += 1\n",
    "\n",
    "        data_stats = {}\n",
    "        data_stats[\"num_users\"] = len(num_users)\n",
    "        data_stats[\"num_items\"] = len(num_items)\n",
    "        data_stats[\"num_train_interactions\"] = num_interactions\n",
    "        data_stats[\"num_test\"] = num_test\n",
    "        data_stats[\"num_val\"] = num_val\n",
    "\n",
    "        return data_stats\n",
    "\n",
    "    def save_index(self, path, statistics = True):\n",
    "        os.makedirs(path, exist_ok = True)\n",
    "        with open(path + \"/index.npz\", \"wb\") as f: np.savez_compressed(f, data = self.index)\n",
    "\n",
    "        if statistics:\n",
    "            data_stats = self.measure_data_stats() \n",
    "            if self.complete_data_stats is None: print(\"FULL DATA:\", data_stats)\n",
    "            else: \n",
    "                def convert(key): return round(100.0 - (100.0 * float(data_stats[key] / float(self.complete_data_stats[key]))), 2)\n",
    "                print(\"SAMPLE SIZE: {}% users ; {}% items ; {}% train interactions ; {}% test interactions removed\".format(\n",
    "                    convert('num_users'), convert('num_items'), convert('num_train_interactions'), convert('num_test')\n",
    "                ))\n",
    "            with open(path + \"/data_stats.json\", 'w') as f: json.dump(data_stats, f)\n",
    "\n",
    "    def load_index(self, path):\n",
    "        self.index = np.load(path + \"/index.npz\")['data']\n",
    "        if self.complete_data_stats is None: self.complete_data_stats = self.measure_data_stats()\n",
    "\n",
    "    def save_data(self, path):\n",
    "        flat_data = []\n",
    "        for u in range(len(self.data)):\n",
    "            flat_data += list(map(lambda x: [ u ] + x, self.data[u]))\n",
    "        flat_data = np.array(flat_data)\n",
    "\n",
    "        shape = [ len(flat_data) ]\n",
    "\n",
    "        os.makedirs(path, exist_ok = True)\n",
    "        with h5py.File(path + '/total_data.hdf5', 'w') as file:\n",
    "            dset = {}\n",
    "            dset['user'] = file.create_dataset(\"user\", shape, dtype = 'i4', maxshape = shape, compression=\"gzip\")\n",
    "            dset['item'] = file.create_dataset(\"item\", shape, dtype = 'i4', maxshape = shape, compression=\"gzip\")\n",
    "            dset['rating'] = file.create_dataset(\"rating\", shape, dtype = 'f', maxshape = shape, compression=\"gzip\")\n",
    "            dset['time'] = file.create_dataset(\"time\", shape, dtype = 'i4', maxshape = shape, compression=\"gzip\")\n",
    "\n",
    "            dset['user'][:] = flat_data[:, 0]\n",
    "            dset['item'][:] = flat_data[:, 1]\n",
    "            dset['rating'][:] = flat_data[:, 2]\n",
    "            dset['time'][:] = flat_data[:, 3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Where to store the datasets?\n",
    "!mkdir -p datasets/\n",
    "\n",
    "# Where to store the logs/models of trained models\n",
    "!mkdir -p experiments/sampling_runs/results/logs/trained/\n",
    "!mkdir -p experiments/sampling_runs/results/models/trained/\n",
    "\n",
    "# Where to store the logs/models of trained proxy models for SVP\n",
    "!mkdir -p experiments/sampling_runs/results/logs/SVP/\n",
    "!mkdir -p experiments/sampling_runs/results/models/SVP/\n",
    "\n",
    "# Base directory for all Data-Genie experiments\n",
    "!mkdir -p experiments/data_genie/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Magazine_Subscripti 100%[===================>]   3.57M  6.52MB/s    in 0.5s    \n",
      "ml-100k.zip         100%[===================>]   4.70M  24.1MB/s    in 0.2s    \n"
     ]
    }
   ],
   "source": [
    "# # download_amazon_magazine\n",
    "# !mkdir datasets/magazine/\n",
    "# link=\"http://deepyeti.ucsd.edu/jianmo/amazon/categoryFilesSmall/Magazine_Subscriptions.csv\"\n",
    "# !wget $link -P datasets/magazine/ -q --show-progress\n",
    "# !mv datasets/magazine/Magazine_Subscriptions.csv datasets/magazine/data.csv\n",
    "\n",
    "# download_ml_100k\n",
    "link=\"https://files.grouplens.org/datasets/movielens/ml-100k.zip\"\n",
    "!wget $link -P datasets/ -q --show-progress\n",
    "!unzip -qq datasets/ml-100k.zip -d datasets/ \n",
    "!rm datasets/ml-100k.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = './datasets/ml-100k/u.data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep(dataset):\t\n",
    "\tf = open(data_path)\n",
    "\tusers, items, ratings, time = [], [], [], []\n",
    "\n",
    "\tline = f.readline()\n",
    "\twhile line:\n",
    "\t\tu, i, r, t = line.strip().split('\\t')\n",
    "\t\tusers.append(int(u))\n",
    "\t\titems.append(int(i))\n",
    "\t\tratings.append(float(r))\n",
    "\t\ttime.append(int(t))\n",
    "\t\tline = f.readline()\n",
    "\n",
    "\tmin_user = min(users) ; max_user = max(users)\n",
    "\tnum_users = len(set(users))\n",
    "\n",
    "\tif min_user == 1:\n",
    "\t\tassert num_users == max_user\n",
    "\telse:\n",
    "\t\tassert num_users == max_user + 1\n",
    "\n",
    "\tdata = [ [] for _ in range(num_users) ]\n",
    "\tfor i in range(len(users)):\n",
    "\t\tdata[users[i] - min_user].append([ items[i], ratings[i], time[i] ])\n",
    "\n",
    "\t# Time sort data\n",
    "\tfor i in range(len(data)): \n",
    "\t\tdata[i].sort(key = lambda x: x[2]) \n",
    "\n",
    "\t# Shuffling users\n",
    "\t# indices = np.arange(len(data)) ; np.random.shuffle(indices)\n",
    "\t# data = np.array(data)[indices].tolist()\n",
    "\n",
    "\treturn rating_data(remap_items(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "!!!!!!!! STARTED PROCESSING ml-100k !!!!!!!!\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "leave_2 split, Overall:\n",
      "FULL DATA: {'num_users': 943, 'num_items': 1682, 'num_train_interactions': 98114, 'num_test': 943, 'num_val': 943}\n",
      "\n",
      "leave_2 split, user history random sampling\n",
      "SAMPLE SIZE: 0.0% users ; 1.9% items ; 20.38% train interactions ; 0.0% test interactions removed\n",
      "SAMPLE SIZE: 0.0% users ; 3.69% items ; 40.39% train interactions ; 0.0% test interactions removed\n",
      "SAMPLE SIZE: 0.0% users ; 8.32% items ; 60.38% train interactions ; 0.0% test interactions removed\n",
      "SAMPLE SIZE: 0.0% users ; 15.76% items ; 80.39% train interactions ; 0.0% test interactions removed\n",
      "SAMPLE SIZE: 0.0% users ; 22.89% items ; 90.43% train interactions ; 0.0% test interactions removed\n",
      "SAMPLE SIZE: 0.0% users ; 47.15% items ; 99.41% train interactions ; 0.0% test interactions removed\n",
      "\n",
      "leave_2 split, user random sampling\n",
      "SAMPLE SIZE: 0.0% users ; 0.71% items ; 20.01% train interactions ; 0.0% test interactions removed\n",
      "SAMPLE SIZE: 0.0% users ; 3.98% items ; 40.07% train interactions ; 0.0% test interactions removed\n",
      "SAMPLE SIZE: 0.0% users ; 8.5% items ; 60.15% train interactions ; 0.0% test interactions removed\n",
      "SAMPLE SIZE: 0.0% users ; 14.45% items ; 80.01% train interactions ; 0.0% test interactions removed\n",
      "SAMPLE SIZE: 0.0% users ; 24.79% items ; 90.01% train interactions ; 0.0% test interactions removed\n",
      "SAMPLE SIZE: 0.0% users ; 50.12% items ; 99.36% train interactions ; 0.0% test interactions removed\n",
      "\n",
      "leave_2 split, interaction random sampling\n",
      "SAMPLE SIZE: 0.0% users ; 1.9% items ; 20.0% train interactions ; 0.0% test interactions removed\n",
      "SAMPLE SIZE: 0.0% users ; 4.1% items ; 40.0% train interactions ; 0.0% test interactions removed\n",
      "SAMPLE SIZE: 0.0% users ; 7.37% items ; 60.0% train interactions ; 0.0% test interactions removed\n",
      "SAMPLE SIZE: 0.0% users ; 14.86% items ; 80.0% train interactions ; 0.0% test interactions removed\n",
      "SAMPLE SIZE: 0.0% users ; 23.66% items ; 90.0% train interactions ; 0.0% test interactions removed\n",
      "SAMPLE SIZE: 0.0% users ; 45.3% items ; 99.0% train interactions ; 0.0% test interactions removed\n",
      "\n",
      "leave_2 split, user history temporal sampling\n",
      "SAMPLE SIZE: 0.0% users ; 0.42% items ; 20.38% train interactions ; 0.0% test interactions removed\n",
      "SAMPLE SIZE: 0.0% users ; 2.85% items ; 40.39% train interactions ; 0.0% test interactions removed\n",
      "SAMPLE SIZE: 0.0% users ; 4.93% items ; 60.38% train interactions ; 0.0% test interactions removed\n",
      "SAMPLE SIZE: 0.0% users ; 10.34% items ; 80.39% train interactions ; 0.0% test interactions removed\n",
      "SAMPLE SIZE: 0.0% users ; 18.07% items ; 90.43% train interactions ; 0.0% test interactions removed\n",
      "SAMPLE SIZE: 0.0% users ; 46.2% items ; 99.41% train interactions ; 0.0% test interactions removed\n",
      "\n",
      "leave_2 split, tail user sampling\n",
      "SAMPLE SIZE: 0.0% users ; 0.42% items ; 20.02% train interactions ; 0.0% test interactions removed\n",
      "SAMPLE SIZE: 0.0% users ; 1.49% items ; 40.09% train interactions ; 0.0% test interactions removed\n",
      "SAMPLE SIZE: 0.0% users ; 2.2% items ; 60.18% train interactions ; 0.0% test interactions removed\n",
      "SAMPLE SIZE: 0.0% users ; 4.58% items ; 80.2% train interactions ; 0.0% test interactions removed\n",
      "SAMPLE SIZE: 0.0% users ; 6.36% items ; 90.28% train interactions ; 0.0% test interactions removed\n",
      "SAMPLE SIZE: 0.0% users ; 30.38% items ; 99.25% train interactions ; 0.0% test interactions removed\n",
      "\n",
      "leave_2 split, pagerank sampling\n",
      "SAMPLE SIZE: 0.0% users ; 45.96% items ; 20.03% train interactions ; 0.0% test interactions removed\n",
      "SAMPLE SIZE: 0.0% users ; 51.19% items ; 40.05% train interactions ; 0.0% test interactions removed\n",
      "SAMPLE SIZE: 0.0% users ; 53.21% items ; 60.04% train interactions ; 0.0% test interactions removed\n",
      "SAMPLE SIZE: 0.0% users ; 54.16% items ; 80.02% train interactions ; 0.0% test interactions removed\n",
      "SAMPLE SIZE: 0.0% users ; 54.34% items ; 90.04% train interactions ; 0.0% test interactions removed\n",
      "SAMPLE SIZE: 0.0% users ; 54.46% items ; 99.06% train interactions ; 0.0% test interactions removed\n",
      "\n",
      "leave_2 split, random walk sampling\n",
      "SAMPLE SIZE: 0.0% users ; 42.63% items ; 20.01% train interactions ; 0.0% test interactions removed\n",
      "SAMPLE SIZE: 0.0% users ; 46.2% items ; 40.01% train interactions ; 0.0% test interactions removed\n",
      "SAMPLE SIZE: 0.0% users ; 46.79% items ; 60.0% train interactions ; 0.0% test interactions removed\n",
      "SAMPLE SIZE: 0.0% users ; 50.54% items ; 80.11% train interactions ; 0.0% test interactions removed\n",
      "SAMPLE SIZE: 0.0% users ; 49.29% items ; 90.06% train interactions ; 0.0% test interactions removed\n",
      "SAMPLE SIZE: 0.0% users ; 50.48% items ; 99.0% train interactions ; 0.0% test interactions removed\n",
      "\n",
      "leave_2 split, forest fire sampling\n",
      "SAMPLE SIZE: 0.0% users ; 40.84% items ; 20.05% train interactions ; 0.0% test interactions removed\n",
      "SAMPLE SIZE: 0.0% users ; 41.02% items ; 40.01% train interactions ; 0.0% test interactions removed\n",
      "SAMPLE SIZE: 0.0% users ; 48.34% items ; 60.16% train interactions ; 0.0% test interactions removed\n",
      "SAMPLE SIZE: 0.0% users ; 50.0% items ; 80.11% train interactions ; 0.0% test interactions removed\n",
      "SAMPLE SIZE: 0.0% users ; 49.58% items ; 90.01% train interactions ; 0.0% test interactions removed\n",
      "SAMPLE SIZE: 0.0% users ; 51.01% items ; 99.0% train interactions ; 0.0% test interactions removed\n",
      "\n",
      "# of users: 943\n",
      "# of items: 1682\n",
      "\n",
      "\n",
      "Simulation run on: 2022-01-15 06:32:52.391076\n",
      "\n",
      "\n",
      "Data reading complete!\n",
      "Number of train batches:   96\n",
      "Number of validation batches:    1\n",
      "Number of test batches:    1\n",
      "MF()\n",
      "\n",
      "Model Built!\n",
      "Starting Training...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 96/96 [00:00<00:00, 370.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model...\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 1 | time =  0.46 | eval = partial | HR@10 = 43.9024 | HR@100 = 100.0 | NDCG@10 = 23.3166 | NDCG@100 = 35.1766 | PSP@10 = 43.9024 | PSP@100 = 100.0 | AUC = 0.807 | dataset = ml-100k (VAL)\n",
      "-----------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 96/96 [00:00<00:00, 371.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 2 | time =  0.28 (VAL)\n",
      "-----------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 96/96 [00:00<00:00, 120.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 3 | time =  0.82 (VAL)\n",
      "-----------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 96/96 [00:00<00:00, 213.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 4 | time =  0.48 (VAL)\n",
      "-----------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 96/96 [00:00<00:00, 216.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 5 | time =  0.46 (VAL)\n",
      "-----------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 96/96 [00:00<00:00, 419.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 6 | time =  0.25 (VAL)\n",
      "-----------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 96/96 [00:00<00:00, 194.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 7 | time =  0.51 (VAL)\n",
      "-----------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 96/96 [00:00<00:00, 211.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 8 | time =  0.47 (VAL)\n",
      "-----------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 96/96 [00:00<00:00, 342.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 9 | time =  0.29 (VAL)\n",
      "-----------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 96/96 [00:00<00:00, 205.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 10 | time =  0.48 (VAL)\n",
      "-----------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 96/96 [00:00<00:00, 563.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 11 | time =  0.19 (VAL)\n",
      "-----------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 96/96 [00:00<00:00, 178.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 12 | time =  0.55 (VAL)\n",
      "-----------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 96/96 [00:00<00:00, 196.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 13 | time =  0.51 (VAL)\n",
      "-----------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 96/96 [00:00<00:00, 396.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 14 | time =  0.27 (VAL)\n",
      "-----------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 96/96 [00:00<00:00, 196.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 15 | time =  0.52 (VAL)\n",
      "-----------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 96/96 [00:00<00:00, 175.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 16 | time =  0.58 (VAL)\n",
      "-----------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 96/96 [00:00<00:00, 563.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 17 | time =  0.20 (VAL)\n",
      "-----------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 96/96 [00:00<00:00, 194.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 18 | time =  0.51 (VAL)\n",
      "-----------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 96/96 [00:00<00:00, 194.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 19 | time =  0.51 (VAL)\n",
      "-----------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 96/96 [00:00<00:00, 431.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 20 | time =  0.24 (VAL)\n",
      "-----------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 96/96 [00:00<00:00, 192.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 21 | time =  0.51 (VAL)\n",
      "-----------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 96/96 [00:00<00:00, 187.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 22 | time =  0.53 (VAL)\n",
      "-----------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 96/96 [00:00<00:00, 457.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 23 | time =  0.24 (VAL)\n",
      "-----------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 96/96 [00:00<00:00, 212.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 24 | time =  0.47 (VAL)\n",
      "-----------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 96/96 [00:00<00:00, 223.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 25 | time =  0.45 (VAL)\n",
      "-----------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 96/96 [00:00<00:00, 487.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 26 | time =  0.22 (VAL)\n",
      "-----------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 96/96 [00:00<00:00, 207.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 27 | time =  0.49 (VAL)\n",
      "-----------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 96/96 [00:00<00:00, 226.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 28 | time =  0.45 (VAL)\n",
      "-----------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 96/96 [00:00<00:00, 511.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 29 | time =  0.22 (VAL)\n",
      "-----------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 96/96 [00:00<00:00, 210.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 30 | time =  0.48 (VAL)\n",
      "-----------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 96/96 [00:00<00:00, 211.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 31 | time =  0.48 (VAL)\n",
      "-----------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 96/96 [00:00<00:00, 383.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 32 | time =  0.28 (VAL)\n",
      "-----------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 96/96 [00:00<00:00, 237.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 33 | time =  0.43 (VAL)\n",
      "-----------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 96/96 [00:00<00:00, 520.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 34 | time =  0.21 (VAL)\n",
      "-----------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 96/96 [00:00<00:00, 216.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 35 | time =  0.47 (VAL)\n",
      "-----------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 96/96 [00:00<00:00, 224.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 36 | time =  0.45 (VAL)\n",
      "-----------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 96/96 [00:00<00:00, 391.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 37 | time =  0.26 (VAL)\n",
      "-----------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 96/96 [00:00<00:00, 215.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 38 | time =  0.46 (VAL)\n",
      "-----------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 96/96 [00:00<00:00, 205.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 39 | time =  0.48 (VAL)\n",
      "-----------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 96/96 [00:00<00:00, 403.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 40 | time =  0.26 (VAL)\n",
      "-----------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 96/96 [00:00<00:00, 227.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 41 | time =  0.44 (VAL)\n",
      "-----------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 96/96 [00:00<00:00, 214.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 42 | time =  0.46 (VAL)\n",
      "-----------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 96/96 [00:00<00:00, 514.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 43 | time =  0.21 (VAL)\n",
      "-----------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 96/96 [00:00<00:00, 203.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 44 | time =  0.49 (VAL)\n",
      "-----------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 96/96 [00:00<00:00, 197.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 45 | time =  0.51 (VAL)\n",
      "-----------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 96/96 [00:00<00:00, 749.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 46 | time =  0.14 (VAL)\n",
      "-----------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 96/96 [00:00<00:00, 180.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 47 | time =  0.54 (VAL)\n",
      "-----------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 96/96 [00:00<00:00, 214.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 48 | time =  0.47 (VAL)\n",
      "-----------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 96/96 [00:00<00:00, 547.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 49 | time =  0.20 (VAL)\n",
      "-----------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 96/96 [00:00<00:00, 210.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 50 | time =  0.49 (VAL)\n",
      "-----------------------------------------------------------------------------------------\n",
      "\n",
      "leave_2 split, SVP: bias_only_forgetting_events, sequential loss\n",
      "SAMPLE SIZE: 0.0% users ; 0.0% items ; 20.0% train interactions ; 0.0% test interactions removed\n",
      "SAMPLE SIZE: 0.0% users ; 0.0% items ; 40.0% train interactions ; 0.0% test interactions removed\n",
      "SAMPLE SIZE: 0.0% users ; 0.12% items ; 60.0% train interactions ; 0.0% test interactions removed\n",
      "SAMPLE SIZE: 0.0% users ; 1.37% items ; 80.0% train interactions ; 0.0% test interactions removed\n",
      "SAMPLE SIZE: 0.0% users ; 4.64% items ; 90.0% train interactions ; 0.0% test interactions removed\n",
      "SAMPLE SIZE: 0.0% users ; 27.35% items ; 99.0% train interactions ; 0.0% test interactions removed\n",
      "\n",
      "leave_2 split, SVP: bias_only_forgetting_events_propensity, sequential loss\n",
      "SAMPLE SIZE: 0.0% users ; 0.0% items ; 20.0% train interactions ; 0.0% test interactions removed\n",
      "SAMPLE SIZE: 0.0% users ; 0.0% items ; 40.0% train interactions ; 0.0% test interactions removed\n",
      "SAMPLE SIZE: 0.0% users ; 0.12% items ; 60.0% train interactions ; 0.0% test interactions removed\n",
      "SAMPLE SIZE: 0.0% users ; 1.66% items ; 80.0% train interactions ; 0.0% test interactions removed\n",
      "SAMPLE SIZE: 0.0% users ; 6.6% items ; 90.0% train interactions ; 0.0% test interactions removed\n",
      "SAMPLE SIZE: 0.0% users ; 30.62% items ; 99.0% train interactions ; 0.0% test interactions removed\n",
      "\n",
      "leave_2 split, SVP: bias_only_forgetting_events_user, sequential loss\n",
      "SAMPLE SIZE: 0.0% users ; 0.0% items ; 20.04% train interactions ; 0.0% test interactions removed\n",
      "SAMPLE SIZE: 0.0% users ; 0.54% items ; 40.05% train interactions ; 0.0% test interactions removed\n",
      "SAMPLE SIZE: 0.0% users ; 1.25% items ; 60.03% train interactions ; 0.0% test interactions removed\n",
      "SAMPLE SIZE: 0.0% users ; 2.68% items ; 80.63% train interactions ; 0.0% test interactions removed\n",
      "SAMPLE SIZE: 0.0% users ; 4.99% items ; 90.05% train interactions ; 0.0% test interactions removed\n",
      "SAMPLE SIZE: 0.0% users ; 37.57% items ; 99.18% train interactions ; 0.0% test interactions removed\n",
      "\n",
      "leave_2 split, SVP: bias_only_forgetting_events_user_propensity, sequential loss\n",
      "SAMPLE SIZE: 0.0% users ; 0.24% items ; 20.04% train interactions ; 0.0% test interactions removed\n",
      "SAMPLE SIZE: 0.0% users ; 0.89% items ; 40.09% train interactions ; 0.0% test interactions removed\n",
      "SAMPLE SIZE: 0.0% users ; 2.68% items ; 60.02% train interactions ; 0.0% test interactions removed\n",
      "SAMPLE SIZE: 0.0% users ; 6.24% items ; 80.07% train interactions ; 0.0% test interactions removed\n",
      "SAMPLE SIZE: 0.0% users ; 10.29% items ; 90.04% train interactions ; 0.0% test interactions removed\n",
      "SAMPLE SIZE: 0.0% users ; 46.91% items ; 99.1% train interactions ; 0.0% test interactions removed\n",
      "\n",
      "# of users: 943\n",
      "# of items: 1682\n",
      "\n",
      "\n",
      "Simulation run on: 2022-01-15 06:33:28.358451\n",
      "\n",
      "\n",
      "Data reading complete!\n",
      "Number of train batches:   96\n",
      "Number of validation batches:    1\n",
      "Number of test batches:    1\n",
      "MF(\n",
      "  (user_embedding): Embedding(943, 10)\n",
      "  (item_embedding): Embedding(1682, 10)\n",
      "  (dropout): Dropout(p=0.3, inplace=False)\n",
      ")\n",
      "\n",
      "Model Built!\n",
      "Starting Training...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 96/96 [00:00<00:00, 225.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model...\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 1 | time =  0.62 | eval = partial | HR@10 = 44.3266 | HR@100 = 100.0 | NDCG@10 = 23.8439 | NDCG@100 = 35.5882 | PSP@10 = 44.3266 | PSP@100 = 100.0 | AUC = 0.8074 | dataset = ml-100k (VAL)\n",
      "-----------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 96/96 [00:00<00:00, 127.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 2 | time =  0.77 (VAL)\n",
      "-----------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 96/96 [00:00<00:00, 194.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 3 | time =  0.52 (VAL)\n",
      "-----------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 96/96 [00:00<00:00, 244.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 4 | time =  0.41 (VAL)\n",
      "-----------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 96/96 [00:00<00:00, 169.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 5 | time =  0.58 (VAL)\n",
      "-----------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 96/96 [00:00<00:00, 259.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 6 | time =  0.39 (VAL)\n",
      "-----------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 96/96 [00:00<00:00, 206.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 7 | time =  0.49 (VAL)\n",
      "-----------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 96/96 [00:00<00:00, 145.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 8 | time =  0.68 (VAL)\n",
      "-----------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 96/96 [00:00<00:00, 282.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 9 | time =  0.35 (VAL)\n",
      "-----------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 96/96 [00:00<00:00, 203.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 10 | time =  0.49 (VAL)\n",
      "-----------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 96/96 [00:00<00:00, 145.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 11 | time =  0.67 (VAL)\n",
      "-----------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 96/96 [00:00<00:00, 154.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 12 | time =  0.63 (VAL)\n",
      "-----------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 96/96 [00:00<00:00, 248.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 13 | time =  0.40 (VAL)\n",
      "-----------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 96/96 [00:00<00:00, 259.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 14 | time =  0.40 (VAL)\n",
      "-----------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 96/96 [00:00<00:00, 275.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 15 | time =  0.37 (VAL)\n",
      "-----------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 96/96 [00:00<00:00, 209.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 16 | time =  0.48 (VAL)\n",
      "-----------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 96/96 [00:00<00:00, 286.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 17 | time =  0.35 (VAL)\n",
      "-----------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 96/96 [00:00<00:00, 210.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 18 | time =  0.47 (VAL)\n",
      "-----------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 96/96 [00:00<00:00, 283.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 19 | time =  0.35 (VAL)\n",
      "-----------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 96/96 [00:00<00:00, 211.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 20 | time =  0.47 (VAL)\n",
      "-----------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 96/96 [00:00<00:00, 247.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 21 | time =  0.41 (VAL)\n",
      "-----------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 96/96 [00:00<00:00, 205.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 22 | time =  0.49 (VAL)\n",
      "-----------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 96/96 [00:00<00:00, 159.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 23 | time =  0.61 (VAL)\n",
      "-----------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 96/96 [00:00<00:00, 166.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 24 | time =  0.59 (VAL)\n",
      "-----------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 96/96 [00:00<00:00, 261.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 25 | time =  0.39 (VAL)\n",
      "-----------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 96/96 [00:00<00:00, 248.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 26 | time =  0.40 (VAL)\n",
      "-----------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 96/96 [00:00<00:00, 292.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 27 | time =  0.35 (VAL)\n",
      "-----------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 96/96 [00:00<00:00, 188.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 28 | time =  0.52 (VAL)\n",
      "-----------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 96/96 [00:00<00:00, 169.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 29 | time =  0.59 (VAL)\n",
      "-----------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 96/96 [00:00<00:00, 159.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 30 | time =  0.62 (VAL)\n",
      "-----------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 96/96 [00:00<00:00, 291.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 31 | time =  0.34 (VAL)\n",
      "-----------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 96/96 [00:00<00:00, 181.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 32 | time =  0.54 (VAL)\n",
      "-----------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 96/96 [00:00<00:00, 140.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 33 | time =  0.71 (VAL)\n",
      "-----------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 96/96 [00:00<00:00, 159.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 34 | time =  0.61 (VAL)\n",
      "-----------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 96/96 [00:00<00:00, 256.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 35 | time =  0.39 (VAL)\n",
      "-----------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 96/96 [00:00<00:00, 246.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 36 | time =  0.41 (VAL)\n",
      "-----------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 96/96 [00:00<00:00, 278.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 37 | time =  0.36 (VAL)\n",
      "-----------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 96/96 [00:00<00:00, 189.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 38 | time =  0.53 (VAL)\n",
      "-----------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 96/96 [00:00<00:00, 290.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 39 | time =  0.35 (VAL)\n",
      "-----------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 96/96 [00:00<00:00, 214.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 40 | time =  0.46 (VAL)\n",
      "-----------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 96/96 [00:00<00:00, 165.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 41 | time =  0.61 (VAL)\n",
      "-----------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 96/96 [00:00<00:00, 156.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 42 | time =  0.63 (VAL)\n",
      "-----------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 96/96 [00:00<00:00, 145.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 43 | time =  0.68 (VAL)\n",
      "-----------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 96/96 [00:00<00:00, 266.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 44 | time =  0.38 (VAL)\n",
      "-----------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 96/96 [00:00<00:00, 215.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 45 | time =  0.45 (VAL)\n",
      "-----------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 96/96 [00:00<00:00, 152.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 46 | time =  0.65 (VAL)\n",
      "-----------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 96/96 [00:00<00:00, 148.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 47 | time =  0.66 (VAL)\n",
      "-----------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 96/96 [00:00<00:00, 153.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 48 | time =  0.64 (VAL)\n",
      "-----------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 96/96 [00:00<00:00, 136.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 49 | time =  0.73 (VAL)\n",
      "-----------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 96/96 [00:00<00:00, 163.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 50 | time =  0.61 (VAL)\n",
      "-----------------------------------------------------------------------------------------\n",
      "\n",
      "leave_2 split, SVP: MF_dot_forgetting_events, sequential loss\n",
      "SAMPLE SIZE: 0.0% users ; 0.0% items ; 20.0% train interactions ; 0.0% test interactions removed\n",
      "SAMPLE SIZE: 0.0% users ; 0.0% items ; 40.0% train interactions ; 0.0% test interactions removed\n",
      "SAMPLE SIZE: 0.0% users ; 0.0% items ; 60.0% train interactions ; 0.0% test interactions removed\n",
      "SAMPLE SIZE: 0.0% users ; 0.12% items ; 80.0% train interactions ; 0.0% test interactions removed\n",
      "SAMPLE SIZE: 0.0% users ; 1.01% items ; 90.0% train interactions ; 0.0% test interactions removed\n",
      "SAMPLE SIZE: 0.0% users ; 26.4% items ; 99.0% train interactions ; 0.0% test interactions removed\n",
      "\n",
      "leave_2 split, SVP: MF_dot_forgetting_events_propensity, sequential loss\n",
      "SAMPLE SIZE: 0.0% users ; 0.0% items ; 20.0% train interactions ; 0.0% test interactions removed\n",
      "SAMPLE SIZE: 0.0% users ; 0.0% items ; 40.0% train interactions ; 0.0% test interactions removed\n",
      "SAMPLE SIZE: 0.0% users ; 0.0% items ; 60.0% train interactions ; 0.0% test interactions removed\n",
      "SAMPLE SIZE: 0.0% users ; 0.18% items ; 80.0% train interactions ; 0.0% test interactions removed\n",
      "SAMPLE SIZE: 0.0% users ; 2.62% items ; 90.0% train interactions ; 0.0% test interactions removed\n",
      "SAMPLE SIZE: 0.0% users ; 27.23% items ; 99.0% train interactions ; 0.0% test interactions removed\n",
      "\n",
      "leave_2 split, SVP: MF_dot_forgetting_events_user, sequential loss\n",
      "SAMPLE SIZE: 0.0% users ; 0.06% items ; 20.05% train interactions ; 0.0% test interactions removed\n",
      "SAMPLE SIZE: 0.0% users ; 0.42% items ; 40.07% train interactions ; 0.0% test interactions removed\n",
      "SAMPLE SIZE: 0.0% users ; 1.13% items ; 60.01% train interactions ; 0.0% test interactions removed\n",
      "SAMPLE SIZE: 0.0% users ; 3.69% items ; 80.06% train interactions ; 0.0% test interactions removed\n",
      "SAMPLE SIZE: 0.0% users ; 4.76% items ; 90.01% train interactions ; 0.0% test interactions removed\n",
      "SAMPLE SIZE: 0.0% users ; 40.37% items ; 99.23% train interactions ; 0.0% test interactions removed\n",
      "\n",
      "leave_2 split, SVP: MF_dot_forgetting_events_user_propensity, sequential loss\n",
      "SAMPLE SIZE: 0.0% users ; 0.3% items ; 20.22% train interactions ; 0.0% test interactions removed\n",
      "SAMPLE SIZE: 0.0% users ; 1.01% items ; 40.1% train interactions ; 0.0% test interactions removed\n",
      "SAMPLE SIZE: 0.0% users ; 2.26% items ; 60.05% train interactions ; 0.0% test interactions removed\n",
      "SAMPLE SIZE: 0.0% users ; 7.73% items ; 80.02% train interactions ; 0.0% test interactions removed\n",
      "SAMPLE SIZE: 0.0% users ; 14.98% items ; 90.01% train interactions ; 0.0% test interactions removed\n",
      "SAMPLE SIZE: 0.0% users ; 42.98% items ; 99.01% train interactions ; 0.0% test interactions removed\n"
     ]
    }
   ],
   "source": [
    "# from initial_data_prep_code import movielens, amazon, goodreads, beeradvocate\n",
    "# from data_path_constants import get_data_path\n",
    "# from svp_handler import SVPHandler\n",
    "\n",
    "percent_sample = [ 20, 40, 60, 80, 90, 99 ]\n",
    "\n",
    "# Which datasets to prep?\n",
    "for dataset in [\n",
    "\t'ml-100k',\n",
    "    # 'magazine',\n",
    "\t# 'luxury',\n",
    "\t# 'video_games',\n",
    "\t# 'beeradvocate',\n",
    "\t# 'goodreads_comics',\n",
    "]:\n",
    "\n",
    "\tprint(\"\\n\\n\\n!!!!!!!! STARTED PROCESSING {} !!!!!!!!\\n\\n\\n\".format(dataset))\n",
    "\n",
    "\t# if dataset in [ 'ml-100k' ]: total_data = movielens.prep(dataset)\n",
    "\tif dataset in [ 'ml-100k' ]: total_data = prep(dataset)\n",
    "\telif dataset in [ 'luxury', 'magazine', 'video_games' ]: total_data = amazon.prep(dataset)\n",
    "\telif dataset in [ 'goodreads_comics' ]: total_data = goodreads.prep(dataset)\n",
    "\telif dataset in [ 'beeradvocate' ]: total_data = beeradvocate.prep(dataset)\n",
    "\n",
    "\t# Store original data\n",
    "\ttotal_data.save_data(get_data_path(dataset))\n",
    "\n",
    "\t# Sampling\n",
    "\t# for train_test_split in [ '20_percent_hist', 'leave_2' ]:\n",
    "\tfor train_test_split in ['leave_2']:\n",
    "\n",
    "\t\ttotal_data.complete_data_stats = None # Since task changed\n",
    "\t\tpath_uptil_now = get_data_path(dataset) + \"/\" + train_test_split + \"/\"\n",
    "\n",
    "\t\t# Make full-data (No sampling)\n",
    "\t\ttotal_data.train_test_split(train_test_split)\n",
    "\t\tprint(\"\\n{} split, Overall:\".format(train_test_split))\n",
    "\t\ttotal_data.save_index(path_uptil_now + \"/complete_data/\")\n",
    "\n",
    "\t\t# Frequency sample from user hist (Stratified)\n",
    "\t\tprint(\"\\n{} split, user history random sampling\".format(train_test_split))\n",
    "\t\tfor percent in percent_sample:\n",
    "\t\t\ttotal_data.load_index(path_uptil_now + \"/complete_data/\") # Re-load index map\n",
    "\t\t\ttotal_data.frequency_sample(percent, 0)\n",
    "\t\t\ttotal_data.save_index(path_uptil_now + str(percent) + \"_perc_freq_user_rns\")\n",
    "\n",
    "\t\t# Sample users randomly\n",
    "\t\tprint(\"\\n{} split, user random sampling\".format(train_test_split))\n",
    "\t\tfor percent in percent_sample:\n",
    "\t\t\ttotal_data.load_index(path_uptil_now + \"/complete_data/\") # Re-load index map\n",
    "\t\t\ttotal_data.user_random_sample(percent)\n",
    "\t\t\ttotal_data.save_index(path_uptil_now + str(percent) + \"_perc_user_rns\")\n",
    "\n",
    "\t\t# Sample interactions randomly\n",
    "\t\tprint(\"\\n{} split, interaction random sampling\".format(train_test_split))\n",
    "\t\tfor percent in percent_sample:\n",
    "\t\t\ttotal_data.load_index(path_uptil_now + \"/complete_data/\") # Re-load index map\n",
    "\t\t\ttotal_data.interaction_random_sample(percent)\n",
    "\t\t\ttotal_data.save_index(path_uptil_now + str(percent) + \"_perc_interaction_rns\")\n",
    "\n",
    "\t\t# Temporal sampling\n",
    "\t\tprint(\"\\n{} split, user history temporal sampling\".format(train_test_split))\n",
    "\t\tfor percent in percent_sample:\n",
    "\t\t\ttotal_data.load_index(path_uptil_now + \"/complete_data/\") # Re-load index map\n",
    "\t\t\ttotal_data.temporal_sample(percent)\n",
    "\t\t\ttotal_data.save_index(path_uptil_now + str(percent) + \"_perc_temporal\")\n",
    "\n",
    "\t\t# Remove tail users sampling\n",
    "\t\tprint(\"\\n{} split, tail user sampling\".format(train_test_split))\n",
    "\t\tfor percent in percent_sample:\n",
    "\t\t\ttotal_data.load_index(path_uptil_now + \"/complete_data/\") # Re-load index map\n",
    "\t\t\ttotal_data.tail_user_remove(percent)\n",
    "\t\t\ttotal_data.save_index(path_uptil_now + str(percent) + \"_perc_tail_user_remove\")\n",
    "\n",
    "\t\t# Pagerank based sampling\n",
    "\t\tprint(\"\\n{} split, pagerank sampling\".format(train_test_split))\n",
    "\t\tfor percent in percent_sample:\n",
    "\t\t\ttotal_data.load_index(path_uptil_now + \"/complete_data/\") # Re-load index map\n",
    "\t\t\ttotal_data.pagerank_sample(percent)\n",
    "\t\t\ttotal_data.save_index(path_uptil_now + str(percent) + \"_perc_pagerank\")\n",
    "\n",
    "\t\t# RW based sampling\n",
    "\t\tprint(\"\\n{} split, random walk sampling\".format(train_test_split))\n",
    "\t\tfor percent in percent_sample:\n",
    "\t\t\ttotal_data.load_index(path_uptil_now + \"/complete_data/\") # Re-load index map\n",
    "\t\t\ttotal_data.random_walk_sample(percent)\n",
    "\t\t\ttotal_data.save_index(path_uptil_now + str(percent) + \"_perc_random_walk\")\n",
    "\n",
    "\t\t# Forest-fire based sampling\n",
    "\t\tprint(\"\\n{} split, forest fire sampling\".format(train_test_split))\n",
    "\t\tfor percent in percent_sample:\n",
    "\t\t\ttotal_data.load_index(path_uptil_now + \"/complete_data/\") # Re-load index map\n",
    "\t\t\ttotal_data.forest_fire_sample(percent)\n",
    "\t\t\ttotal_data.save_index(path_uptil_now + str(percent) + \"_perc_forest_fire\")\n",
    "\n",
    "\t\t# Sample interactions according to SVP\n",
    "\t\thyper_params = {}\n",
    "\t\thyper_params['dataset'] = dataset\n",
    "\t\thyper_params['sampling'] = 'complete_data' # While training the proxy model\n",
    "\n",
    "\t\tfor proxy_model in [ 'bias_only', 'MF_dot' ]:\n",
    "\t\t\tscenarios = [ 'sequential' ] if train_test_split == 'leave_2' else [ 'implicit', 'explicit' ]\n",
    "\n",
    "\t\t\tfor loss_type in scenarios:\n",
    "\t\t\t\tprint() ; svp_handler = SVPHandler(proxy_model, loss_type, hyper_params)\n",
    "\n",
    "\t\t\t\tfor sampling in [ \n",
    "\t\t\t\t\t'forgetting_events', \n",
    "\t\t\t\t\t'forgetting_events_propensity',\n",
    "\t\t\t\t\t'forgetting_events_user', \n",
    "\t\t\t\t\t'forgetting_events_user_propensity',\n",
    "\t\t\t\t]:\n",
    "\t\t\t\t\tprint(\"\\n{} split, SVP: {}_{}, {} loss\".format(train_test_split, proxy_model, sampling, loss_type))\n",
    "\t\t\t\t\tfor percent in percent_sample:\n",
    "\t\t\t\t\t\ttotal_data.load_index(path_uptil_now + \"/complete_data/\") # Re-load index map\n",
    "\t\t\t\t\t\ttotal_data.svp_sample(percent, svp_handler, sampling)\n",
    "\t\t\t\t\t\ttotal_data.save_index(path_uptil_now + \"svp_{}_{}/{}_perc_{}\".format(proxy_model, loss_type, percent, sampling))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[01;34m./datasets/ml-100k\u001b[00m\n",
      "├── [5.6M]  \u001b[01;34m20_percent_hist\u001b[00m\n",
      "│   ├── [ 42K]  \u001b[01;34m20_perc_forest_fire\u001b[00m\n",
      "│   │   ├── [ 106]  data_stats.json\n",
      "│   │   └── [ 38K]  index.npz\n",
      "│   ├── [ 47K]  \u001b[01;34m20_perc_freq_user_rns\u001b[00m\n",
      "│   │   ├── [ 106]  data_stats.json\n",
      "│   │   └── [ 43K]  index.npz\n",
      "│   ├── [ 47K]  \u001b[01;34m20_perc_interaction_rns\u001b[00m\n",
      "│   │   ├── [ 106]  data_stats.json\n",
      "│   │   └── [ 42K]  index.npz\n",
      "│   ├── [ 44K]  \u001b[01;34m20_perc_pagerank\u001b[00m\n",
      "│   │   ├── [ 106]  data_stats.json\n",
      "│   │   └── [ 39K]  index.npz\n",
      "│   ├── [ 44K]  \u001b[01;34m20_perc_random_walk\u001b[00m\n",
      "│   │   ├── [ 106]  data_stats.json\n",
      "│   │   └── [ 40K]  index.npz\n",
      "│   ├── [ 33K]  \u001b[01;34m20_perc_tail_user_remove\u001b[00m\n",
      "│   │   ├── [ 106]  data_stats.json\n",
      "│   │   └── [ 29K]  index.npz\n",
      "│   ├── [ 35K]  \u001b[01;34m20_perc_temporal\u001b[00m\n",
      "│   │   ├── [ 106]  data_stats.json\n",
      "│   │   └── [ 31K]  index.npz\n",
      "│   ├── [ 33K]  \u001b[01;34m20_perc_user_rns\u001b[00m\n",
      "│   │   ├── [ 106]  data_stats.json\n",
      "│   │   └── [ 29K]  index.npz\n",
      "│   ├── [ 51K]  \u001b[01;34m40_perc_forest_fire\u001b[00m\n",
      "│   │   ├── [ 106]  data_stats.json\n",
      "│   │   └── [ 47K]  index.npz\n",
      "│   ├── [ 52K]  \u001b[01;34m40_perc_freq_user_rns\u001b[00m\n",
      "│   │   ├── [ 106]  data_stats.json\n",
      "│   │   └── [ 48K]  index.npz\n",
      "│   ├── [ 52K]  \u001b[01;34m40_perc_interaction_rns\u001b[00m\n",
      "│   │   ├── [ 106]  data_stats.json\n",
      "│   │   └── [ 48K]  index.npz\n",
      "│   ├── [ 48K]  \u001b[01;34m40_perc_pagerank\u001b[00m\n",
      "│   │   ├── [ 106]  data_stats.json\n",
      "│   │   └── [ 43K]  index.npz\n",
      "│   ├── [ 49K]  \u001b[01;34m40_perc_random_walk\u001b[00m\n",
      "│   │   ├── [ 106]  data_stats.json\n",
      "│   │   └── [ 45K]  index.npz\n",
      "│   ├── [ 34K]  \u001b[01;34m40_perc_tail_user_remove\u001b[00m\n",
      "│   │   ├── [ 106]  data_stats.json\n",
      "│   │   └── [ 30K]  index.npz\n",
      "│   ├── [ 36K]  \u001b[01;34m40_perc_temporal\u001b[00m\n",
      "│   │   ├── [ 106]  data_stats.json\n",
      "│   │   └── [ 32K]  index.npz\n",
      "│   ├── [ 34K]  \u001b[01;34m40_perc_user_rns\u001b[00m\n",
      "│   │   ├── [ 106]  data_stats.json\n",
      "│   │   └── [ 30K]  index.npz\n",
      "│   ├── [ 47K]  \u001b[01;34m60_perc_forest_fire\u001b[00m\n",
      "│   │   ├── [ 106]  data_stats.json\n",
      "│   │   └── [ 43K]  index.npz\n",
      "│   ├── [ 52K]  \u001b[01;34m60_perc_freq_user_rns\u001b[00m\n",
      "│   │   ├── [ 106]  data_stats.json\n",
      "│   │   └── [ 48K]  index.npz\n",
      "│   ├── [ 52K]  \u001b[01;34m60_perc_interaction_rns\u001b[00m\n",
      "│   │   ├── [ 106]  data_stats.json\n",
      "│   │   └── [ 48K]  index.npz\n",
      "│   ├── [ 48K]  \u001b[01;34m60_perc_pagerank\u001b[00m\n",
      "│   │   ├── [ 106]  data_stats.json\n",
      "│   │   └── [ 44K]  index.npz\n",
      "│   ├── [ 49K]  \u001b[01;34m60_perc_random_walk\u001b[00m\n",
      "│   │   ├── [ 106]  data_stats.json\n",
      "│   │   └── [ 45K]  index.npz\n",
      "│   ├── [ 33K]  \u001b[01;34m60_perc_tail_user_remove\u001b[00m\n",
      "│   │   ├── [ 106]  data_stats.json\n",
      "│   │   └── [ 29K]  index.npz\n",
      "│   ├── [ 36K]  \u001b[01;34m60_perc_temporal\u001b[00m\n",
      "│   │   ├── [ 106]  data_stats.json\n",
      "│   │   └── [ 32K]  index.npz\n",
      "│   ├── [ 34K]  \u001b[01;34m60_perc_user_rns\u001b[00m\n",
      "│   │   ├── [ 106]  data_stats.json\n",
      "│   │   └── [ 30K]  index.npz\n",
      "│   ├── [ 45K]  \u001b[01;34m80_perc_forest_fire\u001b[00m\n",
      "│   │   ├── [ 106]  data_stats.json\n",
      "│   │   └── [ 41K]  index.npz\n",
      "│   ├── [ 47K]  \u001b[01;34m80_perc_freq_user_rns\u001b[00m\n",
      "│   │   ├── [ 106]  data_stats.json\n",
      "│   │   └── [ 43K]  index.npz\n",
      "│   ├── [ 47K]  \u001b[01;34m80_perc_interaction_rns\u001b[00m\n",
      "│   │   ├── [ 106]  data_stats.json\n",
      "│   │   └── [ 43K]  index.npz\n",
      "│   ├── [ 44K]  \u001b[01;34m80_perc_pagerank\u001b[00m\n",
      "│   │   ├── [ 106]  data_stats.json\n",
      "│   │   └── [ 40K]  index.npz\n",
      "│   ├── [ 44K]  \u001b[01;34m80_perc_random_walk\u001b[00m\n",
      "│   │   ├── [ 106]  data_stats.json\n",
      "│   │   └── [ 40K]  index.npz\n",
      "│   ├── [ 33K]  \u001b[01;34m80_perc_tail_user_remove\u001b[00m\n",
      "│   │   ├── [ 106]  data_stats.json\n",
      "│   │   └── [ 29K]  index.npz\n",
      "│   ├── [ 36K]  \u001b[01;34m80_perc_temporal\u001b[00m\n",
      "│   │   ├── [ 106]  data_stats.json\n",
      "│   │   └── [ 32K]  index.npz\n",
      "│   ├── [ 33K]  \u001b[01;34m80_perc_user_rns\u001b[00m\n",
      "│   │   ├── [ 106]  data_stats.json\n",
      "│   │   └── [ 29K]  index.npz\n",
      "│   ├── [ 40K]  \u001b[01;34m90_perc_forest_fire\u001b[00m\n",
      "│   │   ├── [ 105]  data_stats.json\n",
      "│   │   └── [ 36K]  index.npz\n",
      "│   ├── [ 42K]  \u001b[01;34m90_perc_freq_user_rns\u001b[00m\n",
      "│   │   ├── [ 105]  data_stats.json\n",
      "│   │   └── [ 38K]  index.npz\n",
      "│   ├── [ 42K]  \u001b[01;34m90_perc_interaction_rns\u001b[00m\n",
      "│   │   ├── [ 105]  data_stats.json\n",
      "│   │   └── [ 38K]  index.npz\n",
      "│   ├── [ 40K]  \u001b[01;34m90_perc_pagerank\u001b[00m\n",
      "│   │   ├── [ 105]  data_stats.json\n",
      "│   │   └── [ 36K]  index.npz\n",
      "│   ├── [ 41K]  \u001b[01;34m90_perc_random_walk\u001b[00m\n",
      "│   │   ├── [ 105]  data_stats.json\n",
      "│   │   └── [ 36K]  index.npz\n",
      "│   ├── [ 32K]  \u001b[01;34m90_perc_tail_user_remove\u001b[00m\n",
      "│   │   ├── [ 105]  data_stats.json\n",
      "│   │   └── [ 28K]  index.npz\n",
      "│   ├── [ 35K]  \u001b[01;34m90_perc_temporal\u001b[00m\n",
      "│   │   ├── [ 105]  data_stats.json\n",
      "│   │   └── [ 31K]  index.npz\n",
      "│   ├── [ 33K]  \u001b[01;34m90_perc_user_rns\u001b[00m\n",
      "│   │   ├── [ 105]  data_stats.json\n",
      "│   │   └── [ 28K]  index.npz\n",
      "│   ├── [ 33K]  \u001b[01;34m99_perc_forest_fire\u001b[00m\n",
      "│   │   ├── [ 104]  data_stats.json\n",
      "│   │   └── [ 29K]  index.npz\n",
      "│   ├── [ 32K]  \u001b[01;34m99_perc_freq_user_rns\u001b[00m\n",
      "│   │   ├── [ 104]  data_stats.json\n",
      "│   │   └── [ 28K]  index.npz\n",
      "│   ├── [ 33K]  \u001b[01;34m99_perc_interaction_rns\u001b[00m\n",
      "│   │   ├── [ 104]  data_stats.json\n",
      "│   │   └── [ 29K]  index.npz\n",
      "│   ├── [ 33K]  \u001b[01;34m99_perc_pagerank\u001b[00m\n",
      "│   │   ├── [ 104]  data_stats.json\n",
      "│   │   └── [ 29K]  index.npz\n",
      "│   ├── [ 33K]  \u001b[01;34m99_perc_random_walk\u001b[00m\n",
      "│   │   ├── [ 104]  data_stats.json\n",
      "│   │   └── [ 29K]  index.npz\n",
      "│   ├── [ 31K]  \u001b[01;34m99_perc_tail_user_remove\u001b[00m\n",
      "│   │   ├── [ 104]  data_stats.json\n",
      "│   │   └── [ 27K]  index.npz\n",
      "│   ├── [ 32K]  \u001b[01;34m99_perc_temporal\u001b[00m\n",
      "│   │   ├── [ 104]  data_stats.json\n",
      "│   │   └── [ 28K]  index.npz\n",
      "│   ├── [ 31K]  \u001b[01;34m99_perc_user_rns\u001b[00m\n",
      "│   │   ├── [ 104]  data_stats.json\n",
      "│   │   └── [ 27K]  index.npz\n",
      "│   ├── [ 31K]  \u001b[01;34mcomplete_data\u001b[00m\n",
      "│   │   ├── [ 106]  data_stats.json\n",
      "│   │   └── [ 27K]  index.npz\n",
      "│   ├── [940K]  \u001b[01;34msvp_bias_only_explicit\u001b[00m\n",
      "│   │   ├── [ 46K]  \u001b[01;34m20_perc_forgetting_events\u001b[00m\n",
      "│   │   │   ├── [ 106]  data_stats.json\n",
      "│   │   │   └── [ 42K]  index.npz\n",
      "│   │   ├── [ 46K]  \u001b[01;34m20_perc_forgetting_events_propensity\u001b[00m\n",
      "│   │   │   ├── [ 106]  data_stats.json\n",
      "│   │   │   └── [ 42K]  index.npz\n",
      "│   │   ├── [ 33K]  \u001b[01;34m20_perc_forgetting_events_user\u001b[00m\n",
      "│   │   │   ├── [ 106]  data_stats.json\n",
      "│   │   │   └── [ 29K]  index.npz\n",
      "│   │   ├── [ 33K]  \u001b[01;34m20_perc_forgetting_events_user_propensity\u001b[00m\n",
      "│   │   │   ├── [ 106]  data_stats.json\n",
      "│   │   │   └── [ 29K]  index.npz\n",
      "│   │   ├── [ 51K]  \u001b[01;34m40_perc_forgetting_events\u001b[00m\n",
      "│   │   │   ├── [ 106]  data_stats.json\n",
      "│   │   │   └── [ 47K]  index.npz\n",
      "│   │   ├── [ 51K]  \u001b[01;34m40_perc_forgetting_events_propensity\u001b[00m\n",
      "│   │   │   ├── [ 106]  data_stats.json\n",
      "│   │   │   └── [ 47K]  index.npz\n",
      "│   │   ├── [ 34K]  \u001b[01;34m40_perc_forgetting_events_user\u001b[00m\n",
      "│   │   │   ├── [ 106]  data_stats.json\n",
      "│   │   │   └── [ 30K]  index.npz\n",
      "│   │   ├── [ 34K]  \u001b[01;34m40_perc_forgetting_events_user_propensity\u001b[00m\n",
      "│   │   │   ├── [ 106]  data_stats.json\n",
      "│   │   │   └── [ 30K]  index.npz\n",
      "│   │   ├── [ 51K]  \u001b[01;34m60_perc_forgetting_events\u001b[00m\n",
      "│   │   │   ├── [ 106]  data_stats.json\n",
      "│   │   │   └── [ 47K]  index.npz\n",
      "│   │   ├── [ 51K]  \u001b[01;34m60_perc_forgetting_events_propensity\u001b[00m\n",
      "│   │   │   ├── [ 106]  data_stats.json\n",
      "│   │   │   └── [ 47K]  index.npz\n",
      "│   │   ├── [ 34K]  \u001b[01;34m60_perc_forgetting_events_user\u001b[00m\n",
      "│   │   │   ├── [ 106]  data_stats.json\n",
      "│   │   │   └── [ 30K]  index.npz\n",
      "│   │   ├── [ 34K]  \u001b[01;34m60_perc_forgetting_events_user_propensity\u001b[00m\n",
      "│   │   │   ├── [ 106]  data_stats.json\n",
      "│   │   │   └── [ 30K]  index.npz\n",
      "│   │   ├── [ 47K]  \u001b[01;34m80_perc_forgetting_events\u001b[00m\n",
      "│   │   │   ├── [ 106]  data_stats.json\n",
      "│   │   │   └── [ 43K]  index.npz\n",
      "│   │   ├── [ 47K]  \u001b[01;34m80_perc_forgetting_events_propensity\u001b[00m\n",
      "│   │   │   ├── [ 106]  data_stats.json\n",
      "│   │   │   └── [ 42K]  index.npz\n",
      "│   │   ├── [ 33K]  \u001b[01;34m80_perc_forgetting_events_user\u001b[00m\n",
      "│   │   │   ├── [ 106]  data_stats.json\n",
      "│   │   │   └── [ 29K]  index.npz\n",
      "│   │   ├── [ 34K]  \u001b[01;34m80_perc_forgetting_events_user_propensity\u001b[00m\n",
      "│   │   │   ├── [ 106]  data_stats.json\n",
      "│   │   │   └── [ 29K]  index.npz\n",
      "│   │   ├── [ 41K]  \u001b[01;34m90_perc_forgetting_events\u001b[00m\n",
      "│   │   │   ├── [ 105]  data_stats.json\n",
      "│   │   │   └── [ 37K]  index.npz\n",
      "│   │   ├── [ 41K]  \u001b[01;34m90_perc_forgetting_events_propensity\u001b[00m\n",
      "│   │   │   ├── [ 105]  data_stats.json\n",
      "│   │   │   └── [ 37K]  index.npz\n",
      "│   │   ├── [ 33K]  \u001b[01;34m90_perc_forgetting_events_user\u001b[00m\n",
      "│   │   │   ├── [ 105]  data_stats.json\n",
      "│   │   │   └── [ 28K]  index.npz\n",
      "│   │   ├── [ 33K]  \u001b[01;34m90_perc_forgetting_events_user_propensity\u001b[00m\n",
      "│   │   │   ├── [ 105]  data_stats.json\n",
      "│   │   │   └── [ 29K]  index.npz\n",
      "│   │   ├── [ 33K]  \u001b[01;34m99_perc_forgetting_events\u001b[00m\n",
      "│   │   │   ├── [ 104]  data_stats.json\n",
      "│   │   │   └── [ 29K]  index.npz\n",
      "│   │   ├── [ 33K]  \u001b[01;34m99_perc_forgetting_events_propensity\u001b[00m\n",
      "│   │   │   ├── [ 104]  data_stats.json\n",
      "│   │   │   └── [ 29K]  index.npz\n",
      "│   │   ├── [ 31K]  \u001b[01;34m99_perc_forgetting_events_user\u001b[00m\n",
      "│   │   │   ├── [ 104]  data_stats.json\n",
      "│   │   │   └── [ 27K]  index.npz\n",
      "│   │   └── [ 32K]  \u001b[01;34m99_perc_forgetting_events_user_propensity\u001b[00m\n",
      "│   │       ├── [ 104]  data_stats.json\n",
      "│   │       └── [ 27K]  index.npz\n",
      "│   ├── [937K]  \u001b[01;34msvp_bias_only_implicit\u001b[00m\n",
      "│   │   ├── [ 46K]  \u001b[01;34m20_perc_forgetting_events\u001b[00m\n",
      "│   │   │   ├── [ 106]  data_stats.json\n",
      "│   │   │   └── [ 42K]  index.npz\n",
      "│   │   ├── [ 46K]  \u001b[01;34m20_perc_forgetting_events_propensity\u001b[00m\n",
      "│   │   │   ├── [ 106]  data_stats.json\n",
      "│   │   │   └── [ 42K]  index.npz\n",
      "│   │   ├── [ 33K]  \u001b[01;34m20_perc_forgetting_events_user\u001b[00m\n",
      "│   │   │   ├── [ 106]  data_stats.json\n",
      "│   │   │   └── [ 29K]  index.npz\n",
      "│   │   ├── [ 33K]  \u001b[01;34m20_perc_forgetting_events_user_propensity\u001b[00m\n",
      "│   │   │   ├── [ 106]  data_stats.json\n",
      "│   │   │   └── [ 29K]  index.npz\n",
      "│   │   ├── [ 51K]  \u001b[01;34m40_perc_forgetting_events\u001b[00m\n",
      "│   │   │   ├── [ 106]  data_stats.json\n",
      "│   │   │   └── [ 47K]  index.npz\n",
      "│   │   ├── [ 51K]  \u001b[01;34m40_perc_forgetting_events_propensity\u001b[00m\n",
      "│   │   │   ├── [ 106]  data_stats.json\n",
      "│   │   │   └── [ 47K]  index.npz\n",
      "│   │   ├── [ 34K]  \u001b[01;34m40_perc_forgetting_events_user\u001b[00m\n",
      "│   │   │   ├── [ 106]  data_stats.json\n",
      "│   │   │   └── [ 30K]  index.npz\n",
      "│   │   ├── [ 34K]  \u001b[01;34m40_perc_forgetting_events_user_propensity\u001b[00m\n",
      "│   │   │   ├── [ 106]  data_stats.json\n",
      "│   │   │   └── [ 30K]  index.npz\n",
      "│   │   ├── [ 51K]  \u001b[01;34m60_perc_forgetting_events\u001b[00m\n",
      "│   │   │   ├── [ 106]  data_stats.json\n",
      "│   │   │   └── [ 47K]  index.npz\n",
      "│   │   ├── [ 51K]  \u001b[01;34m60_perc_forgetting_events_propensity\u001b[00m\n",
      "│   │   │   ├── [ 106]  data_stats.json\n",
      "│   │   │   └── [ 46K]  index.npz\n",
      "│   │   ├── [ 34K]  \u001b[01;34m60_perc_forgetting_events_user\u001b[00m\n",
      "│   │   │   ├── [ 106]  data_stats.json\n",
      "│   │   │   └── [ 30K]  index.npz\n",
      "│   │   ├── [ 34K]  \u001b[01;34m60_perc_forgetting_events_user_propensity\u001b[00m\n",
      "│   │   │   ├── [ 106]  data_stats.json\n",
      "│   │   │   └── [ 30K]  index.npz\n",
      "│   │   ├── [ 46K]  \u001b[01;34m80_perc_forgetting_events\u001b[00m\n",
      "│   │   │   ├── [ 106]  data_stats.json\n",
      "│   │   │   └── [ 42K]  index.npz\n",
      "│   │   ├── [ 46K]  \u001b[01;34m80_perc_forgetting_events_propensity\u001b[00m\n",
      "│   │   │   ├── [ 106]  data_stats.json\n",
      "│   │   │   └── [ 42K]  index.npz\n",
      "│   │   ├── [ 33K]  \u001b[01;34m80_perc_forgetting_events_user\u001b[00m\n",
      "│   │   │   ├── [ 106]  data_stats.json\n",
      "│   │   │   └── [ 29K]  index.npz\n",
      "│   │   ├── [ 34K]  \u001b[01;34m80_perc_forgetting_events_user_propensity\u001b[00m\n",
      "│   │   │   ├── [ 106]  data_stats.json\n",
      "│   │   │   └── [ 30K]  index.npz\n",
      "│   │   ├── [ 41K]  \u001b[01;34m90_perc_forgetting_events\u001b[00m\n",
      "│   │   │   ├── [ 105]  data_stats.json\n",
      "│   │   │   └── [ 37K]  index.npz\n",
      "│   │   ├── [ 41K]  \u001b[01;34m90_perc_forgetting_events_propensity\u001b[00m\n",
      "│   │   │   ├── [ 105]  data_stats.json\n",
      "│   │   │   └── [ 37K]  index.npz\n",
      "│   │   ├── [ 33K]  \u001b[01;34m90_perc_forgetting_events_user\u001b[00m\n",
      "│   │   │   ├── [ 105]  data_stats.json\n",
      "│   │   │   └── [ 29K]  index.npz\n",
      "│   │   ├── [ 33K]  \u001b[01;34m90_perc_forgetting_events_user_propensity\u001b[00m\n",
      "│   │   │   ├── [ 105]  data_stats.json\n",
      "│   │   │   └── [ 29K]  index.npz\n",
      "│   │   ├── [ 33K]  \u001b[01;34m99_perc_forgetting_events\u001b[00m\n",
      "│   │   │   ├── [ 104]  data_stats.json\n",
      "│   │   │   └── [ 29K]  index.npz\n",
      "│   │   ├── [ 33K]  \u001b[01;34m99_perc_forgetting_events_propensity\u001b[00m\n",
      "│   │   │   ├── [ 104]  data_stats.json\n",
      "│   │   │   └── [ 29K]  index.npz\n",
      "│   │   ├── [ 31K]  \u001b[01;34m99_perc_forgetting_events_user\u001b[00m\n",
      "│   │   │   ├── [ 104]  data_stats.json\n",
      "│   │   │   └── [ 27K]  index.npz\n",
      "│   │   └── [ 32K]  \u001b[01;34m99_perc_forgetting_events_user_propensity\u001b[00m\n",
      "│   │       ├── [ 104]  data_stats.json\n",
      "│   │       └── [ 27K]  index.npz\n",
      "│   ├── [940K]  \u001b[01;34msvp_MF_dot_explicit\u001b[00m\n",
      "│   │   ├── [ 46K]  \u001b[01;34m20_perc_forgetting_events\u001b[00m\n",
      "│   │   │   ├── [ 106]  data_stats.json\n",
      "│   │   │   └── [ 42K]  index.npz\n",
      "│   │   ├── [ 46K]  \u001b[01;34m20_perc_forgetting_events_propensity\u001b[00m\n",
      "│   │   │   ├── [ 106]  data_stats.json\n",
      "│   │   │   └── [ 42K]  index.npz\n",
      "│   │   ├── [ 33K]  \u001b[01;34m20_perc_forgetting_events_user\u001b[00m\n",
      "│   │   │   ├── [ 106]  data_stats.json\n",
      "│   │   │   └── [ 29K]  index.npz\n",
      "│   │   ├── [ 33K]  \u001b[01;34m20_perc_forgetting_events_user_propensity\u001b[00m\n",
      "│   │   │   ├── [ 106]  data_stats.json\n",
      "│   │   │   └── [ 29K]  index.npz\n",
      "│   │   ├── [ 51K]  \u001b[01;34m40_perc_forgetting_events\u001b[00m\n",
      "│   │   │   ├── [ 106]  data_stats.json\n",
      "│   │   │   └── [ 47K]  index.npz\n",
      "│   │   ├── [ 51K]  \u001b[01;34m40_perc_forgetting_events_propensity\u001b[00m\n",
      "│   │   │   ├── [ 106]  data_stats.json\n",
      "│   │   │   └── [ 47K]  index.npz\n",
      "│   │   ├── [ 34K]  \u001b[01;34m40_perc_forgetting_events_user\u001b[00m\n",
      "│   │   │   ├── [ 106]  data_stats.json\n",
      "│   │   │   └── [ 30K]  index.npz\n",
      "│   │   ├── [ 34K]  \u001b[01;34m40_perc_forgetting_events_user_propensity\u001b[00m\n",
      "│   │   │   ├── [ 106]  data_stats.json\n",
      "│   │   │   └── [ 30K]  index.npz\n",
      "│   │   ├── [ 51K]  \u001b[01;34m60_perc_forgetting_events\u001b[00m\n",
      "│   │   │   ├── [ 106]  data_stats.json\n",
      "│   │   │   └── [ 47K]  index.npz\n",
      "│   │   ├── [ 51K]  \u001b[01;34m60_perc_forgetting_events_propensity\u001b[00m\n",
      "│   │   │   ├── [ 106]  data_stats.json\n",
      "│   │   │   └── [ 47K]  index.npz\n",
      "│   │   ├── [ 34K]  \u001b[01;34m60_perc_forgetting_events_user\u001b[00m\n",
      "│   │   │   ├── [ 106]  data_stats.json\n",
      "│   │   │   └── [ 30K]  index.npz\n",
      "│   │   ├── [ 34K]  \u001b[01;34m60_perc_forgetting_events_user_propensity\u001b[00m\n",
      "│   │   │   ├── [ 106]  data_stats.json\n",
      "│   │   │   └── [ 30K]  index.npz\n",
      "│   │   ├── [ 47K]  \u001b[01;34m80_perc_forgetting_events\u001b[00m\n",
      "│   │   │   ├── [ 106]  data_stats.json\n",
      "│   │   │   └── [ 43K]  index.npz\n",
      "│   │   ├── [ 47K]  \u001b[01;34m80_perc_forgetting_events_propensity\u001b[00m\n",
      "│   │   │   ├── [ 106]  data_stats.json\n",
      "│   │   │   └── [ 43K]  index.npz\n",
      "│   │   ├── [ 33K]  \u001b[01;34m80_perc_forgetting_events_user\u001b[00m\n",
      "│   │   │   ├── [ 106]  data_stats.json\n",
      "│   │   │   └── [ 29K]  index.npz\n",
      "│   │   ├── [ 34K]  \u001b[01;34m80_perc_forgetting_events_user_propensity\u001b[00m\n",
      "│   │   │   ├── [ 106]  data_stats.json\n",
      "│   │   │   └── [ 29K]  index.npz\n",
      "│   │   ├── [ 42K]  \u001b[01;34m90_perc_forgetting_events\u001b[00m\n",
      "│   │   │   ├── [ 105]  data_stats.json\n",
      "│   │   │   └── [ 38K]  index.npz\n",
      "│   │   ├── [ 41K]  \u001b[01;34m90_perc_forgetting_events_propensity\u001b[00m\n",
      "│   │   │   ├── [ 105]  data_stats.json\n",
      "│   │   │   └── [ 37K]  index.npz\n",
      "│   │   ├── [ 33K]  \u001b[01;34m90_perc_forgetting_events_user\u001b[00m\n",
      "│   │   │   ├── [ 105]  data_stats.json\n",
      "│   │   │   └── [ 28K]  index.npz\n",
      "│   │   ├── [ 33K]  \u001b[01;34m90_perc_forgetting_events_user_propensity\u001b[00m\n",
      "│   │   │   ├── [ 105]  data_stats.json\n",
      "│   │   │   └── [ 29K]  index.npz\n",
      "│   │   ├── [ 33K]  \u001b[01;34m99_perc_forgetting_events\u001b[00m\n",
      "│   │   │   ├── [ 104]  data_stats.json\n",
      "│   │   │   └── [ 29K]  index.npz\n",
      "│   │   ├── [ 33K]  \u001b[01;34m99_perc_forgetting_events_propensity\u001b[00m\n",
      "│   │   │   ├── [ 104]  data_stats.json\n",
      "│   │   │   └── [ 29K]  index.npz\n",
      "│   │   ├── [ 31K]  \u001b[01;34m99_perc_forgetting_events_user\u001b[00m\n",
      "│   │   │   ├── [ 104]  data_stats.json\n",
      "│   │   │   └── [ 27K]  index.npz\n",
      "│   │   └── [ 32K]  \u001b[01;34m99_perc_forgetting_events_user_propensity\u001b[00m\n",
      "│   │       ├── [ 104]  data_stats.json\n",
      "│   │       └── [ 27K]  index.npz\n",
      "│   └── [938K]  \u001b[01;34msvp_MF_dot_implicit\u001b[00m\n",
      "│       ├── [ 46K]  \u001b[01;34m20_perc_forgetting_events\u001b[00m\n",
      "│       │   ├── [ 106]  data_stats.json\n",
      "│       │   └── [ 42K]  index.npz\n",
      "│       ├── [ 46K]  \u001b[01;34m20_perc_forgetting_events_propensity\u001b[00m\n",
      "│       │   ├── [ 106]  data_stats.json\n",
      "│       │   └── [ 42K]  index.npz\n",
      "│       ├── [ 33K]  \u001b[01;34m20_perc_forgetting_events_user\u001b[00m\n",
      "│       │   ├── [ 106]  data_stats.json\n",
      "│       │   └── [ 29K]  index.npz\n",
      "│       ├── [ 33K]  \u001b[01;34m20_perc_forgetting_events_user_propensity\u001b[00m\n",
      "│       │   ├── [ 106]  data_stats.json\n",
      "│       │   └── [ 29K]  index.npz\n",
      "│       ├── [ 51K]  \u001b[01;34m40_perc_forgetting_events\u001b[00m\n",
      "│       │   ├── [ 106]  data_stats.json\n",
      "│       │   └── [ 47K]  index.npz\n",
      "│       ├── [ 51K]  \u001b[01;34m40_perc_forgetting_events_propensity\u001b[00m\n",
      "│       │   ├── [ 106]  data_stats.json\n",
      "│       │   └── [ 47K]  index.npz\n",
      "│       ├── [ 34K]  \u001b[01;34m40_perc_forgetting_events_user\u001b[00m\n",
      "│       │   ├── [ 106]  data_stats.json\n",
      "│       │   └── [ 30K]  index.npz\n",
      "│       ├── [ 34K]  \u001b[01;34m40_perc_forgetting_events_user_propensity\u001b[00m\n",
      "│       │   ├── [ 106]  data_stats.json\n",
      "│       │   └── [ 30K]  index.npz\n",
      "│       ├── [ 51K]  \u001b[01;34m60_perc_forgetting_events\u001b[00m\n",
      "│       │   ├── [ 106]  data_stats.json\n",
      "│       │   └── [ 47K]  index.npz\n",
      "│       ├── [ 51K]  \u001b[01;34m60_perc_forgetting_events_propensity\u001b[00m\n",
      "│       │   ├── [ 106]  data_stats.json\n",
      "│       │   └── [ 47K]  index.npz\n",
      "│       ├── [ 34K]  \u001b[01;34m60_perc_forgetting_events_user\u001b[00m\n",
      "│       │   ├── [ 106]  data_stats.json\n",
      "│       │   └── [ 30K]  index.npz\n",
      "│       ├── [ 34K]  \u001b[01;34m60_perc_forgetting_events_user_propensity\u001b[00m\n",
      "│       │   ├── [ 106]  data_stats.json\n",
      "│       │   └── [ 30K]  index.npz\n",
      "│       ├── [ 47K]  \u001b[01;34m80_perc_forgetting_events\u001b[00m\n",
      "│       │   ├── [ 106]  data_stats.json\n",
      "│       │   └── [ 42K]  index.npz\n",
      "│       ├── [ 46K]  \u001b[01;34m80_perc_forgetting_events_propensity\u001b[00m\n",
      "│       │   ├── [ 106]  data_stats.json\n",
      "│       │   └── [ 42K]  index.npz\n",
      "│       ├── [ 33K]  \u001b[01;34m80_perc_forgetting_events_user\u001b[00m\n",
      "│       │   ├── [ 106]  data_stats.json\n",
      "│       │   └── [ 29K]  index.npz\n",
      "│       ├── [ 34K]  \u001b[01;34m80_perc_forgetting_events_user_propensity\u001b[00m\n",
      "│       │   ├── [ 106]  data_stats.json\n",
      "│       │   └── [ 29K]  index.npz\n",
      "│       ├── [ 42K]  \u001b[01;34m90_perc_forgetting_events\u001b[00m\n",
      "│       │   ├── [ 105]  data_stats.json\n",
      "│       │   └── [ 38K]  index.npz\n",
      "│       ├── [ 42K]  \u001b[01;34m90_perc_forgetting_events_propensity\u001b[00m\n",
      "│       │   ├── [ 105]  data_stats.json\n",
      "│       │   └── [ 37K]  index.npz\n",
      "│       ├── [ 32K]  \u001b[01;34m90_perc_forgetting_events_user\u001b[00m\n",
      "│       │   ├── [ 105]  data_stats.json\n",
      "│       │   └── [ 28K]  index.npz\n",
      "│       ├── [ 33K]  \u001b[01;34m90_perc_forgetting_events_user_propensity\u001b[00m\n",
      "│       │   ├── [ 105]  data_stats.json\n",
      "│       │   └── [ 29K]  index.npz\n",
      "│       ├── [ 33K]  \u001b[01;34m99_perc_forgetting_events\u001b[00m\n",
      "│       │   ├── [ 104]  data_stats.json\n",
      "│       │   └── [ 29K]  index.npz\n",
      "│       ├── [ 33K]  \u001b[01;34m99_perc_forgetting_events_propensity\u001b[00m\n",
      "│       │   ├── [ 104]  data_stats.json\n",
      "│       │   └── [ 29K]  index.npz\n",
      "│       ├── [ 31K]  \u001b[01;34m99_perc_forgetting_events_user\u001b[00m\n",
      "│       │   ├── [ 104]  data_stats.json\n",
      "│       │   └── [ 27K]  index.npz\n",
      "│       └── [ 32K]  \u001b[01;34m99_perc_forgetting_events_user_propensity\u001b[00m\n",
      "│           ├── [ 104]  data_stats.json\n",
      "│           └── [ 27K]  index.npz\n",
      "├── [ 716]  \u001b[01;32mallbut.pl\u001b[00m\n",
      "├── [1.7M]  \u001b[01;34mleave_2\u001b[00m\n",
      "│   ├── [ 22K]  \u001b[01;34m20_perc_forest_fire\u001b[00m\n",
      "│   │   ├── [ 102]  data_stats.json\n",
      "│   │   └── [ 18K]  index.npz\n",
      "│   ├── [ 30K]  \u001b[01;34m20_perc_freq_user_rns\u001b[00m\n",
      "│   │   ├── [ 103]  data_stats.json\n",
      "│   │   └── [ 26K]  index.npz\n",
      "│   ├── [ 30K]  \u001b[01;34m20_perc_interaction_rns\u001b[00m\n",
      "│   │   ├── [ 103]  data_stats.json\n",
      "│   │   └── [ 26K]  index.npz\n",
      "│   ├── [ 24K]  \u001b[01;34m20_perc_pagerank\u001b[00m\n",
      "│   │   ├── [ 102]  data_stats.json\n",
      "│   │   └── [ 20K]  index.npz\n",
      "│   ├── [ 26K]  \u001b[01;34m20_perc_random_walk\u001b[00m\n",
      "│   │   ├── [ 102]  data_stats.json\n",
      "│   │   └── [ 21K]  index.npz\n",
      "│   ├── [7.7K]  \u001b[01;34m20_perc_tail_user_remove\u001b[00m\n",
      "│   │   ├── [ 103]  data_stats.json\n",
      "│   │   └── [3.6K]  index.npz\n",
      "│   ├── [9.7K]  \u001b[01;34m20_perc_temporal\u001b[00m\n",
      "│   │   ├── [ 103]  data_stats.json\n",
      "│   │   └── [5.6K]  index.npz\n",
      "│   ├── [7.8K]  \u001b[01;34m20_perc_user_rns\u001b[00m\n",
      "│   │   ├── [ 103]  data_stats.json\n",
      "│   │   └── [3.7K]  index.npz\n",
      "│   ├── [ 31K]  \u001b[01;34m40_perc_forest_fire\u001b[00m\n",
      "│   │   ├── [ 102]  data_stats.json\n",
      "│   │   └── [ 27K]  index.npz\n",
      "│   ├── [ 36K]  \u001b[01;34m40_perc_freq_user_rns\u001b[00m\n",
      "│   │   ├── [ 103]  data_stats.json\n",
      "│   │   └── [ 32K]  index.npz\n",
      "│   ├── [ 36K]  \u001b[01;34m40_perc_interaction_rns\u001b[00m\n",
      "│   │   ├── [ 103]  data_stats.json\n",
      "│   │   └── [ 32K]  index.npz\n",
      "│   ├── [ 29K]  \u001b[01;34m40_perc_pagerank\u001b[00m\n",
      "│   │   ├── [ 102]  data_stats.json\n",
      "│   │   └── [ 25K]  index.npz\n",
      "│   ├── [ 32K]  \u001b[01;34m40_perc_random_walk\u001b[00m\n",
      "│   │   ├── [ 102]  data_stats.json\n",
      "│   │   └── [ 28K]  index.npz\n",
      "│   ├── [7.7K]  \u001b[01;34m40_perc_tail_user_remove\u001b[00m\n",
      "│   │   ├── [ 103]  data_stats.json\n",
      "│   │   └── [3.6K]  index.npz\n",
      "│   ├── [ 10K]  \u001b[01;34m40_perc_temporal\u001b[00m\n",
      "│   │   ├── [ 103]  data_stats.json\n",
      "│   │   └── [6.0K]  index.npz\n",
      "│   ├── [7.8K]  \u001b[01;34m40_perc_user_rns\u001b[00m\n",
      "│   │   ├── [ 103]  data_stats.json\n",
      "│   │   └── [3.7K]  index.npz\n",
      "│   ├── [ 32K]  \u001b[01;34m60_perc_forest_fire\u001b[00m\n",
      "│   │   ├── [ 102]  data_stats.json\n",
      "│   │   └── [ 28K]  index.npz\n",
      "│   ├── [ 36K]  \u001b[01;34m60_perc_freq_user_rns\u001b[00m\n",
      "│   │   ├── [ 103]  data_stats.json\n",
      "│   │   └── [ 32K]  index.npz\n",
      "│   ├── [ 36K]  \u001b[01;34m60_perc_interaction_rns\u001b[00m\n",
      "│   │   ├── [ 103]  data_stats.json\n",
      "│   │   └── [ 32K]  index.npz\n",
      "│   ├── [ 29K]  \u001b[01;34m60_perc_pagerank\u001b[00m\n",
      "│   │   ├── [ 102]  data_stats.json\n",
      "│   │   └── [ 25K]  index.npz\n",
      "│   ├── [ 30K]  \u001b[01;34m60_perc_random_walk\u001b[00m\n",
      "│   │   ├── [ 102]  data_stats.json\n",
      "│   │   └── [ 26K]  index.npz\n",
      "│   ├── [7.7K]  \u001b[01;34m60_perc_tail_user_remove\u001b[00m\n",
      "│   │   ├── [ 103]  data_stats.json\n",
      "│   │   └── [3.6K]  index.npz\n",
      "│   ├── [9.9K]  \u001b[01;34m60_perc_temporal\u001b[00m\n",
      "│   │   ├── [ 103]  data_stats.json\n",
      "│   │   └── [5.8K]  index.npz\n",
      "│   ├── [7.8K]  \u001b[01;34m60_perc_user_rns\u001b[00m\n",
      "│   │   ├── [ 103]  data_stats.json\n",
      "│   │   └── [3.7K]  index.npz\n",
      "│   ├── [ 22K]  \u001b[01;34m80_perc_forest_fire\u001b[00m\n",
      "│   │   ├── [ 102]  data_stats.json\n",
      "│   │   └── [ 18K]  index.npz\n",
      "│   ├── [ 30K]  \u001b[01;34m80_perc_freq_user_rns\u001b[00m\n",
      "│   │   ├── [ 103]  data_stats.json\n",
      "│   │   └── [ 26K]  index.npz\n",
      "│   ├── [ 30K]  \u001b[01;34m80_perc_interaction_rns\u001b[00m\n",
      "│   │   ├── [ 103]  data_stats.json\n",
      "│   │   └── [ 26K]  index.npz\n",
      "│   ├── [ 24K]  \u001b[01;34m80_perc_pagerank\u001b[00m\n",
      "│   │   ├── [ 102]  data_stats.json\n",
      "│   │   └── [ 20K]  index.npz\n",
      "│   ├── [ 23K]  \u001b[01;34m80_perc_random_walk\u001b[00m\n",
      "│   │   ├── [ 102]  data_stats.json\n",
      "│   │   └── [ 19K]  index.npz\n",
      "│   ├── [7.7K]  \u001b[01;34m80_perc_tail_user_remove\u001b[00m\n",
      "│   │   ├── [ 103]  data_stats.json\n",
      "│   │   └── [3.6K]  index.npz\n",
      "│   ├── [9.4K]  \u001b[01;34m80_perc_temporal\u001b[00m\n",
      "│   │   ├── [ 103]  data_stats.json\n",
      "│   │   └── [5.3K]  index.npz\n",
      "│   ├── [7.7K]  \u001b[01;34m80_perc_user_rns\u001b[00m\n",
      "│   │   ├── [ 103]  data_stats.json\n",
      "│   │   └── [3.6K]  index.npz\n",
      "│   ├── [ 20K]  \u001b[01;34m90_perc_forest_fire\u001b[00m\n",
      "│   │   ├── [ 101]  data_stats.json\n",
      "│   │   └── [ 16K]  index.npz\n",
      "│   ├── [ 22K]  \u001b[01;34m90_perc_freq_user_rns\u001b[00m\n",
      "│   │   ├── [ 102]  data_stats.json\n",
      "│   │   └── [ 18K]  index.npz\n",
      "│   ├── [ 23K]  \u001b[01;34m90_perc_interaction_rns\u001b[00m\n",
      "│   │   ├── [ 102]  data_stats.json\n",
      "│   │   └── [ 19K]  index.npz\n",
      "│   ├── [ 19K]  \u001b[01;34m90_perc_pagerank\u001b[00m\n",
      "│   │   ├── [ 101]  data_stats.json\n",
      "│   │   └── [ 15K]  index.npz\n",
      "│   ├── [ 19K]  \u001b[01;34m90_perc_random_walk\u001b[00m\n",
      "│   │   ├── [ 101]  data_stats.json\n",
      "│   │   └── [ 15K]  index.npz\n",
      "│   ├── [7.7K]  \u001b[01;34m90_perc_tail_user_remove\u001b[00m\n",
      "│   │   ├── [ 102]  data_stats.json\n",
      "│   │   └── [3.6K]  index.npz\n",
      "│   ├── [8.9K]  \u001b[01;34m90_perc_temporal\u001b[00m\n",
      "│   │   ├── [ 102]  data_stats.json\n",
      "│   │   └── [4.8K]  index.npz\n",
      "│   ├── [7.7K]  \u001b[01;34m90_perc_user_rns\u001b[00m\n",
      "│   │   ├── [ 102]  data_stats.json\n",
      "│   │   └── [3.6K]  index.npz\n",
      "│   ├── [9.9K]  \u001b[01;34m99_perc_forest_fire\u001b[00m\n",
      "│   │   ├── [ 100]  data_stats.json\n",
      "│   │   └── [5.8K]  index.npz\n",
      "│   ├── [9.3K]  \u001b[01;34m99_perc_freq_user_rns\u001b[00m\n",
      "│   │   ├── [ 100]  data_stats.json\n",
      "│   │   └── [5.2K]  index.npz\n",
      "│   ├── [ 10K]  \u001b[01;34m99_perc_interaction_rns\u001b[00m\n",
      "│   │   ├── [ 100]  data_stats.json\n",
      "│   │   └── [6.2K]  index.npz\n",
      "│   ├── [9.6K]  \u001b[01;34m99_perc_pagerank\u001b[00m\n",
      "│   │   ├── [ 100]  data_stats.json\n",
      "│   │   └── [5.5K]  index.npz\n",
      "│   ├── [ 10K]  \u001b[01;34m99_perc_random_walk\u001b[00m\n",
      "│   │   ├── [ 100]  data_stats.json\n",
      "│   │   └── [5.9K]  index.npz\n",
      "│   ├── [7.6K]  \u001b[01;34m99_perc_tail_user_remove\u001b[00m\n",
      "│   │   ├── [ 101]  data_stats.json\n",
      "│   │   └── [3.5K]  index.npz\n",
      "│   ├── [7.9K]  \u001b[01;34m99_perc_temporal\u001b[00m\n",
      "│   │   ├── [ 100]  data_stats.json\n",
      "│   │   └── [3.8K]  index.npz\n",
      "│   ├── [7.6K]  \u001b[01;34m99_perc_user_rns\u001b[00m\n",
      "│   │   ├── [ 100]  data_stats.json\n",
      "│   │   └── [3.5K]  index.npz\n",
      "│   ├── [7.6K]  \u001b[01;34mcomplete_data\u001b[00m\n",
      "│   │   ├── [ 103]  data_stats.json\n",
      "│   │   └── [3.5K]  index.npz\n",
      "│   ├── [411K]  \u001b[01;34msvp_bias_only_sequential\u001b[00m\n",
      "│   │   ├── [ 29K]  \u001b[01;34m20_perc_forgetting_events\u001b[00m\n",
      "│   │   │   ├── [ 103]  data_stats.json\n",
      "│   │   │   └── [ 25K]  index.npz\n",
      "│   │   ├── [ 29K]  \u001b[01;34m20_perc_forgetting_events_propensity\u001b[00m\n",
      "│   │   │   ├── [ 103]  data_stats.json\n",
      "│   │   │   └── [ 24K]  index.npz\n",
      "│   │   ├── [7.8K]  \u001b[01;34m20_perc_forgetting_events_user\u001b[00m\n",
      "│   │   │   ├── [ 103]  data_stats.json\n",
      "│   │   │   └── [3.7K]  index.npz\n",
      "│   │   ├── [7.7K]  \u001b[01;34m20_perc_forgetting_events_user_propensity\u001b[00m\n",
      "│   │   │   ├── [ 103]  data_stats.json\n",
      "│   │   │   └── [3.6K]  index.npz\n",
      "│   │   ├── [ 34K]  \u001b[01;34m40_perc_forgetting_events\u001b[00m\n",
      "│   │   │   ├── [ 103]  data_stats.json\n",
      "│   │   │   └── [ 30K]  index.npz\n",
      "│   │   ├── [ 34K]  \u001b[01;34m40_perc_forgetting_events_propensity\u001b[00m\n",
      "│   │   │   ├── [ 103]  data_stats.json\n",
      "│   │   │   └── [ 30K]  index.npz\n",
      "│   │   ├── [7.8K]  \u001b[01;34m40_perc_forgetting_events_user\u001b[00m\n",
      "│   │   │   ├── [ 103]  data_stats.json\n",
      "│   │   │   └── [3.7K]  index.npz\n",
      "│   │   ├── [7.7K]  \u001b[01;34m40_perc_forgetting_events_user_propensity\u001b[00m\n",
      "│   │   │   ├── [ 103]  data_stats.json\n",
      "│   │   │   └── [3.6K]  index.npz\n",
      "│   │   ├── [ 34K]  \u001b[01;34m60_perc_forgetting_events\u001b[00m\n",
      "│   │   │   ├── [ 103]  data_stats.json\n",
      "│   │   │   └── [ 30K]  index.npz\n",
      "│   │   ├── [ 34K]  \u001b[01;34m60_perc_forgetting_events_propensity\u001b[00m\n",
      "│   │   │   ├── [ 103]  data_stats.json\n",
      "│   │   │   └── [ 30K]  index.npz\n",
      "│   │   ├── [7.8K]  \u001b[01;34m60_perc_forgetting_events_user\u001b[00m\n",
      "│   │   │   ├── [ 103]  data_stats.json\n",
      "│   │   │   └── [3.7K]  index.npz\n",
      "│   │   ├── [7.7K]  \u001b[01;34m60_perc_forgetting_events_user_propensity\u001b[00m\n",
      "│   │   │   ├── [ 103]  data_stats.json\n",
      "│   │   │   └── [3.6K]  index.npz\n",
      "│   │   ├── [ 28K]  \u001b[01;34m80_perc_forgetting_events\u001b[00m\n",
      "│   │   │   ├── [ 103]  data_stats.json\n",
      "│   │   │   └── [ 24K]  index.npz\n",
      "│   │   ├── [ 28K]  \u001b[01;34m80_perc_forgetting_events_propensity\u001b[00m\n",
      "│   │   │   ├── [ 103]  data_stats.json\n",
      "│   │   │   └── [ 24K]  index.npz\n",
      "│   │   ├── [7.7K]  \u001b[01;34m80_perc_forgetting_events_user\u001b[00m\n",
      "│   │   │   ├── [ 103]  data_stats.json\n",
      "│   │   │   └── [3.6K]  index.npz\n",
      "│   │   ├── [7.7K]  \u001b[01;34m80_perc_forgetting_events_user_propensity\u001b[00m\n",
      "│   │   │   ├── [ 103]  data_stats.json\n",
      "│   │   │   └── [3.6K]  index.npz\n",
      "│   │   ├── [ 22K]  \u001b[01;34m90_perc_forgetting_events\u001b[00m\n",
      "│   │   │   ├── [ 102]  data_stats.json\n",
      "│   │   │   └── [ 18K]  index.npz\n",
      "│   │   ├── [ 22K]  \u001b[01;34m90_perc_forgetting_events_propensity\u001b[00m\n",
      "│   │   │   ├── [ 102]  data_stats.json\n",
      "│   │   │   └── [ 18K]  index.npz\n",
      "│   │   ├── [7.7K]  \u001b[01;34m90_perc_forgetting_events_user\u001b[00m\n",
      "│   │   │   ├── [ 102]  data_stats.json\n",
      "│   │   │   └── [3.6K]  index.npz\n",
      "│   │   ├── [7.7K]  \u001b[01;34m90_perc_forgetting_events_user_propensity\u001b[00m\n",
      "│   │   │   ├── [ 102]  data_stats.json\n",
      "│   │   │   └── [3.6K]  index.npz\n",
      "│   │   ├── [9.9K]  \u001b[01;34m99_perc_forgetting_events\u001b[00m\n",
      "│   │   │   ├── [ 101]  data_stats.json\n",
      "│   │   │   └── [5.8K]  index.npz\n",
      "│   │   ├── [9.9K]  \u001b[01;34m99_perc_forgetting_events_propensity\u001b[00m\n",
      "│   │   │   ├── [ 101]  data_stats.json\n",
      "│   │   │   └── [5.8K]  index.npz\n",
      "│   │   ├── [7.6K]  \u001b[01;34m99_perc_forgetting_events_user\u001b[00m\n",
      "│   │   │   ├── [ 101]  data_stats.json\n",
      "│   │   │   └── [3.5K]  index.npz\n",
      "│   │   └── [7.6K]  \u001b[01;34m99_perc_forgetting_events_user_propensity\u001b[00m\n",
      "│   │       ├── [ 100]  data_stats.json\n",
      "│   │       └── [3.5K]  index.npz\n",
      "│   └── [416K]  \u001b[01;34msvp_MF_dot_sequential\u001b[00m\n",
      "│       ├── [ 29K]  \u001b[01;34m20_perc_forgetting_events\u001b[00m\n",
      "│       │   ├── [ 103]  data_stats.json\n",
      "│       │   └── [ 25K]  index.npz\n",
      "│       ├── [ 29K]  \u001b[01;34m20_perc_forgetting_events_propensity\u001b[00m\n",
      "│       │   ├── [ 103]  data_stats.json\n",
      "│       │   └── [ 25K]  index.npz\n",
      "│       ├── [7.8K]  \u001b[01;34m20_perc_forgetting_events_user\u001b[00m\n",
      "│       │   ├── [ 103]  data_stats.json\n",
      "│       │   └── [3.7K]  index.npz\n",
      "│       ├── [7.7K]  \u001b[01;34m20_perc_forgetting_events_user_propensity\u001b[00m\n",
      "│       │   ├── [ 103]  data_stats.json\n",
      "│       │   └── [3.6K]  index.npz\n",
      "│       ├── [ 35K]  \u001b[01;34m40_perc_forgetting_events\u001b[00m\n",
      "│       │   ├── [ 103]  data_stats.json\n",
      "│       │   └── [ 31K]  index.npz\n",
      "│       ├── [ 35K]  \u001b[01;34m40_perc_forgetting_events_propensity\u001b[00m\n",
      "│       │   ├── [ 103]  data_stats.json\n",
      "│       │   └── [ 31K]  index.npz\n",
      "│       ├── [7.8K]  \u001b[01;34m40_perc_forgetting_events_user\u001b[00m\n",
      "│       │   ├── [ 103]  data_stats.json\n",
      "│       │   └── [3.6K]  index.npz\n",
      "│       ├── [7.8K]  \u001b[01;34m40_perc_forgetting_events_user_propensity\u001b[00m\n",
      "│       │   ├── [ 103]  data_stats.json\n",
      "│       │   └── [3.7K]  index.npz\n",
      "│       ├── [ 35K]  \u001b[01;34m60_perc_forgetting_events\u001b[00m\n",
      "│       │   ├── [ 103]  data_stats.json\n",
      "│       │   └── [ 31K]  index.npz\n",
      "│       ├── [ 35K]  \u001b[01;34m60_perc_forgetting_events_propensity\u001b[00m\n",
      "│       │   ├── [ 103]  data_stats.json\n",
      "│       │   └── [ 31K]  index.npz\n",
      "│       ├── [7.7K]  \u001b[01;34m60_perc_forgetting_events_user\u001b[00m\n",
      "│       │   ├── [ 103]  data_stats.json\n",
      "│       │   └── [3.6K]  index.npz\n",
      "│       ├── [7.8K]  \u001b[01;34m60_perc_forgetting_events_user_propensity\u001b[00m\n",
      "│       │   ├── [ 103]  data_stats.json\n",
      "│       │   └── [3.7K]  index.npz\n",
      "│       ├── [ 29K]  \u001b[01;34m80_perc_forgetting_events\u001b[00m\n",
      "│       │   ├── [ 103]  data_stats.json\n",
      "│       │   └── [ 25K]  index.npz\n",
      "│       ├── [ 29K]  \u001b[01;34m80_perc_forgetting_events_propensity\u001b[00m\n",
      "│       │   ├── [ 103]  data_stats.json\n",
      "│       │   └── [ 25K]  index.npz\n",
      "│       ├── [7.7K]  \u001b[01;34m80_perc_forgetting_events_user\u001b[00m\n",
      "│       │   ├── [ 103]  data_stats.json\n",
      "│       │   └── [3.6K]  index.npz\n",
      "│       ├── [7.8K]  \u001b[01;34m80_perc_forgetting_events_user_propensity\u001b[00m\n",
      "│       │   ├── [ 103]  data_stats.json\n",
      "│       │   └── [3.7K]  index.npz\n",
      "│       ├── [ 22K]  \u001b[01;34m90_perc_forgetting_events\u001b[00m\n",
      "│       │   ├── [ 102]  data_stats.json\n",
      "│       │   └── [ 18K]  index.npz\n",
      "│       ├── [ 22K]  \u001b[01;34m90_perc_forgetting_events_propensity\u001b[00m\n",
      "│       │   ├── [ 102]  data_stats.json\n",
      "│       │   └── [ 18K]  index.npz\n",
      "│       ├── [7.7K]  \u001b[01;34m90_perc_forgetting_events_user\u001b[00m\n",
      "│       │   ├── [ 102]  data_stats.json\n",
      "│       │   └── [3.6K]  index.npz\n",
      "│       ├── [7.7K]  \u001b[01;34m90_perc_forgetting_events_user_propensity\u001b[00m\n",
      "│       │   ├── [ 102]  data_stats.json\n",
      "│       │   └── [3.6K]  index.npz\n",
      "│       ├── [ 10K]  \u001b[01;34m99_perc_forgetting_events\u001b[00m\n",
      "│       │   ├── [ 101]  data_stats.json\n",
      "│       │   └── [6.1K]  index.npz\n",
      "│       ├── [ 10K]  \u001b[01;34m99_perc_forgetting_events_propensity\u001b[00m\n",
      "│       │   ├── [ 101]  data_stats.json\n",
      "│       │   └── [6.0K]  index.npz\n",
      "│       ├── [7.6K]  \u001b[01;34m99_perc_forgetting_events_user\u001b[00m\n",
      "│       │   ├── [ 101]  data_stats.json\n",
      "│       │   └── [3.5K]  index.npz\n",
      "│       └── [7.7K]  \u001b[01;34m99_perc_forgetting_events_user_propensity\u001b[00m\n",
      "│           ├── [ 100]  data_stats.json\n",
      "│           └── [3.6K]  index.npz\n",
      "├── [ 643]  \u001b[01;32mmku.sh\u001b[00m\n",
      "├── [6.6K]  README\n",
      "├── [368K]  total_data.hdf5\n",
      "├── [1.5M]  u1.base\n",
      "├── [383K]  u1.test\n",
      "├── [1.5M]  u2.base\n",
      "├── [386K]  u2.test\n",
      "├── [1.5M]  u3.base\n",
      "├── [387K]  u3.test\n",
      "├── [1.5M]  u4.base\n",
      "├── [388K]  u4.test\n",
      "├── [1.5M]  u5.base\n",
      "├── [388K]  u5.test\n",
      "├── [1.7M]  ua.base\n",
      "├── [182K]  ua.test\n",
      "├── [1.7M]  ub.base\n",
      "├── [182K]  ub.test\n",
      "├── [1.9M]  u.data\n",
      "├── [ 202]  u.genre\n",
      "├── [  36]  u.info\n",
      "├── [231K]  u.item\n",
      "├── [ 193]  u.occupation\n",
      "└── [ 22K]  u.user\n",
      "\n",
      "  23M used in 250 directories, 508 files\n"
     ]
    }
   ],
   "source": [
    "!tree --du -h -C ./datasets/ml-100k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "- [https://arxiv.org/abs/2201.04768v1](https://arxiv.org/abs/2201.04768v1)\n",
    "- [https://github.com/noveens/sampling_cf](https://github.com/noveens/sampling_cf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Theory\n",
    "| Sampling Strategy | What is sampled? |\n",
    "| --- | --- |\n",
    "| Random | Interactions |\n",
    "| Stratified | Interactions |\n",
    "| Temporal | Interactions |\n",
    "| SVP-CF w/ MF | Interactions |\n",
    "| SVP-CF w/ Bias-only | Interactions |\n",
    "| SVP-CF-Prop w/ MF | Interactions |\n",
    "| SVP-CF-Prop w/ Bias-only | Interactions |\n",
    "| Random | Users |\n",
    "| Head | Users |\n",
    "| SVP-CF w/ MF | Users |\n",
    "| SVP-CF w/ Bias-only | Users |\n",
    "| SVP-CF-Prop w/ MF | Users |\n",
    "| SVP-CF-Prop w/ Bias-only | Users |\n",
    "| Centrality | Graph |\n",
    "| Random-Walk | Graph |\n",
    "| Forest-Fire | Graph |\n",
    "\n",
    "### Interaction sampling\n",
    "\n",
    "- In Random Interaction Sampling, we generate D𝑠,𝑝 by randomly sampling 𝑝% of all the user-item interactions in D.\n",
    "- User-history Stratified Sampling is another popular sampling technique to generate smaller CF-datasets. To match the user-frequency distribution amongst D and D𝑠,𝑝 , it randomly samples 𝑝% of interactions from each user’s consumption history.\n",
    "- Unlike random stratified sampling, User-history Temporal Sampling samples 𝑝% of the most recent interactions for each user. This strategy is representative of the popular practice of making data subsets from the online traffic of the last 𝑥 days.\n",
    "\n",
    "### User sampling\n",
    "\n",
    "*To ensure a fair comparison amongst the different kinds of sampling schemes, we retain exactly 𝑝% of the total interactions in D𝑠,𝑝 .*\n",
    "\n",
    "- In Random User Sampling, we retain users from D at random. To be more specific, we iteratively preserve all the interactions for a random user until we have retained 𝑝% of the original interactions.\n",
    "- Another strategy we employ is Head User Sampling, in which we iteratively remove the user with the least amount of total interactions. This method is representative of commonly used data pre-processing strategies to make data suitable for parameter-heavy algorithms. Sampling the data in such a way can introduce bias toward users from minority groups which might raise concerns from a diversity and fairness perspective.\n",
    "\n",
    "### Graph sampling\n",
    "\n",
    "- In Centrality-based Sampling, we proceed by computing the pagerank centrality scores for each node in G, and retain all the edges (interactions) of the top scoring nodes until a total 𝑝% of the original interactions have been preserved.\n",
    "- Another popular strategy we employ is Random-walk Sampling, which performs multiple random-walks with restart on G and retains the edges amongst those pairs of nodes that have been visited at least once. We keep expanding our walk until 𝑝% of the initial edges have been retained.\n",
    "- We also utilize Forest-fire Sampling, which is a snowball sampling method and proceeds by randomly “burning” the outgoing edges of visited nodes. It initially starts with a random node, and then propagates to a random subset of previously unvisited neighbors. The propagation is terminated once we have created a graph-subset with 𝑝% of the initial edges.\n",
    "\n",
    "### SVP-CF: Selection-Via-Proxy for CF data\n",
    "\n",
    "Irrespective of whether to sample users or interactions, SVPCF proceeds by training an inexpensive proxy model P on the full, original data D and modifies the forgetting-events approach to retain the points with the highest importance."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
