{"nbformat":4,"nbformat_minor":5,"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.10"},"colab":{"name":"2022-01-26-transformer4rec-xlnet.ipynb","provenance":[{"file_id":"https://github.com/recohut/nbs/blob/main/raw/T382183%20%7C%20Transformers4Rec%20XLNet%20on%20Synthetic%20data%20.ipynb","timestamp":1644674256804},{"file_id":"https://github.com/NVIDIA-Merlin/Transformers4Rec/blob/main/examples/getting-started-session-based/01-ETL-with-NVTabular.ipynb","timestamp":1633949614399}],"collapsed_sections":["02a41961","0b72337b","24ca5c73","18148079","03933343","ff2ceded","4c714512","41277491","8af6146d","12ba5fc4","cbd0c5fd"]}},"cells":[{"cell_type":"markdown","metadata":{"id":"eIV0pbarAxkG"},"source":["# Transformers4Rec XLNet on Synthetic data "],"id":"eIV0pbarAxkG"},{"cell_type":"markdown","metadata":{"id":"f0f8b4e3"},"source":["In this tutorial we introduce the [Transformers4Rec](https://github.com/NVIDIA-Merlin/Transformers4Rec) library for sequential and session-based recommendation. This tutorial uses the PyTorch API, but a TensorFlow API is also available. Transformers4Rec integrates with the popular [HuggingFaceâ€™s Transformers](https://github.com/huggingface/transformers) and make it possible to experiment with cutting-edge implementation of the latest NLP Transformer architectures.  \n","\n","We demonstrate how to build a session-based recommendation model with the [XLNET](https://arxiv.org/abs/1906.08237) Transformer architecture. The XLNet architecture was designed to leverage the best of both auto-regressive language modeling and auto-encoding with its Permutation Language Modeling training method. In this example we will use XLNET with masked language modeling (MLM) training method, which showed very promising results."],"id":"f0f8b4e3"},{"cell_type":"markdown","metadata":{"id":"ab6085c0"},"source":["First, we are going to generate synthetic data and then create sequential features with [NVTabular](https://github.com/NVIDIA-Merlin/NVTabular). Such data will be used to train a session-based recommendation model.\n","\n","NVTabular is a feature engineering and preprocessing library for tabular data designed to quickly and easily manipulate terabyte scale datasets used to train deep learning based recommender systems. It provides a high level abstraction to simplify code and accelerates computation on the GPU using the RAPIDS cuDF library."],"id":"ab6085c0"},{"cell_type":"markdown","metadata":{"id":"add26d16"},"source":["## Import required libraries"],"id":"add26d16"},{"cell_type":"code","metadata":{"id":"1e8dae24"},"source":["import os\n","import glob\n","\n","import torch \n","import numpy as np\n","import pandas as pd\n","\n","import cudf\n","import cupy as cp\n","import nvtabular as nvt\n","\n","from transformers4rec import torch as tr\n","from transformers4rec.torch.ranking_metric import NDCGAt, AvgPrecisionAt, RecallAt"],"id":"1e8dae24","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"c3206b3f"},"source":["## Define Input/Output Path"],"id":"c3206b3f"},{"cell_type":"code","metadata":{"id":"105dd71c"},"source":["INPUT_DATA_DIR = os.environ.get(\"INPUT_DATA_DIR\", \"/workspace/data/\")"],"id":"105dd71c","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"36498a01"},"source":["## Create a Synthetic Input Data"],"id":"36498a01"},{"cell_type":"code","metadata":{"id":"929036ae"},"source":["NUM_ROWS = 100000\n","long_tailed_item_distribution = np.clip(np.random.lognormal(3., 1., NUM_ROWS).astype(np.int32), 1, 50000)\n","\n","# generate random item interaction features \n","df = pd.DataFrame(np.random.randint(70000, 80000, NUM_ROWS), columns=['session_id'])\n","df['item_id'] = long_tailed_item_distribution\n","\n","# generate category mapping for each item-id\n","df['category'] = pd.cut(df['item_id'], bins=334, labels=np.arange(1, 335)).astype(np.int32)\n","df['timestamp/age_days'] = np.random.uniform(0, 1, NUM_ROWS)\n","df['timestamp/weekday/sin']= np.random.uniform(0, 1, NUM_ROWS)\n","\n","# generate day mapping for each session \n","map_day = dict(zip(df.session_id.unique(), np.random.randint(1, 10, size=(df.session_id.nunique()))))\n","df['day'] =  df.session_id.map(map_day)"],"id":"929036ae","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"cd861fcd"},"source":["- Visualize couple of rows of the synthetic dataset"],"id":"cd861fcd"},{"cell_type":"code","metadata":{"id":"9617e30c","outputId":"f190c609-8e89-40fa-c4df-176ada3d9454"},"source":["df.head()"],"id":"9617e30c","execution_count":null,"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>session_id</th>\n","      <th>item_id</th>\n","      <th>category</th>\n","      <th>timestamp/age_days</th>\n","      <th>timestamp/weekday/sin</th>\n","      <th>day</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>72794</td>\n","      <td>25</td>\n","      <td>8</td>\n","      <td>0.425057</td>\n","      <td>0.796974</td>\n","      <td>9</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>72989</td>\n","      <td>57</td>\n","      <td>18</td>\n","      <td>0.729572</td>\n","      <td>0.924252</td>\n","      <td>7</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>78236</td>\n","      <td>2</td>\n","      <td>1</td>\n","      <td>0.922154</td>\n","      <td>0.532076</td>\n","      <td>4</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>72766</td>\n","      <td>9</td>\n","      <td>3</td>\n","      <td>0.956614</td>\n","      <td>0.567720</td>\n","      <td>3</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>76730</td>\n","      <td>9</td>\n","      <td>3</td>\n","      <td>0.361798</td>\n","      <td>0.611959</td>\n","      <td>4</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   session_id  item_id  category  timestamp/age_days  timestamp/weekday/sin  \\\n","0       72794       25         8            0.425057               0.796974   \n","1       72989       57        18            0.729572               0.924252   \n","2       78236        2         1            0.922154               0.532076   \n","3       72766        9         3            0.956614               0.567720   \n","4       76730        9         3            0.361798               0.611959   \n","\n","   day  \n","0    9  \n","1    7  \n","2    4  \n","3    3  \n","4    4  "]},"execution_count":5,"metadata":{},"output_type":"execute_result"}]},{"cell_type":"markdown","metadata":{"id":"fae36e04"},"source":["## Feature Engineering with NVTabular"],"id":"fae36e04"},{"cell_type":"markdown","metadata":{"id":"139de226"},"source":["Deep Learning models require dense input features. Categorical features are sparse, and need to be represented by dense embeddings in the model. To allow for that, categorical features need first to be encoded as contiguous integers `(0, ..., |C|)`, where `|C|` is the feature cardinality (number of unique values), so that their embeddings can be efficiently stored in embedding layers.  We will use NVTabular to preprocess the categorical features, so that all categorical columns are encoded as contiguous integers.  Note that in the `Categorify` op we set `start_index=1`, the reason for that we want the encoded null values to start from `1` instead of `0` because we reserve `0` for padding the sequence features."],"id":"139de226"},{"cell_type":"markdown","metadata":{"id":"55b3bb9c"},"source":["Here our goal is to create sequential features.  In this cell, we are creating temporal features and grouping them together at the session level, sorting the interactions by time. Note that we also trim each feature sequence in a  session to a certain length. Here, we use the NVTabular library so that we can easily preprocess and create features on GPU with a few lines."],"id":"55b3bb9c"},{"cell_type":"code","metadata":{"id":"a256f195"},"source":["# Categorify categorical features\n","categ_feats = ['session_id', 'item_id', 'category'] >> nvt.ops.Categorify(start_index=1)\n","\n","# Define Groupby Workflow\n","groupby_feats = categ_feats + ['day', 'timestamp/age_days', 'timestamp/weekday/sin']\n","\n","# Groups interaction features by session and sorted by timestamp\n","groupby_features = groupby_feats >> nvt.ops.Groupby(\n","    groupby_cols=[\"session_id\"], \n","    aggs={\n","        \"item_id\": [\"list\", \"count\"],\n","        \"category\": [\"list\"],     \n","        \"day\": [\"first\"],\n","        \"timestamp/age_days\": [\"list\"],\n","        'timestamp/weekday/sin': [\"list\"],\n","        },\n","    name_sep=\"-\")\n","\n","# Select and truncate the sequential features\n","sequence_features_truncated = (groupby_features['category-list', 'item_id-list', \n","                                          'timestamp/age_days-list', 'timestamp/weekday/sin-list']) >> \\\n","                            nvt.ops.ListSlice(0,20) >> nvt.ops.Rename(postfix = '_trim')\n","\n","# Filter out sessions with length 1 (not valid for next-item prediction training and evaluation)\n","MINIMUM_SESSION_LENGTH = 2\n","selected_features = groupby_features['item_id-count', 'day-first', 'session_id'] + sequence_features_truncated\n","filtered_sessions = selected_features >> nvt.ops.Filter(f=lambda df: df[\"item_id-count\"] >= MINIMUM_SESSION_LENGTH)\n","\n","\n","workflow = nvt.Workflow(filtered_sessions)\n","dataset = nvt.Dataset(df, cpu=False)\n","# Generating statistics for the features\n","workflow.fit(dataset)\n","# Applying the preprocessing and returning an NVTabular dataset\n","sessions_ds = workflow.transform(dataset)\n","# Converting the NVTabular dataset to a Dask cuDF dataframe (`to_ddf()`) and then to cuDF dataframe (`.compute()`)\n","sessions_gdf = sessions_ds.to_ddf().compute()"],"id":"a256f195","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4dcbca33","outputId":"3d3c969c-76e7-4106-f353-70496ccf6480"},"source":["sessions_gdf.head(3)"],"id":"4dcbca33","execution_count":null,"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>item_id-count</th>\n","      <th>day-first</th>\n","      <th>session_id</th>\n","      <th>category-list_trim</th>\n","      <th>item_id-list_trim</th>\n","      <th>timestamp/age_days-list_trim</th>\n","      <th>timestamp/weekday/sin-list_trim</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>25</td>\n","      <td>3</td>\n","      <td>2</td>\n","      <td>[24, 12, 6, 3, 3, 6, 9, 2, 8, 15, 9, 5, 6, 8, ...</td>\n","      <td>[79, 36, 13, 5, 8, 12, 27, 4, 21, 42, 28, 10, ...</td>\n","      <td>[0.4751982727759114, 0.055393015414691105, 0.2...</td>\n","      <td>[0.8122129009074556, 0.5284590396837701, 0.041...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>24</td>\n","      <td>6</td>\n","      <td>3</td>\n","      <td>[3, 12, 16, 14, 13, 10, 13, 9, 24, 19, 32, 68,...</td>\n","      <td>[2, 33, 55, 40, 39, 23, 38, 27, 78, 57, 109, 1...</td>\n","      <td>[0.5303167840438227, 0.800766191594587, 0.3993...</td>\n","      <td>[0.0484016923364502, 0.9895741720728333, 0.020...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>23</td>\n","      <td>7</td>\n","      <td>4</td>\n","      <td>[2, 11, 3, 11, 6, 9, 2, 29, 21, 3, 5, 3, 5, 12...</td>\n","      <td>[4, 32, 5, 30, 13, 26, 3, 87, 62, 2, 22, 5, 14...</td>\n","      <td>[0.40259610248511546, 0.7994956663950287, 0.11...</td>\n","      <td>[0.13638022767099878, 0.5088356162643055, 0.06...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   item_id-count  day-first  session_id  \\\n","0             25          3           2   \n","1             24          6           3   \n","2             23          7           4   \n","\n","                                  category-list_trim  \\\n","0  [24, 12, 6, 3, 3, 6, 9, 2, 8, 15, 9, 5, 6, 8, ...   \n","1  [3, 12, 16, 14, 13, 10, 13, 9, 24, 19, 32, 68,...   \n","2  [2, 11, 3, 11, 6, 9, 2, 29, 21, 3, 5, 3, 5, 12...   \n","\n","                                   item_id-list_trim  \\\n","0  [79, 36, 13, 5, 8, 12, 27, 4, 21, 42, 28, 10, ...   \n","1  [2, 33, 55, 40, 39, 23, 38, 27, 78, 57, 109, 1...   \n","2  [4, 32, 5, 30, 13, 26, 3, 87, 62, 2, 22, 5, 14...   \n","\n","                        timestamp/age_days-list_trim  \\\n","0  [0.4751982727759114, 0.055393015414691105, 0.2...   \n","1  [0.5303167840438227, 0.800766191594587, 0.3993...   \n","2  [0.40259610248511546, 0.7994956663950287, 0.11...   \n","\n","                     timestamp/weekday/sin-list_trim  \n","0  [0.8122129009074556, 0.5284590396837701, 0.041...  \n","1  [0.0484016923364502, 0.9895741720728333, 0.020...  \n","2  [0.13638022767099878, 0.5088356162643055, 0.06...  "]},"execution_count":7,"metadata":{},"output_type":"execute_result"}]},{"cell_type":"markdown","metadata":{"id":"2458c28f"},"source":["It is possible to save the preprocessing workflow. That is useful to apply the same preprocessing to other data (with the same schema) and also to deploy the session-based recommendation pipeline to Triton Inference Server."],"id":"2458c28f"},{"cell_type":"code","metadata":{"id":"ff88e98f"},"source":["workflow.save('workflow_etl')"],"id":"ff88e98f","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"02a41961"},"source":["## Export pre-processed data by day"],"id":"02a41961"},{"cell_type":"markdown","metadata":{"id":"f9cedca3"},"source":["In this example we are going to split the preprocessed parquet files by days, to allow for temporal training and evaluation. There will be a folder for each day and three parquet files within each day folder: train.parquet, validation.parquet and test.parquet"],"id":"f9cedca3"},{"cell_type":"code","metadata":{"id":"12d3e59b"},"source":["OUTPUT_FOLDER = os.environ.get(\"OUTPUT_FOLDER\",os.path.join(INPUT_DATA_DIR, \"sessions_by_day\"))\n","!mkdir -p $OUTPUT_FOLDER"],"id":"12d3e59b","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6c67a92b","outputId":"20667850-4128-49bd-96b2-53cbda889b6c"},"source":["from transformers4rec.data.preprocessing import save_time_based_splits\n","save_time_based_splits(data=nvt.Dataset(sessions_gdf),\n","                       output_dir= OUTPUT_FOLDER,\n","                       partition_col='day-first',\n","                       timestamp_col='session_id', \n","                      )"],"id":"6c67a92b","execution_count":null,"outputs":[{"name":"stderr","output_type":"stream","text":["Creating time-based splits: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 10.96it/s]\n"]}]},{"cell_type":"markdown","metadata":{"id":"0b72337b"},"source":["## Checking the preprocessed outputs"],"id":"0b72337b"},{"cell_type":"code","metadata":{"id":"dd04ec82"},"source":["TRAIN_PATHS = sorted(glob.glob(os.path.join(OUTPUT_FOLDER, \"1\", \"train.parquet\")))"],"id":"dd04ec82","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8e5e6358","outputId":"3316ae01-4d0d-4449-d6e0-5e7c0ef21039"},"source":["gdf = cudf.read_parquet(TRAIN_PATHS[0])\n","gdf.head()"],"id":"8e5e6358","execution_count":null,"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>item_id-count</th>\n","      <th>session_id</th>\n","      <th>category-list_trim</th>\n","      <th>item_id-list_trim</th>\n","      <th>timestamp/age_days-list_trim</th>\n","      <th>timestamp/weekday/sin-list_trim</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>22</td>\n","      <td>7</td>\n","      <td>[6, 9, 5, 2, 7, 8, 5, 2, 6, 16, 2, 10, 35, 5, ...</td>\n","      <td>[15, 27, 10, 3, 17, 19, 10, 3, 13, 53, 3, 25, ...</td>\n","      <td>[0.05018395805258469, 0.3675245026471312, 0.45...</td>\n","      <td>[0.4569657788661693, 0.3016987134228405, 0.444...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>20</td>\n","      <td>17</td>\n","      <td>[3, 3, 3, 2, 29, 3, 4, 3, 17, 2, 21, 16, 8, 4,...</td>\n","      <td>[5, 8, 8, 4, 92, 5, 7, 5, 47, 3, 62, 52, 20, 1...</td>\n","      <td>[0.6514417831915809, 0.4076816703281344, 0.632...</td>\n","      <td>[0.14429839204039463, 0.9164664830523597, 0.28...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>19</td>\n","      <td>45</td>\n","      <td>[4, 34, 9, 3, 4, 11, 3, 7, 7, 6, 3, 5, 20, 8, ...</td>\n","      <td>[7, 95, 26, 2, 7, 32, 5, 17, 17, 15, 5, 10, 59...</td>\n","      <td>[0.8375365213796201, 0.5405179079133022, 0.779...</td>\n","      <td>[0.7202554739875682, 0.22750431643945657, 0.22...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>19</td>\n","      <td>62</td>\n","      <td>[10, 7, 8, 4, 26, 27, 5, 13, 6, 2, 9, 8, 3, 11...</td>\n","      <td>[25, 17, 19, 7, 75, 81, 10, 37, 12, 3, 29, 19,...</td>\n","      <td>[0.4649937741449487, 0.5034045853366875, 0.566...</td>\n","      <td>[0.05637671244260656, 0.26188954412734744, 0.1...</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>19</td>\n","      <td>63</td>\n","      <td>[30, 14, 6, 3, 5, 6, 5, 2, 11, 9, 9, 45, 5, 9,...</td>\n","      <td>[96, 43, 12, 5, 22, 13, 22, 3, 31, 29, 26, 134...</td>\n","      <td>[0.17334992894139045, 0.883403092448823, 0.933...</td>\n","      <td>[0.2423479210589905, 0.7296242799474274, 0.335...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   item_id-count  session_id  \\\n","0             22           7   \n","1             20          17   \n","2             19          45   \n","4             19          62   \n","5             19          63   \n","\n","                                  category-list_trim  \\\n","0  [6, 9, 5, 2, 7, 8, 5, 2, 6, 16, 2, 10, 35, 5, ...   \n","1  [3, 3, 3, 2, 29, 3, 4, 3, 17, 2, 21, 16, 8, 4,...   \n","2  [4, 34, 9, 3, 4, 11, 3, 7, 7, 6, 3, 5, 20, 8, ...   \n","4  [10, 7, 8, 4, 26, 27, 5, 13, 6, 2, 9, 8, 3, 11...   \n","5  [30, 14, 6, 3, 5, 6, 5, 2, 11, 9, 9, 45, 5, 9,...   \n","\n","                                   item_id-list_trim  \\\n","0  [15, 27, 10, 3, 17, 19, 10, 3, 13, 53, 3, 25, ...   \n","1  [5, 8, 8, 4, 92, 5, 7, 5, 47, 3, 62, 52, 20, 1...   \n","2  [7, 95, 26, 2, 7, 32, 5, 17, 17, 15, 5, 10, 59...   \n","4  [25, 17, 19, 7, 75, 81, 10, 37, 12, 3, 29, 19,...   \n","5  [96, 43, 12, 5, 22, 13, 22, 3, 31, 29, 26, 134...   \n","\n","                        timestamp/age_days-list_trim  \\\n","0  [0.05018395805258469, 0.3675245026471312, 0.45...   \n","1  [0.6514417831915809, 0.4076816703281344, 0.632...   \n","2  [0.8375365213796201, 0.5405179079133022, 0.779...   \n","4  [0.4649937741449487, 0.5034045853366875, 0.566...   \n","5  [0.17334992894139045, 0.883403092448823, 0.933...   \n","\n","                     timestamp/weekday/sin-list_trim  \n","0  [0.4569657788661693, 0.3016987134228405, 0.444...  \n","1  [0.14429839204039463, 0.9164664830523597, 0.28...  \n","2  [0.7202554739875682, 0.22750431643945657, 0.22...  \n","4  [0.05637671244260656, 0.26188954412734744, 0.1...  \n","5  [0.2423479210589905, 0.7296242799474274, 0.335...  "]},"execution_count":12,"metadata":{},"output_type":"execute_result"}]},{"cell_type":"markdown","metadata":{"id":"a6461a96"},"source":["We have created session-level features to train a session-based recommendation model using NVTabular. Now we will train a session-based recommendation model using [XLNet](https://arxiv.org/abs/1906.08237), one of the state-of-the-art NLP model."],"id":"a6461a96"},{"cell_type":"markdown","metadata":{"id":"ab881baf"},"source":["Next, we will learn:\n","\n","- Accelerating data loading of parquet files with multiple features on PyTorch using NVTabular library\n","- Training and evaluating a Transformer-based (XLNET-MLM) session-based recommendation model with multiple features"],"id":"ab881baf"},{"cell_type":"markdown","metadata":{"id":"3b80af76"},"source":["Transformers4Rec library relies on a schema object to automatically build all necessary layers to represent, normalize and aggregate input features. As you can see below, `schema.pb` is a protobuf file that contains metadata including statistics about features such as cardinality, min and max values and also tags features based on their characteristics and dtypes (e.g., categorical, continuous, list, integer)."],"id":"3b80af76"},{"cell_type":"markdown","metadata":{"id":"24ca5c73"},"source":["## Manually set the schema "],"id":"24ca5c73"},{"cell_type":"code","metadata":{"id":"6b6a1f17","outputId":"0010ec27-8dff-4194-c154-0d959526acaf"},"source":["from merlin_standard_lib import Schema\n","SCHEMA_PATH = \"schema.pb\"\n","schema = Schema().from_proto_text(SCHEMA_PATH)\n","!cat $SCHEMA_PATH"],"id":"6b6a1f17","execution_count":null,"outputs":[{"name":"stdout","output_type":"stream","text":["feature {\n","  name: \"session_id\"\n","  type: INT\n","  int_domain {\n","    name: \"session_id\"\n","    min: 1\n","    max: 100001\n","    is_categorical: false\n","  }\n","  annotation {\n","    tag: \"groupby_col\"\n","  }\n","}\n","feature {\n","  name: \"category-list_trim\"\n","  value_count {\n","    min: 2\n","    max: 20\n","  }\n","  type: INT\n","  int_domain {\n","    name: \"category-list_trim\"\n","    min: 1\n","    max: 400\n","    is_categorical: true\n","  }\n","  annotation {\n","    tag: \"list\"\n","    tag: \"categorical\"\n","    tag: \"item\"\n","  }\n","}\n","feature {\n","  name: \"item_id-list_trim\"\n","  value_count {\n","    min: 2\n","    max: 20\n","  }\n","  type: INT\n","  int_domain {\n","    name: \"item_id/list\"\n","    min: 1\n","    max: 50005\n","    is_categorical: true\n","  }\n","  annotation {\n","    tag: \"item_id\"\n","    tag: \"list\"\n","    tag: \"categorical\"\n","    tag: \"item\"\n","  }\n","}\n","feature {\n","  name: \"timestamp/age_days-list_trim\"\n","  value_count {\n","    min: 2\n","    max: 20\n","  }\n","  type: FLOAT\n","  float_domain {\n","    name: \"timestamp/age_days-list_trim\"\n","    min: 0.0000003\n","    max: 0.9999999\n","  }\n","  annotation {\n","    tag: \"continuous\"\n","    tag: \"list\"\n","  }\n","}\n","feature {\n","  name: \"timestamp/weekday/sin-list_trim\"\n","  value_count {\n","    min: 2\n","    max: 20\n","  }\n","  type: FLOAT\n","  float_domain {\n","    name: \"timestamp/weekday-sin_trim\"\n","    min: 0.0000003\n","    max: 0.9999999\n","  }\n","  annotation {\n","    tag: \"time\"\n","    tag: \"list\"\n","  }\n","}"]}]},{"cell_type":"code","metadata":{"id":"5fcf0717"},"source":["# You can select a subset of features for training\n","schema = schema.select_by_name(['item_id-list_trim', \n","                                'category-list_trim', \n","                                'timestamp/weekday/sin-list_trim',\n","                                'timestamp/age_days-list_trim'])"],"id":"5fcf0717","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"18148079"},"source":["## Define the sequential input module"],"id":"18148079"},{"cell_type":"markdown","metadata":{"id":"985341a3"},"source":["Below we define our `input` block using the `TabularSequenceFeatures` [class](https://github.com/NVIDIA-Merlin/Transformers4Rec/blob/main/transformers4rec/torch/features/sequence.py#L91). The `from_schema()` method processes the schema and creates the necessary layers to represent features and aggregate them. It keeps only features tagged as `categorical` and `continuous` and supports data aggregation methods like `concat` and `elementwise-sum` techniques. It also support data augmentation techniques like stochastic swap noise. It outputs an interaction representation after combining all features and also the input mask according to the training task (more on this later).\n"],"id":"985341a3"},{"cell_type":"markdown","metadata":{"id":"8d0532bc"},"source":["The `max_sequence_length` argument defines the maximum sequence length of our sequential input, and if `continuous_projection` argument is set, all numerical features are concatenated and projected by an MLP block so that continuous features are represented by a vector of size defined by user, which is `64` in this example."],"id":"8d0532bc"},{"cell_type":"code","metadata":{"id":"0b2d13d0"},"source":["inputs = tr.TabularSequenceFeatures.from_schema(\n","        schema,\n","        max_sequence_length=20,\n","        continuous_projection=64,\n","        d_output=100,\n","        masking=\"mlm\",\n",")"],"id":"0b2d13d0","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"54260cd3"},"source":["The output of the `TabularSequenceFeatures` module is the sequence of interactions embeddings vectors defined in the following steps:\n","- 1. Create sequence inputs: If the schema contains non sequential features, expand each feature to a sequence by repeating the value as many as the `max_sequence_length` value.  \n","- 2. Get a representation vector of categorical features: Project each sequential categorical feature using the related embedding table. The resulting tensor is of shape (bs, max_sequence_length, embed_dim).\n","- 3. Project scalar values if `continuous_projection` is set : Apply an MLP layer with hidden size equal to `continuous_projection` vector size value. The resulting tensor is of shape (batch_size, max_sequence_length, continuous_projection).\n","- 4. Aggregate the list of features vectors to represent each interaction in the sequence with one vector: For example, `concat` will concat all vectors based on the last dimension `-1` and the resulting tensor will be of shape (batch_size, max_sequence_length, D) where D is the sum over all embedding dimensions and the value of continuous_projection. \n","- 5. If masking schema is set (needed only for the NextItemPredictionTask training), the masked labels are derived from the sequence of raw item-ids and the sequence of interactions embeddings are processed to mask information about the masked positions."],"id":"54260cd3"},{"cell_type":"markdown","metadata":{"id":"03933343"},"source":["## Define the Transformer Block"],"id":"03933343"},{"cell_type":"markdown","metadata":{"id":"3b661085"},"source":["In the next cell, the whole model is build with a few lines of code. \n","Here is a brief explanation of the main classes:  \n","- [XLNetConfig](https://github.com/NVIDIA-Merlin/Transformers4Rec/blob/main/transformers4rec/config/transformer.py#L261) - We have injected in the HF transformers config classes like `XLNetConfig`the `build()` method, that provides default configuration to Transformer architectures for session-based recommendation. Here we use it to instantiate and configure an XLNET architecture.  \n","- [TransformerBlock](https://github.com/NVIDIA-Merlin/Transformers4Rec/blob/main/transformers4rec/torch/block/transformer.py#L37) class integrates with HF Transformers, which are made available as a sequence processing module for session-based and sequential-based recommendation models.  \n","- [NextItemPredictionTask](https://github.com/NVIDIA-Merlin/Transformers4Rec/blob/main/transformers4rec/torch/model/head.py#L238) supports the next-item prediction task. We also support other predictions [tasks](https://github.com/NVIDIA-Merlin/Transformers4Rec/blob/main/transformers4rec/torch/model/head.py), like classification and regression for the whole sequence. "],"id":"3b661085"},{"cell_type":"code","metadata":{"id":"e0b19550"},"source":["# Define XLNetConfig class and set default parameters for HF XLNet config  \n","transformer_config = tr.XLNetConfig.build(\n","    d_model=64, n_head=4, n_layer=2, total_seq_length=20\n",")\n","# Define the model block including: inputs, masking, projection and transformer block.\n","body = tr.SequentialBlock(\n","    inputs, tr.MLPBlock([64]), tr.TransformerBlock(transformer_config, masking=inputs.masking)\n",")\n","\n","# Defines the evaluation top-N metrics and the cut-offs\n","metrics = [NDCGAt(top_ks=[20, 40], labels_onehot=True),  \n","           RecallAt(top_ks=[20, 40], labels_onehot=True)]\n","\n","# Define a head related to next item prediction task \n","head = tr.Head(\n","    body,\n","    tr.NextItemPredictionTask(weight_tying=True, hf_format=True, \n","                              metrics=metrics),\n","    inputs=inputs,\n",")\n","\n","# Get the end-to-end Model class \n","model = tr.Model(head)"],"id":"e0b19550","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6ea528f2"},"source":["Note that we can easily define an RNN-based model inside the `SequentialBlock` instead of a Transformer-based model. You can explore this [tutorial](https://github.com/NVIDIA-Merlin/Transformers4Rec/tree/main/examples/tutorial) for a GRU-based model example."],"id":"6ea528f2"},{"cell_type":"markdown","metadata":{"id":"ff2ceded"},"source":["## Train the model "],"id":"ff2ceded"},{"cell_type":"markdown","metadata":{"id":"f43212e3"},"source":["We use the NVTabular PyTorch Dataloader for optimized loading of multiple features from input parquet files. You can learn more about this data loader [here](https://nvidia-merlin.github.io/NVTabular/main/training/pytorch.html)."],"id":"f43212e3"},{"cell_type":"markdown","metadata":{"id":"4c714512"},"source":["## Set Training arguments"],"id":"4c714512"},{"cell_type":"code","metadata":{"id":"88cfd979"},"source":["from transformers4rec.config.trainer import T4RecTrainingArguments\n","from transformers4rec.torch import Trainer\n","# Set hyperparameters for training \n","train_args = T4RecTrainingArguments(data_loader_engine='nvtabular', \n","                                    dataloader_drop_last = True,\n","                                    report_to = [], \n","                                    gradient_accumulation_steps = 1,\n","                                    per_device_train_batch_size = 256, \n","                                    per_device_eval_batch_size = 32,\n","                                    output_dir = \"./tmp\", \n","                                    learning_rate=0.0005,\n","                                    lr_scheduler_type='cosine', \n","                                    learning_rate_num_cosine_cycles_by_epoch=1.5,\n","                                    num_train_epochs=5,\n","                                    max_sequence_length=20, \n","                                    no_cuda=False)"],"id":"88cfd979","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3b20ca71"},"source":["Note that we add an argument `data_loader_engine='nvtabular'` to automatically load the features needed for training using the schema. The default value is nvtabular for optimized GPU-based data-loading. Optionally a PyarrowDataLoader (pyarrow) can also be used as a basic option, but it is slower and works only for small datasets, as the full data is loaded to CPU memory."],"id":"3b20ca71"},{"cell_type":"markdown","metadata":{"id":"41277491"},"source":["## Daily Fine-Tuning: Training over a time window"],"id":"41277491"},{"cell_type":"markdown","metadata":{"id":"061a162b"},"source":["Here we do daily fine-tuning meaning that we use the first day to train and second day to evaluate, then we use the second day data to train the model by resuming from the first step, and evaluate on the third day, so on so forth."],"id":"061a162b"},{"cell_type":"markdown","metadata":{"id":"612a5034"},"source":["We have extended the HuggingFace transformers `Trainer` class (PyTorch only) to support evaluation of RecSys metrics. In this example, the evaluation of the session-based recommendation model is performed using traditional Top-N ranking metrics such as Normalized Discounted Cumulative Gain (NDCG@20) and Hit Rate (HR@20). NDCG accounts for rank of the relevant item in the recommendation list and is a more fine-grained metric than HR, which only verifies whether the relevant item is among the top-n items. HR@n is equivalent to Recall@n when there is only one relevant item in the recommendation list."],"id":"612a5034"},{"cell_type":"code","metadata":{"id":"5880612b"},"source":["# Instantiate the T4Rec Trainer, which manages training and evaluation for the PyTorch API\n","trainer = Trainer(\n","    model=model,\n","    args=train_args,\n","    schema=schema,\n","    compute_metrics=True,\n",")"],"id":"5880612b","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5e839647"},"source":["- Define the output folder of the processed parquet files"],"id":"5e839647"},{"cell_type":"code","metadata":{"id":"9ffbc397"},"source":["OUTPUT_DIR = os.environ.get(\"OUTPUT_DIR\", \"/workspace/data/sessions_by_day\")"],"id":"9ffbc397","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1d5e98ae","outputId":"cf50855f-b7b5-4e99-d0c8-5fb501f13212"},"source":["start_time_window_index = 1\n","final_time_window_index = 7\n","#Iterating over days of one week\n","for time_index in range(start_time_window_index, final_time_window_index):\n","    # Set data \n","    time_index_train = time_index\n","    time_index_eval = time_index + 1\n","    train_paths = glob.glob(os.path.join(OUTPUT_DIR, f\"{time_index_train}/train.parquet\"))\n","    eval_paths = glob.glob(os.path.join(OUTPUT_DIR, f\"{time_index_eval}/valid.parquet\"))\n","    print(train_paths)\n","    \n","    # Train on day related to time_index \n","    print('*'*20)\n","    print(\"Launch training for day %s are:\" %time_index)\n","    print('*'*20 + '\\n')\n","    trainer.train_dataset_or_path = train_paths\n","    trainer.reset_lr_scheduler()\n","    trainer.train()\n","    trainer.state.global_step +=1\n","    print('finished')\n","    \n","    # Evaluate on the following day\n","    trainer.eval_dataset_or_path = eval_paths\n","    train_metrics = trainer.evaluate(metric_key_prefix='eval')\n","    print('*'*20)\n","    print(\"Eval results for day %s are:\\t\" %time_index_eval)\n","    print('\\n' + '*'*20 + '\\n')\n","    for key in sorted(train_metrics.keys()):\n","        print(\" %s = %s\" % (key, str(train_metrics[key]))) \n","    trainer.wipe_memory()"],"id":"1d5e98ae","execution_count":null,"outputs":[{"name":"stdout","output_type":"stream","text":["['/workspace/data/sessions_by_day/1/train.parquet']\n","********************\n","Launch training for day 1 are:\n","********************\n","\n"]},{"name":"stderr","output_type":"stream","text":["***** Running training *****\n","  Num examples = 768\n","  Num Epochs = 5\n","  Instantaneous batch size per device = 256\n","  Total train batch size (w. parallel, distributed & accumulation) = 1024\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 15\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='15' max='15' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [15/15 00:00, Epoch 5/5]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n"]},{"name":"stdout","output_type":"stream","text":["finished\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='21' max='3' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [3/3 00:05]\n","    </div>\n","    "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["********************\n","Eval results for day 2 are:\t\n","\n","********************\n","\n"," epoch = 5.0\n"," eval/loss = 10.33902645111084\n"," eval/next-item/ndcg_at_20 = 0.0209574606269598\n"," eval/next-item/ndcg_at_40 = 0.02929079346358776\n"," eval/next-item/recall_at_20 = 0.0520833358168602\n"," eval/next-item/recall_at_40 = 0.09375\n"," eval_runtime = 0.0882\n"," eval_samples_per_second = 1088.723\n"," eval_steps_per_second = 11.341\n"]},{"name":"stderr","output_type":"stream","text":["***** Running training *****\n","  Num examples = 768\n","  Num Epochs = 5\n","  Instantaneous batch size per device = 256\n","  Total train batch size (w. parallel, distributed & accumulation) = 1024\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 15\n"]},{"name":"stdout","output_type":"stream","text":["['/workspace/data/sessions_by_day/2/train.parquet']\n","********************\n","Launch training for day 2 are:\n","********************\n","\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='15' max='15' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [15/15 00:00, Epoch 5/5]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n"]},{"name":"stdout","output_type":"stream","text":["finished\n","********************\n","Eval results for day 3 are:\t\n","\n","********************\n","\n"," epoch = 5.0\n"," eval/loss = 9.9208345413208\n"," eval/next-item/ndcg_at_20 = 0.053623538464307785\n"," eval/next-item/ndcg_at_40 = 0.08480535447597504\n"," eval/next-item/recall_at_20 = 0.1354166716337204\n"," eval/next-item/recall_at_40 = 0.28125\n"," eval_runtime = 0.0886\n"," eval_samples_per_second = 1083.283\n"," eval_steps_per_second = 11.284\n"]},{"name":"stderr","output_type":"stream","text":["***** Running training *****\n","  Num examples = 768\n","  Num Epochs = 5\n","  Instantaneous batch size per device = 256\n","  Total train batch size (w. parallel, distributed & accumulation) = 1024\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 15\n"]},{"name":"stdout","output_type":"stream","text":["['/workspace/data/sessions_by_day/3/train.parquet']\n","********************\n","Launch training for day 3 are:\n","********************\n","\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='15' max='15' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [15/15 00:00, Epoch 5/5]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n"]},{"name":"stdout","output_type":"stream","text":["finished\n","********************\n","Eval results for day 4 are:\t\n","\n","********************\n","\n"," epoch = 5.0\n"," eval/loss = 9.478459358215332\n"," eval/next-item/ndcg_at_20 = 0.08707290887832642\n"," eval/next-item/ndcg_at_40 = 0.11134807765483856\n"," eval/next-item/recall_at_20 = 0.2291666716337204\n"," eval/next-item/recall_at_40 = 0.34375\n"," eval_runtime = 0.0888\n"," eval_samples_per_second = 1080.702\n"," eval_steps_per_second = 11.257\n"]},{"name":"stderr","output_type":"stream","text":["***** Running training *****\n","  Num examples = 768\n","  Num Epochs = 5\n","  Instantaneous batch size per device = 256\n"]},{"name":"stdout","output_type":"stream","text":["['/workspace/data/sessions_by_day/4/train.parquet']\n","********************\n","Launch training for day 4 are:\n","********************\n","\n"]},{"name":"stderr","output_type":"stream","text":["  Total train batch size (w. parallel, distributed & accumulation) = 1024\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 15\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='15' max='15' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [15/15 00:00, Epoch 5/5]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n"]},{"name":"stdout","output_type":"stream","text":["finished\n","********************\n","Eval results for day 5 are:\t\n","\n","********************\n","\n"," epoch = 5.0\n"," eval/loss = 8.747222900390625\n"," eval/next-item/ndcg_at_20 = 0.13623034954071045\n"," eval/next-item/ndcg_at_40 = 0.179062157869339\n"," eval/next-item/recall_at_20 = 0.34375\n"," eval/next-item/recall_at_40 = 0.5520833730697632\n"," eval_runtime = 0.0947\n"," eval_samples_per_second = 1013.663\n"," eval_steps_per_second = 10.559\n","['/workspace/data/sessions_by_day/5/train.parquet']\n","********************\n","Launch training for day 5 are:\n","********************\n","\n"]},{"name":"stderr","output_type":"stream","text":["***** Running training *****\n","  Num examples = 768\n","  Num Epochs = 5\n","  Instantaneous batch size per device = 256\n","  Total train batch size (w. parallel, distributed & accumulation) = 1024\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 15\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='15' max='15' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [15/15 00:00, Epoch 5/5]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n"]},{"name":"stdout","output_type":"stream","text":["finished\n","********************\n","Eval results for day 6 are:\t\n","\n","********************\n","\n"," epoch = 5.0\n"," eval/loss = 8.153544425964355\n"," eval/next-item/ndcg_at_20 = 0.13773086667060852\n"," eval/next-item/ndcg_at_40 = 0.18069738149642944\n"," eval/next-item/recall_at_20 = 0.3541666865348816\n"," eval/next-item/recall_at_40 = 0.5625\n"," eval_runtime = 0.0944\n"," eval_samples_per_second = 1017.132\n"," eval_steps_per_second = 10.595\n"]},{"name":"stderr","output_type":"stream","text":["***** Running training *****\n","  Num examples = 768\n"]},{"name":"stdout","output_type":"stream","text":["['/workspace/data/sessions_by_day/6/train.parquet']\n","********************\n","Launch training for day 6 are:\n","********************\n","\n"]},{"name":"stderr","output_type":"stream","text":["  Num Epochs = 5\n","  Instantaneous batch size per device = 256\n","  Total train batch size (w. parallel, distributed & accumulation) = 1024\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 15\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='15' max='15' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [15/15 00:00, Epoch 5/5]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n"]},{"name":"stdout","output_type":"stream","text":["finished\n","********************\n","Eval results for day 7 are:\t\n","\n","********************\n","\n"," epoch = 5.0\n"," eval/loss = 7.682921886444092\n"," eval/next-item/ndcg_at_20 = 0.16451486945152283\n"," eval/next-item/ndcg_at_40 = 0.19865691661834717\n"," eval/next-item/recall_at_20 = 0.3958333432674408\n"," eval/next-item/recall_at_40 = 0.5625\n"," eval_runtime = 0.091\n"," eval_samples_per_second = 1055.176\n"," eval_steps_per_second = 10.991\n"]}]},{"cell_type":"markdown","metadata":{"id":"8af6146d"},"source":["## Saves the model"],"id":"8af6146d"},{"cell_type":"code","metadata":{"id":"3f03cd91","outputId":"e604fd31-ae4a-45c3-d931-d60ebe7e609d"},"source":["trainer._save_model_and_checkpoint(save_model_class=True)"],"id":"3f03cd91","execution_count":null,"outputs":[{"name":"stderr","output_type":"stream","text":["Saving model checkpoint to ./tmp/checkpoint-16\n","Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n"]}]},{"cell_type":"markdown","metadata":{"id":"12ba5fc4"},"source":["## Reloads the model"],"id":"12ba5fc4"},{"cell_type":"code","metadata":{"id":"562ad5a2"},"source":["trainer.load_model_trainer_states_from_checkpoint('./tmp/checkpoint-%s'%trainer.state.global_step)"],"id":"562ad5a2","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"cbd0c5fd"},"source":["## Re-compute eval metrics of validation data"],"id":"cbd0c5fd"},{"cell_type":"code","metadata":{"id":"dc39a93a"},"source":["eval_data_paths = glob.glob(os.path.join(OUTPUT_DIR, f\"{time_index_eval}/valid.parquet\"))"],"id":"dc39a93a","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"70b9ba80","outputId":"121af87e-91bb-481d-b16d-ef5493b2e9c7"},"source":["# set new data from day 7\n","eval_metrics = trainer.evaluate(eval_dataset=eval_data_paths, metric_key_prefix='eval')\n","for key in sorted(eval_metrics.keys()):\n","    print(\"  %s = %s\" % (key, str(eval_metrics[key])))"],"id":"70b9ba80","execution_count":null,"outputs":[{"name":"stdout","output_type":"stream","text":["  epoch = 5.0\n","  eval/loss = 7.682921886444092\n","  eval/next-item/ndcg_at_20 = 0.16451486945152283\n","  eval/next-item/ndcg_at_40 = 0.19865691661834717\n","  eval/next-item/recall_at_20 = 0.3958333432674408\n","  eval/next-item/recall_at_40 = 0.5625\n","  eval_runtime = 0.1\n","  eval_samples_per_second = 960.363\n","  eval_steps_per_second = 10.004\n"]}]},{"cell_type":"markdown","metadata":{"id":"d8826ea0"},"source":["That's it!  \n","You have just trained your session-based recommendation model using Transformers4Rec."],"id":"d8826ea0"},{"cell_type":"markdown","metadata":{"id":"0c1e2394"},"source":["Tip: We can easily log and visualize model training and evaluation on [Weights & Biases (W&B)](https://wandb.ai/home), [Tensorboard](https://www.tensorflow.org/tensorboard) and [NVIDIA DLLogger](https://github.com/NVIDIA/dllogger). By default, the HuggingFace transformers `Trainer` (which we extend) uses Weights & Biases (W&B) to log training and evaluation metrics, which provides nice results visualization and comparison between different runs."],"id":"0c1e2394"}]}