{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"2022-01-24-stamp-diginetica.ipynb","provenance":[{"file_id":"https://github.com/recohut/nbs/blob/main/raw/T266819%20%7C%20STAMP%20on%20Diginetica%20in%20TF%202x.ipynb","timestamp":1644669104788}],"collapsed_sections":[],"authorship_tag":"ABX9TyNUGxGNdxLK0jzqQfHPCCMk"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# STAMP on Diginetica in TF 2.x"],"metadata":{"id":"oMU14qbn6tWx"}},{"cell_type":"code","metadata":{"id":"N-C8WVmWyt1c"},"source":["!pip install tensorflow==2.5.0\n","!pip install tensorflow-gpu==2.5.0"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"R3QiS0AHzPNr"},"source":["!git clone https://github.com/RecoHut-Datasets/diginetica.git"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"tOgRfWKuzGx3"},"source":["import os\n","import pandas as pd\n","import numpy as np\n","import random\n","from tqdm import tqdm\n","from sklearn.preprocessing import LabelEncoder\n","from time import time\n","\n","import tensorflow as tf\n","from tensorflow.keras.regularizers import l2\n","from tensorflow.keras.losses import Loss\n","from tensorflow.keras.losses import binary_crossentropy\n","from tensorflow.keras.optimizers import Adam\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from tensorflow.keras.layers import Layer, Dense, LayerNormalization, \\\n","    Dropout, Embedding, Flatten, Input"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5csalgjaBO6a"},"source":["os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n","os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n","\n","file = 'diginetica/train-item-views.csv'\n","maxlen = 8\n","\n","embed_dim = 20\n","K = 20\n","\n","learning_rate = 0.005\n","batch_size = 1024\n","epochs = 10"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qTaga32Y39Kl"},"source":["def sparseFeature(feat, feat_num, embed_dim=4):\n","    \"\"\"\n","    create dictionary for sparse feature\n","    :param feat: feature name\n","    :param feat_num: the total number of sparse features that do not repeat\n","    :param embed_dim: embedding dimension\n","    :return:\n","    \"\"\"\n","    return {'feat': feat, 'feat_num': feat_num, 'embed_dim': embed_dim}"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0tA7dlQVA727"},"source":["def denseFeature(feat):\n","    \"\"\"\n","    create dictionary for dense feature\n","    :param feat: dense feature name\n","    :return:\n","    \"\"\"\n","    return {'feat': feat}"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8xyT05c-A6lF"},"source":["def convert_sequence(data_df):\n","    \"\"\"\n","    :param data_df: train, val or test\n","    \"\"\"\n","    data_sequence = []\n","    for sessionId, df in tqdm(data_df[['sessionId', 'itemId']].groupby(['sessionId'])):\n","        item_list = df['itemId'].tolist()\n","\n","        for i in range(1, len(item_list)):\n","            hist_i = item_list[:i]\n","            # hist_item, next_click_item(label)\n","            data_sequence.append([hist_i, item_list[i]])\n","\n","    return data_sequence"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZQD91xJPA5TV"},"source":["def create_diginetica_dataset(file, embed_dim=8, maxlen=40):\n","    \"\"\"\n","    :param file: A string. dataset path\n","    :param embed_dim: A scalar. latent factor\n","    :param maxlen: A scalar. \n","    :return: feature_columns, behavior_list, train, val, test\n","    \"\"\"\n","    print('==========Data Preprocess Start============')\n","    # load dataset\n","    data_df = pd.read_csv(file, sep=\";\") # (1235380, 5)\n","    \n","    # filter out sessions of length of 1\n","    data_df['session_count'] = data_df.groupby('sessionId')['sessionId'].transform('count')\n","    data_df = data_df[data_df.session_count > 1]  # (1144686, 6)\n","\n","    # filter out items that appear less than 5 times\n","    data_df['item_count'] = data_df.groupby('itemId')['itemId'].transform('count')\n","    data_df = data_df[data_df.item_count >= 5]  # (1004834, 7)\n","\n","    # label encoder itemId, {0, 1, ..., }\n","    le = LabelEncoder()\n","    data_df['itemId'] = le.fit_transform(data_df['itemId'])\n","    \n","     # sorted by eventdate, sessionId\n","    data_df = data_df.sort_values(by=['eventdate', 'sessionId'])\n","\n","    # split dataset, 1 day for valdation, 7 days for test\n","    train = data_df[data_df.eventdate < '2016-05-25']  # (916485, 7)\n","    val = data_df[data_df.eventdate == '2016-05-25']  # (10400, 7)\n","    test = data_df[data_df.eventdate > '2016-05-25']  # (77949, 7)\n","\n","    # convert sequence\n","    train = pd.DataFrame(convert_sequence(train), columns=['hist', 'label'])\n","    val = pd.DataFrame(convert_sequence(val), columns=['hist', 'label'])\n","    test = pd.DataFrame(convert_sequence(test), columns=['hist', 'label'])\n","    \n","    # Padding\n","    # not have dense inputs and other sparse inputs\n","    print('==================Padding===================')\n","    train_X = [np.array([0.] * len(train)), np.array([0] * len(train)),\n","               np.expand_dims(pad_sequences(train['hist'], maxlen=maxlen), axis=1)]\n","    train_y = train['label'].values\n","    val_X = [np.array([0] * len(val)), np.array([0] * len(val)),\n","               np.expand_dims(pad_sequences(val['hist'], maxlen=maxlen), axis=1)]\n","    val_y = val['label'].values\n","    test_X = [np.array([0] * len(test)), np.array([0] * len(test)),\n","               np.expand_dims(pad_sequences(test['hist'], maxlen=maxlen), axis=1)]\n","    test_y = test['label'].values\n","\n","    # item pooling\n","    item_pooling = np.sort(data_df['itemId'].unique().reshape(-1, 1), axis=0)\n","\n","    # feature columns, dense feature columns + sparse feature columns\n","    item_num = data_df['itemId'].max() + 1\n","    feature_columns = [[],\n","                       [sparseFeature('item_id', item_num, embed_dim)]]\n","\n","    # behavior list\n","    behavior_list = ['item_id']\n","\n","    print('===========Data Preprocess End=============')\n","    \n","    return feature_columns, behavior_list, item_pooling, (train_X, train_y), (val_X, val_y), (test_X, test_y)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"tcmDAMAn_6My"},"source":["class Attention_Layer(Layer):\n","    \"\"\"\n","    Attention Layer\n","    \"\"\"\n","    def __init__(self, d, reg=1e-4):\n","        \"\"\"\n","        :param d: A scalar. The dimension of embedding.\n","        :param reg: A scalar. The regularizer of parameters\n","        \"\"\"\n","        self.d = d\n","        self.reg = reg\n","        super(Attention_Layer, self).__init__()\n","\n","    def build(self, input_shape):\n","        self.W0 = self.add_weight(name='W0',\n","                                  shape=(self.d, 1),\n","                                  initializer=tf.random_normal_initializer,\n","                                  regularizer=l2(self.reg),\n","                                  trainable=True)\n","        self.W1 = self.add_weight(name='W1',\n","                                  shape=(self.d, self.d),\n","                                  initializer=tf.random_normal_initializer,\n","                                  regularizer=l2(self.reg),\n","                                  trainable=True)\n","        self.W2 = self.add_weight(name='W2',\n","                                  shape=(self.d, self.d),\n","                                  initializer=tf.random_normal_initializer,\n","                                  regularizer=l2(self.reg),\n","                                  trainable=True)\n","        self.W3 = self.add_weight(name='W3',\n","                                  shape=(self.d, self.d),\n","                                  initializer=tf.random_normal_initializer,\n","                                  regularizer=l2(self.reg),\n","                                  trainable=True)\n","        self.b = self.add_weight(name='b',\n","                                  shape=(self.d,),\n","                                  initializer=tf.random_normal_initializer,\n","                                  regularizer=l2(self.reg),\n","                                  trainable=True)\n","\n","    def call(self, inputs):\n","        seq_embed, m_s, x_t = inputs\n","        \"\"\"\n","        seq_embed: (None, seq_len, d)\n","        W1: (d, d)\n","        x_t: (None, d)\n","        W2: (d, d)\n","        m_s: (None, d)\n","        W3: (d, d)\n","        W0: (d, 1)\n","        \"\"\"\n","        alpha = tf.matmul(tf.nn.sigmoid(\n","            tf.tensordot(seq_embed, self.W1, axes=[2, 0]) + tf.expand_dims(tf.matmul(x_t, self.W2), axis=1) +\n","            tf.expand_dims(tf.matmul(m_s, self.W3), axis=1) + self.b), self.W0)\n","        m_a = tf.reduce_sum(tf.multiply(alpha, seq_embed), axis=1)  # (None, d)\n","        return m_a"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"CRnDzYLGByvm"},"source":["class STAMP(tf.keras.Model):\n","    def __init__(self, feature_columns, behavior_feature_list, item_pooling, maxlen=40, activation='tanh', embed_reg=1e-4):\n","        \"\"\"\n","        STAMP\n","        :param feature_columns: A list. dense_feature_columns + sparse_feature_columns\n","        :param behavior_feature_list: A list. the list of behavior feature names\n","        :param item_pooling: A Ndarray or Tensor, shape=(m, n),\n","        m is the number of items, and n is the number of behavior feature. The item pooling.\n","        :param activation: A String. The activation of FFN.\n","        :param maxlen: A scalar. Maximum sequence length.\n","        :param embed_reg: A scalar. The regularizer of embedding.\n","        \"\"\"\n","        super(STAMP, self).__init__()\n","        # maximum sequence length\n","        self.maxlen = maxlen\n","\n","        # item pooling\n","        self.item_pooling = item_pooling\n","        self.dense_feature_columns, self.sparse_feature_columns = feature_columns\n","\n","        # len\n","        self.other_sparse_len = len(self.sparse_feature_columns) - len(behavior_feature_list)\n","        self.dense_len = len(self.dense_feature_columns)\n","        # if behavior feature list contains itemId and item category id, seq_len = 2\n","        self.seq_len = len(behavior_feature_list)\n","\n","        # embedding dim, each sparse feature embedding dimension is the same\n","        self.embed_dim = self.sparse_feature_columns[0]['embed_dim']\n","\n","        # other embedding layers\n","        self.embed_sparse_layers = [Embedding(input_dim=feat['feat_num'],\n","                                              input_length=1,\n","                                              output_dim=feat['embed_dim'],\n","                                              embeddings_initializer='random_uniform',\n","                                              embeddings_regularizer=l2(embed_reg))\n","                                    for feat in self.sparse_feature_columns\n","                                    if feat['feat'] not in behavior_feature_list]\n","        # behavior embedding layers\n","        self.embed_seq_layers = [Embedding(input_dim=feat['feat_num'],\n","                                           input_length=1,\n","                                           output_dim=feat['embed_dim'],\n","                                           embeddings_initializer='random_uniform',\n","                                           embeddings_regularizer=l2(embed_reg))\n","                                 for feat in self.sparse_feature_columns\n","                                 if feat['feat'] in behavior_feature_list]\n","\n","        # Attention\n","        self.attention_layer = Attention_Layer(d=self.embed_dim)\n","\n","        # FNN, hidden unit must be equal to embedding dimension\n","        self.ffn1 = Dense(self.embed_dim, activation=activation)\n","        self.ffn2 = Dense(self.embed_dim, activation=activation)\n","\n","    def call(self, inputs):\n","        # dense_inputs and sparse_inputs is empty\n","        dense_inputs, sparse_inputs, seq_inputs = inputs\n","        \n","        x = dense_inputs\n","        # other\n","        for i in range(self.other_sparse_len):\n","            x = tf.concat([x, self.embed_sparse_layers[i](sparse_inputs[:, i])], axis=-1)\n","\n","        # seq\n","        seq_embed, m_t, item_pooling_embed = None, None, None\n","        for i in range(self.seq_len):\n","            # item sequence embedding\n","            seq_embed = self.embed_seq_layers[i](seq_inputs[:, i]) if seq_embed is None \\\n","                else seq_embed + self.embed_seq_layers[i](seq_inputs[:, i])\n","            # last click item embedding\n","            m_t = self.embed_seq_layers[i](seq_inputs[:, i, -1]) if m_t is None \\\n","                else m_t + self.embed_seq_layers[i](seq_inputs[-1, i, -1])  # (None, d)\n","            # item pooling embedding \n","            item_pooling_embed = self.embed_seq_layers[i](self.item_pooling[:, i]) \\\n","                if item_pooling_embed is None \\\n","                else item_pooling_embed + self.embed_seq_layers[i](self.item_pooling[:, i])  # (m, d)\n","\n","        # calculate m_s        \n","        m_s = tf.reduce_mean(seq_embed, axis=1)  # (None, d)\n","\n","        # attention\n","        m_a = self.attention_layer([seq_embed, m_s, m_t])  # (None, d)\n","        # if model is STMP, m_a = m_s\n","        # m_a = m_s\n","\n","        # try to add other embedding vector\n","        if self.other_sparse_len != 0 or self.dense_len != 0:\n","            m_a = tf.concat([m_a, x], axis=-1)\n","            m_t = tf.concat([m_t, x], axis=-1)\n","\n","        # FFN\n","        h_s = self.ffn1(m_a)  # (None, d)\n","        h_t = self.ffn2(m_t)  # (None, d)\n","\n","        # Calculate\n","        # h_t * item_pooling_embed, (None, 1, d) * (m, d) = (None, m, d)\n","        # () mat h_s, (None, m, d) matmul (None, d, 1) = (None, m, 1)\n","        z = tf.matmul(tf.multiply(tf.expand_dims(h_t, axis=1), item_pooling_embed), tf.expand_dims(h_s, axis=-1))\n","        z = tf.squeeze(z, axis=-1)  # (None, m)\n","\n","        # Outputs\n","        outputs = tf.nn.softmax(z)\n","        return outputs\n","\n","    def summary(self):\n","        dense_inputs = Input(shape=(self.dense_len,), dtype=tf.float32)\n","        sparse_inputs = Input(shape=(self.other_sparse_len,), dtype=tf.int32)\n","        seq_inputs = Input(shape=(self.seq_len, self.maxlen), dtype=tf.int32)\n","        tf.keras.Model(inputs=[dense_inputs, sparse_inputs, seq_inputs],\n","                       outputs=self.call([dense_inputs, sparse_inputs, seq_inputs])).summary()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"UiXY7XvoB0LQ"},"source":["def test_model():\n","    dense_features = []  # [{'feat': 'a'}, {'feat': 'b'}]\n","    sparse_features = [{'feat': 'item_id', 'feat_num': 100, 'embed_dim': 8},\n","                       {'feat': 'cate_id', 'feat_num': 100, 'embed_dim': 8},\n","                       {'feat': 'adv_id', 'feat_num': 100, 'embed_dim': 8}]\n","    behavior_list = ['item_id', 'cate_id']\n","    item_pooling = tf.constant([[1, 1], [2, 2], [3, 3], [4, 4], [5, 5]])\n","    features = [dense_features, sparse_features]\n","    model = STAMP(features, behavior_list, item_pooling)\n","    model.summary()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"bPFJ28sdBF9V"},"source":["def getHit(pred_y, true_y):\n","    \"\"\"\n","    calculate hit rate\n","    :return:\n","    \"\"\"\n","    # reversed\n","    pred_index = np.argsort(-pred_y)[:, :_K]\n","    return sum([true_y[i] in pred_index[i] for i in range(len(pred_index))]) / len(pred_index)\n","\n","\n","def getMRR(pred_y, true_y):\n","    \"\"\"\n","    \"\"\"\n","    pred_index = np.argsort(-pred_y)[:, :_K]\n","    return sum([1 / (np.where(true_y[i] == pred_index[i])[0][0] + 1) \\\n","        for i in range(len(pred_index)) if len(np.where(true_y[i] == pred_index[i])[0]) != 0]) / len(pred_index)\n","\n","\n","def evaluate_model(model, test, K):\n","    \"\"\"\n","    evaluate model\n","    :param model: model\n","    :param test: test set\n","    :param K: top K\n","    :return: hit rate, mrr\n","    \"\"\"\n","    global _K\n","    _K = K\n","    test_X, test_y = test\n","    pred_y = model.predict(test_X)\n","    hit_rate = getHit(pred_y, test_y)\n","    mrr = getMRR(pred_y, test_y)\n","    \n","    \n","    return hit_rate, mrr"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"cVb72bBnBZWh"},"source":["# ========================== Create dataset =======================\n","feature_columns, behavior_list, item_pooling, train, val, test = create_diginetica_dataset(file, embed_dim, maxlen)\n","train_X, train_y = train\n","val_X, val_y = val\n","# ============================Build Model==========================\n","model = STAMP(feature_columns, behavior_list, item_pooling, maxlen)\n","model.summary()\n","# ============================model checkpoint======================\n","# check_path = 'save/sas_weights.epoch_{epoch:04d}.val_loss_{val_loss:.4f}.ckpt'\n","# checkpoint = tf.keras.callbacks.ModelCheckpoint(check_path, save_weights_only=True,\n","#                                                 verbose=1, period=5)\n","# =========================Compile============================\n","# CrossEntropy()\n","# tf.losses.SparseCategoricalCrossentropy()\n","model.compile(loss=tf.losses.SparseCategoricalCrossentropy(), optimizer=Adam(learning_rate=learning_rate))\n","\n","for epoch in range(epochs):\n","    # ===========================Fit==============================\n","    t1 = time()\n","    model.fit(\n","        train_X,\n","        train_y,\n","        validation_data=(val_X, val_y),\n","        epochs=1,\n","        # callbacks=[tensorboard, checkpoint],\n","        batch_size=batch_size,\n","        )\n","    # ===========================Test==============================\n","    t2 = time()\n","    hit_rate, mrr = evaluate_model(model, test, K)\n","    print('Iteration %d Fit [%.1f s], Evaluate [%.1f s]: HR = %.4f, MRR = %.4f, '\n","            % (epoch, t2 - t1, time() - t2, hit_rate, mrr))"],"execution_count":null,"outputs":[]}]}