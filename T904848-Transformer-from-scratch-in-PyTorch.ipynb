{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"T904848 | Transformer from scratch in PyTorch","provenance":[],"collapsed_sections":[],"toc_visible":true,"authorship_tag":"ABX9TyOQUpCV3xNofoyaKkO6PENf"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"4SuL06w0jfZR"},"source":["# Transformer from scratch in PyTorch"]},{"cell_type":"markdown","metadata":{"id":"PKbsUSTuiPQU"},"source":["Coding the scaled dot-product attention is pretty straightforward — just a few matrix multiplications, plus a softmax function. For added simplicity, we omit the optional Mask operation."]},{"cell_type":"code","metadata":{"id":"fdvcDkbKiMA6"},"source":["from torch import Tensor\n","import torch.nn.functional as f\n","\n","\n","def scaled_dot_product_attention(query: Tensor, key: Tensor, value: Tensor) -> Tensor:\n","    temp = query.bmm(key.transpose(1, 2))\n","    scale = query.size(-1) ** 0.5\n","    softmax = f.softmax(temp / scale, dim=-1)\n","    return softmax.bmm(value)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fiLDxyPJiQty"},"source":["Note that MatMul operations are translated to torch.bmm in PyTorch. That’s because Q, K, and V (query, key, and value arrays) are batches of matrices, each with shape (batch_size, sequence_length, num_features). Batch matrix multiplication is only performed over the last two dimensions.\n","\n","From the diagram above, we see that multi-head attention is composed of several identical attention heads. Each attention head contains 3 linear layers, followed by scaled dot-product attention. Let’s encapsulate this in an AttentionHead layer:"]},{"cell_type":"code","metadata":{"id":"5t097JfqiQrc"},"source":["import torch\n","from torch import nn\n","\n","\n","class AttentionHead(nn.Module):\n","    def __init__(self, dim_in: int, dim_k: int, dim_v: int):\n","        super().__init__()\n","        self.q = nn.Linear(dim_in, dim_k)\n","        self.k = nn.Linear(dim_in, dim_k)\n","        self.v = nn.Linear(dim_in, dim_v)\n","\n","    def forward(self, query: Tensor, key: Tensor, value: Tensor) -> Tensor:\n","        return scaled_dot_product_attention(self.q(query), self.k(key), self.v(value))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"e4V_iqTSiQno"},"source":["Now, it’s very easy to build the multi-head attention layer. Just combine num_heads different attention heads and a Linear layer for the output."]},{"cell_type":"code","metadata":{"id":"2L_SNeYFiQlz"},"source":["class MultiHeadAttention(nn.Module):\n","    def __init__(self, num_heads: int, dim_in: int, dim_k: int, dim_v: int):\n","        super().__init__()\n","        self.heads = nn.ModuleList(\n","            [AttentionHead(dim_in, dim_k, dim_v) for _ in range(num_heads)]\n","        )\n","        self.linear = nn.Linear(num_heads * dim_v, dim_in)\n","\n","    def forward(self, query: Tensor, key: Tensor, value: Tensor) -> Tensor:\n","        return self.linear(\n","            torch.cat([h(query, key, value) for h in self.heads], dim=-1)\n","        )"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KBaOoNjMiQjm"},"source":["We need one more component before building the complete transformer: positional encoding. Notice that MultiHeadAttention has no trainable components that operate over the sequence dimension (axis 1). Everything operates over the feature dimension (axis 2), and so it is independent of sequence length. We have to provide positional information to the model, so that it knows about the relative position of data points in the input sequences."]},{"cell_type":"code","metadata":{"id":"vH2OGPkniQhH"},"source":["def position_encoding(\n","    seq_len: int, dim_model: int, device: torch.device = torch.device(\"cpu\"),\n",") -> Tensor:\n","    pos = torch.arange(seq_len, dtype=torch.float, device=device).reshape(1, -1, 1)\n","    dim = torch.arange(dim_model, dtype=torch.float, device=device).reshape(1, 1, -1)\n","    phase = pos / 1e4 ** (dim // dim_model)\n","\n","    return torch.where(dim.long() % 2 == 0, torch.sin(phase), torch.cos(phase))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"JnmZrPpFid3w"},"source":["We can code the encoder/decoder modules independently of one another, and then combine them at the end. But first we need a few more pieces of information, which aren’t included in the figure above. For example, how should we choose to build the feed forward networks?"]},{"cell_type":"code","metadata":{"id":"RXG7YhX8ihQl"},"source":["def feed_forward(dim_input: int = 512, dim_feedforward: int = 2048) -> nn.Module:\n","    return nn.Sequential(\n","        nn.Linear(dim_input, dim_feedforward),\n","        nn.ReLU(),\n","        nn.Linear(dim_feedforward, dim_input),\n","    )"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"q-NWu_5uiuSn"},"source":["The output of each sub-layer is LayerNorm(x + Sublayer(x)), where Sublayer(x) is the function implemented by the sub-layer itself. … We apply dropout to the output of each sub-layer, before it is added to the sub-layer input and normalized."]},{"cell_type":"code","metadata":{"id":"B-IOnfeyihNA"},"source":["class Residual(nn.Module):\n","    def __init__(self, sublayer: nn.Module, dimension: int, dropout: float = 0.1):\n","        super().__init__()\n","        self.sublayer = sublayer\n","        self.norm = nn.LayerNorm(dimension)\n","        self.dropout = nn.Dropout(dropout)\n","\n","    def forward(self, *tensors: Tensor) -> Tensor:\n","        # Assume that the \"value\" tensor is given last, so we can compute the\n","        # residual.  This matches the signature of 'MultiHeadAttention'.\n","        return self.norm(tensors[-1] + self.dropout(self.sublayer(*tensors)))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"W5D8RZrqihK2"},"source":["Time to dive in and create the encoder. Using the utility methods we just built, this is pretty easy."]},{"cell_type":"code","metadata":{"id":"tbCpBmFcihIk"},"source":["class TransformerEncoderLayer(nn.Module):\n","    def __init__(\n","        self, \n","        dim_model: int = 512, \n","        num_heads: int = 6, \n","        dim_feedforward: int = 2048, \n","        dropout: float = 0.1, \n","    ):\n","        super().__init__()\n","        dim_k = dim_v = dim_model // num_heads\n","        self.attention = Residual(\n","            MultiHeadAttention(num_heads, dim_model, dim_k, dim_v),\n","            dimension=dim_model,\n","            dropout=dropout,\n","        )\n","        self.feed_forward = Residual(\n","            feed_forward(dim_model, dim_feedforward),\n","            dimension=dim_model,\n","            dropout=dropout,\n","        )\n","\n","    def forward(self, src: Tensor) -> Tensor:\n","        src = self.attention(src, src, src)\n","        return self.feed_forward(src)\n","\n","\n","class TransformerEncoder(nn.Module):\n","    def __init__(\n","        self, \n","        num_layers: int = 6,\n","        dim_model: int = 512, \n","        num_heads: int = 8, \n","        dim_feedforward: int = 2048, \n","        dropout: float = 0.1, \n","    ):\n","        super().__init__()\n","        self.layers = nn.ModuleList([\n","            TransformerEncoderLayer(dim_model, num_heads, dim_feedforward, dropout)\n","            for _ in range(num_layers)\n","        ])\n","\n","    def forward(self, src: Tensor) -> Tensor:\n","        seq_len, dimension = src.size(1), src.size(2)\n","        src += position_encoding(seq_len, dimension)\n","        for layer in self.layers:\n","            src = layer(src)\n","\n","        return src"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GZrsGydnihGL"},"source":["The decoder module is extremely similar. Just a few small differences:\n","- The decoder accepts two arguments (target and memory), rather than one.\n","- There are two multi-head attention modules per layer, instead of one.\n","- The second multi-head attention accepts memory for two of its inputs."]},{"cell_type":"code","metadata":{"id":"dxezkFJzihEL"},"source":["class TransformerDecoderLayer(nn.Module):\n","    def __init__(\n","        self, \n","        dim_model: int = 512, \n","        num_heads: int = 6, \n","        dim_feedforward: int = 2048, \n","        dropout: float = 0.1, \n","    ):\n","        super().__init__()\n","        dim_k = dim_v = dim_model // num_heads\n","        self.attention_1 = Residual(\n","            MultiHeadAttention(num_heads, dim_model, dim_k, dim_v),\n","            dimension=dim_model,\n","            dropout=dropout,\n","        )\n","        self.attention_2 = Residual(\n","            MultiHeadAttention(num_heads, dim_model, dim_k, dim_v),\n","            dimension=dim_model,\n","            dropout=dropout,\n","        )\n","        self.feed_forward = Residual(\n","            feed_forward(dim_model, dim_feedforward),\n","            dimension=dim_model,\n","            dropout=dropout,\n","        )\n","\n","    def forward(self, tgt: Tensor, memory: Tensor) -> Tensor:\n","        tgt = self.attention_1(tgt, tgt, tgt)\n","        tgt = self.attention_2(memory, memory, tgt)\n","        return self.feed_forward(tgt)\n","\n","\n","class TransformerDecoder(nn.Module):\n","    def __init__(\n","        self, \n","        num_layers: int = 6,\n","        dim_model: int = 512, \n","        num_heads: int = 8, \n","        dim_feedforward: int = 2048, \n","        dropout: float = 0.1, \n","    ):\n","        super().__init__()\n","        self.layers = nn.ModuleList([\n","            TransformerDecoderLayer(dim_model, num_heads, dim_feedforward, dropout)\n","            for _ in range(num_layers)\n","        ])\n","        self.linear = nn.Linear(dim_model, dim_model)\n","\n","    def forward(self, tgt: Tensor, memory: Tensor) -> Tensor:\n","        seq_len, dimension = tgt.size(1), tgt.size(2)\n","        tgt += position_encoding(seq_len, dimension)\n","        for layer in self.layers:\n","            tgt = layer(tgt, memory)\n","\n","        return torch.softmax(self.linear(tgt), dim=-1)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qISz9Njni6Y_"},"source":["Lastly, we need to wrap everything up into a single Transformer class. This requires minimal work, because it’s nothing new — just throw an encoder and decoder together, and pass data through them in the correct order."]},{"cell_type":"code","metadata":{"id":"mtdMH0rbi66X"},"source":["class Transformer(nn.Module):\n","    def __init__(\n","        self, \n","        num_encoder_layers: int = 6,\n","        num_decoder_layers: int = 6,\n","        dim_model: int = 512, \n","        num_heads: int = 6, \n","        dim_feedforward: int = 2048, \n","        dropout: float = 0.1, \n","        activation: nn.Module = nn.ReLU(),\n","    ):\n","        super().__init__()\n","        self.encoder = TransformerEncoder(\n","            num_layers=num_encoder_layers,\n","            dim_model=dim_model,\n","            num_heads=num_heads,\n","            dim_feedforward=dim_feedforward,\n","            dropout=dropout,\n","        )\n","        self.decoder = TransformerDecoder(\n","            num_layers=num_decoder_layers,\n","            dim_model=dim_model,\n","            num_heads=num_heads,\n","            dim_feedforward=dim_feedforward,\n","            dropout=dropout,\n","        )\n","\n","    def forward(self, src: Tensor, tgt: Tensor) -> Tensor:\n","        return self.decoder(tgt, self.encoder(src))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"F_VtLkoci8zB"},"source":["And we’re done! Let’s create a simple test, as a sanity check for our implementation. We can construct random tensors for src and tgt, check that our model executes without errors, and confirm that the output tensor has the correct shape."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"np60EZkci-JP","executionInfo":{"status":"ok","timestamp":1634026193801,"user_tz":-330,"elapsed":2353,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}},"outputId":"e53e7f16-7e37-4d8f-cdcd-96654401456c"},"source":["src = torch.rand(64, 16, 512)\n","tgt = torch.rand(64, 16, 512)\n","out = Transformer()(src, tgt)\n","print(out.shape)\n","# torch.Size([64, 16, 512])"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([64, 16, 512])\n"]}]}]}