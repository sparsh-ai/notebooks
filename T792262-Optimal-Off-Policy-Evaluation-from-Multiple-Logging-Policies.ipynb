{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"T792262 | Optimal Off-Policy Evaluation from Multiple Logging Policies","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyNN/XBQzA57HcKKSbNUgl8e"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"yKsArBS7bUEz"},"source":["# Optimal Off-Policy Evaluation from Multiple Logging Policies"]},{"cell_type":"markdown","metadata":{"id":"Af03hWaQVe1m"},"source":["## Imports"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gtLCLFvuVnyY","executionInfo":{"status":"ok","timestamp":1633519327877,"user_tz":-330,"elapsed":20,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}},"outputId":"4736b922-98ca-44a0-87a4-c2f36a43fc26"},"source":["%tensorflow_version 1.x"],"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["TensorFlow 1.x selected.\n"]}]},{"cell_type":"code","metadata":{"id":"GeM0qrS3VezY","executionInfo":{"status":"ok","timestamp":1633519555309,"user_tz":-330,"elapsed":539,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}}},"source":["from typing import Dict, List\n","import argparse\n","import time\n","import pickle\n","import warnings\n","from pathlib import Path\n","\n","import yaml\n","import numpy as np\n","import pandas as pd\n","from dataclasses import dataclass\n","\n","from sklearn.preprocessing import LabelEncoder\n","from sklearn.model_selection import train_test_split\n","from sklearn.model_selection import StratifiedKFold, KFold\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.exceptions import ConvergenceWarning\n","\n","import tensorflow as tf\n","from tensorflow.python.framework import ops\n","\n","warnings.filterwarnings(action=\"ignore\", category=ConvergenceWarning)"],"execution_count":9,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vE6yqaJoWIdd"},"source":["## Configurations"]},{"cell_type":"code","metadata":{"id":"RgxG3QP2WIa2","executionInfo":{"status":"ok","timestamp":1633519451927,"user_tz":-330,"elapsed":670,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}}},"source":["!mkdir -p conf"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bEnQtqYQWL6Q","executionInfo":{"status":"ok","timestamp":1633519498360,"user_tz":-330,"elapsed":820,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}},"outputId":"bab28d5e-ca21-4008-8a90-babe9dcf255c"},"source":["%%writefile conf/policy_params.yaml\n","evaluation: 1.0\n","behavior1: 0.95\n","behavior2: 0.05"],"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Writing conf/policy_params.yaml\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SfCBk0ZIWL33","executionInfo":{"status":"ok","timestamp":1633519498953,"user_tz":-330,"elapsed":7,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}},"outputId":"cd100dd3-a426-4763-9980-353d8d96e23c"},"source":["%%writefile conf/q_func_hyperparams.yaml\n","eta: 0.01\n","std: 0.01\n","lam: 0.001\n","batch_size: 256\n","epochs: 200"],"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Writing conf/q_func_hyperparams.yaml\n"]}]},{"cell_type":"markdown","metadata":{"id":"hFk96QhgVew-"},"source":["## Dataset"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2LuwCjZNYYxp","executionInfo":{"status":"ok","timestamp":1633520205872,"user_tz":-330,"elapsed":3205,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}},"outputId":"83aaba30-a4eb-4857-b8b7-a95b542f824e"},"source":["!mkdir -p data/optdigits\n","!wget -q --show-progress -O data/optdigits/optdigits.tra https://archive.ics.uci.edu/ml/machine-learning-databases/optdigits/optdigits.tra\n","!wget -q --show-progress -O data/optdigits/optdigits.tes https://archive.ics.uci.edu/ml/machine-learning-databases/optdigits/optdigits.tes"],"execution_count":17,"outputs":[{"output_type":"stream","name":"stdout","text":["data/optdigits/optd 100%[===================>] 550.43K   741KB/s    in 0.7s    \n","data/optdigits/optd 100%[===================>] 258.51K   436KB/s    in 0.6s    \n"]}]},{"cell_type":"code","metadata":{"id":"HsZC26TzVVDl","executionInfo":{"status":"ok","timestamp":1633519685697,"user_tz":-330,"elapsed":555,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}}},"source":["def load_datasets(\n","    data: str, ratio: float, test_size: float = 0.5, random_state: int = 12345\n","):\n","    \"\"\"Load and preprocess raw multiclass classification data.\"\"\"\n","    data_path = Path(f\"data/{data}\")\n","    le = LabelEncoder()\n","    if data == \"optdigits\":\n","        data_ = np.r_[\n","            np.loadtxt(data_path / f\"{data}.tra\", delimiter=\",\"),\n","            np.loadtxt(data_path / f\"{data}.tes\", delimiter=\",\"),\n","        ]\n","    elif data == \"pendigits\":\n","        data_ = np.r_[\n","            np.loadtxt(data_path / f\"{data}.tra\", delimiter=\",\"),\n","            np.loadtxt(data_path / f\"{data}.tes\", delimiter=\",\"),\n","        ]\n","    elif data == \"sat\":\n","        data_ = np.r_[\n","            np.loadtxt(data_path / f\"{data}.trn\", delimiter=\" \"),\n","            np.loadtxt(data_path / f\"{data}.tst\", delimiter=\" \"),\n","        ]\n","        data_[:, -1] = np.where(data_[:, -1] == 7, 5, data_[:, -1] - 1)\n","    elif data == \"letter\":\n","        data_ = np.genfromtxt(\n","            data_path / \"letter-recognition.data\", delimiter=\",\", dtype=\"str\"\n","        )\n","        data_ = np.c_[data_[:, 1:], le.fit_transform(data_[:, 0])].astype(float)\n","\n","    np.random.shuffle(data_)\n","    data_tr, data_ev = train_test_split(\n","        data_, test_size=test_size, random_state=random_state\n","    )\n","    n_train, n_eval = data_tr.shape[0], data_ev.shape[0]\n","    n_dim = np.int(data_tr.shape[1] / 2)\n","    y_tr, y_ev = data_tr[:, -1].astype(int), data_ev[:, -1].astype(int)\n","    n_class = np.unique(y_tr).shape[0]\n","    y_full_ev = np.zeros((n_eval, n_class))\n","    y_full_ev[np.arange(n_eval), y_ev] = 1\n","    X_tr, X_ev = data_tr[:, :-1], data_ev[:, :-1]\n","    X_tr1, X_tr2 = data_tr[:, :n_dim], data_tr[:, n_dim:]\n","    X_ev1, X_ev2 = data_ev[:, :n_dim], data_ev[:, n_dim:]\n","\n","    # multiple logger index generation\n","    ratio1 = ratio / (1 + ratio)\n","    n_eval1 = np.int(n_eval * ratio1)\n","    idx1 = np.ones(n_eval, dtype=bool)\n","    idx1[n_eval1:] = False\n","\n","    return dict(\n","        n_train=n_train,\n","        n_eval=n_eval,\n","        n_dim=n_dim,\n","        n_class=n_class,\n","        n_behavior_policies=2,\n","        X_tr=X_tr,\n","        X_tr1=X_tr1,\n","        X_tr2=X_tr2,\n","        X_ev=X_ev,\n","        X_ev1=X_ev1,\n","        X_ev2=X_ev2,\n","        y_tr=y_tr,\n","        y_ev=y_ev,\n","        y_full_ev=y_full_ev,\n","        idx1=idx1,\n","        ratio1=(n_eval1 / n_eval),\n","    )\n","\n","\n","def generate_bandit_feedback(data_dict: Dict, pi_b1: np.ndarray, pi_b2: np.ndarray):\n","    \"\"\"Generate logged bandit feedback data.\"\"\"\n","    n_eval = data_dict[\"n_eval\"]\n","    idx1, ratio1 = data_dict[\"idx1\"], data_dict[\"ratio1\"]\n","    idx1_expanded = np.expand_dims(idx1, 1)\n","    pi_b = pi_b1 * idx1_expanded + pi_b2 * (1 - idx1_expanded)\n","    pi_b_star = pi_b1 * ratio1 + pi_b2 * (1.0 - ratio1)\n","    action_set = np.arange(data_dict[\"n_class\"])\n","    actions = np.zeros(data_dict[\"n_eval\"], dtype=int)\n","    for i, pvals in enumerate(pi_b):\n","        actions[i] = np.random.choice(action_set, p=pvals)\n","    rewards = data_dict[\"y_full_ev\"][np.arange(n_eval), actions]\n","    return dict(\n","        n_eval=data_dict[\"n_eval\"],\n","        n_class=data_dict[\"n_class\"],\n","        X_ev=data_dict[\"X_ev\"],\n","        pi_b=pi_b,\n","        pi_b_star=pi_b_star,\n","        actions=actions,\n","        idx1=idx1,\n","        rewards=rewards,\n","    )"],"execution_count":12,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5eQuiovsV0RU"},"source":["## Policies"]},{"cell_type":"code","metadata":{"id":"3K1uMnSoV0OM","executionInfo":{"status":"ok","timestamp":1633519499605,"user_tz":-330,"elapsed":6,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}}},"source":["def train_policies(data_dict: Dict, random_state: int = 0) -> List[np.ndarray]:\n","    \"\"\"Train evaluation and behavior policies.\"\"\"\n","    with open(\"./conf/policy_params.yaml\", \"rb\") as f:\n","        policy_params = yaml.safe_load(f)\n","\n","    policy_list = list()\n","    for pol in policy_params.keys():\n","        # make label predictions\n","        X_tr, y_tr = data_dict[f\"X_tr\"], data_dict[f\"y_tr\"]\n","        clf = LogisticRegression(\n","            random_state=random_state,\n","            solver=\"lbfgs\",\n","            multi_class=\"multinomial\",\n","        ).fit(X=X_tr, y=y_tr)\n","        preds = clf.predict(X=data_dict[f\"X_ev\"]).astype(int)\n","        # transform predictions into distribution over actions\n","        alpha = policy_params[pol]\n","        pi = np.zeros((data_dict[\"n_eval\"], data_dict[\"n_class\"]))\n","        pi[:, :] = (1.0 - alpha) / data_dict[\"n_class\"]\n","        pi[np.arange(data_dict[\"n_eval\"]), preds] = (\n","            alpha + (1.0 - alpha) / data_dict[\"n_class\"]\n","        )\n","        policy_list.append(pi)\n","    return policy_list"],"execution_count":7,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-MdsSR2HVvZK"},"source":["## Estimators"]},{"cell_type":"code","metadata":{"id":"HSBmxB3PVvXD","executionInfo":{"status":"ok","timestamp":1633519501438,"user_tz":-330,"elapsed":1286,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}}},"source":["def calc_ground_truth(y_true: np.ndarray, pi: np.ndarray) -> float:\n","    \"\"\"Calculate the ground-truth policy value of an eval policy\"\"\"\n","    return pi[np.arange(y_true.shape[0]), y_true].mean()\n","\n","\n","def calc_ipw(\n","    rewards: np.ndarray,\n","    actions: np.ndarray,\n","    pi_b: np.ndarray,\n","    pi_e: np.ndarray,\n",") -> float:\n","    n_data = actions.shape[0]\n","    iw = pi_e[np.arange(n_data), actions] / pi_b[np.arange(n_data), actions]\n","    return (rewards * iw).mean()\n","\n","\n","def calc_var(\n","    rewards: np.ndarray,\n","    actions: np.ndarray,\n","    pi_b: np.ndarray,\n","    pi_e: np.ndarray,\n","    estimated_q_func: np.ndarray,\n","):\n","    n_data = actions.shape[0]\n","    v = np.average(estimated_q_func, weights=pi_e, axis=1)\n","    shifted_rewards = rewards - estimated_q_func[np.arange(n_data), actions]\n","    iw = pi_e[np.arange(n_data), actions] / pi_b[np.arange(n_data), actions]\n","    return np.var(shifted_rewards * iw + v)\n","\n","\n","def calc_weighted(\n","    rewards: np.ndarray,\n","    actions: np.ndarray,\n","    idx1: np.ndarray,\n","    pi_b: np.ndarray,\n","    pi_e: np.ndarray,\n","    estimated_q_func: np.ndarray = None,\n","    n_fold: int = 2,\n",") -> float:\n","    estimated_rewards_list = list()\n","    if estimated_q_func is None:\n","        estimated_q_func = np.zeros((actions.shape[0], np.int(actions.max() + 1)))\n","    kf = KFold(n_splits=n_fold, shuffle=True, random_state=12345)\n","    for train_idx, test_idx in kf.split(rewards):\n","        rewards_tr, rewards_ev = rewards[train_idx], rewards[test_idx]\n","        actions_tr, actions_ev = actions[train_idx], actions[test_idx]\n","        idx1_tr, idx1_ev = idx1[train_idx], idx1[test_idx]\n","        pi_b_tr, pi_b_ev = pi_b[train_idx], pi_b[test_idx]\n","        pi_e_tr, pi_e_ev = pi_e[train_idx], pi_e[test_idx]\n","        estimated_q_func_tr = estimated_q_func[train_idx]\n","        estimated_q_func_ev = estimated_q_func[test_idx]\n","        # estimate lambda with one of the fold\n","        n_data1, n_data2 = idx1_tr.sum(), (~idx1_tr).sum()\n","        var1 = calc_var(\n","            rewards=rewards_tr[idx1_tr],\n","            actions=actions_tr[idx1_tr],\n","            pi_b=pi_b_tr[idx1_tr],\n","            pi_e=pi_e_tr[idx1_tr],\n","            estimated_q_func=estimated_q_func_tr[idx1_tr],\n","        )\n","        var2 = calc_var(\n","            rewards=rewards_tr[~idx1_tr],\n","            actions=actions_tr[~idx1_tr],\n","            pi_b=pi_b_tr[~idx1_tr],\n","            pi_e=pi_e_tr[~idx1_tr],\n","            estimated_q_func=estimated_q_func_tr[~idx1_tr],\n","        )\n","        denominator = (n_data1 / var1) + (n_data2 / var2)\n","        lam1 = (n_data1 / var1) / denominator\n","        lam2 = (n_data2 / var2) / denominator\n","        # estimate the policy value with the other fold\n","        iw1 = (\n","            pi_e_ev[idx1_ev, actions_ev[idx1_ev]]\n","            / pi_b_ev[idx1_ev, actions_ev[idx1_ev]]\n","        )\n","        iw2 = (\n","            pi_e_ev[~idx1_ev, actions_ev[~idx1_ev]]\n","            / pi_b_ev[~idx1_ev, actions_ev[~idx1_ev]]\n","        )\n","        v1 = np.average(estimated_q_func_ev[idx1_ev], weights=pi_e_ev[idx1_ev], axis=1)\n","        v2 = np.average(\n","            estimated_q_func_ev[~idx1_ev], weights=pi_e_ev[~idx1_ev], axis=1\n","        )\n","        shifted_rewards1 = (\n","            rewards_ev[idx1_ev] - estimated_q_func_ev[idx1_ev, actions_ev[idx1_ev]]\n","        )\n","        shifted_rewards2 = (\n","            rewards_ev[~idx1_ev] - estimated_q_func_ev[~idx1_ev, actions_ev[~idx1_ev]]\n","        )\n","        estimated_rewards = lam1 * (iw1 * shifted_rewards1 + v1).mean()\n","        estimated_rewards += lam2 * (iw2 * shifted_rewards2 + v2).mean()\n","        estimated_rewards_list.append(estimated_rewards)\n","    return np.mean(estimated_rewards_list)\n","\n","\n","def calc_dr(\n","    rewards: np.ndarray,\n","    actions: np.ndarray,\n","    estimated_q_func: np.ndarray,\n","    pi_b: np.ndarray,\n","    pi_e: np.ndarray,\n",") -> float:\n","    n_data = actions.shape[0]\n","    v = np.average(estimated_q_func, weights=pi_e, axis=1)\n","    iw = pi_e[np.arange(n_data), actions] / pi_b[np.arange(n_data), actions]\n","    shifted_rewards = rewards - estimated_q_func[np.arange(n_data), actions]\n","    return (iw * shifted_rewards + v).mean()\n","\n","\n","def estimate_q_func(\n","    bandit_feedback,\n","    pi_e: np.ndarray,\n","    fitting_method: str = \"naive\",\n","    k_fold: int = 2,\n",") -> np.ndarray:\n","    # hyperparam\n","    with open(\"./conf/q_func_hyperparams.yaml\", \"rb\") as f:\n","        q_func_hyperparams = yaml.safe_load(f)\n","\n","    X = bandit_feedback[\"X_ev\"]\n","    y = bandit_feedback[\"rewards\"]\n","    pi_b_star = bandit_feedback[\"pi_b_star\"]\n","    idx1 = bandit_feedback[\"idx1\"].astype(int)\n","    a = pd.get_dummies(bandit_feedback[\"actions\"]).values\n","    skf = StratifiedKFold(n_splits=k_fold)\n","    skf.get_n_splits(X, y)\n","    estimated_q_func = np.zeros((bandit_feedback[\"n_eval\"], bandit_feedback[\"n_class\"]))\n","    for train_idx, test_idx in skf.split(X, y):\n","        X_tr, X_ev = X[train_idx], X[test_idx]\n","        y_tr, a_tr = y[train_idx], a[train_idx].astype(float)\n","        pi_e_tr = pi_e[train_idx]\n","        pi_b_star_tr = pi_b_star[train_idx]\n","        idx1_tr = idx1[train_idx]\n","        ops.reset_default_graph()\n","        clf = QFuncEstimator(\n","            num_features=X_tr.shape[1],\n","            num_classes=bandit_feedback[\"n_class\"],\n","            fitting_method=fitting_method,\n","            eta=q_func_hyperparams[\"eta\"],\n","            std=q_func_hyperparams[\"std\"],\n","            lam=q_func_hyperparams[\"lam\"],\n","            batch_size=q_func_hyperparams[\"batch_size\"],\n","            epochs=q_func_hyperparams[\"epochs\"],\n","        )\n","        clf.train(\n","            X=X_tr,\n","            a=a_tr,\n","            y=y_tr,\n","            pi_e=pi_e_tr,\n","            pi_b_star=pi_b_star_tr,\n","            idx1=idx1_tr,\n","        )\n","        for a_idx in np.arange(bandit_feedback[\"n_class\"]):\n","            estimated_q_func_for_a = clf.predict(X=X_ev, a_idx=a_idx)[:, a_idx]\n","            estimated_q_func[test_idx, a_idx] = estimated_q_func_for_a\n","        clf.s.close()\n","    return estimated_q_func\n","\n","\n","@dataclass\n","class QFuncEstimator:\n","    num_features: int\n","    num_classes: int\n","    eta: float = 0.01\n","    std: float = 0.01\n","    lam: float = 0.001\n","    batch_size: int = 256\n","    epochs: int = 200\n","    fitting_method: str = \"stratified\"\n","\n","    def __post_init__(self) -> None:\n","        \"\"\"Initialize Class.\"\"\"\n","        tf.set_random_seed(0)\n","        self.s = tf.Session()\n","        self.create_placeholders()\n","        self.build_graph()\n","        self.create_losses()\n","        self.add_optimizer()\n","\n","    def create_placeholders(self) -> None:\n","        \"\"\"Create the placeholders to be used.\"\"\"\n","        self.input_X = tf.placeholder(\n","            \"float32\", shape=(None, self.num_features), name=\"input_X\"\n","        )\n","        self.input_A = tf.placeholder(\n","            \"float32\", shape=(None, self.num_classes), name=\"input_A\"\n","        )\n","        self.input_R = tf.placeholder(\"float32\", shape=(None,), name=\"input_R\")\n","        self.input_pi_e = tf.placeholder(\n","            \"float32\", shape=(None, self.num_classes), name=\"input_pi_e\"\n","        )\n","        self.input_pi_b_star = tf.placeholder(\n","            \"float32\", shape=(None, self.num_classes), name=\"input_pi_b_star\"\n","        )\n","        self.input_idx1 = tf.placeholder(\"float32\", shape=(None,), name=\"input_idx1\")\n","\n","    def build_graph(self) -> None:\n","        \"\"\"Build the main tensorflow graph with embedding layers.\"\"\"\n","        self.weights = tf.Variable(\n","            tf.random_normal(\n","                [self.num_features + self.num_classes, self.num_classes],\n","                stddev=self.std,\n","            )\n","        )\n","        self.bias = tf.Variable(tf.random_normal([self.num_classes], stddev=self.std))\n","\n","        with tf.variable_scope(\"prediction\"):\n","            input_X = tf.concat([self.input_X, self.input_A], axis=1)\n","            self.preds = tf.sigmoid(tf.matmul(input_X, self.weights) + self.bias)\n","\n","    def create_losses(self) -> None:\n","        \"\"\"Create the losses.\"\"\"\n","        with tf.name_scope(\"loss\"):\n","            shifted_rewards = self.input_R - tf.reduce_sum(\n","                self.preds * self.input_A, axis=1\n","            )\n","            if self.fitting_method == \"normal\":\n","                self.loss = tf.reduce_mean(tf.square(shifted_rewards))\n","            else:\n","                ratio1 = tf.reduce_mean(self.input_idx1)\n","                input_idx2 = tf.ones_like(self.input_idx1) - self.input_idx1\n","                ratio2 = tf.reduce_mean(input_idx2)\n","                pi_e = tf.reduce_sum(self.input_pi_e * self.input_A, 1)\n","                pi_b_star = tf.reduce_sum(self.input_pi_b_star * self.input_A, 1)\n","                v = tf.reduce_sum(self.input_pi_e * self.preds, 1)\n","                phi = (pi_e / pi_b_star) * shifted_rewards + v\n","                phi1 = self.input_idx1 * phi\n","                phi2 = input_idx2 * phi\n","                if self.fitting_method == \"stratified\":\n","                    self.loss = ratio1 * tf.reduce_mean(tf.square(phi1))\n","                    self.loss += ratio2 * tf.reduce_mean(tf.square(phi2))\n","                    self.loss -= ratio1 * tf.square(tf.reduce_mean(phi1))\n","                    self.loss -= ratio2 * tf.square(tf.reduce_mean(phi2))\n","                elif self.fitting_method == \"naive\":\n","                    self.loss = tf.reduce_mean(tf.square(phi))\n","                    self.loss -= tf.square(tf.reduce_mean(phi))\n","\n","            self.var_list = [self.weights, self.bias]\n","            l2_reg = [tf.nn.l2_loss(v) for v in self.var_list]\n","            self.loss += self.lam * tf.add_n(l2_reg)\n","\n","    def add_optimizer(self) -> None:\n","        \"\"\"Add the required optimizer to the graph.\"\"\"\n","        with tf.name_scope(\"optimizer\"):\n","            self.apply_grads = tf.train.MomentumOptimizer(\n","                learning_rate=self.eta, momentum=0.8\n","            ).minimize(self.loss, var_list=self.var_list)\n","\n","    def train(\n","        self,\n","        X: np.ndarray,\n","        a: np.ndarray,\n","        y: np.ndarray,\n","        pi_e: np.ndarray,\n","        pi_b_star: np.ndarray,\n","        idx1: np.ndarray,\n","    ) -> None:\n","        self.s.run(tf.global_variables_initializer())\n","        for _ in np.arange(self.epochs):\n","            arr = np.arange(X.shape[0])\n","            np.random.shuffle(arr)\n","            for idx in np.arange(0, X.shape[0], self.batch_size):\n","                arr_ = arr[idx : idx + self.batch_size]\n","                self.s.run(\n","                    self.apply_grads,\n","                    feed_dict={\n","                        self.input_X: X[arr_],\n","                        self.input_A: a[arr_],\n","                        self.input_R: y[arr_],\n","                        self.input_pi_e: pi_e[arr_],\n","                        self.input_pi_b_star: pi_b_star[arr_],\n","                        self.input_idx1: idx1[arr_],\n","                    },\n","                )\n","\n","    def predict(self, X: np.ndarray, a_idx: int):\n","        a_ = np.zeros((X.shape[0], self.num_classes))\n","        a_[:, a_idx] = 1\n","        return self.s.run(self.preds, feed_dict={self.input_X: X, self.input_A: a_})\n","\n","\n","def estimate_pi_b(bandit_feedback, k_fold: int = 2) -> None:\n","    X = bandit_feedback[\"X_ev\"]\n","    idx1 = bandit_feedback[\"idx1\"]\n","    a = bandit_feedback[\"actions\"]\n","    skf = StratifiedKFold(n_splits=k_fold, shuffle=True)\n","    skf.get_n_splits(X, a)\n","    estimated_pi_b1 = np.zeros((bandit_feedback[\"n_eval\"], bandit_feedback[\"n_class\"]))\n","    estimated_pi_b2 = np.zeros((bandit_feedback[\"n_eval\"], bandit_feedback[\"n_class\"]))\n","    estimated_pi_b_star = np.zeros(\n","        (bandit_feedback[\"n_eval\"], bandit_feedback[\"n_class\"])\n","    )\n","    for train_idx, test_idx in skf.split(X, a):\n","        X_tr, X_ev = X[train_idx], X[test_idx]\n","        idx1_tr, a_tr = idx1[train_idx], a[train_idx]\n","        clf = LogisticRegression(random_state=12345)\n","        clf.fit(X=X_tr[idx1_tr], y=a_tr[idx1_tr])\n","        estimated_pi_b1[test_idx, :] = clf.predict_proba(X_ev)\n","        clf = LogisticRegression(random_state=12345)\n","        clf.fit(X=X_tr[~idx1_tr], y=a_tr[~idx1_tr])\n","        estimated_pi_b2[test_idx, :] = clf.predict_proba(X_ev)\n","        clf = LogisticRegression(random_state=12345)\n","        clf.fit(X=X_tr, y=a_tr)\n","        estimated_pi_b_star[test_idx, :] = clf.predict_proba(X_ev)\n","    idx1 = np.expand_dims(idx1.astype(int), 1)\n","    bandit_feedback[\"pi_b\"] = np.clip(\n","        idx1 * estimated_pi_b1 + (1 - idx1) * estimated_pi_b2, 1e-6, 1.0\n","    )\n","    bandit_feedback[\"pi_b_star\"] = np.clip(estimated_pi_b_star, 1e-6, 1.0)\n","    return bandit_feedback"],"execution_count":8,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"18Yu3VRbVvU1"},"source":["## Run"]},{"cell_type":"code","metadata":{"id":"Tc7u7QVsVvS-","executionInfo":{"status":"ok","timestamp":1633519597856,"user_tz":-330,"elapsed":504,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}}},"source":["def calc_rel_rmse(policy_value_true: float, policy_value_estimated: float) -> float:\n","    return np.sqrt(\n","        (((policy_value_true - policy_value_estimated) / policy_value_true) ** 2).mean()\n","    )"],"execution_count":10,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kRdVkp4XWu2q","executionInfo":{"status":"ok","timestamp":1633519776392,"user_tz":-330,"elapsed":1038,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}},"outputId":"2aa671b1-c82d-42ec-e164-4cd58f7a6022"},"source":["parser = argparse.ArgumentParser()\n","parser.add_argument(\"--num_sims\", \"-n\", type=int, default=200)\n","parser.add_argument(\"--data\", \"-d\", type=str, default='optdigits') # data in ['optdigits','pendigits','sat','letter']\n","parser.add_argument(\"--test_size\", \"-t\", type=float, default=0.7)\n","parser.add_argument(\"--is_estimate_pi_b\", \"-i\", default=True, action=\"store_true\")\n","args = parser.parse_args(args={})\n","print(args)"],"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["Namespace(data='optdigits', is_estimate_pi_b=True, num_sims=200, test_size=0.7)\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3fWlEeC0VvQH","executionInfo":{"status":"ok","timestamp":1633539924046,"user_tz":-330,"elapsed":19709985,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}},"outputId":"642c85e2-7ef1-4552-d5d3-41db1b5f8c47"},"source":["# configurations\n","num_sims = args.num_sims\n","data = args.data\n","test_size = args.test_size\n","is_estimate_pi_b = args.is_estimate_pi_b\n","np.random.seed(12345)\n","ratio_list = [0.1, 0.2, 0.5, 1, 2, 4, 10]\n","estimator_names = [\n","    \"ground_truth\",\n","    \"IS-Avg\",\n","    \"IS\",\n","    \"IS-PW(f)\",\n","    \"DR-Avg\",\n","    \"DR-PW\",\n","    \"DR\",\n","    \"MRDR\",\n","    \"SMRDR\",\n","]\n","log_path = (\n","    Path(\"log\") / data / f\"test_size={test_size}\" / \"estimated_pi_b\"\n","    if is_estimate_pi_b\n","    else Path(\"log\") / data / f\"test_size={test_size}\" / \"true_pi_b\"\n",")\n","log_path.mkdir(parents=True, exist_ok=True)\n","raw_results_path = log_path / \"raw_results\"\n","raw_results_path.mkdir(parents=True, exist_ok=True)\n","\n","rel_rmse_results = {\n","    name: {r: np.zeros(num_sims) for r in ratio_list} for name in estimator_names\n","}\n","for ratio in ratio_list:\n","    start = time.time()\n","    ope_results = {name: np.zeros(num_sims) for name in estimator_names}\n","    for sim_id in np.arange(num_sims):\n","        # load and split data\n","        data_dict = load_datasets(\n","            data=data, test_size=test_size, ratio=ratio, random_state=sim_id\n","        )\n","        # train eval and two behavior policies\n","        pi_e, pi_b1, pi_b2 = train_policies(\n","            data_dict=data_dict,\n","            random_state=sim_id,\n","        )\n","        # generate bandit feedback\n","        bandit_feedback_ = generate_bandit_feedback(\n","            data_dict=data_dict, pi_b1=pi_b1, pi_b2=pi_b2\n","        )\n","        # estimate pi_b1, pi_b2, and pi_b_star with 2-fold cross-fitting\n","        if is_estimate_pi_b:\n","            bandit_feedback = estimate_pi_b(bandit_feedback=bandit_feedback_)\n","        else:\n","            bandit_feedback = bandit_feedback_\n","        # estimate q-function with 2-fold cross-fitting\n","        estimated_q_func = estimate_q_func(\n","            bandit_feedback=bandit_feedback,\n","            pi_e=pi_e,\n","            fitting_method=\"normal\",\n","        )\n","        estimated_q_func_with_mrdr_wrong = estimate_q_func(\n","            bandit_feedback=bandit_feedback,\n","            pi_e=pi_e,\n","            fitting_method=\"naive\",\n","        )\n","        estimated_q_func_with_mrdr = estimate_q_func(\n","            bandit_feedback=bandit_feedback,\n","            pi_e=pi_e,\n","            fitting_method=\"stratified\",\n","        )\n","        # off-policy evaluation\n","        ope_results[\"ground_truth\"][sim_id] = calc_ground_truth(\n","            y_true=data_dict[\"y_ev\"], pi=pi_e\n","        )\n","        ope_results[\"IS-Avg\"][sim_id] = calc_ipw(\n","            rewards=bandit_feedback[\"rewards\"],\n","            actions=bandit_feedback[\"actions\"],\n","            pi_b=bandit_feedback[\"pi_b\"],\n","            pi_e=pi_e,\n","        )\n","        ope_results[\"IS\"][sim_id] = calc_ipw(\n","            rewards=bandit_feedback[\"rewards\"],\n","            actions=bandit_feedback[\"actions\"],\n","            pi_b=bandit_feedback[\"pi_b_star\"],\n","            pi_e=pi_e,\n","        )\n","        ope_results[\"IS-PW(f)\"][sim_id] = calc_weighted(\n","            rewards=bandit_feedback[\"rewards\"],\n","            actions=bandit_feedback[\"actions\"],\n","            idx1=bandit_feedback[\"idx1\"],\n","            pi_b=bandit_feedback[\"pi_b\"],\n","            pi_e=pi_e,\n","        )\n","        ope_results[\"DR-Avg\"][sim_id] = calc_dr(\n","            rewards=bandit_feedback[\"rewards\"],\n","            actions=bandit_feedback[\"actions\"],\n","            estimated_q_func=estimated_q_func,\n","            pi_b=bandit_feedback[\"pi_b\"],\n","            pi_e=pi_e,\n","        )\n","        ope_results[\"DR-PW\"][sim_id] = calc_weighted(\n","            rewards=bandit_feedback[\"rewards\"],\n","            actions=bandit_feedback[\"actions\"],\n","            idx1=bandit_feedback[\"idx1\"],\n","            pi_b=bandit_feedback[\"pi_b\"],\n","            pi_e=pi_e,\n","            estimated_q_func=estimated_q_func,\n","        )\n","        ope_results[\"DR\"][sim_id] = calc_dr(\n","            rewards=bandit_feedback[\"rewards\"],\n","            actions=bandit_feedback[\"actions\"],\n","            estimated_q_func=estimated_q_func,\n","            pi_b=bandit_feedback[\"pi_b_star\"],\n","            pi_e=pi_e,\n","        )\n","        ope_results[\"MRDR\"][sim_id] = calc_dr(\n","            rewards=bandit_feedback[\"rewards\"],\n","            actions=bandit_feedback[\"actions\"],\n","            estimated_q_func=estimated_q_func_with_mrdr_wrong,\n","            pi_b=bandit_feedback[\"pi_b_star\"],\n","            pi_e=pi_e,\n","        )\n","        ope_results[\"SMRDR\"][sim_id] = calc_dr(\n","            rewards=bandit_feedback[\"rewards\"],\n","            actions=bandit_feedback[\"actions\"],\n","            estimated_q_func=estimated_q_func_with_mrdr,\n","            pi_b=bandit_feedback[\"pi_b_star\"],\n","            pi_e=pi_e,\n","        )\n","        if ((sim_id + 1) % 20) == 0:\n","            print(\n","                f\"ratio={ratio}-{sim_id+1}th: {np.round((time.time() - start) / 60, 2)}min\"\n","            )\n","    # save raw off-policy evaluation results.\n","    with open(raw_results_path / f\"ratio={ratio}.pkl\", mode=\"wb\") as f:\n","        pickle.dump(ope_results, f)\n","    for estimator in estimator_names:\n","        rel_rmse_results[estimator][ratio] = calc_rel_rmse(\n","            policy_value_true=ope_results[\"ground_truth\"],\n","            policy_value_estimated=ope_results[estimator],\n","        )\n","    print(f\"finish ratio={ratio}: {np.round((time.time() - start) / 60, 2)}min\")\n","    print(\"=\" * 50)\n","\n","# save results of the evaluation of OPE\n","rel_rmse_results_df = pd.DataFrame(rel_rmse_results).drop(\"ground_truth\", 1)\n","rel_rmse_results_df.T.round(5).to_csv(log_path / f\"rel_rmse.csv\")"],"execution_count":18,"outputs":[{"output_type":"stream","name":"stdout","text":["ratio=0.1-20th: 4.68min\n","ratio=0.1-40th: 9.34min\n","ratio=0.1-60th: 14.03min\n","ratio=0.1-80th: 18.7min\n","ratio=0.1-100th: 23.36min\n","ratio=0.1-120th: 28.05min\n","ratio=0.1-140th: 32.74min\n","ratio=0.1-160th: 37.42min\n","ratio=0.1-180th: 42.12min\n","ratio=0.1-200th: 46.86min\n","finish ratio=0.1: 46.86min\n","==================================================\n","ratio=0.2-20th: 4.73min\n","ratio=0.2-40th: 9.39min\n","ratio=0.2-60th: 14.04min\n","ratio=0.2-80th: 18.72min\n","ratio=0.2-100th: 23.4min\n","ratio=0.2-120th: 28.04min\n","ratio=0.2-140th: 32.7min\n","ratio=0.2-160th: 37.38min\n","ratio=0.2-180th: 42.08min\n","ratio=0.2-200th: 46.76min\n","finish ratio=0.2: 46.76min\n","==================================================\n","ratio=0.5-20th: 4.68min\n","ratio=0.5-40th: 9.34min\n","ratio=0.5-60th: 14.04min\n","ratio=0.5-80th: 18.75min\n","ratio=0.5-100th: 23.48min\n","ratio=0.5-120th: 28.18min\n","ratio=0.5-140th: 32.87min\n","ratio=0.5-160th: 37.55min\n","ratio=0.5-180th: 42.25min\n","ratio=0.5-200th: 46.96min\n","finish ratio=0.5: 46.96min\n","==================================================\n","ratio=1-20th: 4.74min\n","ratio=1-40th: 9.52min\n","ratio=1-60th: 14.19min\n","ratio=1-80th: 18.85min\n","ratio=1-100th: 23.49min\n","ratio=1-120th: 28.19min\n","ratio=1-140th: 32.92min\n","ratio=1-160th: 37.63min\n","ratio=1-180th: 42.35min\n","ratio=1-200th: 47.13min\n","finish ratio=1: 47.13min\n","==================================================\n","ratio=2-20th: 4.73min\n","ratio=2-40th: 9.43min\n","ratio=2-60th: 14.12min\n","ratio=2-80th: 18.83min\n","ratio=2-100th: 23.55min\n","ratio=2-120th: 28.28min\n","ratio=2-140th: 32.95min\n","ratio=2-160th: 37.7min\n","ratio=2-180th: 42.41min\n","ratio=2-200th: 47.1min\n","finish ratio=2: 47.1min\n","==================================================\n","ratio=4-20th: 4.69min\n","ratio=4-40th: 9.34min\n","ratio=4-60th: 13.98min\n","ratio=4-80th: 18.6min\n","ratio=4-100th: 23.22min\n","ratio=4-120th: 27.85min\n","ratio=4-140th: 32.52min\n","ratio=4-160th: 37.22min\n","ratio=4-180th: 41.96min\n","ratio=4-200th: 46.65min\n","finish ratio=4: 46.65min\n","==================================================\n","ratio=10-20th: 4.71min\n","ratio=10-40th: 9.48min\n","ratio=10-60th: 14.27min\n","ratio=10-80th: 19.05min\n","ratio=10-100th: 23.73min\n","ratio=10-120th: 28.38min\n","ratio=10-140th: 33.03min\n","ratio=10-160th: 37.7min\n","ratio=10-180th: 42.36min\n","ratio=10-200th: 47.02min\n","finish ratio=10: 47.02min\n","==================================================\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":297},"id":"0acTR2zfka6Q","executionInfo":{"status":"ok","timestamp":1633540012626,"user_tz":-330,"elapsed":622,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}},"outputId":"2caac3ef-1581-437b-8274-958be9889a68"},"source":["pd.read_csv('./log/optdigits/test_size=0.7/estimated_pi_b/rel_rmse.csv', index_col=0)"],"execution_count":22,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>0.1</th>\n","      <th>0.2</th>\n","      <th>0.5</th>\n","      <th>1.0</th>\n","      <th>2.0</th>\n","      <th>4.0</th>\n","      <th>10.0</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>IS-Avg</th>\n","      <td>208.48864</td>\n","      <td>535.32569</td>\n","      <td>2637.91477</td>\n","      <td>905.71031</td>\n","      <td>135.95046</td>\n","      <td>87.50504</td>\n","      <td>4222.50872</td>\n","    </tr>\n","    <tr>\n","      <th>IS</th>\n","      <td>0.51822</td>\n","      <td>0.53494</td>\n","      <td>0.52986</td>\n","      <td>0.49601</td>\n","      <td>0.44738</td>\n","      <td>0.40806</td>\n","      <td>0.41228</td>\n","    </tr>\n","    <tr>\n","      <th>IS-PW(f)</th>\n","      <td>28.30699</td>\n","      <td>1.17752</td>\n","      <td>0.75899</td>\n","      <td>3.61226</td>\n","      <td>43.51955</td>\n","      <td>107.73743</td>\n","      <td>46.19869</td>\n","    </tr>\n","    <tr>\n","      <th>DR-Avg</th>\n","      <td>131.46072</td>\n","      <td>225.88591</td>\n","      <td>510.52975</td>\n","      <td>319.42255</td>\n","      <td>117.08375</td>\n","      <td>62.25383</td>\n","      <td>232.50566</td>\n","    </tr>\n","    <tr>\n","      <th>DR-PW</th>\n","      <td>6.19500</td>\n","      <td>0.06585</td>\n","      <td>0.06765</td>\n","      <td>0.23945</td>\n","      <td>9.66307</td>\n","      <td>96.67823</td>\n","      <td>54.94231</td>\n","    </tr>\n","    <tr>\n","      <th>DR</th>\n","      <td>0.08051</td>\n","      <td>0.07398</td>\n","      <td>0.05631</td>\n","      <td>0.03551</td>\n","      <td>0.02426</td>\n","      <td>0.05821</td>\n","      <td>0.20872</td>\n","    </tr>\n","    <tr>\n","      <th>MRDR</th>\n","      <td>0.04909</td>\n","      <td>0.05391</td>\n","      <td>0.05649</td>\n","      <td>0.06436</td>\n","      <td>0.06722</td>\n","      <td>0.11189</td>\n","      <td>0.40688</td>\n","    </tr>\n","    <tr>\n","      <th>SMRDR</th>\n","      <td>0.04077</td>\n","      <td>0.04424</td>\n","      <td>0.05186</td>\n","      <td>0.05897</td>\n","      <td>0.07331</td>\n","      <td>0.11474</td>\n","      <td>0.44046</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                0.1        0.2         0.5  ...        2.0        4.0        10.0\n","IS-Avg    208.48864  535.32569  2637.91477  ...  135.95046   87.50504  4222.50872\n","IS          0.51822    0.53494     0.52986  ...    0.44738    0.40806     0.41228\n","IS-PW(f)   28.30699    1.17752     0.75899  ...   43.51955  107.73743    46.19869\n","DR-Avg    131.46072  225.88591   510.52975  ...  117.08375   62.25383   232.50566\n","DR-PW       6.19500    0.06585     0.06765  ...    9.66307   96.67823    54.94231\n","DR          0.08051    0.07398     0.05631  ...    0.02426    0.05821     0.20872\n","MRDR        0.04909    0.05391     0.05649  ...    0.06722    0.11189     0.40688\n","SMRDR       0.04077    0.04424     0.05186  ...    0.07331    0.11474     0.44046\n","\n","[8 rows x 7 columns]"]},"metadata":{},"execution_count":22}]},{"cell_type":"code","metadata":{"id":"maBZT-D8kfe7"},"source":["!apt-get install tree"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"54sMX48ekpRa","executionInfo":{"status":"ok","timestamp":1633540067992,"user_tz":-330,"elapsed":590,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}},"outputId":"72288b9b-e2ec-4702-acba-114536cf5c92"},"source":["!tree --du -h ./log"],"execution_count":25,"outputs":[{"output_type":"stream","name":"stdout","text":["./log\n","└── [119K]  optdigits\n","    └── [115K]  test_size=0.7\n","        └── [111K]  estimated_pi_b\n","            ├── [106K]  raw_results\n","            │   ├── [ 15K]  ratio=0.1.pkl\n","            │   ├── [ 15K]  ratio=0.2.pkl\n","            │   ├── [ 15K]  ratio=0.5.pkl\n","            │   ├── [ 15K]  ratio=10.pkl\n","            │   ├── [ 15K]  ratio=1.pkl\n","            │   ├── [ 15K]  ratio=2.pkl\n","            │   └── [ 15K]  ratio=4.pkl\n","            └── [ 557]  rel_rmse.csv\n","\n"," 123K used in 4 directories, 8 files\n"]}]}]}