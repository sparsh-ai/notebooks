{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"T975104 | SSE-PT on ML-1m in Tensorflow","provenance":[],"collapsed_sections":[],"toc_visible":true,"authorship_tag":"ABX9TyNQ9LWvi5vuoc98QVGY0reG"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"vjzY_CswktRy"},"source":["# SSE-PT on ML-1m in Tensorflow"]},{"cell_type":"markdown","metadata":{"id":"MZYkplPxJrY2"},"source":["## Setup"]},{"cell_type":"code","metadata":{"id":"BKDaEDy3ea6X"},"source":["%tensorflow_version 1.x"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2Oqc1o50Js5y"},"source":["### Imports"]},{"cell_type":"code","metadata":{"id":"530_VyUoKItO"},"source":["from __future__ import print_function\n","\n","import sys\n","import copy\n","import random\n","import numpy as np\n","from collections import defaultdict\n","from multiprocessing import Process, Queue\n","\n","import os\n","import time\n","import pickle\n","import argparse\n","import tensorflow as tf\n","from tqdm import tqdm"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BmtX6HEHKIqY"},"source":["### Params"]},{"cell_type":"code","metadata":{"id":"cxh7Jb13KKz1"},"source":["parser = argparse.ArgumentParser()\n","parser.add_argument('--dataset', default='ml1m')\n","parser.add_argument('--train_dir', default='default')\n","parser.add_argument('--batch_size', default=128, type=int)\n","parser.add_argument('--lr', default=0.001, type=float)\n","parser.add_argument('--maxlen', default=50, type=int)\n","parser.add_argument('--user_hidden_units', default=50, type=int)\n","parser.add_argument('--item_hidden_units', default=50, type=int)\n","parser.add_argument('--num_blocks', default=2, type=int)\n","parser.add_argument('--num_epochs', default=100, type=int)\n","parser.add_argument('--num_heads', default=1, type=int)\n","parser.add_argument('--dropout_rate', default=0.5, type=float)\n","parser.add_argument('--threshold_user', default=1.0, type=float)\n","parser.add_argument('--threshold_item', default=1.0, type=float)\n","parser.add_argument('--l2_emb', default=0.0, type=float)\n","parser.add_argument('--gpu', default=0, type=int)\n","parser.add_argument('--print_freq', default=5, type=int)\n","parser.add_argument('--k', default=10, type=int)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ENxgOEI2dmgC"},"source":["## Download ML1M Processed data\n","\n","Each line of the txt format data contains a user id and an item id, where both user id and item id are indexed from 1 consecutively. Each line represents one interaction between the user and the item. For every user, their interactions were sorted by timestamp."]},{"cell_type":"code","metadata":{"id":"nd_9xbawdqob"},"source":["!wget -q --show-progress https://github.com/wuliwei9278/SSE-PT/raw/master/data/ml1m.txt"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9AzLy4D9er-I"},"source":["## Utils"]},{"cell_type":"code","metadata":{"id":"C6jEC_Xner57"},"source":["def data_partition(fname):\n","    usernum = 0\n","    itemnum = 0\n","    User = defaultdict(list)\n","    user_train = {}\n","    user_valid = {}\n","    user_test = {}\n","    # assume user/item index starting from 1\n","    f = open('%s.txt' % fname, 'r')\n","    for line in f:\n","        u, i = line.rstrip().split(' ')\n","        u = int(u)\n","        i = int(i)\n","        usernum = max(u, usernum)\n","        itemnum = max(i, itemnum)\n","        User[u].append(i)\n","\n","    for user in User:\n","        nfeedback = len(User[user])\n","        if nfeedback < 3:\n","            user_train[user] = User[user]\n","            user_valid[user] = []\n","            user_test[user] = []\n","        else:\n","            user_train[user] = User[user][:-2]\n","            user_valid[user] = []\n","            user_valid[user].append(User[user][-2])\n","            user_test[user] = []\n","            user_test[user].append(User[user][-1])\n","    return [user_train, user_valid, user_test, usernum, itemnum]\n","\n","\n","def evaluate(model, dataset, args, sess):\n","    [train, valid, test, usernum, itemnum] = copy.deepcopy(dataset)\n","\n","    NDCG = 0.0\n","    HT = 0.0\n","    valid_user = 0.0\n","\n","    if usernum > 10000:\n","        users = random.sample(range(1, usernum + 1), 10000)\n","    else:\n","        users = range(1, usernum + 1)\n","    for u in users:\n","\n","        if len(train[u]) < 1 or len(test[u]) < 1: continue\n","\n","        seq = np.zeros([args.maxlen], dtype=np.int32)\n","        idx = args.maxlen - 1\n","        seq[idx] = valid[u][0]\n","        idx -= 1\n","        for i in reversed(train[u]):\n","            seq[idx] = i\n","            idx -= 1\n","            if idx == -1: break\n","        rated = set(train[u])\n","        rated.add(0)\n","        item_idx = [test[u][0]]\n","        for _ in range(100):\n","            t = np.random.randint(1, itemnum + 1)\n","            while t in rated: t = np.random.randint(1, itemnum + 1)\n","            item_idx.append(t)\n","\n","        predictions = -model.predict(sess, [u], [seq], item_idx)\n","        predictions = predictions[0]\n","        #print(predictions)\n","        rank = predictions.argsort().argsort()[0]\n","\n","        valid_user += 1\n","\n","        if rank < args.k:\n","            NDCG += 1 / np.log2(rank + 2)\n","            HT += 1\n","        if valid_user % 1000 == 0:\n","            #print '.',\n","            sys.stdout.flush()\n","\n","    return NDCG / valid_user, HT / valid_user\n","\n","\n","def evaluate_valid(model, dataset, args, sess):\n","    [train, valid, test, usernum, itemnum] = copy.deepcopy(dataset)\n","\n","    NDCG = 0.0\n","    valid_user = 0.0\n","    HT = 0.0\n","    if usernum>10000:\n","        users = random.sample(range(1, usernum + 1), 10000)\n","    else:\n","        users = range(1, usernum + 1)\n","    for u in users:\n","        if len(train[u]) < 1 or len(valid[u]) < 1: continue\n","\n","        seq = np.zeros([args.maxlen], dtype=np.int32)\n","        idx = args.maxlen - 1\n","        for i in reversed(train[u]):\n","            seq[idx] = i\n","            idx -= 1\n","            if idx == -1: break\n","\n","        rated = set(train[u])\n","        rated.add(0)\n","        item_idx = [valid[u][0]]\n","        for _ in range(100):\n","            t = np.random.randint(1, itemnum + 1)\n","            while t in rated: t = np.random.randint(1, itemnum + 1)\n","            item_idx.append(t)\n","\n","        predictions = -model.predict(sess, [u], [seq], item_idx)\n","        predictions = predictions[0]\n","\n","        rank = predictions.argsort().argsort()[0]\n","\n","        valid_user += 1\n","\n","        if rank < args.k:\n","            NDCG += 1 / np.log2(rank + 2)\n","            HT += 1\n","        if valid_user % 100 == 0:\n","            #print '.',\n","            sys.stdout.flush()\n","\n","    return NDCG / valid_user, HT / valid_user"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"XcFL9q0QeHka"},"source":["## Sampler"]},{"cell_type":"code","metadata":{"id":"AycfKNuLeHip"},"source":["def random_neq(l, r, s):\n","    t = np.random.randint(l, r)\n","    while t in s:\n","        t = np.random.randint(l, r)\n","    return t\n","\n","\n","def sample_function(user_train, usernum, itemnum, batch_size, maxlen,  \n","                    threshold_user, threshold_item,\n","                    result_queue, SEED):\n","    def sample():\n","\n","        user = np.random.randint(1, usernum + 1)\n","        while len(user_train[user]) <= 1: user = np.random.randint(1, usernum + 1)\n","\n","        seq = np.zeros([maxlen], dtype=np.int32)\n","        pos = np.zeros([maxlen], dtype=np.int32)\n","        neg = np.zeros([maxlen], dtype=np.int32)\n","        nxt = user_train[user][-1]\n","        idx = maxlen - 1\n","\n","        ts = set(user_train[user])\n","\n","        for i in reversed(user_train[user][:-1]):\n","            #seq[idx] = i\n","            \n","            # SSE for user side (2 lines)\n","            if random.random() > threshold_item:\n","                i = np.random.randint(1, itemnum + 1)\n","                nxt = np.random.randint(1, itemnum + 1)\n","            seq[idx] = i\n","            pos[idx] = nxt\n","            if nxt != 0: neg[idx] = random_neq(1, itemnum + 1, ts)\n","            nxt = i\n","            idx -= 1\n","            if idx == -1: break\n","        \n","        # SSE for item side (2 lines)\n","        if random.random() > threshold_user:\n","            user = np.random.randint(1, usernum + 1)\n","        # equivalent to hard parameter sharing\n","        #user = 1\t\n","     \n","        return (user, seq, pos, neg)\n","\n","    np.random.seed(SEED)\n","    while True:\n","        one_batch = []\n","        for i in range(batch_size):\n","            one_batch.append(sample())\n","\n","        result_queue.put(zip(*one_batch))\n","\n","\n","class WarpSampler(object):\n","    def __init__(self, User, usernum, itemnum, batch_size=64, maxlen=10, \n","                 threshold_user=1.0, threshold_item=1.0, n_workers=1):\n","        self.result_queue = Queue(maxsize=n_workers * 10)\n","        self.processors = []\n","        for i in range(n_workers):\n","            self.processors.append(\n","                Process(target=sample_function, args=(User,\n","                                                      usernum,\n","                                                      itemnum,\n","                                                      batch_size,\n","                                                      maxlen,\n","                                                      threshold_user,\n","                                                      threshold_item,\n","                                                      self.result_queue,\n","                                                      np.random.randint(2e9)\n","                                                      )))\n","            self.processors[-1].daemon = True\n","            self.processors[-1].start()\n","\n","    def next_batch(self):\n","        return self.result_queue.get()\n","\n","    def close(self):\n","        for p in self.processors:\n","            p.terminate()\n","            p.join()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"67XCVru3eHhc"},"source":["## Model"]},{"cell_type":"code","metadata":{"id":"hSWEevSPeHew"},"source":["def positional_encoding(dim, sentence_length, dtype=tf.float32):\n","\n","    encoded_vec = np.array([pos/np.power(10000, 2*i/dim) for pos in range(sentence_length) for i in range(dim)])\n","    encoded_vec[::2] = np.sin(encoded_vec[::2])\n","    encoded_vec[1::2] = np.cos(encoded_vec[1::2])\n","\n","    return tf.convert_to_tensor(encoded_vec.reshape([sentence_length, dim]), dtype=dtype)\n","\n","def normalize(inputs, \n","              epsilon = 1e-8,\n","              scope=\"ln\",\n","              reuse=None):\n","    '''Applies layer normalization.\n","    \n","    Args:\n","      inputs: A tensor with 2 or more dimensions, where the first dimension has\n","        `batch_size`.\n","      epsilon: A floating number. A very small number for preventing ZeroDivision Error.\n","      scope: Optional scope for `variable_scope`.\n","      reuse: Boolean, whether to reuse the weights of a previous layer\n","        by the same name.\n","      \n","    Returns:\n","      A tensor with the same shape and data dtype as `inputs`.\n","    '''\n","    with tf.variable_scope(scope, reuse=reuse):\n","        inputs_shape = inputs.get_shape()\n","        params_shape = inputs_shape[-1:]\n","    \n","        mean, variance = tf.nn.moments(inputs, [-1], keep_dims=True)\n","        beta= tf.Variable(tf.zeros(params_shape))\n","        gamma = tf.Variable(tf.ones(params_shape))\n","        normalized = (inputs - mean) / ( (variance + epsilon) ** (.5) )\n","        outputs = gamma * normalized + beta\n","        \n","    return outputs\n","\n","def embedding(inputs, \n","              vocab_size, \n","              num_units, \n","              zero_pad=True, \n","              scale=True,\n","              l2_reg=0.0,\n","              scope=\"embedding\", \n","              with_t=False,\n","              reuse=None):\n","    '''Embeds a given tensor.\n","    Args:\n","      inputs: A `Tensor` with type `int32` or `int64` containing the ids\n","         to be looked up in `lookup table`.\n","      vocab_size: An int. Vocabulary size.\n","      num_units: An int. Number of embedding hidden units.\n","      zero_pad: A boolean. If True, all the values of the fist row (id 0)\n","        should be constant zeros.\n","      scale: A boolean. If True. the outputs is multiplied by sqrt num_units.\n","      scope: Optional scope for `variable_scope`.\n","      reuse: Boolean, whether to reuse the weights of a previous layer\n","        by the same name.\n","    Returns:\n","      A `Tensor` with one more rank than inputs's. The last dimensionality\n","        should be `num_units`.\n","        \n","    For example,\n","    \n","    ```\n","    import tensorflow as tf\n","    \n","    inputs = tf.to_int32(tf.reshape(tf.range(2*3), (2, 3)))\n","    outputs = embedding(inputs, 6, 2, zero_pad=True)\n","    with tf.Session() as sess:\n","        sess.run(tf.global_variables_initializer())\n","        print sess.run(outputs)\n","    >>\n","    [[[ 0.          0.        ]\n","      [ 0.09754146  0.67385566]\n","      [ 0.37864095 -0.35689294]]\n","     [[-1.01329422 -1.09939694]\n","      [ 0.7521342   0.38203377]\n","      [-0.04973143 -0.06210355]]]\n","    ```\n","    \n","    ```\n","    import tensorflow as tf\n","    \n","    inputs = tf.to_int32(tf.reshape(tf.range(2*3), (2, 3)))\n","    outputs = embedding(inputs, 6, 2, zero_pad=False)\n","    with tf.Session() as sess:\n","        sess.run(tf.global_variables_initializer())\n","        print sess.run(outputs)\n","    >>\n","    [[[-0.19172323 -0.39159766]\n","      [-0.43212751 -0.66207761]\n","      [ 1.03452027 -0.26704335]]\n","     [[-0.11634696 -0.35983452]\n","      [ 0.50208133  0.53509563]\n","      [ 1.22204471 -0.96587461]]]    \n","    ```    \n","    '''\n","    with tf.variable_scope(scope, reuse=reuse):\n","        lookup_table = tf.get_variable('lookup_table',\n","                                       dtype=tf.float32,\n","                                       shape=[vocab_size, num_units],\n","                                       #initializer=tf.contrib.layers.xavier_initializer(),\n","                                       regularizer=tf.contrib.layers.l2_regularizer(l2_reg))\n","        if zero_pad:\n","            lookup_table = tf.concat((tf.zeros(shape=[1, num_units]),\n","                                      lookup_table[1:, :]), 0)\n","        outputs = tf.nn.embedding_lookup(lookup_table, inputs)\n","        \n","        if scale:\n","            outputs = outputs * (num_units ** 0.5) \n","    if with_t: return outputs,lookup_table\n","    else: return outputs\n","\n","\n","def multihead_attention(queries, \n","                        keys, \n","                        num_units=None, \n","                        num_heads=8, \n","                        dropout_rate=0,\n","                        is_training=True,\n","                        causality=False,\n","                        scope=\"multihead_attention\", \n","                        reuse=None,\n","                        with_qk=False):\n","    '''Applies multihead attention.\n","    \n","    Args:\n","      queries: A 3d tensor with shape of [N, T_q, C_q].\n","      keys: A 3d tensor with shape of [N, T_k, C_k].\n","      num_units: A scalar. Attention size.\n","      dropout_rate: A floating point number.\n","      is_training: Boolean. Controller of mechanism for dropout.\n","      causality: Boolean. If true, units that reference the future are masked. \n","      num_heads: An int. Number of heads.\n","      scope: Optional scope for `variable_scope`.\n","      reuse: Boolean, whether to reuse the weights of a previous layer\n","        by the same name.\n","        \n","    Returns\n","      A 3d tensor with shape of (N, T_q, C)  \n","    '''\n","    with tf.variable_scope(scope, reuse=reuse):\n","        # Set the fall back option for num_units\n","        if num_units is None:\n","            num_units = queries.get_shape().as_list[-1]\n","        \n","        # Linear projections\n","        # Q = tf.layers.dense(queries, num_units, activation=tf.nn.relu) # (N, T_q, C)\n","        # K = tf.layers.dense(keys, num_units, activation=tf.nn.relu) # (N, T_k, C)\n","        # V = tf.layers.dense(keys, num_units, activation=tf.nn.relu) # (N, T_k, C)\n","        Q = tf.layers.dense(queries, num_units, activation=None) # (N, T_q, C)\n","        K = tf.layers.dense(keys, num_units, activation=None) # (N, T_k, C)\n","        V = tf.layers.dense(keys, num_units, activation=None) # (N, T_k, C)\n","        \n","        # Split and concat\n","        Q_ = tf.concat(tf.split(Q, num_heads, axis=2), axis=0) # (h*N, T_q, C/h) \n","        K_ = tf.concat(tf.split(K, num_heads, axis=2), axis=0) # (h*N, T_k, C/h) \n","        V_ = tf.concat(tf.split(V, num_heads, axis=2), axis=0) # (h*N, T_k, C/h) \n","\n","        # Multiplication\n","        outputs = tf.matmul(Q_, tf.transpose(K_, [0, 2, 1])) # (h*N, T_q, T_k)\n","        \n","        # Scale\n","        outputs = outputs / (K_.get_shape().as_list()[-1] ** 0.5)\n","        \n","        # Key Masking\n","        key_masks = tf.sign(tf.abs(tf.reduce_sum(keys, axis=-1))) # (N, T_k)\n","        key_masks = tf.tile(key_masks, [num_heads, 1]) # (h*N, T_k)\n","        key_masks = tf.tile(tf.expand_dims(key_masks, 1), [1, tf.shape(queries)[1], 1]) # (h*N, T_q, T_k)\n","        \n","        paddings = tf.ones_like(outputs)*(-2**32+1)\n","        outputs = tf.where(tf.equal(key_masks, 0), paddings, outputs) # (h*N, T_q, T_k)\n","  \n","        # Causality = Future blinding\n","        if causality:\n","            diag_vals = tf.ones_like(outputs[0, :, :]) # (T_q, T_k)\n","            tril = tf.linalg.LinearOperatorLowerTriangular(diag_vals).to_dense() # (T_q, T_k)\n","            masks = tf.tile(tf.expand_dims(tril, 0), [tf.shape(outputs)[0], 1, 1]) # (h*N, T_q, T_k)\n","   \n","            paddings = tf.ones_like(masks)*(-2**32+1)\n","            outputs = tf.where(tf.equal(masks, 0), paddings, outputs) # (h*N, T_q, T_k)\n","  \n","        # Activation\n","        outputs = tf.nn.softmax(outputs) # (h*N, T_q, T_k)\n","        attention = outputs\n","        #attention = tf.reduce_mean(outputs, axis=0) \n","\n","        # Query Masking\n","        query_masks = tf.sign(tf.abs(tf.reduce_sum(queries, axis=-1))) # (N, T_q)\n","        query_masks = tf.tile(query_masks, [num_heads, 1]) # (h*N, T_q)\n","        query_masks = tf.tile(tf.expand_dims(query_masks, -1), [1, 1, tf.shape(keys)[1]]) # (h*N, T_q, T_k)\n","        outputs *= query_masks # broadcasting. (N, T_q, C)\n","          \n","        # Dropouts\n","        outputs = tf.layers.dropout(outputs, rate=dropout_rate, training=tf.convert_to_tensor(is_training))\n","               \n","        # Weighted sum\n","        outputs = tf.matmul(outputs, V_) # ( h*N, T_q, C/h)\n","        \n","        # Restore shape\n","        outputs = tf.concat(tf.split(outputs, num_heads, axis=0), axis=2 ) # (N, T_q, C)\n","              \n","        # Residual connection\n","        outputs += queries\n","              \n","        # Normalize\n","        #outputs = normalize(outputs) # (N, T_q, C)\n"," \n","    if with_qk: return Q,K\n","    else: return outputs, attention\n","\n","def feedforward(inputs, \n","                num_units=[2048, 512],\n","                scope=\"multihead_attention\", \n","                dropout_rate=0.2,\n","                is_training=True,\n","                reuse=None):\n","    '''Point-wise feed forward net.\n","    \n","    Args:\n","      inputs: A 3d tensor with shape of [N, T, C].\n","      num_units: A list of two integers.\n","      scope: Optional scope for `variable_scope`.\n","      reuse: Boolean, whether to reuse the weights of a previous layer\n","        by the same name.\n","        \n","    Returns:\n","      A 3d tensor with the same shape and dtype as inputs\n","    '''\n","    with tf.variable_scope(scope, reuse=reuse):\n","        # Inner layer\n","        params = {\"inputs\": inputs, \"filters\": num_units[0], \"kernel_size\": 1,\n","                  \"activation\": tf.nn.relu, \"use_bias\": True}\n","        outputs = tf.layers.conv1d(**params)\n","        outputs = tf.layers.dropout(outputs, rate=dropout_rate, training=tf.convert_to_tensor(is_training))\n","        # Readout layer\n","        params = {\"inputs\": outputs, \"filters\": num_units[1], \"kernel_size\": 1,\n","                  \"activation\": None, \"use_bias\": True}\n","        outputs = tf.layers.conv1d(**params)\n","        outputs = tf.layers.dropout(outputs, rate=dropout_rate, training=tf.convert_to_tensor(is_training))\n","        \n","        # Residual connection\n","        outputs += inputs\n","        \n","        # Normalize\n","        #outputs = normalize(outputs)\n","    \n","    return outputs"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5bayVGn8eHWg"},"source":["class Model():\n","    def __init__(self, usernum, itemnum, args, reuse=tf.AUTO_REUSE):\n","        self.is_training = tf.placeholder(tf.bool, shape=())\n","        self.u = tf.placeholder(tf.int32, shape=(None))\n","        self.input_seq = tf.placeholder(tf.int32, shape=(None, args.maxlen))\n","        self.pos = tf.placeholder(tf.int32, shape=(None, args.maxlen))\n","        self.neg = tf.placeholder(tf.int32, shape=(None, args.maxlen))\n","        pos = self.pos\n","        neg = self.neg\n","        mask = tf.expand_dims(tf.to_float(tf.not_equal(self.input_seq, 0)), -1)\n","\n","        with tf.variable_scope(\"SASRec\", reuse=reuse):\n","            # sequence embedding, item embedding table\n","            self.seq, item_emb_table = embedding(self.input_seq,\n","                                                 vocab_size=itemnum + 1,\n","                                                 num_units=args.item_hidden_units,\n","                                                 zero_pad=True,\n","                                                 scale=True,\n","                                                 l2_reg=args.l2_emb,\n","                                                 scope=\"input_embeddings\",\n","                                                 with_t=True,\n","                                                 reuse=reuse\n","                                                 )\n","            self.item_emb_table = item_emb_table\n","            # Positional Encoding\n","            t, pos_emb_table = embedding(\n","                tf.tile(tf.expand_dims(tf.range(tf.shape(self.input_seq)[1]), 0), [tf.shape(self.input_seq)[0], 1]),\n","                vocab_size=args.maxlen,\n","                num_units=args.item_hidden_units + args.user_hidden_units,\n","                zero_pad=False,\n","                scale=False,\n","                l2_reg=args.l2_emb,\n","                scope=\"dec_pos\",\n","                reuse=reuse,\n","                with_t=True\n","            )\n","            #self.seq += t\n","\n","            # User Encoding\n","            u0_latent, user_emb_table = embedding(self.u[0],\n","                                                 vocab_size=usernum + 1,\n","                                                 num_units=args.user_hidden_units,\n","                                                 zero_pad=False,\n","                                                 scale=True,\n","                                                 l2_reg=args.l2_emb,\n","                                                 scope=\"user_embeddings\",\n","                                                 with_t=True,\n","                                                 reuse=reuse\n","                                                 )\n","            self.user_emb_table = user_emb_table\n","            # Has dim: B by C\n","            u_latent = embedding(self.u,\n","                                 vocab_size=usernum + 1,\n","                                 num_units=args.user_hidden_units,\n","                                 zero_pad=False,\n","                                 scale=True,\n","                                 l2_reg=args.l2_emb,\n","                                 scope=\"user_embeddings\",\n","                                 with_t=False,\n","                                 reuse=reuse\n","                                 )\n","            # Change dim to B by T by C\n","            self.u_latent = tf.tile(tf.expand_dims(u_latent, 1), [1, tf.shape(self.input_seq)[1], 1])\n","\n","            # Concat item embedding with user embedding\n","            self.hidden_units = args.item_hidden_units + args.user_hidden_units\n","            self.seq = tf.reshape(tf.concat([self.seq, self.u_latent], 2),\n","                                  [tf.shape(self.input_seq)[0], -1, self.hidden_units])\n","            self.seq += t\n","            # Dropout\n","            self.seq = tf.layers.dropout(self.seq,\n","                                         rate=args.dropout_rate,\n","                                         training=tf.convert_to_tensor(self.is_training))\n","            self.seq *= mask\n","\n","            # Build blocks\n","            self.attention = []\n","            for i in range(args.num_blocks):\n","                with tf.variable_scope(\"num_blocks_%d\" % i):\n","\n","                    # Self-attention\n","                    self.seq, attention = multihead_attention(queries=normalize(self.seq),\n","                                                   keys=self.seq,\n","                                                   num_units=self.hidden_units,\n","                                                   num_heads=args.num_heads,\n","                                                   dropout_rate=args.dropout_rate,\n","                                                   is_training=self.is_training,\n","                                                   causality=True,\n","                                                   scope=\"self_attention\")\n","                    self.attention.append(attention)\n","                    # Feed forward\n","                    self.seq = feedforward(normalize(self.seq), num_units=[self.hidden_units, self.hidden_units],\n","                                           dropout_rate=args.dropout_rate, is_training=self.is_training)\n","                    self.seq *= mask\n","\n","            self.seq = normalize(self.seq)\n","        \n","        user_emb = tf.reshape(self.u_latent, [tf.shape(self.input_seq)[0] * args.maxlen, \n","                                              args.user_hidden_units])\n","\n","        pos = tf.reshape(pos, [tf.shape(self.input_seq)[0] * args.maxlen])\n","        neg = tf.reshape(neg, [tf.shape(self.input_seq)[0] * args.maxlen])\n","        pos_emb = tf.nn.embedding_lookup(item_emb_table, pos)\n","        neg_emb = tf.nn.embedding_lookup(item_emb_table, neg)\n","\n","        pos_emb = tf.reshape(tf.concat([pos_emb, user_emb], 1), [-1, self.hidden_units])\n","        neg_emb = tf.reshape(tf.concat([neg_emb, user_emb], 1), [-1, self.hidden_units])\n","\n","        seq_emb = tf.reshape(self.seq, [tf.shape(self.input_seq)[0] * args.maxlen, self.hidden_units])\n","\n","        self.test_item = tf.placeholder(tf.int32, shape=(101))\n","        test_item_emb = tf.nn.embedding_lookup(item_emb_table, self.test_item)\n","        \n","        test_user_emb = tf.tile(tf.expand_dims(u0_latent, 0), [101, 1])\n","        # combine item and user emb\n","        test_item_emb = tf.reshape(tf.concat([test_item_emb, test_user_emb], 1), [-1, self.hidden_units])\n","\n","        self.test_logits = tf.matmul(seq_emb, tf.transpose(test_item_emb))\n","        self.test_logits = tf.reshape(self.test_logits, [tf.shape(self.input_seq)[0], args.maxlen, 101])\n","        self.test_logits = self.test_logits[:, -1, :]\n","\n","        # prediction layer\n","        self.pos_logits = tf.reduce_sum(pos_emb * seq_emb, -1)\n","        self.neg_logits = tf.reduce_sum(neg_emb * seq_emb, -1)\n","\n","        # ignore padding items (0)\n","        istarget = tf.reshape(tf.to_float(tf.not_equal(pos, 0)), [tf.shape(self.input_seq)[0] * args.maxlen])\n","        self.loss = tf.reduce_sum(\n","            - tf.log(tf.sigmoid(self.pos_logits) + 1e-24) * istarget -\n","            tf.log(1 - tf.sigmoid(self.neg_logits) + 1e-24) * istarget\n","        ) / tf.reduce_sum(istarget)\n","        \n","        \n","        #self.loss = tf.reduce_sum(\n","        #    - tf.log(tf.exp(tf.sigmoid(self.pos_logits)) + 1e-24) * istarget +\n","        #    tf.log(tf.exp(tf.sigmoid(self.pos_logits)) + tf.exp(tf.sigmoid(self.neg_logits)) + 1e-24) * istarget \n","        #) / tf.reduce_sum(istarget)\n","\n","        #self.loss = tf.reduce_sum(-tf.log(1 + tf.exp(tf.sigmoid(self.pos_logits) - tf.sigmoid(self.neg_logits))) * istarget) / tf.reduce_sum(istarget)\n","            \n","        #self.loss = tf.reduce_sum(-tf.maximum(0.0, self.pos_logits - self.neg_logits - 0.001) * istarget) / tf.reduce_sum(istarget)\n","\n","        #self.loss = tf.reduce_sum(-tf.log(tf.clip_by_value(tf.sigmoid(self.pos_logits - self.neg_logits), 1e-5, 1)) * istarget) / tf.reduce_sum(istarget)\n","        #self.loss = tf.reduce_sum(-tf.square(tf.maximum(self.pos_logits - self.neg_logits - 100, 0)) * istarget) / tf.reduce_sum(istarget)\n","        reg_losses = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)\n","        self.loss += sum(reg_losses)\n","\n","        tf.summary.scalar('loss', self.loss)\n","        self.auc = tf.reduce_sum(\n","            ((tf.sign(self.pos_logits - self.neg_logits) + 1) / 2) * istarget\n","        ) / tf.reduce_sum(istarget)\n","\n","        tf.summary.scalar('auc', self.auc)\n","        self.global_step = tf.Variable(0, name='global_step', trainable=False)\n","        self.optimizer = tf.train.AdamOptimizer(learning_rate=args.lr, beta2=0.98)\n","        self.train_op = self.optimizer.minimize(self.loss, global_step=self.global_step)\n","\n","        self.merged = tf.summary.merge_all()\n","\n","    def predict(self, sess, u, seq, item_idx):\n","        return sess.run(self.test_logits,\n","                        {self.u: u, self.input_seq: seq, self.test_item: item_idx, self.is_training: False})"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GJxaJNJYdzCZ"},"source":["## Run"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"G5pR-Ga4d4Ik","executionInfo":{"status":"ok","timestamp":1633206034646,"user_tz":-330,"elapsed":1567163,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}},"outputId":"8db84278-5dac-4d09-b0ed-ba1de50aff7f"},"source":["def str2bool(s):\n","    if s not in {'False', 'True'}:\n","        raise ValueError('Not a valid boolean string')\n","    return s == 'True'\n","\n","args = parser.parse_args(args={})\n","if not os.path.isdir(args.dataset + '_' + args.train_dir):\n","    os.makedirs(args.dataset + '_' + args.train_dir)\n","with open(os.path.join(args.dataset + '_' + args.train_dir, 'args.txt'), 'w') as f:\n","    params = '\\n'.join([str(k) + ',' + str(v) \n","        for k, v in sorted(vars(args).items(), key=lambda x: x[0])])\n","    print(params)\n","    f.write(params)\n","\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=str(args.gpu)\n","\n","dataset = data_partition(args.dataset)\n","[user_train, user_valid, user_test, usernum, itemnum] = dataset\n","num_batch = len(user_train) // args.batch_size\n","cc = 0.0\n","max_len = 0\n","for u in user_train:\n","    cc += len(user_train[u])\n","    max_len = max(max_len, len(user_train[u]))\n","print(\"\\nThere are {0} users {1} items \\n\".format(usernum, itemnum))\n","print(\"Average sequence length: {0}\\n\".format(cc / len(user_train)))\n","print(\"Maximum length of sequence: {0}\\n\".format(max_len))\n","\n","f = open(os.path.join(args.dataset + '_' + args.train_dir, 'log.txt'), 'w')\n","config = tf.ConfigProto()\n","config.gpu_options.allow_growth = True\n","config.allow_soft_placement = True\n","sess = tf.Session(config=config)\n","\n","sampler = WarpSampler(user_train, usernum, itemnum, \n","            batch_size=args.batch_size, maxlen=args.maxlen,\n","            threshold_user=args.threshold_user, \n","            threshold_item=args.threshold_item,\n","            n_workers=3)\n","model = Model(usernum, itemnum, args)\n","sess.run(tf.global_variables_initializer())\n","\n","T = 0.0\n","t_test = evaluate(model, dataset, args, sess)\n","t_valid = evaluate_valid(model, dataset, args, sess)\n","print(\"epoch:0, time: 0.0(s), valid (NDCG@10: %.4f, HR@10: %.4f), test (NDCG@10: %.4f, HR@10: %.4f)\".format(t_valid[0], t_valid[1], t_test[0], t_test[1]))\n","\n","t0 = time.time()\n","\n","for epoch in range(1, args.num_epochs + 1):\n","    for step in range(num_batch):\n","        u, seq, pos, neg = sampler.next_batch()\n","        user_emb_table, item_emb_table, attention, auc, loss, _ = sess.run([model.user_emb_table, model.item_emb_table, model.attention, model.auc, model.loss, model.train_op],\n","                                    {model.u: u, model.input_seq: seq, model.pos: pos, model.neg: neg,\n","                                     model.is_training: True})\n","        #if epoch % args.print_freq == 0:\n","        #    with open(\"attention_map_{}.pickle\".format(step), 'wb') as fw:\n","        #        pickle.dump(attention, fw)\n","        #    with open(\"batch_{}.pickle\".format(step), 'wb') as fw:\n","        #        pickle.dump([u, seq], fw)\n","        #    with open(\"user_emb.pickle\", 'wb') as fw:\n","        #        pickle.dump(user_emb_table, fw)\n","        #    with open(\"item_emb.pickle\", 'wb') as fw:\n","        #        pickle.dump(item_emb_table, fw)\n","    if epoch % args.print_freq == 0:\n","        t1 = time.time() - t0\n","        T += t1\n","        #print 'Evaluating',\n","        t_test = evaluate(model, dataset, args, sess)\n","        t_valid = evaluate_valid(model, dataset, args, sess)\n","        print('')\n","        print('epoch:%d, time: %f(s), valid (NDCG@10: %.4f, HR@10: %.4f), test (NDCG@10: %.4f, HR@10: %.4f)' % (\n","        epoch, T, t_valid[0], t_valid[1], t_test[0], t_test[1]))\n","        # print(\"[Epoch:{0}, T:{1}, t_valid[0]:{2}, t_valid[1]:{3}, t_test[0]:{4}, t_test[1]{5}],\".format(epoch, T, t_valid[0], t_valid[1], t_test[0], t_test[1]))\n","        #f.write(str(t_valid) + ' ' + str(t_test) + '\\n')\n","        #f.flush()\n","        t0 = time.time()\n","\n","f.close()\n","sampler.close()\n","print(\"Done\")"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["batch_size,128\n","dataset,ml1m\n","dropout_rate,0.5\n","gpu,0\n","item_hidden_units,50\n","k,10\n","l2_emb,0.0\n","lr,0.001\n","maxlen,50\n","num_blocks,2\n","num_epochs,100\n","num_heads,1\n","print_freq,5\n","threshold_item,1.0\n","threshold_user,1.0\n","train_dir,default\n","user_hidden_units,50\n","\n","There are 6040 users 3416 items \n","\n","Average sequence length: 163.49850993377484\n","\n","Maximum length of sequence: 2275\n","\n","WARNING:tensorflow:From <ipython-input-5-1791fbafa0ae>:10: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use `tf.cast` instead.\n","WARNING:tensorflow:\n","The TensorFlow contrib module will not be included in TensorFlow 2.0.\n","For more information, please see:\n","  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n","  * https://github.com/tensorflow/addons\n","  * https://github.com/tensorflow/io (for I/O related ops)\n","If you depend on functionality not listed there, please file an issue.\n","\n","INFO:tensorflow:Scale of 0 disables regularizer.\n","INFO:tensorflow:Scale of 0 disables regularizer.\n","INFO:tensorflow:Scale of 0 disables regularizer.\n","INFO:tensorflow:Scale of 0 disables regularizer.\n","WARNING:tensorflow:From <ipython-input-5-1791fbafa0ae>:73: dropout (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use keras.layers.dropout instead.\n","WARNING:tensorflow:From /tensorflow-1.15.2/python3.7/tensorflow_core/python/layers/core.py:271: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Please use `layer.__call__` method instead.\n","WARNING:tensorflow:From <ipython-input-4-15e366af4f30>:157: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use keras.layers.Dense instead.\n","WARNING:tensorflow:From <ipython-input-4-15e366af4f30>:178: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use tf.where in 2.0, which has the same broadcast rule as np.where\n","WARNING:tensorflow:From <ipython-input-4-15e366af4f30>:240: conv1d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use `tf.keras.layers.Conv1D` instead.\n","epoch:0, time: 0.0(s), valid (NDCG@10: %.4f, HR@10: %.4f), test (NDCG@10: %.4f, HR@10: %.4f)\n","\n","epoch:5, time: 13.194270(s), valid (NDCG@10: 0.2432, HR@10: 0.4414), test (NDCG@10: 0.2467, HR@10: 0.4490)\n","\n","epoch:10, time: 25.101426(s), valid (NDCG@10: 0.2871, HR@10: 0.5119), test (NDCG@10: 0.2733, HR@10: 0.4983)\n","\n","epoch:15, time: 36.982534(s), valid (NDCG@10: 0.3352, HR@10: 0.5856), test (NDCG@10: 0.3245, HR@10: 0.5714)\n","\n","epoch:20, time: 49.052373(s), valid (NDCG@10: 0.4141, HR@10: 0.6717), test (NDCG@10: 0.3990, HR@10: 0.6541)\n","\n","epoch:25, time: 60.990634(s), valid (NDCG@10: 0.4684, HR@10: 0.7318), test (NDCG@10: 0.4499, HR@10: 0.7089)\n","\n","epoch:30, time: 72.758215(s), valid (NDCG@10: 0.5000, HR@10: 0.7623), test (NDCG@10: 0.4730, HR@10: 0.7296)\n","\n","epoch:35, time: 84.473271(s), valid (NDCG@10: 0.5109, HR@10: 0.7712), test (NDCG@10: 0.4823, HR@10: 0.7435)\n","\n","epoch:40, time: 96.489361(s), valid (NDCG@10: 0.5245, HR@10: 0.7796), test (NDCG@10: 0.4905, HR@10: 0.7472)\n","\n","epoch:45, time: 108.657240(s), valid (NDCG@10: 0.5286, HR@10: 0.7833), test (NDCG@10: 0.4946, HR@10: 0.7492)\n","\n","epoch:50, time: 120.731854(s), valid (NDCG@10: 0.5330, HR@10: 0.7894), test (NDCG@10: 0.5004, HR@10: 0.7531)\n","\n","epoch:55, time: 132.606926(s), valid (NDCG@10: 0.5361, HR@10: 0.7907), test (NDCG@10: 0.5035, HR@10: 0.7526)\n","\n","epoch:60, time: 144.529617(s), valid (NDCG@10: 0.5399, HR@10: 0.7954), test (NDCG@10: 0.5042, HR@10: 0.7573)\n","\n","epoch:65, time: 156.635010(s), valid (NDCG@10: 0.5381, HR@10: 0.7969), test (NDCG@10: 0.5055, HR@10: 0.7553)\n","\n","epoch:70, time: 168.605777(s), valid (NDCG@10: 0.5350, HR@10: 0.7930), test (NDCG@10: 0.5028, HR@10: 0.7591)\n","\n","epoch:75, time: 180.586512(s), valid (NDCG@10: 0.5422, HR@10: 0.7959), test (NDCG@10: 0.5023, HR@10: 0.7512)\n","\n","epoch:80, time: 192.596378(s), valid (NDCG@10: 0.5437, HR@10: 0.7912), test (NDCG@10: 0.5051, HR@10: 0.7546)\n","\n","epoch:85, time: 204.121164(s), valid (NDCG@10: 0.5411, HR@10: 0.7934), test (NDCG@10: 0.5033, HR@10: 0.7551)\n","\n","epoch:90, time: 215.967192(s), valid (NDCG@10: 0.5356, HR@10: 0.7889), test (NDCG@10: 0.4992, HR@10: 0.7477)\n","\n","epoch:95, time: 227.702345(s), valid (NDCG@10: 0.5408, HR@10: 0.7940), test (NDCG@10: 0.5006, HR@10: 0.7526)\n","\n","epoch:100, time: 239.351160(s), valid (NDCG@10: 0.5377, HR@10: 0.7957), test (NDCG@10: 0.4963, HR@10: 0.7483)\n","Done\n"]}]}]}