{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"reco-tut-arr-03-preprocessing-variant-II.ipynb","provenance":[{"file_id":"1kAAUEeSIGUEJT_BNMbqxrgPhTrvSFZhr","timestamp":1628010467819},{"file_id":"1M2q2VhJJgJfHLO9QmIitsz6uUksKiePE","timestamp":1628006762969},{"file_id":"1SykHQiR4gObGp9AodqNRrbnZc67M8ihM","timestamp":1627827802496}],"collapsed_sections":[],"mount_file_id":"1-gLtOaVLxIhFUTbUYpmGSJ27ywLQiLJN","authorship_tag":"ABX9TyO9Doo/dapSdV0D3nkaV9mG"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"id":"-UOOzCs9ukul","executionInfo":{"status":"ok","timestamp":1628010535825,"user_tz":-330,"elapsed":746,"user":{"displayName":"Sparsh Agarwal","photoUrl":"","userId":"13037694610922482904"}}},"source":["project_name = \"reco-tut-arr\"; branch = \"main\"; account = \"sparsh-ai\""],"execution_count":1,"outputs":[]},{"cell_type":"code","metadata":{"id":"yjoT7OzOxK8t"},"source":["import os\n","\n","if not os.path.exists('/content/reco-tut-arr'):\n","    !cp /content/drive/MyDrive/mykeys.py /content\n","    import mykeys\n","    !rm /content/mykeys.py\n","    path = \"/content/\" + project_name; \n","    !mkdir \"{path}\"\n","    %cd \"{path}\"\n","    import sys; sys.path.append(path)\n","    !git config --global user.email \"arr@recohut.com\"\n","    !git config --global user.name  \"reco-tut-arr\"\n","    !git init\n","    !git remote add origin https://\"{mykeys.git_token}\":x-oauth-basic@github.com/\"{account}\"/\"{project_name}\".git\n","    !git pull origin \"{branch}\"\n","    !git checkout main\n","else:\n","    %cd '/content/reco-tut-arr'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"h75rrYXdzr6r","executionInfo":{"status":"ok","timestamp":1628010642398,"user_tz":-330,"elapsed":451,"user":{"displayName":"Sparsh Agarwal","photoUrl":"","userId":"13037694610922482904"}}},"source":["import sys\n","sys.path.insert(0,f'/content/{project_name}/code')\n","from utils import *"],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"id":"gIypykr-_z0_","executionInfo":{"status":"ok","timestamp":1628010666105,"user_tz":-330,"elapsed":3921,"user":{"displayName":"Sparsh Agarwal","photoUrl":"","userId":"13037694610922482904"}}},"source":["import pandas as pd\n","import torch\n","from numpy import log, sqrt"],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"id":"Z12sTqxTDvMB"},"source":["vendors = pd.read_parquet('./data/bronze/vendors.parquet.gz')\n","orders = pd.read_parquet('./data/bronze/orders.parquet.gz')\n","train_customers = pd.read_parquet('./data/bronze/train_customers.parquet.gz')\n","train_locations = pd.read_parquet('./data/bronze/train_locations.parquet.gz')\n","test_customers = pd.read_parquet('./data/bronze/test_customers.parquet.gz')\n","test_locations = pd.read_parquet('./data/bronze/test_locations.parquet.gz')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-mzLLYdi_4qx"},"source":["#####################################\n","##         Process Orders          ##\n","#####################################\n","\n","# Train / Test split\n","train_orders = orders[orders['customer_id'].isin(train_customers['akeed_customer_id'])]\n","test_orders = orders[orders['customer_id'].isin(test_customers['akeed_customer_id'])]\n","\n","# Remove duplicate customers and their orders\n","x = train_customers.groupby('akeed_customer_id').size()\n","duplicate_train_customers = train_customers[train_customers['akeed_customer_id'].isin(x[x>1].index)]['akeed_customer_id'].unique()\n","train_customers = train_customers[~train_customers['akeed_customer_id'].isin(duplicate_train_customers)]\n","train_orders = train_orders[~train_orders['customer_id'].isin(duplicate_train_customers)]\n","\n","# Sort orders by datetime\n","train_orders['created_at'] = pd.to_datetime(train_orders['created_at'])\n","train_orders.sort_values(by=['created_at'], inplace=True)\n","\n","# Map vendor ids to range(0,num_vendors)\n","train_orders, v_id_map, v_inv_map = integer_encoding(df=train_orders, cols=['vendor_id'], drop_old=True, monotone_mapping=True)\n","\n","\n","#####################################\n","##         Process Vendors         ##\n","#####################################\n","\n","# Remap id column and set to index\n","vendors['id'] = vendors['id'].map(v_id_map)\n","vendors.set_index('id', inplace=True)\n","vendors.sort_index(inplace=True)\n","\n","# Fill primary_tags na with -1 & strip unnecessary characters\n","vendors['primary_tags'] = vendors['primary_tags'].fillna(\"{\\\"primary_tags\\\":\\\"-1\\\"}\").apply(lambda x: int(str(x).split(\"\\\"\")[3]))\n","\n","# Fill vendor_tag na with -1 & convert to list-valued\n","vendors['vendor_tag'] = vendors['vendor_tag'].fillna(str(-1)).apply(lambda x: x.split(\",\")).apply(lambda x: [int(i) for i in x])\n","\n","# Fix an incorrect vendor_category_id\n","vendors.loc[28, 'vendor_category_id'] = 3.0\n","\n","# Get unique vendor tags\n","vendor_tags = [int(tag) for tag in vendors['vendor_tag'].explode().unique()]\n","vendor_tags.sort()\n","\n","# Map vendor tags to range(len(vendor_tag)) monotonically\n","vendor_map = dict()\n","for i, tag in enumerate(vendor_tags):\n","    vendor_map[tag] = i\n","vendors['vendor_tag'] = vendors['vendor_tag'].apply(lambda tags: [vendor_map[tag] for tag in tags])\n","\n","# Add num_orders, amt_sales, and avg_sale as new columns in vendor table\n","orders_vendor_grp = train_orders.groupby(by=['vendor_id'])\n","orders_per_vendor = orders_vendor_grp['akeed_order_id'].count().rename('num_orders')\n","grand_total_per_vendor = orders_vendor_grp['grand_total'].sum().rename('amt_sales')\n","vendors = vendors.merge(orders_per_vendor, how='left', left_on='id', right_index=True)\n","vendors = vendors.merge(grand_total_per_vendor, how='left', left_on='id', right_index=True)\n","vendors['avg_sale'] = vendors['amt_sales'] / vendors['num_orders']\n","\n","# Log-transform to pad the skewness\n","vendors['num_orders_log3'] = vendors['num_orders'].apply(log).apply(log).apply(log)\n","vendors['amt_sales_log3'] = vendors['amt_sales'].apply(log).apply(log).apply(log)\n","vendors['avg_sale_log'] = vendors['avg_sale'].apply(log)\n","\n","# Define location outliers\n","lat_lo, lat_hi = -25, 25\n","long_lo, long_hi = -5, 5\n","v_outliers = (vendors['latitude'] < lat_lo) | (vendors['latitude'] > lat_hi) | (vendors['longitude'] < long_lo) | (vendors['longitude'] > long_hi)\n","\n","# Project vendor outliers\n","for i in vendors[v_outliers].index:\n","        lat = vendors.loc[i, 'latitude']\n","        long = vendors.loc[i, 'longitude']\n","        mag = sqrt(lat**2 + long**2)\n","        vendors.loc[i, 'latitude'] = lat / mag * lat_hi\n","        vendors.loc[i, 'longitude'] = long / mag * long_hi\n","\n","# Choose which columns to use\n","keep_continuous = ['latitude', 'longitude', 'delivery_charge', 'serving_distance', 'prepration_time', 'vendor_rating', 'num_orders_log3', 'amt_sales_log3', 'avg_sale_log']\n","keep_categorical = ['vendor_category_id', 'status', 'rank', 'primary_tags', 'vendor_tag']\n","keep_columns = keep_continuous + keep_categorical\n","vendors = vendors[keep_columns]\n","\n","# Encode categorical columns\n","vendors, _, _ = integer_encoding(df=vendors, cols=['vendor_category_id', 'delivery_charge', 'status', 'rank', 'primary_tags'], drop_old=True, monotone_mapping=True)\n","vendors = multiclass_list_encoding(df=vendors, cols=['primary_tags', 'vendor_tag'], drop_old=True)\n","\n","# Send to Pytorch tensor\n","v_matrix = torch.tensor(vendors)\n","\n","\n","#####################################\n","##         Process Customers       ##\n","#####################################\n","\n","# Add num_orders as new column in customer table\n","orders_per_customer = train_orders.groupby('customer_id')['akeed_order_id'].count().rename('num_orders')\n","train_customers = train_customers.merge(orders_per_customer, how='left', left_on='akeed_customer_id', right_index=True)\n","\n","# Remove customers with no orders\n","train_customers = train_customers[train_customers['num_orders'] > 0]\n","\n","# For each customer, get the sequence of their orders over all locations\n","train_sequences = get_sequences(df=train_orders, target='vendor_id', group_by=['customer_id'])\n","\n","# Represent customers as averages of the vendors they purchased from\n","train_customer_encoded = pool_encodings_from_sequences(sequences=train_sequences, pool_from=vendors)"],"execution_count":null,"outputs":[]}]}