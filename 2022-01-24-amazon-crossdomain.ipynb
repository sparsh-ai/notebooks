{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.4"},"colab":{"name":"2022-01-24-amazon-crossdomain.ipynb","provenance":[{"file_id":"https://github.com/recohut/nbs/blob/main/raw/T266229%20%7C%20Amazon%20Dataset%20Cross-domain%20Data%20Preparation.ipynb","timestamp":1644669105869},{"file_id":"https://github.com/nicknochnack/OpenAI-Reinforcement-Learning-with-Custom-Environment/blob/main/OpenAI%20Custom%20Environment%20Reinforcement%20Learning.ipynb","timestamp":1637650107008}],"collapsed_sections":[]}},"cells":[{"cell_type":"markdown","source":["# Amazon Dataset Cross-domain Data Preparation"],"metadata":{"id":"DtLE6DEP63m3"}},{"cell_type":"markdown","metadata":{"id":"K5TnHUXmwb4Y"},"source":["## Dataset"]},{"cell_type":"code","metadata":{"id":"Lm8C92VVxxwN"},"source":["import _pickle as pickle\n","import gc\n","import gzip\n","import json\n","import os\n","\n","import numpy as np\n","import pandas as pd"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LhmRviJQwb7O","executionInfo":{"status":"ok","timestamp":1637670649664,"user_tz":-330,"elapsed":28422,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}},"outputId":"0d94b739-ede1-408a-d2a0-4503f2da8b03"},"source":["!wget -q --show-progress http://deepyeti.ucsd.edu/jianmo/amazon/categoryFilesSmall/Movies_and_TV_5.json.gz\n","!wget -q --show-progress http://deepyeti.ucsd.edu/jianmo/amazon/categoryFilesSmall/Sports_and_Outdoors_5.json.gz"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Movies_and_TV_5.jso 100%[===================>] 754.66M  44.5MB/s    in 17s     \n","Sports_and_Outdoors 100%[===================>] 395.12M  42.2MB/s    in 9.7s    \n"]}]},{"cell_type":"code","metadata":{"id":"zVE3aLmLxztt"},"source":["def load_pickle(filename):\n","    return pickle.load(open(filename, \"rb\"))\n","\n","\n","def parse(path_tmp):\n","    g = gzip.open(path_tmp, 'rb')\n","    for line in g:\n","        yield json.loads(line)\n","\n","\n","def getDF(path_tmp):\n","    i = 0\n","    df = {}\n","    for d in parse(path_tmp):\n","        df[i] = d\n","        i += 1\n","    return pd.DataFrame.from_dict(df, orient='index')\n","\n","\n","def generate_k_core(df, k):\n","    # on users\n","    # tmp1 = df.groupby(['reviewerID'], as_index=False)['asin'].count()\n","    # tmp1.rename(columns={'asin': 'cnt_item'}, inplace=True)\n","    # on items\n","    tmp2 = df.groupby(['asin'], as_index=False)['reviewerID'].count()\n","    tmp2.rename(columns={'reviewerID': 'cnt_user'}, inplace=True)\n","\n","    df = df.merge(tmp2, on=['asin'])\n","    query = \"cnt_user >= %d\" % k\n","    df = df.query(query).reset_index(drop=True).copy()\n","    df.drop(['cnt_user'], axis=1, inplace=True)\n","    del tmp2\n","    gc.collect()\n","    return df\n","\n","\n","def info(data):\n","    \"\"\" num of user, item, max/min uid/itemID, total interaction\"\"\"\n","    user = set(data['reviewerID'].tolist())\n","    item = set(data['asin'].tolist())\n","    print(\"number of user: \", len(user))\n","    print(\"max user ID: \", max(user))\n","    print(\"Min user ID: \", min(user))\n","    print(\"number of Item: \", len(item))\n","    print(\"Max item ID: \", max(item))\n","    print(\"Min item ID: \", min(item))\n","    print(\"Interactions: \", len(data))\n","\n","\n","def user_limit(df, k=5):\n","    # select users with more than (include) k interacted items\n","    tmp1 = df.groupby(['reviewerID'], as_index=False)['asin'].count()\n","    tmp1.rename(columns={'asin': 'cnt_item'}, inplace=True)\n","    df = df.merge(tmp1, on=['reviewerID'])\n","    query = \"cnt_item >= %d\" % k\n","    df = df.query(query).reset_index(drop=True).copy()\n","    df.drop(['cnt_item'], axis=1, inplace=True)\n","    del tmp1\n","    gc.collect()\n","    return df\n","\n","\n","def category(df):\n","    # reid users and items from 0-N\n","    df['reviewerID'] = pd.Categorical(df.uid).codes\n","    df['asin'] = pd.Categorical(df.asin).codes\n","    df.sort_values(['reviewerID', 'unixReviewTime'], inplace=True)\n","    return df\n","\n","\n","def clean(df):\n","    # pre-processing on the original dataset TODO: change parameters accordingly.\n","    print(\"======================Information of Original dataset:\")\n","    info(df)\n","    # clean with overall verified value. And selected time span.\n","    df = df[df.overall >= 3]                 # select positive interactions.\n","    df = df[df.verified]\n","    df = df[df.unixReviewTime > 1506816000]  # interactions after 2017, 10, 1, 0, 0, 0\n","    # earliest --> 2016, 10, 1, 0, 0, 0; 1506816000 --> 2017, 10, 1, 0, 0, 0\n","\n","    # drop redundant columns\n","    df = df.drop([\"overall\", \"verified\", 'reviewerName', 'reviewText', 'summary', 'vote', 'style', 'image'], axis=1)\n","    print(\"======================Information of cleaned dataset:\")\n","    info(df)\n","\n","    # k-core\n","    df = generate_k_core(df, k=5)  # item\n","    print(\"======================Information of %d-core dataset:\" % 5)\n","    info(df)\n","\n","    df = user_limit(df, k=5)   # user\n","    print(\"======================Information of dataset where all users have at least %d interaction:\" % 5)\n","    info(df)\n","\n","    # Convert User_ID and Item_ID and sort by reviewerID and time\n","    # df['reviewerID'] = pd.Categorical(df.reviewerID).codes\n","    # df['reviewerID'] = df[\"reviewerID\"] + 1\n","    df['asin'] = pd.Categorical(df.asin).codes\n","    df['asin'] = df[\"asin\"] + 1\n","    df.sort_values(by=['reviewerID', \"unixReviewTime\"], inplace=True)\n","\n","    df.head()\n","    print(\"======================final single-domain data convert to categorical data\")\n","    info(df)\n","    # to list\n","    item_list = df.groupby('reviewerID')['asin'].apply(list).reset_index(name='asin_list')\n","    # ts_list = df.groupby('reviewerID')['unixReviewTime'].apply(list).reset_index(name='ts_list')\n","\n","    return item_list\n","\n","\n","def main_process(source_file, outfile):\n","    a_df = getDF(source_file)\n","    a_df_cleaned_list = clean(a_df)\n","    print(a_df_cleaned_list.head())\n","    a_df_cleaned_list.to_csv(outfile, index=False, header=True)\n","    # generate_tfrecord(a_df_cleaned_list, out_tf, in_type=\"df\")\n","\n","\n","def find_overlap(df1, df2):\n","    a_users = df1['reviewerID'].values\n","    b_users = df2['reviewerID'].values\n","    overlap_users = []\n","    for user in a_users:\n","        if user in b_users:\n","            overlap_users.append(user)\n","    print(len(a_users))\n","    print(len(b_users))\n","    print(len(overlap_users))\n","    print(\"saving overlap data\")\n","    overlap_data = {}\n","    for index, data_line in df1.iterrows():\n","        if data_line[\"reviewerID\"] in overlap_users:\n","            overlap_data[data_line[\"reviewerID\"]] = [data_line[\"asin_list\"]]\n","\n","    for index, data_line in df2.iterrows():\n","        if data_line[\"reviewerID\"] in overlap_users:\n","            if data_line[\"reviewerID\"] in overlap_data:\n","                overlap_data[data_line[\"reviewerID\"]].append(data_line[\"asin_list\"])\n","            else:\n","                print(\"wrong user\")\n","    return overlap_data\n","\n","\n","def statistic_pub(df1, df2):\n","    a_users = df1['reviewerID'].values\n","    b_users = df2['reviewerID'].values\n","    overlap_users = []\n","    a_only = []\n","    b_only = []\n","    for user in a_users:\n","        if user in b_users:\n","            overlap_users.append(user)\n","        else:\n","            a_only.append(user)\n","    for user in b_users:\n","        if user not in overlap_users:\n","            b_only.append(user)\n","    print(f\"number of users in domain a: {len(a_users)}\")\n","    print(len(b_users))\n","    print(len(a_only))\n","    print(len(b_only))\n","    print(len(overlap_users))\n","    print(\"saving overlap data\")\n","    # a_only: users with interactions only in a domain\n","    # b_only: users with interactions only in b domain\n","    # x_user: overlapped users.\n","    data_info = {\"a_only\": a_only, \"b_only\": b_only, \"x_user\": overlap_users}\n","\n","    return data_info\n","\n","\n","def str2int(in_str):\n","    \"\"\"\n","    input sample: '[11506, 10463, 34296, 15541]'\n","    \"\"\"\n","    data_list = in_str.strip()[1:-1].split(\",\")  # remove \"[]\" and split\n","    data = list(map(int, data_list))\n","    return data\n","\n","\n","def pf2pickle_over(overlap_dict, outfile):\n","    \"\"\"\n","    save overlapped data.\n","    \"\"\"\n","    data_pickle = {\"seq_a\": [], \"len_a\": [], \"val_a\": [], \"test_a\": [],\n","                   \"seq_b\": [], \"len_b\": [], \"val_b\": [], \"test_b\": []}\n","    users = list(overlap_dict.keys())\n","    num_recorded = 0\n","    for user in users:\n","        a_behavior, b_behavior = overlap_dict[user]\n","        a_behavior = str2int(a_behavior)\n","        b_behavior = str2int(b_behavior)\n","\n","        # if len(a_behavior) < 5 or len(b_behavior) < 5:\n","        #   continue\n","\n","        val = a_behavior[-2]\n","        test = a_behavior[-1]\n","        behavior = a_behavior[:-2]\n","\n","        val_b = b_behavior[-2]\n","        test_b = b_behavior[-1]\n","        behavior_b = b_behavior[:-2]\n","\n","        num_recorded += 1\n","        data_pickle[\"seq_a\"].append(behavior)\n","        data_pickle[\"len_a\"].append(len(behavior))\n","        data_pickle[\"val_a\"].append(val)\n","        data_pickle[\"test_a\"].append(test)\n","\n","        data_pickle[\"seq_b\"].append(behavior_b)\n","        data_pickle[\"len_b\"].append(len(behavior_b))\n","        data_pickle[\"val_b\"].append(val_b)\n","        data_pickle[\"test_b\"].append(test_b)\n","\n","    print(\"save number of samples: \", num_recorded)\n","    with open(outfile, \"wb\") as fid:\n","        pickle.dump(data_pickle, fid, -1)\n","\n","\n","def df2pickle(infile, outfile, in_type):\n","    \"\"\"\n","    train, valid, and test split and store all results into pickle files.\n","    \"\"\"\n","    if in_type == \"pickle\":\n","        data = pickle.load(open(infile, \"rb\"))\n","    elif in_type == \"txt\":\n","        data = open(infile, \"r\")\n","    elif in_type == \"df\":\n","        # data = pd.read_csv(infile)  # if input a filename\n","        # data = data.iterrows()\n","        data = infile.iterrows()\n","    else:\n","        data = infile\n","    # result pickle file\n","    data_pickle = {\"seq\": [], \"len\": [], \"val\": [], \"test\": []}\n","\n","    for data_line in data:\n","        if in_type == \"txt\":\n","            line_splits = data_line.strip().split(\"|\")\n","            user, behavior, timestamp = line_splits\n","            behavior = behavior.split(\",\")\n","            behavior = list(map(int, behavior))\n","        elif in_type == \"pickle\":  # pickle data\n","            user, behavior, timestamp = data_line[0], data_line[1], data_line[2]\n","        elif in_type == \"df\" or in_type == \"csv\":\n","            user, behavior, timestamp = data_line[1][\"reviewerID\"], data_line[1][\"asin_list\"], None\n","            if type(behavior) == str:\n","                behavior = [int(val) for val in behavior[1:-1].strip().split(\",\")]\n","        else:\n","            behavior = [int(val) for val in data_line[1:-1].strip().split(\",\")]\n","        # if len(behavior) < 3:  # already 5-cores.\n","        #    continue\n","        val = behavior[-2]\n","        test = behavior[-1]\n","        behavior = behavior[:-2]\n","\n","        data_pickle[\"seq\"].append(behavior)\n","        data_pickle[\"len\"].append(len(behavior))\n","        data_pickle[\"val\"].append(val)\n","        data_pickle[\"test\"].append(test)\n","\n","    print(\"saving\", outfile)\n","    with open(outfile, \"wb\") as fid:\n","        pickle.dump(data_pickle, fid, -1)\n","\n","\n","def cross_data(df1, df2, data_info, a_name, b_name, path):\n","    \"\"\"\n","    Args:\n","        df1: dataframe in domain a\n","        df2: dataframe in domain b\n","        data_info: which {\"a_only\": [], \"b_only\": [], \"x_users\": []}\n","        a_name: name of domain a\n","        b_name: name of domain b\n","        path: out path.\n","    Returns:\n","        a_name.pickle\n","        a_name_only.pickle\n","        b_name.pickle\n","        b_name_only.pickle\n","        \"a_name\"_\"b_name\".pickle\n","    \"\"\"\n","    # Out put config\n","    a_all_name = os.path.join(path, a_name + \".pickle\")\n","    b_all_name = os.path.join(path, b_name + \".pickle\")\n","    a_only_name = os.path.join(path, a_name + \"_only.pickle\")\n","    b_only_name = os.path.join(path, b_name + \"_only.pickle\")\n","    overlap_name = os.path.join(path, a_name + \"_\" + b_name + \".pickle\")\n","\n","    # all users in each domain.\n","    df2pickle(df1, a_all_name, in_type=\"df\")\n","    df2pickle(df2, b_all_name, in_type=\"df\")\n","\n","    # overlap data\n","    overlap_users = data_info[\"x_user\"]\n","    overlap_data = {}\n","    for _, data_line in df1.iterrows():\n","        if data_line[\"reviewerID\"] in overlap_users:\n","            overlap_data[data_line[\"reviewerID\"]] = [data_line[\"asin_list\"]]\n","\n","    for _, data_line in df2.iterrows():\n","        if data_line[\"reviewerID\"] in overlap_users:\n","            if data_line[\"reviewerID\"] in overlap_data:\n","                overlap_data[data_line[\"reviewerID\"]].append(data_line[\"asin_list\"])\n","            else:\n","                print(\"wrong user\")\n","    pf2pickle_over(overlap_data, overlap_name)\n","\n","    # domain-specific users\n","    a_only_data = find_domain_only_data(df1, overlap_users)\n","    b_only_data = find_domain_only_data(df2, overlap_users)\n","\n","    df2pickle(a_only_data, a_only_name, in_type=\"list\")\n","    df2pickle(b_only_data, b_only_name, in_type=\"list\")\n","\n","\n","def find_domain_only_data(df, overlap_user):\n","    data = []\n","    for _, d_row in df.iterrows():\n","        if d_row[\"reviewerID\"] not in overlap_user:\n","            data.append(d_row[\"asin_list\"])\n","    return data\n","\n","\n","def data_frequency(infile, outfile, total=None):\n","    data = load_pickle(infile)\n","    freq = np.zeros(total, dtype=float)\n","    for index in range(len(data[\"val\"])):\n","        seq = data[\"seq\"][index]\n","        val = data[\"val\"][index]\n","        test = data[\"test\"][index]\n","        for item in seq + [val] + [test]:\n","            freq[item - 1] += 1\n","    # check if there are items with 0 frequency. (which is incorrect.)\n","    zero_index = np.where(freq == 0)\n","    if len(zero_index[0]) > 0:\n","        print(zero_index)\n","    else:\n","        # add 0 at the beginning, for pad_index.\n","        freq = np.insert(freq, 0, 0, axis=0)\n","        with open(outfile, \"wb\") as fid:\n","            pickle.dump(freq, fid, -1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"umKlJoJgyAMc"},"source":["def freq_main():\n","    names = [\"book\", \"movie\", \"sport\", \"cloth\"]\n","    # in\n","    movie = \"movie.pickle\"\n","    book = \"book.pickle\"\n","    clothing = \"cloth.pickle\"\n","    sport = \"sport.pickle\"\n","    # out\n","    movie_o = \"movie_freq.pickle\"\n","    book_o = \"book_freq.pickle\"\n","    clothing_o = \"cloth_freq.pickle\"\n","    sport_o = \"sport_freq.pickle\"\n","    org_data = {\"book\": book, \"movie\": movie, \"sport\": sport, \"cloth\": clothing}\n","    output_data = {\"book\": book_o, \"movie\": movie_o, \"sport\": sport_o, \"cloth\": clothing_o}\n","    # total_all = [53014, 5637, 12790, 44512]    # [1, n]\n","    total_all = [51366, 5536, 11835, 42139]      # [1, n]\n","    for idx in range(len(names)):\n","        data_frequency(org_data[names[idx]], output_data[names[idx]], total=total_all[idx])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"o8Na_73yyGGG"},"source":["def form_cross_domain_sets(file_a, file_b, a_name, b_name, out_path):\n","    if not os.path.isdir(out_path):\n","        os.mkdir(out_path)\n","    a_data = pd.read_csv(file_a)\n","    b_data = pd.read_csv(file_b)\n","    data_info = statistic_pub(a_data, b_data)\n","    # print(len(data_info[0]))\n","    # cross_data(a_data, b_data, data_info, a_name, b_name, out_path, rate=0)\n","    cross_data(a_data, b_data, data_info, a_name, b_name, out_path)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"FVkwV2_ZzHgu"},"source":["!mkdir -p /content/guru/movie_sport"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"W-DBrp5A1Q__"},"source":["data_dir = \"/content\"\n","out_dir = \"/content/guru\"\n","\n","movie_source = os.path.join(data_dir, \"Movies_and_TV_5.json.gz\")\n","sport_source = os.path.join(data_dir, \"Sports_and_Outdoors_5.json.gz\")\n","\n","# preprocessed dataset\n","movie_out = os.path.join(out_dir, \"movie.csv\")\n","sport_out = os.path.join(out_dir, \"sport.csv\")\n","\n","main_process(movie_source, movie_out)\n","main_process(sport_source, sport_out)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8AXuW42MyERv","executionInfo":{"status":"ok","timestamp":1637671617130,"user_tz":-330,"elapsed":4628,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}},"outputId":"52872f93-a49d-48f2-bc9c-d96f8b7ab47f"},"source":["# form cross-domain datasets\n","form_cross_domain_sets(movie_out, sport_out, 'movie', 'sport',\n","                        out_path=os.path.join(out_dir, \"movie_sport\"))\n","\n","# calculate item frequency in each domain.\n","# freq_main()"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["number of users in domain a: 4261\n","9024\n","4156\n","8919\n","105\n","saving overlap data\n","saving /content/guru/movie_sport/movie.pickle\n","saving /content/guru/movie_sport/sport.pickle\n","save number of samples:  105\n","saving /content/guru/movie_sport/movie_only.pickle\n","saving /content/guru/movie_sport/sport_only.pickle\n"]}]},{"cell_type":"markdown","metadata":{"id":"7O4yDobS1sFG"},"source":["```\n",".\"a_domain\"-\"b_domain\"\n","├── a_only.pickle         # users in domain a only\n","├── b_only.pickle         # users in domain b only\n","├── a.pickle              # all users in domain a\n","├── b.pickle              # all users in domain b\n","├── a_b.pickle            # overlapped users of domain a and b  \n","```"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kEcjs8qTzE_1","executionInfo":{"status":"ok","timestamp":1637671962783,"user_tz":-330,"elapsed":1281,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}},"outputId":"67baf4dc-1f80-4ad2-90ef-3730deff4f12"},"source":["!git clone https://github.com/Chain123/RecGURU.git\n","%cd RecGURU"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Cloning into 'RecGURU'...\n","remote: Enumerating objects: 48, done.\u001b[K\n","remote: Counting objects: 100% (48/48), done.\u001b[K\n","remote: Compressing objects: 100% (43/43), done.\u001b[K\n","remote: Total 48 (delta 8), reused 40 (delta 4), pack-reused 0\u001b[K\n","Unpacking objects: 100% (48/48), done.\n","/content/RecGURU\n"]}]},{"cell_type":"code","metadata":{"id":"IaNUBsgB2v7r"},"source":[""],"execution_count":null,"outputs":[]}]}