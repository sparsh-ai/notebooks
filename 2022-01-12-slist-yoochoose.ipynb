{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"2022-01-12-slist-yoochoose.ipynb","provenance":[{"file_id":"https://github.com/recohut/notebook/blob/master/_notebooks/2022-01-12-slist-yoochoose.ipynb","timestamp":1644610170654},{"file_id":"https://github.com/recohut/nbs/blob/main/raw/P293191%20%7C%20SLIST%20on%20Yoochoose%20Preprocessed%20Sample%20Dataset.ipynb","timestamp":1644607241308}],"collapsed_sections":[],"mount_file_id":"1jAKIE2Lz_IL3da8Weo1SbuO7aE9pZHjH","authorship_tag":"ABX9TyO9ubD0hM7XQwObcEWdPcbO"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# SLIST on Yoochoose Preprocessed Sample Dataset"],"metadata":{"id":"TthNcibSQiab"}},{"cell_type":"markdown","source":["## Executive summary\n","\n","| | |\n","| --- | --- |\n","| Prblm Stmnt | The goal of session-based recommendation is to predict the next item(s) a user would likely choose to consume, given a sequence of previously consumed items in a session. Formally, we build a session-based model M(𝑠) that takes a session ⁍ for ⁍ as input and returns a list of top-𝑁 candidate items to be consumed as the next one ⁍. |\n","| Solution | Firstly, we devise two linear models focusing on different properties of sessions: (i) Session-aware Linear Item Similarity (SLIS) model aims at better handling session consistency, and (ii) Session-aware Linear Item Transition (SLIT) model focuses more on sequential dependency. With both SLIS and SLIT, we relax the constraint to incorporate repeated items and introduce a weighting scheme to take the timeliness of sessions into account. Combining these two types of models, we then suggest a unified model, namely Session-aware Item Similarity/Transition (SLIST) model, which is a generalized solution to holistically cover various properties of sessions. |\n","| Dataset | Yoochoose |\n","| Preprocessing | We discard the sessions having only one interaction and items appearing less than five times following the convention. We hold-out the sessions from the last 𝑁-days for test purposes and used the last 𝑁 days in the training set for the validation set. To evaluate session-based recommender models, we adopt the iterative revealing scheme, which iteratively exposes the item of a session to the model. Each item in the session is sequentially appended to the input of the model. Therefore, this scheme is useful for reflecting the sequential user behavior throughout a session |\n","| Metrics | HR, MRR, Coverage, Popularity |\n","| Models | SLIST\n","| Cluster | Python 3.x |\n","| Tags | LinearRecommender, SessionBasedRecommender |"],"metadata":{"id":"YxLc0jRFV3S-"}},{"cell_type":"markdown","source":["## Process flow\n","\n","![](https://github.com/RecoHut-Stanzas/S181315/raw/main/images/process_flow_prototype_1.svg)"],"metadata":{"id":"UYrwO2hmWLjr"}},{"cell_type":"markdown","source":["## Setup"],"metadata":{"id":"B_x3nAE9QZ_7"}},{"cell_type":"markdown","source":["### Imports"],"metadata":{"id":"I1mMP5dnQbML"}},{"cell_type":"code","source":["import os.path\n","import numpy as np\n","import pandas as pd\n","from _datetime import datetime, timezone, timedelta\n","\n","from tqdm import tqdm\n","import collections as col\n","import scipy\n","import os\n","import pickle\n","\n","from scipy import sparse\n","from scipy.sparse.linalg import inv\n","from scipy.sparse import csr_matrix, csc_matrix, vstack\n","from sklearn.preprocessing import normalize"],"metadata":{"id":"BZTJO2V8E6IJ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Dataset"],"metadata":{"id":"PP2SgjqSQhxB"}},{"cell_type":"markdown","source":["### Load data\n","\n","Preprocessed Yoochoose clicks 100k"],"metadata":{"id":"5SpMxtzFQcXy"}},{"cell_type":"code","source":["!mkdir -p prepared\n","!wget -O prepared/events_test.txt -q --show-progress https://github.com/RecoHut-Stanzas/S181315/raw/main/data/rsc15/prepared/yoochoose-clicks-100k_test.txt\n","!wget -O prepared/events_train_full.txt -q --show-progress https://github.com/RecoHut-Stanzas/S181315/raw/main/data/rsc15/prepared/yoochoose-clicks-100k_train_full.txt\n","!wget -O prepared/events_train_tr.txt -q --show-progress https://github.com/RecoHut-Stanzas/S181315/raw/main/data/rsc15/prepared/yoochoose-clicks-100k_train_tr.txt\n","!wget -O prepared/events_train_valid.txt -q --show-progress https://github.com/RecoHut-Stanzas/S181315/raw/main/data/rsc15/prepared/yoochoose-clicks-100k_train_valid.txt"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0iiHncDp6gvs","executionInfo":{"status":"ok","timestamp":1639118974794,"user_tz":-330,"elapsed":2453,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}},"outputId":"a6a1504e-a5bd-4c45-e1e2-7671e4535311"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["prepared/events_tes 100%[===================>] 404.64K  --.-KB/s    in 0.007s  \n","prepared/events_tra 100%[===================>]   2.05M  --.-KB/s    in 0.02s   \n","prepared/events_tra 100%[===================>]   1.55M  --.-KB/s    in 0.02s   \n","prepared/events_tra 100%[===================>] 493.18K  --.-KB/s    in 0.008s  \n"]}]},{"cell_type":"code","source":["def load_data_session( path, file, sessions_train=None, sessions_test=None, slice_num=None, train_eval=False ):\n","    '''\n","    Loads a tuple of training and test set with the given parameters. \n","    Parameters\n","    --------\n","    path : string\n","        Base path to look in for the prepared data files\n","    file : string\n","        Prefix of  the dataset you want to use.\n","        \"yoochoose-clicks-full\" loads yoochoose-clicks-full_train_full.txt and yoochoose-clicks-full_test.txt\n","    rows_train : int or None\n","        Number of rows to load from the training set file. \n","        This option will automatically filter the test set to only retain items included in the training set.  \n","    rows_test : int or None\n","        Number of rows to load from the test set file. \n","    slice_num : \n","        Adds a slice index to the constructed file_path\n","        yoochoose-clicks-full_train_full.0.txt\n","    density : float\n","        Percentage of the sessions to randomly retain from the original data (0-1). \n","        The result is cached for the execution of multiple experiments. \n","    Returns\n","    --------\n","    out : tuple of pandas.DataFrame\n","        (train, test)\n","    \n","    '''\n","    \n","    print('START load data') \n","    import time\n","    st = time.time()\n","    sc = time.perf_counter()\n","    \n","    split = ''\n","    if( slice_num != None and isinstance(slice_num, int ) ):\n","        split = '.'+str(slice_num)\n","    \n","    train_appendix = '_train_full'\n","    test_appendix = '_test'\n","    if train_eval:\n","        train_appendix = '_train_tr'\n","        test_appendix = '_train_valid'\n","            \n","    train = pd.read_csv(path + file + train_appendix +split+'.txt', sep='\\t' )\n","    test = pd.read_csv(path + file + test_appendix +split+'.txt', sep='\\t' )\n","        \n","    if( sessions_train != None ):\n","        keep = train.sort_values('Time', ascending=False).SessionId.unique()[:(sessions_train-1)]\n","        train = train[ np.in1d( train.SessionId, keep ) ]\n","        test = test[np.in1d(test.ItemId, train.ItemId)]\n","    \n","    if( sessions_test != None ):\n","        keep = test.SessionId.unique()[:(sessions_test)]\n","        test = test[ np.in1d( test.SessionId, keep ) ]\n","    \n","    session_lengths = test.groupby('SessionId').size()\n","    test = test[np.in1d(test.SessionId, session_lengths[ session_lengths>1 ].index)]\n","    \n","    #output\n","    data_start = datetime.fromtimestamp( train.Time.min(), timezone.utc )\n","    data_end = datetime.fromtimestamp( train.Time.max(), timezone.utc )\n","    \n","    print('Loaded train set\\n\\tEvents: {}\\n\\tSessions: {}\\n\\tItems: {}\\n\\tSpan: {} / {}\\n'.\n","          format( len(train), train.SessionId.nunique(), train.ItemId.nunique(), data_start.date().isoformat(), data_end.date().isoformat() ) )\n","    \n","    data_start = datetime.fromtimestamp( test.Time.min(), timezone.utc )\n","    data_end = datetime.fromtimestamp( test.Time.max(), timezone.utc )\n","    \n","    print('Loaded test set\\n\\tEvents: {}\\n\\tSessions: {}\\n\\tItems: {}\\n\\tSpan: {} / {}\\n'.\n","          format( len(test), test.SessionId.nunique(), test.ItemId.nunique(), data_start.date().isoformat(), data_end.date().isoformat() ) )\n","    \n","    check_data(train, test)\n","    \n","    print( 'END load data ', (time.perf_counter()-sc), 'c / ', (time.time()-st), 's' ) \n","    \n","    return (train, test)"],"metadata":{"id":"tkzyj627E9F2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def check_data( train, test ):\n","    \n","    if 'ItemId' in train.columns and 'SessionId' in train.columns:\n","    \n","        new_in_test = set( test.ItemId.unique() ) - set( train.ItemId.unique() )\n","        if len( new_in_test ) > 0:\n","            print( 'WAAAAAARRRNIIIIING: new items in test set' )\n","            \n","        session_min_train = train.groupby( 'SessionId' ).size().min()\n","        if session_min_train == 0:\n","            print( 'WAAAAAARRRNIIIIING: session length 1 in train set' )\n","            \n","        session_min_test = test.groupby( 'SessionId' ).size().min()\n","        if session_min_test == 0:\n","            print( 'WAAAAAARRRNIIIIING: session length 1 in train set' )\n","          \n","    else: \n","        print( 'data check not possible due to individual column names' )"],"metadata":{"id":"SioAVZHXFCP1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def evaluate_sessions(pr, metrics, test_data, train_data, items=None, cut_off=20, session_key='SessionId', item_key='ItemId', time_key='Time'):\n","    '''\n","    Evaluates the baselines wrt. recommendation accuracy measured by recall@N and MRR@N. Has no batch evaluation capabilities. Breaks up ties.\n","    Parameters\n","    --------\n","    pr : baseline predictor\n","        A trained instance of a baseline predictor.\n","    metrics : list\n","        A list of metric classes providing the proper methods\n","    test_data : pandas.DataFrame\n","        Test data. It contains the transactions of the test set.It has one column for session IDs, one for item IDs and one for the timestamp of the events (unix timestamps).\n","        It must have a header. Column names are arbitrary, but must correspond to the keys you use in this function.\n","    train_data : pandas.DataFrame\n","        Training data. Only required for selecting the set of item IDs of the training set.\n","    items : 1D list or None\n","        The list of item ID that you want to compare the score of the relevant item to. If None, all items of the training set are used. Default value is None.\n","    cut-off : int\n","        Cut-off value (i.e. the length of the recommendation list; N for recall@N and MRR@N). Defauld value is 20.\n","    session_key : string\n","        Header of the session ID column in the input file (default: 'SessionId')\n","    item_key : string\n","        Header of the item ID column in the input file (default: 'ItemId')\n","    time_key : string\n","        Header of the timestamp column in the input file (default: 'Time')\n","    Returns\n","    --------\n","    out :  list of tuples\n","        (metric_name, value)\n","    '''\n","\n","    actions = len(test_data)\n","    sessions = len(test_data[session_key].unique())\n","    count = 0\n","    print('START evaluation of ', actions, ' actions in ', sessions, ' sessions')\n","\n","    import time\n","    sc = time.perf_counter()\n","    st = time.time()\n","\n","    time_sum = 0\n","    time_sum_clock = 0\n","    time_count = 0\n","\n","    for m in metrics:\n","        m.reset()\n","\n","    test_data.sort_values([session_key, time_key], inplace=True)\n","    items_to_predict = train_data[item_key].unique()\n","    prev_iid, prev_sid = -1, -1\n","    pos = 0\n","\n","    for i in tqdm(range(len(test_data))):\n","\n","        # if count % 1000 == 0:\n","        #     print(f'eval process: {count} of  {actions} actions: {(count / actions * 100.0):.2f} % in {(time.time()-st):.2f} s')\n","\n","        sid = test_data[session_key].values[i]\n","        iid = test_data[item_key].values[i]\n","        ts = test_data[time_key].values[i]\n","        if prev_sid != sid:\n","            prev_sid = sid\n","            pos = 0\n","        else:\n","            if items is not None:\n","                if np.in1d(iid, items):\n","                    items_to_predict = items\n","                else:\n","                    items_to_predict = np.hstack(([iid], items))\n","\n","            crs = time.perf_counter()\n","            trs = time.time()\n","\n","            for m in metrics:\n","                if hasattr(m, 'start_predict'):\n","                    m.start_predict(pr)\n","\n","            preds = pr.predict_next(sid, prev_iid, items_to_predict, timestamp=ts)\n","\n","            for m in metrics:\n","                if hasattr(m, 'stop_predict'):\n","                    m.stop_predict(pr)\n","\n","            preds[np.isnan(preds)] = 0\n","#             preds += 1e-8 * np.random.rand(len(preds)) #Breaking up ties\n","            preds.sort_values(ascending=False, inplace=True)\n","\n","            time_sum_clock += time.perf_counter()-crs\n","            time_sum += time.time()-trs\n","            time_count += 1\n","\n","            for m in metrics:\n","                if hasattr(m, 'add'):\n","                    m.add(preds, iid, for_item=prev_iid, session=sid, position=pos)\n","\n","            pos += 1\n","\n","        prev_iid = iid\n","\n","        count += 1\n","\n","    print('\\nEND evaluation in ', (time.perf_counter()-sc), 'c / ', (time.time()-st), 's')\n","    print('    avg rt ', (time_sum/time_count), 's / ', (time_sum_clock/time_count), 'c')\n","    print('    time count ', (time_count), 'count/', (time_sum), ' sum')\n","\n","    res = []\n","    for m in metrics:\n","        if type(m).__name__ == 'Time_usage_testing':\n","            res.append(m.result_second(time_sum_clock/time_count))\n","            res.append(m.result_cpu(time_sum_clock / time_count))\n","        else:\n","            res.append(m.result())\n","\n","    return res"],"metadata":{"id":"D-KD8eRwFQFj"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Metrics"],"metadata":{"id":"QalAVbeRRMCr"}},{"cell_type":"code","source":["class MRR: \n","    '''\n","    MRR( length=20 )\n","    Used to iteratively calculate the average mean reciprocal rank for a result list with the defined length. \n","    Parameters\n","    -----------\n","    length : int\n","        MRR@length\n","    '''\n","    def __init__(self, length=20):\n","        self.length = length;\n","    \n","    def init(self, train):\n","        '''\n","        Do initialization work here.\n","        \n","        Parameters\n","        --------\n","        train: pandas.DataFrame\n","            Training data. It contains the transactions of the sessions. It has one column for session IDs, one for item IDs and one for the timestamp of the events (unix timestamps).\n","            It must have a header. Column names are arbitrary, but must correspond to the ones you set during the initialization of the network (session_key, item_key, time_key properties).\n","        '''\n","        return\n","        \n","    def reset(self):\n","        '''\n","        Reset for usage in multiple evaluations\n","        '''\n","        self.test=0;\n","        self.pos=0\n","        \n","        self.test_popbin = {}\n","        self.pos_popbin = {}\n","        \n","        self.test_position = {}\n","        self.pos_position = {}\n","    \n","    def skip(self, for_item = 0, session = -1 ):\n","        pass\n","        \n","    def add(self, result, next_item, for_item=0, session=0, pop_bin=None, position=None ):\n","        '''\n","        Update the metric with a result set and the correct next item.\n","        Result must be sorted correctly.\n","        \n","        Parameters\n","        --------\n","        result: pandas.Series\n","            Series of scores with the item id as the index\n","        '''\n","        res = result[:self.length]\n","        \n","        self.test += 1\n","        \n","        if pop_bin is not None:\n","            if pop_bin not in self.test_popbin:\n","                self.test_popbin[pop_bin] = 0\n","                self.pos_popbin[pop_bin] = 0\n","            self.test_popbin[pop_bin] += 1\n","        \n","        if position is not None:\n","            if position not in self.test_position:\n","                self.test_position[position] = 0\n","                self.pos_position[position] = 0\n","            self.test_position[position] += 1\n","        \n","        if next_item in res.index:\n","            rank = res.index.get_loc( next_item )+1\n","            self.pos += ( 1.0/rank )\n","            \n","            if pop_bin is not None:\n","                self.pos_popbin[pop_bin] += ( 1.0/rank )\n","            \n","            if position is not None:\n","                self.pos_position[position] += ( 1.0/rank )\n","                   \n","        \n","        \n","    def add_batch(self, result, next_item):\n","        '''\n","        Update the metric with a result set and the correct next item.\n","        \n","        Parameters\n","        --------\n","        result: pandas.DataFrame\n","            Prediction scores for selected items for every event of the batch.\n","            Columns: events of the batch; rows: items. Rows are indexed by the item IDs.\n","        next_item: Array of correct next items\n","        '''\n","        i=0\n","        for part, series in result.iteritems(): \n","            result.sort_values( part, ascending=False, inplace=True )\n","            self.add( series, next_item[i] )\n","            i += 1\n","        \n","    def result(self):\n","        '''\n","        Return a tuple of a description string and the current averaged value\n","        '''\n","        return (\"MRR@\" + str(self.length) + \": \"), (self.pos/self.test), self.result_pop_bin(), self.result_position()\n","    \n","    def result_pop_bin(self):\n","        '''\n","        Return a tuple of a description string and the current averaged value\n","        '''\n","        csv = ''\n","        csv += 'Bin: ;'\n","        for key in self.test_popbin:\n","            csv += str(key) + ';'\n","        csv += '\\nPrecision@' + str(self.length) + ': ;'\n","        for key in self.test_popbin:\n","            csv += str( self.pos_popbin[key] / self.test_popbin[key] ) + ';'\n","            \n","        return csv\n","    \n","    def result_position(self):\n","        '''\n","        Return a tuple of a description string and the current averaged value\n","        '''\n","        csv = ''\n","        csv += 'Pos: ;'\n","        for key in self.test_position:\n","            csv += str(key) + ';'\n","        csv += '\\nPrecision@' + str(self.length) + ': ;'\n","        for key in self.test_position:\n","            csv += str( self.pos_position[key] / self.test_position[key] ) + ';'\n","            \n","        return csv"],"metadata":{"id":"fUVIGWZz2xPu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class HitRate: \n","    '''\n","    MRR( length=20 )\n","    Used to iteratively calculate the average hit rate for a result list with the defined length. \n","    Parameters\n","    -----------\n","    length : int\n","        HitRate@length\n","    '''\n","    \n","    def __init__(self, length=20):\n","        self.length = length;\n","    \n","    def init(self, train):\n","        '''\n","        Do initialization work here.\n","        \n","        Parameters\n","        --------\n","        train: pandas.DataFrame\n","            Training data. It contains the transactions of the sessions. It has one column for session IDs, one for item IDs and one for the timestamp of the events (unix timestamps).\n","            It must have a header. Column names are arbitrary, but must correspond to the ones you set during the initialization of the network (session_key, item_key, time_key properties).\n","        '''\n","        return\n","        \n","    def reset(self):\n","        '''\n","        Reset for usage in multiple evaluations\n","        '''\n","        self.test=0;\n","        self.hit=0\n","        \n","        self.test_popbin = {}\n","        self.hit_popbin = {}\n","        \n","        self.test_position = {}\n","        self.hit_position = {}\n","        \n","    def add(self, result, next_item, for_item=0, session=0, pop_bin=None, position=None):\n","        '''\n","        Update the metric with a result set and the correct next item.\n","        Result must be sorted correctly.\n","        \n","        Parameters\n","        --------\n","        result: pandas.Series\n","            Series of scores with the item id as the index\n","        '''\n","        \n","        self.test += 1\n","         \n","        if pop_bin is not None:\n","            if pop_bin not in self.test_popbin:\n","                self.test_popbin[pop_bin] = 0\n","                self.hit_popbin[pop_bin] = 0\n","            self.test_popbin[pop_bin] += 1\n","        \n","        if position is not None:\n","            if position not in self.test_position:\n","                self.test_position[position] = 0\n","                self.hit_position[position] = 0\n","            self.test_position[position] += 1\n","                \n","        if next_item in result[:self.length].index:\n","            self.hit += 1\n","            \n","            if pop_bin is not None:\n","                self.hit_popbin[pop_bin] += 1\n","            \n","            if position is not None:\n","                self.hit_position[position] += 1\n","            \n","        \n","        \n","    def add_batch(self, result, next_item):\n","        '''\n","        Update the metric with a result set and the correct next item.\n","        \n","        Parameters\n","        --------\n","        result: pandas.DataFrame\n","            Prediction scores for selected items for every event of the batch.\n","            Columns: events of the batch; rows: items. Rows are indexed by the item IDs.\n","        next_item: Array of correct next items\n","        '''\n","        i=0\n","        for part, series in result.iteritems(): \n","            result.sort_values( part, ascending=False, inplace=True )\n","            self.add( series, next_item[i] )\n","            i += 1\n","        \n","    def result(self):\n","        '''\n","        Return a tuple of a description string and the current averaged value\n","        '''\n","        return (\"HitRate@\" + str(self.length) + \": \"), (self.hit/self.test), self.result_pop_bin(), self.result_position()\n","\n","    \n","    def result_pop_bin(self):\n","        '''\n","        Return a tuple of a description string and the current averaged value\n","        '''\n","        csv = ''\n","        csv += 'Bin: ;'\n","        for key in self.test_popbin:\n","            csv += str(key) + ';'\n","        csv += '\\nHitRate@' + str(self.length) + ': ;'\n","        for key in self.test_popbin:\n","            csv += str( self.hit_popbin[key] / self.test_popbin[key] ) + ';'\n","            \n","        return csv\n","    \n","    def result_position(self):\n","        '''\n","        Return a tuple of a description string and the current averaged value\n","        '''\n","        csv = ''\n","        csv += 'Pos: ;'\n","        for key in self.test_position:\n","            csv += str(key) + ';'\n","        csv += '\\nHitRate@' + str(self.length) + ': ;'\n","        for key in self.test_position:\n","            csv += str( self.hit_position[key] / self.test_position[key] ) + ';'\n","            \n","        return csv"],"metadata":{"id":"NL1uEgE6FUg1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class Coverage:\n","    '''\n","    Coverage( length=20 )\n","    Used to iteratively calculate the coverage of an algorithm regarding the item space. \n","    Parameters\n","    -----------\n","    length : int\n","        Coverage@length\n","    '''\n","    \n","    item_key = 'ItemId'\n","    \n","    def __init__(self, length=20):\n","        self.num_items = 0\n","        self.length = length\n","        self.time = 0;\n","        \n","    def init(self, train):\n","        '''\n","        Do initialization work here.\n","        \n","        Parameters\n","        --------\n","        train: pandas.DataFrame\n","            Training data. It contains the transactions of the sessions. It has one column for session IDs, one for item IDs and one for the timestamp of the events (unix timestamps).\n","            It must have a header. Column names are arbitrary, but must correspond to the ones you set during the initialization of the network (session_key, item_key, time_key properties).\n","        '''        \n","        self.coverage_set = set()\n","        self.items = set(train[self.item_key].unique()) # keep track of full item list\n","        self.num_items = len( train[self.item_key].unique() )\n","        \n","    def reset(self):\n","        '''\n","        Reset for usage in multiple evaluations\n","        '''\n","        self.coverage_set = set()\n","        return\n","    \n","    def skip(self, for_item = 0, session = -1 ):\n","        pass\n","    \n","    def add(self, result, next_item, for_item=0, session=0, pop_bin=None, position=None):\n","        '''\n","        Update the metric with a result set and the correct next item.\n","        Result must be sorted correctly.\n","        \n","        Parameters\n","        --------\n","        result: pandas.Series\n","            Series of scores with the item id as the index\n","        '''\n","        recs = result[:self.length]\n","        items = recs.index.unique()\n","        self.coverage_set.update( items )\n","        self.items.update( items ) # update items\n","        self.num_items = len( self.items )\n","        \n","    def add_multiple(self, result, next_items, for_item=0, session=0, position=None):   \n","        self.add(result, next_items[0], for_item, session)\n","        \n","    def add_batch(self, result, next_item):\n","        '''\n","        Update the metric with a result set and the correct next item.\n","        \n","        Parameters\n","        --------\n","        result: pandas.DataFrame\n","            Prediction scores for selected items for every event of the batch.\n","            Columns: events of the batch; rows: items. Rows are indexed by the item IDs.\n","        next_item: Array of correct next items\n","        '''\n","        i=0\n","        for part, series in result.iteritems(): \n","            result.sort_values( part, ascending=False, inplace=True )\n","            self.add( series, next_item[i] )\n","            i += 1\n","        \n","        \n","    def result(self):\n","        '''\n","        Return a tuple of a description string and the current averaged value\n","        '''\n","        return (\"Coverage@\" + str(self.length) + \": \"), ( len(self.coverage_set) / self.num_items )"],"metadata":{"id":"QDlDFjyE28eB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class Popularity:\n","    '''\n","    Popularity( length=20 )\n","    Used to iteratively calculate the average overall popularity of an algorithm's recommendations. \n","    Parameters\n","    -----------\n","    length : int\n","        Coverage@length\n","    '''\n","    \n","    session_key = 'SessionId'\n","    item_key    = 'ItemId'\n","    \n","    def __init__(self, length=20):\n","        self.length = length;\n","        self.sum = 0\n","        self.tests = 0\n","    \n","    def init(self, train):\n","        '''\n","        Do initialization work here.\n","        \n","        Parameters\n","        --------\n","        train: pandas.DataFrame\n","            Training data. It contains the transactions of the sessions. It has one column for session IDs, one for item IDs and one for the timestamp of the events (unix timestamps).\n","            It must have a header. Column names are arbitrary, but must correspond to the ones you set during the initialization of the network (session_key, item_key, time_key properties).\n","        '''\n","        self.train_actions = len( train.index )\n","        #group the data by the itemIds\n","        grp = train.groupby(self.item_key)\n","        #count the occurence of every itemid in the trainingdataset\n","        self.pop_scores = grp.size()\n","        #sort it according to the  score\n","        self.pop_scores.sort_values(ascending=False, inplace=True)\n","        #normalize\n","        self.pop_scores = self.pop_scores / self.pop_scores[:1].values[0]\n","        \n","    def reset(self):\n","        '''\n","        Reset for usage in multiple evaluations\n","        '''\n","        self.tests=0;\n","        self.sum=0\n","     \n","    def skip(self, for_item = 0, session = -1 ):\n","        pass \n","        \n","    def add(self, result, next_item, for_item=0, session=0, pop_bin=None, position=None):\n","        '''\n","        Update the metric with a result set and the correct next item.\n","        Result must be sorted correctly.\n","        \n","        Parameters\n","        --------\n","        result: pandas.Series\n","            Series of scores with the item id as the index\n","        '''\n","        #only keep the k- first predictions\n","        recs = result[:self.length]\n","        #take the unique values out of those top scorers\n","        items = recs.index.unique()\n","                \n","        self.sum += ( self.pop_scores[ items ].sum() / len( items ) )\n","        self.tests += 1\n","    \n","    def add_multiple(self, result, next_items, for_item=0, session=0, position=None):   \n","        self.add(result, next_items[0], for_item, session)\n","    \n","    def add_batch(self, result, next_item):\n","        '''\n","        Update the metric with a result set and the correct next item.\n","        \n","        Parameters\n","        --------\n","        result: pandas.DataFrame\n","            Prediction scores for selected items for every event of the batch.\n","            Columns: events of the batch; rows: items. Rows are indexed by the item IDs.\n","        next_item: Array of correct next items\n","        '''\n","        i=0\n","        for part, series in result.iteritems(): \n","            result.sort_values( part, ascending=False, inplace=True )\n","            self.add( series, next_item[i] )\n","            i += 1\n","        \n","    def result(self):\n","        '''\n","        Return a tuple of a description string and the current averaged value\n","        '''\n","        return (\"Popularity@\" + str( self.length ) + \": \"), ( self.sum / self.tests )"],"metadata":{"id":"_cvJCNlb3BUx"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## SLIST model"],"metadata":{"id":"F5cC3PEPRPpE"}},{"cell_type":"code","source":["class SLIST:\n","    '''\n","    SLIST(reg=10)\n","    Parameters\n","    --------\n","    Will be added\n","    --------\n","    '''\n","\n","    # Must need\n","    def __init__(self, reg=10, alpha=0.5, session_weight=-1, train_weight=-1, predict_weight=-1,\n","                 direction='part', normalize='l1', epsilon=10.0, session_key='SessionId', item_key='ItemId',\n","                 verbose=False):\n","        self.reg = reg\n","        self.normalize = normalize\n","        self.epsilon = epsilon\n","        self.alpha = alpha\n","        self.direction = direction \n","        self.train_weight = float(train_weight)\n","        self.predict_weight = float(predict_weight)\n","        self.session_weight = session_weight*24*3600\n","\n","        self.session_key = session_key\n","        self.item_key = item_key\n","\n","        # updated while recommending\n","        self.session = -1\n","        self.session_items = []\n","        \n","        self.verbose = verbose\n","\n","    # Must need\n","    def fit(self, data, test=None):\n","        '''\n","        Trains the predictor.\n","        Parameters\n","        --------\n","        data: pandas.DataFrame\n","            Training data. It contains the transactions of the sessions. It has one column for session IDs, one for item IDs and one for the timestamp of the events (unix timestamps).\n","            It must have a header. Column names are arbitrary, but must correspond to the ones you set during the initialization of the network (session_key, item_key, time_key properties).\n","        '''\n","        # make new session ids(1 ~ #sessions)\n","        sessionids = data[self.session_key].unique()\n","        self.n_sessions = len(sessionids)\n","        self.sessionidmap = pd.Series(data=np.arange(self.n_sessions), index=sessionids)\n","        data = pd.merge(data, pd.DataFrame({self.session_key: sessionids, 'SessionIdx': self.sessionidmap[sessionids].values}), on=self.session_key, how='inner')\n","\n","        # make new item ids(1 ~ #items)\n","        itemids = data[self.item_key].unique()\n","        self.n_items = len(itemids)\n","        self.itemidmap = pd.Series(data=np.arange(self.n_items), index=itemids)\n","        data = pd.merge(data, pd.DataFrame({self.item_key: itemids, 'ItemIdx': self.itemidmap[itemids].values}), on=self.item_key, how='inner')\n","\n","        # ||X - XB||\n","        input1, target1, row_weight1 = self.make_train_matrix(data, weight_by='SLIS')\n","        # ||Y - ZB||\n","        input2, target2, row_weight2 = self.make_train_matrix(data, weight_by='SLIT')\n","        # alpha * ||X - XB|| + (1-alpha) * ||Y - ZB||\n","        input1.data = np.sqrt(self.alpha) * input1.data\n","        target1.data = np.sqrt(self.alpha) * target1.data\n","        input2.data = np.sqrt(1-self.alpha) * input2.data\n","        target2.data = np.sqrt(1-self.alpha) * target2.data\n","\n","        input_matrix = vstack([input1, input2])\n","        target_matrix = vstack([target1, target2])\n","        w2 = row_weight1 + row_weight2  # list\n","\n","        # P = (X^T * X + λI)^−1 = (G + λI)^−1\n","        # (A+B)^-1 = A^-1 - A^-1 * B * (A+B)^-1\n","        # P =  G\n","        W2 = sparse.diags(w2, dtype=np.float32)\n","        G = input_matrix.transpose().dot(W2).dot(input_matrix).toarray()\n","        if self.verbose:\n","            print(f\"G is made. Sparsity:{(1 - np.count_nonzero(G)/(self.n_items**2))*100}%\")\n","\n","        P = np.linalg.inv(G + np.identity(self.n_items, dtype=np.float32) * self.reg)\n","        if self.verbose:\n","            print(\"P is made\")\n","        del G\n","\n","        if self.alpha == 1:\n","            C = -P @ (input_matrix.transpose().dot(W2).dot(input_matrix-target_matrix).toarray())\n","\n","            mu = np.zeros(self.n_items)\n","            mu += self.reg\n","            mu_nonzero_idx = np.where(1 - np.diag(P)*self.reg + np.diag(C) >= self.epsilon)\n","            mu[mu_nonzero_idx] = (np.diag(1 - self.epsilon + C) / np.diag(P))[mu_nonzero_idx]\n","\n","            # B = I - Pλ + C\n","            self.enc_w = np.identity(self.n_items, dtype=np.float32) - P @ np.diag(mu) + C\n","            if self.verbose:\n","                print(\"weight matrix is made\")\n","        else:\n","            self.enc_w = P @ input_matrix.transpose().dot(W2).dot(target_matrix).toarray()\n","\n","\n","    def make_train_matrix(self, data, weight_by='SLIT'):\n","        input_row = []\n","        target_row = []\n","        input_col = []\n","        target_col = []\n","        input_data = []\n","        target_data = []\n","\n","        maxtime = data.Time.max()\n","        w2 = []\n","        sessionlengthmap = data['SessionIdx'].value_counts(sort=False)\n","        rowid = -1\n","        \n","        directory = os.path.dirname('./data_ckpt/')\n","        if not os.path.exists(directory):\n","            os.makedirs(directory)\n","\n","        if weight_by == 'SLIT':\n","            if os.path.exists(f'./data_ckpt/{self.n_sessions}_{self.n_items}_{self.direction}_SLIT.p'):\n","                with open(f'./data_ckpt/{self.n_sessions}_{self.n_items}_{self.direction}_SLIT.p','rb') as f:\n","                    input_row, input_col, input_data, target_row, target_col, target_data, w2 = pickle.load(f)\n","            else:\n","                for sid, session in tqdm(data.groupby(['SessionIdx']), desc=weight_by):\n","                    slen = sessionlengthmap[sid]\n","                    # sessionitems = session['ItemIdx'].tolist() # sorted by itemid\n","                    sessionitems = session.sort_values(['Time'])['ItemIdx'].tolist()  # sorted by time\n","                    slen = len(sessionitems)\n","                    if slen <= 1:\n","                        continue\n","                    stime = session['Time'].max()\n","                    w2 += [stime-maxtime] * (slen-1)\n","                    for t in range(slen-1):\n","                        rowid += 1\n","                        # input matrix\n","                        if self.direction == 'part':\n","                            input_row += [rowid] * (t+1)\n","                            input_col += sessionitems[:t+1]\n","                            for s in range(t+1):\n","                                input_data.append(-abs(t-s))\n","                            target_row += [rowid] * (slen - (t+1))\n","                            target_col += sessionitems[t+1:]\n","                            for s in range(t+1, slen):\n","                                target_data.append(-abs((t+1)-s))\n","                        elif self.direction == 'all':\n","                            input_row += [rowid] * slen\n","                            input_col += sessionitems\n","                            for s in range(slen):\n","                                input_data.append(-abs(t-s))\n","                            target_row += [rowid] * slen\n","                            target_col += sessionitems\n","                            for s in range(slen):\n","                                target_data.append(-abs((t+1)-s))\n","                        elif self.direction == 'sr':\n","                            input_row += [rowid]\n","                            input_col += [sessionitems[t]]\n","                            input_data.append(0)\n","                            target_row += [rowid] * (slen - (t+1))\n","                            target_col += sessionitems[t+1:]\n","                            for s in range(t+1, slen):\n","                                target_data.append(-abs((t+1)-s))\n","                        else:\n","                            raise (\"You have to choose right 'direction'!\")\n","                with open(f'./data_ckpt/{self.n_sessions}_{self.n_items}_{self.direction}_SLIT.p','wb') as f:\n","                    pickle.dump([input_row, input_col, input_data, target_row, target_col, target_data, w2], f, protocol=4)\n","            input_data = list(np.exp(np.array(input_data) / self.train_weight))\n","            target_data = list(np.exp(np.array(target_data) / self.train_weight))\n","        elif weight_by == 'SLIS':\n","            if os.path.exists(f'./data_ckpt/{self.n_sessions}_{self.n_items}_SLIS.p'):\n","                with open(f'./data_ckpt/{self.n_sessions}_{self.n_items}_SLIS.p','rb') as f:\n","                    input_row, input_col, input_data, target_row, target_col, target_data, w2 = pickle.load(f)\n","            else:\n","                for sid, session in tqdm(data.groupby(['SessionIdx']), desc=weight_by):\n","                    rowid += 1\n","                    slen = sessionlengthmap[sid]\n","                    sessionitems = session['ItemIdx'].tolist()\n","                    stime = session['Time'].max()\n","                    w2.append(stime-maxtime)\n","                    input_row += [rowid] * slen\n","                    input_col += sessionitems\n","\n","                target_row = input_row\n","                target_col = input_col\n","                input_data = np.ones_like(input_row)\n","                target_data = np.ones_like(target_row)\n","                \n","                with open(f'./data_ckpt/{self.n_sessions}_{self.n_items}_SLIS.p','wb') as f:\n","                    pickle.dump([input_row, input_col, input_data, target_row, target_col, target_data, w2], f, protocol=4)\n","        else:\n","            raise (\"You have to choose right 'weight_by'!\")\n","\n","        # Use train_weight or not\n","        input_data = input_data if self.train_weight > 0 else list(np.ones_like(input_data))\n","        target_data = target_data if self.train_weight > 0 else list(np.ones_like(target_data))\n","\n","        # Use session_weight or not\n","        w2 = list(np.exp(np.array(w2) / self.session_weight))\n","        w2 = w2 if self.session_weight > 0 else list(np.ones_like(w2))\n","\n","        # Make sparse_matrix\n","        input_matrix = csr_matrix((input_data, (input_row, input_col)), shape=(max(input_row)+1, self.n_items), dtype=np.float32)\n","        target_matrix = csr_matrix((target_data, (target_row, target_col)), shape=input_matrix.shape, dtype=np.float32)\n","        if self.verbose:\n","            print(f\"[{weight_by}]sparse matrix {input_matrix.shape} is made.  Sparsity:{(1 - input_matrix.count_nonzero()/(self.n_items*input_matrix.shape[0]))*100}%\")\n","\n","\n","        if weight_by == 'SLIT':\n","            pass\n","        elif weight_by == 'SLIS':\n","            # Value of repeated items --> 1\n","            input_matrix.data = np.ones_like(input_matrix.data)\n","            target_matrix.data = np.ones_like(target_matrix.data)\n","\n","        # Normalization\n","        if self.normalize == 'l1':\n","            input_matrix = normalize(input_matrix, 'l1')\n","        elif self.normalize == 'l2':\n","            input_matrix = normalize(input_matrix, 'l2')\n","        else:\n","            pass\n","\n","        return input_matrix, target_matrix, w2\n","\n","    # 필수\n","\n","    def predict_next(self, session_id, input_item_id, predict_for_item_ids, input_user_id=None, skip=False, type='view', timestamp=0):\n","        '''\n","        Gives predicton scores for a selected set of items on how likely they be the next item in the session.\n","        Parameters\n","        --------\n","        session_id : int or string\n","            The session IDs of the event.\n","        input_item_id : int or string\n","            The item ID of the event.\n","        predict_for_item_ids : 1D array\n","            IDs of items for which the network should give prediction scores. Every ID must be in the set of item IDs of the training set.\n","        Returns\n","        --------\n","        out : pandas.Series\n","            Prediction scores for selected items on how likely to be the next item of this session. Indexed by the item IDs.\n","        '''\n","        # new session\n","        if session_id != self.session:\n","            self.session_items = []\n","            self.session = session_id\n","            self.session_times = []\n","\n","        if type == 'view':\n","            if input_item_id in self.itemidmap.index:\n","                self.session_items.append(input_item_id)\n","                self.session_times.append(timestamp)\n","\n","        # item id transfomration\n","        session_items_new_id = self.itemidmap[self.session_items].values\n","        predict_for_item_ids_new_id = self.itemidmap[predict_for_item_ids].values\n","        \n","        if session_items_new_id.shape[0] == 0:\n","            skip = True\n","\n","        if skip:\n","            return pd.Series(data=0, index=predict_for_item_ids)\n","\n","        W_test = np.ones_like(self.session_items, dtype=np.float32)\n","        W_test = self.enc_w[session_items_new_id[-1], session_items_new_id]\n","        for i in range(len(W_test)):\n","            W_test[i] = np.exp(-abs(i+1-len(W_test))/self.predict_weight)\n","\n","        W_test = W_test if self.predict_weight > 0 else np.ones_like(W_test)\n","        W_test = W_test.reshape(-1, 1)\n","\n","        # [session_items, num_items]\n","        preds = self.enc_w[session_items_new_id] * W_test\n","        # [num_items]\n","        preds = np.sum(preds, axis=0)\n","        preds = preds[predict_for_item_ids_new_id]\n","\n","        series = pd.Series(data=preds, index=predict_for_item_ids)\n","\n","        series = series / series.max()\n","        \n","        # remove current item from series of prediction\n","        # series.drop(labels=[input_item_id])\n","        \n","        return series\n","\n","    # 필수\n","    def clear(self):\n","        self.enc_w = {}"],"metadata":{"id":"FnPMwD7_32BW"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Main"],"metadata":{"id":"qtKntJplRUu9"}},{"cell_type":"code","source":["'''\n","FILE PARAMETERS\n","'''\n","PATH_PROCESSED = './prepared/'\n","FILE = 'events'"],"metadata":{"id":"yOMQysGb3Iwp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["'''\n","MODEL HYPERPARAMETER TUNING\n","'''\n","alpha = 0.2 #[0.2, 0.4, 0.6, 0.8] \n","direction = 'all' # sr / part / all\n","reg = 10\n","train_weight = 1 #0.5 #[0.125, 0.25, 0.5, 1, 2, 4, 8]\n","predict_weight = 1 #4 #[0.125, 0.25, 0.5, 1, 2, 4, 8]\n","session_weight = 1 #256 #[1, 2, 4, 8, 16, 32, 64, 128, 256]"],"metadata":{"id":"RYIZjliz3LUh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Training\n","train, val = load_data_session(PATH_PROCESSED, FILE, train_eval=True)\n","model = SLIST(alpha=alpha, direction=direction, reg=reg, train_weight=train_weight, \n","              predict_weight=predict_weight, session_weight=session_weight)\n","model.fit(train, val)\n","\n","mrr = MRR(length=100)\n","hr = HitRate()\n","pop = Popularity()\n","pop.init(train)\n","cov = Coverage()\n","cov.init(train)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4a4mvCbi3TAy","outputId":"0d221f82-da35-42aa-e0b1-91fa3f8051cd","executionInfo":{"status":"ok","timestamp":1639119017665,"user_tz":-330,"elapsed":17591,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["START load data\n","Loaded train set\n","\tEvents: 53254\n","\tSessions: 13629\n","\tItems: 2873\n","\tSpan: 2014-04-01 / 2014-04-06\n","\n","Loaded test set\n","\tEvents: 16539\n","\tSessions: 4084\n","\tItems: 2029\n","\tSpan: 2014-04-06 / 2014-04-07\n","\n","END load data  0.08983836199999473 c /  0.0898427963256836 s\n"]},{"output_type":"stream","name":"stderr","text":["SLIS: 100%|██████████| 13629/13629 [00:03<00:00, 3569.65it/s]\n","SLIT: 100%|██████████| 13629/13629 [00:09<00:00, 1411.28it/s]\n"]}]},{"cell_type":"code","source":["result = evaluate_sessions(model, [mrr, hr, pop, cov], val, train)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"n94vXWR67O36","executionInfo":{"status":"ok","timestamp":1639119130459,"user_tz":-330,"elapsed":62776,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}},"outputId":"5d2f7bbe-e29f-46f5-9929-16a28702ef9d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["START evaluation of  16539  actions in  4084  sessions\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 16539/16539 [01:02<00:00, 263.56it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","END evaluation in  62.77697779699997 c /  62.77697801589966 s\n","    avg rt  0.003989196120376618 s /  0.003987816305178503 c\n","    time count  12455 count/ 49.68543767929077  sum\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}]},{"cell_type":"code","source":["result"],"metadata":{"id":"SpsUr_403z0U","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1639119133944,"user_tz":-330,"elapsed":555,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}},"outputId":"b51ed462-d70f-41df-d262-3bc6927f15ff"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[('MRR@100: ',\n","  0.3521687942062188,\n","  'Bin: ;\\nPrecision@100: ;',\n","  'Pos: ;0;1;2;3;4;5;6;7;8;9;10;11;12;13;14;15;16;17;18;19;20;21;22;23;24;25;26;27;28;29;30;31;32;33;34;35;36;37;38;39;40;41;42;43;44;45;46;47;48;49;50;51;\\nPrecision@100: ;0.39043544699954863;0.3325325335562308;0.358431025984701;0.31195065232051633;0.3372114712929858;0.32268523251291464;0.3555237695229066;0.3166781959167088;0.34238487011710844;0.3221189158972095;0.31515860275983537;0.2895189958634645;0.3960849891052557;0.3853882229014253;0.3197574713916099;0.3025236016461091;0.3004164800841002;0.36369765591924186;0.3808260628265034;0.29341860165801975;0.2725921570091597;0.34991145218417946;0.24232666413393963;0.4091435185185185;0.2363186813186813;0.22504370629370626;0.18930041152263372;0.11895735686058267;0.14806547619047616;0.3020408163265306;0.13849206349206347;0.3333333333333333;0.3482142857142857;0.10606060606060605;0.125;0.6;0.2;0.0;0.03333333333333333;0.0;0.06666666666666667;0.3333333333333333;0.14285714285714285;0.1111111111111111;0.058823529411764705;0.14285714285714285;0.0;0.06666666666666667;0.0;0.058823529411764705;0.047619047619047616;0.3333333333333333;'),\n"," ('HitRate@20: ',\n","  0.637173825772782,\n","  'Bin: ;\\nHitRate@20: ;',\n","  'Pos: ;0;1;2;3;4;5;6;7;8;9;10;11;12;13;14;15;16;17;18;19;20;21;22;23;24;25;26;27;28;29;30;31;32;33;34;35;36;37;38;39;40;41;42;43;44;45;46;47;48;49;50;51;\\nHitRate@20: ;0.6339373163565132;0.6320914479254869;0.6607958251793868;0.627134724857685;0.6466575716234653;0.626641651031895;0.6666666666666666;0.6081504702194357;0.6629213483146067;0.6051282051282051;0.6329113924050633;0.59375;0.7247706422018348;0.6630434782608695;0.5833333333333334;0.639344262295082;0.6;0.6122448979591837;0.7297297297297297;0.6774193548387096;0.4583333333333333;0.6363636363636364;0.6842105263157895;0.7222222222222222;0.5333333333333333;0.7272727272727273;0.4444444444444444;0.4444444444444444;0.625;0.42857142857142855;0.5;0.5;0.75;0.5;0.5;1.0;1.0;0.0;0.0;0.0;1.0;1.0;1.0;1.0;1.0;1.0;0.0;1.0;0.0;1.0;0.0;1.0;'),\n"," ('Popularity@20: ', 0.12476653149405403),\n"," ('Coverage@20: ', 0.9557953358858337)]"]},"metadata":{},"execution_count":27}]},{"cell_type":"markdown","source":["---"],"metadata":{"id":"ogypTsIWHEyR"}},{"cell_type":"code","source":["!pip install -q watermark\n","%reload_ext watermark\n","%watermark -a \"Sparsh A.\" -m -iv -u -t -d"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HY5KNqOJHEyT","executionInfo":{"status":"ok","timestamp":1639119146044,"user_tz":-330,"elapsed":3786,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}},"outputId":"ed8e9ba0-c8c1-4072-8e22-f40f8efb340e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Author: Sparsh A.\n","\n","Last updated: 2021-12-10 06:52:33\n","\n","Compiler    : GCC 7.5.0\n","OS          : Linux\n","Release     : 5.4.104+\n","Machine     : x86_64\n","Processor   : x86_64\n","CPU cores   : 2\n","Architecture: 64bit\n","\n","numpy  : 1.19.5\n","IPython: 5.5.0\n","scipy  : 1.4.1\n","pandas : 1.1.5\n","\n"]}]},{"cell_type":"markdown","source":["---"],"metadata":{"id":"3RXL1ys5HEyU"}},{"cell_type":"markdown","source":["**END**"],"metadata":{"id":"E8qwaCPdHEyU"}}]}