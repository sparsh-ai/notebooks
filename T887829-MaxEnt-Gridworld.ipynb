{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"T887829 | MaxEnt Gridworld","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyP8+4o548KIig7oY41eonN+"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"1SIvNc90BQjE"},"source":["# MaxEnt Gridworld\n","\n","> Implementation of MaxEnt-IRL model for FBER recommendation system, based on the approach of Ziebart et al. 2008 paper: Maximum Entropy Inverse Reinforcement Learning."]},{"cell_type":"code","metadata":{"id":"JrOXKw1-6TjT"},"source":["import copy\n","\n","import warnings\n","warnings.filterwarnings('ignore')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9_2RUPIt6ThH"},"source":["\"\"\"\n","Find the value function associated with a policy. Based on Sutton & Barto, 1998.\n","\n","Matthew Alger, 2015\n","matthew.alger@anu.edu.au\n","\"\"\"\n","\n","import numpy as np\n","\n","def value(policy, n_states, transition_probabilities, reward, discount,\n","                    threshold=1e-2):\n","    \"\"\"\n","    Find the value function associated with a policy.\n","\n","    policy: List of action ints for each state.\n","    n_states: Number of states. int.\n","    transition_probabilities: Function taking (state, action, state) to\n","        transition probabilities.\n","    reward: Vector of rewards for each state.\n","    discount: MDP discount factor. float.\n","    threshold: Convergence threshold, default 1e-2. float.\n","    -> Array of values for each state\n","    \"\"\"\n","    v = np.zeros(n_states)\n","\n","    diff = float(\"inf\")\n","    while diff > threshold:\n","        diff = 0\n","        for s in range(n_states):\n","            vs = v[s]\n","            a = policy[s]\n","            v[s] = sum(transition_probabilities[s, a, k] *\n","                       (reward[k] + discount * v[k])\n","                       for k in range(n_states))\n","            diff = max(diff, abs(vs - v[s]))\n","\n","    return v\n","\n","def optimal_value(n_states, n_actions, transition_probabilities, reward,\n","                  discount, threshold=1e-2):\n","    \"\"\"\n","    Find the optimal value function.\n","\n","    n_states: Number of states. int.\n","    n_actions: Number of actions. int.\n","    transition_probabilities: Function taking (state, action, state) to\n","        transition probabilities.\n","    reward: Vector of rewards for each state.\n","    discount: MDP discount factor. float.\n","    threshold: Convergence threshold, default 1e-2. float.\n","    -> Array of values for each state\n","    \"\"\"\n","\n","    v = np.zeros(n_states)\n","\n","    diff = float(\"inf\")\n","    while diff > threshold:\n","        diff = 0\n","        for s in range(n_states):\n","            max_v = float(\"-inf\")\n","            for a in range(n_actions):\n","                tp = transition_probabilities[s, a, :]\n","                max_v = max(max_v, np.dot(tp, reward + discount*v))\n","\n","            new_diff = abs(v[s] - max_v)\n","            if new_diff > diff:\n","                diff = new_diff\n","            v[s] = max_v\n","\n","    return v\n","\n","def find_policy(n_states, n_actions, transition_probabilities, reward, discount,\n","                threshold=1e-2, v=None, stochastic=True):\n","    \"\"\"\n","    Find the optimal policy.\n","\n","    n_states: Number of states. int.\n","    n_actions: Number of actions. int.\n","    transition_probabilities: Function taking (state, action, state) to\n","        transition probabilities.\n","    reward: Vector of rewards for each state.\n","    discount: MDP discount factor. float.\n","    threshold: Convergence threshold, default 1e-2. float.\n","    v: Value function (if known). Default None.\n","    stochastic: Whether the policy should be stochastic. Default True.\n","    -> Action probabilities for each state or action int for each state\n","        (depending on stochasticity).\n","    \"\"\"\n","\n","    if v is None:\n","        v = optimal_value(n_states, n_actions, transition_probabilities, reward,\n","                          discount, threshold)\n","\n","    if stochastic:\n","        # Get Q using equation 9.2 from Ziebart's thesis.\n","        Q = np.zeros((n_states, n_actions))\n","        for i in range(n_states):\n","            for j in range(n_actions):\n","                p = transition_probabilities[i, j, :]\n","                Q[i, j] = p.dot(reward + discount*v)\n","        Q -= Q.max(axis=1).reshape((n_states, 1))  # For numerical stability.\n","        Q = np.exp(Q)/np.exp(Q).sum(axis=1).reshape((n_states, 1))\n","        return Q\n","\n","    def _policy(s):\n","        return max(list(range(n_actions)),\n","                   key=lambda a: sum(transition_probabilities[s, a, k] *\n","                                     (reward[k] + discount * v[k])\n","                                     for k in range(n_states)))\n","    policy = np.array([_policy(s) for s in range(n_states)])\n","    return policy"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"JqzvZGEX8vzr"},"source":["\"\"\"\n","Implements the gridworld MDP.\n","\n","Matthew Alger, 2015\n","matthew.alger@anu.edu.au\n","\"\"\"\n","\n","import numpy as np\n","import numpy.random as rn\n","\n","class Gridworld(object):\n","    \"\"\"\n","    Gridworld MDP.\n","    \"\"\"\n","\n","    def __init__(self, grid_size, wind, discount):\n","        \"\"\"\n","        grid_size: Grid size. int.\n","        wind: Chance of moving randomly. float.\n","        discount: MDP discount. float.\n","        -> Gridworld\n","        \"\"\"\n","\n","        self.actions = ((1, 0), (0, 1), (-1, 0), (0, -1))\n","        self.n_actions = len(self.actions)\n","        self.n_states = grid_size**2\n","        self.grid_size = grid_size\n","        self.wind = wind\n","        self.discount = discount\n","\n","        # Preconstruct the transition probability array.\n","        self.transition_probability = np.array(\n","            [[[self._transition_probability(i, j, k)\n","               for k in range(self.n_states)]\n","              for j in range(self.n_actions)]\n","             for i in range(self.n_states)])\n","\n","    def __str__(self):\n","        return \"Gridworld({}, {}, {})\".format(self.grid_size, self.wind,\n","                                              self.discount)\n","\n","    def feature_vector(self, i, feature_map=\"ident\"):\n","        \"\"\"\n","        Get the feature vector associated with a state integer.\n","\n","        i: State int.\n","        feature_map: Which feature map to use (default ident). String in {ident,\n","            coord, proxi}.\n","        -> Feature vector.\n","        \"\"\"\n","\n","        if feature_map == \"coord\":\n","            f = np.zeros(self.grid_size)\n","            x, y = i % self.grid_size, i // self.grid_size\n","            f[x] += 1\n","            f[y] += 1\n","            return f\n","        if feature_map == \"proxi\":\n","            f = np.zeros(self.n_states)\n","            x, y = i % self.grid_size, i // self.grid_size\n","            for b in range(self.grid_size):\n","                for a in range(self.grid_size):\n","                    dist = abs(x - a) + abs(y - b)\n","                    f[self.point_to_int((a, b))] = dist\n","            return f\n","        # Assume identity map.\n","        f = np.zeros(self.n_states)\n","        f[i] = 1\n","        return f\n","\n","    def feature_matrix(self, feature_map=\"ident\"):\n","        \"\"\"\n","        Get the feature matrix for this gridworld.\n","\n","        feature_map: Which feature map to use (default ident). String in {ident,\n","            coord, proxi}.\n","        -> NumPy array with shape (n_states, d_states).\n","        \"\"\"\n","\n","        features = []\n","        for n in range(self.n_states):\n","            f = self.feature_vector(n, feature_map)\n","            features.append(f)\n","        return np.array(features)\n","\n","    def int_to_point(self, i):\n","        \"\"\"\n","        Convert a state int into the corresponding coordinate.\n","\n","        i: State int.\n","        -> (x, y) int tuple.\n","        \"\"\"\n","\n","        return (i % self.grid_size, i // self.grid_size)\n","\n","    def point_to_int(self, p):\n","        \"\"\"\n","        Convert a coordinate into the corresponding state int.\n","\n","        p: (x, y) tuple.\n","        -> State int.\n","        \"\"\"\n","\n","        return p[0] + p[1]*self.grid_size\n","\n","    def neighbouring(self, i, k):\n","        \"\"\"\n","        Get whether two points neighbour each other. Also returns true if they\n","        are the same point.\n","\n","        i: (x, y) int tuple.\n","        k: (x, y) int tuple.\n","        -> bool.\n","        \"\"\"\n","\n","        return abs(i[0] - k[0]) + abs(i[1] - k[1]) <= 1\n","\n","    def _transition_probability(self, i, j, k):\n","        \"\"\"\n","        Get the probability of transitioning from state i to state k given\n","        action j.\n","\n","        i: State int.\n","        j: Action int.\n","        k: State int.\n","        -> p(s_k | s_i, a_j)\n","        \"\"\"\n","\n","        xi, yi = self.int_to_point(i)\n","        xj, yj = self.actions[j]\n","        xk, yk = self.int_to_point(k)\n","\n","        if not self.neighbouring((xi, yi), (xk, yk)):\n","            return 0.0\n","\n","        # Is k the intended state to move to?\n","        if (xi + xj, yi + yj) == (xk, yk):\n","            return 1 - self.wind + self.wind/self.n_actions\n","\n","        # If these are not the same point, then we can move there by wind.\n","        if (xi, yi) != (xk, yk):\n","            return self.wind/self.n_actions\n","\n","        # If these are the same point, we can only move here by either moving\n","        # off the grid or being blown off the grid. Are we on a corner or not?\n","        if (xi, yi) in {(0, 0), (self.grid_size-1, self.grid_size-1),\n","                        (0, self.grid_size-1), (self.grid_size-1, 0)}:\n","            # Corner.\n","            # Can move off the edge in two directions.\n","            # Did we intend to move off the grid?\n","            if not (0 <= xi + xj < self.grid_size and\n","                    0 <= yi + yj < self.grid_size):\n","                # We intended to move off the grid, so we have the regular\n","                # success chance of staying here plus an extra chance of blowing\n","                # onto the *other* off-grid square.\n","                return 1 - self.wind + 2*self.wind/self.n_actions\n","            else:\n","                # We can blow off the grid in either direction only by wind.\n","                return 2*self.wind/self.n_actions\n","        else:\n","            # Not a corner. Is it an edge?\n","            if (xi not in {0, self.grid_size-1} and\n","                yi not in {0, self.grid_size-1}):\n","                # Not an edge.\n","                return 0.0\n","\n","            # Edge.\n","            # Can only move off the edge in one direction.\n","            # Did we intend to move off the grid?\n","            if not (0 <= xi + xj < self.grid_size and\n","                    0 <= yi + yj < self.grid_size):\n","                # We intended to move off the grid, so we have the regular\n","                # success chance of staying here.\n","                return 1 - self.wind + self.wind/self.n_actions\n","            else:\n","                # We can blow off the grid only by wind.\n","                return self.wind/self.n_actions\n","\n","    def reward(self, state_int):\n","        \"\"\"\n","        Reward for being in state state_int.\n","\n","        state_int: State integer. int.\n","        -> Reward.\n","        \"\"\"\n","\n","        if state_int == self.n_states - 1:\n","            return 1\n","        return 0\n","\n","    def average_reward(self, n_trajectories, trajectory_length, policy):\n","        \"\"\"\n","        Calculate the average total reward obtained by following a given policy\n","        over n_paths paths.\n","\n","        policy: Map from state integers to action integers.\n","        n_trajectories: Number of trajectories. int.\n","        trajectory_length: Length of an episode. int.\n","        -> Average reward, standard deviation.\n","        \"\"\"\n","\n","        trajectories = self.generate_trajectories(n_trajectories,\n","                                             trajectory_length, policy)\n","        rewards = [[r for _, _, r in trajectory] for trajectory in trajectories]\n","        rewards = np.array(rewards)\n","\n","        # Add up all the rewards to find the total reward.\n","        total_reward = rewards.sum(axis=1)\n","\n","        # Return the average reward and standard deviation.\n","        return total_reward.mean(), total_reward.std()\n","\n","    def optimal_policy(self, state_int):\n","        \"\"\"\n","        The optimal policy for this gridworld.\n","\n","        state_int: What state we are in. int.\n","        -> Action int.\n","        \"\"\"\n","\n","        sx, sy = self.int_to_point(state_int)\n","\n","        if sx < self.grid_size and sy < self.grid_size:\n","            return rn.randint(0, 2)\n","        if sx < self.grid_size-1:\n","            return 0\n","        if sy < self.grid_size-1:\n","            return 1\n","        raise ValueError(\"Unexpected state.\")\n","\n","    def optimal_policy_deterministic(self, state_int):\n","        \"\"\"\n","        Deterministic version of the optimal policy for this gridworld.\n","\n","        state_int: What state we are in. int.\n","        -> Action int.\n","        \"\"\"\n","\n","        sx, sy = self.int_to_point(state_int)\n","        if sx < sy:\n","            return 0\n","        return 1\n","\n","    def generate_trajectories(self, n_trajectories, trajectory_length, policy,\n","                                    random_start=False):\n","        \"\"\"\n","        Generate n_trajectories trajectories with length trajectory_length,\n","        following the given policy.\n","\n","        n_trajectories: Number of trajectories. int.\n","        trajectory_length: Length of an episode. int.\n","        policy: Map from state integers to action integers.\n","        random_start: Whether to start randomly (default False). bool.\n","        -> [[(state int, action int, reward float)]]\n","        \"\"\"\n","\n","        trajectories = []\n","        for _ in range(n_trajectories):\n","            if random_start:\n","                sx, sy = rn.randint(self.grid_size), rn.randint(self.grid_size)\n","            else:\n","                sx, sy = 0, 0\n","\n","            trajectory = []\n","            for _ in range(trajectory_length):\n","                if rn.random() < self.wind:\n","                    action = self.actions[rn.randint(0, 4)]\n","                else:\n","                    # Follow the given policy.\n","                    action = self.actions[policy(self.point_to_int((sx, sy)))]\n","\n","                if (0 <= sx + action[0] < self.grid_size and\n","                        0 <= sy + action[1] < self.grid_size):\n","                    next_sx = sx + action[0]\n","                    next_sy = sy + action[1]\n","                else:\n","                    next_sx = sx\n","                    next_sy = sy\n","\n","                state_int = self.point_to_int((sx, sy))\n","                action_int = self.actions.index(action)\n","                next_state_int = self.point_to_int((next_sx, next_sy))\n","                reward = self.reward(next_state_int)\n","                trajectory.append((state_int, action_int, reward))\n","\n","                sx = next_sx\n","                sy = next_sy\n","\n","            trajectories.append(trajectory)\n","\n","        return np.array(trajectories)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Mw-9oEBW8oyr"},"source":["# Quick unit test using gridworld.\n","\n","gw = Gridworld(3, 0.3, 0.9)\n","\n","v = value([gw.optimal_policy_deterministic(s) for s in range(gw.n_states)],\n","            gw.n_states,\n","            gw.transition_probability,\n","            [gw.reward(s) for s in range(gw.n_states)],\n","            gw.discount)\n","\n","assert np.isclose(v,\n","                    [5.7194282, 6.46706692, 6.42589811,\n","                    6.46706692, 7.47058224, 7.96505174,\n","                    6.42589811, 7.96505174, 8.19268666], 1).all()\n","\n","opt_v = optimal_value(gw.n_states,\n","                        gw.n_actions,\n","                        gw.transition_probability,\n","                        [gw.reward(s) for s in range(gw.n_states)],\n","                        gw.discount)\n","\n","assert np.isclose(v, opt_v).all()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5oOuPJEW83PI"},"source":["\"\"\"\n","Implements maximum entropy inverse reinforcement learning (Ziebart et al., 2008)\n","\n","Matthew Alger, 2015\n","matthew.alger@anu.edu.au\n","\"\"\"\n","\n","from itertools import product\n","\n","import numpy as np\n","import numpy.random as rn\n","\n","def irl(feature_matrix, n_actions, discount, transition_probability,\n","        trajectories, epochs, learning_rate):\n","    \"\"\"\n","    Find the reward function for the given trajectories.\n","\n","    feature_matrix: Matrix with the nth row representing the nth state. NumPy\n","        array with shape (N, D) where N is the number of states and D is the\n","        dimensionality of the state.\n","    n_actions: Number of actions A. int.\n","    discount: Discount factor of the MDP. float.\n","    transition_probability: NumPy array mapping (state_i, action, state_k) to\n","        the probability of transitioning from state_i to state_k under action.\n","        Shape (N, A, N).\n","    trajectories: 3D array of state/action pairs. States are ints, actions\n","        are ints. NumPy array with shape (T, L, 2) where T is the number of\n","        trajectories and L is the trajectory length.\n","    epochs: Number of gradient descent steps. int.\n","    learning_rate: Gradient descent learning rate. float.\n","    -> Reward vector with shape (N,).\n","    \"\"\"\n","\n","    n_states, d_states = feature_matrix.shape\n","\n","    # Initialise weights.\n","    alpha = rn.uniform(size=(d_states,))\n","\n","    # Calculate the feature expectations \\tilde{phi}.\n","    feature_expectations = find_feature_expectations(feature_matrix,\n","                                                     trajectories)\n","\n","    # Gradient descent on alpha.\n","    for i in range(epochs):\n","        # print(\"i: {}\".format(i))\n","        r = feature_matrix.dot(alpha)\n","        expected_svf = find_expected_svf(n_states, r, n_actions, discount,\n","                                         transition_probability, trajectories)\n","        grad = feature_expectations - feature_matrix.T.dot(expected_svf)\n","\n","        alpha += learning_rate * grad\n","\n","    return feature_matrix.dot(alpha).reshape((n_states,))\n","\n","def find_svf(n_states, trajectories):\n","    \"\"\"\n","    Find the state visitation frequency from trajectories.\n","\n","    n_states: Number of states. int.\n","    trajectories: 3D array of state/action pairs. States are ints, actions\n","        are ints. NumPy array with shape (T, L, 2) where T is the number of\n","        trajectories and L is the trajectory length.\n","    -> State visitation frequencies vector with shape (N,).\n","    \"\"\"\n","\n","    svf = np.zeros(n_states)\n","\n","    for trajectory in trajectories:\n","        for state, _, _ in trajectory:\n","            svf[state] += 1\n","\n","    svf /= trajectories.shape[0]\n","\n","    return svf\n","\n","def find_feature_expectations(feature_matrix, trajectories):\n","    \"\"\"\n","    Find the feature expectations for the given trajectories. This is the\n","    average path feature vector.\n","\n","    feature_matrix: Matrix with the nth row representing the nth state. NumPy\n","        array with shape (N, D) where N is the number of states and D is the\n","        dimensionality of the state.\n","    trajectories: 3D array of state/action pairs. States are ints, actions\n","        are ints. NumPy array with shape (T, L, 2) where T is the number of\n","        trajectories and L is the trajectory length.\n","    -> Feature expectations vector with shape (D,).\n","    \"\"\"\n","\n","    feature_expectations = np.zeros(feature_matrix.shape[1])\n","\n","    for trajectory in trajectories:\n","        for state, _, _ in trajectory:\n","            feature_expectations += feature_matrix[state]\n","\n","    feature_expectations /= trajectories.shape[0]\n","\n","    return feature_expectations\n","\n","def find_expected_svf(n_states, r, n_actions, discount,\n","                      transition_probability, trajectories):\n","    \"\"\"\n","    Find the expected state visitation frequencies using algorithm 1 from\n","    Ziebart et al. 2008.\n","\n","    n_states: Number of states N. int.\n","    alpha: Reward. NumPy array with shape (N,).\n","    n_actions: Number of actions A. int.\n","    discount: Discount factor of the MDP. float.\n","    transition_probability: NumPy array mapping (state_i, action, state_k) to\n","        the probability of transitioning from state_i to state_k under action.\n","        Shape (N, A, N).\n","    trajectories: 3D array of state/action pairs. States are ints, actions\n","        are ints. NumPy array with shape (T, L, 2) where T is the number of\n","        trajectories and L is the trajectory length.\n","    -> Expected state visitation frequencies vector with shape (N,).\n","    \"\"\"\n","\n","    n_trajectories = trajectories.shape[0]\n","    trajectory_length = trajectories.shape[1]\n","\n","    policy = find_policy(n_states, r, n_actions, discount,\n","                                    transition_probability)\n","    # policy = find_policy(n_states, n_actions,\n","                                        #  transition_probability, r, discount)\n","\n","    start_state_count = np.zeros(n_states)\n","    for trajectory in trajectories:\n","        start_state_count[trajectory[0, 0]] += 1\n","    p_start_state = start_state_count/n_trajectories\n","\n","    expected_svf = np.tile(p_start_state, (trajectory_length, 1)).T\n","    for t in range(1, trajectory_length):\n","        expected_svf[:, t] = 0\n","        for i, j, k in product(list(range(n_states)), list(range(n_actions)), list(range(n_states))):\n","            expected_svf[k, t] += (expected_svf[i, t-1] *\n","                                  policy[i, j] * # Stochastic policy\n","                                  transition_probability[i, j, k])\n","\n","    return expected_svf.sum(axis=1)\n","\n","def softmax(x1, x2):\n","    \"\"\"\n","    Soft-maximum calculation, from algorithm 9.2 in Ziebart's PhD thesis.\n","\n","    x1: float.\n","    x2: float.\n","    -> softmax(x1, x2)\n","    \"\"\"\n","\n","    max_x = max(x1, x2)\n","    min_x = min(x1, x2)\n","    return max_x + np.log(1 + np.exp(min_x - max_x))\n","\n","def find_policy(n_states, r, n_actions, discount,\n","                           transition_probability):\n","    \"\"\"\n","    Find a policy with linear value iteration. Based on the code accompanying\n","    the Levine et al. GPIRL paper and on Ziebart's PhD thesis (algorithm 9.1).\n","\n","    n_states: Number of states N. int.\n","    r: Reward. NumPy array with shape (N,).\n","    n_actions: Number of actions A. int.\n","    discount: Discount factor of the MDP. float.\n","    transition_probability: NumPy array mapping (state_i, action, state_k) to\n","        the probability of transitioning from state_i to state_k under action.\n","        Shape (N, A, N).\n","    -> NumPy array of states and the probability of taking each action in that\n","        state, with shape (N, A).\n","    \"\"\"\n","\n","    # V = value_iteration.value(n_states, transition_probability, r, discount)\n","\n","    # NumPy's dot really dislikes using inf, so I'm making everything finite\n","    # using nan_to_num.\n","    V = np.nan_to_num(np.ones((n_states, 1)) * float(\"-inf\"))\n","\n","    diff = np.ones((n_states,))\n","    while (diff > 1e-4).all():  # Iterate until convergence.\n","        new_V = r.copy()\n","        for j in range(n_actions):\n","            for i in range(n_states):\n","                new_V[i] = softmax(new_V[i], r[i] + discount*\n","                    np.sum(transition_probability[i, j, k] * V[k]\n","                           for k in range(n_states)))\n","\n","        # # This seems to diverge, so we z-score it (engineering hack).\n","        new_V = (new_V - new_V.mean())/new_V.std()\n","\n","        diff = abs(V - new_V)\n","        V = new_V\n","\n","    # We really want Q, not V, so grab that using equation 9.2 from the thesis.\n","    Q = np.zeros((n_states, n_actions))\n","    for i in range(n_states):\n","        for j in range(n_actions):\n","            p = np.array([transition_probability[i, j, k]\n","                          for k in range(n_states)])\n","            Q[i, j] = p.dot(r + discount*V)\n","\n","    # Softmax by row to interpret these values as probabilities.\n","    Q -= Q.max(axis=1).reshape((n_states, 1))  # For numerical stability.\n","    Q = np.exp(Q)/np.exp(Q).sum(axis=1).reshape((n_states, 1))\n","    return Q\n","\n","def expected_value_difference(n_states, n_actions, transition_probability,\n","    reward, discount, p_start_state, optimal_value, true_reward):\n","    \"\"\"\n","    Calculate the expected value difference, which is a proxy to how good a\n","    recovered reward function is.\n","\n","    n_states: Number of states. int.\n","    n_actions: Number of actions. int.\n","    transition_probability: NumPy array mapping (state_i, action, state_k) to\n","        the probability of transitioning from state_i to state_k under action.\n","        Shape (N, A, N).\n","    reward: Reward vector mapping state int to reward. Shape (N,).\n","    discount: Discount factor. float.\n","    p_start_state: Probability vector with the ith component as the probability\n","        that the ith state is the start state. Shape (N,).\n","    optimal_value: Value vector for the ground reward with optimal policy.\n","        The ith component is the value of the ith state. Shape (N,).\n","    true_reward: True reward vector. Shape (N,).\n","    -> Expected value difference. float.\n","    \"\"\"\n","\n","    policy = value_iteration.find_policy(n_states, n_actions,\n","        transition_probability, reward, discount)\n","    value = value_iteration.value(policy.argmax(axis=1), n_states,\n","        transition_probability, true_reward, discount)\n","\n","    evd = optimal_value.dot(p_start_state) - value.dot(p_start_state)\n","    return evd\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":281},"id":"aZqrVXx49JGK","executionInfo":{"status":"ok","timestamp":1636567177820,"user_tz":-330,"elapsed":12793,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}},"outputId":"5eb15c55-ddff-447c-ab42-4c8ff52691ce"},"source":["\"\"\"\n","Run maximum entropy inverse reinforcement learning on the gridworld MDP.\n","\n","Matthew Alger, 2015\n","matthew.alger@anu.edu.au\n","\"\"\"\n","\n","import numpy as np\n","import matplotlib.pyplot as plt\n","\n","\n","def main(grid_size, discount, n_trajectories, epochs, learning_rate):\n","    \"\"\"\n","    Run maximum entropy inverse reinforcement learning on the gridworld MDP.\n","\n","    Plots the reward function.\n","\n","    grid_size: Grid size. int.\n","    discount: MDP discount factor. float.\n","    n_trajectories: Number of sampled trajectories. int.\n","    epochs: Gradient descent iterations. int.\n","    learning_rate: Gradient descent learning rate. float.\n","    \"\"\"\n","\n","    wind = 0.3\n","    trajectory_length = 3*grid_size\n","\n","    gw = Gridworld(grid_size, wind, discount)\n","    trajectories = gw.generate_trajectories(n_trajectories,\n","                                            trajectory_length,\n","                                            gw.optimal_policy)\n","    feature_matrix = gw.feature_matrix()\n","    ground_r = np.array([gw.reward(s) for s in range(gw.n_states)])\n","    r = irl(feature_matrix, gw.n_actions, discount,\n","        gw.transition_probability, trajectories, epochs, learning_rate)\n","\n","    plt.subplot(1, 2, 1)\n","    plt.pcolor(ground_r.reshape((grid_size, grid_size)))\n","    plt.colorbar()\n","    plt.title(\"Groundtruth reward\")\n","    plt.subplot(1, 2, 2)\n","    plt.pcolor(r.reshape((grid_size, grid_size)))\n","    plt.colorbar()\n","    plt.title(\"Recovered reward\")\n","    plt.show()\n","\n","if __name__ == '__main__':\n","    main(5, 0.01, 20, 200, 0.01)"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 4 Axes>"]},"metadata":{"needs_background":"light"}}]}]}