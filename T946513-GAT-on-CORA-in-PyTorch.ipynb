{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"T946513 | GAT on CORA in PyTorch","provenance":[{"file_id":"1MLa8I6LkNp7yMC9h7Tt6tiNWJsg6Q9Ls","timestamp":1638106054440}],"collapsed_sections":[],"mount_file_id":"1MLa8I6LkNp7yMC9h7Tt6tiNWJsg6Q9Ls","authorship_tag":"ABX9TyNMVr83BqEmhayCPZXP/Jwb"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8S8VFwCjxNZa","executionInfo":{"status":"ok","timestamp":1638106724565,"user_tz":-330,"elapsed":6245,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}},"outputId":"a1a43f7f-b20b-4e2c-9551-8b5c73e68d59"},"source":["!pip install -q dgl"],"execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[K     |████████████████████████████████| 4.4 MB 8.0 MB/s \n","\u001b[?25h"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"W8_VhUsDYzLE","executionInfo":{"status":"ok","timestamp":1638106767240,"user_tz":-330,"elapsed":702,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}},"outputId":"c12b7b04-08d7-4651-e9f8-fa71294e79fa"},"source":["import numpy as np\n","import pandas as pd\n","import random\n","import os, sys, pickle\n","import random, math, gc\n","\n","from collections import defaultdict\n","\n","import torch\n","import torch.nn as nn\n","from torch.utils.data import Dataset, DataLoader\n","import torch.optim as optim\n","import torch.nn.functional as F\n","\n","from datetime import datetime\n","import matplotlib.pyplot as plt\n","\n","import warnings\n","warnings.filterwarnings('ignore')\n","\n","import dgl\n","from dgl.data import CoraGraphDataset"],"execution_count":15,"outputs":[{"output_type":"stream","name":"stderr","text":["DGL backend not selected or invalid.  Assuming PyTorch for now.\n","Using backend: pytorch\n"]},{"output_type":"stream","name":"stdout","text":["Setting the default backend to \"pytorch\". You can change it in the ~/.dgl/config.json file or export the DGLBACKEND environment variable.  Valid options are: pytorch, mxnet, tensorflow (all lowercase)\n"]}]},{"cell_type":"code","metadata":{"id":"VxRBHd8aZYEL"},"source":["def seed_everything(seed=1234):\n","    random.seed(seed)\n","    os.environ['PYTHONHASHSEED'] = str(seed)\n","    np.random.seed(seed)\n","    torch.manual_seed(seed)\n","    torch.cuda.manual_seed(seed)\n","    torch.backends.cudnn.deterministic = True"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qAY1qpJdyBgK","executionInfo":{"status":"ok","timestamp":1630651401173,"user_tz":-330,"elapsed":4842,"user":{"displayName":"Sparsh Agarwal","photoUrl":"","userId":"13037694610922482904"}},"outputId":"3a303c9f-772c-45be-e4b1-98aa09288f69"},"source":["import numpy as np\n","import pandas as pd\n","import random\n","import os, sys, pickle\n","import random, math, gc\n","\n","from collections import defaultdict\n","\n","import torch\n","import torch.nn as nn\n","from torch.utils.data import Dataset, DataLoader\n","import torch.optim as optim\n","import torch.nn.functional as F\n","\n","from datetime import datetime\n","import matplotlib.pyplot as plt\n","\n","import warnings\n","warnings.filterwarnings('ignore')\n","\n","import dgl\n","from dgl.data import CoraGraphDataset"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["DGL backend not selected or invalid.  Assuming PyTorch for now.\n","Using backend: pytorch\n"]},{"output_type":"stream","name":"stdout","text":["Setting the default backend to \"pytorch\". You can change it in the ~/.dgl/config.json file or export the DGLBACKEND environment variable.  Valid options are: pytorch, mxnet, tensorflow (all lowercase)\n"]}]},{"cell_type":"code","metadata":{"id":"ANmISYaJyBgN"},"source":["def seed_everything(seed=1234):\n","    random.seed(seed)\n","    os.environ['PYTHONHASHSEED'] = str(seed)\n","    np.random.seed(seed)\n","    torch.manual_seed(seed)\n","    torch.cuda.manual_seed(seed)\n","    torch.backends.cudnn.deterministic = True"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9ausYME5Z5nA"},"source":["def load_data():\n","\n","    graph = CoraGraphDataset()[0]\n","    train_mask = ~(graph.ndata['test_mask'] |  graph.ndata['val_mask'])\n","    val_mask = graph.ndata['val_mask']\n","    test_mask = graph.ndata['test_mask']\n","\n","    feat = graph.ndata['feat']\n","    label = graph.ndata['label']\n","    n_nodes = graph.num_nodes()\n","    edges = graph.edges()\n","    adj = np.zeros((n_nodes, n_nodes))\n","    for src, dst in zip(edges[0].numpy(), edges[1].numpy()):\n","        adj[src, dst] += 1\n","    \n","    return train_mask, val_mask, test_mask, feat, label, torch.LongTensor(adj)\n","\n","def seed_everything(seed=1234):\n","    random.seed(seed)\n","    os.environ['PYTHONHASHSEED'] = str(seed)\n","    np.random.seed(seed)\n","    torch.manual_seed(seed)\n","    torch.cuda.manual_seed(seed)\n","    torch.backends.cudnn.deterministic = True"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Jkok3V-DZ1LQ"},"source":["class GATLayer(nn.Module):\n","    def __init__(self, input_dim, out_dim, device):\n","        super(GATLayer, self).__init__()\n","        self.input_dim = input_dim\n","        self.out_dim = out_dim\n","        \n","        self.W = nn.Linear(input_dim, out_dim)\n","        self.a = nn.Linear(2*self.out_dim, 1)\n","        \n","        self.device = device\n","        nn.init.xavier_uniform_(self.W.weight, gain=1.414)\n","        nn.init.xavier_uniform_(self.a.weight, gain=1.414)\n","        \n","    def forward(self, h, adj):\n","        # h : (batch_size, input_dim)\n","        # adj : (batch_size, batch_size) => batch에 해당하는 adj matrix\n","        batch_size = h.size(0)\n","        wh = self.W(h) # wh : (batch_size, hidden_dim)\n","\n","        repeat_wh = wh.repeat_interleave(batch_size, dim=0)\n","        tile_wh = wh.repeat(batch_size, 1)\n","        \n","        wh_concat = torch.cat([repeat_wh, tile_wh], dim=1) # whwh : (batch_size*batch_size, 2*hidden_dim)\n","        wh_concat = F.leaky_relu(self.a(wh_concat), negative_slope=0.2) # awhwh : (batch_size*batch_size, 1)\n","        wh_concat = wh_concat.view(batch_size, batch_size, -1).squeeze() # awhwh : (batch_size, batch_size, 1)\n","\n","        small = -9e15 * torch.ones(batch_size, batch_size).to(self.device)\n","        \n","        masked_attention = torch.where(adj > 0, wh_concat, small) # masked_attention : (batch_size, batch_size, n_heads)\n","        attention_weight = F.softmax(masked_attention, dim=1) # attention_weight : (n_heads, batch_size, batch_size)\n","        \n","        return torch.mm(attention_weight, wh).squeeze()\n","\n","class MultiHeadGATLayer(nn.Module):\n","    '''\n","    Attention is all you need 에서 한 방식으로 multihead attention 을 구현해봄\n","    계속 에러가 나는데 원인을 찾지 못했다... ㅜㅜ\n","    '''\n","    def __init__(self, input_dim, out_dim, n_heads, device, concat):\n","        super(MultiHeadGATLayer, self).__init__()\n","        self.input_dim = input_dim\n","        self.out_dim = out_dim\n","        self.n_heads = n_heads\n","        self.head_dim = out_dim // n_heads\n","        self.concat = concat\n","        self.W = nn.Linear(input_dim, out_dim)\n","        self.a = nn.Linear(2*self.n_heads, 1)\n","        \n","        self.device = device\n","        nn.init.xavier_uniform_(self.W.weight, gain=1.414)\n","        nn.init.xavier_uniform_(self.a.weight, gain=1.414)\n","        \n","    def forward(self, h, adj):\n","        # h : (batch_size, input_dim)\n","        # adj : (batch_size, batch_size) => batch에 해당하는 adj matrix\n","        batch_size = h.size(0)\n","        wh = self.W(h) # wh : (batch_size, hidden_dim)\n","        wh_head = wh.view(batch_size, self.n_heads, self.head_dim)\n","\n","        repeat_wh = wh_head.repeat_interleave(batch_size, dim=0)\n","        tile_wh = wh_head.repeat(batch_size, 1, 1)\n","        \n","        wh_concat = torch.cat([repeat_wh, tile_wh], dim=2) # whwh : (batch_size*batch_size, 2*hidden_dim)\n","        wh_concat = F.leaky_relu(self.a(wh_concat), negative_slope=0.2) # awhwh : (batch_size*batch_size, 1)\n","        wh_concat = wh_concat.view(batch_size, batch_size, -1).squeeze() # awhwh : (batch_size, batch_size, 1)\n","\n","        small = -9e15 * torch.ones_like(wh_concat).to(self.device)\n","        adj = adj.repeat(self.n_heads, 1, 1).permute(1,2,0)\n","\n","        masked_attention = torch.where(adj > 0, wh_concat, small) # masked_attention : (batch_size, batch_size, n_heads)\n","        attention_weight = F.softmax(masked_attention, dim=1).permute(2,0,1) # attention_weight : (n_heads, batch_size, batch_size)\n","        \n","        if self.concat:\n","            return F.elu(torch.bmm(attention_weight, wh_head.permute(1,0,2)).squeeze()).view(-1, self.out_dim)\n","        else:\n","            return torch.bmm(attention_weight, wh_head.permute(1,0,2)).squeeze().mean(dim=0)\n","\n","\n","class GAT(nn.Module):\n","    def __init__(self, config):\n","        super(GAT, self).__init__()\n","        \n","        self.multihead_attention = [GATLayer(config.input_dim, config.hidden_dim, config.device) for _ in range(config.n_heads)]\n","        for i, mha in enumerate(self.multihead_attention):\n","            self.add_module(f'attention_head{i}', mha)\n","        \n","        self.outgat = GATLayer(config.n_heads*config.hidden_dim, config.output_dim, config.device)\n","\n","        self.dropout1 = nn.Dropout(config.dropout)\n","        self.dropout2 = nn.Dropout(config.dropout)\n","\n","    def forward(self, h, adj):\n","        h = self.dropout1(h)\n","        out = torch.cat([F.elu(mha(h, adj)) for mha in self.multihead_attention], axis=1)\n","        out = self.dropout1(out)\n","        out = F.elu(self.outgat(out, adj))\n","        return out"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Io0S9f4-aCmo","executionInfo":{"status":"ok","timestamp":1630651723038,"user_tz":-330,"elapsed":404,"user":{"displayName":"Sparsh Agarwal","photoUrl":"","userId":"13037694610922482904"}},"outputId":"673f4fb4-ff0e-44d1-bc59-3ceefc163b0b"},"source":["train_mask, val_mask, test_mask, feat, label, adj = load_data()\n","\n","batch_size = len(train_mask)\n","input_dim = feat.shape[1]\n","output_dim = label.unique().shape[0]"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["  NumNodes: 2708\n","  NumEdges: 10556\n","  NumFeats: 1433\n","  NumClasses: 7\n","  NumTrainingSamples: 140\n","  NumValidationSamples: 500\n","  NumTestSamples: 1000\n","Done loading data from cached files.\n"]}]},{"cell_type":"code","metadata":{"id":"ndxAuO4JaAV8"},"source":["class Config:\n","    lr = 0.005\n","    weight_decay = 5e-4\n","    hidden_dim = 32\n","    epochs = 200\n","    early_stopping_round = None\n","    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","    seed = 1995\n","    n_heads = 8\n","    dropout = 0.6\n","    alpha = 0.2\n","    bs = batch_size\n","    input_dim = input_dim\n","    output_dim = output_dim\n","\n","args = Config()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3fumyMgIZmzd"},"source":["seed_everything(args.seed)\n","\n","model = GAT(args)\n","model = model.to(args.device)\n","optimizer = optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)\n","loss_fn = nn.CrossEntropyLoss()\n","\n","if 'cuda' in args.device:\n","    feat = feat.to(args.device)\n","    label = label.to(args.device)\n","    adj = adj.to(args.device)\n","    train_mask = train_mask.to(args.device)\n","    test_mask = test_mask.to(args.device)\n","    \n","train_label = label[train_mask]\n","test_label = label[test_mask]\n","\n","\n","history = defaultdict(list)\n","start = datetime.now()\n","best_loss, early_step, best_epoch = 0, 0, 0\n","for epoch in range(args.epochs):\n","    model.train()\n","    optimizer.zero_grad()\n","    output = model(feat, adj)\n","    acc = torch.sum(train_label == torch.argmax(output[train_mask], axis=1)) / len(train_label)\n","    loss = loss_fn(output[train_mask], train_label)\n","    loss.backward()\n","    optimizer.step()\n","\n","    history['train_loss'].append(loss.item())\n","    history['train_acc'].append(acc)\n","\n","    model.eval()\n","    with torch.no_grad():    \n","        output = model(feat, adj)\n","        acc = torch.sum(test_label == torch.argmax(output[test_mask], axis=1)) / len(test_label)\n","        loss = loss_fn(output[test_mask], test_label)\n","\n","    history['valid_loss'].append(loss.item())\n","    history['valid_acc'].append(acc)\n","\n","    if epoch == 0 or epoch == args.epochs-1 or (epoch+1)%10 == 0:\n","        print(f'EPOCH {epoch+1} : TRAINING loss {history[\"train_loss\"][-1]:.3f}, TRAINING ACC {history[\"train_acc\"][-1]:.3f}, VALID loss {history[\"valid_loss\"][-1]:.3f}, VALID ACC {history[\"valid_acc\"][-1]:.3f}')\n","    \n","    if history['valid_acc'][-1] > best_loss:\n","        best_loss = history['valid_acc'][-1]\n","        best_epoch = epoch\n","\n","end = datetime.now()\n","print(end-start)\n","print(f'At EPOCH {best_epoch + 1}, We have Best Acc {best_loss}')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"EsFzSRvnxb3X"},"source":["---"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QKXM9U7sxb3Z","executionInfo":{"status":"ok","timestamp":1638106792273,"user_tz":-330,"elapsed":3002,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}},"outputId":"729e3d64-42f2-4de7-9685-304a7dfbd0dc"},"source":["!pip install -q watermark\n","%reload_ext watermark\n","%watermark -a \"Sparsh A.\" -m -iv -u -t -d"],"execution_count":17,"outputs":[{"output_type":"stream","name":"stdout","text":["Author: Sparsh A.\n","\n","Last updated: 2021-11-28 13:39:52\n","\n","Compiler    : GCC 7.5.0\n","OS          : Linux\n","Release     : 5.4.104+\n","Machine     : x86_64\n","Processor   : x86_64\n","CPU cores   : 2\n","Architecture: 64bit\n","\n","numpy     : 1.19.5\n","sys       : 3.7.12 (default, Sep 10 2021, 00:21:48) \n","[GCC 7.5.0]\n","IPython   : 5.5.0\n","torch     : 1.10.0+cu111\n","networkx  : 2.6.3\n","dgl       : 0.6.1\n","pandas    : 1.1.5\n","matplotlib: 3.2.2\n","\n"]}]},{"cell_type":"markdown","metadata":{"id":"Bn4s9s_Axb3Z"},"source":["---"]},{"cell_type":"markdown","metadata":{"id":"jFXf3bKdxb3a"},"source":["**END**"]}]}