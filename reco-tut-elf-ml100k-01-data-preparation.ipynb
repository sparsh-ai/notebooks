{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"reco-tut-elf-ml100k-01-data-preparation.ipynb","provenance":[{"file_id":"1vRFN2OMXJN6phWLVd7xItGEuzBoxzsq0","timestamp":1628348811833}],"collapsed_sections":[],"mount_file_id":"1oECZ1-YDDxo-5cuTGuiO7t-d4V6qvi_2","authorship_tag":"ABX9TyOVV2s6yOaiQAhKf6hEktPm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"id":"xolR7gYoJCEb"},"source":["import os\n","project_name = \"reco-tut-elf\"; branch = \"main\"; account = \"sparsh-ai\"\n","project_path = os.path.join('/content', project_name)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Yq6V2furI-Qf","executionInfo":{"status":"ok","timestamp":1628348924298,"user_tz":-330,"elapsed":4277,"user":{"displayName":"Sparsh Agarwal","photoUrl":"","userId":"13037694610922482904"}},"outputId":"714ab8f7-6e28-4ec8-9bc6-1f37017143bb"},"source":["if not os.path.exists(project_path):\n","    !cp /content/drive/MyDrive/mykeys.py /content\n","    import mykeys\n","    !rm /content/mykeys.py\n","    path = \"/content/\" + project_name; \n","    !mkdir \"{path}\"\n","    %cd \"{path}\"\n","    import sys; sys.path.append(path)\n","    !git config --global user.email \"recotut@recohut.com\"\n","    !git config --global user.name  \"reco-tut\"\n","    !git init\n","    !git remote add origin https://\"{mykeys.git_token}\":x-oauth-basic@github.com/\"{account}\"/\"{project_name}\".git\n","    !git pull origin \"{branch}\"\n","    !git checkout main\n","else:\n","    %cd \"{project_path}\""],"execution_count":null,"outputs":[{"output_type":"stream","text":["/content/reco-tut-elf\n","Initialized empty Git repository in /content/reco-tut-elf/.git/\n","remote: Enumerating objects: 49, done.\u001b[K\n","remote: Counting objects: 100% (49/49), done.\u001b[K\n","remote: Compressing objects: 100% (38/38), done.\u001b[K\n","remote: Total 49 (delta 7), reused 48 (delta 7), pack-reused 0\u001b[K\n","Unpacking objects: 100% (49/49), done.\n","From https://github.com/sparsh-ai/reco-tut-elf\n"," * branch            main       -> FETCH_HEAD\n"," * [new branch]      main       -> origin/main\n","Branch 'main' set up to track remote branch 'main' from 'origin'.\n","Switched to a new branch 'main'\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"lRc_TZM6I-Ql"},"source":["!git status"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"PW8XIzLQI-Qm"},"source":["!git add . && git commit -m 'commit' && git push origin \"{branch}\""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"iZjJ6LFCKFBX"},"source":["---"]},{"cell_type":"code","metadata":{"id":"9u0cM6SsKFcG"},"source":["import torch\n","import random\n","import numpy as np\n","import pandas as pd\n","from copy import deepcopy\n","from torch.utils.data import DataLoader, Dataset\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics.pairwise import pairwise_distances\n","from sklearn.metrics.pairwise import cosine_similarity"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZGSJwMwcKXyo"},"source":["random.seed(1)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MfvFirx3LGLI"},"source":["### Read dataset"]},{"cell_type":"code","metadata":{"id":"s4Jc-qFiKayc"},"source":["def read_data():\n","    \"\"\"Read dataset\"\"\"\n","\n","    dataset = pd.DataFrame()\n","\n","    # Load Movielens 100K Data\n","    data_dir = './data/bronze/ml-100k/u.data'\n","    dataset = pd.read_csv(data_dir, sep='\\t', header=None, names=['uid', 'mid', 'rating', 'timestamp'],\n","                                engine='python')\n","\n","    # Reindex data\n","    user_id = dataset[['uid']].drop_duplicates().reindex()\n","    user_id['userId'] = np.arange(len(user_id))\n","    dataset = pd.merge(dataset, user_id, on=['uid'], how='left')\n","    item_id = dataset[['mid']].drop_duplicates()\n","    item_id['itemId'] = np.arange(len(item_id))\n","    dataset = pd.merge(dataset, item_id, on=['mid'], how='left')\n","    if 'test' in dataset:\n","        dataset = dataset[['userId', 'itemId', 'rating', 'timestamp', 'test']]\n","    else:\n","        dataset = dataset[['userId', 'itemId', 'rating', 'timestamp']]\n","\n","    return dataset"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":204},"id":"nmw_W9Q7NCSg","executionInfo":{"status":"ok","timestamp":1628349755451,"user_tz":-330,"elapsed":495,"user":{"displayName":"Sparsh Agarwal","photoUrl":"","userId":"13037694610922482904"}},"outputId":"e1190226-4981-4e61-e9d7-7fefae7ae7d1"},"source":["dataset = read_data()\n","dataset.head()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>userId</th>\n","      <th>itemId</th>\n","      <th>rating</th>\n","      <th>timestamp</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>3</td>\n","      <td>881250949</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>3</td>\n","      <td>891717742</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>2</td>\n","      <td>2</td>\n","      <td>1</td>\n","      <td>878887116</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>3</td>\n","      <td>3</td>\n","      <td>2</td>\n","      <td>880606923</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>4</td>\n","      <td>4</td>\n","      <td>1</td>\n","      <td>886397596</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   userId  itemId  rating  timestamp\n","0       0       0       3  881250949\n","1       1       1       3  891717742\n","2       2       2       1  878887116\n","3       3       3       2  880606923\n","4       4       4       1  886397596"]},"metadata":{"tags":[]},"execution_count":14}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rPxGvPB9NQDL","executionInfo":{"status":"ok","timestamp":1628349758520,"user_tz":-330,"elapsed":9,"user":{"displayName":"Sparsh Agarwal","photoUrl":"","userId":"13037694610922482904"}},"outputId":"429daeef-2de7-4bad-a57c-7fc19e27722b"},"source":["dataset.info()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["<class 'pandas.core.frame.DataFrame'>\n","Int64Index: 100000 entries, 0 to 99999\n","Data columns (total 4 columns):\n"," #   Column     Non-Null Count   Dtype\n","---  ------     --------------   -----\n"," 0   userId     100000 non-null  int64\n"," 1   itemId     100000 non-null  int64\n"," 2   rating     100000 non-null  int64\n"," 3   timestamp  100000 non-null  int64\n","dtypes: int64(4)\n","memory usage: 3.8 MB\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"a5Bkp6cILI4o"},"source":["### Data loaders"]},{"cell_type":"code","metadata":{"id":"0JVxCs2LLOSn"},"source":["class data_loader(Dataset):\n","    \"\"\"Convert user, item, negative and target Tensors into Pytorch Dataset\"\"\"\n","\n","    def __init__(self, user_tensor, positive_item_tensor, negative_item_tensor, target_tensor):\n","        self.user_tensor = user_tensor\n","        self.positive_item_tensor = positive_item_tensor\n","        self.negative_item_tensor = negative_item_tensor\n","        self.target_tensor = target_tensor\n","\n","    def __getitem__(self, index):\n","        return self.user_tensor[index], self.positive_item_tensor[index], self.negative_item_tensor[index], self.target_tensor[index]\n","\n","    def __len__(self):\n","        return self.user_tensor.size(0)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QDmAbTzlLV-s"},"source":["class data_loader_implicit(Dataset):\n","    \"\"\"Convert user and item Tensors into Pytorch Dataset\"\"\"\n","\n","    def __init__(self, user_tensor, item_tensor):\n","        self.user_tensor = user_tensor\n","        self.item_tensor = item_tensor\n","\n","    def __getitem__(self, index):\n","        return self.user_tensor[index], self.item_tensor[index]\n","\n","    def __len__(self):\n","        return self.user_tensor.size(0)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"b1X-_itmLZkN"},"source":["class data_loader_test_explicit(Dataset):\n","    \"\"\"Convert user, item and target Tensors into Pytorch Dataset\"\"\"\n","\n","    def __init__(self, user_tensor, item_tensor, target_tensor):\n","        self.user_tensor = user_tensor\n","        self.item_tensor = item_tensor\n","        self.target_tensor = target_tensor\n","\n","    def __getitem__(self, index):\n","        return self.user_tensor[index], self.item_tensor[index], self.target_tensor[index]\n","\n","    def __len__(self):\n","        return self.user_tensor.size(0)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"C7VN-pGkLfo-"},"source":["class data_loader_negatives(Dataset):\n","    \"\"\"Convert user and item negative Tensors into Pytorch Dataset\"\"\"\n","\n","    def __init__(self, user_neg_tensor, item_neg_tensor):\n","        self.user_neg_tensor = user_neg_tensor\n","        self.item_neg_tensor = item_neg_tensor\n","\n","    def __getitem__(self, index):\n","        return self.user_neg_tensor[index], self.item_neg_tensor[index]\n","\n","    def __len__(self):\n","        return self.user_neg_tensor.size(0)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"FixxozhZLjhJ"},"source":["## Data sampler"]},{"cell_type":"code","metadata":{"id":"dss0coxcLwYG"},"source":["class SampleGenerator(object):\n","    \"\"\"Construct dataset\"\"\"\n","\n","    def __init__(self, ratings, config, split_val):\n","        \"\"\"\n","        args:\n","            ratings: pd.DataFrame containing 4 columns = ['userId', 'itemId', 'rating', 'timestamp']\n","            config: dictionary containing the configuration hyperparameters\n","            split_val: boolean that takes True if we are using a validation set\n","        \"\"\"\n","        assert 'userId' in ratings.columns\n","        assert 'itemId' in ratings.columns\n","        assert 'rating' in ratings.columns\n","\n","        self.config = config\n","        self.ratings = ratings\n","        self.split_val = split_val\n","        self.preprocess_ratings = self._binarize(ratings)\n","        self.user_pool = set(self.ratings['userId'].unique())\n","        self.item_pool = set(self.ratings['itemId'].unique())\n","        # create negative item samples\n","        self.negatives = self._sample_negative(ratings, self.split_val)\n","        if self.config['loo_eval']:\n","            if self.split_val:\n","                self.train_ratings, self.val_ratings = self._split_loo(self.preprocess_ratings, split_val=True)\n","            else:\n","                self.train_ratings, self.test_ratings = self._split_loo(self.preprocess_ratings, split_val=False)\n","        else:\n","            self.test_rate = self.config['test_rate']\n","            if self.split_val:\n","                self.train_ratings, self.val_ratings = self.train_test_split_random(self.ratings, split_val=True)\n","            else:\n","                self.train_ratings, self.test_ratings = self.train_test_split_random(self.ratings, split_val=False)\n","\n","    def _binarize(self, ratings):\n","        \"\"\"binarize into 0 or 1 for imlicit feedback\"\"\"\n","        ratings = deepcopy(ratings)\n","        ratings['rating'] = 1.0\n","        return ratings\n","\n","    def train_test_split_random(self, ratings, split_val):\n","        \"\"\"Random train/test split\"\"\"\n","        if 'test' in list(ratings):\n","            test = ratings[ratings['test'] == 1]\n","            train = ratings[ratings['test'] == 0]\n","        else:\n","            train, test = train_test_split(ratings, test_size=self.test_rate)\n","        if split_val:\n","            train, val = train_test_split(train, test_size=self.test_rate / (1 - self.test_rate))\n","            return train[['userId', 'itemId', 'rating']], val[['userId', 'itemId', 'rating']]\n","        else:\n","            return train[['userId', 'itemId', 'rating']], test[['userId', 'itemId', 'rating']]\n","\n","    def _split_loo(self, ratings, split_val):\n","        \"\"\"leave-one-out train/test split\"\"\"\n","        if 'test' in list(ratings):\n","            test = ratings[ratings['test'] == 1]\n","            ratings = ratings[ratings['test'] == 0]\n","            if split_val:\n","                ratings['rank_latest'] = ratings.groupby(['userId'])['timestamp'].rank(method='first', ascending=False)\n","                val = ratings[ratings['rank_latest'] == 1]\n","                train = ratings[ratings['rank_latest'] > 1]\n","                return train[['userId', 'itemId', 'rating']], val[['userId', 'itemId', 'rating']]\n","            return ratings[['userId', 'itemId', 'rating']], test[['userId', 'itemId', 'rating']]\n","        ratings['rank_latest'] = ratings.groupby(['userId'])['timestamp'].rank(method='first', ascending=False)\n","        test = ratings[ratings['rank_latest'] == 1]\n","        if split_val:\n","            val = ratings[ratings['rank_latest'] == 2]\n","            train = ratings[ratings['rank_latest'] > 2]\n","            assert train['userId'].nunique() == test['userId'].nunique() == val['userId'].nunique()\n","            return train[['userId', 'itemId', 'rating']], val[['userId', 'itemId', 'rating']]\n","        train = ratings[ratings['rank_latest'] > 1]\n","        assert train['userId'].nunique() == test['userId'].nunique()\n","        return train[['userId', 'itemId', 'rating']], test[['userId', 'itemId', 'rating']]\n","\n","    def _sample_negative(self, ratings, split_val):\n","        \"\"\"return all negative items & 100 sampled negative test items & 100 sampled negative val items\"\"\"\n","        interact_status = ratings.groupby('userId')['itemId'].apply(set).reset_index().rename(\n","            columns={'itemId': 'interacted_items'})\n","        interact_status['negative_items'] = interact_status['interacted_items'].apply(lambda x: self.item_pool - x)\n","        interact_status['test_negative_samples'] = interact_status['negative_items'].apply(lambda x: random.sample(x, 100))\n","        interact_status['negative_items'] = interact_status.apply(lambda x: (x.negative_items - set(x.test_negative_samples)), axis=1)\n","        if split_val:\n","            interact_status['val_negative_samples'] = interact_status['negative_items'].apply(lambda x: random.sample(x, 100))\n","            interact_status['negative_items'] = interact_status.apply(lambda x: (x.negative_items - set(x.val_negative_samples)), axis=1)\n","            return interact_status[['userId', 'negative_items', 'test_negative_samples', 'val_negative_samples']]\n","        else:\n","            return interact_status[['userId', 'negative_items', 'test_negative_samples']]\n","\n","    def train_data_loader(self, batch_size):\n","        \"\"\"instance train loader for one training epoch\"\"\"\n","        train_ratings = pd.merge(self.train_ratings, self.negatives[['userId', 'negative_items']], on='userId')\n","        users = [int(x) for x in train_ratings['userId']]\n","        items = [int(x) for x in train_ratings['itemId']]\n","        ratings = [float(x) for x in train_ratings['rating']]\n","        neg_items = [random.choice(list(neg_list)) for neg_list in train_ratings['negative_items']]\n","        dataset = data_loader(user_tensor=torch.LongTensor(users),\n","                              positive_item_tensor=torch.LongTensor(items),\n","                              negative_item_tensor=torch.LongTensor(neg_items),\n","                              target_tensor=torch.FloatTensor(ratings))\n","        return DataLoader(dataset, batch_size=batch_size, shuffle=True)\n","\n","    def test_data_loader(self, batch_size):\n","        \"\"\"create evaluation data\"\"\"\n","        if self.config['loo_eval']:\n","            test_ratings = pd.merge(self.test_ratings, self.negatives[['userId', 'test_negative_samples']], on='userId')\n","            test_users, test_items, negative_users, negative_items = [], [], [], []\n","            for row in test_ratings.itertuples():\n","                test_users.append(int(row.userId))\n","                test_items.append(int(row.itemId))\n","                for i in range(len(row.test_negative_samples)):\n","                    negative_users.append(int(row.userId))\n","                    negative_items.append(int(row.test_negative_samples[i]))\n","            dataset = data_loader_implicit(user_tensor=torch.LongTensor(test_users),\n","                                           item_tensor=torch.LongTensor(test_items))\n","            dataset_negatives = data_loader_negatives(user_neg_tensor=torch.LongTensor(negative_users),\n","                                                      item_neg_tensor=torch.LongTensor(negative_items))\n","            return [DataLoader(dataset, batch_size=batch_size, shuffle=False), DataLoader(dataset_negatives, batch_size=batch_size, shuffle=False)]\n","        else:\n","            test_ratings = self.test_ratings\n","            test_users = [int(x) for x in test_ratings['userId']]\n","            test_items = [int(x) for x in test_ratings['itemId']]\n","            test_ratings = [float(x) for x in test_ratings['rating']]\n","            dataset = data_loader_test_explicit(user_tensor=torch.LongTensor(test_users),\n","                                                item_tensor=torch.LongTensor(test_items),\n","                                                target_tensor=torch.FloatTensor(test_ratings))\n","            return DataLoader(dataset, batch_size=batch_size, shuffle=False)\n","\n","    def val_data_loader(self, batch_size):\n","        \"\"\"create validation data\"\"\"\n","        if self.config['loo_eval']:\n","            val_ratings = pd.merge(self.val_ratings, self.negatives[['userId', 'val_negative_samples']], on='userId')\n","            val_users, val_items, negative_users, negative_items = [], [], [], []\n","            for row in val_ratings.itertuples():\n","                val_users.append(int(row.userId))\n","                val_items.append(int(row.itemId))\n","                for i in range(len(row.val_negative_samples)):\n","                    negative_users.append(int(row.userId))\n","                    negative_items.append(int(row.val_negative_samples[i]))\n","            dataset = data_loader_implicit(user_tensor=torch.LongTensor(val_users),\n","                                           item_tensor=torch.LongTensor(val_items))\n","            dataset_negatives = data_loader_negatives(user_neg_tensor=torch.LongTensor(negative_users),\n","                                                      item_neg_tensor=torch.LongTensor(negative_items))\n","            return [DataLoader(dataset, batch_size=batch_size, shuffle=False), DataLoader(dataset_negatives, batch_size=batch_size, shuffle=False)]\n","        else:\n","            val_ratings = self.val_ratings\n","            val_users = [int(x) for x in val_ratings['userId']]\n","            val_items = [int(x) for x in val_ratings['itemId']]\n","            val_ratings = [float(x) for x in val_ratings['rating']]\n","            dataset = data_loader_test_explicit(user_tensor=torch.LongTensor(val_users),\n","                                                item_tensor=torch.LongTensor(val_items),\n","                                                target_tensor=torch.FloatTensor(val_ratings))\n","            return DataLoader(dataset, batch_size=batch_size, shuffle=False)\n","\n","    def create_explainability_matrix(self, include_test=False):\n","        \"\"\"create explainability matrix\"\"\"\n","        if not include_test:\n","            print('Creating explainability matrix...')\n","            interaction_matrix = pd.crosstab(self.train_ratings.userId, self.train_ratings.itemId)\n","            missing_columns = list(set(range(self.config['num_items'])) - set(list(interaction_matrix)))\n","            missing_rows = list(set(range(self.config['num_users'])) - set(interaction_matrix.index))\n","            for missing_column in missing_columns:\n","                interaction_matrix[missing_column] = [0] * len(interaction_matrix)\n","            for missing_row in missing_rows:\n","                interaction_matrix.loc[missing_row] = [0] * self.config['num_items']\n","            interaction_matrix = np.array(interaction_matrix[list(range(self.config['num_items']))].sort_index())\n","        elif not self.split_val:\n","            print('Creating test explainability matrix...')\n","            interaction_matrix = np.array(pd.crosstab(self.preprocess_ratings.userId, self.preprocess_ratings.itemId)[\n","                                              list(range(self.config['num_items']))].sort_index())\n","        else:\n","            print('Creating val explainability matrix...')\n","            interaction_matrix = pd.crosstab(self.train_ratings.userId.append(self.val_ratings.userId), self.train_ratings.itemId.append(self.val_ratings.itemId))\n","            missing_columns = list(set(range(self.config['num_items'])) - set(list(interaction_matrix)))\n","            missing_rows = list(set(range(self.config['num_users'])) - set(interaction_matrix.index))\n","            for missing_column in missing_columns:\n","                interaction_matrix[missing_column] = [0] * len(interaction_matrix)\n","            for missing_row in missing_rows:\n","                interaction_matrix.loc[missing_row] = [0] * self.config['num_items']\n","            interaction_matrix = np.array(interaction_matrix[list(range(self.config['num_items']))].sort_index())\n","        #item_similarity_matrix = 1 - pairwise_distances(interaction_matrix.T, metric = \"hamming\")\n","        item_similarity_matrix = cosine_similarity(interaction_matrix.T)\n","        np.fill_diagonal(item_similarity_matrix, 0)\n","        neighborhood = [np.argpartition(row, - self.config['neighborhood'])[- self.config['neighborhood']:]\n","                        for row in item_similarity_matrix]\n","        explainability_matrix = np.array([[sum([interaction_matrix[user, neighbor] for neighbor in neighborhood[item]])\n","                                           for item in range(self.config['num_items'])] for user in\n","                                          range(self.config['num_users'])]) / self.config['neighborhood']\n","        #explainability_matrix[explainability_matrix < 0.1] = 0\n","        #explainability_matrix = explainability_matrix + self.config['epsilon']\n","        return explainability_matrix\n","\n","    def create_popularity_vector(self, include_test=False):\n","        \"\"\"create popularity vector\"\"\"\n","        if not include_test:\n","            print('Creating popularity vector...')\n","            interaction_matrix = pd.crosstab(self.train_ratings.userId, self.train_ratings.itemId)\n","            missing_columns = list(set(range(self.config['num_items'])) - set(list(interaction_matrix)))\n","            missing_rows = list(set(range(self.config['num_users'])) - set(interaction_matrix.index))\n","            for missing_column in missing_columns:\n","                interaction_matrix[missing_column] = [0] * len(interaction_matrix)\n","            for missing_row in missing_rows:\n","                interaction_matrix.loc[missing_row] = [0] * self.config['num_items']\n","            interaction_matrix = np.array(interaction_matrix[list(range(self.config['num_items']))].sort_index())\n","        elif not self.split_val:\n","            print('Creating test popularity vector...')\n","            interaction_matrix = np.array(pd.crosstab(self.preprocess_ratings.userId, self.preprocess_ratings.itemId)[\n","                                              list(range(self.config['num_items']))].sort_index())\n","        else:\n","            print('Creating val popularity vector...')\n","            interaction_matrix = pd.crosstab(self.train_ratings.userId.append(self.val_ratings.userId),\n","                                             self.train_ratings.itemId.append(self.val_ratings.itemId))\n","            missing_columns = list(set(range(self.config['num_items'])) - set(list(interaction_matrix)))\n","            missing_rows = list(set(range(self.config['num_users'])) - set(interaction_matrix.index))\n","            for missing_column in missing_columns:\n","                interaction_matrix[missing_column] = [0] * len(interaction_matrix)\n","            for missing_row in missing_rows:\n","                interaction_matrix.loc[missing_row] = [0] * self.config['num_items']\n","            interaction_matrix = np.array(interaction_matrix[list(range(self.config['num_items']))].sort_index())\n","        popularity_vector = np.sum(interaction_matrix, axis=0)\n","        popularity_vector = (popularity_vector / max(popularity_vector)) ** 0.5\n","        return popularity_vector\n","\n","    def create_neighborhood(self, include_test=False):\n","        \"\"\"Determine item neighbors\"\"\"\n","        if not include_test:\n","            print('Determining item neighborhoods...')\n","            interaction_matrix = pd.crosstab(self.train_ratings.userId, self.train_ratings.itemId)\n","            missing_columns = list(set(range(self.config['num_items'])) - set(list(interaction_matrix)))\n","            missing_rows = list(set(range(self.config['num_users'])) - set(interaction_matrix.index))\n","            for missing_column in missing_columns:\n","                interaction_matrix[missing_column] = [0] * len(interaction_matrix)\n","            for missing_row in missing_rows:\n","                interaction_matrix.loc[missing_row] = [0] * self.config['num_items']\n","            interaction_matrix = np.array(interaction_matrix[list(range(self.config['num_items']))].sort_index())\n","        elif not self.split_val:\n","            print('Determining test item neighborhoods...')\n","            interaction_matrix = np.array(pd.crosstab(self.preprocess_ratings.userId, self.preprocess_ratings.itemId)[\n","                                              list(range(self.config['num_items']))].sort_index())\n","        else:\n","            print('Determining val item neighborhoods...')\n","            interaction_matrix = pd.crosstab(self.train_ratings.userId.append(self.val_ratings.userId),\n","                                             self.train_ratings.itemId.append(self.val_ratings.itemId))\n","            missing_columns = list(set(range(self.config['num_items'])) - set(list(interaction_matrix)))\n","            missing_rows = list(set(range(self.config['num_users'])) - set(interaction_matrix.index))\n","            for missing_column in missing_columns:\n","                interaction_matrix[missing_column] = [0] * len(interaction_matrix)\n","            for missing_row in missing_rows:\n","                interaction_matrix.loc[missing_row] = [0] * self.config['num_items']\n","            interaction_matrix = np.array(interaction_matrix[list(range(self.config['num_items']))].sort_index())\n","        item_similarity_matrix = cosine_similarity(interaction_matrix.T)\n","        np.fill_diagonal(item_similarity_matrix, 0)\n","        neighborhood = np.array([np.argpartition(row, - self.config['neighborhood'])[- self.config['neighborhood']:]\n","                        for row in item_similarity_matrix])\n","        return neighborhood, item_similarity_matrix"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YEVwv6ZDORyQ"},"source":["config = {'model': 'BPR',\n","          'dataset': 'ml-100k',\n","          'num_epoch': 50,\n","          'batch_size': 100,\n","          'num_users': len(dataset['userId'].unique()),\n","          'num_items': len(dataset['itemId'].unique()),\n","          'test_rate': 0.2,\n","          'loo_eval': True,\n","          'neighborhood': 20,\n","          }"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6wILWSOYQQ6v"},"source":["sample_generator = SampleGenerator(dataset, config, split_val=False)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"w2QnfchnQL5F","executionInfo":{"status":"ok","timestamp":1628350548375,"user_tz":-330,"elapsed":402,"user":{"displayName":"Sparsh Agarwal","photoUrl":"","userId":"13037694610922482904"}},"outputId":"63ee84d5-7dac-449d-e38a-a33caabdec20"},"source":["# DataLoader\n","test_data = sample_generator.test_data_loader(config['batch_size'])\n","test_data"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[<torch.utils.data.dataloader.DataLoader at 0x7f07d5faf310>,\n"," <torch.utils.data.dataloader.DataLoader at 0x7f07d5d566d0>]"]},"metadata":{"tags":[]},"execution_count":18}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DoJQUAvKSlf-","executionInfo":{"status":"ok","timestamp":1628351215912,"user_tz":-330,"elapsed":624,"user":{"displayName":"Sparsh Agarwal","photoUrl":"","userId":"13037694610922482904"}},"outputId":"148ad86a-0db1-4c43-8198-e207ee6b2d29"},"source":["len(test_data[0]), len(test_data[1])"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(10, 943)"]},"metadata":{"tags":[]},"execution_count":36}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nZ1Zwc3WQM5J","executionInfo":{"status":"ok","timestamp":1628350918228,"user_tz":-330,"elapsed":817,"user":{"displayName":"Sparsh Agarwal","photoUrl":"","userId":"13037694610922482904"}},"outputId":"63534421-7262-4f0d-d200-569e1cf4e977"},"source":["for (idx, batch) in enumerate(test_data[0]):\n","    print('idx: {}\\n{}\\n{}\\n{}\\n{}'.format(idx, '='*100,\n","                                           batch[0], '='*100,\n","                                           batch[1]))\n","    break"],"execution_count":null,"outputs":[{"output_type":"stream","text":["idx: 0\n","====================================================================================================\n","tensor([ 50,  51,  63,  84, 106, 109, 135, 138, 146,  35, 160, 163, 128, 103,\n","        199, 110, 141, 212,  41, 234, 137,  52, 174, 221,  78, 226, 257, 258,\n","        262, 265, 254, 205,  89, 268,  87, 140, 287, 161, 294,  96, 241, 302,\n","        290, 130,  39, 117, 188, 201,  49, 222, 281, 232,  10,  68, 102,  43,\n","        150,  56, 166, 312,  66, 115, 275,  24,  15, 247, 266, 118,  69,  92,\n","         59, 190, 291, 323, 203, 156, 162,  25, 246,  74, 326, 328, 314,  71,\n","        334, 194, 274, 182, 301, 111, 272,  54, 331,  26, 309, 305, 297, 315,\n","        225, 219])\n","====================================================================================================\n","tensor([  51,   52,   67,   23,  144,   47,  206,  211,  228,  233,  245,  255,\n","         297,  311,  376,  388,  393,  331,  130,  466,  491,  144,  534,  487,\n","         559,  502,  153,  594,    1,  217,  602,  189,  254,   77,  430,  722,\n","          25,  136,  498,  492,  582,  189,  187,  153,  751,  833,  842,  115,\n","         233,  403,  346,  514,  205,  437,  130,  885,  432,    9,  288,   46,\n","          95,   68,  206,  799,  925,  242,  261,  940,  573,  437,  723,  166,\n","         278,   60,  995,  241,   91,  510,  364,  474, 1017,  378,  357,  357,\n","         280,  881,  928,  146,  136,  869,  538, 1038,  101,  966,  305,  157,\n","          68,  244,  436,  173])\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"A3rda5MySNpp","executionInfo":{"status":"ok","timestamp":1628351035061,"user_tz":-330,"elapsed":439,"user":{"displayName":"Sparsh Agarwal","photoUrl":"","userId":"13037694610922482904"}},"outputId":"7beeb544-f70f-4a42-90a4-f6c18ef6c479"},"source":["for (idx, batch) in enumerate(test_data[1]):\n","    print('idx: {}\\n{}\\n{}\\n{}\\n{}'.format(idx, '='*100,\n","                                           batch[0], '='*100,\n","                                           batch[1]))\n","    break"],"execution_count":null,"outputs":[{"output_type":"stream","text":["idx: 0\n","====================================================================================================\n","tensor([50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50,\n","        50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50,\n","        50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50,\n","        50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50,\n","        50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50,\n","        50, 50, 50, 50, 50, 50, 50, 50, 50, 50])\n","====================================================================================================\n","tensor([1287,  215,  738,  387, 1430, 1028, 1272,  167,  951,  899,  456,  133,\n","         142, 1386, 1320,  524,  684,  816,  767, 1212,  680,  897, 1078, 1293,\n","        1248, 1355,  148,  398,  858,  724, 1098, 1533, 1036,  722, 1442,  251,\n","         933,  686,   17,  470,  633,  856, 1391, 1412,  294,  401,  579, 1467,\n","        1062, 1283,   96,  340, 1208,  645,  119, 1374,  234,  553, 1328, 1228,\n","         235,  377, 1469, 1333, 1163,  943, 1188,  516,  977,  898,  114,  296,\n","        1027, 1672,  749, 1049,  608, 1548,  781,  188, 1180,  911,  272, 1425,\n","        1449,  486,  892, 1014,  132, 1231, 1504,  753, 1109, 1086,  388,  129,\n","         426,  397,   38,  723])\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-NAOzdp1R3Wo","executionInfo":{"status":"ok","timestamp":1628351031863,"user_tz":-330,"elapsed":19196,"user":{"displayName":"Sparsh Agarwal","photoUrl":"","userId":"13037694610922482904"}},"outputId":"35317742-66ca-49aa-bc0c-94b9dbb33e87"},"source":["# Create explainability matrix\n","explainability_matrix = sample_generator.create_explainability_matrix()\n","explainability_matrix"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Creating explainability matrix...\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["array([[0.25, 0.15, 0.  , ..., 0.  , 0.  , 0.  ],\n","       [0.25, 0.35, 0.  , ..., 0.  , 0.  , 0.  ],\n","       [0.05, 0.05, 0.15, ..., 0.  , 0.  , 0.  ],\n","       ...,\n","       [0.2 , 0.25, 0.  , ..., 0.  , 0.  , 0.  ],\n","       [0.3 , 0.8 , 0.  , ..., 0.  , 0.  , 0.  ],\n","       [0.05, 0.1 , 0.  , ..., 0.  , 0.  , 0.  ]])"]},"metadata":{"tags":[]},"execution_count":27}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2anJY8ksS-1v","executionInfo":{"status":"ok","timestamp":1628351233098,"user_tz":-330,"elapsed":10,"user":{"displayName":"Sparsh Agarwal","photoUrl":"","userId":"13037694610922482904"}},"outputId":"c9e61c0f-ed98-437b-fadc-535c5001912f"},"source":["explainability_matrix.shape"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(943, 1682)"]},"metadata":{"tags":[]},"execution_count":37}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"m2nVqEyGSVRx","executionInfo":{"status":"ok","timestamp":1628351084700,"user_tz":-330,"elapsed":19129,"user":{"displayName":"Sparsh Agarwal","photoUrl":"","userId":"13037694610922482904"}},"outputId":"56f68bb4-feee-41e3-fac4-bb879d886964"},"source":["test_explainability_matrix = sample_generator.create_explainability_matrix(include_test=True)\n","test_explainability_matrix"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Creating test explainability matrix...\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["array([[0.25, 0.15, 0.05, ..., 0.  , 0.  , 0.  ],\n","       [0.25, 0.35, 0.  , ..., 0.  , 0.  , 0.  ],\n","       [0.05, 0.05, 0.15, ..., 0.  , 0.  , 0.  ],\n","       ...,\n","       [0.25, 0.25, 0.  , ..., 0.  , 0.  , 0.  ],\n","       [0.3 , 0.8 , 0.  , ..., 0.  , 0.  , 0.  ],\n","       [0.05, 0.1 , 0.  , ..., 0.  , 0.  , 0.  ]])"]},"metadata":{"tags":[]},"execution_count":29}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"06R9VESETBbe","executionInfo":{"status":"ok","timestamp":1628351243453,"user_tz":-330,"elapsed":705,"user":{"displayName":"Sparsh Agarwal","photoUrl":"","userId":"13037694610922482904"}},"outputId":"c1a816fd-c864-43e3-ddb3-7a38badb86ab"},"source":["test_explainability_matrix.shape"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(943, 1682)"]},"metadata":{"tags":[]},"execution_count":38}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"chUssxmOSYhG","executionInfo":{"status":"ok","timestamp":1628351089550,"user_tz":-330,"elapsed":1515,"user":{"displayName":"Sparsh Agarwal","photoUrl":"","userId":"13037694610922482904"}},"outputId":"a5e68d12-1b48-4f66-a812-3cb1b8264c01"},"source":["# Create popularity vector\n","popularity_vector = sample_generator.create_popularity_vector()\n","popularity_vector"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Creating popularity vector...\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["array([0.45108565, 0.71505549, 0.15036188, ..., 0.04170288, 0.04170288,\n","       0.04170288])"]},"metadata":{"tags":[]},"execution_count":30}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rj6p7lJXSfg5","executionInfo":{"status":"ok","timestamp":1628351106256,"user_tz":-330,"elapsed":439,"user":{"displayName":"Sparsh Agarwal","photoUrl":"","userId":"13037694610922482904"}},"outputId":"473d7ad3-da7f-48d3-8d5e-8e7f53140886"},"source":["popularity_vector.shape"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(1682,)"]},"metadata":{"tags":[]},"execution_count":32}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CPaS9dT_SZva","executionInfo":{"status":"ok","timestamp":1628351090787,"user_tz":-330,"elapsed":623,"user":{"displayName":"Sparsh Agarwal","photoUrl":"","userId":"13037694610922482904"}},"outputId":"b509244c-3eeb-4196-99e8-acd82c25c938"},"source":["test_popularity_vector = sample_generator.create_popularity_vector(include_test=True)\n","test_popularity_vector"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Creating test popularity vector...\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["array([0.44798003, 0.71374643, 0.14932668, ..., 0.04141577, 0.04141577,\n","       0.04141577])"]},"metadata":{"tags":[]},"execution_count":31}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5YYNhk5dTE1g","executionInfo":{"status":"ok","timestamp":1628351258841,"user_tz":-330,"elapsed":458,"user":{"displayName":"Sparsh Agarwal","photoUrl":"","userId":"13037694610922482904"}},"outputId":"0a8552f2-9bab-4d5c-8bf2-48633d3af823"},"source":["test_popularity_vector.shape"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(1682,)"]},"metadata":{"tags":[]},"execution_count":40}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NgVXDpovTH5R","executionInfo":{"status":"ok","timestamp":1628351287779,"user_tz":-330,"elapsed":1724,"user":{"displayName":"Sparsh Agarwal","photoUrl":"","userId":"13037694610922482904"}},"outputId":"2568f883-c0e1-484b-c643-62c0dec3128f"},"source":["#Create item neighborhood\n","neighborhood, item_similarity_matrix = sample_generator.create_neighborhood()\n","neighborhood"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Determining item neighborhoods...\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["array([[ 404,  807,  360, ...,  243,  491,  568],\n","       [ 117,    4,  960, ...,  157,  404,   43],\n","       [1173,  576,  937, ..., 1251, 1623, 1365],\n","       ...,\n","       [1477, 1333, 1678, ..., 1604, 1565, 1681],\n","       [1477, 1333, 1678, ..., 1604, 1565, 1681],\n","       [1571, 1333, 1477, ..., 1679, 1680, 1638]])"]},"metadata":{"tags":[]},"execution_count":41}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5HzgvEDATNy5","executionInfo":{"status":"ok","timestamp":1628351294170,"user_tz":-330,"elapsed":491,"user":{"displayName":"Sparsh Agarwal","photoUrl":"","userId":"13037694610922482904"}},"outputId":"27554fd3-718c-4b8f-905d-bf8a66079394"},"source":["neighborhood.shape"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(1682, 20)"]},"metadata":{"tags":[]},"execution_count":42}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lWyB8QFMTMCg","executionInfo":{"status":"ok","timestamp":1628351300512,"user_tz":-330,"elapsed":533,"user":{"displayName":"Sparsh Agarwal","photoUrl":"","userId":"13037694610922482904"}},"outputId":"c3be4322-9dd9-46aa-b77a-75cb48080f73"},"source":["item_similarity_matrix"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[0.        , 0.33429131, 0.02564103, ..., 0.09245003, 0.09245003,\n","        0.09245003],\n","       [0.33429131, 0.        , 0.08087693, ..., 0.05832118, 0.05832118,\n","        0.05832118],\n","       [0.02564103, 0.08087693, 0.        , ..., 0.        , 0.        ,\n","        0.        ],\n","       ...,\n","       [0.09245003, 0.05832118, 0.        , ..., 0.        , 1.        ,\n","        1.        ],\n","       [0.09245003, 0.05832118, 0.        , ..., 1.        , 0.        ,\n","        1.        ],\n","       [0.09245003, 0.05832118, 0.        , ..., 1.        , 1.        ,\n","        0.        ]])"]},"metadata":{"tags":[]},"execution_count":43}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jo-LWfiyTQKv","executionInfo":{"status":"ok","timestamp":1628351306229,"user_tz":-330,"elapsed":463,"user":{"displayName":"Sparsh Agarwal","photoUrl":"","userId":"13037694610922482904"}},"outputId":"97019f21-09c1-4b03-f555-dc4af0bbbc09"},"source":["item_similarity_matrix.shape"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(1682, 1682)"]},"metadata":{"tags":[]},"execution_count":44}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5sPgSIdNTIwh","executionInfo":{"status":"ok","timestamp":1628351314051,"user_tz":-330,"elapsed":1887,"user":{"displayName":"Sparsh Agarwal","photoUrl":"","userId":"13037694610922482904"}},"outputId":"d21ca472-c9b2-4ac2-f89c-37249059fa9f"},"source":["_, test_item_similarity_matrix = sample_generator.create_neighborhood(include_test=True)\n","test_item_similarity_matrix"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Determining test item neighborhoods...\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["array([[0.        , 0.34332768, 0.02564103, ..., 0.09245003, 0.09245003,\n","        0.09245003],\n","       [0.34332768, 0.        , 0.08046742, ..., 0.05802589, 0.05802589,\n","        0.05802589],\n","       [0.02564103, 0.08046742, 0.        , ..., 0.        , 0.        ,\n","        0.        ],\n","       ...,\n","       [0.09245003, 0.05802589, 0.        , ..., 0.        , 1.        ,\n","        1.        ],\n","       [0.09245003, 0.05802589, 0.        , ..., 1.        , 0.        ,\n","        1.        ],\n","       [0.09245003, 0.05802589, 0.        , ..., 1.        , 1.        ,\n","        0.        ]])"]},"metadata":{"tags":[]},"execution_count":45}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bv_rqyEATU7N","executionInfo":{"status":"ok","timestamp":1628351323335,"user_tz":-330,"elapsed":549,"user":{"displayName":"Sparsh Agarwal","photoUrl":"","userId":"13037694610922482904"}},"outputId":"b60ce541-4fc5-47bb-b2f7-8d2662259eca"},"source":["test_item_similarity_matrix.shape"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(1682, 1682)"]},"metadata":{"tags":[]},"execution_count":46}]}]}