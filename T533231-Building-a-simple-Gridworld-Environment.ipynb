{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"T533231 | Building a simple Gridworld Environment","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyO5jae4JeV6X+yGVk48jkiA"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"JgaLL-rEdzOR"},"source":["# Building a simple Gridworld Environment"]},{"cell_type":"code","metadata":{"id":"C2yeuMR4VLaG"},"source":["import copy\n","import sys\n","import numpy as np\n","from collections import defaultdict"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"AwcVbA9zVSX5"},"source":["# Grid cell state and color mapping\n","EMPTY = BLACK = 0\n","WALL = GRAY = 1\n","AGENT = BLUE = 2\n","BOMB = RED = 3\n","GOAL = GREEN = 4"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mNWdDHcXVsx3"},"source":["# RGB color value table\n","COLOR_MAP = {\n","    BLACK: [0.0, 0.0, 0.0],\n","    GRAY: [0.5, 0.5, 0.5],\n","    BLUE: [0.0, 0.0, 1.0],\n","    RED: [1.0, 0.0, 0.0],\n","    GREEN: [0.0, 1.0, 0.0],\n","}"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"AN7AOH5DVv3y"},"source":["# Action mapping\n","NOOP = 0\n","DOWN = 1\n","UP = 2\n","LEFT = 3\n","RIGHT = 4"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"yCiTDjM9VxV2"},"source":["class GridworldEnv(gym.Env):\n","    def __init__(self, max_steps=100):\n","        \"\"\"Initialize Gridworld\n","        Args:\n","            max_steps (int, optional): Max steps per episode. Defaults to 100.\n","        \"\"\"\n","        # Observations\n","        self.grid_layout = \"\"\"\n","        1 1 1 1 1 1 1 1\n","        1 2 0 0 0 0 0 1\n","        1 0 1 1 1 0 0 1\n","        1 0 1 0 1 0 0 1\n","        1 0 1 4 1 0 0 1\n","        1 0 3 0 0 0 0 1\n","        1 0 0 0 0 0 0 1\n","        1 1 1 1 1 1 1 1\n","        \"\"\"\n","        self.initial_grid_state = np.fromstring(self.grid_layout, dtype=int, sep=\" \")\n","        self.initial_grid_state = self.initial_grid_state.reshape(8, 8)\n","        self.grid_state = copy.deepcopy(self.initial_grid_state)\n","        self.observation_space = gym.spaces.Box(\n","            low=0, high=6, shape=self.grid_state.shape\n","        )\n","        self.img_shape = [256, 256, 3]\n","        self.metadata = {\"render.modes\": [\"human\"]}\n","        # Actions\n","        self.action_space = gym.spaces.Discrete(5)\n","        self.actions = [NOOP, UP, DOWN, LEFT, RIGHT]\n","        self.action_pos_dict = defaultdict(\n","            lambda: [0, 0],\n","            {\n","                NOOP: [0, 0],\n","                UP: [-1, 0],\n","                DOWN: [1, 0],\n","                LEFT: [0, -1],\n","                RIGHT: [0, 1],\n","            },\n","        )\n","        (self.agent_state, self.goal_state) = self.get_state()\n","        self.step_num = 0  # To keep track of number of steps\n","        self.max_steps = max_steps\n","        self.done = False\n","        self.info = {\"status\": \"Live\"}\n","        self.viewer = None\n","\n","    def step(self, action):\n","        \"\"\"Return next observation, reward, done , info\"\"\"\n","        action = int(action)\n","        reward = 0.0\n","\n","        next_state = (\n","            self.agent_state[0] + self.action_pos_dict[action][0],\n","            self.agent_state[1] + self.action_pos_dict[action][1],\n","        )\n","\n","        next_state_invalid = (\n","            next_state[0] < 0 or next_state[0] >= self.grid_state.shape[0]\n","        ) or (next_state[1] < 0 or next_state[1] >= self.grid_state.shape[1])\n","        if next_state_invalid:\n","            # Leave the agent state unchanged\n","            next_state = self.agent_state\n","            self.info[\"status\"] = \"Next state is invalid\"\n","\n","        next_agent_state = self.grid_state[next_state[0], next_state[1]]\n","\n","        # Calculate reward\n","        if next_agent_state == EMPTY:\n","            # Move agent from previous state to the next state on the grid\n","            self.info[\"status\"] = \"Agent moved to a new cell\"\n","            self.grid_state[next_state[0], next_state[1]] = AGENT\n","            self.grid_state[self.agent_state[0], self.agent_state[1]] = EMPTY\n","            self.agent_state = copy.deepcopy(next_state)\n","\n","        elif next_agent_state == WALL:\n","            self.info[\"status\"] = \"Agent bumped into a wall\"\n","            reward = -0.1\n","        # Terminal states\n","        elif next_agent_state == GOAL:\n","            self.info[\"status\"] = \"Agent reached the GOAL \"\n","            self.done = True\n","            reward = 1\n","        elif next_agent_state == BOMB:\n","            self.info[\"status\"] = \"Agent stepped on a BOMB\"\n","            self.done = True\n","            reward = -1\n","        # elif next_agent_state == AGENT:\n","        else:\n","            # NOOP or next state is invalid\n","            self.done = False\n","\n","        self.step_num += 1\n","\n","        # Check if max steps per episode has been reached\n","        if self.step_num >= self.max_steps:\n","            self.done = True\n","            self.info[\"status\"] = \"Max steps reached\"\n","\n","        if self.done:\n","            done = True\n","            terminal_state = copy.deepcopy(self.grid_state)\n","            terminal_info = copy.deepcopy(self.info)\n","            _ = self.reset()\n","            return (terminal_state, reward, done, terminal_info)\n","\n","        return self.grid_state, reward, self.done, self.info\n","\n","    def reset(self):\n","        self.grid_state = copy.deepcopy(self.initial_grid_state)\n","        (\n","            self.agent_state,\n","            self.agent_goal_state,\n","        ) = self.get_state()\n","        self.step_num = 0\n","        self.done = False\n","        self.info[\"status\"] = \"Live\"\n","        return self.grid_state\n","\n","    def get_state(self):\n","        start_state = np.where(self.grid_state == AGENT)\n","        goal_state = np.where(self.grid_state == GOAL)\n","\n","        start_or_goal_not_found = not (start_state[0] and goal_state[0])\n","        if start_or_goal_not_found:\n","            sys.exit(\n","                \"Start and/or Goal state not present in the Gridworld. \"\n","                \"Check the Grid layout\"\n","            )\n","        start_state = (start_state[0][0], start_state[1][0])\n","        goal_state = (goal_state[0][0], goal_state[1][0])\n","\n","        return start_state, goal_state\n","\n","    def gridarray_to_image(self, img_shape=None):\n","        if img_shape is None:\n","            img_shape = self.img_shape\n","        observation = np.random.randn(*img_shape) * 0.0\n","        scale_x = int(observation.shape[0] / self.grid_state.shape[0])\n","        scale_y = int(observation.shape[1] / self.grid_state.shape[1])\n","        for i in range(self.grid_state.shape[0]):\n","            for j in range(self.grid_state.shape[1]):\n","                for k in range(3):  # 3-channel RGB image\n","                    pixel_value = COLOR_MAP[self.grid_state[i, j]][k]\n","                    observation[\n","                        i * scale_x : (i + 1) * scale_x,\n","                        j * scale_y : (j + 1) * scale_y,\n","                        k,\n","                    ] = pixel_value\n","        return (255 * observation).astype(np.uint8)\n","\n","    def render(self, mode=\"rgb_array\", close=False):\n","        if close:\n","            if self.viewer is not None:\n","                self.viewer.close()\n","                self.viewer = None\n","            return\n","\n","        img = self.gridarray_to_image()\n","        if mode == \"rgb_array\":\n","            return img\n","        elif mode == \"human\":\n","            from gym.envs.classic_control import rendering\n","\n","            if self.viewer is None:\n","                self.viewer = rendering.SimpleImageViewer()\n","            self.viewer.imshow(img)\n","\n","    def close(self):\n","        self.render(close=True)\n","\n","    @staticmethod\n","    def get_action_meanings():\n","        return [\"NOOP\", \"DOWN\", \"UP\", \"LEFT\", \"RIGHT\"]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"d4tofDM9WPTM","executionInfo":{"status":"ok","timestamp":1638436549900,"user_tz":-330,"elapsed":824,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}},"outputId":"b912c877-ca63-4e50-9cc8-928e7cc3c8aa"},"source":["if __name__ == \"__main__\":\n","    env = GridworldEnv()\n","    obs = env.reset()\n","    done = False\n","    step_num = 1\n","    # Run one episode\n","    while not done:\n","        # Sample a random action from the action space\n","        action = env.action_space.sample()\n","        next_obs, reward, done, info = env.step(action)\n","        print(f\"step#:{step_num} reward:{reward} done:{done} info:{info}\")\n","        step_num += 1\n","        env.render()\n","    env.close()"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["step#:1 reward:-0.1 done:False info:{'status': 'Agent bumped into a wall'}\n","step#:2 reward:0.0 done:False info:{'status': 'Agent moved to a new cell'}\n","step#:3 reward:-0.1 done:False info:{'status': 'Agent bumped into a wall'}\n","step#:4 reward:-0.1 done:False info:{'status': 'Agent bumped into a wall'}\n","step#:5 reward:-0.1 done:False info:{'status': 'Agent bumped into a wall'}\n","step#:6 reward:0.0 done:False info:{'status': 'Agent bumped into a wall'}\n","step#:7 reward:-0.1 done:False info:{'status': 'Agent bumped into a wall'}\n","step#:8 reward:0.0 done:False info:{'status': 'Agent moved to a new cell'}\n","step#:9 reward:0.0 done:False info:{'status': 'Agent moved to a new cell'}\n","step#:10 reward:0.0 done:False info:{'status': 'Agent moved to a new cell'}\n","step#:11 reward:0.0 done:False info:{'status': 'Agent moved to a new cell'}\n","step#:12 reward:0.0 done:False info:{'status': 'Agent moved to a new cell'}\n","step#:13 reward:-0.1 done:False info:{'status': 'Agent bumped into a wall'}\n","step#:14 reward:-0.1 done:False info:{'status': 'Agent bumped into a wall'}\n","step#:15 reward:0.0 done:False info:{'status': 'Agent moved to a new cell'}\n","step#:16 reward:0.0 done:False info:{'status': 'Agent moved to a new cell'}\n","step#:17 reward:-0.1 done:False info:{'status': 'Agent bumped into a wall'}\n","step#:18 reward:0.0 done:False info:{'status': 'Agent bumped into a wall'}\n","step#:19 reward:-0.1 done:False info:{'status': 'Agent bumped into a wall'}\n","step#:20 reward:-0.1 done:False info:{'status': 'Agent bumped into a wall'}\n","step#:21 reward:-0.1 done:False info:{'status': 'Agent bumped into a wall'}\n","step#:22 reward:-0.1 done:False info:{'status': 'Agent bumped into a wall'}\n","step#:23 reward:0.0 done:False info:{'status': 'Agent moved to a new cell'}\n","step#:24 reward:0.0 done:False info:{'status': 'Agent moved to a new cell'}\n","step#:25 reward:0.0 done:False info:{'status': 'Agent moved to a new cell'}\n","step#:26 reward:0.0 done:False info:{'status': 'Agent moved to a new cell'}\n","step#:27 reward:0.0 done:False info:{'status': 'Agent moved to a new cell'}\n","step#:28 reward:0.0 done:False info:{'status': 'Agent moved to a new cell'}\n","step#:29 reward:-0.1 done:False info:{'status': 'Agent bumped into a wall'}\n","step#:30 reward:0.0 done:False info:{'status': 'Agent moved to a new cell'}\n","step#:31 reward:0.0 done:False info:{'status': 'Agent moved to a new cell'}\n","step#:32 reward:0.0 done:False info:{'status': 'Agent moved to a new cell'}\n","step#:33 reward:-0.1 done:False info:{'status': 'Agent bumped into a wall'}\n","step#:34 reward:0.0 done:False info:{'status': 'Agent moved to a new cell'}\n","step#:35 reward:-0.1 done:False info:{'status': 'Agent bumped into a wall'}\n","step#:36 reward:-1 done:True info:{'status': 'Agent stepped on a BOMB'}\n"]}]},{"cell_type":"markdown","metadata":{"id":"gBqT6gGfb140"},"source":["---"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1gwfn3wCb143","executionInfo":{"status":"ok","timestamp":1638436678248,"user_tz":-330,"elapsed":4059,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}},"outputId":"5de97eaf-4b69-4334-d0b1-9c1e14e89c6d"},"source":["!pip install -q watermark\n","%reload_ext watermark\n","%watermark -a \"Sparsh A.\" -m -iv -u -t -d"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Author: Sparsh A.\n","\n","Last updated: 2021-12-02 09:18:00\n","\n","Compiler    : GCC 7.5.0\n","OS          : Linux\n","Release     : 5.4.104+\n","Machine     : x86_64\n","Processor   : x86_64\n","CPU cores   : 2\n","Architecture: 64bit\n","\n","gym    : 0.17.3\n","sys    : 3.7.12 (default, Sep 10 2021, 00:21:48) \n","[GCC 7.5.0]\n","IPython: 5.5.0\n","numpy  : 1.19.5\n","\n"]}]},{"cell_type":"markdown","metadata":{"id":"wF2c-0j5b144"},"source":["---"]},{"cell_type":"markdown","metadata":{"id":"-PH0vMRxb145"},"source":["**END**"]}]}