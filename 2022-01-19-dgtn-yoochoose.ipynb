{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"2022-01-19-dgtn-yoochoose.ipynb","provenance":[{"file_id":"https://github.com/recohut/nbs/blob/main/raw/T050069%20%7C%20DGTN%20on%20Yoochoose%20in%20PyTorch.ipynb","timestamp":1644651208282},{"file_id":"1xBn4HDiXTzxOUsCgEh9WymDFl9Ldo7O6","timestamp":1638180737096}],"collapsed_sections":[],"mount_file_id":"1xBn4HDiXTzxOUsCgEh9WymDFl9Ldo7O6","authorship_tag":"ABX9TyNKCQ/UfK1hZD70oX7iTses"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"MLN_V2dJchPv"},"source":["# DGTN on Yoochoose in PyTorch"]},{"cell_type":"markdown","metadata":{"id":"g0MVtmA0cimy"},"source":["## Setup"]},{"cell_type":"code","metadata":{"id":"05rycybwtrgI"},"source":["import numpy as np\n","import pandas as pd\n","from datetime import datetime, timezone, timedelta\n","import os\n","import pickle\n","import time\n","import math\n","import collections\n","import logging\n","import argparse\n","from tqdm.notebook import tqdm"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ToOgW5Crhl1a"},"source":["import torch\n","from torch import Tensor\n","import torch.nn as nn\n","from torch.nn import Module, Parameter\n","from torch.nn import Parameter as Param\n","import torch.nn.functional as F\n","\n","from torch.utils.tensorboard import SummaryWriter\n","\n","from torch_geometric.nn.inits import uniform\n","from torch_geometric.nn.conv import MessagePassing\n","from torch_geometric.utils import remove_self_loops, add_self_loops, softmax\n","from torch_geometric.data import InMemoryDataset, Data, Dataset, DataLoader\n","from torch_geometric.nn import GATConv, SGConv, GCNConv, GatedGraphConv"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"B7ZtDP_ObNIM"},"source":["## Preprocessing"]},{"cell_type":"code","metadata":{"id":"TfaZ1Uw7twYw"},"source":["raw_path = 'yoochoose-clicks'\n","save_path = 'processed'"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ZKNqm1XWbPZ1"},"source":["### Unaugmented"]},{"cell_type":"code","metadata":{"id":"NSVO6oQHt_FD"},"source":["def load_data(file):\n","    print(\"Start load_data\")\n","    # load csv\n","    data = pd.read_csv(file+'.dat', sep=',', header=0, usecols=[0, 1, 2],\n","                       dtype={0: np.int32, 1: str, 2: np.int64})\n","    # specify header names\n","    data.columns = ['SessionId', 'TimeStr', 'ItemId']\n","\n","    # convert time string to timestamp and remove the original column\n","    data['Time'] = data.TimeStr.apply(lambda x: datetime.strptime(x, '%Y-%m-%dT%H:%M:%S.%fZ').timestamp()) #This is not UTC. It does not really matter.\n","    del(data['TimeStr'])\n","\n","    # output\n","    data_start = datetime.fromtimestamp(data.Time.min(), timezone.utc)\n","    data_end = datetime.fromtimestamp(data.Time.max(), timezone.utc)\n","\n","    print('Loaded data set\\n\\tEvents: {}\\n\\tSessions: {}\\n\\tItems: {}\\n\\tSpan: {} / {}\\n\\n'.\n","          format(len(data), data.SessionId.nunique(), data.ItemId.nunique(),\n","                 data_start.date().isoformat(), data_end.date().isoformat()))\n","    return data\n","\n","\n","def filter_data(data, min_item_support=5, min_session_length=2):\n","    print(\"Start filter_data\")\n","    # y?\n","    session_lengths = data.groupby('SessionId').size()\n","    data = data[np.in1d(data.SessionId, session_lengths[session_lengths > 1].index)]\n","\n","    # filter item support\n","    item_supports = data.groupby('ItemId').size()\n","    data = data[np.in1d(data.ItemId, item_supports[item_supports >= min_item_support].index)]\n","\n","    # filter session length\n","    session_lengths = data.groupby('SessionId').size()\n","    data = data[np.in1d(data.SessionId, session_lengths[session_lengths >= min_session_length].index)]\n","\n","    # output\n","    data_start = datetime.fromtimestamp(data.Time.min(), timezone.utc)\n","    data_end = datetime.fromtimestamp(data.Time.max(), timezone.utc)\n","\n","    print('Filtered data set\\n\\tEvents: {}\\n\\tSessions: {}\\n\\tItems: {}\\n\\tSpan: {} / {}\\n\\n'.\n","          format(len(data), data.SessionId.nunique(), data.ItemId.nunique(),\n","                 data_start.date().isoformat(), data_end.date().isoformat()))\n","    return data\n","\n","\n","def split_train_test(data):\n","    print(\"Start split_train_test\")\n","    tmax = data.Time.max()\n","    session_max_times = data.groupby('SessionId').Time.max()\n","    session_train = session_max_times[session_max_times < tmax-86400].index\n","    session_test = session_max_times[session_max_times >= tmax-86400].index\n","    train = data[np.in1d(data.SessionId, session_train)]\n","    test = data[np.in1d(data.SessionId, session_test)]\n","    test = test[np.in1d(test.ItemId, train.ItemId)]\n","    tslength = test.groupby('SessionId').size()\n","    test = test[np.in1d(test.SessionId, tslength[tslength >= 2].index)]\n","    print('Full train set\\n\\tEvents: {}\\n\\tSessions: {}\\n\\tItems: {}'.format(len(train), train.SessionId.nunique(), train.ItemId.nunique()))\n","    print('Test set\\n\\tEvents: {}\\n\\tSessions: {}\\n\\tItems: {}'.format(len(test), test.SessionId.nunique(), test.ItemId.nunique()))\n","\n","    return train, test\n","\n","\n","def split_train(train, percentage=4):\n","    print(\"Start split_train\")\n","    train.sort_values(['SessionId', 'Time'], inplace=True)\n","    length = int(len(train) / percentage)\n","    train = train[-length:]\n","    for i in range(len(train)):\n","        if list(train['SessionId'].values)[i] != list(train['SessionId'].values)[i+1]:\n","            break\n","    train = train[i+1:]\n","    print('train 1/{} set\\n\\tEvents: {}\\n\\tSessions: {}\\n\\tItems: {}'.format(percentage, len(train), train.SessionId.nunique(), train.ItemId.nunique()))\n","    return train\n","\n","\n","def get_dict(data):\n","    print(\"Start get_dict\")\n","    item2idx = {}\n","    pop_scores = data.groupby('ItemId').size().sort_values(ascending=False)\n","    pop_scores = pop_scores / pop_scores[:1].values[0]\n","    items = pop_scores.index\n","    for idx, item in enumerate(items):\n","        item2idx[item] = idx+1\n","\n","    return item2idx\n","\n","def process_seqs(seqs, shift):\n","    start = time.time()\n","    labs = []\n","    index = shift\n","    for count, seq in enumerate(seqs):\n","        index += (len(seq) - 1)\n","        labs += [index]\n","        end = time.time()\n","        print(\"\\rprocess_seqs: [%d/%d], %.2f, usetime: %fs, \" % (count, len(seqs), count/len(seqs) * 100, end - start),\n","              end='', flush=True)\n","    print(\"\\n\")\n","    return seqs, labs\n","\n","\n","def get_sequence(data, item2idx, shift=-1):\n","    start = time.time()\n","    sess_ids = data.drop_duplicates('SessionId', 'first')\n","    sess_ids.sort_values(['Time'], inplace=True)\n","    sess_ids = sess_ids['SessionId'].unique()\n","    seqs = []\n","    for count, sess_id in enumerate(sess_ids):\n","        seq = data[data['SessionId'].isin([sess_id])]['ItemId'].values\n","        outseq = []\n","        for i in seq:\n","            if i in item2idx:\n","                outseq += [item2idx[i]]\n","        seqs += [outseq]\n","        end = time.time()\n","        print(\"\\rGet_sequence: [%d/%d], %.2f , usetime: %fs\" % (count, len(sess_ids), count/len(sess_ids) * 100, end - start),\n","              end='', flush=True)\n","\n","    print(\"\\n\")\n","    out_seqs, labs = process_seqs(seqs, shift)\n","    print(len(out_seqs), len(labs))\n","    return out_seqs, labs\n","\n","\n","def filter_test(train, test, percentage=4):\n","    print(\"Start filter_test\")\n","    test = test[np.in1d(test.ItemId, train.ItemId)]\n","    session_lengths = test.groupby('SessionId').size()\n","    test = test[np.in1d(test.SessionId, session_lengths[session_lengths > 1].index)]\n","    print('test after filter 1/{} set\\n\\tEvents: {}\\n\\tSessions: {}\\n\\tItems: {}'.format(percentage, len(test), test.SessionId.nunique(), test.ItemId.nunique()))\n","    return test\n","\n","\n","def preprocess(train, test, percentage=4, path=save_path):\n","    print(\"--------------\")\n","    print(\"Start preprocess yoochoose1_\"+str(percentage))\n","    train = split_train(train, percentage)\n","    item2idx = get_dict(train)\n","    test = filter_test(train, test, percentage)\n","    train_seqs, train_labs = get_sequence(train, item2idx)\n","    test_seqs, test_labs = get_sequence(test, item2idx, train_labs[-1])\n","    train = (train_seqs, train_labs)\n","    test = (test_seqs, test_labs)\n","    if not os.path.exists(path):\n","        os.makedirs(path)\n","    print(\"Start Save data\")\n","    pickle.dump(test, open(path+'/unaug_test.txt', 'wb'))\n","    pickle.dump(train, open(path+'/unaug_train.txt', 'wb'))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"tl9tQIl0uhF_"},"source":["data = load_data(raw_path)\n","data = filter_data(data)\n","train, test = split_train_test(data)\n","preprocess(train, test, 16)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vIrpIvJsgUIi","executionInfo":{"status":"ok","timestamp":1638270077148,"user_tz":-330,"elapsed":3423,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}},"outputId":"afdbf848-9351-48ad-a021-7d63a59563a1"},"source":["# to save time, we are loading already processed data\n","!mkdir -p processed\n","!cd processed && wget -q --show-progress https://github.com/sparsh-ai/stanza/raw/S521137/neigh_retrieval/unaugment_data/yoochoose1_64/unaug_train.txt\n","!cd processed && wget -q --show-progress https://github.com/sparsh-ai/stanza/raw/S521137/neigh_retrieval/unaugment_data/yoochoose1_64/unaug_test.txt"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["unaug_train.txt     100%[===================>]   2.61M  --.-KB/s    in 0.08s   \n","unaug_test.txt      100%[===================>] 363.86K  --.-KB/s    in 0.04s   \n"]}]},{"cell_type":"markdown","metadata":{"id":"UvhADxQ2bdUj"},"source":["### Augmented"]},{"cell_type":"code","metadata":{"id":"GLhfWYTvb4yB"},"source":["def load_data(file):\n","    print(\"Start load_data\")\n","    # load csv\n","    data = pd.read_csv(file+'.csv', sep=';', header=0, usecols=[0, 2, 3, 4], dtype={0: np.int32, 1: np.int64, 2: str, 3: str})\n","    # specify header names\n","    data.columns = ['SessionId', 'ItemId', 'Timeframe', 'Eventdate']\n","    # convert time string to timestamp and remove the original column\n","    data['Time'] = data.Eventdate.apply(lambda x: datetime.strptime(x, '%Y-%m-%d').timestamp())\n","    print(data['Time'].max())\n","    del(data['Eventdate'])\n","\n","    # output\n","    data_start = datetime.fromtimestamp(data.Time.min(), timezone.utc)\n","    data_end = datetime.fromtimestamp(data.Time.max(), timezone.utc)\n","\n","    print('Loaded data set\\n\\tEvents: {}\\n\\tSessions: {}\\n\\tItems: {}\\n\\tSpan: {} / {}\\n\\n'.\n","          format(len(data), data.SessionId.nunique(), data.ItemId.nunique(),\n","                 data_start.date().isoformat(), data_end.date().isoformat()))\n","    return data\n","\n","\n","def filter_data(data, min_item_support=5, min_session_length=2):\n","    print(\"Start filter_data\")\n","\n","    # y?\n","    session_lengths = data.groupby('SessionId').size()\n","    data = data[np.in1d(data.SessionId, session_lengths[session_lengths > 1].index)]\n","\n","    # filter item support\n","    item_supports = data.groupby('ItemId').size()\n","    data = data[np.in1d(data.ItemId, item_supports[item_supports >= min_item_support].index)]\n","\n","    # filter session length\n","    session_lengths = data.groupby('SessionId').size()\n","    data = data[np.in1d(data.SessionId, session_lengths[session_lengths >= min_session_length].index)]\n","    print(data['Time'].min())\n","    print(data['Time'].max())\n","    # output\n","    data_start = datetime.fromtimestamp(data.Time.astype(np.int64).min(), timezone.utc)\n","    data_end = datetime.fromtimestamp(data.Time.astype(np.int64).max(), timezone.utc)\n","\n","    print('Filtered data set\\n\\tEvents: {}\\n\\tSessions: {}\\n\\tItems: {}\\n\\tSpan: {} / {}\\n\\n'.\n","          format(len(data), data.SessionId.nunique(), data.ItemId.nunique(),\n","                 data_start.date().isoformat(), data_end.date().isoformat()))\n","    return data\n","\n","\n","def split_train_test(data):\n","    print(\"Start split_train_test\")\n","    tmax = data.Time.max()\n","    session_max_times = data.groupby('SessionId').Time.max()\n","    session_train = session_max_times[session_max_times < tmax-7*86400].index\n","    session_test = session_max_times[session_max_times >= tmax-7*86400].index\n","    train = data[np.in1d(data.SessionId, session_train)]\n","    test = data[np.in1d(data.SessionId, session_test)]\n","    test = test[np.in1d(test.ItemId, train.ItemId)]\n","    tslength = test.groupby('SessionId').size()\n","    test = test[np.in1d(test.SessionId, tslength[tslength >= 2].index)]\n","\n","    print('Full train set\\n\\tEvents: {}\\n\\tSessions: {}\\n\\tItems: {}'.format(len(train), train.SessionId.nunique(), train.ItemId.nunique()))\n","    print('Test set\\n\\tEvents: {}\\n\\tSessions: {}\\n\\tItems: {}'.format(len(test), test.SessionId.nunique(), test.ItemId.nunique()))\n","\n","    return train, test\n","\n","\n","def get_dict(data):\n","    print(\"Start get_dict\")\n","    item2idx = {}\n","    pop_scores = data.groupby('ItemId').size().sort_values(ascending=False)\n","    pop_scores = pop_scores / pop_scores[:1].values[0]\n","    items = pop_scores.index\n","    for idx, item in enumerate(items):\n","        item2idx[item] = idx+1\n","\n","    return item2idx\n","\n","\n","def process_seqs(seqs):\n","    start = time.time()\n","    out_seqs = []\n","    labs = []\n","    for count, seq in enumerate(seqs):\n","        for i in range(1, len(seq)):\n","            tar = seq[i]\n","            labs += [tar]\n","            out_seqs += [seq[:i]]\n","        end = time.time()\n","        print(\"\\rprocess_seqs: [%d/%d], %.2f, usetime: %fs, \" % (count, len(seqs), count/len(seqs) * 100, end - start),\n","              end='', flush=True)\n","    print(\"\\n\")\n","    return out_seqs, labs\n","\n","\n","def get_sequence(data, item2idx):\n","    start = time.time()\n","    sess_ids = data.drop_duplicates('SessionId', 'first')\n","    print(sess_ids)\n","    sess_ids.sort_values(['Time'], inplace=True)\n","    sess_ids = sess_ids['SessionId'].unique()\n","    seqs = []\n","    for count, sess_id in enumerate(sess_ids):\n","        seq = data[data['SessionId'].isin([sess_id])].sort_values(['Timeframe'])\n","        seq = seq['ItemId'].values\n","        outseq = []\n","        for i in seq:\n","            if i in item2idx:\n","                outseq += [item2idx[i]]\n","        seqs += [outseq]\n","        end = time.time()\n","        print(\"\\rGet_sequence: [%d/%d], %.2f , usetime: %fs\" % (count, len(sess_ids), count/len(sess_ids) * 100, end - start),\n","              end='', flush=True)\n","    print(\"\\n\")\n","    out_seqs, labs = process_seqs(seqs)\n","    print(len(out_seqs), len(labs))\n","    return out_seqs, labs\n","\n","\n","def preprocess(train, test, path=save_path):\n","    print(\"--------------\")\n","    print(\"Start preprocess cikm16\")\n","    item2idx = get_dict(train)\n","    train_seqs, train_labs = get_sequence(train, item2idx)\n","    test_seqs, test_labs = get_sequence(test, item2idx)\n","    train = (train_seqs, train_labs)\n","    test = (test_seqs, test_labs)\n","    if not os.path.exists(path):\n","        os.makedirs(path)\n","    print(\"Start Save data\")\n","\n","    pickle.dump(test, open(path+'/test.txt', 'wb'))\n","    pickle.dump(train, open(path+'/train.txt', 'wb'))\n","    print(\"finished\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9XXa7eK-br--"},"source":["data = load_data(raw_path)\n","data = filter_data(data)\n","train, test = split_train_test(data)\n","preprocess(train, test)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IfJq7TPIbdQr","executionInfo":{"status":"ok","timestamp":1638270091416,"user_tz":-330,"elapsed":3588,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}},"outputId":"64381a17-60c5-4983-a540-61c6d30f30ce"},"source":["# to save time, we are loading already processed data\n","!mkdir -p processed\n","!cd processed && wget -q --show-progress https://github.com/sparsh-ai/stanza/raw/S521137/datasets/yoochoose1_64/raw/train.txt\n","!cd processed && wget -q --show-progress https://github.com/sparsh-ai/stanza/raw/S521137/datasets/yoochoose1_64/raw/test.txt"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["train.txt           100%[===================>]   8.24M  --.-KB/s    in 0.1s    \n","test.txt            100%[===================>]   1.25M  --.-KB/s    in 0.07s   \n"]}]},{"cell_type":"markdown","metadata":{"id":"zB8v252abXBo"},"source":["## Neighborhood Retrieval"]},{"cell_type":"code","metadata":{"id":"CiE8LV3MaoHS"},"source":["class KNN:\n","    def __init__(self, k, all_sess, unaug_data, unaug_index, threshold=0.5, samples=1000):\n","        self.k = k\n","        self.all_sess = all_sess\n","        self.threshold = threshold\n","        self.samples = samples\n","        self.item_sess_map = self.get_item_sess_map(unaug_index, unaug_data)\n","        self.no_pro_data = unaug_data\n","        self.no_pro_index = unaug_index\n","\n","\n","    def get_item_sess_map(self, unaug_index, unaug_data):\n","        item_sess_map = {}\n","        for index, sess in zip(unaug_index, unaug_data):\n","            items = np.unique(sess[:-1])\n","            for item in items:\n","                if item not in item_sess_map.keys():\n","                    item_sess_map[item] = []\n","                item_sess_map[item].append(index)\n","        print(\"get_item_sess_map over\")\n","        return item_sess_map\n","\n","    def jaccard(self, first, second):\n","\n","        intersection = len(set(first).intersection(set(second)))\n","        union = len(set(first).union(set(second)))\n","        res = intersection / union\n","\n","        return res\n","\n","    def cosine(self, first, second):\n","\n","        li = len(set(first).intersection(set(second)))\n","        la = len(first)\n","        lb = len(second)\n","        result = li / (math.sqrt(la) * math.sqrt(lb))\n","\n","        return result\n","\n","    def vec(self, first, second, pos_map):\n","        a = set(first).intersection(set(second))\n","        sum = 0\n","        for i in a:\n","            sum += pos_map[i]\n","\n","        result = sum / len(pos_map)\n","\n","        return result\n","\n","    def find_sess(self, sess, item_sess_map):\n","        items = np.unique(sess)\n","        sess_index = []\n","        for item in items:\n","            sess_index += item_sess_map[item]\n","        return sess_index\n","\n","    def calc_similarity(self, target_session, all_data, sess_index):\n","        neighbors = []\n","        session_items = np.unique(target_session)\n","\n","        possible_sess_index = self.find_sess(session_items, self.item_sess_map)\n","        possible_sess_index = [p_index for p_index in possible_sess_index if p_index < sess_index]\n","        possible_sess_index = sorted(np.unique(possible_sess_index))[-self.samples:]\n","        possible_sess_index = sorted(np.unique(possible_sess_index))\n","\n","        pos_map = {}\n","        length = len(target_session)\n","\n","        count = 1\n","        for item in target_session:\n","            pos_map[item] = count / length\n","            count += 1\n","\n","        for index in possible_sess_index:\n","            session = all_data[index]\n","            session_items_test = np.unique(session)\n","            similarity = np.around(self.cosine(session_items_test, session_items), 4)\n","            if similarity >= self.threshold:\n","                neighbors.append([index, similarity])\n","\n","        return neighbors\n","\n","    def get_neigh_sess(self, index):\n","        all_sess_neigh = []\n","        start = time.time()\n","        all_sess = self.all_sess[index:]\n","        for sess in all_sess:\n","            possible_neighbors = self.calc_similarity(sess, self.all_sess, index)\n","            possible_neighbors = sorted(possible_neighbors, reverse=True, key=lambda x: x[1])\n","\n","            if len(possible_neighbors) > 0:\n","                possible_neighbors = list(np.asarray(possible_neighbors)[:, 0])\n","            if len(possible_neighbors) > self.k:\n","                all_sess_neigh.append(possible_neighbors[:self.k])\n","            elif len(possible_neighbors) > 0:\n","                all_sess_neigh.append(possible_neighbors)\n","            else:\n","                all_sess_neigh.append(0)\n","            index += 1\n","            end = time.time()\n","\n","            if index % (len(self.all_sess) // 100) == 0:\n","                print(\"\\rProcess_seqs: [%d/%d], %.2f, usetime: %fs, \" % (index, len(self.all_sess), index/len(self.all_sess) * 100, end - start),\n","              end='', flush=True)\n","\n","        return all_sess_neigh"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QpwTWoZsdR0z","outputId":"d95e36fd-e4f0-4021-b1ad-d6d3edcb2368"},"source":["org_test_data = pickle.load(open(save_path + '/test.txt', 'rb'))\n","org_train_data = pickle.load(open(save_path + '/train.txt', 'rb'))\n","unaug_test_data = pickle.load(open(save_path + '/unaug_test.txt', 'rb'))\n","unaug_train_data = pickle.load(open(save_path + '/unaug_train.txt', 'rb'))\n","\n","test_data = org_test_data[0]\n","train_data = org_train_data[0]\n","all_data = np.concatenate((train_data, test_data), axis=0)\n","\n","unaug_data = np.concatenate((unaug_train_data[0], unaug_test_data[0]), axis=0)\n","unaug_index = np.concatenate((unaug_train_data[1], unaug_test_data[1]), axis=0)\n","\n","del org_test_data, org_train_data\n","del test_data, train_data\n","del unaug_train_data, unaug_test_data\n","\n","k_num = [20,40,60,100,140, 160, 180, 200]\n","\n","for k in k_num:\n","    knn = KNN(k, all_data, unaug_data, unaug_index)\n","    all_sess_neigh = knn.get_neigh_sess(0)\n","    pickle.dump(all_sess_neigh, open(save_path+\"/neigh_data_\"+str(k)+\".txt\", \"wb\"))\n","    lens = 0\n","    for i in all_sess_neigh:\n","        if i != 0:\n","            lens += len(i)\n","    print(lens / len(all_sess_neigh))"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["<string>:6: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n"]},{"output_type":"stream","name":"stdout","text":["get_item_sess_map over\n","Process_seqs: [107600/430448], 25.00, usetime: 959.600880s, "]}]},{"cell_type":"code","metadata":{"id":"p7pFAf1Qhh_Y"},"source":["def print_txt(base_path, args, results, epochs, top_k, note=None, save_config=True):\n","    path = base_path + \"\\Best_result_top-\"+str(top_k)+\".txt\"\n","    outfile = open(path, 'w')\n","    if note is not None:\n","        outfile.write(\"Note:\\n\"+note+\"\\n\")\n","    if save_config:\n","        outfile.write(\"Configs:\\n\")\n","        for attr, value in sorted(args.__dict__.items()):\n","            outfile.write(\"{} = {}\\n\".format(attr, value))\n","\n","    outfile.write('\\nBest results:\\n')\n","    outfile.write(\"Mrr@{}:\\t{}\\tEpoch: {}\\n\".format(top_k, results[1], epochs[1]))\n","    outfile.write(\"Recall@{}:\\t{}\\tEpoch: {}\\n\".format(top_k, results[0], epochs[0]))\n","    outfile.close()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"G6TKOLdRhl7_"},"source":["## Model"]},{"cell_type":"code","metadata":{"id":"TaKUuC68jRIj"},"source":["class MultiSessionsGraph(InMemoryDataset):\n","    \"\"\"Every session is a graph.\"\"\"\n","    def __init__(self, root, phrase, knn_phrase, transform=None, pre_transform=None):\n","        \"\"\"\n","        Args:\n","            root: 'sample', 'yoochoose1_4', 'yoochoose1_64' or 'diginetica'\n","            phrase: 'train' or 'test'\n","        \"\"\"\n","        assert phrase in ['train', 'test']\n","        self.phrase = phrase\n","        self.knn_phrase = knn_phrase\n","        super(MultiSessionsGraph, self).__init__(root, transform, pre_transform)\n","        self.data, self.slices = torch.load(self.processed_paths[0])\n","\n","    @property\n","    def raw_file_names(self):\n","        return [self.phrase + '.txt']\n","\n","    @property\n","    def processed_file_names(self):\n","        return [self.phrase + '.pt']\n","\n","    def download(self):\n","        pass\n","\n","    def find_neighs(self, index, knn_data):\n","        sess_neighs = knn_data[index]\n","        if sess_neighs == 0:\n","            return []\n","        else:\n","            return list(np.asarray(sess_neighs).astype(np.int32))\n","\n","    def multi_process(self, train_data, knn_data, sess_index, y):\n","        # find neigh\n","        neigh_index = self.find_neighs(sess_index, knn_data)\n","        # neigh_index = []\n","        neigh_index.append(sess_index)\n","        temp_neighs = train_data[neigh_index]\n","        neighs = []\n","\n","        # append y\n","        for neigh, idx in zip(temp_neighs, neigh_index):\n","            if idx != sess_index:\n","                neigh.append(y[idx])\n","            neighs.append(neigh)\n","\n","        nodes = {}    # dict{15: 0, 16: 1, 18: 2, ...}\n","        all_senders = []\n","        all_receivers = []\n","        x = []\n","        i = 0\n","        for sess in neighs:\n","            senders = []\n","            for node in sess:\n","                if node not in nodes:\n","                    nodes[node] = i\n","                    x.append([node])\n","                    i += 1\n","                senders.append(nodes[node])\n","            receivers = senders[:]\n","\n","            if len(senders) != 1:\n","                del senders[-1]  # the last item is a receiver\n","                del receivers[0]  # the first item is a sender\n","            all_senders += senders\n","            all_receivers += receivers\n","\n","        sess = train_data[sess_index]\n","        sess_item_index = [nodes[item] for item in sess]\n","        # num_count = [count[i[0]] for i in x]\n","\n","        sess_masks = np.zeros(len(nodes))\n","        sess_masks[sess_item_index] = 1\n","\n","        pair = {}\n","        sur_senders = all_senders[:]\n","        sur_receivers = all_receivers[:]\n","        i = 0\n","        for sender, receiver in zip(sur_senders, sur_receivers):\n","            if str(sender) + '-' + str(receiver) in pair:\n","                pair[str(sender) + '-' + str(receiver)] += 1\n","                del all_senders[i]\n","                del all_receivers[i]\n","            else:\n","                pair[str(sender) + '-' + str(receiver)] = 1\n","                i += 1\n","\n","        node_num = len(x)\n","\n","        # num_count = torch.tensor(num_count, dtype=torch.float)\n","        edge_index = torch.tensor([all_senders, all_receivers], dtype=torch.long)\n","        x = torch.tensor(x, dtype=torch.long)\n","        node_num = torch.tensor([node_num], dtype=torch.long)\n","        sess_item_idx = torch.tensor(sess_item_index, dtype=torch.long)\n","        sess_masks = torch.tensor(sess_masks, dtype=torch.long)\n","\n","        return x, edge_index, node_num, sess_item_idx, sess_masks\n","\n","    def single_process(self, sequence, y):\n","        # sequence = [1, 2, 3, 2, 4]\n","        count = collections.Counter(sequence)\n","        i = 0\n","        nodes = {}    # dict{15: 0, 16: 1, 18: 2, ...}\n","        senders = []\n","        x = []\n","        for node in sequence:\n","            if node not in nodes:\n","                nodes[node] = i\n","                x.append([node])\n","                i += 1\n","            senders.append(nodes[node])\n","        receivers = senders[:]\n","        num_count = [count[i[0]] for i in x]\n","\n","        sess_item_index = [nodes[item] for item in sequence]\n","\n","        if len(senders) != 1:\n","            del senders[-1]  # the last item is a receiver\n","            del receivers[0]  # the first item is a sender\n","\n","        pair = {}\n","        sur_senders = senders[:]\n","        sur_receivers = receivers[:]\n","        i = 0\n","        for sender, receiver in zip(sur_senders, sur_receivers):\n","            if str(sender) + '-' + str(receiver) in pair:\n","                pair[str(sender) + '-' + str(receiver)] += 1\n","                del senders[i]\n","                del receivers[i]\n","            else:\n","                pair[str(sender) + '-' + str(receiver)] = 1\n","                i += 1\n","\n","        count = collections.Counter(senders)\n","        out_degree_inv = [1 / count[i] for i in senders]\n","\n","        count = collections.Counter(receivers)\n","        in_degree_inv = [1 / count[i] for i in receivers]\n","\n","        in_degree_inv = torch.tensor(in_degree_inv, dtype=torch.float)\n","        out_degree_inv = torch.tensor(out_degree_inv, dtype=torch.float)\n","\n","        edge_count = [pair[str(senders[i]) + '-' + str(receivers[i])] for i in range(len(senders))]\n","        edge_count = torch.tensor(edge_count, dtype=torch.float)\n","\n","        # senders, receivers = senders + receivers, receivers + senders\n","\n","        edge_index = torch.tensor([senders, receivers], dtype=torch.long)\n","        x = torch.tensor(x, dtype=torch.long)\n","        y = torch.tensor([y], dtype=torch.long)\n","        num_count = torch.tensor(num_count, dtype=torch.float)\n","        sequence = torch.tensor(sequence, dtype=torch.long)\n","        sequence_len = torch.tensor([len(sequence)], dtype=torch.long)\n","        sess_item_idx = torch.tensor(sess_item_index, dtype=torch.long)\n","\n","\n","        return x, y, num_count, edge_index, edge_count, sess_item_idx, sequence_len, in_degree_inv, out_degree_inv\n","\n","    def process(self):\n","        start = time.time()\n","        train_data = pickle.load(open(self.raw_dir + '/' + 'train.txt', 'rb'))\n","        test_data = pickle.load(open(self.raw_dir + '/' + 'test.txt', 'rb'))\n","        # knn_data = np.load(self.raw_dir + '/' + self.knn_phrase + '.npy')\n","        knn_data = pickle.load(open(self.raw_dir + '/' + self.knn_phrase + '.txt', \"rb\"))\n","        data_list = []\n","        if self.phrase == \"train\":\n","            sess_index = 0\n","            data = train_data\n","            total_data = np.asarray(train_data[0])\n","            total_label = np.asarray(train_data[1])\n","        else:\n","            sess_index = len(train_data[0])\n","            data = test_data\n","            total_data = np.concatenate((train_data[0], test_data[0]), axis=0)\n","            total_label = np.concatenate((train_data[1], test_data[1]), axis=0)\n","\n","        for sequence, y in zip(data[0], data[1]):\n","\n","            mt_x, mt_edge_index, mt_node_num, mt_sess_item_idx, sess_masks = \\\n","                self.multi_process(total_data, knn_data, sess_index, total_label)\n","\n","            x, y, num_count, edge_index, edge_count, sess_item_idx, sequence_len, in_degree_inv, out_degree_inv = \\\n","                self.single_process(sequence, y)\n","\n","            session_graph = Data(x=x, y=y, num_count=num_count, sess_item_idx=sess_item_idx,\n","                                    edge_index=edge_index, edge_count=edge_count, sequence_len=sequence_len,\n","                                    in_degree_inv=in_degree_inv, out_degree_inv=out_degree_inv,\n","                                    mt_x=mt_x, mt_edge_index=mt_edge_index, mt_node_num=mt_node_num,\n","                                    mt_sess_item_idx=mt_sess_item_idx, sess_masks=sess_masks)\n","\n","            data_list.append(session_graph)\n","            sess_index += 1\n","\n","            end = time.time()\n","            if sess_index % (len(data[0]) // 1000) == 0:\n","                print(\"\\rProcess_seqs: [%d/%d], %.2f, usetime: %fs, \" % (sess_index, len(data[0]), sess_index/len(data[0]) * 100, end - start),\n","              end='', flush=True)\n","        print('\\nStart collate')\n","        data, slices = self.collate(data_list)\n","        print('\\nStart save')\n","        torch.save((data, slices), self.processed_paths[0])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"W4yM19tPh6o7"},"source":["def uniform(size, tensor):\n","    bound = 1.0 / math.sqrt(size)\n","    if tensor is not None:\n","        tensor.data.uniform_(-bound, bound)\n","\n","\n","def kaiming_uniform(tensor, fan, a):\n","    if tensor is not None:\n","        bound = math.sqrt(6 / ((1 + a**2) * fan))\n","        tensor.data.uniform_(-bound, bound)\n","\n","\n","def glorot(tensor):\n","    if tensor is not None:\n","        stdv = math.sqrt(6.0 / (tensor.size(-2) + tensor.size(-1)))\n","        tensor.data.uniform_(-stdv, stdv)\n","\n","\n","def zeros(tensor):\n","    if tensor is not None:\n","        tensor.data.fill_(0)\n","\n","\n","def ones(tensor):\n","    if tensor is not None:\n","        tensor.data.fill_(1)\n","\n","\n","def normal(tensor, mean, std):\n","    if tensor is not None:\n","        tensor.data.normal_(mean, std)\n","\n","\n","def reset(nn):\n","    def _reset(item):\n","        if hasattr(item, 'reset_parameters'):\n","            item.reset_parameters()\n","\n","    if nn is not None:\n","        if hasattr(nn, 'children') and len(list(nn.children())) > 0:\n","            for item in nn.children():\n","                _reset(item)\n","        else:\n","            _reset(nn)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NHUMPFVlc1zV"},"source":["class InOutGATConv(MessagePassing):\n","    r\"\"\"The graph attentional operator from the `\"Graph Attention Networks\"\n","    <https://arxiv.org/abs/1710.10903>`_ paper\n","    .. math::\n","        \\mathbf{x}^{\\prime}_i = \\alpha_{i,i}\\mathbf{\\Theta}\\mathbf{x}_{i} +\n","        \\sum_{j \\in \\mathcal{N}(i)} \\alpha_{i,j}\\mathbf{\\Theta}\\mathbf{x}_{j},\n","    where the attention coefficients :math:`\\alpha_{i,j}` are computed as\n","    .. math::\n","        \\alpha_{i,j} =\n","        \\frac{\n","        \\exp\\left(\\mathrm{LeakyReLU}\\left(\\mathbf{a}^{\\top}\n","        [\\mathbf{\\Theta}\\mathbf{x}_i \\, \\Vert \\, \\mathbf{\\Theta}\\mathbf{x}_j]\n","        \\right)\\right)}\n","        {\\sum_{k \\in \\mathcal{N}(i) \\cup \\{ i \\}}\n","        \\exp\\left(\\mathrm{LeakyReLU}\\left(\\mathbf{a}^{\\top}\n","        [\\mathbf{\\Theta}\\mathbf{x}_i \\, \\Vert \\, \\mathbf{\\Theta}\\mathbf{x}_k]\n","        \\right)\\right)}.\n","    Args:\n","        in_channels (int): Size of each input sample.\n","        out_channels (int): Size of each output sample.\n","        heads (int, optional): Number of multi-head-attentions.\n","            (default: :obj:`1`)\n","        concat (bool, optional): If set to :obj:`False`, the multi-head\n","            attentions are averaged instead of concatenated.\n","            (default: :obj:`True`)\n","        negative_slope (float, optional): LeakyReLU angle of the negative\n","            slope. (default: :obj:`0.2`)\n","        dropout (float, optional): Dropout probability of the normalized\n","            attention coefficients which exposes each node to a stochastically\n","            sampled neighborhood during training. (default: :obj:`0`)\n","        bias (bool, optional): If set to :obj:`False`, the layer will not learn\n","            an additive bias. (default: :obj:`True`)\n","        **kwargs (optional): Additional arguments of\n","            :class:`torch_geometric.nn.conv.MessagePassing`.\n","    \"\"\"\n","\n","    def __init__(self,\n","                 in_channels,\n","                 out_channels,\n","                 heads=8,\n","                 concat=False,\n","                 negative_slope=0.2,\n","                 dropout=0,\n","                 bias=True,\n","                 middle_layer=False,\n","                 **kwargs):\n","        super(InOutGATConv, self).__init__(aggr='add', **kwargs)\n","\n","        self.in_channels = in_channels\n","        self.out_channels = out_channels\n","        self.heads = heads\n","        self.concat = concat\n","        self.middle_layer = middle_layer\n","        self.negative_slope = negative_slope\n","        self.dropout = dropout\n","\n","        self.weight1 = Parameter(\n","            torch.Tensor(2, in_channels, heads * out_channels))\n","        self.weight2 = Parameter(\n","            torch.Tensor(2, in_channels, heads * out_channels))\n","        self.att = Parameter(torch.Tensor(1, heads, 2 * out_channels))\n","\n","        if bias and concat:\n","            self.bias = Parameter(torch.Tensor(heads * out_channels))\n","        elif bias and not concat:\n","            self.bias = Parameter(torch.Tensor(out_channels))\n","        else:\n","            self.register_parameter('bias', None)\n","        if concat and not middle_layer:\n","            self.rnn = torch.nn.GRUCell(2 * out_channels * heads, in_channels * heads, bias=bias)\n","        elif middle_layer:\n","            self.rnn = torch.nn.GRUCell(2 * out_channels * heads, in_channels, bias=bias)\n","        else:\n","            self.rnn = torch.nn.GRUCell(2 * out_channels, out_channels, bias=bias)\n","\n","        self.reset_parameters()\n","\n","    def reset_parameters(self):\n","        glorot(self.weight1)\n","        glorot(self.weight2)\n","        glorot(self.att)\n","        zeros(self.bias)\n","\n","    def forward(self, x, edge_index, sess_masks):\n","        \"\"\"\"\"\"\n","        edge_index, _ = remove_self_loops(edge_index)\n","        edge_index, _ = add_self_loops(edge_index, num_nodes=x.size(0))\n","        sess_masks = sess_masks.view(sess_masks.shape[0], 1).float()\n","        xs = x * sess_masks\n","        xns = x * (1 - sess_masks)\n","\n","        # self.flow = 'source_to_target'\n","        # x1 = torch.mm(x, self.weight[0]).view(-1, self.heads, self.out_channels)\n","        # m1 = self.propagate(edge_index, x=x1, num_nodes=x.size(0))\n","        # self.flow = 'target_to_source'\n","        # x2 = torch.mm(x, self.weight[1]).view(-1, self.heads, self.out_channels)\n","        # m2 = self.propagate(edge_index, x=x2, num_nodes=x.size(0))\n","\n","        self.flow = 'source_to_target'\n","        x1s = torch.mm(xs, self.weight1[0]).view(-1, self.heads, self.out_channels)\n","        print(x1s.shape())\n","        x1ns = torch.mm(xns, self.weight2[0]).view(-1, self.heads, self.out_channels)\n","        print(x1ns.shape())\n","        x1 = x1s + x1ns\n","        m1 = self.propagate(edge_index, x=x1, num_nodes=x.size(0))\n","        self.flow = 'target_to_source'\n","        x2s = torch.mm(xs, self.weight1[1]).view(-1, self.heads, self.out_channels)\n","        x2ns = torch.mm(xns, self.weight2[1]).view(-1, self.heads, self.out_channels)\n","        x2 = x2s + x2ns\n","        m2 = self.propagate(edge_index, x=x2, num_nodes=x.size(0))\n","\n","        if not self.middle_layer:\n","            if self.concat:\n","                x = x.repeat(1, self.heads)\n","            else:\n","                x = x.view(-1, self.heads, self.out_channels).mean(dim=1)\n","\n","        # x = self.rnn(torch.cat((m1, m2), dim=-1), x)\n","        x = m1 + m2\n","        # x = m1\n","        return x\n","\n","    def message(self, edge_index_i, x_i, x_j, num_nodes):\n","        # Compute attention coefficients.\n","        alpha = (torch.cat([x_i, x_j], dim=-1) * self.att).sum(dim=-1)\n","        alpha = F.leaky_relu(alpha, self.negative_slope)\n","        alpha = softmax(alpha, edge_index_i, num_nodes)\n","\n","        # Sample attention coefficients stochastically.\n","        alpha = F.dropout(alpha, p=self.dropout, training=self.training)\n","\n","        return x_j * alpha.view(-1, self.heads, 1)\n","\n","    def update(self, aggr_out):\n","        if self.concat is True:\n","            aggr_out = aggr_out.view(-1, self.heads * self.out_channels)\n","        else:\n","            aggr_out = aggr_out.mean(dim=1)\n","\n","        if self.bias is not None:\n","            aggr_out = aggr_out + self.bias\n","        return aggr_out\n","\n","    def __repr__(self):\n","        return '{}({}, {}, heads={})'.format(self.__class__.__name__,\n","                                             self.in_channels,\n","                                             self.out_channels, self.heads)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"utiXyL0Vc7Yo"},"source":["class InOutGATConv_intra(MessagePassing):\n","    r\"\"\"The graph attentional operator from the `\"Graph Attention Networks\"\n","    <https://arxiv.org/abs/1710.10903>`_ paper\n","    .. math::\n","        \\mathbf{x}^{\\prime}_i = \\alpha_{i,i}\\mathbf{\\Theta}\\mathbf{x}_{i} +\n","        \\sum_{j \\in \\mathcal{N}(i)} \\alpha_{i,j}\\mathbf{\\Theta}\\mathbf{x}_{j},\n","    where the attention coefficients :math:`\\alpha_{i,j}` are computed as\n","    .. math::\n","        \\alpha_{i,j} =\n","        \\frac{\n","        \\exp\\left(\\mathrm{LeakyReLU}\\left(\\mathbf{a}^{\\top}\n","        [\\mathbf{\\Theta}\\mathbf{x}_i \\, \\Vert \\, \\mathbf{\\Theta}\\mathbf{x}_j]\n","        \\right)\\right)}\n","        {\\sum_{k \\in \\mathcal{N}(i) \\cup \\{ i \\}}\n","        \\exp\\left(\\mathrm{LeakyReLU}\\left(\\mathbf{a}^{\\top}\n","        [\\mathbf{\\Theta}\\mathbf{x}_i \\, \\Vert \\, \\mathbf{\\Theta}\\mathbf{x}_k]\n","        \\right)\\right)}.\n","    Args:\n","        in_channels (int): Size of each input sample.\n","        out_channels (int): Size of each output sample.\n","        heads (int, optional): Number of multi-head-attentions.\n","            (default: :obj:`1`)\n","        concat (bool, optional): If set to :obj:`False`, the multi-head\n","            attentions are averaged instead of concatenated.\n","            (default: :obj:`True`)\n","        negative_slope (float, optional): LeakyReLU angle of the negative\n","            slope. (default: :obj:`0.2`)\n","        dropout (float, optional): Dropout probability of the normalized\n","            attention coefficients which exposes each node to a stochastically\n","            sampled neighborhood during training. (default: :obj:`0`)\n","        bias (bool, optional): If set to :obj:`False`, the layer will not learn\n","            an additive bias. (default: :obj:`True`)\n","        **kwargs (optional): Additional arguments of\n","            :class:`torch_geometric.nn.conv.MessagePassing`.\n","    \"\"\"\n","\n","    def __init__(self,\n","                 in_channels,\n","                 out_channels,\n","                 heads=8,\n","                 concat=True,\n","                 negative_slope=0.2,\n","                 dropout=0,\n","                 bias=True,\n","                 middle_layer=False,\n","                 **kwargs):\n","        super(InOutGATConv_intra, self).__init__(aggr='add', **kwargs)\n","\n","        self.in_channels = in_channels\n","        self.out_channels = out_channels\n","        self.heads = heads\n","        self.concat = concat\n","        self.middle_layer = middle_layer\n","        self.negative_slope = negative_slope\n","        self.dropout = dropout\n","\n","        self.weight = Parameter(\n","            torch.Tensor(2, in_channels, heads * out_channels))\n","        self.weight1 = Parameter(\n","            torch.Tensor(2, in_channels, heads * out_channels))\n","        self.weight2 = Parameter(\n","            torch.Tensor(2, in_channels, heads * out_channels))\n","        self.att = Parameter(torch.Tensor(1, heads, 2 * out_channels))\n","\n","        if bias and concat:\n","            self.bias = Parameter(torch.Tensor(heads * out_channels))\n","        elif bias and not concat:\n","            self.bias = Parameter(torch.Tensor(out_channels))\n","        else:\n","            self.register_parameter('bias', None)\n","        if concat and not middle_layer:\n","            self.rnn = torch.nn.GRUCell(2 * out_channels * heads, in_channels * heads, bias=bias)\n","        elif middle_layer:\n","            self.rnn = torch.nn.GRUCell(2 * out_channels * heads, in_channels, bias=bias)\n","        else:\n","            self.rnn = torch.nn.GRUCell(2 * out_channels, out_channels, bias=bias)\n","\n","        self.reset_parameters()\n","\n","    def reset_parameters(self):\n","        glorot(self.weight1)\n","        glorot(self.weight2)\n","        glorot(self.att)\n","        zeros(self.bias)\n","\n","    def forward(self, x, edge_index, sess_masks):\n","        \"\"\"\"\"\"\n","        edge_index, _ = remove_self_loops(edge_index)\n","        edge_index, _ = add_self_loops(edge_index, num_nodes=x.size(0))\n","        # sess_masks = sess_masks.view(sess_masks.shape[0], 1).float()\n","        # xs = x * sess_masks\n","        # xns = x * (1 - sess_masks)\n","\n","        self.flow = 'source_to_target'\n","        x1 = torch.mm(x, self.weight[0]).view(-1, self.heads, self.out_channels)\n","        m1 = self.propagate(edge_index, x=x1, num_nodes=x.size(0))\n","        self.flow = 'target_to_source'\n","        x2 = torch.mm(x, self.weight[1]).view(-1, self.heads, self.out_channels)\n","        m2 = self.propagate(edge_index, x=x2, num_nodes=x.size(0))\n","\n","        # self.flow = 'source_to_target'\n","        # x1s = torch.mm(xs, self.weight1[0]).view(-1, self.heads, self.out_channels)\n","        # x1ns = torch.mm(xns, self.weight2[0]).view(-1, self.heads, self.out_channels)\n","        # x1 = x1s + x1ns\n","        # m1 = self.propagate(edge_index, x=x1, num_nodes=x.size(0))\n","        # self.flow = 'target_to_source'\n","        # x2s = torch.mm(xs, self.weight1[1]).view(-1, self.heads, self.out_channels)\n","        # x2ns = torch.mm(xns, self.weight2[1]).view(-1, self.heads, self.out_channels)\n","        # x2 = x2s + x2ns\n","        # m2 = self.propagate(edge_index, x=x2, num_nodes=x.size(0))\n","\n","        if not self.middle_layer:\n","            if self.concat:\n","                x = x.repeat(1, self.heads)\n","            else:\n","                x = x.view(-1, self.heads, self.out_channels).mean(dim=1)\n","\n","        # x = self.rnn(torch.cat((m1, m2), dim=-1), x)\n","        x = m1 + m2\n","        return x\n","\n","    def message(self, edge_index_i, x_i, x_j, num_nodes):\n","        # Compute attention coefficients.\n","        alpha = (torch.cat([x_i, x_j], dim=-1) * self.att).sum(dim=-1)\n","        alpha = F.leaky_relu(alpha, self.negative_slope)\n","        alpha = softmax(alpha, edge_index_i, num_nodes)\n","\n","        # Sample attention coefficients stochastically.\n","        alpha = F.dropout(alpha, p=self.dropout, training=self.training)\n","\n","        return x_j * alpha.view(-1, self.heads, 1)\n","\n","    def update(self, aggr_out):\n","        if self.concat is True:\n","            aggr_out = aggr_out.view(-1, self.heads * self.out_channels)\n","        else:\n","            aggr_out = aggr_out.mean(dim=1)\n","\n","        if self.bias is not None:\n","            aggr_out = aggr_out + self.bias\n","        return aggr_out\n","\n","    def __repr__(self):\n","        return '{}({}, {}, heads={})'.format(self.__class__.__name__,\n","                                             self.in_channels,\n","                                             self.out_channels, self.heads)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hG8oOf2wh6mv"},"source":["class InOutGGNN(MessagePassing):\n","    r\"\"\"The gated graph convolution operator from the `\"Gated Graph Sequence\n","    Neural Networks\" <https://arxiv.org/abs/1511.05493>`_ paper\n","    .. math::\n","        \\mathbf{h}_i^{(0)} &= \\mathbf{x}_i \\, \\Vert \\, \\mathbf{0}\n","        \\mathbf{m}_i^{(l+1)} &= \\sum_{j \\in \\mathcal{N}(i)} \\mathbf{\\Theta}\n","        \\cdot \\mathbf{h}_j^{(l)}\n","        \\mathbf{h}_i^{(l+1)} &= \\textrm{GRU} (\\mathbf{m}_i^{(l+1)},\n","        \\mathbf{h}_i^{(l)})\n","    up to representation :math:`\\mathbf{h}_i^{(L)}`.\n","    The number of input channels of :math:`\\mathbf{x}_i` needs to be less or\n","    equal than :obj:`out_channels`.\n","    Args:\n","        out_channels (int): Size of each input sample.\n","        num_layers (int): The sequence length :math:`L`.\n","        aggr (string): The aggregation scheme to use\n","            (:obj:`\"add\"`, :obj:`\"mean\"`, :obj:`\"max\"`).\n","            (default: :obj:`\"add\"`)\n","        bias (bool, optional): If set to :obj:`False`, the layer will not learn\n","            an additive bias. (default: :obj:`True`)\n","    \"\"\"\n","\n","    def __init__(self, out_channels, num_layers, aggr='add', bias=True):\n","        super(InOutGGNN, self).__init__(aggr)\n","\n","        self.out_channels = out_channels\n","        self.num_layers = num_layers\n","\n","        self.weight = Param(Tensor(num_layers, 2, out_channels, out_channels))\n","        self.rnn = torch.nn.GRUCell(2 * out_channels, out_channels, bias=bias)\n","        self.bias_in = Param(Tensor(self.out_channels))\n","        self.bias_out = Param(Tensor(self.out_channels))\n","\n","        self.reset_parameters()\n","\n","    def reset_parameters(self):\n","        size = self.out_channels\n","        uniform(size, self.weight)\n","        self.rnn.reset_parameters()\n","\n","    def forward(self, x, edge_index, edge_weight=[None, None]):\n","        #print(edge_weight[0].size(), edge_weight[1].size)\n","\n","        \"\"\"\"\"\"\n","        h = x if x.dim() == 2 else x.unsqueeze(-1)\n","        if h.size(1) > self.out_channels:\n","            raise ValueError('The number of input channels is not allowed to '\n","                             'be larger than the number of output channels')\n","\n","        if h.size(1) < self.out_channels:\n","            zero = h.new_zeros(h.size(0), self.out_channels - h.size(1))\n","            h = torch.cat([h, zero], dim=1)\n","\n","        for i in range(self.num_layers):\n","            self.flow = 'source_to_target'\n","            h1 = torch.matmul(h, self.weight[i, 0])\n","            m1 = self.propagate(edge_index, x=h1, edge_weight=edge_weight[0], bias=self.bias_in)\n","            self.flow = 'target_to_source'\n","            h2 = torch.matmul(h, self.weight[i, 1])\n","            m2 = self.propagate(edge_index, x=h2, edge_weight=edge_weight[1], bias=self.bias_out)\n","            h = self.rnn(torch.cat((m1, m2), dim=-1), h)\n","\n","        return h\n","\n","    def message(self, x_j, edge_weight):\n","        if edge_weight is not None:\n","            return edge_weight.view(-1, 1) * x_j\n","        return x_j\n","\n","    def update(self, aggr_out, bias):\n","        if bias is not None:\n","            return aggr_out + bias\n","        else:\n","            return aggr_out\n","\n","    def __repr__(self):\n","        return '{}({}, num_layers={})'.format(\n","            self.__class__.__name__, self.out_channels, self.num_layers)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"M4EFTsBkh6iI"},"source":["class SRGNN(nn.Module):\n","    \"\"\"\n","    Args:\n","        hidden_size: the number of units in a hidden layer.\n","        n_node: the number of items in the whole item set for embedding layer.\n","    \"\"\"\n","    def __init__(self, hidden_size, n_node, dropout=0.5, negative_slope=0.2, heads=8, item_fusing=False):\n","        super(SRGNN, self).__init__()\n","        self.hidden_size, self.n_node = hidden_size, n_node\n","        self.item_fusing = item_fusing\n","        self.embedding = nn.Embedding(self.n_node, self.hidden_size)\n","        # self.gated = InOutGGNN(self.hidden_size, num_layers=1)\n","\n","        self.gcn = GCNConv(in_channels=hidden_size, out_channels=hidden_size)\n","        self.gcn2 = GCNConv(in_channels=hidden_size, out_channels=hidden_size)\n","\n","        self.gated = SGConv(in_channels=hidden_size, out_channels=hidden_size, K=2)\n","        # self.gated = InOutGATConv_intra(in_channels=hidden_size, out_channels=hidden_size, dropout=dropout,\n","        #                           negative_slope=negative_slope, heads=heads, concat=True)\n","        # self.gated2 = InOutGATConv(in_channels=hidden_size * heads, out_channels=hidden_size, dropout=dropout,\n","        #                            negative_slope=negative_slope, heads=heads, concat=True, middle_layer=True)\n","        # self.gated3 = InOutGATConv(in_channels=hidden_size * heads, out_channels=hidden_size, dropout=dropout,\n","        #                            negative_slope=negative_slope, heads=heads, concat=False)\n","\n","        self.W_1 = nn.Linear(self.hidden_size * 8, self.hidden_size)\n","        self.W_2 = nn.Linear(self.hidden_size * 8, self.hidden_size)\n","        self.q = nn.Linear(self.hidden_size, 1)\n","        self.W_3 = nn.Linear(16 * self.hidden_size, self.hidden_size)\n","\n","        self.loss_function = nn.CrossEntropyLoss()\n","        self.reset_parameters()\n","\n","    def reset_parameters(self):\n","        stdv = 1.0 / math.sqrt(self.hidden_size)\n","        for weight in self.parameters():\n","            weight.data.uniform_(-stdv, stdv)\n","\n","    def rebuilt_sess(self, session_embedding, batchs, sess_item_index, seq_lens):\n","        sections = torch.bincount(batchs)\n","        split_embs = torch.split(session_embedding, tuple(sections.cpu().numpy()))\n","        sess_item_index = torch.split(sess_item_index, tuple(seq_lens.cpu().numpy()))\n","\n","        rebuilt_sess = []\n","        for embs, index in zip(split_embs, sess_item_index):\n","            sess = tuple(embs[i].view(1, -1) for i in index)\n","            sess = torch.cat(sess, dim=0)\n","            rebuilt_sess.append(sess)\n","        return tuple(rebuilt_sess)\n","\n","\n","    def get_h_s(self, hidden, seq_len):\n","        # split whole x back into graphs G_i\n","        v_n = tuple(nodes[-1].view(1, -1) for nodes in hidden)\n","        v_n_repeat = tuple(nodes[-1].view(1, -1).repeat(nodes.shape[0], 1) for nodes in hidden)\n","        v_n_repeat = torch.cat(v_n_repeat, dim=0)\n","        hidden = torch.cat(hidden, dim=0)\n","\n","        # Eq(6)\n","        # print(\"v_n_repeat\", v_n_repeat.size())\n","        # print(\"hidden\", hidden.size())\n","        alpha = self.q(torch.sigmoid(self.W_1(v_n_repeat) + self.W_2(hidden)))    # |V|_i * 1\n","\n","        s_g_whole = alpha * hidden    # |V|_i * hidden_size\n","        s_g_split = torch.split(s_g_whole, tuple(seq_len.cpu().numpy()))    # split whole s_g into graphs G_i\n","        s_g = tuple(torch.sum(embeddings, dim=0).view(1, -1) for embeddings in s_g_split)\n","\n","        # Eq(7)\n","        # print(\"torch.cat((torch.cat(v_n, dim=0), torch.cat(s_g, dim=0)), dim=1)\", torch.cat((torch.cat(v_n, dim=0), torch.cat(s_g, dim=0)), dim=1).size())\n","        h_s = self.W_3(torch.cat((torch.cat(v_n, dim=0), torch.cat(s_g, dim=0)), dim=1))\n","        # h_s = torch.cat((torch.cat(v_n, dim=0), torch.cat(s_g, dim=0)), dim=1)\n","        return h_s\n","\n","    def forward(self, data, hidden):\n","        edge_index, batch, edge_count, in_degree_inv, out_degree_inv, num_count, sess_item_index, seq_len = \\\n","            data.edge_index, data.batch, data.edge_count, data.in_degree_inv, data.out_degree_inv,\\\n","            data.num_count, data.sess_item_idx, data.sequence_len\n","\n","        hidden = self.gated.forward(hidden, edge_index)\n","        # hidden = self.gcn.forward(hidden, edge_index)\n","        # hidden = self.gcn2.forward(hidden, edge_index)\n","        sess_embs = self.rebuilt_sess(hidden, batch, sess_item_index, seq_len)\n","        if self.item_fusing:\n","            return sess_embs\n","        else:\n","            return self.get_h_s(sess_embs, seq_len)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jO_DQcFgizkN"},"source":["class GroupGraph(Module):\n","    def __init__(self, hidden_size, dropout=0.5, negative_slope=0.2, heads=8, item_fusing=False):\n","        super(GroupGraph, self).__init__()\n","        self.hidden_size = hidden_size\n","        self.item_fusing = item_fusing\n","\n","        self.W_1 = nn.Linear(8 * self.hidden_size, self.hidden_size)\n","        self.W_2 = nn.Linear(8 * self.hidden_size, self.hidden_size)\n","        self.q = nn.Linear(self.hidden_size, 1)\n","        self.W_3 = nn.Linear(16 * self.hidden_size, self.hidden_size)\n","\n","        # self.gat = GATConv(in_channels=hidden_size, out_channels=hidden_size, dropout=dropout, negative_slope=negative_slope, heads=heads, concat=True)\n","        # self.gat2 = GATConv(in_channels=hidden_size*heads, out_channels=hidden_size*heads, dropout=dropout, negative_slope=negative_slope, heads=heads, concat=False)\n","        # self.gat3 = GATConv(in_channels=hidden_size*heads, out_channels=hidden_size, dropout=dropout, negative_slope=negative_slope, heads=heads, concat=True)\n","        # self.gat_out = GATConv(in_channels=hidden_size*heads, out_channels=hidden_size, dropout=dropout, negative_slope=negative_slope, heads=heads, concat=False)\n","        # self.gated = InOutGGNN(self.hidden_size, num_layers=2)\n","        self.gcn = GCNConv(in_channels=hidden_size, out_channels=hidden_size)\n","        self.gcn2 = GCNConv(in_channels=hidden_size, out_channels=hidden_size)\n","\n","        self.sgcn = SGConv(in_channels=hidden_size, out_channels=hidden_size, K=2)\n","        # self.gat = InOutGATConv(in_channels=hidden_size, out_channels=hidden_size, dropout=dropout,\n","        #                           negative_slope=negative_slope, heads=heads, concat=True)\n","        # self.gat2 = InOutGATConv(in_channels=hidden_size * heads, out_channels=hidden_size, dropout=dropout,\n","        #                            negative_slope=negative_slope, heads=heads, concat=False)\n","        #\n","\n","    def group_att_old(self, session_embedding, node_num, batch_h_s):  # hs: # batch_size x latent_size\n","        v_i = torch.split(session_embedding, tuple(node_num))    # split whole x back into graphs G_i\n","        h_s_repeat = tuple(h_s.view(1, -1).repeat(nodes.shape[0], 1) for h_s, nodes in zip(batch_h_s, v_i))    # repeat |V|_i times for the last node embedding\n","\n","        alpha = self.q(torch.sigmoid(self.W_1(torch.cat(h_s_repeat, dim=0)) + self.W_2(session_embedding)))    # |V|_i * 1\n","        s_g_whole = alpha * session_embedding    # |V|_i * hidden_size\n","        s_g_split = torch.split(s_g_whole, tuple(node_num.cpu().numpy()))    # split whole s_g into graphs G_i\n","        s_g = tuple(torch.sum(embeddings, dim=0).view(1, -1) for embeddings in s_g_split)\n","\n","        return torch.cat(s_g, dim=0)\n","\n","    def group_att(self, session_embedding, hidden, node_num, num_count):  # hs: # batch_size x latent_size\n","        v_i = torch.split(session_embedding, tuple(node_num))    # split whole x back into graphs G_i\n","        v_n = tuple(nodes[-1].view(1, -1) for nodes in hidden)\n","        v_n_repeat = tuple(sess_nodes[-1].view(1, -1).repeat(nodes.shape[0], 1) for sess_nodes, nodes in zip(hidden, v_i))    # repeat |V|_i times for the last node embedding\n","\n","        alpha = self.q(torch.sigmoid(self.W_1(torch.cat(v_n_repeat, dim=0)) + self.W_2(session_embedding)))    # |V|_i * 1\n","        s_g_whole = num_count.view(-1, 1) * alpha * session_embedding    # |V|_i * hidden_size\n","        s_g_split = torch.split(s_g_whole, tuple(node_num.cpu().numpy()))    # split whole s_g into graphs G_i\n","        s_g = tuple(torch.sum(embeddings, dim=0).view(1, -1) for embeddings in s_g_split)\n","\n","        h_s = self.W_3(torch.cat((torch.cat(v_n, dim=0), torch.cat(s_g, dim=0)), dim=1))\n","\n","        return h_s\n","\n","\n","    def rebuilt_sess(self, session_embedding, node_num, sess_item_index, seq_lens):\n","        split_embs = torch.split(session_embedding, tuple(node_num))\n","        sess_item_index = torch.split(sess_item_index, tuple(seq_lens.cpu().numpy()))\n","\n","        rebuilt_sess = []\n","        for embs, index in zip(split_embs, sess_item_index):\n","            sess = tuple(embs[i].view(1, -1) for i in index)\n","            sess = torch.cat(sess, dim=0)\n","            rebuilt_sess.append(sess)\n","        return tuple(rebuilt_sess)\n","\n","    def get_h_group(self, hidden, seq_len):\n","        # split whole x back into graphs G_i\n","        v_n = tuple(nodes[-1].view(1, -1) for nodes in hidden)\n","        v_n_repeat = tuple(nodes[-1].view(1, -1).repeat(nodes.shape[0], 1) for nodes in hidden)\n","        v_n_repeat = torch.cat(v_n_repeat, dim=0)\n","        hidden = torch.cat(hidden, dim=0)\n","\n","        # Eq(5)\n","        alpha = self.q(torch.sigmoid(self.W_1(v_n_repeat) + self.W_2(hidden)))    # |V|_i * 1\n","        s_g_whole = alpha * hidden    # |V|_i * hidden_size\n","        # s_g_whole = hidden\n","        s_g_split = torch.split(s_g_whole, tuple(seq_len.cpu().numpy()))    # split whole s_g into graphs G_i\n","        s_g = tuple(torch.sum(embeddings, dim=0).view(1, -1) for embeddings in s_g_split)\n","        # s_g = tuple(torch.mean(embeddings, dim=0).view(1, -1) for embeddings in s_g_split)\n","\n","        h_s = self.W_3(torch.cat((torch.cat(v_n, dim=0), torch.cat(s_g, dim=0)), dim=1))\n","        # h_s = torch.cat((torch.cat(v_n, dim=0), torch.cat(s_g, dim=0)), dim=1)\n","        return h_s\n","\n","    def h_mean(self, hidden, node_num):\n","        split_embs = torch.split(hidden, tuple(node_num))\n","        means = []\n","        for embs in split_embs:\n","            mean = torch.mean(embs, dim=0)\n","            means.append(mean)\n","\n","        means = torch.cat(tuple(means), dim=0).view(len(split_embs), -1)\n","\n","        return means\n","\n","    def forward(self, hidden, data):\n","        # edge_index, node_num, batch, sess_item_index, seq_lens, sess_masks = \\\n","        #     data.mt_edge_index, data.mt_node_num, data.batch, data.mt_sess_item_idx, data.sequence_len, data.sess_masks\n","        edge_index, node_num, batch, sess_item_index, seq_lens = \\\n","            data.mt_edge_index, data.mt_node_num, data.batch, data.mt_sess_item_idx, data.sequence_len\n","\n","\n","        # edge_count, in_degree_inv, out_degree_inv = data.mt_edge_count, data.mt_in_degree_inv, data.mt_out_degree_inv\n","        # hidden = self.gat.forward(hidden, edge_index, sess_masks)\n","        # hidden = self.gat2.forward(hidden, edge_index)\n","        # hidden = self.gat3.forward(hidden, edge_index)\n","\n","        # hidden = self.gat.forward(hidden, edge_index, sess_masks)\n","\n","        hidden - self.sgcn(hidden, edge_index)\n","        # hidden = self.gcn.forward(hidden, edge_index)\n","        # hidden = self.gcn2.forward(hidden, edge_index)\n","\n","        # hidden = self.gat.forward(hidden, edge_index)\n","        # hidden = self.gated.forward(hidden, edge_index, [edge_count * in_degree_inv, edge_count * out_degree_inv])\n","        # hidden = self.gated.forward(hidden, edge_index)\n","\n","        # hidden = self.gat1.forward(hidden, edge_index)\n","\n","        sess_hidden = self.rebuilt_sess(hidden, node_num, sess_item_index, seq_lens)\n","\n","        if self.item_fusing:\n","            return sess_hidden\n","        else:\n","            return self.get_h_group(sess_hidden, seq_lens)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2POOFBLKi4r0"},"source":["class Embedding2Score(nn.Module):\n","    def __init__(self, hidden_size, n_node, using_represent, item_fusing):\n","        super(Embedding2Score, self).__init__()\n","        self.hidden_size = hidden_size\n","        self.n_node = n_node\n","        self.using_represent = using_represent\n","        self.item_fusing = item_fusing\n","\n","\n","        self.W_1 = nn.Linear(self.hidden_size, self.hidden_size * 2)\n","        self.W_2 = nn.Linear(self.hidden_size, self.hidden_size)\n","        self.W_3 = nn.Linear(self.hidden_size, self.hidden_size)\n","\n","    def forward(self, h_s, h_group, final_s, item_embedding_table):\n","        emb = item_embedding_table.weight.transpose(1, 0)\n","        if self.item_fusing:\n","            z_i_hat = torch.mm(final_s, emb)\n","        else:\n","            gate = F.sigmoid(self.W_2(h_s) + self.W_3(h_group))\n","            sess_rep = h_s * gate + h_group * (1 - gate)\n","            if self.using_represent == 'comb':\n","                z_i_hat = torch.mm(sess_rep, emb)\n","            elif self.using_represent == 'h_s':\n","                z_i_hat = torch.mm(h_s, emb)\n","            elif self.using_represent == 'h_group':\n","                z_i_hat = torch.mm(h_group, emb)\n","            else:\n","                raise NotImplementedError\n","\n","        return z_i_hat,"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"nVxGPh8hc_fy"},"source":["class ItemFusing(nn.Module):\n","    def __init__(self, hidden_size):\n","        super(ItemFusing, self).__init__()\n","        self.hidden_size = hidden_size\n","        self.use_rnn = True\n","        self.Wf1 = nn.Linear(self.hidden_size, self.hidden_size)\n","        self.Wf2 = nn.Linear(self.hidden_size, self.hidden_size)\n","\n","        self.W_1 = nn.Linear(self.hidden_size, self.hidden_size)\n","        self.W_2 = nn.Linear(self.hidden_size, self.hidden_size)\n","        self.q = nn.Linear(self.hidden_size, 1)\n","        self.W_3 = nn.Linear(2 * self.hidden_size, self.hidden_size)\n","\n","        self.rnn = torch.nn.GRUCell(hidden_size, hidden_size, bias=True)\n","\n","    def forward(self, intra_item_emb, inter_item_emb, seq_len):\n","        final_emb = self.item_fusing(intra_item_emb, inter_item_emb)\n","        # final_emb = self.avg_fusing(intra_item_emb, inter_item_emb)\n","        final_s = self.get_final_s(final_emb, seq_len)\n","        return final_s\n","\n","    def item_fusing(self, local_emb, global_emb):\n","        local_emb = torch.cat(local_emb, dim=0)\n","        global_emb = torch.cat(global_emb, dim=0)\n","        if self.use_rnn:\n","            final_emb = self.rnn(local_emb, global_emb)\n","        else:\n","            gate = F.sigmoid(self.Wf1(local_emb) + self.Wf2(global_emb))\n","            final_emb = local_emb * gate + global_emb * (1 - gate)\n","\n","        return final_emb\n","\n","    def cnn_fusing(self, local_emb, global_emb):\n","        local_emb = torch.cat(local_emb, dim=0)\n","        global_emb = torch.cat(global_emb, dim=0)\n","        embedding = torch.stack([local_emb, global_emb], dim=2)\n","        embedding = embedding.permute(0, 2, 1)\n","        embedding = self.conv(embedding).permute(0, 2, 1)\n","        embedding = self.W_c(embedding).squeeze()\n","        return embedding\n","\n","    def max_fusing(self, local_emb, global_emb):\n","        local_emb = torch.cat(local_emb, dim=0)\n","        global_emb = torch.cat(global_emb, dim=0)\n","        embedding = torch.stack([local_emb, global_emb], dim=2)\n","        embedding = torch.max(embedding, dim=2)[0]\n","        return embedding\n","\n","    def avg_fusing(self, local_emb, global_emb):\n","        local_emb = torch.cat(local_emb, dim=0)\n","        global_emb = torch.cat(global_emb, dim=0)\n","        embedding = (local_emb + global_emb) / 2\n","        return embedding\n","\n","    def concat_fusing(self, local_emb, global_emb):\n","        local_emb = torch.cat(local_emb, dim=0)\n","        global_emb = torch.cat(global_emb, dim=0)\n","        embedding = torch.cat([local_emb, global_emb], dim=1)\n","        embedding = self.W_4(embedding)\n","        return embedding\n","    def get_final_s(self, hidden, seq_len):\n","        hidden = torch.split(hidden, tuple(seq_len.cpu().numpy()))\n","        v_n = tuple(nodes[-1].view(1, -1) for nodes in hidden)\n","        v_n_repeat = tuple(nodes[-1].view(1, -1).repeat(nodes.shape[0], 1) for nodes in hidden)\n","        v_n_repeat = torch.cat(v_n_repeat, dim=0)\n","        hidden = torch.cat(hidden, dim=0)\n","\n","        # Eq(6)\n","        alpha = self.q(torch.sigmoid(self.W_1(v_n_repeat) + self.W_2(hidden)))    # |V|_i * 1\n","        s_g_whole = alpha * hidden    # |V|_i * hidden_size\n","        s_g_split = torch.split(s_g_whole, tuple(seq_len.cpu().numpy()))    # split whole s_g into graphs G_i\n","        s_g = tuple(torch.sum(embeddings, dim=0).view(1, -1) for embeddings in s_g_split)\n","\n","        # Eq(7)\n","        h_s = self.W_3(torch.cat((torch.cat(v_n, dim=0), torch.cat(s_g, dim=0)), dim=1))\n","        # h_s = torch.cat((torch.cat(v_n, dim=0), torch.cat(s_g, dim=0)), dim=1)\n","        return h_s"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"TPzp1NJydA7T"},"source":["class NARM(nn.Module):\n","    def __init__(self, opt):\n","        super(NARM, self).__init__()\n","        self.hidden_size = opt.hidden_size\n","        self.gru = nn.GRU(self.hidden_size * 2, self.hidden_size, batch_first=True)\n","        self.linear_one = nn.Linear(self.hidden_size, self.hidden_size, bias=True)\n","        self.linear_two = nn.Linear(self.hidden_size, self.hidden_size, bias=True)\n","        self.linear_three = nn.Linear(self.hidden_size, 1, bias=False)\n","\n","    def sess_att(self, hidden, ht, mask):\n","        q1 = self.linear_one(ht).view(ht.shape[0], 1, ht.shape[1])  # batch_size x 1 x latent_size\n","        q2 = self.linear_two(hidden)  # batch_size x seq_length x latent_size\n","        alpha = self.linear_three(torch.sigmoid(q1 + q2))\n","        hs = torch.sum(alpha * hidden * mask.view(mask.shape[0], -1, 1).float(), 1)\n","        # hs = torch.sum(alpha * hidden, 1)\n","        return hs\n","\n","    def padding(self, intra_item_embs, inter_item_embs, seq_lens):\n","        inter_padded, intra_padded = [], []\n","        max_len = max(seq_lens).detach().cpu().numpy()\n","        for intra_item_emb, inter_item_emb, seq_len in zip(intra_item_embs, inter_item_embs, seq_lens):\n","            if intra_item_emb.size(0) < max_len:\n","                pad_vec = torch.zeros(max_len - intra_item_emb.size(0), self.hidden_size)\n","                pad_vec = pad_vec.to('cuda')\n","                intra_item_emb = torch.cat((intra_item_emb, pad_vec), dim=0)\n","                inter_item_emb = torch.cat((inter_item_emb, pad_vec), dim=0)\n","            inter_padded.append(inter_item_emb.unsqueeze(dim=0))\n","            intra_padded.append(intra_item_emb.unsqueeze(dim=0))\n","        inter_padded = torch.cat(tuple(inter_padded), dim=0)\n","        intra_padded = torch.cat(tuple(intra_padded), dim=0)\n","        item_embs = torch.cat((inter_padded, intra_padded), dim=-1)\n","        return item_embs\n","\n","    def get_h_s(self, padded, seq_lens, masks):\n","        outputs, _ = self.gru(padded)\n","        output_last = outputs[torch.arange(seq_lens.shape[0]).long(), seq_lens - 1]\n","        hs = self.sess_att(outputs, output_last, masks)\n","        return hs\n","\n","    def forward(self, intra_item_embs, inter_item_embs, seq_lens):\n","        max_len = max(seq_lens).detach().cpu().numpy()\n","        masks = [[1] * le + [0] * (max_len - le) for le in seq_lens.detach().cpu().numpy()]\n","        masks = torch.tensor(masks).to('cuda')\n","        item_embs = self.padding(intra_item_embs, inter_item_embs, seq_lens)\n","        return self.get_h_s(item_embs, seq_lens, masks)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"X4KOMqxBdCQ-"},"source":["class CNNFusing(nn.Module):\n","    def __init__(self, hidden_size, num_filters):\n","        super(CNNFusing, self).__init__()\n","        self.hidden_size = hidden_size\n","        self.num_filters = num_filters\n","\n","        self.Wf1 = nn.Linear(self.hidden_size, self.hidden_size)\n","        self.Wf2 = nn.Linear(self.hidden_size, self.hidden_size)\n","\n","        self.W_1 = nn.Linear(self.hidden_size, self.hidden_size)\n","        self.W_2 = nn.Linear(self.hidden_size, self.hidden_size)\n","        self.q = nn.Linear(self.hidden_size, 1)\n","        self.W_3 = nn.Linear(2 * self.hidden_size, self.hidden_size)\n","        self.W_4 = nn.Linear(self.hidden_size * 2, self.hidden_size, bias=False)\n","\n","        # self.conv = torch.nn.Conv2d(in_channels=self.hidden_size, out_channels=self.hidden_size, kernel_size=(1, 2))\n","        self.conv = torch.nn.Conv1d(in_channels=2, out_channels=self.num_filters, kernel_size=1)\n","        self.W_c = nn.Linear(self.num_filters, 1)\n","    # def forward(self, inter_item_emb, intra_item_emb, seq_len):\n","    #     final_emb = self.cnn_fusing(inter_item_emb, intra_item_emb)\n","    #     final_s = self.get_final_s(final_emb, seq_len)\n","    #     return final_s\n","    def forward(self, intra_item_emb, inter_item_emb, seq_len):\n","        # final_emb = self.cnn_fusing(intra_item_emb, inter_item_emb)\n","        # final_emb = self.concat_fusing(intra_item_emb, inter_item_emb)\n","        # final_emb = self.avg_fusing(intra_item_emb, inter_item_emb)\n","        final_emb = self.max_fusing(intra_item_emb, inter_item_emb)\n","        # final_emb = intra_item_emb\n","        final_s = self.get_final_s(final_emb, seq_len)\n","        return final_s\n","\n","    def cnn_fusing(self, local_emb, global_emb):\n","        local_emb = torch.cat(local_emb, dim=0)\n","        global_emb = torch.cat(global_emb, dim=0)\n","        embedding = torch.stack([local_emb, global_emb], dim=2)\n","        embedding = embedding.permute(0, 2, 1)\n","        embedding = self.conv(embedding).permute(0, 2, 1)\n","        embedding = self.W_c(embedding).squeeze()\n","        return embedding\n","\n","    def max_fusing(self, local_emb, global_emb):\n","        local_emb = torch.cat(local_emb, dim=0)\n","        global_emb = torch.cat(global_emb, dim=0)\n","        embedding = torch.stack([local_emb, global_emb], dim=2)\n","        embedding = torch.max(embedding, dim=2)[0]\n","        return embedding\n","\n","    def avg_fusing(self, local_emb, global_emb):\n","        local_emb = torch.cat(local_emb, dim=0)\n","        global_emb = torch.cat(global_emb, dim=0)\n","        embedding = (local_emb + global_emb) / 2\n","        return embedding\n","\n","    def concat_fusing(self, local_emb, global_emb):\n","        local_emb = torch.cat(local_emb, dim=0)\n","        global_emb = torch.cat(global_emb, dim=0)\n","        embedding = torch.cat([local_emb, global_emb], dim=1)\n","        embedding = self.W_4(embedding)\n","        return embedding\n","\n","    def get_final_s(self, hidden, seq_len):\n","        hidden = torch.split(hidden, tuple(seq_len.cpu().numpy()))\n","        v_n = tuple(nodes[-1].view(1, -1) for nodes in hidden)\n","        v_n_repeat = tuple(nodes[-1].view(1, -1).repeat(nodes.shape[0], 1) for nodes in hidden)\n","        v_n_repeat = torch.cat(v_n_repeat, dim=0)\n","        hidden = torch.cat(hidden, dim=0)\n","\n","        # Eq(6)\n","        alpha = self.q(torch.sigmoid(self.W_1(v_n_repeat) + self.W_2(hidden)))  # |V|_i * 1\n","        s_g_whole = alpha * hidden  # |V|_i * hidden_size\n","        s_g_split = torch.split(s_g_whole, tuple(seq_len.cpu().numpy()))  # split whole s_g into graphs G_i\n","        s_g = tuple(torch.sum(embeddings, dim=0).view(1, -1) for embeddings in s_g_split)\n","\n","        # Eq(7)\n","        h_s = self.W_3(torch.cat((torch.cat(v_n, dim=0), torch.cat(s_g, dim=0)), dim=1))\n","        # h_s = torch.cat((torch.cat(v_n, dim=0), torch.cat(s_g, dim=0)), dim=1)\n","        return h_s"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"M93ioShMdD87"},"source":["class GraphModel(nn.Module):\n","    def __init__(self, opt, n_node):\n","        super(GraphModel, self).__init__()\n","        self.hidden_size, self.n_node = opt.hidden_size, n_node\n","        self.embedding = nn.Embedding(self.n_node, self.hidden_size)\n","        self.dropout = opt.gat_dropout\n","        self.negative_slope = opt.negative_slope\n","        self.heads = opt.heads\n","        self.item_fusing = opt.item_fusing\n","        self.num_filters = opt.num_filters\n","\n","        self.srgnn = SRGNN(self.hidden_size, n_node=n_node, item_fusing=opt.item_fusing)\n","        self.group_graph = GroupGraph(self.hidden_size, dropout=self.dropout, negative_slope=self.negative_slope,\n","                                      heads=self.heads, item_fusing=opt.item_fusing)\n","        self.fuse_model = ItemFusing(self.hidden_size)\n","        self.narm = NARM(opt)\n","        self.cnn_fusing = CNNFusing(self.hidden_size, self.num_filters)\n","        self.e2s = Embedding2Score(self.hidden_size, n_node, opt.using_represent, opt.item_fusing)\n","\n","        self.loss_function = nn.CrossEntropyLoss()\n","        self.reset_parameters()\n","\n","    def reset_parameters(self):\n","        stdv = 1.0 / math.sqrt(self.hidden_size)\n","        for weight in self.parameters():\n","            weight.data.uniform_(-stdv, stdv)\n","\n","    def forward(self, data):\n","        if self.item_fusing:\n","            x = data.x - 1\n","            embedding = self.embedding(x)\n","            embedding = embedding.squeeze()\n","            intra_item_emb = self.srgnn(data, embedding)\n","            num_filters = self.num_filters\n","\n","            mt_x = data.mt_x - 1\n","\n","            embedding = self.embedding(mt_x)\n","            embedding = embedding.squeeze()\n","\n","            inter_item_emb = self.group_graph.forward(embedding, data)\n","\n","            # final_s = self.fuse_model.forward(intra_item_emb, inter_item_emb, data.sequence_len)\n","            # final_s = self.narm.forward(intra_item_emb, inter_item_emb, data.sequence_len)\n","            final_s = self.cnn_fusing.forward(intra_item_emb, inter_item_emb, data.sequence_len)\n","\n","            scores = self.e2s(h_s=None, h_group=None, final_s=final_s, item_embedding_table=self.embedding)\n","\n","        else:\n","            x = data.x - 1\n","            embedding = self.embedding(x)\n","            embedding = embedding.squeeze()\n","            h_s = self.srgnn(data, embedding)\n","\n","            mt_x = data.mt_x - 1\n","\n","            embedding = self.embedding(mt_x)\n","            embedding = embedding.squeeze()\n","\n","            h_group = self.group_graph.forward(embedding, data)\n","            scores = self.e2s(h_s=h_s, h_group=h_group, final_s=None, item_embedding_table=self.embedding)\n","\n","        return scores[0]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WrO1uT0dhlyt"},"source":["## Trainer"]},{"cell_type":"code","metadata":{"id":"UPqeYRD1h1ZM"},"source":["def forward(model, loader, device, writer, epoch, top_k=20, optimizer=None, train_flag=True):\n","    start = time.time()\n","    if train_flag:\n","        model.train()\n","    else:\n","        model.eval()\n","        hit10, mrr10 = [], []\n","        hit5, mrr5 = [], []\n","        hit20, mrr20 = [], []\n","\n","    mean_loss = 0.0\n","    updates_per_epoch = len(loader)\n","    test_dict = {}\n","    for i, batch in enumerate(loader):\n","        if train_flag:\n","            optimizer.zero_grad()\n","        scores = model(batch.to(device))\n","        targets = batch.y - 1\n","        loss = model.loss_function(scores, targets)\n","\n","        if train_flag:\n","            loss.backward()\n","            optimizer.step()\n","            writer.add_scalar('loss/train_batch_loss', loss.item(), epoch * updates_per_epoch + i)\n","        else:\n","            sub_scores = scores.topk(20)[1]    # batch * top_k\n","            for score, target in zip(sub_scores.detach().cpu().numpy(), targets.detach().cpu().numpy()):\n","                hit20.append(np.isin(target, score))\n","                if len(np.where(score == target)[0]) == 0:\n","                    mrr20.append(0)\n","                else:\n","                    mrr20.append(1 / (np.where(score == target)[0][0] + 1))\n","\n","            sub_scores = scores.topk(top_k)[1]    # batch * top_k\n","            for score, target in zip(sub_scores.detach().cpu().numpy(), targets.detach().cpu().numpy()):\n","                hit10.append(np.isin(target, score))\n","                if len(np.where(score == target)[0]) == 0:\n","                    mrr10.append(0)\n","                else:\n","                    mrr10.append(1 / (np.where(score == target)[0][0] + 1))\n","\n","            sub_scores = scores.topk(5)[1]    # batch * top_k\n","            for score, target in zip(sub_scores.detach().cpu().numpy(), targets.detach().cpu().numpy()):\n","                hit5.append(np.isin(target, score))\n","                if len(np.where(score == target)[0]) == 0:\n","                    mrr5.append(0)\n","                else:\n","                    mrr5.append(1 / (np.where(score == target)[0][0] + 1))\n","\n","\n","        mean_loss += loss / batch.num_graphs\n","        end = time.time()\n","        print(\"\\rProcess: [%d/%d]   %.2f   usetime: %fs\" % (i, updates_per_epoch, i/updates_per_epoch * 100, end - start),\n","              end='', flush=True)\n","    print('\\n')\n","\n","    if train_flag:\n","        writer.add_scalar('loss/train_loss', mean_loss.item(), epoch)\n","        print(\"Train_loss: \", mean_loss.item())\n","    else:\n","        writer.add_scalar('loss/test_loss', mean_loss.item(), epoch)\n","        hit20 = np.mean(hit20) * 100\n","        mrr20 = np.mean(mrr20) * 100\n","\n","        hit10 = np.mean(hit10) * 100\n","        mrr10 = np.mean(mrr10) * 100\n","\n","        hit5 = np.mean(hit5) * 100\n","        mrr5 = np.mean(mrr5) * 100\n","        # writer.add_scalar('index/hit', hit, epoch)\n","        # writer.add_scalar('index/mrr', mrr, epoch)\n","        print(\"Result:\")\n","        print(\"\\tMrr@\", 20, \": \", mrr20)\n","        print(\"\\tRecall@\", 20, \": \", hit20)\n","\n","        print(\"\\tMrr@\", top_k, \": \", mrr10)\n","        print(\"\\tRecall@\", top_k, \": \", hit10)\n","\n","        print(\"\\tMrr@\", 5, \": \", mrr5)\n","        print(\"\\tRecall@\", 5, \": \", hit5)\n","        # for seq_len in range(1, 31):\n","        #     sub_hit = test_dict[seq_len][0]\n","        #     sub_mrr = test_dict[seq_len][1]\n","        #     print(\"Len \", seq_len, \": Recall@\", top_k, \": \", np.mean(sub_hit) * 100, \"Mrr@\", top_k, \": \", np.mean(sub_mrr) * 100)\n","\n","        return mrr20, hit20, mrr10, hit10, mrr5, hit5\n","\n","\n","def case_study(model, loader, device, n_node):\n","    model.eval()\n","    for i, batch in enumerate(loader):\n","        sc, ss, sg, mg, alpha_s, alpha_g = model(batch.to(device))\n","        targets = batch.y - 1\n","        scs = sc.topk(n_node)[1].detach().cpu().numpy()\n","        sss = ss.topk(n_node)[1].detach().cpu().numpy()\n","        sgs = sg.topk(n_node)[1].detach().cpu().numpy()\n","        mgs = mg.detach().cpu().numpy()\n","        targets = targets.detach().cpu().numpy()\n","\n","        # batch * top_k\n","        for sc, ss, sg, ms, a_s, a_g, target in zip(scs, sss, sgs, mgs, alpha_s, alpha_g, targets):\n","            rc = np.where(sc == target)[0][0] + 1\n","            rs = np.where(ss == target)[0][0] + 1\n","            rg = np.where(sg == target)[0][0] + 1\n","            print(\"rank c:\", rc, \"rank s:\", rs, \"rank g:\", rg, \"gate:\", ms)\n","            print(\"att s:\", a_s, \"att g:\", a_g)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"OWHZhL0fh1W2"},"source":["## Main"]},{"cell_type":"code","metadata":{"id":"SSGHP_PEjlKP"},"source":["parser = argparse.ArgumentParser()\n","parser.add_argument('--dataset', default='yoochoose1_64', help='dataset name: diginetica/yoochoose1_64/sample')\n","parser.add_argument('--batch_size', type=int, default=128, help='input batch size')\n","parser.add_argument('--hidden_size', type=int, default=100, help='hidden state size')\n","parser.add_argument('--epoch', type=int, default=15, help='the number of epochs to train for')\n","parser.add_argument('--lr', type=float, default=0.001, help='learning rate')  # [0.001, 0.0005, 0.0001]\n","parser.add_argument('--lr_dc', type=float, default=0.5, help='learning rate decay rate')\n","parser.add_argument('--lr_dc_step', type=int, default=4, help='the number of steps after which the learning rate decay')\n","parser.add_argument('--l2', type=float, default=1e-5, help='l2 penalty')  # [0.001, 0.0005, 0.0001, 0.00005, 0.00001]\n","parser.add_argument('--top_k', type=int, default=20, help='top K indicator for evaluation')\n","parser.add_argument('--negative_slope', type=float, default=0.2, help='negative_slope')\n","parser.add_argument('--gat_dropout', type=float, default=0.6, help='dropout rate in gat')\n","parser.add_argument('--heads', type=int, default=8, help='gat heads number')\n","parser.add_argument('--num_filters', type=int, default=2, help='gat heads number')\n","parser.add_argument('--using_represent', type=str, default='comb', help='comb, h_s, h_group')\n","parser.add_argument('--predict', type=bool, default=False, help='gat heads number')\n","parser.add_argument('--item_fusing', type=bool, default=True, help='gat heads number')\n","parser.add_argument('--random_seed', type=int, default=24, help='input batch size')\n","parser.add_argument('--id', type=int, default=120, help='id')\n","opt = parser.parse_args(args={})"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Px0rbj69dOLk"},"source":["def main():\n","\n","    torch.manual_seed(opt.random_seed)\n","    torch.cuda.manual_seed(opt.random_seed)\n","    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","    # device = torch.device('cpu')\n","\n","    cur_dir = os.getcwd()\n","    train_dataset = MultiSessionsGraph(cur_dir + '/datasets/' + opt.dataset, phrase='train', knn_phrase='neigh_data_'+str(opt.id))\n","    train_loader = DataLoader(train_dataset, batch_size=opt.batch_size, shuffle=True)\n","    test_dataset = MultiSessionsGraph(cur_dir + '/datasets/' + opt.dataset, phrase='test', knn_phrase='neigh_data_'+str(opt.id))\n","    test_loader = DataLoader(test_dataset, batch_size=opt.batch_size, shuffle=False)\n","\n","    log_dir = cur_dir + '/log/' + str(opt.dataset) + '/' + time.strftime(\n","        \"%Y-%m-%d %H:%M:%S\", time.localtime())\n","    if not os.path.exists(log_dir):\n","        os.makedirs(log_dir)\n","    writer = SummaryWriter(log_dir)\n","\n","    if opt.dataset == 'cikm16':\n","        n_node = 43097\n","    elif opt.dataset == 'yoochoose1_64':\n","        n_node = 17400\n","    else:\n","        n_node = 309\n","\n","    model = GraphModel(opt, n_node=n_node).to(device)\n","\n","    multigraph_parameters = list(map(id, model.group_graph.parameters()))\n","    srgnn_parameters = (p for p in model.parameters() if id(p) not in multigraph_parameters)\n","    parameters = [{\"params\": model.group_graph.parameters(), \"lr\": 0.001}, {\"params\": srgnn_parameters}]\n","\n","    # best 0.1\n","    lambda1 = lambda epoch: 0.1 ** (epoch // 3)\n","    lambda2 = lambda epoch: 0.1 ** (epoch // 3)\n","\n","    optimizer = torch.optim.Adam(parameters, lr=opt.lr, weight_decay=opt.l2)\n","    #scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=opt.lr_dc_step, gamma=opt.lr_dc)\n","    scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=[lambda1, lambda2])\n","\n","    if not opt.predict:\n","        best_result20 = [0, 0]\n","        best_epoch20 = [0, 0]\n","\n","        best_result10 = [0, 0]\n","        best_epoch10 = [0, 0]\n","\n","        best_result5 = [0, 0]\n","        best_epoch5 = [0, 0]\n","        for epoch in range(opt.epoch):\n","            scheduler.step(epoch)\n","            print(\"Epoch \", epoch)\n","            forward(model, train_loader, device, writer, epoch, top_k=opt.top_k, optimizer=optimizer, train_flag=True)\n","            with torch.no_grad():\n","                mrr20, hit20, mrr10, hit10, mrr5, hit5 = forward(model, test_loader, device, writer, epoch, top_k=opt.top_k, train_flag=False)\n","\n","            if hit20 >= best_result20[0]:\n","                best_result20[0] = hit20\n","                best_epoch20[0] = epoch\n","                # torch.save(model.state_dict(), log_dir+'/best_recall_params.pkl')\n","            if mrr20 >= best_result20[1]:\n","                best_result20[1] = mrr20\n","                best_epoch20[1] = epoch\n","\n","            if hit10 >= best_result10[0]:\n","                best_result10[0] = hit10\n","                best_epoch10[0] = epoch\n","                # torch.save(model.state_dict(), log_dir+'/best_recall_params.pkl')\n","            if mrr10 >= best_result10[1]:\n","                best_result10[1] = mrr10\n","                best_epoch10[1] = epoch\n","                # torch.save(model.state_dict(), log_dir+'/best_mrr_params.pkl')\n","\n","            if hit5 >= best_result5[0]:\n","                best_result5[0] = hit5\n","                best_epoch5[0] = epoch\n","                # torch.save(model.state_dict(), log_dir+'/best_recall_params.pkl')\n","            if mrr5 >= best_result5[1]:\n","                best_result5[1] = mrr5\n","                best_epoch5[1] = epoch\n","\n","            print('Best Result:')\n","            print('\\tMrr@%d:\\t%.4f\\tEpoch:\\t%d' % (20, best_result20[1], best_epoch20[1]))\n","            print('\\tRecall@%d:\\t%.4f\\tEpoch:\\t%d\\n' % (20, best_result20[0], best_epoch20[0]))\n","            print('\\tMrr@%d:\\t%.4f\\tEpoch:\\t%d' % (opt.top_k, best_result10[1], best_epoch10[1]))\n","            print('\\tRecall@%d:\\t%.4f\\tEpoch:\\t%d\\n' % (opt.top_k, best_result10[0], best_epoch10[0]))\n","            print('\\tMrr@%d:\\t%.4f\\tEpoch:\\t%d' % (5, best_result5[1], best_epoch5[1]))\n","            print('\\tRecall@%d:\\t%.4f\\tEpoch:\\t%d' % (5, best_result5[0], best_epoch5[0]))\n","            print(\"-\"*20)\n","        # print_txt(log_dir, opt, best_result, best_epoch, opt.top_k, note, save_config=True)\n","    else:\n","        log_dir = 'log/cikm16/2019-08-19 14:27:33'\n","        model.load_state_dict(torch.load(log_dir+'/best_mrr_params.pkl'))\n","        mrr, hit = forward(model, test_loader, device, writer, 0, top_k=opt.top_k, train_flag=False)\n","        best_result = [hit, mrr]\n","        best_epoch = [0, 0]\n","        # print_txt(log_dir, opt, best_result, best_epoch, opt.top_k, save_config=False)\n","\n","if __name__ == '__main__':\n","    main()"],"execution_count":null,"outputs":[]}]}