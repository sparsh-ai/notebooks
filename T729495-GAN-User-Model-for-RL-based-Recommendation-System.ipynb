{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"T729495 | GAN User Model for RL-based Recommendation System","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyMqCXrmAyLGOuEZyB8qANzK"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"Vz-vq1q-qyxl"},"source":["# GAN User Model for RL-based Recommendation System"]},{"cell_type":"markdown","metadata":{"id":"Cv-RRs3Uq2Yo"},"source":["A model-based RL framework for recommendation systems, where a user behavior model and the associated reward function are learned in unified minmax framework, and then RL policies are learned using this model.\n","\n","Since reinforcement learning can take into account long-term reward, it holds the promise to improve users’ long-term engagement with an online platform. In the RL framework, a recommendation system aims to find a policy $π(s, I)$ to choose from a set $I$ of items in user state $s$, such that the cumulative expected reward is maximized,\n","\n","$$\\pi^* = \\argmax_{\\pi(s^t,I^t} \\mathbb{E}[\\sum_{t=0}^\\infty\\gamma^tr(s^t,a^t)]$$\n","\n","<p><center><figure><img src='_images/T729495_1.png'><figcaption>Illustration of the interaction between a user and the recommendation system. Green arrows represent the recommender information flow and orange represents user’s information flow.</figcaption></figure></center></p>\n","\n","Several key aspects are as follows:\n","\n","1. **Environment** will correspond to a logged online user who can click on one of the k items displayed by the recommendation system in each page view (or interaction).\n","2. **State** $s^t ∈ S$  will correspond to an ordered sequence of a user’s historical clicks.\n","3. $Action$ $A^t$  of the recommender will correspond to a subset of k items chosen by the recommender to display to the user. Itemset means the set of all subsets of k items of $I^t$, where $I^t ⊂ I$ are available items to recommend at time t.\n","4. **State Transition** $P(·|s^t,A^t)$ will correspond to a user behavior model which returns the transition probability for $s^{t+1}$ given previous state $s^t$ and the set of items $A^t$ displayed by the system. It is equivalent to the distribution $φ(s^t , A^t)$ over a user’s actions.\n","5. **Reward Function** will correspond to a user’s utility or satisfaction after making her choice. Here we assume that the reward to the recommendation system is the same as the user’s utility. Thus, a recommendation algorithm which optimizes its long-term reward is designed to satisfy the user in a long run. One can also include the company’s benefit to the reward, but we will focus on users’ satisfaction.\n","6. **Policy** will correspond to a recommendation strategy which returns the probability of displaying a subset $A^t$ of $I^t$ in user state $s^t$.\n","\n","Since both the reward function and the state transition model are unknown, we need to learn them from data. Once they are learned, the optimal policy $π^∗$ can be estimated by repeated querying the model using algorithms such as Q-learning.\n","\n","<p><center><figure><img src='_images/T729495_2.png'><figcaption>Architecture of models parameterized by either (a) position weight (PW) or (b) LSTM. (c) Cascading Q-networks.</figcaption></figure></center></p>"]},{"cell_type":"markdown","metadata":{"id":"8U9S6dUPiccj"},"source":["## Setup"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"63CyFBy8fT12","executionInfo":{"status":"ok","timestamp":1634798196793,"user_tz":-330,"elapsed":629,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}},"outputId":"a0d302c3-1b29-47ef-8b18-b78a9bbf62f1"},"source":["%tensorflow_version 1.x"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["TensorFlow 1.x selected.\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"iH74RGw8cIA0","executionInfo":{"status":"ok","timestamp":1634797128263,"user_tz":-330,"elapsed":456,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}},"outputId":"fced9687-b35f-4a89-fbfd-87bddd1dc53d"},"source":["!wget -q --show-progress -O yelp.txt https://raw.githubusercontent.com/sparsh-ai/drl-recsys/main/data/bronze/yelp.txt?token=APAMRF52LJRGV4LHOTCBSMDBOEDGM"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\ryelp.txt              0%[                    ]       0  --.-KB/s               \ryelp.txt            100%[===================>] 443.24K  --.-KB/s    in 0.03s   \n"]}]},{"cell_type":"code","metadata":{"id":"XzQ56fKVd7ox"},"source":["from __future__ import print_function\n","from __future__ import absolute_import\n","from __future__ import division\n","\n","import os\n","import numpy as np\n","import pickle\n","import pandas as pd\n","import datetime\n","import itertools\n","import tensorflow as tf\n","import threading"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lRbt2S13ibL3"},"source":["## Params"]},{"cell_type":"code","metadata":{"id":"8FcWtxHKgLAk"},"source":["class Args:\n","    dataset = 'yelp' # 'choose rsc, tb, or yelp'\n","    data_folder = '.'\n","    save_dir = './scratch'\n","    resplit = False\n","    num_thread = 10\n","    learning_rate = 1e-3\n","    batch_size = 128\n","    num_itrs = 2000\n","    rnn_hidden_dim = 20 # LSTM hidden sizes\n","    pw_dim = 4 # position weight dim\n","    pw_band_size = 20 # position weight banded size (i.e. length of history)\n","    dims = '64-64'\n","    user_model = 'LSTM' # architecture choice: LSTM or PW\n","\n","cmd_args = Args()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HdFfrOb6igBd"},"source":["## Data Preprocessing"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":419},"id":"XMF7E9xVckfn","executionInfo":{"status":"ok","timestamp":1634797188442,"user_tz":-330,"elapsed":630,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}},"outputId":"7c063256-7fad-4341-8000-c91f148184cf"},"source":["# The format of processed data:\n","# data_behavior[user][0] is user_id\n","# data_behavior[user][1][t] is displayed list at time t\n","# data_behavior[user][2][t] is picked id at time t\n","\n","filename = './'+cmd_args.dataset+'.txt'\n","\n","raw_data = pd.read_csv(filename, sep='\\t', usecols=[1, 3, 5, 7, 6], dtype={1: int, 3: int, 7: int, 5:int, 6:int})\n","\n","raw_data.drop_duplicates(subset=['session_new_index','Time','item_new_index','is_click'], inplace=True)\n","raw_data.sort_values(by='is_click',inplace=True)\n","raw_data.drop_duplicates(keep='last', subset=['session_new_index','Time','item_new_index'], inplace=True)\n","\n","raw_data"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Time</th>\n","      <th>is_click</th>\n","      <th>session_new_index</th>\n","      <th>item_new_index</th>\n","      <th>tr_val_tst</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4212</th>\n","      <td>1109</td>\n","      <td>0</td>\n","      <td>909</td>\n","      <td>91</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>4210</th>\n","      <td>1108</td>\n","      <td>0</td>\n","      <td>908</td>\n","      <td>453</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>4209</th>\n","      <td>1108</td>\n","      <td>0</td>\n","      <td>908</td>\n","      <td>452</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>4206</th>\n","      <td>1106</td>\n","      <td>0</td>\n","      <td>906</td>\n","      <td>564</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>1144</th>\n","      <td>309</td>\n","      <td>1</td>\n","      <td>256</td>\n","      <td>50</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2273</th>\n","      <td>601</td>\n","      <td>1</td>\n","      <td>507</td>\n","      <td>187</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1146</th>\n","      <td>310</td>\n","      <td>1</td>\n","      <td>257</td>\n","      <td>102</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>5249</th>\n","      <td>1365</td>\n","      <td>1</td>\n","      <td>1117</td>\n","      <td>764</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>6755</th>\n","      <td>1724</td>\n","      <td>1</td>\n","      <td>1405</td>\n","      <td>420</td>\n","      <td>2</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>6676 rows × 5 columns</p>\n","</div>"],"text/plain":["      Time  is_click  session_new_index  item_new_index  tr_val_tst\n","0        0         0                  0               0           0\n","4212  1109         0                909              91           1\n","4210  1108         0                908             453           1\n","4209  1108         0                908             452           1\n","4206  1106         0                906             564           1\n","...    ...       ...                ...             ...         ...\n","1144   309         1                256              50           0\n","2273   601         1                507             187           0\n","1146   310         1                257             102           0\n","5249  1365         1               1117             764           2\n","6755  1724         1               1405             420           2\n","\n","[6676 rows x 5 columns]"]},"metadata":{},"execution_count":5}]},{"cell_type":"markdown","metadata":{"id":"tNYs7GQ7dZKU"},"source":["- The column 'session_new_index' corresponds to user ID\n","- The column 'item_new_index' corresponds to item ID\n","- If several items have the same 'Time' index, then they are displayed at the same time (in the same display set)"]},{"cell_type":"code","metadata":{"id":"Su7Z2dI-dBXi"},"source":["sizes = raw_data.nunique()\n","size_user = sizes['session_new_index']\n","size_item = sizes['item_new_index']\n","\n","data_user = raw_data.groupby(by='session_new_index')\n","data_behavior = [[] for _ in range(size_user)]\n","\n","train_user = []\n","vali_user = []\n","test_user = []\n","\n","sum_length = 0\n","event_cnt = 0\n","\n","for user in range(size_user):\n","    data_behavior[user] = [[], [], []]\n","    data_behavior[user][0] = user\n","    data_u = data_user.get_group(user)\n","    split_tag = list(data_u['tr_val_tst'])[0]\n","    if split_tag == 0:\n","        train_user.append(user)\n","    elif split_tag == 1:\n","        vali_user.append(user)\n","    else:\n","        test_user.append(user)\n","\n","    data_u_time = data_u.groupby(by='Time')\n","    time_set = np.array(list(set(data_u['Time'])))\n","    time_set.sort()\n","\n","    true_t = 0\n","    for t in range(len(time_set)):\n","        display_set = data_u_time.get_group(time_set[t])\n","        event_cnt += 1\n","        sum_length += len(display_set)\n","\n","        data_behavior[user][1].append(list(display_set['item_new_index']))\n","        data_behavior[user][2].append(int(display_set[display_set.is_click==1]['item_new_index']))\n","\n","new_features = np.eye(size_item)\n","\n","filename = './'+cmd_args.dataset+'.pkl'\n","file = open(filename, 'wb')\n","pickle.dump(data_behavior, file, protocol=pickle.HIGHEST_PROTOCOL)\n","pickle.dump(new_features, file, protocol=pickle.HIGHEST_PROTOCOL)\n","file.close()\n","\n","filename = './'+cmd_args.dataset+'-split.pkl'\n","file = open(filename, 'wb')\n","pickle.dump(train_user, file, protocol=pickle.HIGHEST_PROTOCOL)\n","pickle.dump(vali_user, file, protocol=pickle.HIGHEST_PROTOCOL)\n","pickle.dump(test_user, file, protocol=pickle.HIGHEST_PROTOCOL)\n","file.close()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HbygpZuJiVuR"},"source":["## Dataset"]},{"cell_type":"code","metadata":{"id":"3TPCN9zRdLmR"},"source":["class Dataset(object):\n","\n","    def __init__(self, args):\n","        self.data_folder = args.data_folder\n","        self.dataset = args.dataset\n","        self.model_type = args.user_model\n","        self.band_size = args.pw_band_size\n","\n","        data_filename = os.path.join(args.data_folder, args.dataset+'.pkl')\n","        f = open(data_filename, 'rb')\n","        data_behavior = pickle.load(f)\n","        item_feature = pickle.load(f)\n","        f.close()\n","        # data_behavior[user][0] is user_id\n","        # data_behavior[user][1][t] is displayed list at time t\n","        # data_behavior[user][2][t] is picked id at time t\n","        self.size_item = len(item_feature)\n","        self.size_user = len(data_behavior)\n","        self.f_dim = len(item_feature[0])\n","\n","        # Load user splits\n","        filename = os.path.join(self.data_folder, self.dataset+'-split.pkl')\n","        pkl_file = open(filename, 'rb')\n","        self.train_user = pickle.load(pkl_file)\n","        self.vali_user = pickle.load(pkl_file)\n","        self.test_user = pickle.load(pkl_file)\n","        pkl_file.close()\n","\n","        # Process data\n","\n","        k_max = 0\n","        for d_b in data_behavior:\n","            for disp in d_b[1]:\n","                k_max = max(k_max, len(disp))\n","\n","        self.data_click = [[] for x in range(self.size_user)]\n","        self.data_disp = [[] for x in range(self.size_user)]\n","        self.data_time = np.zeros(self.size_user, dtype=np.int)\n","        self.data_news_cnt = np.zeros(self.size_user, dtype=np.int)\n","        self.feature = [[] for x in range(self.size_user)]\n","        self.feature_click = [[] for x in range(self.size_user)]\n","\n","        for user in range(self.size_user):\n","            # (1) count number of clicks\n","            click_t = 0\n","            num_events = len(data_behavior[user][1])\n","            click_t += num_events\n","            self.data_time[user] = click_t\n","            # (2)\n","            news_dict = {}\n","            self.feature_click[user] = np.zeros([click_t, self.f_dim])\n","            click_t = 0\n","            for event in range(num_events):\n","                disp_list = data_behavior[user][1][event]\n","                pick_id = data_behavior[user][2][event]\n","                for id in disp_list:\n","                    if id not in news_dict:\n","                        news_dict[id] = len(news_dict)  # for each user, news id start from 0\n","                id = pick_id\n","                self.data_click[user].append([click_t, news_dict[id]])\n","                self.feature_click[user][click_t] = item_feature[id]\n","                for idd in disp_list:\n","                    self.data_disp[user].append([click_t, news_dict[idd]])\n","                click_t += 1  # splitter a event with 2 clickings to 2 events\n","\n","            self.data_news_cnt[user] = len(news_dict)\n","\n","            self.feature[user] = np.zeros([self.data_news_cnt[user], self.f_dim])\n","\n","            for id in news_dict:\n","                self.feature[user][news_dict[id]] = item_feature[id]\n","            self.feature[user] = self.feature[user].tolist()\n","            self.feature_click[user] = self.feature_click[user].tolist()\n","        self.max_disp_size = k_max\n","\n","    def random_split_user(self):\n","        num_users = len(self.train_user) + len(self.vali_user) + len(self.test_user)\n","        shuffle_order = np.arange(num_users)\n","        np.random.shuffle(shuffle_order)\n","        self.train_user = shuffle_order[0:len(self.train_user)].tolist()\n","        self.vali_user = shuffle_order[len(self.train_user):len(self.train_user)+len(self.vali_user)].tolist()\n","        self.test_user = shuffle_order[len(self.train_user)+len(self.vali_user):].tolist()\n","\n","    def data_process_for_placeholder(self, user_set):\n","\n","        if self.model_type == 'PW':\n","            sec_cnt_x = 0\n","            news_cnt_short_x = 0\n","            news_cnt_x = 0\n","            click_2d_x = []\n","            disp_2d_x = []\n","\n","            tril_indice = []\n","            tril_value_indice = []\n","\n","            disp_2d_split_sec = []\n","            feature_clicked_x = []\n","\n","            disp_current_feature_x = []\n","            click_sub_index_2d = []\n","\n","            for u in user_set:\n","                t_indice = []\n","                for kk in range(min(self.band_size-1, self.data_time[u]-1)):\n","                    t_indice += map(lambda x: [x + kk+1 + sec_cnt_x, x + sec_cnt_x], np.arange(self.data_time[u] - (kk+1)))\n","\n","                tril_indice += t_indice\n","                tril_value_indice += map(lambda x: (x[0] - x[1] - 1), t_indice)\n","\n","                click_2d_tmp = map(lambda x: [x[0] + sec_cnt_x, x[1]], self.data_click[u])\n","                click_2d_x += click_2d_tmp\n","\n","                disp_2d_tmp = map(lambda x: [x[0] + sec_cnt_x, x[1]], self.data_disp[u])\n","                click_sub_index_tmp = map(lambda x: disp_2d_tmp.index(x), click_2d_tmp)\n","\n","                click_sub_index_2d += map(lambda x: x+len(disp_2d_x), click_sub_index_tmp)\n","                disp_2d_x += disp_2d_tmp\n","                disp_2d_split_sec += map(lambda x: x[0] + sec_cnt_x, self.data_disp[u])\n","\n","                sec_cnt_x += self.data_time[u]\n","                news_cnt_short_x = max(news_cnt_short_x, self.data_news_cnt[u])\n","                news_cnt_x += self.data_news_cnt[u]\n","                disp_current_feature_x += map(lambda x: self.feature[u][x], [idd[1] for idd in self.data_disp[u]])\n","                feature_clicked_x += self.feature_click[u]\n","\n","            return click_2d_x, disp_2d_x, \\\n","                   disp_current_feature_x, sec_cnt_x, tril_indice, tril_value_indice, \\\n","                   disp_2d_split_sec, news_cnt_short_x, click_sub_index_2d, feature_clicked_x\n","\n","        else:\n","            news_cnt_short_x = 0\n","            u_t_dispid = []\n","            u_t_dispid_split_ut = []\n","            u_t_dispid_feature = []\n","\n","            u_t_clickid = []\n","\n","            size_user = len(user_set)\n","            max_time = 0\n","\n","            click_sub_index = []\n","\n","            for u in user_set:\n","                max_time = max(max_time, self.data_time[u])\n","\n","            user_time_dense = np.zeros([size_user, max_time], dtype=np.float32)\n","            click_feature = np.zeros([max_time, size_user, self.f_dim])\n","\n","            for u_idx in range(size_user):\n","                u = user_set[u_idx]\n","\n","                u_t_clickid_tmp = []\n","                u_t_dispid_tmp = []\n","\n","                for x in self.data_click[u]:\n","                    t, click_id = x\n","                    click_feature[t][u_idx] = self.feature[u][click_id]\n","                    u_t_clickid_tmp.append([u_idx, t, click_id])\n","                    user_time_dense[u_idx, t] = 1.0\n","\n","                u_t_clickid = u_t_clickid + u_t_clickid_tmp\n","\n","                for x in self.data_disp[u]:\n","                    t, disp_id = x\n","                    u_t_dispid_tmp.append([u_idx, t, disp_id])\n","                    u_t_dispid_split_ut.append([u_idx, t])\n","                    u_t_dispid_feature.append(self.feature[u][disp_id])\n","\n","                click_sub_index_tmp = map(lambda x: u_t_dispid_tmp.index(x), u_t_clickid_tmp)\n","                click_sub_index += map(lambda x: x+len(u_t_dispid), click_sub_index_tmp)\n","\n","                u_t_dispid = u_t_dispid + u_t_dispid_tmp\n","                news_cnt_short_x = max(news_cnt_short_x, self.data_news_cnt[u])\n","\n","            if self.model_type != 'LSTM':\n","                print('model type not supported. using LSTM')\n","\n","            return size_user, max_time, news_cnt_short_x, u_t_dispid, u_t_dispid_split_ut, np.array(u_t_dispid_feature),\\\n","                   click_feature, click_sub_index, u_t_clickid, user_time_dense\n","\n","    def data_process_for_placeholder_L2(self, user_set):\n","        news_cnt_short_x = 0\n","        u_t_dispid = []\n","        u_t_dispid_split_ut = []\n","        u_t_dispid_feature = []\n","\n","        u_t_clickid = []\n","\n","        size_user = len(user_set)\n","        max_time = 0\n","\n","        click_sub_index = []\n","\n","        for u in user_set:\n","            max_time = max(max_time, self.data_time[u])\n","\n","        user_time_dense = np.zeros([size_user, max_time], dtype=np.float32)\n","        click_feature = np.zeros([max_time, size_user, self.f_dim])\n","\n","        for u_idx in range(size_user):\n","            u = user_set[u_idx]\n","\n","            item_cnt = [{} for _ in range(self.data_time[u])]\n","\n","            u_t_clickid_tmp = []\n","            u_t_dispid_tmp = []\n","            for x in self.data_disp[u]:\n","                t, disp_id = x\n","                u_t_dispid_split_ut.append([u_idx, t])\n","                u_t_dispid_feature.append(self.feature[u][disp_id])\n","                if disp_id not in item_cnt[t]:\n","                    item_cnt[t][disp_id] = len(item_cnt[t])\n","                u_t_dispid_tmp.append([u_idx, t, item_cnt[t][disp_id]])\n","\n","            for x in self.data_click[u]:\n","                t, click_id = x\n","                click_feature[t][u_idx] = self.feature[u][click_id]\n","                u_t_clickid_tmp.append([u_idx, t, item_cnt[t][click_id]])\n","                user_time_dense[u_idx, t] = 1.0\n","\n","            u_t_clickid = u_t_clickid + u_t_clickid_tmp\n","\n","            click_sub_index_tmp = map(lambda x: u_t_dispid_tmp.index(x), u_t_clickid_tmp)\n","            click_sub_index += map(lambda x: x+len(u_t_dispid), click_sub_index_tmp)\n","\n","            u_t_dispid = u_t_dispid + u_t_dispid_tmp\n","            # news_cnt_short_x = max(news_cnt_short_x, data_news_cnt[u])\n","            news_cnt_short_x = self.max_disp_size\n","\n","        return size_user, max_time, news_cnt_short_x, \\\n","               u_t_dispid, u_t_dispid_split_ut, np.array(u_t_dispid_feature), click_feature, click_sub_index, \\\n","               u_t_clickid, user_time_dense\n","\n","    def prepare_validation_data_L2(self, num_sets, v_user):\n","        vali_thread_u = [[] for _ in range(num_sets)]\n","        size_user_v = [[] for _ in range(num_sets)]\n","        max_time_v = [[] for _ in range(num_sets)]\n","        news_cnt_short_v = [[] for _ in range(num_sets)]\n","        u_t_dispid_v = [[] for _ in range(num_sets)]\n","        u_t_dispid_split_ut_v = [[] for _ in range(num_sets)]\n","        u_t_dispid_feature_v = [[] for _ in range(num_sets)]\n","        click_feature_v = [[] for _ in range(num_sets)]\n","        click_sub_index_v = [[] for _ in range(num_sets)]\n","        u_t_clickid_v = [[] for _ in range(num_sets)]\n","        ut_dense_v = [[] for _ in range(num_sets)]\n","        for ii in range(len(v_user)):\n","            vali_thread_u[ii % num_sets].append(v_user[ii])\n","        for ii in range(num_sets):\n","            size_user_v[ii], max_time_v[ii], news_cnt_short_v[ii], u_t_dispid_v[ii],\\\n","            u_t_dispid_split_ut_v[ii], u_t_dispid_feature_v[ii], click_feature_v[ii], \\\n","            click_sub_index_v[ii], u_t_clickid_v[ii], ut_dense_v[ii] = self.data_process_for_placeholder_L2(vali_thread_u[ii])\n","        return vali_thread_u, size_user_v, max_time_v, news_cnt_short_v, u_t_dispid_v, u_t_dispid_split_ut_v,\\\n","               u_t_dispid_feature_v, click_feature_v, click_sub_index_v, u_t_clickid_v, ut_dense_v\n","\n","    def prepare_validation_data(self, num_sets, v_user):\n","\n","        if self.model_type == 'PW':\n","            vali_thread_u = [[] for _ in range(num_sets)]\n","            click_2d_v = [[] for _ in range(num_sets)]\n","            disp_2d_v = [[] for _ in range(num_sets)]\n","            feature_v = [[] for _ in range(num_sets)]\n","            sec_cnt_v = [[] for _ in range(num_sets)]\n","            tril_ind_v = [[] for _ in range(num_sets)]\n","            tril_value_ind_v = [[] for _ in range(num_sets)]\n","            disp_2d_split_sec_v = [[] for _ in range(num_sets)]\n","            feature_clicked_v = [[] for _ in range(num_sets)]\n","            news_cnt_short_v = [[] for _ in range(num_sets)]\n","            click_sub_index_2d_v = [[] for _ in range(num_sets)]\n","            for ii in range(len(v_user)):\n","                vali_thread_u[ii % num_sets].append(v_user[ii])\n","            for ii in range(num_sets):\n","                click_2d_v[ii], disp_2d_v[ii], feature_v[ii], sec_cnt_v[ii], tril_ind_v[ii], tril_value_ind_v[ii], \\\n","                disp_2d_split_sec_v[ii], news_cnt_short_v[ii], click_sub_index_2d_v[ii], feature_clicked_v[ii] = self.data_process_for_placeholder(vali_thread_u[ii])\n","            return vali_thread_u, click_2d_v, disp_2d_v, feature_v, sec_cnt_v, tril_ind_v, tril_value_ind_v, \\\n","                   disp_2d_split_sec_v, news_cnt_short_v, click_sub_index_2d_v, feature_clicked_v\n","\n","        else:\n","            if self.model_type != 'LSTM':\n","                print('model type not supported. using LSTM')\n","            vali_thread_u = [[] for _ in range(num_sets)]\n","            size_user_v = [[] for _ in range(num_sets)]\n","            max_time_v = [[] for _ in range(num_sets)]\n","            news_cnt_short_v = [[] for _ in range(num_sets)]\n","            u_t_dispid_v = [[] for _ in range(num_sets)]\n","            u_t_dispid_split_ut_v = [[] for _ in range(num_sets)]\n","            u_t_dispid_feature_v = [[] for _ in range(num_sets)]\n","            click_feature_v = [[] for _ in range(num_sets)]\n","            click_sub_index_v = [[] for _ in range(num_sets)]\n","            u_t_clickid_v = [[] for _ in range(num_sets)]\n","            ut_dense_v = [[] for _ in range(num_sets)]\n","            for ii in range(len(v_user)):\n","                vali_thread_u[ii % num_sets].append(v_user[ii])\n","            for ii in range(num_sets):\n","                size_user_v[ii], max_time_v[ii], news_cnt_short_v[ii], u_t_dispid_v[ii],\\\n","                u_t_dispid_split_ut_v[ii], u_t_dispid_feature_v[ii], click_feature_v[ii], \\\n","                click_sub_index_v[ii], u_t_clickid_v[ii], ut_dense_v[ii] = self.data_process_for_placeholder(vali_thread_u[ii])\n","            return vali_thread_u, size_user_v, max_time_v, news_cnt_short_v, u_t_dispid_v, u_t_dispid_split_ut_v,\\\n","                   u_t_dispid_feature_v, click_feature_v, click_sub_index_v, u_t_clickid_v, ut_dense_v"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fi6SRKOtiRep"},"source":["## GAN-based User model"]},{"cell_type":"code","metadata":{"id":"jcBJA9wid22w"},"source":["def mlp(x, hidden_dims, output_dim, activation, sd, act_last=False):\n","    hidden_dims = tuple(map(int, hidden_dims.split(\"-\")))\n","    for h in hidden_dims:\n","        x = tf.layers.dense(x, h, activation=activation, trainable=True,\n","                            kernel_initializer=tf.truncated_normal_initializer(stddev=sd))\n","    if act_last:\n","        return tf.layers.dense(x, output_dim, activation=activation, trainable=True,\n","                               kernel_initializer=tf.truncated_normal_initializer(stddev=sd))\n","    else:\n","        return tf.layers.dense(x, output_dim, trainable=True,\n","                               kernel_initializer=tf.truncated_normal_initializer(stddev=sd))\n","\n","\n","class UserModelLSTM(object):\n","\n","    def __init__(self, f_dim, args, max_disp_size=None):\n","\n","        self.f_dim = f_dim\n","        self.placeholder = {}\n","        self.rnn_hidden = args.rnn_hidden_dim\n","        self.hidden_dims = args.dims\n","        self.lr = args.learning_rate\n","        self.max_disp_size = max_disp_size\n","\n","    def construct_placeholder(self):\n","\n","        self.placeholder['clicked_feature'] = tf.placeholder(tf.float32, (None, None, self.f_dim))  # (time, user=batch, f_dim)\n","        self.placeholder['ut_dispid_feature'] = tf.placeholder(tf.float32, shape=[None, self.f_dim])  # # (user*time*dispid, _f_dim)\n","        self.placeholder['ut_dispid_ut'] = tf.placeholder(dtype=tf.int64, shape=[None, 2])\n","        self.placeholder['ut_dispid'] = tf.placeholder(dtype=tf.int64, shape=[None, 3])\n","        self.placeholder['ut_clickid'] = tf.placeholder(dtype=tf.int64, shape=[None, 3])\n","        self.placeholder['ut_clickid_val'] = tf.placeholder(dtype=tf.float32, shape=[None])\n","        self.placeholder['click_sublist_index'] = tf.placeholder(dtype=tf.int64, shape=[None])\n","\n","        self.placeholder['ut_dense'] = tf.placeholder(dtype=tf.float32, shape=[None, None])\n","\n","        self.placeholder['time'] = tf.placeholder(dtype=tf.int64)\n","        self.placeholder['item_size'] = tf.placeholder(dtype=tf.int64)\n","\n","    def construct_computation_graph(self):\n","\n","        batch_size = tf.shape(self.placeholder['clicked_feature'])[1]\n","        denseshape = tf.concat([tf.cast(tf.reshape(batch_size, [-1]), tf.int64), tf.reshape(self.placeholder['time'], [-1]), tf.reshape(self.placeholder['item_size'], [-1])], 0)\n","\n","        # construct lstm\n","        cell = tf.contrib.rnn.BasicLSTMCell(self.rnn_hidden, state_is_tuple=True)\n","        initial_state = cell.zero_state(batch_size, tf.float32)\n","        rnn_outputs, rnn_states = tf.nn.dynamic_rnn(cell, self.placeholder['clicked_feature'], initial_state=initial_state, time_major=True)\n","        # rnn_outputs: (time, user=batch, rnn_hidden)\n","        # (1) output forward one-step (2) then transpose\n","        u_bar_feature = tf.concat([tf.zeros([1, batch_size, self.rnn_hidden], dtype=tf.float32), rnn_outputs], 0)\n","        u_bar_feature = tf.transpose(u_bar_feature, perm=[1, 0, 2])  # (user, time, rnn_hidden)\n","        # gather corresponding feature\n","        u_bar_feature_gather = tf.gather_nd(u_bar_feature, self.placeholder['ut_dispid_ut'])\n","        combine_feature = tf.concat([u_bar_feature_gather, self.placeholder['ut_dispid_feature']], axis=1)\n","        # indicate size\n","        combine_feature = tf.reshape(combine_feature, [-1, self.rnn_hidden + self.f_dim])\n","\n","        # utility\n","        u_net = mlp(combine_feature, self.hidden_dims, 1, activation=tf.nn.elu, sd=1e-1, act_last=False)\n","        u_net = tf.reshape(u_net, [-1])\n","\n","        click_u_tensor = tf.SparseTensor(self.placeholder['ut_clickid'], tf.gather(u_net, self.placeholder['click_sublist_index']), dense_shape=denseshape)\n","        disp_exp_u_tensor = tf.SparseTensor(self.placeholder['ut_dispid'], tf.exp(u_net), dense_shape=denseshape)  # (user, time, id)\n","        disp_sum_exp_u_tensor = tf.sparse_reduce_sum(disp_exp_u_tensor, axis=2)\n","        sum_click_u_tensor = tf.sparse_reduce_sum(click_u_tensor, axis=2)\n","\n","        loss_tmp = - sum_click_u_tensor + tf.log(disp_sum_exp_u_tensor + 1)  # (user, time) loss\n","        loss_sum = tf.reduce_sum(tf.multiply(self.placeholder['ut_dense'], loss_tmp))\n","        event_cnt = tf.reduce_sum(self.placeholder['ut_dense'])\n","        loss = loss_sum / event_cnt\n","\n","        dense_exp_disp_util = tf.sparse_tensor_to_dense(disp_exp_u_tensor, default_value=0.0, validate_indices=False)\n","\n","        click_tensor = tf.sparse_to_dense(self.placeholder['ut_clickid'], denseshape, self.placeholder['ut_clickid_val'], default_value=0.0, validate_indices=False)\n","        argmax_click = tf.argmax(click_tensor, axis=2)\n","        argmax_disp = tf.argmax(dense_exp_disp_util, axis=2)\n","\n","        top_2_disp = tf.nn.top_k(dense_exp_disp_util, k=2, sorted=False)[1]\n","        argmax_compare = tf.cast(tf.equal(argmax_click, argmax_disp), tf.float32)\n","        precision_1_sum = tf.reduce_sum(tf.multiply(self.placeholder['ut_dense'], argmax_compare))\n","        tmpshape = tf.concat([tf.cast(tf.reshape(batch_size, [-1]), tf.int64), tf.reshape(self.placeholder['time'], [-1]), tf.constant([1], dtype=tf.int64)], 0)\n","        top2_compare = tf.reduce_sum(tf.cast(tf.equal(tf.reshape(argmax_click, tmpshape), tf.cast(top_2_disp, tf.int64)), tf.float32), axis=2)\n","        precision_2_sum = tf.reduce_sum(tf.multiply(self.placeholder['ut_dense'], top2_compare))\n","        precision_1 = precision_1_sum / event_cnt\n","        precision_2 = precision_2_sum / event_cnt\n","\n","        return loss, precision_1, precision_2, loss_sum, precision_1_sum, precision_2_sum, event_cnt\n","\n","    def construct_computation_graph_u(self):\n","\n","        batch_size = tf.shape(self.placeholder['clicked_feature'])[1]\n","\n","        # construct lstm\n","        cell = tf.contrib.rnn.BasicLSTMCell(self.rnn_hidden, state_is_tuple=True)\n","        initial_state = cell.zero_state(batch_size, tf.float32)\n","        rnn_outputs, rnn_states = tf.nn.dynamic_rnn(cell, self.placeholder['clicked_feature'], initial_state=initial_state, time_major=True)\n","        # rnn_outputs: (time, user=batch, rnn_hidden)\n","        # (1) output forward one-step (2) then transpose\n","        u_bar_feature = tf.concat([tf.zeros([1, batch_size, self.rnn_hidden], dtype=tf.float32), rnn_outputs], 0)\n","        u_bar_feature = tf.transpose(u_bar_feature, perm=[1, 0, 2])  # (user, time, rnn_hidden)\n","        # gather corresponding feature\n","        u_bar_feature_gather = tf.gather_nd(u_bar_feature, self.placeholder['ut_dispid_ut'])\n","        combine_feature = tf.concat([u_bar_feature_gather, self.placeholder['ut_dispid_feature']], axis=1)\n","        # indicate size\n","        combine_feature = tf.reshape(combine_feature, [-1, self.rnn_hidden + self.f_dim])\n","\n","        # utility\n","        u_net = mlp(combine_feature, self.hidden_dims, 1, activation=tf.nn.elu, sd=1e-1, act_last=False)\n","        self.u_net = tf.reshape(u_net, [-1])\n","        self.min_trainable_variables = tf.trainable_variables()\n","\n","    def construct_computation_graph_policy(self):\n","        batch_size = tf.shape(self.placeholder['clicked_feature'])[1]\n","        denseshape = tf.concat([tf.cast(tf.reshape(batch_size, [-1]), tf.int64), tf.reshape(self.placeholder['time'], [-1]), tf.reshape(self.placeholder['item_size'], [-1])], 0)\n","\n","        with tf.variable_scope('lstm2'):\n","            cell2 = tf.contrib.rnn.BasicLSTMCell(self.rnn_hidden, state_is_tuple=True)\n","            initial_state2 = cell2.zero_state(batch_size, tf.float32)\n","            rnn_outputs2, rnn_states2 = tf.nn.dynamic_rnn(cell2, self.placeholder['clicked_feature'], initial_state=initial_state2, time_major=True)\n","\n","        u_bar_feature2 = tf.concat([tf.zeros([1, batch_size, self.rnn_hidden], dtype=tf.float32), rnn_outputs2], 0)\n","        u_bar_feature2 = tf.transpose(u_bar_feature2, perm=[1, 0, 2])  # (user, time, rnn_hidden)\n","\n","        u_bar_feature_gather2 = tf.gather_nd(u_bar_feature2, self.placeholder['ut_dispid_ut'])\n","        combine_feature2 = tf.concat([u_bar_feature_gather2, self.placeholder['ut_dispid_feature']], axis=1)\n","\n","        combine_feature2 = tf.reshape(combine_feature2, [-1, self.rnn_hidden + self.f_dim])\n","\n","        pi_net = mlp(combine_feature2, '256-32', 1, tf.nn.elu, 1e-2)\n","        pi_net = tf.reshape(pi_net, [-1])\n","\n","        disp_pi_tensor = tf.SparseTensor(self.placeholder['ut_dispid'], pi_net, dense_shape=denseshape)\n","\n","        disp_pi_dense_tensor = tf.sparse_add((-10000.0) * tf.ones(tf.cast(denseshape, tf.int32)), disp_pi_tensor)\n","\n","        disp_pi_dense_tensor = tf.reshape(disp_pi_dense_tensor, [tf.cast(batch_size, tf.int32), tf.cast(self.placeholder['time'], tf.int32), self.max_disp_size])\n","\n","        pi_net = tf.contrib.layers.softmax(disp_pi_dense_tensor)\n","\n","        pi_net_val = tf.gather_nd(pi_net, self.placeholder['ut_dispid'])\n","\n","        loss_max_sum = tf.reduce_sum(tf.multiply(pi_net_val, self.u_net - 0.5 * pi_net_val))\n","        event_cnt = tf.reduce_sum(self.placeholder['ut_dense'])\n","\n","        loss_max = loss_max_sum / event_cnt\n","\n","        sum_click_u_tensor = tf.reduce_sum(tf.gather(self.u_net, self.placeholder['click_sublist_index']))\n","        loss_min_sum = loss_max_sum - sum_click_u_tensor\n","        loss_min = loss_min_sum / event_cnt\n","\n","        click_tensor = tf.sparse_to_dense(self.placeholder['ut_clickid'], denseshape, self.placeholder['ut_clickid_val'], default_value=0.0)\n","        argmax_click = tf.argmax(click_tensor, axis=2)\n","        argmax_disp = tf.argmax(pi_net, axis=2)\n","\n","        top_2_disp = tf.nn.top_k(pi_net, k=2, sorted=False)[1]\n","        argmax_compare = tf.cast(tf.equal(argmax_click, argmax_disp), tf.float32)\n","        precision_1_sum = tf.reduce_sum(tf.multiply(self.placeholder['ut_dense'], argmax_compare))\n","        tmpshape = tf.concat([tf.cast(tf.reshape(batch_size, [-1]), tf.int64), tf.reshape(self.placeholder['time'], [-1]), tf.constant([1], dtype=tf.int64)], 0)\n","        top2_compare = tf.reduce_sum(tf.cast(tf.equal(tf.reshape(argmax_click, tmpshape), tf.cast(top_2_disp, tf.int64)), tf.float32), axis=2)\n","        precision_2_sum = tf.reduce_sum(tf.multiply(self.placeholder['ut_dense'], top2_compare))\n","        precision_1 = precision_1_sum / event_cnt\n","        precision_2 = precision_2_sum / event_cnt\n","\n","        opt = tf.train.AdamOptimizer(learning_rate=self.lr)\n","        max_trainable_variables = list(set(tf.trainable_variables()) - set(self.min_trainable_variables))\n","\n","        # lossL2_min = tf.add_n([tf.nn.l2_loss(v) for v in min_trainable_variables if 'bias' not in v.name]) * _regularity\n","        # lossL2_max = tf.add_n([tf.nn.l2_loss(v) for v in max_trainable_variables if 'bias' not in v.name]) * _regularity\n","        train_min_op = opt.minimize(loss_min, var_list=self.min_trainable_variables)\n","        train_max_op = opt.minimize(-loss_max, var_list=max_trainable_variables)\n","\n","        self.init_variables = list(set(tf.global_variables()) - set(self.min_trainable_variables))\n","\n","        return train_min_op, train_max_op, loss_min, loss_max, precision_1, precision_2, loss_min_sum, loss_max_sum, precision_1_sum, precision_2_sum, event_cnt\n","\n","    def construct_model(self, is_training, reuse=False):\n","        with tf.variable_scope('model', reuse=reuse):\n","            loss, precision_1, precision_2, loss_sum, precision_1_sum, precision_2_sum, event_cnt = self.construct_computation_graph()\n","\n","        if is_training:\n","            global_step = tf.Variable(0, trainable=False)\n","            learning_rate = tf.train.exponential_decay(self.lr, global_step, 100000, 0.96, staircase=True)\n","            opt = tf.train.AdamOptimizer(learning_rate=learning_rate)\n","            train_op = opt.minimize(loss, global_step=global_step)\n","\n","            return train_op, loss, precision_1, precision_2, loss_sum, precision_1_sum, precision_2_sum, event_cnt\n","        else:\n","            return loss, precision_1, precision_2, loss_sum, precision_1_sum, precision_2_sum, event_cnt\n","\n","\n","class UserModelPW(object):\n","\n","    def __init__(self, f_dim, args):\n","        self.f_dim = f_dim\n","        self.placeholder = {}\n","        self.hidden_dims = args.dims\n","        self.lr = args.learning_rate\n","        self.pw_dim = args.pw_dim\n","        self.band_size = args.pw_band_size\n","\n","    def construct_placeholder(self):\n","        self.placeholder['disp_current_feature'] = tf.placeholder(dtype=tf.float32, shape=[None, self.f_dim])\n","        self.placeholder['Xs_clicked'] = tf.placeholder(dtype=tf.float32, shape=[None, self.f_dim])\n","\n","        self.placeholder['item_size'] = tf.placeholder(dtype=tf.int64, shape=[])\n","        self.placeholder['section_length'] = tf.placeholder(dtype=tf.int64)\n","        self.placeholder['click_indices'] = tf.placeholder(dtype=tf.int64, shape=[None, 2])\n","        self.placeholder['click_values'] = tf.placeholder(dtype=tf.float32, shape=[None])\n","        self.placeholder['disp_indices'] = tf.placeholder(dtype=tf.int64, shape=[None, 2])\n","\n","        self.placeholder['disp_2d_split_sec_ind'] = tf.placeholder(dtype=tf.int64, shape=[None])\n","\n","        self.placeholder['cumsum_tril_indices'] = tf.placeholder(dtype=tf.int64, shape=[None, 2])\n","        self.placeholder['cumsum_tril_value_indices'] = tf.placeholder(dtype=tf.int64, shape=[None])\n","\n","        self.placeholder['click_2d_subindex'] = tf.placeholder(dtype=tf.int64, shape=[None])\n","\n","    def construct_computation_graph(self):\n","\n","        denseshape = [self.placeholder['section_length'], self.placeholder['item_size']]\n","\n","        # (1) history feature --- net ---> clicked_feature\n","        # (1) construct cumulative history\n","        click_history = [[] for _ in range(self.pw_dim)]\n","        for ii in range(self.pw_dim):\n","            position_weight = tf.get_variable('p_w'+str(ii), [self.band_size], initializer=tf.constant_initializer(0.0001))\n","            cumsum_tril_value = tf.gather(position_weight, self.placeholder['cumsum_tril_value_indices'])\n","            cumsum_tril_matrix = tf.SparseTensor(self.placeholder['cumsum_tril_indices'], cumsum_tril_value,\n","                                                 [self.placeholder['section_length'], self.placeholder['section_length']])  # sec by sec\n","            click_history[ii] = tf.sparse_tensor_dense_matmul(cumsum_tril_matrix, self.placeholder['Xs_clicked'])  # Xs_clicked: section by _f_dim\n","        concat_history = tf.concat(click_history, axis=1)\n","        disp_history_feature = tf.gather(concat_history, self.placeholder['disp_2d_split_sec_ind'])\n","\n","        # (4) combine features\n","        concat_disp_features = tf.reshape(tf.concat([disp_history_feature, self.placeholder['disp_current_feature']], axis=1),\n","                                          [-1, self.f_dim * self.pw_dim + self.f_dim])\n","\n","        # (5) compute utility\n","        u_disp = mlp(concat_disp_features, self.hidden_dims, 1, tf.nn.elu, 1e-3, act_last=False)\n","\n","        # (5)\n","        exp_u_disp = tf.exp(u_disp)\n","        sum_exp_disp_ubar_ut = tf.segment_sum(exp_u_disp, self.placeholder['disp_2d_split_sec_ind'])\n","        sum_click_u_bar_ut = tf.gather(u_disp, self.placeholder['click_2d_subindex'])\n","\n","        # (6) loss and precision\n","        click_tensor = tf.SparseTensor(self.placeholder['click_indices'], self.placeholder['click_values'], denseshape)\n","        click_cnt = tf.sparse_reduce_sum(click_tensor, axis=1)\n","        loss_sum = tf.reduce_sum(- sum_click_u_bar_ut + tf.log(sum_exp_disp_ubar_ut + 1))\n","        event_cnt = tf.reduce_sum(click_cnt)\n","        loss = loss_sum / event_cnt\n","\n","        exp_disp_ubar_ut = tf.SparseTensor(self.placeholder['disp_indices'], tf.reshape(exp_u_disp, [-1]), denseshape)\n","        dense_exp_disp_util = tf.sparse_tensor_to_dense(exp_disp_ubar_ut, default_value=0.0, validate_indices=False)\n","        argmax_click = tf.argmax(tf.sparse_tensor_to_dense(click_tensor, default_value=0.0), axis=1)\n","        argmax_disp = tf.argmax(dense_exp_disp_util, axis=1)\n","\n","        top_2_disp = tf.nn.top_k(dense_exp_disp_util, k=2, sorted=False)[1]\n","\n","        precision_1_sum = tf.reduce_sum(tf.cast(tf.equal(argmax_click, argmax_disp), tf.float32))\n","        precision_1 = precision_1_sum / event_cnt\n","        precision_2_sum = tf.reduce_sum(tf.cast(tf.equal(tf.reshape(argmax_click, [-1, 1]), tf.cast(top_2_disp, tf.int64)), tf.float32))\n","        precision_2 = precision_2_sum / event_cnt\n","\n","        self.lossL2 = tf.add_n([tf.nn.l2_loss(v) for v in tf.trainable_variables() if 'bias' not in v.name]) * 0.05  # regularity\n","        return loss, precision_1, precision_2, loss_sum, precision_1_sum, precision_2_sum, event_cnt\n","\n","    def construct_model(self, is_training, reuse=False):\n","        global lossL2\n","        with tf.variable_scope('model', reuse=reuse):\n","            loss, precision_1, precision_2, loss_sum, precision_1_sum, precision_2_sum, event_cnt = self.construct_computation_graph()\n","\n","        if is_training:\n","            global_step = tf.Variable(0, trainable=False)\n","            learning_rate = tf.train.exponential_decay(self.lr, global_step, 100000, 0.96, staircase=True)\n","            opt = tf.train.AdamOptimizer(learning_rate=learning_rate)\n","            train_op = opt.minimize(loss, global_step=global_step)\n","            return train_op, loss, precision_1, precision_2, loss_sum, precision_1_sum, precision_2_sum, event_cnt\n","        else:\n","            return loss, precision_1, precision_2, loss_sum, precision_1_sum, precision_2_sum, event_cnt"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"yFX3FutuiN_Z"},"source":["## GA User Model with Shannon Entropy"]},{"cell_type":"code","metadata":{"id":"ulBDdLCveTNR"},"source":["def multithread_compute_vali():\n","    global vali_sum, vali_cnt\n","\n","    vali_sum = [0.0, 0.0, 0.0]\n","    vali_cnt = 0\n","    threads = []\n","    for ii in range(cmd_args.num_thread):\n","        thread = threading.Thread(target=vali_eval, args=(1, ii))\n","        thread.start()\n","        threads.append(thread)\n","\n","    for thread in threads:\n","        thread.join()\n","\n","    return vali_sum[0]/vali_cnt, vali_sum[1]/vali_cnt, vali_sum[2]/vali_cnt\n","\n","\n","lock = threading.Lock()\n","\n","\n","def vali_eval(xx, ii):\n","    global vali_sum, vali_cnt\n","    if cmd_args.user_model == 'LSTM':\n","        vali_thread_eval = sess.run([train_loss_sum, train_prec1_sum, train_prec2_sum, train_event_cnt], feed_dict={user_model.placeholder['clicked_feature']: click_feature_vali[ii],\n","                                   user_model.placeholder['ut_dispid_feature']: u_t_dispid_feature_vali[ii],\n","                                   user_model.placeholder['ut_dispid_ut']: np.array(u_t_dispid_split_ut_vali[ii], dtype=np.int64),\n","                                   user_model.placeholder['ut_dispid']: np.array(u_t_dispid_vali[ii], dtype=np.int64),\n","                                   user_model.placeholder['ut_clickid']: np.array(u_t_clickid_vali[ii], dtype=np.int64),\n","                                   user_model.placeholder['ut_clickid_val']: np.ones(len(u_t_clickid_vali[ii]), dtype=np.float32),\n","                                   user_model.placeholder['click_sublist_index']: np.array(click_sub_index_vali[ii], dtype=np.int64),\n","                                   user_model.placeholder['ut_dense']: ut_dense_vali[ii],\n","                                   user_model.placeholder['time']: max_time_vali[ii],\n","                                   user_model.placeholder['item_size']: news_cnt_short_vali[ii]\n","                                   })\n","    elif cmd_args.user_model == 'PW':\n","        vali_thread_eval = sess.run([train_loss_sum, train_prec1_sum, train_prec2_sum, train_event_cnt],\n","                                        feed_dict={user_model.placeholder['disp_current_feature']: feature_vali[ii],\n","                                                user_model.placeholder['item_size']: news_cnt_short_vali[ii],\n","                                                user_model.placeholder['section_length']: sec_cnt_vali[ii],\n","                                                user_model.placeholder['click_indices']: np.array(click_2d_vali[ii]),\n","                                                user_model.placeholder['click_values']: np.ones(len(click_2d_vali[ii]), dtype=np.float32),\n","                                                user_model.placeholder['disp_indices']: np.array(disp_2d_vali[ii]),\n","                                                user_model.placeholder['cumsum_tril_indices']: tril_ind_vali[ii],\n","                                                user_model.placeholder['cumsum_tril_value_indices']: np.array(tril_value_ind_vali[ii], dtype=np.int64),\n","                                                user_model.placeholder['click_2d_subindex']: click_sub_index_2d_vali[ii],\n","                                                user_model.placeholder['disp_2d_split_sec_ind']: disp_2d_split_sec_vali[ii],\n","                                                user_model.placeholder['Xs_clicked']: feature_clicked_vali[ii]})\n","\n","    lock.acquire()\n","    vali_sum[0] += vali_thread_eval[0]\n","    vali_sum[1] += vali_thread_eval[1]\n","    vali_sum[2] += vali_thread_eval[2]\n","    vali_cnt += vali_thread_eval[3]\n","    lock.release()\n","\n","\n","def multithread_compute_test():\n","    global test_sum, test_cnt\n","\n","    num_sets = 1 * cmd_args.num_thread\n","\n","    thread_dist = [[] for _ in range(cmd_args.num_thread)]\n","    for ii in range(num_sets):\n","        thread_dist[ii % cmd_args.num_thread].append(ii)\n","\n","    test_sum = [0.0, 0.0, 0.0]\n","    test_cnt = 0\n","    threads = []\n","    for ii in range(cmd_args.num_thread):\n","        thread = threading.Thread(target=test_eval, args=(1, thread_dist[ii]))\n","        thread.start()\n","        threads.append(thread)\n","\n","    for thread in threads:\n","        thread.join()\n","\n","    return test_sum[0]/test_cnt, test_sum[1]/test_cnt, test_sum[2]/test_cnt\n","\n","\n","def test_eval(xx, thread_dist):\n","    global test_sum, test_cnt\n","    test_thread_eval = [0.0, 0.0, 0.0]\n","    test_thread_cnt = 0\n","    for ii in thread_dist:\n","        if cmd_args.user_model == 'LSTM':\n","            test_set_eval = sess.run([train_loss_sum, train_prec1_sum, train_prec2_sum, train_event_cnt], feed_dict={user_model.placeholder['clicked_feature']: click_feature_test[ii],\n","                                           user_model.placeholder['ut_dispid_feature']: u_t_dispid_feature_test[ii],\n","                                           user_model.placeholder['ut_dispid_ut']: np.array(u_t_dispid_split_ut_test[ii], dtype=np.int64),\n","                                           user_model.placeholder['ut_dispid']: np.array(u_t_dispid_test[ii], dtype=np.int64),\n","                                           user_model.placeholder['ut_clickid']: np.array(u_t_clickid_test[ii], dtype=np.int64),\n","                                           user_model.placeholder['ut_clickid_val']: np.ones(len(u_t_clickid_test[ii]), dtype=np.float32),\n","                                           user_model.placeholder['click_sublist_index']: np.array(click_sub_index_test[ii], dtype=np.int64),\n","                                           user_model.placeholder['ut_dense']: ut_dense_test[ii],\n","                                           user_model.placeholder['time']: max_time_test[ii],\n","                                           user_model.placeholder['item_size']: news_cnt_short_test[ii]\n","                                           })\n","        elif cmd_args.user_model == 'PW':\n","            test_set_eval = sess.run([train_loss_sum, train_prec1_sum, train_prec2_sum, train_event_cnt],\n","                                        feed_dict={user_model.placeholder['disp_current_feature']: feature_test[ii],\n","                                                user_model.placeholder['item_size']: news_cnt_short_test[ii],\n","                                                user_model.placeholder['section_length']: sec_cnt_test[ii],\n","                                                user_model.placeholder['click_indices']: np.array(click_2d_test[ii]),\n","                                                user_model.placeholder['click_values']: np.ones(len(click_2d_test[ii]), dtype=np.float32),\n","                                                user_model.placeholder['disp_indices']: np.array(disp_2d_test[ii]),\n","                                                user_model.placeholder['cumsum_tril_indices']: tril_ind_test[ii],\n","                                                user_model.placeholder['cumsum_tril_value_indices']: np.array(tril_value_ind_test[ii], dtype=np.int64),\n","                                                user_model.placeholder['click_2d_subindex']: click_sub_index_2d_test[ii],\n","                                                user_model.placeholder['disp_2d_split_sec_ind']: disp_2d_split_sec_test[ii],\n","                                                user_model.placeholder['Xs_clicked']: feature_clicked_test[ii]})\n","\n","        test_thread_eval[0] += test_set_eval[0]\n","        test_thread_eval[1] += test_set_eval[1]\n","        test_thread_eval[2] += test_set_eval[2]\n","        test_thread_cnt += test_set_eval[3]\n","\n","    lock.acquire()\n","    test_sum[0] += test_thread_eval[0]\n","    test_sum[1] += test_thread_eval[1]\n","    test_sum[2] += test_thread_eval[2]\n","    test_cnt += test_thread_cnt\n","    lock.release()\n","\n","\n","if __name__ == '__main__':\n","\n","    log_time = datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n","    print(\"%s, start\" % log_time)\n","\n","    dataset = Dataset(cmd_args)\n","\n","    if cmd_args.resplit:\n","        dataset.random_split_user()\n","\n","    log_time = datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n","    print(\"%s, load data completed\" % log_time)\n","\n","    log_time = datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n","    print(\"%s, start to construct graph\" % log_time)\n","\n","    if cmd_args.user_model == 'LSTM':\n","        user_model = UserModelLSTM(dataset.f_dim, cmd_args)\n","    elif cmd_args.user_model == 'PW':\n","        user_model = UserModelPW(dataset.f_dim, cmd_args)\n","    else:\n","        print('using LSTM user model instead.')\n","        user_model = UserModelLSTM(dataset.f_dim, cmd_args)\n","\n","    user_model.construct_placeholder()\n","\n","    train_opt, train_loss, train_prec1, train_prec2, train_loss_sum, train_prec1_sum, train_prec2_sum, train_event_cnt = user_model.construct_model(is_training=True, reuse=False)\n","    log_time = datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n","    print(\"%s, graph completed\" % log_time)\n","\n","    sess = tf.Session()\n","    sess.run(tf.global_variables_initializer())\n","\n","    # prepare validation data\n","    log_time = datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n","    print(\"%s, start prepare vali data\" % log_time)\n","\n","    if cmd_args.user_model == 'LSTM':\n","        vali_thread_user, size_user_vali, max_time_vali, news_cnt_short_vali, u_t_dispid_vali, \\\n","        u_t_dispid_split_ut_vali, u_t_dispid_feature_vali, click_feature_vali, click_sub_index_vali, \\\n","        u_t_clickid_vali, ut_dense_vali = dataset.prepare_validation_data(cmd_args.num_thread, dataset.vali_user)\n","    elif cmd_args.user_model == 'PW':\n","        vali_thread_user, click_2d_vali, disp_2d_vali, \\\n","        feature_vali, sec_cnt_vali, tril_ind_vali, tril_value_ind_vali, disp_2d_split_sec_vali, \\\n","        news_cnt_short_vali, click_sub_index_2d_vali, feature_clicked_vali = dataset.prepare_validation_data(cmd_args.num_thread, dataset.vali_user)\n","    else:\n","        vali_thread_user, size_user_vali, max_time_vali, news_cnt_short_vali, u_t_dispid_vali, \\\n","        u_t_dispid_split_ut_vali, u_t_dispid_feature_vali, click_feature_vali, click_sub_index_vali, \\\n","        u_t_clickid_vali, ut_dense_vali = dataset.prepare_validation_data(cmd_args.num_thread, dataset.vali_user)\n","\n","    log_time = datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n","    print(\"%s, prepare validation data, completed\" % log_time)\n","\n","    best_metric = [100000.0, 0.0, 0.0]\n","\n","    vali_path = cmd_args.save_dir+'/'\n","    if not os.path.exists(vali_path):\n","        os.makedirs(vali_path)\n","\n","    saver = tf.train.Saver(max_to_keep=None)\n","\n","    for i in range(cmd_args.num_itrs):\n","        # training_start_point = (i * cmd_args.batch_size) % (len(dataset.train_user))\n","        # training_user = dataset.train_user[training_start_point: min(training_start_point + cmd_args.batch_size, len(dataset.train_user))]\n","\n","        training_user = np.random.choice(dataset.train_user, cmd_args.batch_size, replace=False)\n","\n","        if cmd_args.user_model == 'LSTM':\n","            size_user_tr, max_time_tr, news_cnt_short_tr, u_t_dispid_tr, u_t_dispid_split_ut_tr, \\\n","            u_t_dispid_feature_tr, click_feature_tr, click_sub_index_tr, u_t_clickid_tr, ut_dense_tr = dataset.data_process_for_placeholder(training_user)\n","\n","            sess.run(train_opt, feed_dict={user_model.placeholder['clicked_feature']: click_feature_tr,\n","                                           user_model.placeholder['ut_dispid_feature']: u_t_dispid_feature_tr,\n","                                           user_model.placeholder['ut_dispid_ut']: np.array(u_t_dispid_split_ut_tr, dtype=np.int64),\n","                                           user_model.placeholder['ut_dispid']: np.array(u_t_dispid_tr, dtype=np.int64),\n","                                           user_model.placeholder['ut_clickid']: np.array(u_t_clickid_tr, dtype=np.int64),\n","                                           user_model.placeholder['ut_clickid_val']: np.ones(len(u_t_clickid_tr), dtype=np.float32),\n","                                           user_model.placeholder['click_sublist_index']: np.array(click_sub_index_tr, dtype=np.int64),\n","                                           user_model.placeholder['ut_dense']: ut_dense_tr,\n","                                           user_model.placeholder['time']: max_time_tr,\n","                                           user_model.placeholder['item_size']: news_cnt_short_tr\n","                                           })\n","        elif cmd_args.user_model == 'PW':\n","            click_2d, disp_2d, feature_tr, sec_cnt, tril_ind, tril_value_ind, disp_2d_split_sect, \\\n","            news_cnt_sht, click_2d_subind, feature_clicked_tr = dataset.data_process_for_placeholder(training_user)\n","\n","            sess.run(train_opt, feed_dict={user_model.placeholder['disp_current_feature']: feature_tr,\n","                                           user_model.placeholder['item_size']: news_cnt_sht,\n","                                           user_model.placeholder['section_length']: sec_cnt,\n","                                           user_model.placeholder['click_indices']: click_2d,\n","                                           user_model.placeholder['click_values']: np.ones(len(click_2d), dtype=np.float32),\n","                                           user_model.placeholder['disp_indices']: np.array(disp_2d),\n","                                           user_model.placeholder['cumsum_tril_indices']: tril_ind,\n","                                           user_model.placeholder['cumsum_tril_value_indices']: np.array(tril_value_ind, dtype=np.int64),\n","                                           user_model.placeholder['click_2d_subindex']: click_2d_subind,\n","                                           user_model.placeholder['disp_2d_split_sec_ind']: disp_2d_split_sect,\n","                                           user_model.placeholder['Xs_clicked']: feature_clicked_tr})\n","\n","        if np.mod(i, 10) == 0:\n","            if i == 0:\n","                log_time = datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n","                print(\"%s, start first iteration validation\" % log_time)\n","            vali_loss_prc = multithread_compute_vali()\n","            if i == 0:\n","                log_time = datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n","                print(\"%s, first iteration validation complete\" % log_time)\n","\n","            log_time = datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n","            print(\"%s: itr%d, vali: %.5f, %.5f, %.5f\" %\n","                  (log_time, i, vali_loss_prc[0], vali_loss_prc[1], vali_loss_prc[2]))\n","\n","            if vali_loss_prc[0] < best_metric[0]:\n","                best_metric[0] = vali_loss_prc[0]\n","                best_save_path = os.path.join(vali_path, 'best-loss')\n","                best_save_path = saver.save(sess, best_save_path)\n","            if vali_loss_prc[1] > best_metric[1]:\n","                best_metric[1] = vali_loss_prc[1]\n","                best_save_path = os.path.join(vali_path, 'best-pre1')\n","                best_save_path = saver.save(sess, best_save_path)\n","            if vali_loss_prc[2] > best_metric[2]:\n","                best_metric[2] = vali_loss_prc[2]\n","                best_save_path = os.path.join(vali_path, 'best-pre2')\n","                best_save_path = saver.save(sess, best_save_path)\n","\n","        log_time = datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n","        print(\"%s, iteration %d train complete\" % (log_time, i))\n","\n","    # test\n","    log_time = datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n","    print(\"%s, start prepare test data\" % log_time)\n","    if cmd_args.user_model == 'LSTM':\n","        test_thread_user, size_user_test, max_time_test, news_cnt_short_test, u_t_dispid_test, \\\n","        u_t_dispid_split_ut_test, u_t_dispid_feature_test, click_feature_test, click_sub_index_test, \\\n","        u_t_clickid_test, ut_dense_test = dataset.prepare_validation_data(cmd_args.num_thread, dataset.test_user)\n","    elif cmd_args.user_model == 'PW':\n","        test_thread_user, click_2d_test, disp_2d_test, \\\n","        feature_test, sec_cnt_test, tril_ind_test, tril_value_ind_test, disp_2d_split_sec_test, \\\n","        news_cnt_short_test, click_sub_index_2d_test, feature_clicked_test = dataset.prepare_validation_data(cmd_args.num_thread, dataset.test_user)\n","\n","    log_time = datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n","    print(\"%s, prepare test data end\" % log_time)\n","\n","    best_save_path = os.path.join(vali_path, 'best-loss')\n","    saver.restore(sess, best_save_path)\n","    test_loss_prc = multithread_compute_test()\n","    vali_loss_prc = multithread_compute_vali()\n","    print(\"test!!!loss!!!, test: %.5f, vali: %.5f\" % (test_loss_prc[0], vali_loss_prc[0]))\n","\n","    best_save_path = os.path.join(vali_path, 'best-pre1')\n","    saver.restore(sess, best_save_path)\n","    test_loss_prc = multithread_compute_test()\n","    vali_loss_prc = multithread_compute_vali()\n","    print(\"test!!!pre1!!!, test: %.5f, vali: %.5f\" % (test_loss_prc[1], vali_loss_prc[1]))\n","\n","    best_save_path = os.path.join(vali_path, 'best-pre2')\n","    saver.restore(sess, best_save_path)\n","    test_loss_prc = multithread_compute_test()\n","    vali_loss_prc = multithread_compute_vali()\n","    print(\"test!!!pre2!!!, test: %.5f, vali: %.5f\" % (test_loss_prc[2], vali_loss_prc[2]))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kU2Ai8efqqug"},"source":["```\n","2021-10-21 06:28:28, iteration 1988 train complete\n","2021-10-21 06:28:28, iteration 1989 train complete\n","2021-10-21 06:28:28: itr1990, vali: nan, 0.00000, 0.38031\n","2021-10-21 06:28:28, iteration 1990 train complete\n","2021-10-21 06:28:28, iteration 1991 train complete\n","2021-10-21 06:28:28, iteration 1992 train complete\n","2021-10-21 06:28:28, iteration 1993 train complete\n","2021-10-21 06:28:28, iteration 1994 train complete\n","2021-10-21 06:28:28, iteration 1995 train complete\n","2021-10-21 06:28:28, iteration 1996 train complete\n","2021-10-21 06:28:28, iteration 1997 train complete\n","2021-10-21 06:28:28, iteration 1998 train complete\n","2021-10-21 06:28:28, iteration 1999 train complete\n","2021-10-21 06:28:28, start prepare test data\n","2021-10-21 06:28:28, prepare test data end\n","INFO:tensorflow:Restoring parameters from ./scratch/best-loss\n","test!!!loss!!!, test: 0.93315, vali: 0.98756\n","INFO:tensorflow:Restoring parameters from ./scratch/best-pre1\n","test!!!pre1!!!, test: 0.74828, vali: 0.67562\n","INFO:tensorflow:Restoring parameters from ./scratch/best-pre2\n","test!!!pre2!!!, test: 0.87643, vali: 0.84340\n","```"]},{"cell_type":"markdown","metadata":{"id":"r3FlwbZ1e4By"},"source":["## GA User Model with L2 Regularization\n","\n","We trained the user model using Shannon Entropy. With this saved model as an initilization, we can continue to train the model using other regularizations. For example, L2."]},{"cell_type":"code","metadata":{"id":"nFyMIZL7i0ut"},"source":["cmd_args.num_thread = 10\n","cmd_args.rnn_hidden_dim = 20\n","cmd_args.learning_rate = 0.0005\n","cmd_args.batch_size = 50\n","cmd_args.num_itrs = 5000\n","cmd_args.resplit = False\n","cmd_args.pw_dim = 4"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZknMAdK9jFzY","executionInfo":{"status":"ok","timestamp":1634798411762,"user_tz":-330,"elapsed":185653,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}},"outputId":"91ab589d-2b46-471d-e1b0-07684cd5a9db"},"source":["def multithread_compute_vali():\n","    global vali_sum, vali_cnt\n","\n","    vali_sum = [0.0, 0.0, 0.0, 0.0]\n","    vali_cnt = 0\n","    threads = []\n","    for ii in range(cmd_args.num_thread):\n","        thread = threading.Thread(target=vali_eval, args=(1, ii))\n","        thread.start()\n","        threads.append(thread)\n","\n","    for thread in threads:\n","        thread.join()\n","\n","    return vali_sum[0]/vali_cnt, vali_sum[1]/vali_cnt, vali_sum[2]/vali_cnt, vali_sum[3]/vali_cnt\n","\n","\n","def vali_eval(xx, ii):\n","    global vali_sum, vali_cnt\n","    vali_thread_eval = sess.run([train_loss_min_sum, train_loss_max_sum, train_prec1_sum, train_prec2_sum, train_event_cnt],\n","                                feed_dict={user_model.placeholder['clicked_feature']: click_feature_vali[ii],\n","                                   user_model.placeholder['ut_dispid_feature']: u_t_dispid_feature_vali[ii],\n","                                   user_model.placeholder['ut_dispid_ut']: np.array(u_t_dispid_split_ut_vali[ii], dtype=np.int64),\n","                                   user_model.placeholder['ut_dispid']: np.array(u_t_dispid_vali[ii], dtype=np.int64),\n","                                   user_model.placeholder['ut_clickid']: np.array(u_t_clickid_vali[ii], dtype=np.int64),\n","                                   user_model.placeholder['ut_clickid_val']: np.ones(len(u_t_clickid_vali[ii]), dtype=np.float32),\n","                                   user_model.placeholder['click_sublist_index']: np.array(click_sub_index_vali[ii], dtype=np.int64),\n","                                   user_model.placeholder['ut_dense']: ut_dense_vali[ii],\n","                                   user_model.placeholder['time']: max_time_vali[ii],\n","                                   user_model.placeholder['item_size']: news_cnt_short_vali[ii]\n","                                   })\n","    lock.acquire()\n","    vali_sum[0] += vali_thread_eval[0]\n","    vali_sum[1] += vali_thread_eval[1]\n","    vali_sum[2] += vali_thread_eval[2]\n","    vali_sum[3] += vali_thread_eval[3]\n","    vali_cnt += vali_thread_eval[4]\n","    lock.release()\n","\n","\n","def multithread_compute_test():\n","    global test_sum, test_cnt\n","\n","    num_sets = cmd_args.num_thread\n","\n","    thread_dist = [[] for _ in range(cmd_args.num_thread)]\n","    for ii in range(num_sets):\n","        thread_dist[ii % cmd_args.num_thread].append(ii)\n","\n","    test_sum = [0.0, 0.0, 0.0, 0.0]\n","    test_cnt = 0\n","    threads = []\n","    for ii in range(cmd_args.num_thread):\n","        thread = threading.Thread(target=test_eval, args=(1, thread_dist[ii]))\n","        thread.start()\n","        threads.append(thread)\n","\n","    for thread in threads:\n","        thread.join()\n","\n","    return test_sum[0]/test_cnt, test_sum[1]/test_cnt, test_sum[2]/test_cnt, test_sum[3]/test_cnt\n","\n","\n","def test_eval(xx, thread_dist):\n","    global test_sum, test_cnt\n","    test_thread_eval = [0.0, 0.0, 0.0, 0.0]\n","    test_thread_cnt = 0\n","    for ii in thread_dist:\n","        test_set_eval = sess.run([train_loss_min_sum, train_loss_max_sum, train_prec1_sum, train_prec2_sum, train_event_cnt],\n","                                 feed_dict={user_model.placeholder['clicked_feature']: click_feature_test[ii],\n","                                           user_model.placeholder['ut_dispid_feature']: u_t_dispid_feature_test[ii],\n","                                           user_model.placeholder['ut_dispid_ut']: np.array(u_t_dispid_split_ut_test[ii], dtype=np.int64),\n","                                           user_model.placeholder['ut_dispid']: np.array(u_t_dispid_test[ii], dtype=np.int64),\n","                                           user_model.placeholder['ut_clickid']: np.array(u_t_clickid_test[ii], dtype=np.int64),\n","                                           user_model.placeholder['ut_clickid_val']: np.ones(len(u_t_clickid_test[ii]), dtype=np.float32),\n","                                           user_model.placeholder['click_sublist_index']: np.array(click_sub_index_test[ii], dtype=np.int64),\n","                                           user_model.placeholder['ut_dense']: ut_dense_test[ii],\n","                                           user_model.placeholder['time']: max_time_test[ii],\n","                                           user_model.placeholder['item_size']: news_cnt_short_test[ii]\n","                                           })\n","        test_thread_eval[0] += test_set_eval[0]\n","        test_thread_eval[1] += test_set_eval[1]\n","        test_thread_eval[2] += test_set_eval[2]\n","        test_thread_eval[3] += test_set_eval[3]\n","        test_thread_cnt += test_set_eval[4]\n","\n","    lock.acquire()\n","    test_sum[0] += test_thread_eval[0]\n","    test_sum[1] += test_thread_eval[1]\n","    test_sum[2] += test_thread_eval[2]\n","    test_sum[3] += test_thread_eval[3]\n","    test_cnt += test_thread_cnt\n","    lock.release()\n","\n","\n","lock = threading.Lock()\n","\n","\n","if __name__ == '__main__':\n","\n","    log_time = datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n","    print(\"%s, start\" % log_time)\n","\n","    dataset = Dataset(cmd_args)\n","\n","    log_time = datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n","    print(\"%s, start construct graph\" % log_time)\n","\n","    # restore pre-trained u function\n","    user_model = UserModelLSTM(dataset.f_dim, cmd_args, dataset.max_disp_size)\n","\n","    user_model.construct_placeholder()\n","    with tf.variable_scope('model', reuse=False):\n","        user_model.construct_computation_graph_u()\n","\n","    saved_path = cmd_args.save_dir+'/'\n","    saver = tf.train.Saver(max_to_keep=None)\n","    sess = tf.Session()\n","    sess.run(tf.variables_initializer(user_model.min_trainable_variables))\n","    best_save_path = os.path.join(saved_path, 'best-pre1')\n","    saver.restore(sess, best_save_path)\n","\n","    # construct policy net\n","    train_min_opt, train_max_opt, train_loss_min, train_loss_max, train_prec1, train_prec2, train_loss_min_sum, \\\n","    train_loss_max_sum, train_prec1_sum, train_prec2_sum, train_event_cnt = user_model.construct_computation_graph_policy()\n","\n","    sess.run(tf.initialize_variables(user_model.init_variables))\n","    log_time = datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n","    print(\"%s, graph completed\" % log_time)\n","\n","    batch_size = 100\n","    batch = 100\n","\n","    if cmd_args.dataset == 'lastfm':\n","        batch_size = 10\n","        batch = 10\n","\n","    iterations = cmd_args.num_itrs\n","\n","    # prepare validation data\n","    log_time = datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n","    print(\"%s, start prepare vali data\" % log_time)\n","    vali_thread_user, size_user_vali, max_time_vali, news_cnt_short_vali, u_t_dispid_vali, \\\n","    u_t_dispid_split_ut_vali, u_t_dispid_feature_vali, click_feature_vali, click_sub_index_vali, \\\n","    u_t_clickid_vali, ut_dense_vali = dataset.prepare_validation_data_L2(cmd_args.num_thread, dataset.vali_user)\n","    log_time = datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n","    print(\"%s, prepare vali data complete\" % log_time)\n","\n","    best_metric = [0.0, 0.0, 0.0, 0.0]\n","\n","    saver = tf.train.Saver(max_to_keep=None)\n","\n","    vali_path = cmd_args.save_dir+'/minmax_L2/'\n","    if not os.path.exists(vali_path):\n","        os.makedirs(vali_path)\n","\n","    for i in range(iterations):\n","\n","        training_user = np.random.choice(len(dataset.train_user), batch, replace=False)\n","        if i == 0:\n","            log_time = datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n","            print(\"%s, start prepare train data\" % log_time)\n","\n","        size_user_tr, max_time_tr, news_cnt_short_tr, u_t_dispid_tr, u_t_dispid_split_ut_tr, \\\n","        u_t_dispid_feature_tr, click_feature_tr, click_sub_index_tr, u_t_clickid_tr, ut_dense_tr = dataset.data_process_for_placeholder_L2(training_user)\n","\n","        if i == 0:\n","            log_time = datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n","            print(\"%s, prepare train data completed\" % log_time)\n","            print(\"%s, start first iteration training\" % log_time)\n","\n","        sess.run(train_max_opt, feed_dict={user_model.placeholder['clicked_feature']: click_feature_tr,\n","                                           user_model.placeholder['ut_dispid_feature']: u_t_dispid_feature_tr,\n","                                           user_model.placeholder['ut_dispid_ut']: np.array(u_t_dispid_split_ut_tr, dtype=np.int64),\n","                                           user_model.placeholder['ut_dispid']: np.array(u_t_dispid_tr, dtype=np.int64),\n","                                           user_model.placeholder['ut_clickid']: np.array(u_t_clickid_tr, dtype=np.int64),\n","                                           user_model.placeholder['ut_clickid_val']: np.ones(len(u_t_clickid_tr), dtype=np.float32),\n","                                           user_model.placeholder['click_sublist_index']: np.array(click_sub_index_tr, dtype=np.int64),\n","                                           user_model.placeholder['ut_dense']: ut_dense_tr,\n","                                           user_model.placeholder['time']: max_time_tr,\n","                                           user_model.placeholder['item_size']: news_cnt_short_tr\n","                                           })\n","\n","        if i == 0:\n","            log_time = datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n","            print(\"%s, first iteration training complete\" % log_time)\n","\n","        if np.mod(i, 100) == 0:\n","            loss_prc = sess.run([train_loss_min, train_loss_max, train_prec1, train_prec2], feed_dict={user_model.placeholder['clicked_feature']: click_feature_tr,\n","                                           user_model.placeholder['ut_dispid_feature']: u_t_dispid_feature_tr,\n","                                           user_model.placeholder['ut_dispid_ut']: np.array(u_t_dispid_split_ut_tr, dtype=np.int64),\n","                                           user_model.placeholder['ut_dispid']: np.array(u_t_dispid_tr, dtype=np.int64),\n","                                           user_model.placeholder['ut_clickid']: np.array(u_t_clickid_tr, dtype=np.int64),\n","                                           user_model.placeholder['ut_clickid_val']: np.ones(len(u_t_clickid_tr), dtype=np.float32),\n","                                           user_model.placeholder['click_sublist_index']: np.array(click_sub_index_tr, dtype=np.int64),\n","                                           user_model.placeholder['ut_dense']: ut_dense_tr,\n","                                           user_model.placeholder['time']: max_time_tr,\n","                                           user_model.placeholder['item_size']: news_cnt_short_tr\n","                                           })\n","            if i == 0:\n","                log_time = datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n","                print(\"%s, start first iteration validation\" % log_time)\n","            vali_loss_prc = multithread_compute_vali()\n","            if i == 0:\n","                log_time = datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n","                print(\"%s, first iteration validation complete\" % log_time)\n","\n","            log_time = datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n","            print(\"%s: itr%d, training: %.5f, %.5f, %.5f, %.5f, vali: %.5f, %.5f, %.5f, %.5f\" %\n","                  (log_time, i, loss_prc[0], loss_prc[1], loss_prc[2], loss_prc[3], vali_loss_prc[0], vali_loss_prc[1], vali_loss_prc[2], vali_loss_prc[3]))\n","\n","            if vali_loss_prc[2] > best_metric[2]:\n","                best_metric[2] = vali_loss_prc[2]\n","                best_save_path = os.path.join(vali_path, 'best-pre1')\n","                best_save_path = saver.save(sess, best_save_path)\n","            if vali_loss_prc[3] > best_metric[3]:\n","                best_metric[3] = vali_loss_prc[3]\n","                best_save_path = os.path.join(vali_path, 'best-pre2')\n","                best_save_path = saver.save(sess, best_save_path)\n","            save_path = os.path.join(vali_path, 'most_recent_iter')\n","            save_path = saver.save(sess, save_path)\n","\n","    # test\n","    log_time = datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n","    print(\"%s, start prepare test data\" % log_time)\n","    test_thread_user, size_user_test, max_time_test, news_cnt_short_test, u_t_dispid_test, \\\n","    u_t_dispid_split_ut_test, u_t_dispid_feature_test, click_feature_test, click_sub_index_test, \\\n","    u_t_clickid_test, ut_dense_test = dataset.prepare_validation_data_L2(cmd_args.num_thread, dataset.test_user)\n","    log_time = datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n","    print(\"%s, prepare test data end\" % log_time)\n","\n","    best_save_path = os.path.join(vali_path, 'best-pre1')\n","    saver.restore(sess, best_save_path)\n","    test_loss_prc = multithread_compute_test()\n","    vali_loss_prc = multithread_compute_vali()\n","    print(\"test!!!best-pre1!!!, test: %.5f, vali: %.5f\" % (test_loss_prc[2], vali_loss_prc[2]))\n","\n","    best_save_path = os.path.join(vali_path, 'best-pre2')\n","    saver.restore(sess, best_save_path)\n","    test_loss_prc = multithread_compute_test()\n","    vali_loss_prc = multithread_compute_vali()\n","    print(\"test!!!best-pre2!!!, test: %.5f, vali: %.5f\" % (test_loss_prc[3], vali_loss_prc[3]))"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["2021-10-21 06:37:12, start\n","2021-10-21 06:37:12, start construct graph\n","WARNING:tensorflow:\n","The TensorFlow contrib module will not be included in TensorFlow 2.0.\n","For more information, please see:\n","  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n","  * https://github.com/tensorflow/addons\n","  * https://github.com/tensorflow/io (for I/O related ops)\n","If you depend on functionality not listed there, please file an issue.\n","\n","WARNING:tensorflow:From <ipython-input-5-6cf3a6077dba>:95: BasicLSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","This class is equivalent as tf.keras.layers.LSTMCell, and will be replaced by that in Tensorflow 2.0.\n","WARNING:tensorflow:From <ipython-input-5-6cf3a6077dba>:97: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n","WARNING:tensorflow:From /tensorflow-1.15.2/python3.7/tensorflow_core/python/ops/rnn_cell_impl.py:735: Layer.add_variable (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Please use `layer.add_weight` method instead.\n","WARNING:tensorflow:From /tensorflow-1.15.2/python3.7/tensorflow_core/python/ops/rnn_cell_impl.py:739: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Call initializer instance with the dtype argument instead of passing it to the constructor\n","WARNING:tensorflow:From <ipython-input-5-6cf3a6077dba>:5: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use keras.layers.Dense instead.\n","WARNING:tensorflow:From /tensorflow-1.15.2/python3.7/tensorflow_core/python/layers/core.py:187: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Please use `layer.__call__` method instead.\n","INFO:tensorflow:Restoring parameters from ./scratch/best-pre1\n","WARNING:tensorflow:From <ipython-input-5-6cf3a6077dba>:152: sparse_to_dense (from tensorflow.python.ops.sparse_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Create a `tf.sparse.SparseTensor` and use `tf.sparse.to_dense` instead.\n"]},{"output_type":"stream","name":"stderr","text":["/tensorflow-1.15.2/python3.7/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n","  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"]},{"output_type":"stream","name":"stdout","text":["WARNING:tensorflow:From /tensorflow-1.15.2/python3.7/tensorflow_core/python/util/tf_should_use.py:198: initialize_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n","Instructions for updating:\n","Use `tf.variables_initializer` instead.\n","2021-10-21 06:37:15, graph completed\n","2021-10-21 06:37:15, start prepare vali data\n","2021-10-21 06:37:15, prepare vali data complete\n","2021-10-21 06:37:15, start prepare train data\n","2021-10-21 06:37:15, prepare train data completed\n","2021-10-21 06:37:15, start first iteration training\n","2021-10-21 06:37:16, first iteration training complete\n","2021-10-21 06:37:16, start first iteration validation\n","2021-10-21 06:37:16, first iteration validation complete\n","2021-10-21 06:37:16: itr0, training: -14.55946, 3.87918, 0.18033, 0.45082, vali: -11.21879, 5.44785, 0.17002, 0.37808\n","INFO:tensorflow:./scratch/minmax_L2/best-pre1 is not in all_model_checkpoint_paths. Manually adding it.\n","INFO:tensorflow:./scratch/minmax_L2/best-pre2 is not in all_model_checkpoint_paths. Manually adding it.\n","INFO:tensorflow:./scratch/minmax_L2/most_recent_iter is not in all_model_checkpoint_paths. Manually adding it.\n","2021-10-21 06:37:20: itr100, training: -6.27841, 12.41326, 0.70588, 0.84874, vali: -3.43435, 13.23230, 0.48770, 0.69799\n","INFO:tensorflow:./scratch/minmax_L2/best-pre1 is not in all_model_checkpoint_paths. Manually adding it.\n","INFO:tensorflow:./scratch/minmax_L2/best-pre2 is not in all_model_checkpoint_paths. Manually adding it.\n","INFO:tensorflow:./scratch/minmax_L2/most_recent_iter is not in all_model_checkpoint_paths. Manually adding it.\n","2021-10-21 06:37:24: itr200, training: -1.83261, 15.74864, 0.84426, 0.97541, vali: 0.62875, 17.29540, 0.58389, 0.78971\n","INFO:tensorflow:./scratch/minmax_L2/best-pre1 is not in all_model_checkpoint_paths. Manually adding it.\n","INFO:tensorflow:./scratch/minmax_L2/best-pre2 is not in all_model_checkpoint_paths. Manually adding it.\n","INFO:tensorflow:./scratch/minmax_L2/most_recent_iter is not in all_model_checkpoint_paths. Manually adding it.\n","2021-10-21 06:37:28: itr300, training: -0.86641, 17.90119, 0.96000, 0.99200, vali: 1.22508, 17.89172, 0.62416, 0.80537\n","INFO:tensorflow:./scratch/minmax_L2/best-pre1 is not in all_model_checkpoint_paths. Manually adding it.\n","INFO:tensorflow:./scratch/minmax_L2/best-pre2 is not in all_model_checkpoint_paths. Manually adding it.\n","INFO:tensorflow:./scratch/minmax_L2/most_recent_iter is not in all_model_checkpoint_paths. Manually adding it.\n","2021-10-21 06:37:32: itr400, training: -0.76307, 18.72485, 0.93333, 0.97500, vali: 1.40389, 18.07054, 0.63982, 0.81208\n","INFO:tensorflow:./scratch/minmax_L2/best-pre1 is not in all_model_checkpoint_paths. Manually adding it.\n","INFO:tensorflow:./scratch/minmax_L2/best-pre2 is not in all_model_checkpoint_paths. Manually adding it.\n","INFO:tensorflow:./scratch/minmax_L2/most_recent_iter is not in all_model_checkpoint_paths. Manually adding it.\n","2021-10-21 06:37:35: itr500, training: -0.65650, 18.61495, 0.97500, 0.98333, vali: 1.49072, 18.15737, 0.64206, 0.81208\n","INFO:tensorflow:./scratch/minmax_L2/best-pre1 is not in all_model_checkpoint_paths. Manually adding it.\n","INFO:tensorflow:./scratch/minmax_L2/most_recent_iter is not in all_model_checkpoint_paths. Manually adding it.\n","2021-10-21 06:37:39: itr600, training: -0.58195, 17.94368, 0.97345, 1.00000, vali: 1.55703, 18.22368, 0.65324, 0.81208\n","INFO:tensorflow:./scratch/minmax_L2/best-pre1 is not in all_model_checkpoint_paths. Manually adding it.\n","INFO:tensorflow:./scratch/minmax_L2/most_recent_iter is not in all_model_checkpoint_paths. Manually adding it.\n","2021-10-21 06:37:43: itr700, training: -0.59892, 18.28909, 0.96667, 0.99167, vali: 1.59338, 18.26002, 0.65324, 0.81208\n","INFO:tensorflow:./scratch/minmax_L2/most_recent_iter is not in all_model_checkpoint_paths. Manually adding it.\n","2021-10-21 06:37:46: itr800, training: -0.57403, 20.35625, 0.96581, 0.98291, vali: 1.61140, 18.27805, 0.65324, 0.81208\n","INFO:tensorflow:./scratch/minmax_L2/most_recent_iter is not in all_model_checkpoint_paths. Manually adding it.\n","2021-10-21 06:37:50: itr900, training: -0.54096, 18.27187, 0.99206, 1.00000, vali: 1.62315, 18.28980, 0.65324, 0.81208\n","INFO:tensorflow:./scratch/minmax_L2/most_recent_iter is not in all_model_checkpoint_paths. Manually adding it.\n","2021-10-21 06:37:53: itr1000, training: -0.54646, 19.26600, 0.98246, 0.99123, vali: 1.63227, 18.29892, 0.65548, 0.81208\n","INFO:tensorflow:./scratch/minmax_L2/best-pre1 is not in all_model_checkpoint_paths. Manually adding it.\n","INFO:tensorflow:./scratch/minmax_L2/most_recent_iter is not in all_model_checkpoint_paths. Manually adding it.\n","2021-10-21 06:37:57: itr1100, training: -0.57691, 18.21611, 0.98361, 0.98361, vali: 1.63925, 18.30590, 0.65996, 0.81208\n","INFO:tensorflow:./scratch/minmax_L2/best-pre1 is not in all_model_checkpoint_paths. Manually adding it.\n","INFO:tensorflow:./scratch/minmax_L2/most_recent_iter is not in all_model_checkpoint_paths. Manually adding it.\n","2021-10-21 06:38:00: itr1200, training: -0.54024, 17.75560, 0.98374, 1.00000, vali: 1.64636, 18.31301, 0.65772, 0.81208\n","INFO:tensorflow:./scratch/minmax_L2/most_recent_iter is not in all_model_checkpoint_paths. Manually adding it.\n","2021-10-21 06:38:04: itr1300, training: -0.60761, 17.55147, 0.95495, 0.96396, vali: 1.65015, 18.31680, 0.65996, 0.81208\n","INFO:tensorflow:./scratch/minmax_L2/most_recent_iter is not in all_model_checkpoint_paths. Manually adding it.\n","2021-10-21 06:38:08: itr1400, training: -0.57991, 19.36499, 0.98198, 0.99099, vali: 1.65279, 18.31944, 0.66219, 0.81432\n","INFO:tensorflow:./scratch/minmax_L2/best-pre1 is not in all_model_checkpoint_paths. Manually adding it.\n","INFO:tensorflow:./scratch/minmax_L2/best-pre2 is not in all_model_checkpoint_paths. Manually adding it.\n","INFO:tensorflow:./scratch/minmax_L2/most_recent_iter is not in all_model_checkpoint_paths. Manually adding it.\n","2021-10-21 06:38:11: itr1500, training: -0.52982, 17.18512, 0.98361, 0.99180, vali: 1.65443, 18.32107, 0.66219, 0.81655\n","INFO:tensorflow:./scratch/minmax_L2/best-pre2 is not in all_model_checkpoint_paths. Manually adding it.\n","INFO:tensorflow:./scratch/minmax_L2/most_recent_iter is not in all_model_checkpoint_paths. Manually adding it.\n","2021-10-21 06:38:15: itr1600, training: -0.55470, 18.16268, 0.96774, 0.97581, vali: 1.65577, 18.32242, 0.65996, 0.81655\n","INFO:tensorflow:./scratch/minmax_L2/most_recent_iter is not in all_model_checkpoint_paths. Manually adding it.\n","2021-10-21 06:38:18: itr1700, training: -0.60629, 19.20172, 0.95000, 0.95000, vali: 1.65788, 18.32453, 0.66443, 0.81655\n","INFO:tensorflow:./scratch/minmax_L2/best-pre1 is not in all_model_checkpoint_paths. Manually adding it.\n","INFO:tensorflow:./scratch/minmax_L2/most_recent_iter is not in all_model_checkpoint_paths. Manually adding it.\n","2021-10-21 06:38:22: itr1800, training: -0.57518, 18.65440, 0.96694, 0.97521, vali: 1.65932, 18.32597, 0.66219, 0.81879\n","INFO:tensorflow:./scratch/minmax_L2/best-pre2 is not in all_model_checkpoint_paths. Manually adding it.\n","INFO:tensorflow:./scratch/minmax_L2/most_recent_iter is not in all_model_checkpoint_paths. Manually adding it.\n","2021-10-21 06:38:26: itr1900, training: -0.53885, 17.52446, 0.98246, 0.99123, vali: 1.66100, 18.32764, 0.66219, 0.81879\n","INFO:tensorflow:./scratch/minmax_L2/most_recent_iter is not in all_model_checkpoint_paths. Manually adding it.\n","2021-10-21 06:38:29: itr2000, training: -0.53044, 18.90793, 0.97561, 0.98374, vali: 1.66162, 18.32827, 0.66219, 0.81879\n","INFO:tensorflow:./scratch/minmax_L2/most_recent_iter is not in all_model_checkpoint_paths. Manually adding it.\n","2021-10-21 06:38:33: itr2100, training: -0.56982, 18.14847, 0.97500, 0.98333, vali: 1.66299, 18.32964, 0.66219, 0.81879\n","INFO:tensorflow:./scratch/minmax_L2/most_recent_iter is not in all_model_checkpoint_paths. Manually adding it.\n","2021-10-21 06:38:36: itr2200, training: -0.57809, 18.67004, 0.98400, 0.98400, vali: 1.66416, 18.33081, 0.66219, 0.82103\n","INFO:tensorflow:./scratch/minmax_L2/best-pre2 is not in all_model_checkpoint_paths. Manually adding it.\n","INFO:tensorflow:./scratch/minmax_L2/most_recent_iter is not in all_model_checkpoint_paths. Manually adding it.\n","2021-10-21 06:38:40: itr2300, training: -0.54540, 18.55279, 0.98387, 0.99194, vali: 1.66503, 18.33168, 0.66219, 0.82327\n","INFO:tensorflow:./scratch/minmax_L2/best-pre2 is not in all_model_checkpoint_paths. Manually adding it.\n","INFO:tensorflow:./scratch/minmax_L2/most_recent_iter is not in all_model_checkpoint_paths. Manually adding it.\n","2021-10-21 06:38:44: itr2400, training: -0.52253, 18.11471, 0.97500, 0.99167, vali: 1.66578, 18.33243, 0.66219, 0.82327\n","INFO:tensorflow:./scratch/minmax_L2/most_recent_iter is not in all_model_checkpoint_paths. Manually adding it.\n","2021-10-21 06:38:47: itr2500, training: -0.52938, 18.34142, 0.96721, 0.96721, vali: 1.66572, 18.33237, 0.66219, 0.82327\n","INFO:tensorflow:./scratch/minmax_L2/most_recent_iter is not in all_model_checkpoint_paths. Manually adding it.\n","2021-10-21 06:38:51: itr2600, training: -0.52058, 19.54539, 0.97391, 0.98261, vali: 1.66563, 18.33228, 0.65996, 0.82327\n","INFO:tensorflow:./scratch/minmax_L2/most_recent_iter is not in all_model_checkpoint_paths. Manually adding it.\n","2021-10-21 06:38:54: itr2700, training: -0.57997, 18.05580, 0.97391, 0.99130, vali: 1.66636, 18.33301, 0.65772, 0.82327\n","INFO:tensorflow:./scratch/minmax_L2/most_recent_iter is not in all_model_checkpoint_paths. Manually adding it.\n","2021-10-21 06:38:58: itr2800, training: -0.50133, 18.18809, 1.00000, 1.00000, vali: 1.66667, 18.33332, 0.65996, 0.82327\n","INFO:tensorflow:./scratch/minmax_L2/most_recent_iter is not in all_model_checkpoint_paths. Manually adding it.\n","2021-10-21 06:39:01: itr2900, training: -0.56055, 19.66512, 0.96721, 0.97541, vali: 1.66686, 18.33351, 0.65772, 0.82550\n","INFO:tensorflow:./scratch/minmax_L2/best-pre2 is not in all_model_checkpoint_paths. Manually adding it.\n","INFO:tensorflow:./scratch/minmax_L2/most_recent_iter is not in all_model_checkpoint_paths. Manually adding it.\n","2021-10-21 06:39:05: itr3000, training: -0.63166, 18.27195, 0.95652, 0.97391, vali: 1.66745, 18.33410, 0.65772, 0.82550\n","INFO:tensorflow:./scratch/minmax_L2/most_recent_iter is not in all_model_checkpoint_paths. Manually adding it.\n","2021-10-21 06:39:09: itr3100, training: -0.50090, 19.38037, 1.00000, 1.00000, vali: 1.66824, 18.33489, 0.65996, 0.82550\n","INFO:tensorflow:./scratch/minmax_L2/most_recent_iter is not in all_model_checkpoint_paths. Manually adding it.\n","2021-10-21 06:39:12: itr3200, training: -0.54834, 18.32218, 0.97321, 0.98214, vali: 1.66819, 18.33483, 0.65996, 0.82550\n","INFO:tensorflow:./scratch/minmax_L2/most_recent_iter is not in all_model_checkpoint_paths. Manually adding it.\n","2021-10-21 06:39:16: itr3300, training: -0.52781, 17.53323, 0.98305, 1.00000, vali: 1.66775, 18.33440, 0.65996, 0.82550\n","INFO:tensorflow:./scratch/minmax_L2/most_recent_iter is not in all_model_checkpoint_paths. Manually adding it.\n","2021-10-21 06:39:19: itr3400, training: -0.53485, 18.05005, 0.99160, 0.99160, vali: 1.66780, 18.33445, 0.65772, 0.82327\n","INFO:tensorflow:./scratch/minmax_L2/most_recent_iter is not in all_model_checkpoint_paths. Manually adding it.\n","2021-10-21 06:39:23: itr3500, training: -0.58746, 18.00185, 0.96667, 0.97500, vali: 1.66792, 18.33457, 0.65996, 0.82327\n","INFO:tensorflow:./scratch/minmax_L2/most_recent_iter is not in all_model_checkpoint_paths. Manually adding it.\n","2021-10-21 06:39:26: itr3600, training: -0.56236, 18.55064, 0.96639, 0.97479, vali: 1.66786, 18.33451, 0.65772, 0.82327\n","INFO:tensorflow:./scratch/minmax_L2/most_recent_iter is not in all_model_checkpoint_paths. Manually adding it.\n","2021-10-21 06:39:30: itr3700, training: -0.51647, 19.16045, 0.99138, 0.99138, vali: 1.66722, 18.33387, 0.65772, 0.82327\n","INFO:tensorflow:./scratch/minmax_L2/most_recent_iter is not in all_model_checkpoint_paths. Manually adding it.\n","2021-10-21 06:39:33: itr3800, training: -0.53260, 17.48034, 0.99194, 0.99194, vali: 1.66652, 18.33317, 0.65772, 0.82327\n","INFO:tensorflow:./scratch/minmax_L2/most_recent_iter is not in all_model_checkpoint_paths. Manually adding it.\n","2021-10-21 06:39:37: itr3900, training: -0.56512, 17.70387, 0.96610, 0.98305, vali: 1.66638, 18.33303, 0.65772, 0.82327\n","INFO:tensorflow:./scratch/minmax_L2/most_recent_iter is not in all_model_checkpoint_paths. Manually adding it.\n","2021-10-21 06:39:40: itr4000, training: -0.58674, 17.74756, 0.97479, 0.98319, vali: 1.66625, 18.33290, 0.65772, 0.82327\n","INFO:tensorflow:./scratch/minmax_L2/most_recent_iter is not in all_model_checkpoint_paths. Manually adding it.\n","2021-10-21 06:39:44: itr4100, training: -0.52532, 19.45704, 0.97638, 0.97638, vali: 1.66553, 18.33218, 0.65772, 0.82327\n","INFO:tensorflow:./scratch/minmax_L2/most_recent_iter is not in all_model_checkpoint_paths. Manually adding it.\n","2021-10-21 06:39:48: itr4200, training: -0.62433, 17.74587, 0.95763, 0.97458, vali: 1.66537, 18.33202, 0.65772, 0.82327\n","INFO:tensorflow:./scratch/minmax_L2/most_recent_iter is not in all_model_checkpoint_paths. Manually adding it.\n","2021-10-21 06:39:51: itr4300, training: -0.60127, 17.62124, 0.96460, 0.96460, vali: 1.66495, 18.33160, 0.65772, 0.82327\n","INFO:tensorflow:./scratch/minmax_L2/most_recent_iter is not in all_model_checkpoint_paths. Manually adding it.\n","2021-10-21 06:39:54: itr4400, training: -0.54567, 19.30546, 0.96774, 0.97581, vali: 1.66376, 18.33041, 0.65772, 0.82103\n","INFO:tensorflow:./scratch/minmax_L2/most_recent_iter is not in all_model_checkpoint_paths. Manually adding it.\n","2021-10-21 06:39:58: itr4500, training: -0.52169, 18.79628, 0.99180, 0.99180, vali: 1.66256, 18.32921, 0.65772, 0.82103\n","INFO:tensorflow:./scratch/minmax_L2/most_recent_iter is not in all_model_checkpoint_paths. Manually adding it.\n","2021-10-21 06:40:01: itr4600, training: -0.58111, 18.74997, 0.96460, 0.98230, vali: 1.66171, 18.32836, 0.65772, 0.82103\n","INFO:tensorflow:./scratch/minmax_L2/most_recent_iter is not in all_model_checkpoint_paths. Manually adding it.\n","2021-10-21 06:40:05: itr4700, training: -0.52880, 19.17885, 0.99138, 0.99138, vali: 1.66046, 18.32711, 0.65772, 0.82103\n","INFO:tensorflow:./scratch/minmax_L2/most_recent_iter is not in all_model_checkpoint_paths. Manually adding it.\n","2021-10-21 06:40:09: itr4800, training: -0.49726, 18.17569, 0.97541, 0.99180, vali: 1.66002, 18.32667, 0.65996, 0.82103\n","INFO:tensorflow:./scratch/minmax_L2/most_recent_iter is not in all_model_checkpoint_paths. Manually adding it.\n","2021-10-21 06:40:13: itr4900, training: -0.51686, 18.48265, 0.99074, 0.99074, vali: 1.65879, 18.32544, 0.65772, 0.82103\n","INFO:tensorflow:./scratch/minmax_L2/most_recent_iter is not in all_model_checkpoint_paths. Manually adding it.\n","2021-10-21 06:40:16, start prepare test data\n","2021-10-21 06:40:16, prepare test data end\n","INFO:tensorflow:Restoring parameters from ./scratch/minmax_L2/best-pre1\n","test!!!best-pre1!!!, test: 0.72082, vali: 0.66443\n","INFO:tensorflow:Restoring parameters from ./scratch/minmax_L2/best-pre2\n","test!!!best-pre2!!!, test: 0.86041, vali: 0.82550\n"]}]},{"cell_type":"code","metadata":{"id":"3PzkrNgHjTWr"},"source":["!apt-get install tree"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"t6nXYT-ZlG8D","executionInfo":{"status":"ok","timestamp":1634798479127,"user_tz":-330,"elapsed":529,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}},"outputId":"57014e9a-3ece-40df-bb1c-cee2b2b3757e"},"source":["!tree --du -h ."],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":[".\n","├── [ 20M]  scratch\n","│   ├── [1.4M]  best-loss.data-00000-of-00001\n","│   ├── [ 931]  best-loss.index\n","│   ├── [205K]  best-loss.meta\n","│   ├── [1.4M]  best-pre1.data-00000-of-00001\n","│   ├── [ 931]  best-pre1.index\n","│   ├── [205K]  best-pre1.meta\n","│   ├── [1.4M]  best-pre2.data-00000-of-00001\n","│   ├── [ 931]  best-pre2.index\n","│   ├── [205K]  best-pre2.meta\n","│   ├── [  75]  checkpoint\n","│   └── [ 15M]  minmax_L2\n","│       ├── [4.7M]  best-pre1.data-00000-of-00001\n","│       ├── [1.7K]  best-pre1.index\n","│       ├── [369K]  best-pre1.meta\n","│       ├── [4.7M]  best-pre2.data-00000-of-00001\n","│       ├── [1.7K]  best-pre2.index\n","│       ├── [369K]  best-pre2.meta\n","│       ├── [  89]  checkpoint\n","│       ├── [4.7M]  most_recent_iter.data-00000-of-00001\n","│       ├── [1.7K]  most_recent_iter.index\n","│       └── [369K]  most_recent_iter.meta\n","├── [5.0M]  yelp.pkl\n","├── [3.9K]  yelp-split.pkl\n","└── [443K]  yelp.txt\n","\n","  25M used in 2 directories, 23 files\n"]}]}]}