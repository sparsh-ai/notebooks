{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"2022-01-24-ml10m.ipynb","provenance":[{"file_id":"https://github.com/recohut/nbs/blob/main/raw/T293194%20%7C%20Random%2C%20Item%20Popularity%2C%20and%20Global%20Effects%20Recommender%20Models%20on%20ML-10m%20Dataset.ipynb","timestamp":1644665861007},{"file_id":"1aLE6E67pdJgWD3qqQB128zbSTwsTRTk5","timestamp":1636803737445},{"file_id":"1RRiC55q03vJg8vr-Fr9MbDVr8VHTN8XZ","timestamp":1636547305496}],"collapsed_sections":[],"toc_visible":true,"authorship_tag":"ABX9TyOFd7RE/XNY4y/RP5Hn8rbA"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Random, Item Popularity, and Global Effects Recommender Models on ML-10m Dataset"],"metadata":{"id":"WlLz323iuT4n"}},{"cell_type":"markdown","metadata":{"id":"CxiWmRiFzT2X"},"source":["## Setup"]},{"cell_type":"markdown","metadata":{"id":"fQ64dOFO0fJe"},"source":["### Git"]},{"cell_type":"code","metadata":{"id":"BaXpEKLdXNPa"},"source":["import os\n","project_name = \"general-recsys\"; branch = \"T434220\"; account = \"sparsh-ai\"\n","project_path = os.path.join('/content', branch)\n","\n","if not os.path.exists(project_path):\n","    !cp -r /content/drive/MyDrive/git_credentials/. ~\n","    !mkdir \"{project_path}\"\n","    %cd \"{project_path}\"\n","    !git init\n","    !git remote add origin https://github.com/\"{account}\"/\"{project_name}\".git\n","    !git pull origin \"{branch}\"\n","    !git checkout -b \"{branch}\"\n","else:\n","    %cd \"{project_path}\""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MZvPHRyMXdlS","executionInfo":{"status":"ok","timestamp":1636807859469,"user_tz":-330,"elapsed":11,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}},"outputId":"f0df3d88-9317-4192-db52-a406a836f35a"},"source":["%cd /content"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/content\n"]}]},{"cell_type":"code","metadata":{"id":"2eRcpGL6XfDs"},"source":["!cd /content/main && git add . && git commit -m 'commit' && git push origin \"{branch}\""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BXJY8c9d4Xi5"},"source":["### Installations"]},{"cell_type":"markdown","metadata":{"id":"GB_yDppW3_Yt"},"source":["### Imports"]},{"cell_type":"code","metadata":{"id":"vrEmNkAAsQlM"},"source":["from tqdm.notebook import tqdm\n","import sys\n","import os\n","import logging\n","from os import path as osp\n","from pathlib import Path\n","from urllib.request import urlretrieve\n","import zipfile\n","\n","import numpy as np\n","import pandas as pd\n","import scipy.sparse as sps\n","\n","import bz2\n","import pickle\n","import _pickle as cPickle\n","\n","import matplotlib.pyplot as plt"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NyxCtlrJ3_Ta"},"source":["### Params"]},{"cell_type":"code","metadata":{"id":"MXBwnUCD3_RD"},"source":["class Args:\n","    datapath_bronze = '/content'\n","    datapath_silver = '/content'\n","\n","    URM_train = None\n","    URM_test = None\n","\n","    top_k = 5\n","\n","args = Args()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Q40X4lHf4JHw"},"source":["### Logger"]},{"cell_type":"code","metadata":{"id":"cibwpV5L4JFb"},"source":["logging.basicConfig(stream=sys.stdout,\n","                    level = logging.DEBUG,\n","                    format='%(asctime)s [%(levelname)s] : %(message)s',\n","                    datefmt='%d-%b-%y %H:%M:%S')\n","\n","logger = logging.getLogger('Logger')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2M0-cN2ZzWE-"},"source":["## Modules"]},{"cell_type":"markdown","metadata":{"id":"qY9Y0q2sz1MS"},"source":["### Utils"]},{"cell_type":"code","metadata":{"id":"n0KPXS9jT9lx"},"source":["def save_pickle(data, path):\n"," with bz2.BZ2File(path + '.pbz2', 'w') as f: \n","    cPickle.dump(data, f)\n","\n","def load_pickle(path):\n","    data = bz2.BZ2File(path+'.pbz2', 'rb')\n","    data = cPickle.load(data)\n","    return data"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"X2u3zmqMXJhs"},"source":["### Dataset"]},{"cell_type":"code","metadata":{"id":"AGTVUdmtwWgZ"},"source":["def download_dataset():\n","    # If file exists, skip the download\n","    data_file_name = args.datapath_bronze + \"movielens_10m.zip\"\n","\n","    # If directory does not exist, create\n","    if not os.path.exists(args.datapath_bronze):\n","        os.makedirs(args.datapath_bronze)\n","\n","    if not os.path.exists(data_file_name):\n","        url_path = 'http://files.grouplens.org/datasets/movielens/ml-10m.zip'\n","        logger.info('Download ml-10m.zip file from {}'.format(url_path))\n","        urlretrieve(url_path, data_file_name)\n","\n","    dataFile = zipfile.ZipFile(args.datapath_bronze + \"movielens_10m.zip\")\n","    args.ratings_path = dataFile.extract(\"ml-10M100K/ratings.dat\", path=args.datapath_bronze)\n","    args.tags_path = dataFile.extract(\"ml-10M100K/tags.dat\", path=args.datapath_bronze)\n","    logger.info('Dataset downloaded in {}'.format(osp.join(args.datapath_bronze,'ml-10M100K')))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"vk93jRMwtEWP"},"source":["def preprocess_dataset():\n","    URM_all_dataframe = pd.read_csv(filepath_or_buffer=args.ratings_path, \n","                                    sep=\"::\", \n","                                    header=None, \n","                                    dtype={0:int, 1:int, 2:float, 3:int},\n","                                    engine='python')\n","\n","    URM_all_dataframe.columns = [\"UserID\", \"ItemID\", \"Interaction\", \"Timestamp\"]\n","    print(URM_all_dataframe.head())\n","\n","    print(\"The number of interactions is {}\".format(len(URM_all_dataframe)))\n","    print(\"We can use this data to create a sparse matrix, notice that we have red UserID and ItemID as int\")\n","    print(\"This is not always possible if the IDs are alphanumeric\")\n","    print(\"Now we can extract the list of unique user id and item id and display some statistics\")\n","\n","    userID_unique = URM_all_dataframe[\"UserID\"].unique()\n","    itemID_unique = URM_all_dataframe[\"ItemID\"].unique()\n","\n","    n_users = len(userID_unique)\n","    n_items = len(itemID_unique)\n","    n_interactions = len(URM_all_dataframe)\n","\n","    print(\"Number of items\\t {}, Number of users\\t {}\".format(n_items, n_users))\n","    print(\"Max ID items\\t {}, Max Id users\\t {}\\n\".format(max(itemID_unique), max(userID_unique)))\n","\n","    print(\"See that the max ID of items and users is higher than the number of unique values -> empty profiles\")\n","    print(\"We should remove empty indices, to do so we create a new mapping\")\n","\n","    user_original_ID_to_index_dict = {}\n","\n","    for user_id in userID_unique:\n","        user_original_ID_to_index_dict[user_id] = len(user_original_ID_to_index_dict)\n","    item_original_ID_to_index_dict = {}\n","\n","    for item_id in itemID_unique:\n","        item_original_ID_to_index_dict[item_id] = len(item_original_ID_to_index_dict)\n","    original_item_ID = 292\n","\n","    print(\"New index for item {} is {}\".format(original_item_ID, item_original_ID_to_index_dict[original_item_ID]))\n","\n","    print(\"We now replace the IDs in the dataframe and we are ready to use the data\")\n","\n","    URM_all_dataframe[\"UserID\"] = [user_original_ID_to_index_dict[user_original] for user_original in\n","                                        URM_all_dataframe[\"UserID\"].values]\n","\n","    URM_all_dataframe[\"ItemID\"] = [item_original_ID_to_index_dict[item_original] for item_original in \n","                                        URM_all_dataframe[\"ItemID\"].values]\n","    print(URM_all_dataframe.head())\n","\n","    userID_unique = URM_all_dataframe[\"UserID\"].unique()\n","    itemID_unique = URM_all_dataframe[\"ItemID\"].unique()\n","\n","    n_users = len(userID_unique)\n","    n_items = len(itemID_unique)\n","    n_interactions = len(URM_all_dataframe)\n","\n","    print(\"Number of items\\t {}, Number of users\\t {}\".format(n_items, n_users))\n","    print(\"Max ID items\\t {}, Max Id users\\t {}\\n\".format(max(itemID_unique), max(userID_unique)))\n","    print(\"Average interactions per user {:.2f}\".format(n_interactions/n_users))\n","    print(\"Average interactions per item {:.2f}\\n\".format(n_interactions/n_items))\n","\n","    print(\"Sparsity {:.2f} %\".format((1-float(n_interactions)/(n_items*n_users))*100))\n","\n","    print(\"To store the data we use a sparse matrix. We build it as a COO matrix and then change its format\")\n","    print(\"The COO constructor expects (data, (row, column))\")\n","\n","    URM_all = sps.coo_matrix((URM_all_dataframe[\"Interaction\"].values, \n","                            (URM_all_dataframe[\"UserID\"].values, URM_all_dataframe[\"ItemID\"].values)))\n","\n","    print(URM_all)\n","\n","    URM_all.tocsr()\n","\n","    print(\"We compute the item popularity as the number of interaction in each column\")\n","    print(\"We can use the properties of sparse matrices in CSC format\")\n","\n","    item_popularity = np.ediff1d(URM_all.tocsc().indptr)\n","    item_popularity = np.sort(item_popularity)\n","\n","    ten_percent = int(n_items/10)\n","\n","    print(\"Average per-item interactions over the whole dataset {:.2f}\".\n","        format(item_popularity.mean()))\n","\n","    print(\"Average per-item interactions for the top 10% popular items {:.2f}\".\n","        format(item_popularity[-ten_percent:].mean()))\n","\n","    print(\"Average per-item interactions for the least 10% popular items {:.2f}\".\n","        format(item_popularity[:ten_percent].mean()))\n","\n","    print(\"Average per-item interactions for the median 10% popular items {:.2f}\".\n","        format(item_popularity[int(n_items*0.45):int(n_items*0.55)].mean()))\n","\n","    print(\"Number of items with zero interactions {}\".\n","        format(np.sum(item_popularity==0)))\n","\n","    print(\"We compute the user activity (profile length) as the number of interaction in each row\")\n","    print(\"We can use the properties of sparse matrices in CSR format\")\n","\n","    user_activity = np.ediff1d(URM_all.tocsr().indptr)\n","    user_activity = np.sort(user_activity)\n","\n","    print(\"The splitting of the data is very important to ensure your algorithm is evaluated in a realistic scenario by using test it has never seen\")\n","\n","    train_test_split = 0.80\n","    n_interactions = URM_all.nnz\n","    train_mask = np.random.choice([True,False], n_interactions, p=[train_test_split, 1-train_test_split])\n","    URM_train = sps.csr_matrix((URM_all.data[train_mask],\n","                                (URM_all.row[train_mask], URM_all.col[train_mask])))\n","    test_mask = np.logical_not(train_mask)\n","    URM_test = sps.csr_matrix((URM_all.data[test_mask],\n","                                (URM_all.row[test_mask], URM_all.col[test_mask])))\n","\n","    save_pickle(URM_train,osp.join(args.datapath_silver,'URM_train'))\n","    save_pickle(URM_test,osp.join(args.datapath_silver,'URM_test'))\n","\n","\n","    ICM_dataframe = pd.read_csv(filepath_or_buffer=args.tags_path, \n","                                sep=\"::\", \n","                                header=None, \n","                                dtype={0:int, 1:int, 2:str, 3:int},\n","                                engine='python')\n","\n","    ICM_dataframe.columns = [\"UserID\", \"ItemID\", \"FeatureID\", \"Timestamp\"]\n","    print(ICM_dataframe.head())\n","\n","    print(\"We can see that most users and items have no data associated to them\")\n","\n","    n_features = len(ICM_dataframe[\"FeatureID\"].unique())\n","    print (\"Number of tags\\t {}, Number of item-tag tuples {}\".format(n_features, len(ICM_dataframe)))\n","\n","    print(\"We now build the sparse ICM matrices\")\n","    print(\"The tags are strings, we should traslate them into numbers so we can use them as indices in the ICM.\")\n","    print(\"We should also ensure that the item and user indices we use in ICM and URM are consistent. To do so we use the same mapper, first we populate it with the URM and then we add the new ids that appear only in the ICM\")\n","\n","    user_original_ID_to_index_dict = {}\n","\n","    for user_id in URM_all_dataframe[\"UserID\"].unique():\n","        user_original_ID_to_index_dict[user_id] = len(user_original_ID_to_index_dict)  \n","\n","    print(\"Unique user_id in the URM are {}\".format(len(user_original_ID_to_index_dict)))\n","        \n","    for user_id in ICM_dataframe[\"UserID\"].unique():\n","        if user_id not in user_original_ID_to_index_dict:\n","            user_original_ID_to_index_dict[user_id] = len(user_original_ID_to_index_dict)\n","            \n","    print(\"Unique user_id in the URM and ICM are {}\".format(len(user_original_ID_to_index_dict)))\n","\n","    item_original_ID_to_index_dict = {}\n","\n","    for item_id in URM_all_dataframe[\"ItemID\"].unique():\n","        item_original_ID_to_index_dict[item_id] = len(item_original_ID_to_index_dict)\n","\n","    print(\"Unique item_id in the URM are {}\".format(len(item_original_ID_to_index_dict)))\n","        \n","    for item_id in ICM_dataframe[\"ItemID\"].unique():\n","        if item_id not in item_original_ID_to_index_dict:\n","            item_original_ID_to_index_dict[item_id] = len(item_original_ID_to_index_dict)\n","            \n","    print(\"Unique item_id in the URM and ICM are {}\".format(len(item_original_ID_to_index_dict)))\n","\n","    feature_original_ID_to_index_dict = {}\n","\n","    for feature_id in ICM_dataframe[\"FeatureID\"].unique():\n","        feature_original_ID_to_index_dict[feature_id] = len(feature_original_ID_to_index_dict)\n","\n","    print(\"Unique feature_id in the URM are {}\".format(len(feature_original_ID_to_index_dict)))\n","\n","    original_feature_ID = \"star wars\"\n","    print(\"New index for feature '{}' is {}\".format(original_feature_ID, feature_original_ID_to_index_dict[original_feature_ID]))\n","\n","    print(\"We can now build the ICM using the new indices\")\n","\n","    URM_all_dataframe[\"UserID\"] = [user_original_ID_to_index_dict[user_original] for user_original in\n","                                        URM_all_dataframe[\"UserID\"].values]\n","\n","    URM_all_dataframe[\"ItemID\"] = [item_original_ID_to_index_dict[item_original] for item_original in \n","                                        URM_all_dataframe[\"ItemID\"].values]\n","\n","    ICM_dataframe[\"UserID\"] = [user_original_ID_to_index_dict[user_original] for user_original in\n","                                        ICM_dataframe[\"UserID\"].values]\n","\n","    ICM_dataframe[\"ItemID\"] = [item_original_ID_to_index_dict[item_original] for item_original in \n","                                        ICM_dataframe[\"ItemID\"].values]\n","\n","    ICM_dataframe[\"FeatureID\"] = [feature_original_ID_to_index_dict[feature_original] for feature_original in \n","                                        ICM_dataframe[\"FeatureID\"].values]\n","\n","    ICM_all = sps.csr_matrix((np.ones(len(ICM_dataframe[\"ItemID\"].values)), \n","                            (ICM_dataframe[\"ItemID\"].values, ICM_dataframe[\"FeatureID\"].values)),\n","                            shape = (n_items, n_features))\n","\n","    ICM_all.data = np.ones_like(ICM_all.data)\n","\n","    ICM_all = sps.csr_matrix(ICM_all)\n","    features_per_item = np.ediff1d(ICM_all.indptr)\n","\n","    ICM_all = sps.csc_matrix(ICM_all)\n","    items_per_feature = np.ediff1d(ICM_all.indptr)\n","\n","    ICM_all = sps.csr_matrix(ICM_all)\n","\n","    print(features_per_item.shape)\n","    print(items_per_feature.shape)\n","\n","    features_per_item = np.sort(features_per_item)\n","    items_per_feature = np.sort(items_per_feature)\n","\n","    save_pickle(ICM_all,osp.join(args.datapath_silver,'ICM_all'))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"cJ4YtoWVXw5Z"},"source":["def load_processed_datasets():\n","    args.URM_train = load_pickle(osp.join(args.datapath_silver,'URM_train'))\n","    args.URM_test = load_pickle(osp.join(args.datapath_silver,'URM_test'))\n","    args.ICM_all = load_pickle(osp.join(args.datapath_silver,'ICM_all'))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_F4vRpFCzYsf"},"source":["### Metrics"]},{"cell_type":"code","metadata":{"id":"DB4nqBeKXNE9"},"source":["def precision(recommended_items, relevant_items):\n","    is_relevant = np.in1d(recommended_items, relevant_items, assume_unique=True)\n","    precision_score = np.sum(is_relevant, dtype=np.float32) / len(is_relevant)\n","    return precision_score"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"UU4AFIvJXN3p"},"source":["def recall(recommended_items, relevant_items):\n","    is_relevant = np.in1d(recommended_items, relevant_items, assume_unique=True)\n","    recall_score = np.sum(is_relevant, dtype=np.float32) / relevant_items.shape[0]\n","    return recall_score"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"43FaRa8-XPeU"},"source":["def MAP(recommended_items, relevant_items):\n","    is_relevant = np.in1d(recommended_items, relevant_items, assume_unique=True)\n","    # Cumulative sum: precision at 1, at 2, at 3 ...\n","    p_at_k = is_relevant * np.cumsum(is_relevant, dtype=np.float32) / (1 + np.arange(is_relevant.shape[0]))\n","    map_score = np.sum(p_at_k) / np.min([relevant_items.shape[0], is_relevant.shape[0]])\n","    return map_score"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Sh_hYaMIbTav"},"source":["### Evaluation"]},{"cell_type":"code","metadata":{"id":"yFOx8syNbTYc"},"source":["def evaluate_algorithm(recommender_object):\n","    cumulative_precision = 0.0\n","    cumulative_recall = 0.0\n","    cumulative_MAP = 0.0\n","    num_eval = 0\n","\n","    for user_id in range(args.URM_test.shape[0]):\n","        relevant_items = args.URM_test.indices[args.URM_test.indptr[user_id]:args.URM_test.indptr[user_id+1]]\n","        \n","        if len(relevant_items)>0:\n","            recommended_items = recommender_object.recommend(user_id, at=args.top_k)\n","            num_eval+=1\n","            cumulative_precision += precision(recommended_items, relevant_items)\n","            cumulative_recall += recall(recommended_items, relevant_items)\n","            cumulative_MAP += MAP(recommended_items, relevant_items)\n","            \n","    cumulative_precision /= num_eval\n","    cumulative_recall /= num_eval\n","    cumulative_MAP /= num_eval\n","    \n","    print(\"Recommender results are: Precision = {:.4f}, Recall = {:.4f}, MAP = {:.4f}\".format(\n","        cumulative_precision, cumulative_recall, cumulative_MAP)) "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3DYlyMq1XWiL"},"source":["### Models"]},{"cell_type":"code","metadata":{"id":"p3whP2SwXWfx"},"source":["class RandomRecommender(object):\n","    \"\"\"In a random recommend we don't have anything to learn from the data\"\"\"\n","    def fit(self, URM_train):\n","        self.n_items = URM_train.shape[1]\n","    \n","    def recommend(self, user_id, at=5):\n","        recommended_items = np.random.choice(self.n_items, at)\n","        return recommended_items"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zvzmJlT6b83p"},"source":["class TopPopRecommender(object):\n","    \"\"\"We recommend to all users the most popular items, \n","    that is those with the highest number of interactions\n","    In this case our model is the item popularity\n","    \"\"\"\n","    def fit(self, URM_train):\n","        self.URM_train = URM_train\n","        item_popularity = np.ediff1d(URM_train.tocsc().indptr)\n","        # We are not interested in sorting the popularity value,\n","        # but to order the items according to it\n","        self.popular_items = np.argsort(item_popularity)\n","        self.popular_items = np.flip(self.popular_items, axis = 0)\n","    \n","    def recommend(self, user_id, at=5):\n","        if args.remove_seen:\n","            \"\"\"remove items already seen by the user. We can either remove them from the\n","            recommended item list or we can set them to a score so low that it will cause \n","            them to end at the very bottom of all the available items\"\"\"\n","            seen_items = self.URM_train.indices[self.URM_train.indptr[user_id]:self.URM_train.indptr[user_id+1]]\n","            unseen_items_mask = np.in1d(self.popular_items, seen_items,\n","                                        assume_unique=True, invert = True)\n","            unseen_items = self.popular_items[unseen_items_mask]\n","            recommended_items = unseen_items[0:at]\n","        else:\n","            recommended_items = self.popular_items[0:at]\n","        return recommended_items"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_X1m6ArKj4Ou"},"source":["class GlobalEffectsRecommender(object):\n","    \"\"\"We recommend to all users the highest rated items.\n","    First we compute the average of all ratings, or global average\n","    We subtract the bias to all ratings\n","    Then we compute the average rating for each item, or itemBias\n","    We cannot use the mean function because it would include also the zero values,\n","    which we want to exclude since they mean \"missing data\"\n","    The mean should be computed only on existing ratings. Unfortunately this \n","    requires to do several operations to change the data classes so the code \n","    is quite long. Also remember to exclude items with no ratings to avoid a \n","    division by zero.\n","    And the average rating for each user, or userBias\n","    Now we can sort the items by their itemBias and use the same recommendation \n","    principle as in TopPop.\n","    \"\"\"\n","    def fit(self, URM_train):\n","        self.URM_train = URM_train\n","        globalAverage = np.mean(URM_train.data)\n","        URM_train_unbiased = URM_train.copy()\n","        URM_train_unbiased.data -= globalAverage\n","        # User Bias\n","        user_mean_rating = URM_train_unbiased.mean(axis=1)\n","        user_mean_rating = np.array(user_mean_rating).squeeze()\n","        \n","        # In order to apply the user bias we have to change the rating value \n","        # in the URM_train_unbiased inner data structures\n","        # If we were to write:\n","        # URM_train_unbiased[user_id].data -= user_mean_rating[user_id]\n","        # we would change the value of a new matrix with no effect on the original data structure\n","        for user_id in range(len(user_mean_rating)):\n","            start_position = URM_train_unbiased.indptr[user_id]\n","            end_position = URM_train_unbiased.indptr[user_id+1]\n","            URM_train_unbiased.data[start_position:end_position] -= user_mean_rating[user_id]\n","        # Item Bias\n","        item_mean_rating = URM_train_unbiased.mean(axis=0)\n","        item_mean_rating = np.array(item_mean_rating).squeeze()\n","\n","        self.bestRatedItems = np.argsort(item_mean_rating)\n","        self.bestRatedItems = np.flip(self.bestRatedItems, axis = 0)\n","\n","    def recommend(self, user_id, at=5, remove_seen=True):\n","        if remove_seen:\n","            seen_items = self.URM_train.indices[self.URM_train.indptr[user_id]:self.URM_train.indptr[user_id+1]]\n","            unseen_items_mask = np.in1d(self.bestRatedItems, seen_items,\n","                                        assume_unique=True, invert = True)\n","            unseen_items = self.bestRatedItems[unseen_items_mask]\n","            recommended_items = unseen_items[0:at]\n","        else:\n","            recommended_items = self.bestRatedItems[0:at]\n","        return recommended_items"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2T4hsKM9Xnxa"},"source":["def train_model():\n","    if args.URM_train is None:\n","        load_processed_datasets()\n","\n","    if args.model_name in ['Random','ItemPopularity','GlobalEffects']:\n","        model_dict = {\n","            'Random':RandomRecommender(),\n","            'ItemPopularity':TopPopRecommender(),\n","            'GlobalEffects':GlobalEffectsRecommender(),\n","        }\n","    \n","        model = model_dict[args.model_name]\n","        model.fit(args.URM_train)\n","\n","        for user_id in range(10):\n","            print('As per {} model, User {} would prefer these {} items: {}'\n","            .format(args.model_name,\n","                    user_id,\n","                    args.top_k,\n","                    model.recommend(user_id, at=args.top_k)\n","            ))\n","        evaluate_algorithm(model)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"bDpG9ILPXWd3"},"source":["## Jobs"]},{"cell_type":"code","metadata":{"id":"2y8mdDjds6dr","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1636807914124,"user_tz":-330,"elapsed":2506,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}},"outputId":"d6d1a602-63d2-4b8d-ed8c-c028f8c9e1df"},"source":["logger.info('JOB START: DOWNLOAD_RAW_DATASET')\n","download_dataset()\n","logger.info('JOB END: DOWNLOAD_RAW_DATASET')"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["13-Nov-21 12:51:54 [INFO] : JOB START: DOWNLOAD_RAW_DATASET\n","13-Nov-21 12:51:56 [INFO] : Dataset downloaded in /content/ml-10M100K\n","13-Nov-21 12:51:56 [INFO] : JOB END: DOWNLOAD_RAW_DATASET\n"]}]},{"cell_type":"code","metadata":{"id":"3ig3tPpB2Fx-","colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"executionInfo":{"status":"ok","timestamp":1636808002136,"user_tz":-330,"elapsed":81836,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}},"outputId":"3d1fe2b1-9690-4b19-fb56-81d4368d555e"},"source":["logger.info('JOB START: PREPROCESSING_MOVIELENS_10M')\n","preprocess_dataset()\n","logger.info('JOB END: PREPROCESSING_MOVIELENS_10M')"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["13-Nov-21 12:52:03 [INFO] : JOB START: DATASET_CONVERSION_PARQUET_TO_CSV\n","   UserID  ItemID  Interaction  Timestamp\n","0       1     122          5.0  838985046\n","1       1     185          5.0  838983525\n","2       1     231          5.0  838983392\n","3       1     292          5.0  838983421\n","4       1     316          5.0  838983392\n","The number of interactions is 10000054\n","We can use this data to create a sparse matrix, notice that we have red UserID and ItemID as int\n","This is not always possible if the IDs are alphanumeric\n","Now we can extract the list of unique user id and item id and display some statistics\n","Number of items\t 10677, Number of users\t 69878\n","Max ID items\t 65133, Max Id users\t 71567\n","\n","See that the max ID of items and users is higher than the number of unique values -> empty profiles\n","We should remove empty indices, to do so we create a new mapping\n","New index for item 292 is 3\n","We now replace the IDs in the dataframe and we are ready to use the data\n","   UserID  ItemID  Interaction  Timestamp\n","0       0       0          5.0  838985046\n","1       0       1          5.0  838983525\n","2       0       2          5.0  838983392\n","3       0       3          5.0  838983421\n","4       0       4          5.0  838983392\n","Number of items\t 10677, Number of users\t 69878\n","Max ID items\t 10676, Max Id users\t 69877\n","\n","Average interactions per user 143.11\n","Average interactions per item 936.60\n","\n","Sparsity 98.66 %\n","To store the data we use a sparse matrix. We build it as a COO matrix and then change its format\n","The COO constructor expects (data, (row, column))\n","  (0, 0)\t5.0\n","  (0, 1)\t5.0\n","  (0, 2)\t5.0\n","  (0, 3)\t5.0\n","  (0, 4)\t5.0\n","  (0, 5)\t5.0\n","  (0, 6)\t5.0\n","  (0, 7)\t5.0\n","  (0, 8)\t5.0\n","  (0, 9)\t5.0\n","  (0, 10)\t5.0\n","  (0, 11)\t5.0\n","  (0, 12)\t5.0\n","  (0, 13)\t5.0\n","  (0, 14)\t5.0\n","  (0, 15)\t5.0\n","  (0, 16)\t5.0\n","  (0, 17)\t5.0\n","  (0, 18)\t5.0\n","  (0, 19)\t5.0\n","  (0, 20)\t5.0\n","  (0, 21)\t5.0\n","  (1, 22)\t5.0\n","  (1, 23)\t3.0\n","  (1, 24)\t5.0\n","  :\t:\n","  (69877, 445)\t2.0\n","  (69877, 463)\t3.0\n","  (69877, 467)\t1.0\n","  (69877, 468)\t4.0\n","  (69877, 191)\t3.0\n","  (69877, 3386)\t3.0\n","  (69877, 475)\t2.0\n","  (69877, 246)\t4.0\n","  (69877, 1646)\t3.0\n","  (69877, 481)\t3.0\n","  (69877, 1081)\t2.0\n","  (69877, 248)\t4.0\n","  (69877, 486)\t4.0\n","  (69877, 2941)\t1.0\n","  (69877, 3448)\t1.0\n","  (69877, 3066)\t1.0\n","  (69877, 5330)\t1.0\n","  (69877, 2065)\t1.0\n","  (69877, 505)\t3.0\n","  (69877, 192)\t5.0\n","  (69877, 518)\t1.0\n","  (69877, 1660)\t2.0\n","  (69877, 537)\t5.0\n","  (69877, 541)\t2.0\n","  (69877, 1671)\t2.0\n","We compute the item popularity as the number of interaction in each column\n","We can use the properties of sparse matrices in CSC format\n","Average per-item interactions over the whole dataset 936.60\n","Average per-item interactions for the top 10% popular items 6479.52\n","Average per-item interactions for the least 10% popular items 5.23\n","Average per-item interactions for the median 10% popular items 136.45\n","Number of items with zero interactions 0\n","We compute the user activity (profile length) as the number of interaction in each row\n","We can use the properties of sparse matrices in CSR format\n","The splitting of the data is very important to ensure your algorithm is evaluated in a realistic scenario by using test it has never seen\n","13-Nov-21 12:53:24 [INFO] : JOB END: DATASET_CONVERSION_PARQUET_TO_CSV\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IHUfmN7yWsWe","executionInfo":{"status":"ok","timestamp":1636809680687,"user_tz":-330,"elapsed":10431,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}},"outputId":"0e35945a-b097-45c5-e55e-1e387e869299"},"source":["logger.info('JOB START: TRAINING_RANDOM_RECOMMENDER_MODEL')\n","args.model_name = 'Random'\n","train_model()\n","logger.info('JOB END: TRAINING_RANDOM_RECOMMENDER_MODEL')"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["13-Nov-21 13:21:13 [INFO] : JOB START: TRAINING_RANDOM_RECOMMENDER_MODEL\n","As per Random model, User 0 would prefer these 5 items: [6823 8976 2753 5195  235]\n","As per Random model, User 1 would prefer these 5 items: [ 2633  9861 10003  4393  5093]\n","As per Random model, User 2 would prefer these 5 items: [1696  346 8495 1385 2402]\n","As per Random model, User 3 would prefer these 5 items: [8465 2700 5098 6802 4291]\n","As per Random model, User 4 would prefer these 5 items: [4559 4637 4954 8351 9438]\n","As per Random model, User 5 would prefer these 5 items: [8178 1158 2202 7470 8215]\n","As per Random model, User 6 would prefer these 5 items: [9464 8398 3497 4266 5924]\n","As per Random model, User 7 would prefer these 5 items: [6223 6330 4248   52  696]\n","As per Random model, User 8 would prefer these 5 items: [6742 3685 2953 9405 2831]\n","As per Random model, User 9 would prefer these 5 items: [2901 7799 7278  847 9579]\n","Recommender results are: Precision = 0.0028, Recall = 0.0005, MAP = 0.0013\n","13-Nov-21 13:21:23 [INFO] : JOB END: TRAINING_RANDOM_RECOMMENDER_MODEL\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OizdBFOLdYZb","executionInfo":{"status":"ok","timestamp":1636809974793,"user_tz":-330,"elapsed":9347,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}},"outputId":"396bb042-9166-44c6-a1f9-ea7cf332589b"},"source":["logger.info('JOB START: ITEM_POP_RECOMMENDER_MODEL')\n","args.model_name = 'ItemPopularity'\n","args.remove_seen = False\n","train_model()\n","logger.info('JOB END: ITEM_POP_RECOMMENDER_MODEL')"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["13-Nov-21 13:26:08 [INFO] : JOB START: ITEM_POP_RECOMMENDER_MODEL\n","As per ItemPopularity model, User 0 would prefer these 5 items: [1008    7  139   14 1293]\n","As per ItemPopularity model, User 1 would prefer these 5 items: [1008    7  139   14 1293]\n","As per ItemPopularity model, User 2 would prefer these 5 items: [1008    7  139   14 1293]\n","As per ItemPopularity model, User 3 would prefer these 5 items: [1008    7  139   14 1293]\n","As per ItemPopularity model, User 4 would prefer these 5 items: [1008    7  139   14 1293]\n","As per ItemPopularity model, User 5 would prefer these 5 items: [1008    7  139   14 1293]\n","As per ItemPopularity model, User 6 would prefer these 5 items: [1008    7  139   14 1293]\n","As per ItemPopularity model, User 7 would prefer these 5 items: [1008    7  139   14 1293]\n","As per ItemPopularity model, User 8 would prefer these 5 items: [1008    7  139   14 1293]\n","As per ItemPopularity model, User 9 would prefer these 5 items: [1008    7  139   14 1293]\n","Recommender results are: Precision = 0.0954, Recall = 0.0305, MAP = 0.0527\n","13-Nov-21 13:26:17 [INFO] : JOB END: ITEM_POP_RECOMMENDER_MODEL\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1i-hLBOTeUi2","executionInfo":{"status":"ok","timestamp":1636810062749,"user_tz":-330,"elapsed":55117,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}},"outputId":"bb01c9fb-983d-41fe-9cb4-7acc74853a9a"},"source":["logger.info('JOB START: ITEM_POP_WITH_FILTER_RECOMMENDER_MODEL')\n","args.model_name = 'ItemPopularity'\n","args.remove_seen = True\n","train_model()\n","logger.info('JOB END: ITEM_POP_WITH_FILTER_RECOMMENDER_MODEL')"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["13-Nov-21 13:26:49 [INFO] : JOB START: ITEM_POP_WITH_FILTER_RECOMMENDER_MODEL\n","As per ItemPopularity model, User 0 would prefer these 5 items: [1008  139 1293   22   19]\n","As per ItemPopularity model, User 1 would prefer these 5 items: [1008    7  139   14 1293]\n","As per ItemPopularity model, User 2 would prefer these 5 items: [1008    7  139   14 1293]\n","As per ItemPopularity model, User 3 would prefer these 5 items: [1008    7  139 1293  175]\n","As per ItemPopularity model, User 4 would prefer these 5 items: [1008    7   14 1293   22]\n","As per ItemPopularity model, User 5 would prefer these 5 items: [1008    7  139   14 1293]\n","As per ItemPopularity model, User 6 would prefer these 5 items: [1008    7   14 1293   22]\n","As per ItemPopularity model, User 7 would prefer these 5 items: [1008    7 1293   22   19]\n","As per ItemPopularity model, User 8 would prefer these 5 items: [1008    7  139   14 1293]\n","As per ItemPopularity model, User 9 would prefer these 5 items: [  14 1293   22   19  175]\n","Recommender results are: Precision = 0.1980, Recall = 0.0529, MAP = 0.1468\n","13-Nov-21 13:27:44 [INFO] : JOB END: ITEM_POP_WITH_FILTER_RECOMMENDER_MODEL\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Taj8oNKcef2O","executionInfo":{"status":"ok","timestamp":1636811920145,"user_tz":-330,"elapsed":50742,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}},"outputId":"b1543dfd-b0f1-486c-dca9-9940ad18a4f8"},"source":["logger.info('JOB START: GLOBAL_EFFECTS_RECOMMENDER_MODEL')\n","args.model_name = 'GlobalEffects'\n","args.remove_seen = True\n","train_model()\n","logger.info('JOB END: GLOBAL_EFFECTS_RECOMMENDER_MODEL')"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["13-Nov-21 13:57:52 [INFO] : JOB START: GLOBAL_EFFECTS_RECOMMENDER_MODEL\n","As per GlobalEffects model, User 0 would prefer these 5 items: [1293  139 1008  133  213]\n","As per GlobalEffects model, User 1 would prefer these 5 items: [1293  139 1008  133  213]\n","As per GlobalEffects model, User 2 would prefer these 5 items: [1293  139 1008  133  213]\n","As per GlobalEffects model, User 3 would prefer these 5 items: [1293  139 1008  133  213]\n","As per GlobalEffects model, User 4 would prefer these 5 items: [1293 1008  213   24    7]\n","As per GlobalEffects model, User 5 would prefer these 5 items: [1293  139 1008  133  213]\n","As per GlobalEffects model, User 6 would prefer these 5 items: [1293 1008  133   34    7]\n","As per GlobalEffects model, User 7 would prefer these 5 items: [1293 1008  133   34    7]\n","As per GlobalEffects model, User 8 would prefer these 5 items: [1293  139 1008  133   24]\n","As per GlobalEffects model, User 9 would prefer these 5 items: [1293  213   24   34   22]\n","Recommender results are: Precision = 0.1684, Recall = 0.0387, MAP = 0.1220\n","13-Nov-21 13:58:42 [INFO] : JOB END: GLOBAL_EFFECTS_RECOMMENDER_MODEL\n"]}]}]}