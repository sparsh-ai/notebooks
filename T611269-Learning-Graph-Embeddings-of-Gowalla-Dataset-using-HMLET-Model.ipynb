{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"T611269 | Learning Graph Embeddings of Gowalla Dataset using HMLET Model","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyMfS//BG15L8yWqxetbFAkJ"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"912c1c27df2645d39444fb31d580b6e9":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_8345b2752ac848d8bd2eeb5fa87452db","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_14fca280eba6402494cdc2bd805bad85","IPY_MODEL_9a5c8c1781ae4affb72f4aed8ecbaa9b","IPY_MODEL_95a586c260424696957c8428ef46f81a"]}},"8345b2752ac848d8bd2eeb5fa87452db":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"14fca280eba6402494cdc2bd805bad85":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_e26fbc49e7c3437b8d6a2946ffa446d5","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"100%","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_a36ec965a3df4d16a950869c120a4120"}},"9a5c8c1781ae4affb72f4aed8ecbaa9b":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_34f38bddc8d4465da785b875bb4178fa","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":396,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":396,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_8bf54d0dfabd4aea849ce964cfa89a28"}},"95a586c260424696957c8428ef46f81a":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_19758a2fd7d54c939132c08eaba49427","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 396/396 [08:39&lt;00:00,  1.31s/it]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_b51a8eab1ea84735b435a6f786ed2008"}},"e26fbc49e7c3437b8d6a2946ffa446d5":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"a36ec965a3df4d16a950869c120a4120":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"34f38bddc8d4465da785b875bb4178fa":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"8bf54d0dfabd4aea849ce964cfa89a28":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"19758a2fd7d54c939132c08eaba49427":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"b51a8eab1ea84735b435a6f786ed2008":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"d5c0fc6dcfba42ff91ec853655866888":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_a070050fb1cd45b99fcc10b05f280f86","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_7c4f106b503d41e5adc491fab41e2689","IPY_MODEL_1d7b32e3299c4f069fcef39433f381d1","IPY_MODEL_b034587a2c0741b4ba7cb9275986d767"]}},"a070050fb1cd45b99fcc10b05f280f86":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"7c4f106b503d41e5adc491fab41e2689":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_8c6bf77ea39d48138f5b57a9188cdbe8","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"100%","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_92689a3815bc4ff5972c792d5ae927ce"}},"1d7b32e3299c4f069fcef39433f381d1":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_45ecfbe2d4194683962fdea8840ec448","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":396,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":396,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_ca3519631d5a4a82a11c7692e3f1cd04"}},"b034587a2c0741b4ba7cb9275986d767":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_a56ccd7e2a6b45839ae5847e24aa611f","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 396/396 [08:39&lt;00:00,  1.31s/it]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_bfbc8a93e6cd436abf843b0965b63a93"}},"8c6bf77ea39d48138f5b57a9188cdbe8":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"92689a3815bc4ff5972c792d5ae927ce":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"45ecfbe2d4194683962fdea8840ec448":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"ca3519631d5a4a82a11c7692e3f1cd04":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"a56ccd7e2a6b45839ae5847e24aa611f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"bfbc8a93e6cd436abf843b0965b63a93":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"0ef4fced1ee84b2eabea4d7f5bf9e40f":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_bfcf80694dee450d81b6b593a1a83a94","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_ecdc27d9ad454b43990aa87333843b6a","IPY_MODEL_271897dd6cb6449e8d1d6d8354d6327a","IPY_MODEL_ed234409a0864d589f727482c7c1b89d"]}},"bfcf80694dee450d81b6b593a1a83a94":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"ecdc27d9ad454b43990aa87333843b6a":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_b10e93e426b64d0ba208fec34851f30c","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"100%","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_18a56d0cef044affbf5315e983d46977"}},"271897dd6cb6449e8d1d6d8354d6327a":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_90f18d27ec964b11b2ed0dc6cdbc072b","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":396,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":396,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_e97daa5463fd4da59442e17ffc02c27a"}},"ed234409a0864d589f727482c7c1b89d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_c18d6b1b980342669d25a89951aa25db","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 396/396 [08:40&lt;00:00,  1.31s/it]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_b39dcd7c2add4224980b51b662a4e994"}},"b10e93e426b64d0ba208fec34851f30c":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"18a56d0cef044affbf5315e983d46977":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"90f18d27ec964b11b2ed0dc6cdbc072b":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"e97daa5463fd4da59442e17ffc02c27a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"c18d6b1b980342669d25a89951aa25db":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"b39dcd7c2add4224980b51b662a4e994":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"7e323e37c96145f3a6c95eab137b15c0":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_b6d78f8875ff40b08341e66bf3c0ddc0","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_2dc4426c29ad4a6da8ad32028b715fa5","IPY_MODEL_c51ebb88de2a4767a1c90019dd701c68","IPY_MODEL_b71bff74949343e992d23878966be450"]}},"b6d78f8875ff40b08341e66bf3c0ddc0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"2dc4426c29ad4a6da8ad32028b715fa5":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_1cf296ab5a564ab4b4d2d8abcbff9310","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"100%","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_c6ba6b3191a54b0fae0c785655402aed"}},"c51ebb88de2a4767a1c90019dd701c68":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_aef511f34d534675be13090e4cb2c484","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":396,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":396,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_5138422f413c4acf8cd9cb763fff7b41"}},"b71bff74949343e992d23878966be450":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_6888aa3b35e14033848e8c63d1edd740","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 396/396 [08:41&lt;00:00,  1.32s/it]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_2c1e50c7114f4a6697f6eddddcdc9146"}},"1cf296ab5a564ab4b4d2d8abcbff9310":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"c6ba6b3191a54b0fae0c785655402aed":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"aef511f34d534675be13090e4cb2c484":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"5138422f413c4acf8cd9cb763fff7b41":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"6888aa3b35e14033848e8c63d1edd740":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"2c1e50c7114f4a6697f6eddddcdc9146":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}}},"cells":[{"cell_type":"markdown","metadata":{"id":"NtgN996tgfc6"},"source":["# Learning Graph Embeddings of Gowalla Dataset using HMLET Model"]},{"cell_type":"markdown","metadata":{"id":"BXJY8c9d4Xi5"},"source":["## Data Ingestion"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DctyNOSdx-7h","executionInfo":{"status":"ok","timestamp":1637227997776,"user_tz":-330,"elapsed":4696,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}},"outputId":"69b1c651-a356-4e5d-d1b7-087a806ed1f1"},"source":["!mkdir -p /content/data/gowalla\n","%cd /content/data/gowalla\n","!wget -q --show-progress https://github.com/RecoHut-Datasets/gowalla/raw/main/silver/v1/s_pre_adj_mat_train.npz\n","!wget -q --show-progress https://github.com/RecoHut-Datasets/gowalla/raw/main/silver/v1/train.txt\n","!wget -q --show-progress https://github.com/RecoHut-Datasets/gowalla/raw/main/silver/v1/test.txt\n","!wget -q --show-progress https://github.com/RecoHut-Datasets/gowalla/raw/main/silver/v1/val.txt\n","%cd /content"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/data/gowalla\n","s_pre_adj_mat_train 100%[===================>]   6.89M  --.-KB/s    in 0.08s   \n","train.txt           100%[===================>]   4.42M  --.-KB/s    in 0.08s   \n","test.txt            100%[===================>] 752.53K  --.-KB/s    in 0.04s   \n","val.txt             100%[===================>] 751.95K  --.-KB/s    in 0.04s   \n","/content\n"]}]},{"cell_type":"markdown","metadata":{"id":"GB_yDppW3_Yt"},"source":["## Imports"]},{"cell_type":"code","metadata":{"id":"vrEmNkAAsQlM"},"source":["import numpy as np\n","from tqdm.notebook import tqdm\n","import sys\n","import os\n","import math\n","import logging\n","import pandas as pd\n","from pathlib import Path\n","from os.path import join, dirname\n","import multiprocessing\n","from scipy.sparse import csr_matrix\n","import scipy.sparse as sp\n","from time import time\n","import time as tm\n","import random\n","from sklearn.metrics import roc_auc_score\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.autograd import Variable\n","from torch.utils.data import Dataset, DataLoader\n","from torch import log\n","from torch import nn, optim\n","from torch.utils.tensorboard import SummaryWriter"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NyxCtlrJ3_Ta"},"source":["## Params"]},{"cell_type":"code","metadata":{"id":"MXBwnUCD3_RD"},"source":["class Args:\n","\n","    # Model\n","    model = 'HMLET_End' # \"model type\", choices={HMLET_End\", \"HMLET_Middle\", \"HMLET_Front\", \"HMLET_All\"}\n","    embedding_dim = 512 # the embedding size\n","    non_linear_acti = 'elu' # activation function to use in non-linear aggregation, choices={\"relu\", \"leaky-relu\", \"elu\"}\n","    dropout = 1 # using the dropout or not\n","    keepprob = 0.6 # dropout node keeping probability\n","                            \n","    # Dataset\n","    dataset = 'gowalla' # dataset, choices={\"gowalla\", \"yelp2018\", \"amazon-book\"\n","    bpr_batch = 2048 # the batch size for bpr loss training procedure\n","\n","    # Gumbel-Softmax\n","    ori_temp = 0.7 # start temperature\n","    min_temp = 0.01 # min temperature\n","    gum_temp_decay = 0.005 # value of temperature decay\n","    epoch_temp_decay = 1 # epoch to apply temperature decay\n","    division_noise = 3 # division number of noise\n","                            \n","    # Train\n","    # epochs = 1000 # train epochs\n","    epochs = 4 # train epochs\n","    lr = 0.001 # the learning rate\n","    decay = 1e-4 # the weight decay for l2 normalizaton\n","                                    \n","    # Test\n","    topks = \"[10,20,30,40,50]\" # at-k test list\n","    testbatch = 100 # the batch size of users for testing\n","    a_split = 0 # split large adj matrix or not\n","    a_n_fold = 100 # the fold num used to split large adj matrix\n","                            \n","    # Util\n","    root_path = '/content'\n","    pretrain = 0 # using pretrained weight or not\n","    pretrained_checkpoint_name = '' # file name of pretrained model\n","    load_epoch = 1 # epoch of pretrained model\n","    seed = 2020 # random seed\n","    multicore = 1 # help using multiprocessing or not\n","    gpu_num = 0 # gpu number\n","    save_checkpoints_path = \"checkpoints\" # path to save weights\n","    save_excel_path = \"excel\" # path to save eval files\n","    tensorboard = 1 # enable tensorboard\n","\n","args = Args()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kaJFxf6d1JDf","executionInfo":{"status":"ok","timestamp":1637229242415,"user_tz":-330,"elapsed":676,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}},"outputId":"82e8415d-a10e-41c9-ce9d-003c8da12279"},"source":["dataset = args.dataset\n","model_name = args.model\n","\n","# Model & Train Param\n","EPOCHS = args.epochs\n","SEED = args.seed\n","pretrain = True if args.pretrain else False\n","load_epoch = args.load_epoch\n","topks = eval(args.topks)\n","a_n_fold = args.a_n_fold\n","tensorboard = True if args.tensorboard else False\n","\n","bpr_batch_size = args.bpr_batch\n","test_u_batch_size = args.testbatch\n","lr = args.lr\n","decay = args.decay\n","\n","config = {}\n","config['embedding_dim'] = args.embedding_dim\n","config['activation_function'] = args.non_linear_acti\n","config['dropout'] = args.dropout\n","config['keep_prob']  = args.keepprob\n","config['a_split'] = args.a_split\n","config['gating_mlp_dims'] = [128, 2]\n","\n","print('='*30)\n","print('Model:', model_name)\n","print('Model config:', config)\n","print('Dataset:', dataset)\n","print(\"EPOCHS:\", EPOCHS)\n","print(\"Pretrain:\", pretrain)\n","print(\"BPR batch size:\", bpr_batch_size)\n","print(\"Test batch size:\", test_u_batch_size)\n","print(\"Test topks:\", topks)\n","print(\"N fold:\", a_n_fold)\n","print(\"Tensorboard:\", tensorboard)\n","print('='*30)\n","\n","# Gumbel-Softmax Param\n","ori_temp = args.ori_temp\n","min_temp = args.min_temp\n","gum_temp_decay = args.gum_temp_decay\n","epoch_temp_decay = args.epoch_temp_decay\n","config['division_noise'] = args.division_noise\n","train_hard = False\n","test_hard = True\n","\n","\n","# PATH\n","ROOT_PATH = args.root_path\n","DATA_PATH = join(ROOT_PATH, 'data', args.dataset)\n","SAVE_FILE_PATH = join(ROOT_PATH, args.save_checkpoints_path, model_name, dataset)\n","LOAD_FILE_PATH = join(ROOT_PATH, args.save_checkpoints_path, model_name, dataset, args.pretrained_checkpoint_name)\n","EXCEL_PATH = join(ROOT_PATH, args.save_excel_path)\n","BOARD_PATH = join(ROOT_PATH, 'tensorboard')\n","\n","print('='*30)\n","print('DATA PATH:', DATA_PATH)\n","print('SAVE FILE PATH:', SAVE_FILE_PATH)\n","print('LOAD FILE PATH:', LOAD_FILE_PATH)\n","print('EXCEL PATH:', EXCEL_PATH)\n","print('BOARD PATH:', BOARD_PATH)\n","print('='*30)\n","\n","# Making folder\n","os.makedirs(SAVE_FILE_PATH, exist_ok=True)\n","os.makedirs(EXCEL_PATH, exist_ok=True)\n","os.makedirs(BOARD_PATH, exist_ok=True)\n","   \n","# GPU\n","print('='*30)\n","print('Cuda:', torch.cuda.is_available())\n","GPU_NUM = args.gpu_num\n","device = torch.device(f'cuda:{GPU_NUM}' if torch.cuda.is_available() else 'cpu')\n","if torch.cuda.is_available():\n","\ttorch.cuda.set_device(device)\n","\tprint('CUDA device:', torch.cuda.current_device())\n","print('='*30)\n","\n","# Multi-processing \n","multicore = args.multicore\n","CORES = multiprocessing.cpu_count() // 2\n","print('='*30)\n","print(\"Multicore:\", multicore)\n","print(\"CORES:\", CORES)\n","print('='*30)\n","\n","# Excel results dict\n","excel_results_valid = {}\n","excel_results_valid['Model'] = []\n","excel_results_valid['Dataset'] = []\n","excel_results_valid['Epochs'] = []\n","excel_results_valid['Precision'] = []\n","excel_results_valid['Recall(HR)'] = []\n","excel_results_valid['Ndcg'] = []\n","\n","excel_results_test = {}\n","excel_results_test['Model'] = []\n","excel_results_test['Dataset'] = []\n","excel_results_test['Epochs'] = []\n","excel_results_test['Precision'] = []\n","excel_results_test['Recall(HR)'] = []\n","excel_results_test['Ndcg'] = []"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["==============================\n","Model: HMLET_End\n","Model config: {'embedding_dim': 512, 'activation_function': 'elu', 'dropout': 1, 'keep_prob': 0.6, 'a_split': 0, 'gating_mlp_dims': [128, 2]}\n","Dataset: gowalla\n","EPOCHS: 4\n","Pretrain: False\n","BPR batch size: 2048\n","Test batch size: 100\n","Test topks: [10, 20, 30, 40, 50]\n","N fold: 100\n","Tensorboard: True\n","==============================\n","==============================\n","DATA PATH: /content/data/gowalla\n","SAVE FILE PATH: /content/checkpoints/HMLET_End/gowalla\n","LOAD FILE PATH: /content/checkpoints/HMLET_End/gowalla/\n","EXCEL PATH: /content/excel\n","BOARD PATH: /content/tensorboard\n","==============================\n","==============================\n","Cuda: True\n","CUDA device: 0\n","==============================\n","==============================\n","Multicore: 1\n","CORES: 1\n","==============================\n"]}]},{"cell_type":"markdown","metadata":{"id":"qY9Y0q2sz1MS"},"source":["## Utils"]},{"cell_type":"code","metadata":{"id":"oCeclRK663Of"},"source":["def set_seed(seed):\n","    np.random.seed(seed)\n","    if torch.cuda.is_available():\n","        torch.cuda.manual_seed(seed)\n","        torch.cuda.manual_seed_all(seed)\n","    torch.manual_seed(seed)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wlLXKc2f65Uf"},"source":["def getFileName():\n","    if model_name == 'mf':\n","        file = f\"mf-{dataset}-{config['latent_dim_rec']}.pth.tar\"\n","    elif model_name == 'lgn':\n","        file = f\"lgn-{dataset}-{config['lightGCN_n_layers']}-{config['latent_dim_rec']}.pth.tar\"\n","    return os.path.join(FILE_PATH,file)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4qORYRxz69VI"},"source":["def minibatch(*tensors, **kwargs):\n","\n","    batch_size = kwargs.get('batch_size', bpr_batch_size)\n","\n","    if len(tensors) == 1:\n","        tensor = tensors[0]\n","        for i in range(0, len(tensor), batch_size):\n","            yield tensor[i:i + batch_size]\n","    else:\n","        for i in range(0, len(tensors[0]), batch_size):\n","            yield tuple(x[i:i + batch_size] for x in tensors)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ErZPo_bw69Sd"},"source":["def shuffle(*arrays, **kwargs):\n","\n","    require_indices = kwargs.get('indices', False)\n","\n","    if len(set(len(x) for x in arrays)) != 1:\n","        raise ValueError('All inputs to shuffle must have '\n","                         'the same length.')\n","\n","    shuffle_indices = np.arange(len(arrays[0]))\n","    np.random.shuffle(shuffle_indices)\n","\n","    if len(arrays) == 1:\n","        result = arrays[0][shuffle_indices]\n","    else:\n","        result = tuple(x[shuffle_indices] for x in arrays)\n","\n","    if require_indices:\n","        return result, shuffle_indices\n","    else:\n","        return result"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"HizlUpCV69Qa"},"source":["class timer:\n","    \"\"\"\n","    Time context manager for code block\n","        with timer():\n","            do something\n","        timer.get()\n","    \"\"\"\n","    from time import time\n","    TAPE = [-1]  # global time record\n","    NAMED_TAPE = {}\n","\n","    @staticmethod\n","    def get():\n","        if len(timer.TAPE) > 1:\n","            return timer.TAPE.pop()\n","        else:\n","            return -1\n","\n","    @staticmethod\n","    def dict(select_keys=None):\n","        hint = \"|\"\n","        if select_keys is None:\n","            for key, value in timer.NAMED_TAPE.items():\n","                hint = hint + f\"{key}:{value:.2f}|\"\n","        else:\n","            for key in select_keys:\n","                value = timer.NAMED_TAPE[key]\n","                hint = hint + f\"{key}:{value:.2f}|\"\n","        return hint\n","\n","    @staticmethod\n","    def zero(select_keys=None):\n","        if select_keys is None:\n","            for key, value in timer.NAMED_TAPE.items():\n","                timer.NAMED_TAPE[key] = 0\n","        else:\n","            for key in select_keys:\n","                timer.NAMED_TAPE[key] = 0\n","\n","    def __init__(self, tape=None, **kwargs):\n","        if kwargs.get('name'):\n","            timer.NAMED_TAPE[kwargs['name']] = timer.NAMED_TAPE[\n","                kwargs['name']] if timer.NAMED_TAPE.get(kwargs['name']) else 0.\n","            self.named = kwargs['name']\n","            if kwargs.get(\"group\"):\n","                #TODO: add group function\n","                pass\n","        else:\n","            self.named = False\n","            self.tape = tape or timer.TAPE\n","\n","    def __enter__(self):\n","        self.start = timer.time()\n","        return self\n","\n","    def __exit__(self, exc_type, exc_val, exc_tb):\n","        if self.named:\n","            timer.NAMED_TAPE[self.named] += timer.time() - self.start\n","        else:\n","            self.tape.append(timer.time() - self.start)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"nhSnKpry7POv"},"source":["## Metrics"]},{"cell_type":"code","metadata":{"id":"MjPJkxKQ7QUM"},"source":["def RecallPrecision_ATk(test_data, r, k):\n","    \"\"\"\n","    test_data should be a list? cause users may have different amount of pos items. shape (test_batch, k)\n","    pred_data : shape (test_batch, k) NOTE: pred_data should be pre-sorted\n","    k : top-k\n","    \"\"\"\n","    right_pred = r[:, :k].sum(1)\n","    precis_n = k\n","    recall_n = np.array([len(test_data[i]) for i in range(len(test_data))])\n","    recall = np.sum(right_pred/recall_n)\n","    precis = np.sum(right_pred)/precis_n\n","    return {'recall': recall, 'precision': precis}\n","\n","\n","def MRRatK_r(r, k):\n","    \"\"\"\n","    Mean Reciprocal Rank\n","    \"\"\"\n","    pred_data = r[:, :k]\n","    scores = np.log2(1./np.arange(1, k+1))\n","    pred_data = pred_data/scores\n","    pred_data = pred_data.sum(1)\n","    return np.sum(pred_data)\n","\n","def NDCGatK_r(test_data,r,k):\n","    \"\"\"\n","    Normalized Discounted Cumulative Gain\n","    rel_i = 1 or 0, so 2^{rel_i} - 1 = 1 or 0\n","    \"\"\"\n","    assert len(r) == len(test_data)\n","    pred_data = r[:, :k]\n","\n","    test_matrix = np.zeros((len(pred_data), k))\n","    for i, items in enumerate(test_data):\n","        length = k if k <= len(items) else len(items)\n","        test_matrix[i, :length] = 1\n","    max_r = test_matrix\n","    idcg = np.sum(max_r * 1./np.log2(np.arange(2, k + 2)), axis=1)\n","    dcg = pred_data*(1./np.log2(np.arange(2, k + 2)))\n","    dcg = np.sum(dcg, axis=1)\n","    idcg[idcg == 0.] = 1.\n","    ndcg = dcg/idcg\n","    ndcg[np.isnan(ndcg)] = 0.\n","    return np.sum(ndcg)\n","\n","def AUC(all_item_scores, dataset, test_data):\n","    \"\"\"\n","        design for a single user\n","    \"\"\"\n","    dataset : BasicDataset\n","    r_all = np.zeros((dataset.m_items, ))\n","    r_all[test_data] = 1\n","    r = r_all[all_item_scores >= 0]\n","    test_item_scores = all_item_scores[all_item_scores >= 0]\n","    return roc_auc_score(r, test_item_scores)\n","\n","def getLabel(test_data, pred_data):\n","    r = []\n","    for i in range(len(test_data)):\n","        groundTrue = test_data[i]\n","        predictTopK = pred_data[i]\n","        pred = list(map(lambda x: x in groundTrue, predictTopK))\n","        pred = np.array(pred).astype(\"float\")\n","        r.append(pred)\n","    return np.array(r).astype('float')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6nPwMgRj4dSn"},"source":["## Sampling"]},{"cell_type":"code","metadata":{"id":"Itnc1ZWA4Zyz"},"source":["sample_ext = False"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"c4JjyZdt6vcw"},"source":["class BPRLoss:\n","    def __init__(self,\n","                 model):\n","        self.model = model\n","        self.weight_decay = decay\n","        self.lr = lr\n","        self.opt = optim.Adam(model.parameters(), lr=self.lr)\n","\n","    def stageOne(self, users, pos, neg, gum_temp, hard):\n","        loss, reg_loss, gating_dist, embs = self.model.bpr_loss(users, pos, neg, gum_temp, hard)\n","        reg_loss = reg_loss*self.weight_decay\n","        loss = loss + reg_loss\n","\n","        self.opt.zero_grad()\n","        loss.backward()\n","        self.opt.step()\n","\n","        return loss.cpu().item(), gating_dist, embs\n","\n","def UniformSample_original(dataset, neg_ratio = 1):\n","    dataset : BasicDataset\n","    allPos = dataset.allPos\n","    start = time()\n","    if sample_ext:\n","        S = sampling.sample_negative(dataset.n_users, dataset.m_items,\n","                                     dataset.trainDataSize, allPos, neg_ratio)\n","    else:\n","        S = UniformSample_original_python(dataset)\n","    return S\n","\n","def UniformSample_original_python(dataset):\n","    \"\"\"\n","    the original impliment of BPR Sampling in LightGCN\n","    :return:\n","        np.array\n","    \"\"\"\n","    total_start = time()\n","    dataset : BasicDataset\n","    user_num = dataset.trainDataSize\n","    users = np.random.randint(0, dataset.n_users, user_num)\n","    allPos = dataset.allPos\n","    S = []\n","    sample_time1 = 0.\n","    sample_time2 = 0.\n","    for i, user in enumerate(users):\n","        start = time()\n","        posForUser = allPos[user]\n","        if len(posForUser) == 0:\n","            continue\n","        sample_time2 += time() - start\n","        posindex = np.random.randint(0, len(posForUser))\n","        positem = posForUser[posindex]\n","        while True:\n","            negitem = np.random.randint(0, dataset.m_items)\n","            if negitem in posForUser:\n","                continue\n","            else:\n","                break\n","        S.append([user, positem, negitem])\n","        end = time()\n","        sample_time1 += end - start\n","    total = time() - total_start\n","    return np.array(S)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"aWWfMIfz14aY"},"source":["## Dataloader"]},{"cell_type":"code","metadata":{"id":"aFd-IA_f14MG"},"source":["class BasicDataset(Dataset):\n","    def __init__(self):\n","        print(\"init dataset\")\n","    \n","    @property\n","    def n_users(self):\n","        raise NotImplementedError\n","    \n","    @property\n","    def m_items(self):\n","        raise NotImplementedError\n","    \n","    @property\n","    def trainDataSize(self):\n","        raise NotImplementedError\n","    \n","    @property\n","    def testDict(self):\n","        raise NotImplementedError\n","    \n","    @property\n","    def allPos(self):\n","        raise NotImplementedError\n","    \n","    def getUserItemFeedback(self, users, items):\n","        raise NotImplementedError\n","    \n","    def getUserPosItems(self, users):\n","        raise NotImplementedError\n","    \n","    def getUserNegItems(self, users):\n","        \"\"\"\n","        not necessary for large dataset\n","        it's stupid to return all neg items in super large dataset\n","        \"\"\"\n","        raise NotImplementedError\n","    \n","    def getSparseGraph(self):\n","        \"\"\"\n","        build a graph in torch.sparse.IntTensor.\n","        Details in NGCF's matrix form\n","        A = \n","            |I,   R|\n","            |R^T, I|\n","        \"\"\"\n","        raise NotImplementedError"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"UEcHRac02JTx"},"source":["class Loader(BasicDataset):\n","    \"\"\"\n","    Dataset type for pytorch \\n\n","    Incldue graph information\n","    gowalla dataset\n","    \"\"\"\n","\n","    def __init__(self, path):\n","        print('Loading', path)\n","        self.split = config['a_split']\n","        self.folds = a_n_fold\n","        self.mode_dict = {'train & valid': 0, \"test\": 1}\n","        self.mode = self.mode_dict['train & valid']\n","        self.n_user = 0\n","        self.m_item = 0\n","        train_file = path + '/train.txt'\n","        valid_file = path + '/val.txt'\n","        test_file = path + '/test.txt'\n","        self.path = path\n","        trainUniqueUsers, trainItem, trainUser = [], [], []\n","        validUniqueUsers, validItem, validUser = [], [], []\n","        testUniqueUsers, testItem, testUser = [], [], []\n","        self.traindataSize = 0\n","        self.validDataSize = 0\n","        self.testDataSize = 0\n","\n","        with open(train_file) as f:\n","            for l in f.readlines():\n","                if len(l) > 0:\n","                    l = l.strip('\\n').split(' ')\n","                    items = [int(i) for i in l[1:]]\n","                    uid = int(l[0])\n","                    trainUniqueUsers.append(uid)\n","                    trainUser.extend([uid] * len(items))\n","                    trainItem.extend(items)\n","                    self.m_item = max(self.m_item, max(items))\n","                    self.n_user = max(self.n_user, uid)\n","                    self.traindataSize += len(items)\n","        self.trainUniqueUsers = np.array(trainUniqueUsers)\n","        self.trainUser = np.array(trainUser)\n","        self.trainItem = np.array(trainItem)\n","\t\t\n","        with open(valid_file) as f:\n","            for l in f.readlines():\n","                if len(l) > 0:\n","                    l = l.strip('\\n').split(' ')\n","                    try:\n","                      items = [int(i) for i in l[1:]]\n","                    except Exception:\n","                      continue\n","                    uid = int(l[0])\n","                    validUniqueUsers.append(uid)\n","                    validUser.extend([uid] * len(items))\n","                    validItem.extend(items)\n","                    self.m_item = max(self.m_item, max(items))\n","                    self.n_user = max(self.n_user, uid)\n","                    self.validDataSize += len(items)\n","        self.validUniqueUsers = np.array(validUniqueUsers)\n","        self.validUser = np.array(validUser)\n","        self.validItem = np.array(validItem)\n","\n","        with open(test_file) as f:\n","            for l in f.readlines():\n","                if len(l) > 0:\n","                    l = l.strip('\\n').split(' ')\n","                    try:\n","                      items = [int(i) for i in l[1:]]\n","                    except Exception:\n","                      continue\n","                    uid = int(l[0])\n","                    testUniqueUsers.append(uid)\n","                    testUser.extend([uid] * len(items))\n","                    testItem.extend(items)\n","                    self.m_item = max(self.m_item, max(items))\n","                    self.n_user = max(self.n_user, uid)\n","                    self.testDataSize += len(items)\n","        self.m_item += 1\n","        self.n_user += 1\n","        self.testUniqueUsers = np.array(testUniqueUsers)\n","        self.testUser = np.array(testUser)\n","        self.testItem = np.array(testItem)\n","        \n","        self.Graph = None\n","        print('='*30)\n","        print(f\"{self.trainDataSize} interactions for training\")\n","        print(f\"{self.testDataSize} interactions for testing\")\n","        print(f\"{dataset} Sparsity : {(self.traindataSize + self.validDataSize + self.testDataSize) / self.n_users / self.m_items}\")\n","        print('='*30)\n","\n","        # (users,items), bipartite graph (train)\n","        self.UserItemNet = csr_matrix((np.ones(len(self.trainUser)), (self.trainUser, self.trainItem)),\n","                                      shape=(self.n_user, self.m_item))\n","        self.users_D = np.array(self.UserItemNet.sum(axis=1)).squeeze()\n","        self.users_D[self.users_D == 0.] = 1\n","        self.items_D = np.array(self.UserItemNet.sum(axis=0)).squeeze()\n","        self.items_D[self.items_D == 0.] = 1.\n","\n","        # pre-calculate\n","        self._allPos = self.getUserPosItems(list(range(self.n_user)))\n","        self.__trainDict = self.__build_train()\n","        self.__validDict = self.__build_valid()\n","        self.__testDict = self.__build_test()\n","        print(f\"{dataset} is ready to go\")\n","\n","    @property\n","    def n_users(self):\n","        return self.n_user\n","    \n","    @property\n","    def m_items(self):\n","        return self.m_item\n","    \n","    @property\n","    def trainDataSize(self):\n","        return self.traindataSize\n","\n","    @property\n","    def trainDict(self):\n","        return self.__trainDict\n","\n","    @property\n","    def validDict(self):\n","        return self.__validDict\n","    \n","    @property\n","    def testDict(self):\n","        return self.__testDict\n","\n","    @property\n","    def allPos(self):\n","        return self._allPos\n","\n","    def _split_A_hat(self,A):\n","        A_fold = []\n","        fold_len = (self.n_users + self.m_items) // self.folds\n","        for i_fold in range(self.folds):\n","            start = i_fold*fold_len\n","            if i_fold == self.folds - 1:\n","                end = self.n_users + self.m_items\n","            else:\n","                end = (i_fold + 1) * fold_len\n","            A_fold.append(self._convert_sp_mat_to_sp_tensor(A[start:end]).coalesce().to(device))\n","        return A_fold\n","\n","    def _convert_sp_mat_to_sp_tensor(self, X):\n","        coo = X.tocoo().astype(np.float32)\n","        row = torch.Tensor(coo.row).long()\n","        col = torch.Tensor(coo.col).long()\n","        index = torch.stack([row, col])\n","        data = torch.FloatTensor(coo.data)\n","        return torch.sparse.FloatTensor(index, data, torch.Size(coo.shape))\n","        \n","    def getSparseGraph(self):\n","        print(\"loading adjacency matrix\")\n","        if self.Graph is None:\n","            try:\n","                pre_adj_mat = sp.load_npz(self.path + '/s_pre_adj_mat_train.npz')\n","                print(\"successfully train loaded...\")\n","                norm_adj_train = pre_adj_mat\n","            except :\n","                print(\"generating adjacency matrix\")\n","                s = time()\n","                adj_mat = sp.dok_matrix((self.n_users + self.m_items, self.n_users + self.m_items), dtype=np.float32)\n","                adj_mat = adj_mat.tolil()\n","                \n","                R = self.UserItemNet.tolil()\n","                adj_mat[:self.n_users, self.n_users:] = R\n","                adj_mat[self.n_users:, :self.n_users] = R.T\n","                adj_mat = adj_mat.todok()\n","                \n","                rowsum = np.array(adj_mat.sum(axis=1))\n","                d_inv = np.power(rowsum, -0.5).flatten()\n","                d_inv[np.isinf(d_inv)] = 0.\n","                d_mat = sp.diags(d_inv)\n","                \n","                norm_adj_train = d_mat.dot(adj_mat)\n","                norm_adj_train = norm_adj_train.dot(d_mat)\n","                norm_adj_train = norm_adj_train.tocsr()\n","                end = time()\n","                print(f\"costing {end-s}s, saved train norm_mat...\")\n","                sp.save_npz(self.path + '/s_pre_adj_mat_train.npz', norm_adj_train)\n","\n","            if self.split:\n","                self.Graph = self._split_A_hat(norm_adj_train)\n","                print(\"done split matrix\")\n","            else:\n","                self.Graph = self._convert_sp_mat_to_sp_tensor(norm_adj_train)\n","                self.Graph = self.Graph.coalesce().to(device)\n","                print(\"don't split the matrix\")\n","        return self.Graph\n","\n","    def __build_train(self):\n","        \"\"\"\n","        return:\n","            dict: {user: [items]}\n","        \"\"\"\n","        train_data = {}\n","        for i, item in enumerate(self.trainItem):\n","            user = self.trainUser[i]\n","            if train_data.get(user):\n","                train_data[user].append(item)\n","            else:\n","                train_data[user] = [item]\n","        return train_data\n","        \n","    def __build_valid(self):\n","        \"\"\"\n","        return:\n","            dict: {user: [items]}\n","        \"\"\"\n","        valid_data = {}\n","        for i, item in enumerate(self.validItem):\n","            user = self.validUser[i]\n","            if valid_data.get(user):\n","                valid_data[user].append(item)\n","            else:\n","                valid_data[user] = [item]\n","        return valid_data\n","\n","    def __build_test(self):\n","        \"\"\"\n","        return:\n","            dict: {user: [items]}\n","        \"\"\"\n","        test_data = {}\n","        for i, item in enumerate(self.testItem):\n","            user = self.testUser[i]\n","            if test_data.get(user):\n","                test_data[user].append(item)\n","            else:\n","                test_data[user] = [item]\n","        return test_data\n","\n","    def getUserItemFeedback(self, users, items):\n","        \"\"\"\n","        users:\n","            shape [-1]\n","        items:\n","            shape [-1]\n","        return:\n","            feedback [-1]\n","        \"\"\"\n","        return np.array(self.UserItemNet[users, items]).astype('uint8').reshape((-1,))\n","\n","    def getUserPosItems(self, users):\n","        posItems = []\n","        for user in users:\n","            posItems.append(self.UserItemNet[user].nonzero()[1])\n","        return posItems"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1n_jTcJTx0LA"},"source":["## Gating Network"]},{"cell_type":"code","metadata":{"id":"XvBM9e8sx8Wl"},"source":["class Gating_Net(nn.Module):\n","\n","    def __init__(self, embedding_dim, mlp_dims):\n","        super(Gating_Net, self).__init__()\n","        self.embedding_dim = embedding_dim\n","        self.softmax =  nn.LogSoftmax(dim=1)\n","        fc_layers = []\n","        for i in range(len(mlp_dims)):\n","            if i == 0:\n","                fc_layers.append(nn.Linear(embedding_dim*2, mlp_dims[i]))\n","            else:\n","                fc_layers.append(nn.Linear(mlp_dims[i-1], mlp_dims[i]))\t\n","            if i != len(mlp_dims) - 1:\n","                fc_layers.append(nn.BatchNorm1d(mlp_dims[i]))\n","                fc_layers.append(nn.ReLU(inplace=True))\n","        self.mlp = nn.Sequential(*fc_layers)\n","\n","    def gumbel_softmax(self, logits, temperature, division_noise, hard):\n","        \"\"\"Sample from the Gumbel-Softmax distribution and optionally discretize.\n","        Args:\n","          logits: [batch_size, n_class] unnormalized log-probs\n","          temperature: non-negative scalar\n","          hard: if True, take argmax, but differentiate w.r.t. soft sample y\n","        Returns:\n","          [batch_size, n_class] sample from the Gumbel-Softmax distribution.\n","          If hard=True, then the returned sample will be one-hot, otherwise it will\n","          be a probabilitiy distribution that sums to 1 across classes\n","        \"\"\"\n","        y = self.gumbel_softmax_sample(logits, temperature, division_noise) ## (0.6, 0.2, 0.1,..., 0.11)\n","        if hard:\n","            k = logits.size(1) # k is numb of classes\n","            # y_hard = tf.cast(tf.one_hot(tf.argmax(y,1),k), y.dtype)  ## (1, 0, 0, ..., 0)\n","            y_hard = torch.eq(y, torch.max(y, dim=1, keepdim=True)[0]).type_as(y)\n","            y = (y_hard - y).detach() + y\n","        return y\n","\n","    def gumbel_softmax_sample(self, logits, temperature, division_noise):\n","        \"\"\" Draw a sample from the Gumbel-Softmax distribution\"\"\"\n","        noise = self.sample_gumbel(logits)\n","        y = (logits + (noise/division_noise)) / temperature\n","        return F.softmax(y)\n","\n","    def sample_gumbel(self, logits):\n","        \"\"\"Sample from Gumbel(0, 1)\"\"\"\n","        noise = torch.rand(logits.size())\n","        eps = 1e-20\n","        noise.add_(eps).log_().neg_()\n","        noise.add_(eps).log_().neg_()\n","        return Variable(noise.float()).cuda()\n","\n","    def forward(self, feature, temperature, hard, division_noise): #z= batch x z_dim // #feature =  batch x num_gen x 256*8*8\n","        x = self.mlp(feature)\n","        out = self.gumbel_softmax(x, temperature, division_noise, hard)\n","        out_value = out.unsqueeze(2)\n","        out = out_value.repeat(1, 1, self.embedding_dim)\n","                \n","        return out, torch.sum(out_value[:,0]), torch.sum(out_value[:,1])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"OuDsPV1mxvQ0"},"source":["## HMLET (End) Model"]},{"cell_type":"code","metadata":{"id":"nAqs6mNixvN1"},"source":["class BasicModel(nn.Module):    \n","\tdef __init__(self):\n","\t\tsuper(BasicModel, self).__init__()\n","\n","\tdef getUsersRating(self, users):\n","\t\traise NotImplementedError"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"In00VlPU3apv"},"source":["class HMLET_End(nn.Module):\n","\tdef __init__(self, \n","\t\t\t\t\tconfig:dict, \n","\t\t\t\t\tdataset:BasicDataset):\n","\t\tsuper(HMLET_End, self).__init__()\n","\t\tself.config = config\n","\t\tself.dataset : dataloader.BasicDataset = dataset\n","\t\tself.__init_model()\n","\n","\tdef __init_model(self):\n","\t\tself.num_users = self.dataset.n_users\n","\t\tself.num_items = self.dataset.m_items\n","\t\tself.embedding_dim = self.config['embedding_dim']\n","   \n","\t\tself.n_layers = 4\n","\t\tself.dropout = self.config['dropout']\n","\t\tself.keep_prob = self.config['keep_prob']\n","\t\tself.A_split = self.config['a_split']\n","\n","\t\t# Embedding\n","\t\tself.embedding_user = torch.nn.Embedding(\n","\t\t\tnum_embeddings=self.num_users, embedding_dim=self.embedding_dim)\n","\t\tself.embedding_item = torch.nn.Embedding(\n","\t\t\tnum_embeddings=self.num_items, embedding_dim=self.embedding_dim)\n","\t\t\n","\t\t# Normal distribution initilizer\n","\t\tnn.init.normal_(self.embedding_user.weight, std=0.1)\n","\t\tnn.init.normal_(self.embedding_item.weight, std=0.1)      \n","\t\t\n","\t\t# Activation function\n","\t\tselected_activation_function = self.config['activation_function']\n","\t\t\n","\t\tif selected_activation_function == 'relu':\n","\t\t\tself.r = nn.ReLU()\n","\t\t\tself.activation_function = self.r\n","\t\tif selected_activation_function == 'leaky-relu':\n","\t\t\tself.leaky = nn.LeakyReLU(0.1)\n","\t\t\tself.activation_function = self.leaky\n","\t\telif selected_activation_function == 'elu':\n","\t\t\tself.elu = nn.ELU()\n","\t\t\tself.activation_function = self.elu\n","\t\tprint('activation_function:',self.activation_function)\n","\t\t\n","\t\tself.g_train = self.dataset.getSparseGraph()\n","\n","\t\t# Gating Net with Gumbel-Softmax\n","\t\tself.gating_network_list = []\n","\t\tfor i in range(2):\n","\t\t\tself.gating_network_list.append(Gating_Net(embedding_dim=self.embedding_dim, mlp_dims=self.config['gating_mlp_dims']).to(device))\n","\n","\tdef __choosing_one(self, features, gumbel_out):\n","\t\tfeature = torch.sum(torch.mul(features, gumbel_out), dim=1)  # batch x embedding_dim (or batch x embedding_dim x layer_num)\n","\t\treturn feature\n","\n","\tdef __dropout_x(self, x, keep_prob):\n","\t\tsize = x.size()\n","\t\tindex = x.indices().t()\n","\t\tvalues = x.values()\n","\t\trandom_index = torch.rand(len(values)) + keep_prob\n","\t\trandom_index = random_index.int().bool()\n","\t\tindex = index[random_index]\n","\t\tvalues = values[random_index]/keep_prob\n","\t\tg = torch.sparse.FloatTensor(index.t(), values, size)\n","\t\treturn g\n","\n","\tdef __dropout(self, keep_prob):\n","\t\tif self.A_split:   \n","\t\t\tgraph = []\n","\t\t\tfor g in self.Graph:\n","\t\t\t\tgraph.append(self.__dropout_x(g, keep_prob))\n","\t\telse:\n","\t\t\tgraph = self.__dropout_x(self.Graph, keep_prob)\n","\t\treturn graph\n","\n","\tdef computer(self, gum_temp, hard):     \n","\t\t\n","\t\tself.Graph = self.g_train   \n","\t\tif self.dropout:\n","\t\t\tif self.training:\n","\t\t\t\tg_droped = self.__dropout(self.keep_prob)\n","\t\t\telse:\n","\t\t\t\tg_droped = self.Graph        \n","\t\telse:\n","\t\t\tg_droped = self.Graph\n","    \n","    \n","\t\t# Init users & items embeddings  \n","\t\tusers_emb = self.embedding_user.weight\n","\t\titems_emb = self.embedding_item.weight\n","      \n","      \n","\t\t## Layer 0\n","\t\tall_emb_0 = torch.cat([users_emb, items_emb])\n","\t\t\n","\t\t# Residual embeddings\n","\t\tembs = [all_emb_0]\n","\t\t\n","   \n","\t\t## Layer 1\n","\t\tall_emb_lin_1 = torch.sparse.mm(g_droped, all_emb_0)\n","\t\t\n","\t\t# Residual embeddings\t\n","\t\tembs.append(all_emb_lin_1)\n","\t\t\n","   \n","\t\t## layer 2\n","\t\tall_emb_lin_2 = torch.sparse.mm(g_droped, all_emb_lin_1)\n","\t\t\n","\t\t# Residual embeddings\n","\t\tembs.append(all_emb_lin_2)\n","\t\t\n","   \n","\t\t## layer 3\n","\t\tall_emb_lin_3 = torch.sparse.mm(g_droped, all_emb_lin_2)\n","\t\tall_emb_non_1 = self.activation_function(torch.sparse.mm(g_droped, all_emb_0))\n","\t\t\n","\t\t# Gating\n","\t\tstack_embedding_1 = torch.stack([all_emb_lin_3, all_emb_non_1],dim=1)\n","\t\tconcat_embeddings_1 = torch.cat((all_emb_lin_3, all_emb_non_1),-1)\n","\n","\t\tgumbel_out_1, lin_count_3, non_count_3 = self.gating_network_list[0](concat_embeddings_1, gum_temp, hard, self.config['division_noise'])\n","\t\tembedding_1 = self.__choosing_one(stack_embedding_1, gumbel_out_1)\n","\n","\t\t# Residual embeddings\n","\t\tembs.append(embedding_1)\n","\t\n","  \t\n","\t\t# layer 4\n","\t\tall_emb_lin_4 = torch.sparse.mm(g_droped, embedding_1)\n","\t\tall_emb_non_2 = self.activation_function(torch.sparse.mm(g_droped, embedding_1))\n","\t\t\n","\t\t# Gating\n","\t\tstack_embedding_2 = torch.stack([all_emb_lin_4, all_emb_non_2],dim=1)\n","\t\tconcat_embeddings_2 = torch.cat((all_emb_lin_4, all_emb_non_2),-1)\n","\n","\t\tgumbel_out_2, lin_count_4, non_count_4 = self.gating_network_list[1](concat_embeddings_2, gum_temp, hard, self.config['division_noise'])\n","\t\tembedding_2 = self.__choosing_one(stack_embedding_2, gumbel_out_2)\n","\n","\t\t# Residual embeddings  \t\t\n","\t\tembs.append(embedding_2)\n","\n","\n","\t\t## Stack & mean residual embeddings\n","\t\tembs = torch.stack(embs, dim=1)\n","\t\tlight_out = torch.mean(embs, dim=1)\n","   \n","\t\tusers, items = torch.split(light_out, [self.num_users, self.num_items])\n","\t\t\n","\t\treturn users, items, [lin_count_3, non_count_3, lin_count_4, non_count_4], embs\n","\n","\tdef getUsersRating(self, users, gum_temp, hard):\n","\t\tall_users, all_items, gating_dist, embs = self.computer(gum_temp, hard)\n","\t\t\n","\t\tusers_emb = all_users[users.long()]\n","\t\titems_emb = all_items\n","\n","\t\trating = self.activation_function(torch.matmul(users_emb, items_emb.t()))\n","\n","\t\treturn rating, gating_dist, embs\n","\n","\tdef getEmbedding(self, users, pos_items, neg_items, gum_temp, hard):\n","\t\tall_users, all_items, gating_dist, embs = self.computer(gum_temp, hard)\n","\t\t\n","\t\tusers_emb = all_users[users]\n","\t\tpos_emb = all_items[pos_items]\n","\t\tneg_emb = all_items[neg_items]\n","\n","\t\tusers_emb_ego = self.embedding_user(users)\n","\t\tpos_emb_ego = self.embedding_item(pos_items)\n","\t\tneg_emb_ego = self.embedding_item(neg_items)\n","\n","\t\treturn users_emb, pos_emb, neg_emb, users_emb_ego, pos_emb_ego, neg_emb_ego, gating_dist, embs\n","\n","\tdef bpr_loss(self, users, pos, neg, gum_temp, hard):\n","\t\t(users_emb, pos_emb, neg_emb, \n","\t\tuserEmb0,  posEmb0, negEmb0, gating_dist, embs) = self.getEmbedding(users.long(), pos.long(), neg.long(), gum_temp, hard)\n","\t\t\n","\t\treg_loss = (1/2)*(userEmb0.norm(2).pow(2) + \n","\t\t\t\t\t\t\tposEmb0.norm(2).pow(2)  +\n","\t\t\t\t\t\t\tnegEmb0.norm(2).pow(2))/float(len(users))\n","\t\t\n","\t\tpos_scores = torch.mul(users_emb, pos_emb)\n","\t\tpos_scores = torch.sum(pos_scores, dim=1)\n","\t\tneg_scores = torch.mul(users_emb, neg_emb)\n","\t\tneg_scores = torch.sum(neg_scores, dim=1)\n","\t\t\n","\t\tloss = torch.mean(torch.nn.functional.softplus(neg_scores - pos_scores))\n","\t\t\n","\t\treturn loss, reg_loss, gating_dist, embs\n","\t\t\n","\tdef forward(self, users, items, gum_temp, hard):\n","\t\t# compute embedding\n","\t\tall_users, all_items, gating_dist, embs = self.computer(gum_temp, hard)\n","\n","\t\tusers_emb = all_users[users]\n","\t\titems_emb = all_items[items]\n","\n","\t\tinner_pro = torch.mul(users_emb, items_emb)\n","\t\tgamma     = torch.sum(inner_pro, dim=1)\n","\n","\t\treturn gamma, gating_dist, embs"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"pNkhPss9xvLC"},"source":["MODELS = {\n","  \"HMLET_End\": HMLET_End\n","}"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"a2HIfFRR7fP1"},"source":["## Procedures"]},{"cell_type":"code","metadata":{"id":"HuIhwj6g7hRJ"},"source":["def BPR_train_original(dataset, recommend_model, loss_class, epoch, gum_temp, hard, w=None):\n","    Recmodel = recommend_model\n","    Recmodel.train()\n","    bpr: BPRLoss = loss_class\n","    \n","    with timer(name=\"Sample\"):\n","        S = UniformSample_original(dataset)\n","    users = torch.Tensor(S[:, 0]).long()\n","    posItems = torch.Tensor(S[:, 1]).long()\n","    negItems = torch.Tensor(S[:, 2]).long()\n","\n","    users = users.to(device)\n","    posItems = posItems.to(device)\n","    negItems = negItems.to(device)\n","    users, posItems, negItems = shuffle(users, posItems, negItems)\n","    total_batch = len(users) // bpr_batch_size + 1\n","    aver_loss = 0.\n","    \n","    for (batch_i,\n","         (batch_users,\n","          batch_pos,\n","          batch_neg)) in enumerate(tqdm(minibatch(users,\n","                                                   posItems,\n","                                                   negItems,\n","                                                   batch_size=bpr_batch_size), total=396)):\n","        cri, gating_dist, embs = bpr.stageOne(batch_users, batch_pos, batch_neg, gum_temp, hard)\n","        aver_loss += cri\n","        if tensorboard:\n","            w.add_scalar(f'BPRLoss/BPR', cri, epoch * int(len(users) / bpr_batch_size) + batch_i)\n","    aver_loss = aver_loss / total_batch\n","\n","    return f\"loss{aver_loss:.3f}\"\n","    \n","def test_one_batch(X):\n","    sorted_items = X[0].numpy()\n","    groundTrue = X[1]\n","    r = getLabel(groundTrue, sorted_items)\n","    pre, recall, ndcg = [], [], []\n","    for k in topks:\n","        ret = RecallPrecision_ATk(groundTrue, r, k)\n","        pre.append(ret['precision'])\n","        recall.append(ret['recall'])\n","        ndcg.append(NDCGatK_r(groundTrue,r,k))\n","    return {'recall':np.array(recall), \n","            'precision':np.array(pre), \n","            'ndcg':np.array(ndcg)}\n","        \n","def Test(dataset, Recmodel, epoch, gum_temp, hard, mode, w=None, multicore=0):\n","\n","    u_batch_size = test_u_batch_size\n","    dataset: BasicDataset\n","    \n","    # Mode\n","    if mode == 'valid':\n","      print('valid mode')\n","      testDict: dict = dataset.validDict\n","      excel_results = excel_results_valid\n","    elif mode == 'test':\n","      print('test mode')\n","      testDict: dict = dataset.testDict\n","      excel_results = excel_results_test\n","    \n","    Recmodel = Recmodel.eval()\n","    max_K = max(topks)\n","    \n","    if multicore == 1:\n","        pool = multiprocessing.Pool(CORES)\n","    \n","    # Results\n","    results = {'precision': np.zeros(len(topks)),\n","               'recall': np.zeros(len(topks)),\n","               'ndcg': np.zeros(len(topks))}\n","               \n","    with torch.no_grad():\n","        users = list(testDict.keys())\n","        try:\n","            assert u_batch_size <= len(users) / 10\n","        except AssertionError:\n","            print(f\"test_u_batch_size is too big for this dataset, try a small one {len(users) // 10}\")\n","            \n","        users_list = []\n","        rating_list = []\n","        groundTrue_list = []\n","        #gating_dist_list = []\n","        #embs_list = []\n","        \n","        total_batch = len(users) // u_batch_size + 1\n","        \n","        for batch_users in minibatch(users, batch_size=u_batch_size):\n","            allPos = dataset.getUserPosItems(batch_users)\n","            groundTrue = [testDict[u] for u in batch_users]\n","            batch_users_gpu = torch.Tensor(batch_users).long()\n","            batch_users_gpu = batch_users_gpu.to(device)\n","\n","            rating, gating_dist, embs = Recmodel.getUsersRating(batch_users_gpu, gum_temp, hard)\n","            #gating_dist_list.append(gating_dist)\n","            #embs_list.append(embs)\n","            \n","            exclude_index = []\n","            exclude_items = []\n","            for range_i, items in enumerate(allPos):\n","                exclude_index.extend([range_i] * len(items))\n","                exclude_items.extend(items)\n","            rating[exclude_index, exclude_items] = -(1<<10)\n","            _, rating_K = torch.topk(rating, k=max_K)\n","            rating = rating.cpu().numpy()\n","\n","            del rating\n","            users_list.append(batch_users)\n","            rating_list.append(rating_K.cpu())\n","            groundTrue_list.append(groundTrue)\n","            \n","        assert total_batch == len(users_list)\n","        \n","        X = zip(rating_list, groundTrue_list)\n","        \n","        if multicore == 1:\n","            pre_results = pool.map(test_one_batch, X)\n","        else:\n","            pre_results = []\n","            for x in X:\n","                pre_results.append(test_one_batch(x))\n","                \n","        scale = float(u_batch_size/len(users))\n","        \n","        for result in pre_results:\n","            results['recall'] += result['recall']\n","            results['precision'] += result['precision']\n","            results['ndcg'] += result['ndcg']\n","            \n","        results['recall'] /= float(len(users))\n","        results['precision'] /= float(len(users))\n","        results['ndcg'] /= float(len(users))\n","        \n","        if tensorboard:\n","            w.add_scalars(f'Test/Recall@{topks}',\n","                          {str(topks[i]): results['recall'][i] for i in range(len(topks))}, epoch)\n","            w.add_scalars(f'Test/Precision@{topks}',\n","                          {str(topks[i]): results['precision'][i] for i in range(len(topks))}, epoch)\n","            w.add_scalars(f'Test/NDCG@{topks}',\n","                          {str(topks[i]): results['ndcg'][i] for i in range(len(topks))}, epoch)\n","        \n","        if multicore == 1:\n","            pool.close()\n","\n","        excel_results['Model'].append(model_name)\n","        excel_results['Dataset'].append(dataset)\n","        excel_results['Epochs'].append(epoch)\n","        excel_results['Precision'].append(results['precision'])\n","        excel_results['Recall(HR)'].append(results['recall'])\n","        excel_results['Ndcg'].append(results['ndcg'])\n","          \n","        excel_data = pd.DataFrame(excel_results)\n","            \n","        print(results)\n","        return results, excel_data"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":839},"id":"tjq8a9oLaQoy","executionInfo":{"status":"ok","timestamp":1637236508340,"user_tz":-330,"elapsed":442,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}},"outputId":"2df3061a-7759-4861-f514-3f39589971a3"},"source":["%reload_ext tensorboard\n","%tensorboard --logdir tensorboard"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["Reusing TensorBoard on port 6006 (pid 227), started 2:16:17 ago. (Use '!kill 227' to kill it.)"]},"metadata":{}},{"output_type":"display_data","data":{"application/javascript":["\n","        (async () => {\n","            const url = new URL(await google.colab.kernel.proxyPort(6006, {'cache': true}));\n","            url.searchParams.set('tensorboardColab', 'true');\n","            const iframe = document.createElement('iframe');\n","            iframe.src = url;\n","            iframe.setAttribute('width', '100%');\n","            iframe.setAttribute('height', '800');\n","            iframe.setAttribute('frameborder', 0);\n","            document.body.appendChild(iframe);\n","        })();\n","    "],"text/plain":["<IPython.core.display.Javascript object>"]},"metadata":{}}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["912c1c27df2645d39444fb31d580b6e9","8345b2752ac848d8bd2eeb5fa87452db","14fca280eba6402494cdc2bd805bad85","9a5c8c1781ae4affb72f4aed8ecbaa9b","95a586c260424696957c8428ef46f81a","e26fbc49e7c3437b8d6a2946ffa446d5","a36ec965a3df4d16a950869c120a4120","34f38bddc8d4465da785b875bb4178fa","8bf54d0dfabd4aea849ce964cfa89a28","19758a2fd7d54c939132c08eaba49427","b51a8eab1ea84735b435a6f786ed2008","d5c0fc6dcfba42ff91ec853655866888","a070050fb1cd45b99fcc10b05f280f86","7c4f106b503d41e5adc491fab41e2689","1d7b32e3299c4f069fcef39433f381d1","b034587a2c0741b4ba7cb9275986d767","8c6bf77ea39d48138f5b57a9188cdbe8","92689a3815bc4ff5972c792d5ae927ce","45ecfbe2d4194683962fdea8840ec448","ca3519631d5a4a82a11c7692e3f1cd04","a56ccd7e2a6b45839ae5847e24aa611f","bfbc8a93e6cd436abf843b0965b63a93","0ef4fced1ee84b2eabea4d7f5bf9e40f","bfcf80694dee450d81b6b593a1a83a94","ecdc27d9ad454b43990aa87333843b6a","271897dd6cb6449e8d1d6d8354d6327a","ed234409a0864d589f727482c7c1b89d","b10e93e426b64d0ba208fec34851f30c","18a56d0cef044affbf5315e983d46977","90f18d27ec964b11b2ed0dc6cdbc072b","e97daa5463fd4da59442e17ffc02c27a","c18d6b1b980342669d25a89951aa25db","b39dcd7c2add4224980b51b662a4e994","7e323e37c96145f3a6c95eab137b15c0","b6d78f8875ff40b08341e66bf3c0ddc0","2dc4426c29ad4a6da8ad32028b715fa5","c51ebb88de2a4767a1c90019dd701c68","b71bff74949343e992d23878966be450","1cf296ab5a564ab4b4d2d8abcbff9310","c6ba6b3191a54b0fae0c785655402aed","aef511f34d534675be13090e4cb2c484","5138422f413c4acf8cd9cb763fff7b41","6888aa3b35e14033848e8c63d1edd740","2c1e50c7114f4a6697f6eddddcdc9146"]},"id":"3Xd6J7Cd8Af9","executionInfo":{"status":"ok","timestamp":1637234734790,"user_tz":-330,"elapsed":3829956,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}},"outputId":"4a3c033d-d150-4b1c-b7a4-1552e94e6d79"},"source":["set_seed(SEED)\n","dataset = Loader(DATA_PATH)\n","model = MODELS[model_name](config, dataset)\n","model = model.to(device)\n","bpr = BPRLoss(model)\n","\n","# Pretrain\n","if pretrain:\n","    try:\n","        pretrained_file = LOAD_FILE_PATH\n","        model.load_state_dict(torch.load(pretrained_file))\n","        print(f\"loaded model weights from {pretrained_file}\")\n","    except FileNotFoundError:\n","        print(f\"{pretrained_file} not exists, start from beginning\")\n","\n","# Tensorboard\n","if tensorboard:\n","    w : SummaryWriter = SummaryWriter(\n","                                    join(BOARD_PATH, tm.strftime(\"%m-%d-%Hh%Mm%Ss-\"))\n","                                    )\n","else:\n","    w = None\n","    print(\"not enable tensorflowboard\")\n","\n","try:\n","    start_epoch = load_epoch\n","    gum_temp = ori_temp\n","    for epoch in range(start_epoch, EPOCHS+1):\n","        start = tm.time()\n","        \n","        print('Train', epoch, '='*30)\n","        print('gum_temp:', gum_temp)\n","        output_information = BPR_train_original(dataset, model, bpr, EPOCHS, gum_temp, hard=train_hard, w=w)\n","        print(f'EPOCH[{epoch}/{EPOCHS}] {output_information}')\n","        \n","        end = tm.time()\n","        print('train time:', end-start)\n","        \n","        if epoch % epoch_temp_decay == 0:\n","            # Temp decay\n","            gum_temp = ori_temp * math.exp(-gum_temp_decay*epoch)\n","            gum_temp = max(gum_temp, min_temp)\n","            print('decay gum_temp:', gum_temp)\n","        \n","        # if epoch % 10 == 0:\n","        if epoch % 1 == 0:\n","            print(\"model save...\")\n","            torch.save(model.state_dict(), SAVE_FILE_PATH+'/'+str(model_name)+'_'+str(ori_temp)+'_'+str(gum_temp_decay)+'_'+str(min_temp)+'_'+str(epoch_temp_decay)+'_'+str(config['division_noise'])+'_'+str(epoch)+\".pth.tar\")\n","            \n","            print('Valid', '='*50)\n","            valid_results, valid_excel_data = Test(dataset, model, epoch, gum_temp, hard=test_hard, mode='valid', w=w, multicore=multicore)\n","\n","            xlxs_dir = EXCEL_PATH + '/valid_'+str(model_name)+'_'+str(config['embedding_dim'])+'_'+str(ori_temp)+'_'+str(gum_temp_decay)+'_'+str(min_temp)+'_'+str(epoch_temp_decay)+'_'+str(config['division_noise'])+'_'+str(config['dropout'])+'_'+str(config['keep_prob'])+'_'+str(topks)+'.xlsx'\n","        \n","            with pd.ExcelWriter(xlxs_dir) as writer:\n","                valid_excel_data.to_excel(writer, sheet_name = 'result')            \n","            \n","            print('Test', '='*50)\n","            test_results, test_excel_data = Test(dataset, model, epoch, gum_temp, hard=test_hard, mode='test', w=w, multicore=multicore)\n","            \n","            xlxs_dir = EXCEL_PATH + '/test_'+str(model_name)+'_'+str(config['embedding_dim'])+'_'+str(ori_temp)+'_'+str(gum_temp_decay)+'_'+str(min_temp)+'_'+str(epoch_temp_decay)+'_'+str(config['division_noise'])+'_'+str(config['dropout'])+'_'+str(config['keep_prob'])+'_'+str(topks)+'.xlsx'\n","        \n","            with pd.ExcelWriter(xlxs_dir) as writer:\n","                test_excel_data.to_excel(writer, sheet_name = 'result')\n","            \n","finally:\n","    if tensorboard:\n","        w.close()"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Loading /content/data/gowalla\n","==============================\n","810128 interactions for training\n","108621 interactions for testing\n","<__main__.Loader object at 0x7fb5d94e2510> Sparsity : 0.0008396216228570436\n","==============================\n","<__main__.Loader object at 0x7fb5d94e2510> is ready to go\n","activation_function: ELU(alpha=1.0)\n","loading adjacency matrix\n","successfully train loaded...\n","don't split the matrix\n","Train 1 ==============================\n","gum_temp: 0.7\n"]},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"912c1c27df2645d39444fb31d580b6e9","version_minor":0,"version_major":2},"text/plain":["  0%|          | 0/396 [00:00<?, ?it/s]"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:41: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"]},{"output_type":"stream","name":"stdout","text":["EPOCH[1/4] loss0.386\n","train time: 534.3337316513062\n","decay gum_temp: 0.6965087354348776\n","model save...\n","Valid ==================================================\n","valid mode\n","{'precision': array([0.02332116, 0.01668453, 0.0136216 , 0.01180293, 0.01056637]), 'recall': array([0.07605411, 0.10674152, 0.13055237, 0.1500007 , 0.16690163]), 'ndcg': array([0.06059464, 0.07001512, 0.07663515, 0.08171554, 0.08589203])}\n","Test ==================================================\n","test mode\n","{'precision': array([0.02286154, 0.01636747, 0.01323375, 0.01149692, 0.01028535]), 'recall': array([0.0733682 , 0.10507128, 0.12697428, 0.14644623, 0.16393784]), 'ndcg': array([0.05854139, 0.06809456, 0.07418164, 0.07923892, 0.08345424])}\n","Train 2 ==============================\n","gum_temp: 0.6965087354348776\n"]},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"d5c0fc6dcfba42ff91ec853655866888","version_minor":0,"version_major":2},"text/plain":["  0%|          | 0/396 [00:00<?, ?it/s]"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["EPOCH[2/4] loss0.125\n","train time: 533.7940173149109\n","decay gum_temp: 0.6930348836244177\n","model save...\n","Valid ==================================================\n","valid mode\n","{'precision': array([0.02645946, 0.0188398 , 0.01532527, 0.01327243, 0.01181431]), 'recall': array([0.08573863, 0.11987728, 0.14631547, 0.1682178 , 0.18665247]), 'ndcg': array([0.06959012, 0.08001479, 0.0873653 , 0.09306554, 0.09759169])}\n","Test ==================================================\n","test mode\n","{'precision': array([0.02595954, 0.01831335, 0.01496528, 0.01297558, 0.01155402]), 'recall': array([0.08334737, 0.11692721, 0.14290043, 0.16482907, 0.18364577]), 'ndcg': array([0.06762457, 0.07773272, 0.08496656, 0.09062759, 0.09517945])}\n","Train 3 ==============================\n","gum_temp: 0.6930348836244177\n"]},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"0ef4fced1ee84b2eabea4d7f5bf9e40f","version_minor":0,"version_major":2},"text/plain":["  0%|          | 0/396 [00:00<?, ?it/s]"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["EPOCH[3/4] loss0.090\n","train time: 534.7814464569092\n","decay gum_temp: 0.6895783577221438\n","model save...\n","Valid ==================================================\n","valid mode\n","{'precision': array([0.02750109, 0.01956158, 0.01601076, 0.01382172, 0.01241183]), 'recall': array([0.08927831, 0.12504565, 0.1532377 , 0.1750567 , 0.19631655]), 'ndcg': array([0.07269772, 0.08360102, 0.09145334, 0.09717201, 0.10236788])}\n","Test ==================================================\n","test mode\n","{'precision': array([0.02699444, 0.01916404, 0.01566191, 0.01349973, 0.01205774]), 'recall': array([0.08641382, 0.12267856, 0.15009758, 0.17216471, 0.19164879]), 'ndcg': array([0.07056823, 0.08149902, 0.08911955, 0.09480704, 0.09956793])}\n","Train 4 ==============================\n","gum_temp: 0.6895783577221438\n"]},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"7e323e37c96145f3a6c95eab137b15c0","version_minor":0,"version_major":2},"text/plain":["  0%|          | 0/396 [00:00<?, ?it/s]"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["EPOCH[4/4] loss0.077\n","train time: 535.70445728302\n","decay gum_temp: 0.6861390713147286\n","model save...\n","Valid ==================================================\n","valid mode\n","{'precision': array([0.02835181, 0.02017617, 0.01645622, 0.01423201, 0.01270456]), 'recall': array([0.09210259, 0.12944523, 0.1578907 , 0.1813253 , 0.20186935]), 'ndcg': array([0.07493502, 0.08629033, 0.09422578, 0.10031303, 0.10533204])}\n","Test ==================================================\n","test mode\n","{'precision': array([0.02766093, 0.01966642, 0.01600688, 0.01386563, 0.01239936]), 'recall': array([0.08931023, 0.12607232, 0.15384466, 0.1774857 , 0.19812262]), 'ndcg': array([0.07260988, 0.08374147, 0.09144997, 0.09752696, 0.1025441 ])}\n"]}]},{"cell_type":"code","metadata":{"id":"DenNZlSA8uT1","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1637236296468,"user_tz":-330,"elapsed":5939,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}},"outputId":"68bed208-5905-4572-da22-397d6292dca6"},"source":["!apt-get -qq install tree"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Selecting previously unselected package tree.\n","(Reading database ... 155219 files and directories currently installed.)\n","Preparing to unpack .../tree_1.7.0-5_amd64.deb ...\n","Unpacking tree (1.7.0-5) ...\n","Setting up tree (1.7.0-5) ...\n","Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Hvjv05JE4zEf","executionInfo":{"status":"ok","timestamp":1637236363050,"user_tz":-330,"elapsed":431,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}},"outputId":"d5e3bb9b-531d-4aef-c653-65d01502f337"},"source":["!tree --du -h -L 3 ."],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":[".\n","├── [ 12K]  checkpoints\n","│   └── [8.0K]  HMLET_End\n","│       └── [4.0K]  gowalla\n","├── [ 13M]  data\n","│   └── [ 13M]  gowalla\n","│       ├── [6.9M]  s_pre_adj_mat_train.npz\n","│       ├── [753K]  test.txt\n","│       ├── [4.4M]  train.txt\n","│       └── [752K]  val.txt\n","├── [ 15K]  excel\n","│   ├── [5.4K]  test_HMLET_End_512_0.7_0.005_0.01_1_3_1_0.6_[10, 20, 30, 40, 50].xlsx\n","│   └── [5.4K]  valid_HMLET_End_512_0.7_0.005_0.01_1_3_1_0.6_[10, 20, 30, 40, 50].xlsx\n","└── [218K]  tensorboard\n","    ├── [ 30K]  11-18-09h39m04s-\n","    │   └── [ 26K]  events.out.tfevents.1637228346.9f9d02c6c3c0.161.0\n","    ├── [ 43K]  11-18-09h56m39s-\n","    │   └── [ 39K]  events.out.tfevents.1637229401.9f9d02c6c3c0.432.0\n","    └── [141K]  11-18-10h21m54s-\n","        ├── [ 77K]  events.out.tfevents.1637230914.9f9d02c6c3c0.432.1\n","        ├── [4.0K]  Test_NDCG@[10, 20, 30, 40, 50]_10\n","        ├── [4.0K]  Test_NDCG@[10, 20, 30, 40, 50]_20\n","        ├── [4.0K]  Test_NDCG@[10, 20, 30, 40, 50]_30\n","        ├── [4.0K]  Test_NDCG@[10, 20, 30, 40, 50]_40\n","        ├── [4.0K]  Test_NDCG@[10, 20, 30, 40, 50]_50\n","        ├── [4.0K]  Test_Precision@[10, 20, 30, 40, 50]_10\n","        ├── [4.0K]  Test_Precision@[10, 20, 30, 40, 50]_20\n","        ├── [4.0K]  Test_Precision@[10, 20, 30, 40, 50]_30\n","        ├── [4.0K]  Test_Precision@[10, 20, 30, 40, 50]_40\n","        ├── [4.0K]  Test_Precision@[10, 20, 30, 40, 50]_50\n","        ├── [4.0K]  Test_Recall@[10, 20, 30, 40, 50]_10\n","        ├── [4.0K]  Test_Recall@[10, 20, 30, 40, 50]_20\n","        ├── [4.0K]  Test_Recall@[10, 20, 30, 40, 50]_30\n","        ├── [4.0K]  Test_Recall@[10, 20, 30, 40, 50]_40\n","        └── [4.0K]  Test_Recall@[10, 20, 30, 40, 50]_50\n","\n","  13M used in 25 directories, 9 files\n"]}]},{"cell_type":"code","metadata":{"id":"JF6Z1Z2F415w"},"source":[""],"execution_count":null,"outputs":[]}]}