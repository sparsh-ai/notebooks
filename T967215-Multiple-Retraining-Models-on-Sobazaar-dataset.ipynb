{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"T967215 | Multiple Retraining Models on Sobazaar dataset","provenance":[{"file_id":"1RRiC55q03vJg8vr-Fr9MbDVr8VHTN8XZ","timestamp":1636618766183}],"collapsed_sections":[],"mount_file_id":"1M5g9Rtk1HxbkmNdQrdKwv0ZQ6XgtbJRO","authorship_tag":"ABX9TyMzrep83NlT2XxugRV3uKV6"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"83c9f2d6bc6a42109250d55a39b4b3e7":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_6af9c2d14cd344408ebeb165aecd5812","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_14a3c2eab70348fd8336ae11dbbfcffd","IPY_MODEL_1f7c997368b34aedb5ecafc1ad6b1121","IPY_MODEL_712fb93cec584db29a18f17f48ce85dc"]}},"6af9c2d14cd344408ebeb165aecd5812":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"14a3c2eab70348fd8336ae11dbbfcffd":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_6149942fe6ad4e0ea119545de5a8d242","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"100%","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_f4349bb599ab413c9c087714eb7ba11f"}},"1f7c997368b34aedb5ecafc1ad6b1121":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_7b90eb42b83641b4890847c92356a8f6","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":842660,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":842660,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_8d3fb8b1ab5742ec983d41c822319729"}},"712fb93cec584db29a18f17f48ce85dc":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_27e8cfe72e7a4a93b1f2e1f705a551a5","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 842660/842660 [35:09&lt;00:00, 407.13it/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_74ce21ef75514a30a12822ebc5165352"}},"6149942fe6ad4e0ea119545de5a8d242":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"f4349bb599ab413c9c087714eb7ba11f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"7b90eb42b83641b4890847c92356a8f6":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"8d3fb8b1ab5742ec983d41c822319729":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"27e8cfe72e7a4a93b1f2e1f705a551a5":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"74ce21ef75514a30a12822ebc5165352":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"30c470ccdef34a2999ab74aad3503ba7":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_7ef1f03aed684fdbbc6602ec174cd2cd","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_45c6daf562174f79ad42975093c84210","IPY_MODEL_d3fc9033cc8f45c9a9bc114a6c590fd9","IPY_MODEL_53e9c4c383124944a89e833d7f0a56df"]}},"7ef1f03aed684fdbbc6602ec174cd2cd":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"45c6daf562174f79ad42975093c84210":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_e69ef6e531314efe9a9ee6dc58e5998b","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"100%","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_813103a929a84c78aa67e72419563cb7"}},"d3fc9033cc8f45c9a9bc114a6c590fd9":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_0fa28afc080a4e98a0f5ccd7d083e3b3","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":842660,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":842660,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_60fe4254494e4bc589c971c5e287ec88"}},"53e9c4c383124944a89e833d7f0a56df":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_bb7538a3fc0045c28b51edc7f7cbbc00","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 842660/842660 [00:03&lt;00:00, 266169.09it/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_d30f88908fe54f779659b41bd1e5ba6f"}},"e69ef6e531314efe9a9ee6dc58e5998b":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"813103a929a84c78aa67e72419563cb7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"0fa28afc080a4e98a0f5ccd7d083e3b3":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"60fe4254494e4bc589c971c5e287ec88":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"bb7538a3fc0045c28b51edc7f7cbbc00":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"d30f88908fe54f779659b41bd1e5ba6f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"1QG8dU-gzPyr"},"source":["# Multiple Retraining Models on Sobazaar dataset"]},{"cell_type":"markdown","metadata":{"id":"CxiWmRiFzT2X"},"source":["## Setup"]},{"cell_type":"markdown","metadata":{"id":"zVtJ4JTGH353"},"source":["### Git"]},{"cell_type":"code","metadata":{"id":"Z3qjPp055tXf","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1636620657504,"user_tz":-330,"elapsed":4282,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}},"outputId":"f5f01ff8-ff91-4be8-d164-3a9d79753b0d"},"source":["import os\n","project_name = \"incremental-learning\"; branch = \"T967215\"; account = \"sparsh-ai\"\n","project_path = os.path.join('/content', branch)\n","\n","if not os.path.exists(project_path):\n","    !cp -r /content/drive/MyDrive/git_credentials/. ~\n","    !mkdir \"{project_path}\"\n","    %cd \"{project_path}\"\n","    !git init\n","    !git remote add origin https://github.com/\"{account}\"/\"{project_name}\".git\n","    !git pull origin \"{branch}\"\n","    !git checkout -b \"{branch}\"\n","else:\n","    %cd \"{project_path}\""],"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/T967215\n","Initialized empty Git repository in /content/T967215/.git/\n","fatal: Couldn't find remote ref T967215\n","Switched to a new branch 'T967215'\n"]}]},{"cell_type":"code","metadata":{"id":"xoKGydGDIwSX","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1636620657506,"user_tz":-330,"elapsed":11,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}},"outputId":"8489373e-9df2-40dd-b93e-5da8ba4ef2ad"},"source":["%cd /content"],"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["/content\n"]}]},{"cell_type":"code","metadata":{"id":"7lAKlgUD5tXi","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1636624928503,"user_tz":-330,"elapsed":20142,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}},"outputId":"1c95a717-c06e-4e2b-80e3-5596ea0f3abb"},"source":["!cd /content/T967215 && git add .\n","!cd /content/T967215 && git commit -m 'commit'"],"execution_count":61,"outputs":[{"output_type":"stream","name":"stdout","text":["[T967215 (root-commit) 9d96cd2] commit\n"," 460 files changed, 526 insertions(+)\n"," create mode 100644 ckpts/BU_train11-23_test24-30_7_1epoch_0.001/period11/Epoch1_TestAUC0.8025_TestLOGLOSS0.8145.ckpt.data-00000-of-00001\n"," create mode 100644 ckpts/BU_train11-23_test24-30_7_1epoch_0.001/period11/Epoch1_TestAUC0.8025_TestLOGLOSS0.8145.ckpt.index\n"," create mode 100644 ckpts/BU_train11-23_test24-30_7_1epoch_0.001/period11/Epoch1_TestAUC0.8025_TestLOGLOSS0.8145.ckpt.meta\n"," create mode 100644 ckpts/BU_train11-23_test24-30_7_1epoch_0.001/period11/checkpoint\n"," create mode 100644 ckpts/BU_train11-23_test24-30_7_1epoch_0.001/period11/config.txt\n"," create mode 100644 ckpts/BU_train11-23_test24-30_7_1epoch_0.001/period12/Epoch1_TestAUC0.8668_TestLOGLOSS0.5196.ckpt.data-00000-of-00001\n"," create mode 100644 ckpts/BU_train11-23_test24-30_7_1epoch_0.001/period12/Epoch1_TestAUC0.8668_TestLOGLOSS0.5196.ckpt.index\n"," create mode 100644 ckpts/BU_train11-23_test24-30_7_1epoch_0.001/period12/Epoch1_TestAUC0.8668_TestLOGLOSS0.5196.ckpt.meta\n"," create mode 100644 ckpts/BU_train11-23_test24-30_7_1epoch_0.001/period12/checkpoint\n"," create mode 100644 ckpts/BU_train11-23_test24-30_7_1epoch_0.001/period12/config.txt\n"," create mode 100644 ckpts/BU_train11-23_test24-30_7_1epoch_0.001/period13/Epoch1_TestAUC0.8662_TestLOGLOSS0.5410.ckpt.data-00000-of-00001\n"," create mode 100644 ckpts/BU_train11-23_test24-30_7_1epoch_0.001/period13/Epoch1_TestAUC0.8662_TestLOGLOSS0.5410.ckpt.index\n"," create mode 100644 ckpts/BU_train11-23_test24-30_7_1epoch_0.001/period13/Epoch1_TestAUC0.8662_TestLOGLOSS0.5410.ckpt.meta\n"," create mode 100644 ckpts/BU_train11-23_test24-30_7_1epoch_0.001/period13/checkpoint\n"," create mode 100644 ckpts/BU_train11-23_test24-30_7_1epoch_0.001/period13/config.txt\n"," create mode 100644 ckpts/BU_train11-23_test24-30_7_1epoch_0.001/period14/Epoch1_TestAUC0.8888_TestLOGLOSS0.4770.ckpt.data-00000-of-00001\n"," create mode 100644 ckpts/BU_train11-23_test24-30_7_1epoch_0.001/period14/Epoch1_TestAUC0.8888_TestLOGLOSS0.4770.ckpt.index\n"," create mode 100644 ckpts/BU_train11-23_test24-30_7_1epoch_0.001/period14/Epoch1_TestAUC0.8888_TestLOGLOSS0.4770.ckpt.meta\n"," create mode 100644 ckpts/BU_train11-23_test24-30_7_1epoch_0.001/period14/checkpoint\n"," create mode 100644 ckpts/BU_train11-23_test24-30_7_1epoch_0.001/period14/config.txt\n"," create mode 100644 ckpts/BU_train11-23_test24-30_7_1epoch_0.001/period15/Epoch1_TestAUC0.7779_TestLOGLOSS1.0132.ckpt.data-00000-of-00001\n"," create mode 100644 ckpts/BU_train11-23_test24-30_7_1epoch_0.001/period15/Epoch1_TestAUC0.7779_TestLOGLOSS1.0132.ckpt.index\n"," create mode 100644 ckpts/BU_train11-23_test24-30_7_1epoch_0.001/period15/Epoch1_TestAUC0.7779_TestLOGLOSS1.0132.ckpt.meta\n"," create mode 100644 ckpts/BU_train11-23_test24-30_7_1epoch_0.001/period15/checkpoint\n"," create mode 100644 ckpts/BU_train11-23_test24-30_7_1epoch_0.001/period15/config.txt\n"," create mode 100644 ckpts/BU_train11-23_test24-30_7_1epoch_0.001/period16/Epoch1_TestAUC0.8123_TestLOGLOSS0.6530.ckpt.data-00000-of-00001\n"," create mode 100644 ckpts/BU_train11-23_test24-30_7_1epoch_0.001/period16/Epoch1_TestAUC0.8123_TestLOGLOSS0.6530.ckpt.index\n"," create mode 100644 ckpts/BU_train11-23_test24-30_7_1epoch_0.001/period16/Epoch1_TestAUC0.8123_TestLOGLOSS0.6530.ckpt.meta\n"," create mode 100644 ckpts/BU_train11-23_test24-30_7_1epoch_0.001/period16/checkpoint\n"," create mode 100644 ckpts/BU_train11-23_test24-30_7_1epoch_0.001/period16/config.txt\n"," create mode 100644 ckpts/BU_train11-23_test24-30_7_1epoch_0.001/period17/Epoch1_TestAUC0.8275_TestLOGLOSS0.5985.ckpt.data-00000-of-00001\n"," create mode 100644 ckpts/BU_train11-23_test24-30_7_1epoch_0.001/period17/Epoch1_TestAUC0.8275_TestLOGLOSS0.5985.ckpt.index\n"," create mode 100644 ckpts/BU_train11-23_test24-30_7_1epoch_0.001/period17/Epoch1_TestAUC0.8275_TestLOGLOSS0.5985.ckpt.meta\n"," create mode 100644 ckpts/BU_train11-23_test24-30_7_1epoch_0.001/period17/checkpoint\n"," create mode 100644 ckpts/BU_train11-23_test24-30_7_1epoch_0.001/period17/config.txt\n"," create mode 100644 ckpts/BU_train11-23_test24-30_7_1epoch_0.001/period18/Epoch1_TestAUC0.7612_TestLOGLOSS0.7718.ckpt.data-00000-of-00001\n"," create mode 100644 ckpts/BU_train11-23_test24-30_7_1epoch_0.001/period18/Epoch1_TestAUC0.7612_TestLOGLOSS0.7718.ckpt.index\n"," create mode 100644 ckpts/BU_train11-23_test24-30_7_1epoch_0.001/period18/Epoch1_TestAUC0.7612_TestLOGLOSS0.7718.ckpt.meta\n"," create mode 100644 ckpts/BU_train11-23_test24-30_7_1epoch_0.001/period18/checkpoint\n"," create mode 100644 ckpts/BU_train11-23_test24-30_7_1epoch_0.001/period18/config.txt\n"," create mode 100644 ckpts/BU_train11-23_test24-30_7_1epoch_0.001/period19/Epoch1_TestAUC0.8044_TestLOGLOSS0.6301.ckpt.data-00000-of-00001\n"," create mode 100644 ckpts/BU_train11-23_test24-30_7_1epoch_0.001/period19/Epoch1_TestAUC0.8044_TestLOGLOSS0.6301.ckpt.index\n"," create mode 100644 ckpts/BU_train11-23_test24-30_7_1epoch_0.001/period19/Epoch1_TestAUC0.8044_TestLOGLOSS0.6301.ckpt.meta\n"," create mode 100644 ckpts/BU_train11-23_test24-30_7_1epoch_0.001/period19/checkpoint\n"," create mode 100644 ckpts/BU_train11-23_test24-30_7_1epoch_0.001/period19/config.txt\n"," create mode 100644 ckpts/BU_train11-23_test24-30_7_1epoch_0.001/period20/Epoch1_TestAUC0.7943_TestLOGLOSS0.6656.ckpt.data-00000-of-00001\n"," create mode 100644 ckpts/BU_train11-23_test24-30_7_1epoch_0.001/period20/Epoch1_TestAUC0.7943_TestLOGLOSS0.6656.ckpt.index\n"," create mode 100644 ckpts/BU_train11-23_test24-30_7_1epoch_0.001/period20/Epoch1_TestAUC0.7943_TestLOGLOSS0.6656.ckpt.meta\n"," create mode 100644 ckpts/BU_train11-23_test24-30_7_1epoch_0.001/period20/checkpoint\n"," create mode 100644 ckpts/BU_train11-23_test24-30_7_1epoch_0.001/period20/config.txt\n"," create mode 100644 ckpts/BU_train11-23_test24-30_7_1epoch_0.001/period21/Epoch1_TestAUC0.7940_TestLOGLOSS0.6647.ckpt.data-00000-of-00001\n"," create mode 100644 ckpts/BU_train11-23_test24-30_7_1epoch_0.001/period21/Epoch1_TestAUC0.7940_TestLOGLOSS0.6647.ckpt.index\n"," create mode 100644 ckpts/BU_train11-23_test24-30_7_1epoch_0.001/period21/Epoch1_TestAUC0.7940_TestLOGLOSS0.6647.ckpt.meta\n"," create mode 100644 ckpts/BU_train11-23_test24-30_7_1epoch_0.001/period21/checkpoint\n"," create mode 100644 ckpts/BU_train11-23_test24-30_7_1epoch_0.001/period21/config.txt\n"," create mode 100644 ckpts/BU_train11-23_test24-30_7_1epoch_0.001/period22/Epoch1_TestAUC0.7836_TestLOGLOSS0.6920.ckpt.data-00000-of-00001\n"," create mode 100644 ckpts/BU_train11-23_test24-30_7_1epoch_0.001/period22/Epoch1_TestAUC0.7836_TestLOGLOSS0.6920.ckpt.index\n"," create mode 100644 ckpts/BU_train11-23_test24-30_7_1epoch_0.001/period22/Epoch1_TestAUC0.7836_TestLOGLOSS0.6920.ckpt.meta\n"," create mode 100644 ckpts/BU_train11-23_test24-30_7_1epoch_0.001/period22/checkpoint\n"," create mode 100644 ckpts/BU_train11-23_test24-30_7_1epoch_0.001/period22/config.txt\n"," create mode 100644 ckpts/BU_train11-23_test24-30_7_1epoch_0.001/period23/Epoch1_TestAUC0.8146_TestLOGLOSS0.6122.ckpt.data-00000-of-00001\n"," create mode 100644 ckpts/BU_train11-23_test24-30_7_1epoch_0.001/period23/Epoch1_TestAUC0.8146_TestLOGLOSS0.6122.ckpt.index\n"," create mode 100644 ckpts/BU_train11-23_test24-30_7_1epoch_0.001/period23/Epoch1_TestAUC0.8146_TestLOGLOSS0.6122.ckpt.meta\n"," create mode 100644 ckpts/BU_train11-23_test24-30_7_1epoch_0.001/period23/checkpoint\n"," create mode 100644 ckpts/BU_train11-23_test24-30_7_1epoch_0.001/period23/config.txt\n"," create mode 100644 ckpts/BU_train11-23_test24-30_7_1epoch_0.001/period24/Epoch1_TestAUC0.7734_TestLOGLOSS0.7131.ckpt.data-00000-of-00001\n"," create mode 100644 ckpts/BU_train11-23_test24-30_7_1epoch_0.001/period24/Epoch1_TestAUC0.7734_TestLOGLOSS0.7131.ckpt.index\n"," create mode 100644 ckpts/BU_train11-23_test24-30_7_1epoch_0.001/period24/Epoch1_TestAUC0.7734_TestLOGLOSS0.7131.ckpt.meta\n"," create mode 100644 ckpts/BU_train11-23_test24-30_7_1epoch_0.001/period24/checkpoint\n"," create mode 100644 ckpts/BU_train11-23_test24-30_7_1epoch_0.001/period24/config.txt\n"," create mode 100644 ckpts/BU_train11-23_test24-30_7_1epoch_0.001/period24/test_metrics.txt\n"," create mode 100644 ckpts/BU_train11-23_test24-30_7_1epoch_0.001/period25/Epoch1_TestAUC0.7849_TestLOGLOSS0.6751.ckpt.data-00000-of-00001\n"," create mode 100644 ckpts/BU_train11-23_test24-30_7_1epoch_0.001/period25/Epoch1_TestAUC0.7849_TestLOGLOSS0.6751.ckpt.index\n"," create mode 100644 ckpts/BU_train11-23_test24-30_7_1epoch_0.001/period25/Epoch1_TestAUC0.7849_TestLOGLOSS0.6751.ckpt.meta\n"," create mode 100644 ckpts/BU_train11-23_test24-30_7_1epoch_0.001/period25/checkpoint\n"," create mode 100644 ckpts/BU_train11-23_test24-30_7_1epoch_0.001/period25/config.txt\n"," create mode 100644 ckpts/BU_train11-23_test24-30_7_1epoch_0.001/period25/test_metrics.txt\n"," create mode 100644 ckpts/BU_train11-23_test24-30_7_1epoch_0.001/period26/Epoch1_TestAUC0.7503_TestLOGLOSS0.7431.ckpt.data-00000-of-00001\n"," create mode 100644 ckpts/BU_train11-23_test24-30_7_1epoch_0.001/period26/Epoch1_TestAUC0.7503_TestLOGLOSS0.7431.ckpt.index\n"," create mode 100644 ckpts/BU_train11-23_test24-30_7_1epoch_0.001/period26/Epoch1_TestAUC0.7503_TestLOGLOSS0.7431.ckpt.meta\n"," create mode 100644 ckpts/BU_train11-23_test24-30_7_1epoch_0.001/period26/checkpoint\n"," create mode 100644 ckpts/BU_train11-23_test24-30_7_1epoch_0.001/period26/config.txt\n"," create mode 100644 ckpts/BU_train11-23_test24-30_7_1epoch_0.001/period26/test_metrics.txt\n"," create mode 100644 ckpts/BU_train11-23_test24-30_7_1epoch_0.001/period27/Epoch1_TestAUC0.7771_TestLOGLOSS0.6741.ckpt.data-00000-of-00001\n"," create mode 100644 ckpts/BU_train11-23_test24-30_7_1epoch_0.001/period27/Epoch1_TestAUC0.7771_TestLOGLOSS0.6741.ckpt.index\n"," create mode 100644 ckpts/BU_train11-23_test24-30_7_1epoch_0.001/period27/Epoch1_TestAUC0.7771_TestLOGLOSS0.6741.ckpt.meta\n"," create mode 100644 ckpts/BU_train11-23_test24-30_7_1epoch_0.001/period27/checkpoint\n"," create mode 100644 ckpts/BU_train11-23_test24-30_7_1epoch_0.001/period27/config.txt\n"," create mode 100644 ckpts/BU_train11-23_test24-30_7_1epoch_0.001/period27/test_metrics.txt\n"," create mode 100644 ckpts/BU_train11-23_test24-30_7_1epoch_0.001/period28/Epoch1_TestAUC0.7741_TestLOGLOSS0.6496.ckpt.data-00000-of-00001\n"," create mode 100644 ckpts/BU_train11-23_test24-30_7_1epoch_0.001/period28/Epoch1_TestAUC0.7741_TestLOGLOSS0.6496.ckpt.index\n"," create mode 100644 ckpts/BU_train11-23_test24-30_7_1epoch_0.001/period28/Epoch1_TestAUC0.7741_TestLOGLOSS0.6496.ckpt.meta\n"," create mode 100644 ckpts/BU_train11-23_test24-30_7_1epoch_0.001/period28/checkpoint\n"," create mode 100644 ckpts/BU_train11-23_test24-30_7_1epoch_0.001/period28/config.txt\n"," create mode 100644 ckpts/BU_train11-23_test24-30_7_1epoch_0.001/period28/test_metrics.txt\n"," create mode 100644 ckpts/BU_train11-23_test24-30_7_1epoch_0.001/period29/Epoch1_TestAUC0.7987_TestLOGLOSS0.6117.ckpt.data-00000-of-00001\n"," create mode 100644 ckpts/BU_train11-23_test24-30_7_1epoch_0.001/period29/Epoch1_TestAUC0.7987_TestLOGLOSS0.6117.ckpt.index\n"," create mode 100644 ckpts/BU_train11-23_test24-30_7_1epoch_0.001/period29/Epoch1_TestAUC0.7987_TestLOGLOSS0.6117.ckpt.meta\n"," create mode 100644 ckpts/BU_train11-23_test24-30_7_1epoch_0.001/period29/checkpoint\n"," create mode 100644 ckpts/BU_train11-23_test24-30_7_1epoch_0.001/period29/config.txt\n"," create mode 100644 ckpts/BU_train11-23_test24-30_7_1epoch_0.001/period29/test_metrics.txt\n"," create mode 100644 ckpts/BU_train11-23_test24-30_7_1epoch_0.001/period30/Epoch1_TestAUC0.7889_TestLOGLOSS0.6412.ckpt.data-00000-of-00001\n"," create mode 100644 ckpts/BU_train11-23_test24-30_7_1epoch_0.001/period30/Epoch1_TestAUC0.7889_TestLOGLOSS0.6412.ckpt.index\n"," create mode 100644 ckpts/BU_train11-23_test24-30_7_1epoch_0.001/period30/Epoch1_TestAUC0.7889_TestLOGLOSS0.6412.ckpt.meta\n"," create mode 100644 ckpts/BU_train11-23_test24-30_7_1epoch_0.001/period30/checkpoint\n"," create mode 100644 ckpts/BU_train11-23_test24-30_7_1epoch_0.001/period30/config.txt\n"," create mode 100644 ckpts/BU_train11-23_test24-30_7_1epoch_0.001/period30/test_metrics.txt\n"," create mode 100644 ckpts/IncCTR_train11-23_test24-30_1epoch_0.1_0.001/period11/Epoch1_TestAUC0.8024_TestLOGLOSS0.6749.ckpt.data-00000-of-00001\n"," create mode 100644 ckpts/IncCTR_train11-23_test24-30_1epoch_0.1_0.001/period11/Epoch1_TestAUC0.8024_TestLOGLOSS0.6749.ckpt.index\n"," create mode 100644 ckpts/IncCTR_train11-23_test24-30_1epoch_0.1_0.001/period11/Epoch1_TestAUC0.8024_TestLOGLOSS0.6749.ckpt.meta\n"," create mode 100644 ckpts/IncCTR_train11-23_test24-30_1epoch_0.1_0.001/period11/checkpoint\n"," create mode 100644 ckpts/IncCTR_train11-23_test24-30_1epoch_0.1_0.001/period11/config.txt\n"," create mode 100644 ckpts/IncCTR_train11-23_test24-30_1epoch_0.1_0.001/period12/Epoch1_TestAUC0.8667_TestLOGLOSS0.4723.ckpt.data-00000-of-00001\n"," create mode 100644 ckpts/IncCTR_train11-23_test24-30_1epoch_0.1_0.001/period12/Epoch1_TestAUC0.8667_TestLOGLOSS0.4723.ckpt.index\n"," create mode 100644 ckpts/IncCTR_train11-23_test24-30_1epoch_0.1_0.001/period12/Epoch1_TestAUC0.8667_TestLOGLOSS0.4723.ckpt.meta\n"," create mode 100644 ckpts/IncCTR_train11-23_test24-30_1epoch_0.1_0.001/period12/checkpoint\n"," create mode 100644 ckpts/IncCTR_train11-23_test24-30_1epoch_0.1_0.001/period12/config.txt\n"," create mode 100644 ckpts/IncCTR_train11-23_test24-30_1epoch_0.1_0.001/period13/Epoch1_TestAUC0.8768_TestLOGLOSS0.4561.ckpt.data-00000-of-00001\n"," create mode 100644 ckpts/IncCTR_train11-23_test24-30_1epoch_0.1_0.001/period13/Epoch1_TestAUC0.8768_TestLOGLOSS0.4561.ckpt.index\n"," create mode 100644 ckpts/IncCTR_train11-23_test24-30_1epoch_0.1_0.001/period13/Epoch1_TestAUC0.8768_TestLOGLOSS0.4561.ckpt.meta\n"," create mode 100644 ckpts/IncCTR_train11-23_test24-30_1epoch_0.1_0.001/period13/checkpoint\n"," create mode 100644 ckpts/IncCTR_train11-23_test24-30_1epoch_0.1_0.001/period13/config.txt\n"," create mode 100644 ckpts/IncCTR_train11-23_test24-30_1epoch_0.1_0.001/period14/Epoch1_TestAUC0.8948_TestLOGLOSS0.4233.ckpt.data-00000-of-00001\n"," create mode 100644 ckpts/IncCTR_train11-23_test24-30_1epoch_0.1_0.001/period14/Epoch1_TestAUC0.8948_TestLOGLOSS0.4233.ckpt.index\n"," create mode 100644 ckpts/IncCTR_train11-23_test24-30_1epoch_0.1_0.001/period14/Epoch1_TestAUC0.8948_TestLOGLOSS0.4233.ckpt.meta\n"," create mode 100644 ckpts/IncCTR_train11-23_test24-30_1epoch_0.1_0.001/period14/checkpoint\n"," create mode 100644 ckpts/IncCTR_train11-23_test24-30_1epoch_0.1_0.001/period14/config.txt\n"," create mode 100644 ckpts/IncCTR_train11-23_test24-30_1epoch_0.1_0.001/period15/Epoch1_TestAUC0.7880_TestLOGLOSS0.7285.ckpt.data-00000-of-00001\n"," create mode 100644 ckpts/IncCTR_train11-23_test24-30_1epoch_0.1_0.001/period15/Epoch1_TestAUC0.7880_TestLOGLOSS0.7285.ckpt.index\n"," create mode 100644 ckpts/IncCTR_train11-23_test24-30_1epoch_0.1_0.001/period15/Epoch1_TestAUC0.7880_TestLOGLOSS0.7285.ckpt.meta\n"," create mode 100644 ckpts/IncCTR_train11-23_test24-30_1epoch_0.1_0.001/period15/checkpoint\n"," create mode 100644 ckpts/IncCTR_train11-23_test24-30_1epoch_0.1_0.001/period15/config.txt\n"," create mode 100644 ckpts/IncCTR_train11-23_test24-30_1epoch_0.1_0.001/period16/Epoch1_TestAUC0.8313_TestLOGLOSS0.5272.ckpt.data-00000-of-00001\n"," create mode 100644 ckpts/IncCTR_train11-23_test24-30_1epoch_0.1_0.001/period16/Epoch1_TestAUC0.8313_TestLOGLOSS0.5272.ckpt.index\n"," create mode 100644 ckpts/IncCTR_train11-23_test24-30_1epoch_0.1_0.001/period16/Epoch1_TestAUC0.8313_TestLOGLOSS0.5272.ckpt.meta\n"," create mode 100644 ckpts/IncCTR_train11-23_test24-30_1epoch_0.1_0.001/period16/checkpoint\n"," create mode 100644 ckpts/IncCTR_train11-23_test24-30_1epoch_0.1_0.001/period16/config.txt\n"," create mode 100644 ckpts/IncCTR_train11-23_test24-30_1epoch_0.1_0.001/period17/Epoch1_TestAUC0.8388_TestLOGLOSS0.5136.ckpt.data-00000-of-00001\n"," create mode 100644 ckpts/IncCTR_train11-23_test24-30_1epoch_0.1_0.001/period17/Epoch1_TestAUC0.8388_TestLOGLOSS0.5136.ckpt.index\n"," create mode 100644 ckpts/IncCTR_train11-23_test24-30_1epoch_0.1_0.001/period17/Epoch1_TestAUC0.8388_TestLOGLOSS0.5136.ckpt.meta\n"," create mode 100644 ckpts/IncCTR_train11-23_test24-30_1epoch_0.1_0.001/period17/checkpoint\n"," create mode 100644 ckpts/IncCTR_train11-23_test24-30_1epoch_0.1_0.001/period17/config.txt\n"," create mode 100644 ckpts/IncCTR_train11-23_test24-30_1epoch_0.1_0.001/period18/Epoch1_TestAUC0.7712_TestLOGLOSS0.6423.ckpt.data-00000-of-00001\n"," create mode 100644 ckpts/IncCTR_train11-23_test24-30_1epoch_0.1_0.001/period18/Epoch1_TestAUC0.7712_TestLOGLOSS0.6423.ckpt.index\n"," create mode 100644 ckpts/IncCTR_train11-23_test24-30_1epoch_0.1_0.001/period18/Epoch1_TestAUC0.7712_TestLOGLOSS0.6423.ckpt.meta\n"," create mode 100644 ckpts/IncCTR_train11-23_test24-30_1epoch_0.1_0.001/period18/checkpoint\n"," create mode 100644 ckpts/IncCTR_train11-23_test24-30_1epoch_0.1_0.001/period18/config.txt\n"," create mode 100644 ckpts/IncCTR_train11-23_test24-30_1epoch_0.1_0.001/period19/Epoch1_TestAUC0.8322_TestLOGLOSS0.5116.ckpt.data-00000-of-00001\n"," create mode 100644 ckpts/IncCTR_train11-23_test24-30_1epoch_0.1_0.001/period19/Epoch1_TestAUC0.8322_TestLOGLOSS0.5116.ckpt.index\n"," create mode 100644 ckpts/IncCTR_train11-23_test24-30_1epoch_0.1_0.001/period19/Epoch1_TestAUC0.8322_TestLOGLOSS0.5116.ckpt.meta\n"," create mode 100644 ckpts/IncCTR_train11-23_test24-30_1epoch_0.1_0.001/period19/checkpoint\n"," create mode 100644 ckpts/IncCTR_train11-23_test24-30_1epoch_0.1_0.001/period19/config.txt\n"," create mode 100644 ckpts/IncCTR_train11-23_test24-30_1epoch_0.1_0.001/period20/Epoch1_TestAUC0.8183_TestLOGLOSS0.5510.ckpt.data-00000-of-00001\n"," create mode 100644 ckpts/IncCTR_train11-23_test24-30_1epoch_0.1_0.001/period20/Epoch1_TestAUC0.8183_TestLOGLOSS0.5510.ckpt.index\n"," create mode 100644 ckpts/IncCTR_train11-23_test24-30_1epoch_0.1_0.001/period20/Epoch1_TestAUC0.8183_TestLOGLOSS0.5510.ckpt.meta\n"," create mode 100644 ckpts/IncCTR_train11-23_test24-30_1epoch_0.1_0.001/period20/checkpoint\n"," create mode 100644 ckpts/IncCTR_train11-23_test24-30_1epoch_0.1_0.001/period20/config.txt\n"," create mode 100644 ckpts/IncCTR_train11-23_test24-30_1epoch_0.1_0.001/period21/Epoch1_TestAUC0.8198_TestLOGLOSS0.5354.ckpt.data-00000-of-00001\n"," create mode 100644 ckpts/IncCTR_train11-23_test24-30_1epoch_0.1_0.001/period21/Epoch1_TestAUC0.8198_TestLOGLOSS0.5354.ckpt.index\n"," create mode 100644 ckpts/IncCTR_train11-23_test24-30_1epoch_0.1_0.001/period21/Epoch1_TestAUC0.8198_TestLOGLOSS0.5354.ckpt.meta\n"," create mode 100644 ckpts/IncCTR_train11-23_test24-30_1epoch_0.1_0.001/period21/checkpoint\n"," create mode 100644 ckpts/IncCTR_train11-23_test24-30_1epoch_0.1_0.001/period21/config.txt\n"," create mode 100644 ckpts/IncCTR_train11-23_test24-30_1epoch_0.1_0.001/period22/Epoch1_TestAUC0.8093_TestLOGLOSS0.5477.ckpt.data-00000-of-00001\n"," create mode 100644 ckpts/IncCTR_train11-23_test24-30_1epoch_0.1_0.001/period22/Epoch1_TestAUC0.8093_TestLOGLOSS0.5477.ckpt.index\n"," create mode 100644 ckpts/IncCTR_train11-23_test24-30_1epoch_0.1_0.001/period22/Epoch1_TestAUC0.8093_TestLOGLOSS0.5477.ckpt.meta\n"," create mode 100644 ckpts/IncCTR_train11-23_test24-30_1epoch_0.1_0.001/period22/checkpoint\n"," create mode 100644 ckpts/IncCTR_train11-23_test24-30_1epoch_0.1_0.001/period22/config.txt\n"," create mode 100644 ckpts/IncCTR_train11-23_test24-30_1epoch_0.1_0.001/period23/Epoch1_TestAUC0.8330_TestLOGLOSS0.5103.ckpt.data-00000-of-00001\n"," create mode 100644 ckpts/IncCTR_train11-23_test24-30_1epoch_0.1_0.001/period23/Epoch1_TestAUC0.8330_TestLOGLOSS0.5103.ckpt.index\n"," create mode 100644 ckpts/IncCTR_train11-23_test24-30_1epoch_0.1_0.001/period23/Epoch1_TestAUC0.8330_TestLOGLOSS0.5103.ckpt.meta\n"," create mode 100644 ckpts/IncCTR_train11-23_test24-30_1epoch_0.1_0.001/period23/checkpoint\n"," create mode 100644 ckpts/IncCTR_train11-23_test24-30_1epoch_0.1_0.001/period23/config.txt\n"," create mode 100644 ckpts/IncCTR_train11-23_test24-30_1epoch_0.1_0.001/period24/Epoch1_TestAUC0.7946_TestLOGLOSS0.5716.ckpt.data-00000-of-00001\n"," create mode 100644 ckpts/IncCTR_train11-23_test24-30_1epoch_0.1_0.001/period24/Epoch1_TestAUC0.7946_TestLOGLOSS0.5716.ckpt.index\n"," create mode 100644 ckpts/IncCTR_train11-23_test24-30_1epoch_0.1_0.001/period24/Epoch1_TestAUC0.7946_TestLOGLOSS0.5716.ckpt.meta\n"," create mode 100644 ckpts/IncCTR_train11-23_test24-30_1epoch_0.1_0.001/period24/checkpoint\n"," create mode 100644 ckpts/IncCTR_train11-23_test24-30_1epoch_0.1_0.001/period24/config.txt\n"," create mode 100644 ckpts/IncCTR_train11-23_test24-30_1epoch_0.1_0.001/period24/test_metrics.txt\n"," create mode 100644 ckpts/IncCTR_train11-23_test24-30_1epoch_0.1_0.001/period25/Epoch1_TestAUC0.8054_TestLOGLOSS0.5466.ckpt.data-00000-of-00001\n"," create mode 100644 ckpts/IncCTR_train11-23_test24-30_1epoch_0.1_0.001/period25/Epoch1_TestAUC0.8054_TestLOGLOSS0.5466.ckpt.index\n"," create mode 100644 ckpts/IncCTR_train11-23_test24-30_1epoch_0.1_0.001/period25/Epoch1_TestAUC0.8054_TestLOGLOSS0.5466.ckpt.meta\n"," create mode 100644 ckpts/IncCTR_train11-23_test24-30_1epoch_0.1_0.001/period25/checkpoint\n"," create mode 100644 ckpts/IncCTR_train11-23_test24-30_1epoch_0.1_0.001/period25/config.txt\n"," create mode 100644 ckpts/IncCTR_train11-23_test24-30_1epoch_0.1_0.001/period25/test_metrics.txt\n"," create mode 100644 ckpts/IncCTR_train11-23_test24-30_1epoch_0.1_0.001/period26/Epoch1_TestAUC0.7787_TestLOGLOSS0.5831.ckpt.data-00000-of-00001\n"," create mode 100644 ckpts/IncCTR_train11-23_test24-30_1epoch_0.1_0.001/period26/Epoch1_TestAUC0.7787_TestLOGLOSS0.5831.ckpt.index\n"," create mode 100644 ckpts/IncCTR_train11-23_test24-30_1epoch_0.1_0.001/period26/Epoch1_TestAUC0.7787_TestLOGLOSS0.5831.ckpt.meta\n"," create mode 100644 ckpts/IncCTR_train11-23_test24-30_1epoch_0.1_0.001/period26/checkpoint\n"," create mode 100644 ckpts/IncCTR_train11-23_test24-30_1epoch_0.1_0.001/period26/config.txt\n"," create mode 100644 ckpts/IncCTR_train11-23_test24-30_1epoch_0.1_0.001/period26/test_metrics.txt\n"," create mode 100644 ckpts/IncCTR_train11-23_test24-30_1epoch_0.1_0.001/period27/Epoch1_TestAUC0.8103_TestLOGLOSS0.5359.ckpt.data-00000-of-00001\n"," create mode 100644 ckpts/IncCTR_train11-23_test24-30_1epoch_0.1_0.001/period27/Epoch1_TestAUC0.8103_TestLOGLOSS0.5359.ckpt.index\n"," create mode 100644 ckpts/IncCTR_train11-23_test24-30_1epoch_0.1_0.001/period27/Epoch1_TestAUC0.8103_TestLOGLOSS0.5359.ckpt.meta\n"," create mode 100644 ckpts/IncCTR_train11-23_test24-30_1epoch_0.1_0.001/period27/checkpoint\n"," create mode 100644 ckpts/IncCTR_train11-23_test24-30_1epoch_0.1_0.001/period27/config.txt\n"," create mode 100644 ckpts/IncCTR_train11-23_test24-30_1epoch_0.1_0.001/period27/test_metrics.txt\n"," create mode 100644 ckpts/IncCTR_train11-23_test24-30_1epoch_0.1_0.001/period28/Epoch1_TestAUC0.7960_TestLOGLOSS0.5541.ckpt.data-00000-of-00001\n"," create mode 100644 ckpts/IncCTR_train11-23_test24-30_1epoch_0.1_0.001/period28/Epoch1_TestAUC0.7960_TestLOGLOSS0.5541.ckpt.index\n"," create mode 100644 ckpts/IncCTR_train11-23_test24-30_1epoch_0.1_0.001/period28/Epoch1_TestAUC0.7960_TestLOGLOSS0.5541.ckpt.meta\n"," create mode 100644 ckpts/IncCTR_train11-23_test24-30_1epoch_0.1_0.001/period28/checkpoint\n"," create mode 100644 ckpts/IncCTR_train11-23_test24-30_1epoch_0.1_0.001/period28/config.txt\n"," create mode 100644 ckpts/IncCTR_train11-23_test24-30_1epoch_0.1_0.001/period28/test_metrics.txt\n"," create mode 100644 ckpts/IncCTR_train11-23_test24-30_1epoch_0.1_0.001/period29/Epoch1_TestAUC0.8140_TestLOGLOSS0.5314.ckpt.data-00000-of-00001\n"," create mode 100644 ckpts/IncCTR_train11-23_test24-30_1epoch_0.1_0.001/period29/Epoch1_TestAUC0.8140_TestLOGLOSS0.5314.ckpt.index\n"," create mode 100644 ckpts/IncCTR_train11-23_test24-30_1epoch_0.1_0.001/period29/Epoch1_TestAUC0.8140_TestLOGLOSS0.5314.ckpt.meta\n"," create mode 100644 ckpts/IncCTR_train11-23_test24-30_1epoch_0.1_0.001/period29/checkpoint\n"," create mode 100644 ckpts/IncCTR_train11-23_test24-30_1epoch_0.1_0.001/period29/config.txt\n"," create mode 100644 ckpts/IncCTR_train11-23_test24-30_1epoch_0.1_0.001/period29/test_metrics.txt\n"," create mode 100644 ckpts/IncCTR_train11-23_test24-30_1epoch_0.1_0.001/period30/Epoch1_TestAUC0.8053_TestLOGLOSS0.5437.ckpt.data-00000-of-00001\n"," create mode 100644 ckpts/IncCTR_train11-23_test24-30_1epoch_0.1_0.001/period30/Epoch1_TestAUC0.8053_TestLOGLOSS0.5437.ckpt.index\n"," create mode 100644 ckpts/IncCTR_train11-23_test24-30_1epoch_0.1_0.001/period30/Epoch1_TestAUC0.8053_TestLOGLOSS0.5437.ckpt.meta\n"," create mode 100644 ckpts/IncCTR_train11-23_test24-30_1epoch_0.1_0.001/period30/checkpoint\n"," create mode 100644 ckpts/IncCTR_train11-23_test24-30_1epoch_0.1_0.001/period30/config.txt\n"," create mode 100644 ckpts/IncCTR_train11-23_test24-30_1epoch_0.1_0.001/period30/test_metrics.txt\n"," create mode 100644 ckpts/SML_emb&mlp_train11-23_test24-30_1epoch_1epoch_0.001_0.01/period11/Epoch1_TestAUC0.7811_TestLOGLOSS0.5616.ckpt.data-00000-of-00001\n"," create mode 100644 ckpts/SML_emb&mlp_train11-23_test24-30_1epoch_1epoch_0.001_0.01/period11/Epoch1_TestAUC0.7811_TestLOGLOSS0.5616.ckpt.index\n"," create mode 100644 ckpts/SML_emb&mlp_train11-23_test24-30_1epoch_1epoch_0.001_0.01/period11/Epoch1_TestAUC0.7811_TestLOGLOSS0.5616.ckpt.meta\n"," create mode 100644 ckpts/SML_emb&mlp_train11-23_test24-30_1epoch_1epoch_0.001_0.01/period11/checkpoint\n"," create mode 100644 ckpts/SML_emb&mlp_train11-23_test24-30_1epoch_1epoch_0.001_0.01/period11/config.txt\n"," create mode 100644 ckpts/SML_emb&mlp_train11-23_test24-30_1epoch_1epoch_0.001_0.01/period12/Epoch1_TestAUC0.8669_TestLOGLOSS0.4693.ckpt.data-00000-of-00001\n"," create mode 100644 ckpts/SML_emb&mlp_train11-23_test24-30_1epoch_1epoch_0.001_0.01/period12/Epoch1_TestAUC0.8669_TestLOGLOSS0.4693.ckpt.index\n"," create mode 100644 ckpts/SML_emb&mlp_train11-23_test24-30_1epoch_1epoch_0.001_0.01/period12/Epoch1_TestAUC0.8669_TestLOGLOSS0.4693.ckpt.meta\n"," create mode 100644 ckpts/SML_emb&mlp_train11-23_test24-30_1epoch_1epoch_0.001_0.01/period12/checkpoint\n"," create mode 100644 ckpts/SML_emb&mlp_train11-23_test24-30_1epoch_1epoch_0.001_0.01/period12/config.txt\n"," create mode 100644 ckpts/SML_emb&mlp_train11-23_test24-30_1epoch_1epoch_0.001_0.01/period13/Epoch1_TestAUC0.8678_TestLOGLOSS0.4572.ckpt.data-00000-of-00001\n"," create mode 100644 ckpts/SML_emb&mlp_train11-23_test24-30_1epoch_1epoch_0.001_0.01/period13/Epoch1_TestAUC0.8678_TestLOGLOSS0.4572.ckpt.index\n"," create mode 100644 ckpts/SML_emb&mlp_train11-23_test24-30_1epoch_1epoch_0.001_0.01/period13/Epoch1_TestAUC0.8678_TestLOGLOSS0.4572.ckpt.meta\n"," create mode 100644 ckpts/SML_emb&mlp_train11-23_test24-30_1epoch_1epoch_0.001_0.01/period13/checkpoint\n"," create mode 100644 ckpts/SML_emb&mlp_train11-23_test24-30_1epoch_1epoch_0.001_0.01/period13/config.txt\n"," create mode 100644 ckpts/SML_emb&mlp_train11-23_test24-30_1epoch_1epoch_0.001_0.01/period14/Epoch1_TestAUC0.8893_TestLOGLOSS0.4217.ckpt.data-00000-of-00001\n"," create mode 100644 ckpts/SML_emb&mlp_train11-23_test24-30_1epoch_1epoch_0.001_0.01/period14/Epoch1_TestAUC0.8893_TestLOGLOSS0.4217.ckpt.index\n"," create mode 100644 ckpts/SML_emb&mlp_train11-23_test24-30_1epoch_1epoch_0.001_0.01/period14/Epoch1_TestAUC0.8893_TestLOGLOSS0.4217.ckpt.meta\n"," create mode 100644 ckpts/SML_emb&mlp_train11-23_test24-30_1epoch_1epoch_0.001_0.01/period14/checkpoint\n"," create mode 100644 ckpts/SML_emb&mlp_train11-23_test24-30_1epoch_1epoch_0.001_0.01/period14/config.txt\n"," create mode 100644 ckpts/SML_emb&mlp_train11-23_test24-30_1epoch_1epoch_0.001_0.01/period15/Epoch1_TestAUC0.8104_TestLOGLOSS0.5267.ckpt.data-00000-of-00001\n"," create mode 100644 ckpts/SML_emb&mlp_train11-23_test24-30_1epoch_1epoch_0.001_0.01/period15/Epoch1_TestAUC0.8104_TestLOGLOSS0.5267.ckpt.index\n"," create mode 100644 ckpts/SML_emb&mlp_train11-23_test24-30_1epoch_1epoch_0.001_0.01/period15/Epoch1_TestAUC0.8104_TestLOGLOSS0.5267.ckpt.meta\n"," create mode 100644 ckpts/SML_emb&mlp_train11-23_test24-30_1epoch_1epoch_0.001_0.01/period15/checkpoint\n"," create mode 100644 ckpts/SML_emb&mlp_train11-23_test24-30_1epoch_1epoch_0.001_0.01/period15/config.txt\n"," create mode 100644 ckpts/SML_emb&mlp_train11-23_test24-30_1epoch_1epoch_0.001_0.01/period16/Epoch1_TestAUC0.8275_TestLOGLOSS0.5163.ckpt.data-00000-of-00001\n"," create mode 100644 ckpts/SML_emb&mlp_train11-23_test24-30_1epoch_1epoch_0.001_0.01/period16/Epoch1_TestAUC0.8275_TestLOGLOSS0.5163.ckpt.index\n"," create mode 100644 ckpts/SML_emb&mlp_train11-23_test24-30_1epoch_1epoch_0.001_0.01/period16/Epoch1_TestAUC0.8275_TestLOGLOSS0.5163.ckpt.meta\n"," create mode 100644 ckpts/SML_emb&mlp_train11-23_test24-30_1epoch_1epoch_0.001_0.01/period16/checkpoint\n"," create mode 100644 ckpts/SML_emb&mlp_train11-23_test24-30_1epoch_1epoch_0.001_0.01/period16/config.txt\n"," create mode 100644 ckpts/SML_emb&mlp_train11-23_test24-30_1epoch_1epoch_0.001_0.01/period17/Epoch1_TestAUC0.8377_TestLOGLOSS0.4997.ckpt.data-00000-of-00001\n"," create mode 100644 ckpts/SML_emb&mlp_train11-23_test24-30_1epoch_1epoch_0.001_0.01/period17/Epoch1_TestAUC0.8377_TestLOGLOSS0.4997.ckpt.index\n"," create mode 100644 ckpts/SML_emb&mlp_train11-23_test24-30_1epoch_1epoch_0.001_0.01/period17/Epoch1_TestAUC0.8377_TestLOGLOSS0.4997.ckpt.meta\n"," create mode 100644 ckpts/SML_emb&mlp_train11-23_test24-30_1epoch_1epoch_0.001_0.01/period17/checkpoint\n"," create mode 100644 ckpts/SML_emb&mlp_train11-23_test24-30_1epoch_1epoch_0.001_0.01/period17/config.txt\n"," create mode 100644 ckpts/SML_emb&mlp_train11-23_test24-30_1epoch_1epoch_0.001_0.01/period18/Epoch1_TestAUC0.7766_TestLOGLOSS0.5638.ckpt.data-00000-of-00001\n"," create mode 100644 ckpts/SML_emb&mlp_train11-23_test24-30_1epoch_1epoch_0.001_0.01/period18/Epoch1_TestAUC0.7766_TestLOGLOSS0.5638.ckpt.index\n"," create mode 100644 ckpts/SML_emb&mlp_train11-23_test24-30_1epoch_1epoch_0.001_0.01/period18/Epoch1_TestAUC0.7766_TestLOGLOSS0.5638.ckpt.meta\n"," create mode 100644 ckpts/SML_emb&mlp_train11-23_test24-30_1epoch_1epoch_0.001_0.01/period18/checkpoint\n"," create mode 100644 ckpts/SML_emb&mlp_train11-23_test24-30_1epoch_1epoch_0.001_0.01/period18/config.txt\n"," create mode 100644 ckpts/SML_emb&mlp_train11-23_test24-30_1epoch_1epoch_0.001_0.01/period19/Epoch1_TestAUC0.8405_TestLOGLOSS0.4942.ckpt.data-00000-of-00001\n"," create mode 100644 ckpts/SML_emb&mlp_train11-23_test24-30_1epoch_1epoch_0.001_0.01/period19/Epoch1_TestAUC0.8405_TestLOGLOSS0.4942.ckpt.index\n"," create mode 100644 ckpts/SML_emb&mlp_train11-23_test24-30_1epoch_1epoch_0.001_0.01/period19/Epoch1_TestAUC0.8405_TestLOGLOSS0.4942.ckpt.meta\n"," create mode 100644 ckpts/SML_emb&mlp_train11-23_test24-30_1epoch_1epoch_0.001_0.01/period19/checkpoint\n"," create mode 100644 ckpts/SML_emb&mlp_train11-23_test24-30_1epoch_1epoch_0.001_0.01/period19/config.txt\n"," create mode 100644 ckpts/SML_emb&mlp_train11-23_test24-30_1epoch_1epoch_0.001_0.01/period20/Epoch1_TestAUC0.8268_TestLOGLOSS0.5081.ckpt.data-00000-of-00001\n"," create mode 100644 ckpts/SML_emb&mlp_train11-23_test24-30_1epoch_1epoch_0.001_0.01/period20/Epoch1_TestAUC0.8268_TestLOGLOSS0.5081.ckpt.index\n"," create mode 100644 ckpts/SML_emb&mlp_train11-23_test24-30_1epoch_1epoch_0.001_0.01/period20/Epoch1_TestAUC0.8268_TestLOGLOSS0.5081.ckpt.meta\n"," create mode 100644 ckpts/SML_emb&mlp_train11-23_test24-30_1epoch_1epoch_0.001_0.01/period20/checkpoint\n"," create mode 100644 ckpts/SML_emb&mlp_train11-23_test24-30_1epoch_1epoch_0.001_0.01/period20/config.txt\n"," create mode 100644 ckpts/SML_emb&mlp_train11-23_test24-30_1epoch_1epoch_0.001_0.01/period21/Epoch1_TestAUC0.8241_TestLOGLOSS0.5145.ckpt.data-00000-of-00001\n"," create mode 100644 ckpts/SML_emb&mlp_train11-23_test24-30_1epoch_1epoch_0.001_0.01/period21/Epoch1_TestAUC0.8241_TestLOGLOSS0.5145.ckpt.index\n"," create mode 100644 ckpts/SML_emb&mlp_train11-23_test24-30_1epoch_1epoch_0.001_0.01/period21/Epoch1_TestAUC0.8241_TestLOGLOSS0.5145.ckpt.meta\n"," create mode 100644 ckpts/SML_emb&mlp_train11-23_test24-30_1epoch_1epoch_0.001_0.01/period21/checkpoint\n"," create mode 100644 ckpts/SML_emb&mlp_train11-23_test24-30_1epoch_1epoch_0.001_0.01/period21/config.txt\n"," create mode 100644 ckpts/SML_emb&mlp_train11-23_test24-30_1epoch_1epoch_0.001_0.01/period22/Epoch1_TestAUC0.8121_TestLOGLOSS0.5281.ckpt.data-00000-of-00001\n"," create mode 100644 ckpts/SML_emb&mlp_train11-23_test24-30_1epoch_1epoch_0.001_0.01/period22/Epoch1_TestAUC0.8121_TestLOGLOSS0.5281.ckpt.index\n"," create mode 100644 ckpts/SML_emb&mlp_train11-23_test24-30_1epoch_1epoch_0.001_0.01/period22/Epoch1_TestAUC0.8121_TestLOGLOSS0.5281.ckpt.meta\n"," create mode 100644 ckpts/SML_emb&mlp_train11-23_test24-30_1epoch_1epoch_0.001_0.01/period22/checkpoint\n"," create mode 100644 ckpts/SML_emb&mlp_train11-23_test24-30_1epoch_1epoch_0.001_0.01/period22/config.txt\n"," create mode 100644 ckpts/SML_emb&mlp_train11-23_test24-30_1epoch_1epoch_0.001_0.01/period23/Epoch1_TestAUC0.8334_TestLOGLOSS0.5058.ckpt.data-00000-of-00001\n"," create mode 100644 ckpts/SML_emb&mlp_train11-23_test24-30_1epoch_1epoch_0.001_0.01/period23/Epoch1_TestAUC0.8334_TestLOGLOSS0.5058.ckpt.index\n"," create mode 100644 ckpts/SML_emb&mlp_train11-23_test24-30_1epoch_1epoch_0.001_0.01/period23/Epoch1_TestAUC0.8334_TestLOGLOSS0.5058.ckpt.meta\n"," create mode 100644 ckpts/SML_emb&mlp_train11-23_test24-30_1epoch_1epoch_0.001_0.01/period23/checkpoint\n"," create mode 100644 ckpts/SML_emb&mlp_train11-23_test24-30_1epoch_1epoch_0.001_0.01/period23/config.txt\n"," create mode 100644 ckpts/SML_emb&mlp_train11-23_test24-30_1epoch_1epoch_0.001_0.01/period24/Epoch1_TestAUC0.8024_TestLOGLOSS0.5378.ckpt.data-00000-of-00001\n"," create mode 100644 ckpts/SML_emb&mlp_train11-23_test24-30_1epoch_1epoch_0.001_0.01/period24/Epoch1_TestAUC0.8024_TestLOGLOSS0.5378.ckpt.index\n"," create mode 100644 ckpts/SML_emb&mlp_train11-23_test24-30_1epoch_1epoch_0.001_0.01/period24/Epoch1_TestAUC0.8024_TestLOGLOSS0.5378.ckpt.meta\n"," create mode 100644 ckpts/SML_emb&mlp_train11-23_test24-30_1epoch_1epoch_0.001_0.01/period24/checkpoint\n"," create mode 100644 ckpts/SML_emb&mlp_train11-23_test24-30_1epoch_1epoch_0.001_0.01/period24/config.txt\n"," create mode 100644 ckpts/SML_emb&mlp_train11-23_test24-30_1epoch_1epoch_0.001_0.01/period24/test_metrics.txt\n"," create mode 100644 ckpts/SML_emb&mlp_train11-23_test24-30_1epoch_1epoch_0.001_0.01/period25/Epoch1_TestAUC0.8160_TestLOGLOSS0.5267.ckpt.data-00000-of-00001\n"," create mode 100644 ckpts/SML_emb&mlp_train11-23_test24-30_1epoch_1epoch_0.001_0.01/period25/Epoch1_TestAUC0.8160_TestLOGLOSS0.5267.ckpt.index\n"," create mode 100644 ckpts/SML_emb&mlp_train11-23_test24-30_1epoch_1epoch_0.001_0.01/period25/Epoch1_TestAUC0.8160_TestLOGLOSS0.5267.ckpt.meta\n"," create mode 100644 ckpts/SML_emb&mlp_train11-23_test24-30_1epoch_1epoch_0.001_0.01/period25/checkpoint\n"," create mode 100644 ckpts/SML_emb&mlp_train11-23_test24-30_1epoch_1epoch_0.001_0.01/period25/config.txt\n"," create mode 100644 ckpts/SML_emb&mlp_train11-23_test24-30_1epoch_1epoch_0.001_0.01/period25/test_metrics.txt\n"," create mode 100644 ckpts/SML_emb&mlp_train11-23_test24-30_1epoch_1epoch_0.001_0.01/period26/Epoch1_TestAUC0.7864_TestLOGLOSS0.5551.ckpt.data-00000-of-00001\n"," create mode 100644 ckpts/SML_emb&mlp_train11-23_test24-30_1epoch_1epoch_0.001_0.01/period26/Epoch1_TestAUC0.7864_TestLOGLOSS0.5551.ckpt.index\n"," create mode 100644 ckpts/SML_emb&mlp_train11-23_test24-30_1epoch_1epoch_0.001_0.01/period26/Epoch1_TestAUC0.7864_TestLOGLOSS0.5551.ckpt.meta\n"," create mode 100644 ckpts/SML_emb&mlp_train11-23_test24-30_1epoch_1epoch_0.001_0.01/period26/checkpoint\n"," create mode 100644 ckpts/SML_emb&mlp_train11-23_test24-30_1epoch_1epoch_0.001_0.01/period26/config.txt\n"," create mode 100644 ckpts/SML_emb&mlp_train11-23_test24-30_1epoch_1epoch_0.001_0.01/period26/test_metrics.txt\n"," create mode 100644 ckpts/SML_emb&mlp_train11-23_test24-30_1epoch_1epoch_0.001_0.01/period27/Epoch1_TestAUC0.8108_TestLOGLOSS0.5310.ckpt.data-00000-of-00001\n"," create mode 100644 ckpts/SML_emb&mlp_train11-23_test24-30_1epoch_1epoch_0.001_0.01/period27/Epoch1_TestAUC0.8108_TestLOGLOSS0.5310.ckpt.index\n"," create mode 100644 ckpts/SML_emb&mlp_train11-23_test24-30_1epoch_1epoch_0.001_0.01/period27/Epoch1_TestAUC0.8108_TestLOGLOSS0.5310.ckpt.meta\n"," create mode 100644 ckpts/SML_emb&mlp_train11-23_test24-30_1epoch_1epoch_0.001_0.01/period27/checkpoint\n"," create mode 100644 ckpts/SML_emb&mlp_train11-23_test24-30_1epoch_1epoch_0.001_0.01/period27/config.txt\n"," create mode 100644 ckpts/SML_emb&mlp_train11-23_test24-30_1epoch_1epoch_0.001_0.01/period27/test_metrics.txt\n"," create mode 100644 ckpts/SML_emb&mlp_train11-23_test24-30_1epoch_1epoch_0.001_0.01/period28/Epoch1_TestAUC0.7972_TestLOGLOSS0.5487.ckpt.data-00000-of-00001\n"," create mode 100644 ckpts/SML_emb&mlp_train11-23_test24-30_1epoch_1epoch_0.001_0.01/period28/Epoch1_TestAUC0.7972_TestLOGLOSS0.5487.ckpt.index\n"," create mode 100644 ckpts/SML_emb&mlp_train11-23_test24-30_1epoch_1epoch_0.001_0.01/period28/Epoch1_TestAUC0.7972_TestLOGLOSS0.5487.ckpt.meta\n"," create mode 100644 ckpts/SML_emb&mlp_train11-23_test24-30_1epoch_1epoch_0.001_0.01/period28/checkpoint\n"," create mode 100644 ckpts/SML_emb&mlp_train11-23_test24-30_1epoch_1epoch_0.001_0.01/period28/config.txt\n"," create mode 100644 ckpts/SML_emb&mlp_train11-23_test24-30_1epoch_1epoch_0.001_0.01/period28/test_metrics.txt\n"," create mode 100644 ckpts/SML_emb&mlp_train11-23_test24-30_1epoch_1epoch_0.001_0.01/period29/Epoch1_TestAUC0.8163_TestLOGLOSS0.5239.ckpt.data-00000-of-00001\n"," create mode 100644 ckpts/SML_emb&mlp_train11-23_test24-30_1epoch_1epoch_0.001_0.01/period29/Epoch1_TestAUC0.8163_TestLOGLOSS0.5239.ckpt.index\n"," create mode 100644 ckpts/SML_emb&mlp_train11-23_test24-30_1epoch_1epoch_0.001_0.01/period29/Epoch1_TestAUC0.8163_TestLOGLOSS0.5239.ckpt.meta\n"," create mode 100644 ckpts/SML_emb&mlp_train11-23_test24-30_1epoch_1epoch_0.001_0.01/period29/checkpoint\n"," create mode 100644 ckpts/SML_emb&mlp_train11-23_test24-30_1epoch_1epoch_0.001_0.01/period29/config.txt\n"," create mode 100644 ckpts/SML_emb&mlp_train11-23_test24-30_1epoch_1epoch_0.001_0.01/period29/test_metrics.txt\n"," create mode 100644 ckpts/SML_emb&mlp_train11-23_test24-30_1epoch_1epoch_0.001_0.01/period30/Epoch1_TestAUC0.8076_TestLOGLOSS0.5358.ckpt.data-00000-of-00001\n"," create mode 100644 ckpts/SML_emb&mlp_train11-23_test24-30_1epoch_1epoch_0.001_0.01/period30/Epoch1_TestAUC0.8076_TestLOGLOSS0.5358.ckpt.index\n"," create mode 100644 ckpts/SML_emb&mlp_train11-23_test24-30_1epoch_1epoch_0.001_0.01/period30/Epoch1_TestAUC0.8076_TestLOGLOSS0.5358.ckpt.meta\n"," create mode 100644 ckpts/SML_emb&mlp_train11-23_test24-30_1epoch_1epoch_0.001_0.01/period30/checkpoint\n"," create mode 100644 ckpts/SML_emb&mlp_train11-23_test24-30_1epoch_1epoch_0.001_0.01/period30/config.txt\n"," create mode 100644 ckpts/SML_emb&mlp_train11-23_test24-30_1epoch_1epoch_0.001_0.01/period30/test_metrics.txt\n"," create mode 100644 ckpts/SPMF_2_train11-23_test24-30_1epoch_1_0.1_0.001/period11/Epoch1_TestAUC0.8009_TestLOGLOSS0.6433.ckpt.data-00000-of-00001\n"," create mode 100644 ckpts/SPMF_2_train11-23_test24-30_1epoch_1_0.1_0.001/period11/Epoch1_TestAUC0.8009_TestLOGLOSS0.6433.ckpt.index\n"," create mode 100644 ckpts/SPMF_2_train11-23_test24-30_1epoch_1_0.1_0.001/period11/Epoch1_TestAUC0.8009_TestLOGLOSS0.6433.ckpt.meta\n"," create mode 100644 ckpts/SPMF_2_train11-23_test24-30_1epoch_1_0.1_0.001/period11/checkpoint\n"," create mode 100644 ckpts/SPMF_2_train11-23_test24-30_1epoch_1_0.1_0.001/period11/config.txt\n"," create mode 100644 ckpts/SPMF_2_train11-23_test24-30_1epoch_1_0.1_0.001/period12/Epoch1_TestAUC0.8507_TestLOGLOSS0.4848.ckpt.data-00000-of-00001\n"," create mode 100644 ckpts/SPMF_2_train11-23_test24-30_1epoch_1_0.1_0.001/period12/Epoch1_TestAUC0.8507_TestLOGLOSS0.4848.ckpt.index\n"," create mode 100644 ckpts/SPMF_2_train11-23_test24-30_1epoch_1_0.1_0.001/period12/Epoch1_TestAUC0.8507_TestLOGLOSS0.4848.ckpt.meta\n"," create mode 100644 ckpts/SPMF_2_train11-23_test24-30_1epoch_1_0.1_0.001/period12/checkpoint\n"," create mode 100644 ckpts/SPMF_2_train11-23_test24-30_1epoch_1_0.1_0.001/period12/config.txt\n"," create mode 100644 ckpts/SPMF_2_train11-23_test24-30_1epoch_1_0.1_0.001/period13/Epoch1_TestAUC0.8567_TestLOGLOSS0.4780.ckpt.data-00000-of-00001\n"," create mode 100644 ckpts/SPMF_2_train11-23_test24-30_1epoch_1_0.1_0.001/period13/Epoch1_TestAUC0.8567_TestLOGLOSS0.4780.ckpt.index\n"," create mode 100644 ckpts/SPMF_2_train11-23_test24-30_1epoch_1_0.1_0.001/period13/Epoch1_TestAUC0.8567_TestLOGLOSS0.4780.ckpt.meta\n"," create mode 100644 ckpts/SPMF_2_train11-23_test24-30_1epoch_1_0.1_0.001/period13/checkpoint\n"," create mode 100644 ckpts/SPMF_2_train11-23_test24-30_1epoch_1_0.1_0.001/period13/config.txt\n"," create mode 100644 ckpts/SPMF_2_train11-23_test24-30_1epoch_1_0.1_0.001/period14/Epoch1_TestAUC0.8917_TestLOGLOSS0.4145.ckpt.data-00000-of-00001\n"," create mode 100644 ckpts/SPMF_2_train11-23_test24-30_1epoch_1_0.1_0.001/period14/Epoch1_TestAUC0.8917_TestLOGLOSS0.4145.ckpt.index\n"," create mode 100644 ckpts/SPMF_2_train11-23_test24-30_1epoch_1_0.1_0.001/period14/Epoch1_TestAUC0.8917_TestLOGLOSS0.4145.ckpt.meta\n"," create mode 100644 ckpts/SPMF_2_train11-23_test24-30_1epoch_1_0.1_0.001/period14/checkpoint\n"," create mode 100644 ckpts/SPMF_2_train11-23_test24-30_1epoch_1_0.1_0.001/period14/config.txt\n"," create mode 100644 ckpts/SPMF_2_train11-23_test24-30_1epoch_1_0.1_0.001/period15/Epoch1_TestAUC0.7899_TestLOGLOSS0.6595.ckpt.data-00000-of-00001\n"," create mode 100644 ckpts/SPMF_2_train11-23_test24-30_1epoch_1_0.1_0.001/period15/Epoch1_TestAUC0.7899_TestLOGLOSS0.6595.ckpt.index\n"," create mode 100644 ckpts/SPMF_2_train11-23_test24-30_1epoch_1_0.1_0.001/period15/Epoch1_TestAUC0.7899_TestLOGLOSS0.6595.ckpt.meta\n"," create mode 100644 ckpts/SPMF_2_train11-23_test24-30_1epoch_1_0.1_0.001/period15/checkpoint\n"," create mode 100644 ckpts/SPMF_2_train11-23_test24-30_1epoch_1_0.1_0.001/period15/config.txt\n"," create mode 100644 ckpts/SPMF_2_train11-23_test24-30_1epoch_1_0.1_0.001/period16/Epoch1_TestAUC0.8277_TestLOGLOSS0.5130.ckpt.data-00000-of-00001\n"," create mode 100644 ckpts/SPMF_2_train11-23_test24-30_1epoch_1_0.1_0.001/period16/Epoch1_TestAUC0.8277_TestLOGLOSS0.5130.ckpt.index\n"," create mode 100644 ckpts/SPMF_2_train11-23_test24-30_1epoch_1_0.1_0.001/period16/Epoch1_TestAUC0.8277_TestLOGLOSS0.5130.ckpt.meta\n"," create mode 100644 ckpts/SPMF_2_train11-23_test24-30_1epoch_1_0.1_0.001/period16/checkpoint\n"," create mode 100644 ckpts/SPMF_2_train11-23_test24-30_1epoch_1_0.1_0.001/period16/config.txt\n"," create mode 100644 ckpts/SPMF_2_train11-23_test24-30_1epoch_1_0.1_0.001/period17/Epoch1_TestAUC0.8363_TestLOGLOSS0.5041.ckpt.data-00000-of-00001\n"," create mode 100644 ckpts/SPMF_2_train11-23_test24-30_1epoch_1_0.1_0.001/period17/Epoch1_TestAUC0.8363_TestLOGLOSS0.5041.ckpt.index\n"," create mode 100644 ckpts/SPMF_2_train11-23_test24-30_1epoch_1_0.1_0.001/period17/Epoch1_TestAUC0.8363_TestLOGLOSS0.5041.ckpt.meta\n"," create mode 100644 ckpts/SPMF_2_train11-23_test24-30_1epoch_1_0.1_0.001/period17/checkpoint\n"," create mode 100644 ckpts/SPMF_2_train11-23_test24-30_1epoch_1_0.1_0.001/period17/config.txt\n"," create mode 100644 ckpts/SPMF_2_train11-23_test24-30_1epoch_1_0.1_0.001/period18/Epoch1_TestAUC0.7742_TestLOGLOSS0.5960.ckpt.data-00000-of-00001\n"," create mode 100644 ckpts/SPMF_2_train11-23_test24-30_1epoch_1_0.1_0.001/period18/Epoch1_TestAUC0.7742_TestLOGLOSS0.5960.ckpt.index\n"," create mode 100644 ckpts/SPMF_2_train11-23_test24-30_1epoch_1_0.1_0.001/period18/Epoch1_TestAUC0.7742_TestLOGLOSS0.5960.ckpt.meta\n"," create mode 100644 ckpts/SPMF_2_train11-23_test24-30_1epoch_1_0.1_0.001/period18/checkpoint\n"," create mode 100644 ckpts/SPMF_2_train11-23_test24-30_1epoch_1_0.1_0.001/period18/config.txt\n"," create mode 100644 ckpts/SPMF_2_train11-23_test24-30_1epoch_1_0.1_0.001/period19/Epoch1_TestAUC0.8276_TestLOGLOSS0.5105.ckpt.data-00000-of-00001\n"," create mode 100644 ckpts/SPMF_2_train11-23_test24-30_1epoch_1_0.1_0.001/period19/Epoch1_TestAUC0.8276_TestLOGLOSS0.5105.ckpt.index\n"," create mode 100644 ckpts/SPMF_2_train11-23_test24-30_1epoch_1_0.1_0.001/period19/Epoch1_TestAUC0.8276_TestLOGLOSS0.5105.ckpt.meta\n"," create mode 100644 ckpts/SPMF_2_train11-23_test24-30_1epoch_1_0.1_0.001/period19/checkpoint\n"," create mode 100644 ckpts/SPMF_2_train11-23_test24-30_1epoch_1_0.1_0.001/period19/config.txt\n"," create mode 100644 ckpts/SPMF_2_train11-23_test24-30_1epoch_1_0.1_0.001/period20/Epoch1_TestAUC0.8184_TestLOGLOSS0.5310.ckpt.data-00000-of-00001\n"," create mode 100644 ckpts/SPMF_2_train11-23_test24-30_1epoch_1_0.1_0.001/period20/Epoch1_TestAUC0.8184_TestLOGLOSS0.5310.ckpt.index\n"," create mode 100644 ckpts/SPMF_2_train11-23_test24-30_1epoch_1_0.1_0.001/period20/Epoch1_TestAUC0.8184_TestLOGLOSS0.5310.ckpt.meta\n"," create mode 100644 ckpts/SPMF_2_train11-23_test24-30_1epoch_1_0.1_0.001/period20/checkpoint\n"," create mode 100644 ckpts/SPMF_2_train11-23_test24-30_1epoch_1_0.1_0.001/period20/config.txt\n"," create mode 100644 ckpts/SPMF_2_train11-23_test24-30_1epoch_1_0.1_0.001/period21/Epoch1_TestAUC0.8192_TestLOGLOSS0.5235.ckpt.data-00000-of-00001\n"," create mode 100644 ckpts/SPMF_2_train11-23_test24-30_1epoch_1_0.1_0.001/period21/Epoch1_TestAUC0.8192_TestLOGLOSS0.5235.ckpt.index\n"," create mode 100644 ckpts/SPMF_2_train11-23_test24-30_1epoch_1_0.1_0.001/period21/Epoch1_TestAUC0.8192_TestLOGLOSS0.5235.ckpt.meta\n"," create mode 100644 ckpts/SPMF_2_train11-23_test24-30_1epoch_1_0.1_0.001/period21/checkpoint\n"," create mode 100644 ckpts/SPMF_2_train11-23_test24-30_1epoch_1_0.1_0.001/period21/config.txt\n"," create mode 100644 ckpts/SPMF_2_train11-23_test24-30_1epoch_1_0.1_0.001/period22/Epoch1_TestAUC0.8082_TestLOGLOSS0.5376.ckpt.data-00000-of-00001\n"," create mode 100644 ckpts/SPMF_2_train11-23_test24-30_1epoch_1_0.1_0.001/period22/Epoch1_TestAUC0.8082_TestLOGLOSS0.5376.ckpt.index\n"," create mode 100644 ckpts/SPMF_2_train11-23_test24-30_1epoch_1_0.1_0.001/period22/Epoch1_TestAUC0.8082_TestLOGLOSS0.5376.ckpt.meta\n"," create mode 100644 ckpts/SPMF_2_train11-23_test24-30_1epoch_1_0.1_0.001/period22/checkpoint\n"," create mode 100644 ckpts/SPMF_2_train11-23_test24-30_1epoch_1_0.1_0.001/period22/config.txt\n"," create mode 100644 ckpts/SPMF_2_train11-23_test24-30_1epoch_1_0.1_0.001/period23/Epoch1_TestAUC0.8323_TestLOGLOSS0.5064.ckpt.data-00000-of-00001\n"," create mode 100644 ckpts/SPMF_2_train11-23_test24-30_1epoch_1_0.1_0.001/period23/Epoch1_TestAUC0.8323_TestLOGLOSS0.5064.ckpt.index\n"," create mode 100644 ckpts/SPMF_2_train11-23_test24-30_1epoch_1_0.1_0.001/period23/Epoch1_TestAUC0.8323_TestLOGLOSS0.5064.ckpt.meta\n"," create mode 100644 ckpts/SPMF_2_train11-23_test24-30_1epoch_1_0.1_0.001/period23/checkpoint\n"," create mode 100644 ckpts/SPMF_2_train11-23_test24-30_1epoch_1_0.1_0.001/period23/config.txt\n"," create mode 100644 ckpts/SPMF_2_train11-23_test24-30_1epoch_1_0.1_0.001/period24/Epoch1_TestAUC0.7932_TestLOGLOSS0.5578.ckpt.data-00000-of-00001\n"," create mode 100644 ckpts/SPMF_2_train11-23_test24-30_1epoch_1_0.1_0.001/period24/Epoch1_TestAUC0.7932_TestLOGLOSS0.5578.ckpt.index\n"," create mode 100644 ckpts/SPMF_2_train11-23_test24-30_1epoch_1_0.1_0.001/period24/Epoch1_TestAUC0.7932_TestLOGLOSS0.5578.ckpt.meta\n"," create mode 100644 ckpts/SPMF_2_train11-23_test24-30_1epoch_1_0.1_0.001/period24/checkpoint\n"," create mode 100644 ckpts/SPMF_2_train11-23_test24-30_1epoch_1_0.1_0.001/period24/config.txt\n"," create mode 100644 ckpts/SPMF_2_train11-23_test24-30_1epoch_1_0.1_0.001/period24/test_metrics.txt\n"," create mode 100644 ckpts/SPMF_2_train11-23_test24-30_1epoch_1_0.1_0.001/period25/Epoch1_TestAUC0.8050_TestLOGLOSS0.5374.ckpt.data-00000-of-00001\n"," create mode 100644 ckpts/SPMF_2_train11-23_test24-30_1epoch_1_0.1_0.001/period25/Epoch1_TestAUC0.8050_TestLOGLOSS0.5374.ckpt.index\n"," create mode 100644 ckpts/SPMF_2_train11-23_test24-30_1epoch_1_0.1_0.001/period25/Epoch1_TestAUC0.8050_TestLOGLOSS0.5374.ckpt.meta\n"," create mode 100644 ckpts/SPMF_2_train11-23_test24-30_1epoch_1_0.1_0.001/period25/checkpoint\n"," create mode 100644 ckpts/SPMF_2_train11-23_test24-30_1epoch_1_0.1_0.001/period25/config.txt\n"," create mode 100644 ckpts/SPMF_2_train11-23_test24-30_1epoch_1_0.1_0.001/period25/test_metrics.txt\n"," create mode 100644 ckpts/SPMF_2_train11-23_test24-30_1epoch_1_0.1_0.001/period26/Epoch1_TestAUC0.7757_TestLOGLOSS0.5752.ckpt.data-00000-of-00001\n"," create mode 100644 ckpts/SPMF_2_train11-23_test24-30_1epoch_1_0.1_0.001/period26/Epoch1_TestAUC0.7757_TestLOGLOSS0.5752.ckpt.index\n"," create mode 100644 ckpts/SPMF_2_train11-23_test24-30_1epoch_1_0.1_0.001/period26/Epoch1_TestAUC0.7757_TestLOGLOSS0.5752.ckpt.meta\n"," create mode 100644 ckpts/SPMF_2_train11-23_test24-30_1epoch_1_0.1_0.001/period26/checkpoint\n"," create mode 100644 ckpts/SPMF_2_train11-23_test24-30_1epoch_1_0.1_0.001/period26/config.txt\n"," create mode 100644 ckpts/SPMF_2_train11-23_test24-30_1epoch_1_0.1_0.001/period26/test_metrics.txt\n"," create mode 100644 ckpts/SPMF_2_train11-23_test24-30_1epoch_1_0.1_0.001/period27/Epoch1_TestAUC0.8058_TestLOGLOSS0.5365.ckpt.data-00000-of-00001\n"," create mode 100644 ckpts/SPMF_2_train11-23_test24-30_1epoch_1_0.1_0.001/period27/Epoch1_TestAUC0.8058_TestLOGLOSS0.5365.ckpt.index\n"," create mode 100644 ckpts/SPMF_2_train11-23_test24-30_1epoch_1_0.1_0.001/period27/Epoch1_TestAUC0.8058_TestLOGLOSS0.5365.ckpt.meta\n"," create mode 100644 ckpts/SPMF_2_train11-23_test24-30_1epoch_1_0.1_0.001/period27/checkpoint\n"," create mode 100644 ckpts/SPMF_2_train11-23_test24-30_1epoch_1_0.1_0.001/period27/config.txt\n"," create mode 100644 ckpts/SPMF_2_train11-23_test24-30_1epoch_1_0.1_0.001/period27/test_metrics.txt\n"," create mode 100644 ckpts/SPMF_2_train11-23_test24-30_1epoch_1_0.1_0.001/period28/Epoch1_TestAUC0.7934_TestLOGLOSS0.5514.ckpt.data-00000-of-00001\n"," create mode 100644 ckpts/SPMF_2_train11-23_test24-30_1epoch_1_0.1_0.001/period28/Epoch1_TestAUC0.7934_TestLOGLOSS0.5514.ckpt.index\n"," create mode 100644 ckpts/SPMF_2_train11-23_test24-30_1epoch_1_0.1_0.001/period28/Epoch1_TestAUC0.7934_TestLOGLOSS0.5514.ckpt.meta\n"," create mode 100644 ckpts/SPMF_2_train11-23_test24-30_1epoch_1_0.1_0.001/period28/checkpoint\n"," create mode 100644 ckpts/SPMF_2_train11-23_test24-30_1epoch_1_0.1_0.001/period28/config.txt\n"," create mode 100644 ckpts/SPMF_2_train11-23_test24-30_1epoch_1_0.1_0.001/period28/test_metrics.txt\n"," create mode 100644 ckpts/SPMF_2_train11-23_test24-30_1epoch_1_0.1_0.001/period29/Epoch1_TestAUC0.8110_TestLOGLOSS0.5309.ckpt.data-00000-of-00001\n"," create mode 100644 ckpts/SPMF_2_train11-23_test24-30_1epoch_1_0.1_0.001/period29/Epoch1_TestAUC0.8110_TestLOGLOSS0.5309.ckpt.index\n"," create mode 100644 ckpts/SPMF_2_train11-23_test24-30_1epoch_1_0.1_0.001/period29/Epoch1_TestAUC0.8110_TestLOGLOSS0.5309.ckpt.meta\n"," create mode 100644 ckpts/SPMF_2_train11-23_test24-30_1epoch_1_0.1_0.001/period29/checkpoint\n"," create mode 100644 ckpts/SPMF_2_train11-23_test24-30_1epoch_1_0.1_0.001/period29/config.txt\n"," create mode 100644 ckpts/SPMF_2_train11-23_test24-30_1epoch_1_0.1_0.001/period29/test_metrics.txt\n"," create mode 100644 ckpts/SPMF_2_train11-23_test24-30_1epoch_1_0.1_0.001/period30/Epoch1_TestAUC0.8011_TestLOGLOSS0.5437.ckpt.data-00000-of-00001\n"," create mode 100644 ckpts/SPMF_2_train11-23_test24-30_1epoch_1_0.1_0.001/period30/Epoch1_TestAUC0.8011_TestLOGLOSS0.5437.ckpt.index\n"," create mode 100644 ckpts/SPMF_2_train11-23_test24-30_1epoch_1_0.1_0.001/period30/Epoch1_TestAUC0.8011_TestLOGLOSS0.5437.ckpt.meta\n"," create mode 100644 ckpts/SPMF_2_train11-23_test24-30_1epoch_1_0.1_0.001/period30/checkpoint\n"," create mode 100644 ckpts/SPMF_2_train11-23_test24-30_1epoch_1_0.1_0.001/period30/config.txt\n"," create mode 100644 ckpts/SPMF_2_train11-23_test24-30_1epoch_1_0.1_0.001/period30/test_metrics.txt\n"," create mode 100644 ckpts/pretrain_train1-10_test11_10epoch_0.001/Epoch10_TestAUC0.8298_TestLOGLOSS1.1370.ckpt.data-00000-of-00001\n"," create mode 100644 ckpts/pretrain_train1-10_test11_10epoch_0.001/Epoch10_TestAUC0.8298_TestLOGLOSS1.1370.ckpt.index\n"," create mode 100644 ckpts/pretrain_train1-10_test11_10epoch_0.001/Epoch10_TestAUC0.8298_TestLOGLOSS1.1370.ckpt.meta\n"," create mode 100644 ckpts/pretrain_train1-10_test11_10epoch_0.001/Epoch1_TestAUC0.8528_TestLOGLOSS0.5142.ckpt.data-00000-of-00001\n"," create mode 100644 ckpts/pretrain_train1-10_test11_10epoch_0.001/Epoch1_TestAUC0.8528_TestLOGLOSS0.5142.ckpt.index\n"," create mode 100644 ckpts/pretrain_train1-10_test11_10epoch_0.001/Epoch1_TestAUC0.8528_TestLOGLOSS0.5142.ckpt.meta\n"," create mode 100644 ckpts/pretrain_train1-10_test11_10epoch_0.001/Epoch2_TestAUC0.8535_TestLOGLOSS0.5567.ckpt.data-00000-of-00001\n"," create mode 100644 ckpts/pretrain_train1-10_test11_10epoch_0.001/Epoch2_TestAUC0.8535_TestLOGLOSS0.5567.ckpt.index\n"," create mode 100644 ckpts/pretrain_train1-10_test11_10epoch_0.001/Epoch2_TestAUC0.8535_TestLOGLOSS0.5567.ckpt.meta\n"," create mode 100644 ckpts/pretrain_train1-10_test11_10epoch_0.001/Epoch3_TestAUC0.8508_TestLOGLOSS0.5885.ckpt.data-00000-of-00001\n"," create mode 100644 ckpts/pretrain_train1-10_test11_10epoch_0.001/Epoch3_TestAUC0.8508_TestLOGLOSS0.5885.ckpt.index\n"," create mode 100644 ckpts/pretrain_train1-10_test11_10epoch_0.001/Epoch3_TestAUC0.8508_TestLOGLOSS0.5885.ckpt.meta\n"," create mode 100644 ckpts/pretrain_train1-10_test11_10epoch_0.001/Epoch4_TestAUC0.8497_TestLOGLOSS0.6384.ckpt.data-00000-of-00001\n"," create mode 100644 ckpts/pretrain_train1-10_test11_10epoch_0.001/Epoch4_TestAUC0.8497_TestLOGLOSS0.6384.ckpt.index\n"," create mode 100644 ckpts/pretrain_train1-10_test11_10epoch_0.001/Epoch4_TestAUC0.8497_TestLOGLOSS0.6384.ckpt.meta\n"," create mode 100644 ckpts/pretrain_train1-10_test11_10epoch_0.001/Epoch5_TestAUC0.8466_TestLOGLOSS0.7079.ckpt.data-00000-of-00001\n"," create mode 100644 ckpts/pretrain_train1-10_test11_10epoch_0.001/Epoch5_TestAUC0.8466_TestLOGLOSS0.7079.ckpt.index\n"," create mode 100644 ckpts/pretrain_train1-10_test11_10epoch_0.001/Epoch5_TestAUC0.8466_TestLOGLOSS0.7079.ckpt.meta\n"," create mode 100644 ckpts/pretrain_train1-10_test11_10epoch_0.001/Epoch6_TestAUC0.8435_TestLOGLOSS0.7923.ckpt.data-00000-of-00001\n"," create mode 100644 ckpts/pretrain_train1-10_test11_10epoch_0.001/Epoch6_TestAUC0.8435_TestLOGLOSS0.7923.ckpt.index\n"," create mode 100644 ckpts/pretrain_train1-10_test11_10epoch_0.001/Epoch6_TestAUC0.8435_TestLOGLOSS0.7923.ckpt.meta\n"," create mode 100644 ckpts/pretrain_train1-10_test11_10epoch_0.001/Epoch7_TestAUC0.8402_TestLOGLOSS0.8806.ckpt.data-00000-of-00001\n"," create mode 100644 ckpts/pretrain_train1-10_test11_10epoch_0.001/Epoch7_TestAUC0.8402_TestLOGLOSS0.8806.ckpt.index\n"," create mode 100644 ckpts/pretrain_train1-10_test11_10epoch_0.001/Epoch7_TestAUC0.8402_TestLOGLOSS0.8806.ckpt.meta\n"," create mode 100644 ckpts/pretrain_train1-10_test11_10epoch_0.001/Epoch8_TestAUC0.8377_TestLOGLOSS0.9704.ckpt.data-00000-of-00001\n"," create mode 100644 ckpts/pretrain_train1-10_test11_10epoch_0.001/Epoch8_TestAUC0.8377_TestLOGLOSS0.9704.ckpt.index\n"," create mode 100644 ckpts/pretrain_train1-10_test11_10epoch_0.001/Epoch8_TestAUC0.8377_TestLOGLOSS0.9704.ckpt.meta\n"," create mode 100644 ckpts/pretrain_train1-10_test11_10epoch_0.001/Epoch9_TestAUC0.8345_TestLOGLOSS1.0346.ckpt.data-00000-of-00001\n"," create mode 100644 ckpts/pretrain_train1-10_test11_10epoch_0.001/Epoch9_TestAUC0.8345_TestLOGLOSS1.0346.ckpt.index\n"," create mode 100644 ckpts/pretrain_train1-10_test11_10epoch_0.001/Epoch9_TestAUC0.8345_TestLOGLOSS1.0346.ckpt.meta\n"," create mode 100644 ckpts/pretrain_train1-10_test11_10epoch_0.001/checkpoint\n"," create mode 100644 ckpts/pretrain_train1-10_test11_10epoch_0.001/config.txt\n"]}]},{"cell_type":"code","metadata":{"id":"IqqZ6Do-uswE","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1636625001663,"user_tz":-330,"elapsed":68068,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}},"outputId":"46fd5b99-f995-4a04-d4e2-158835d8a214"},"source":["!cd /content/T967215 && git pull --rebase origin \"{branch}\"\n","!cd /content/T967215 && git push origin \"{branch}\""],"execution_count":62,"outputs":[{"output_type":"stream","name":"stdout","text":["fatal: Couldn't find remote ref T967215\n","Counting objects: 548, done.\n","Delta compression using up to 2 threads.\n","Compressing objects: 100% (547/547), done.\n","Writing objects: 100% (548/548), 310.98 MiB | 12.37 MiB/s, done.\n","Total 548 (delta 189), reused 0 (delta 0)\n","remote: Resolving deltas: 100% (189/189), done.\u001b[K\n","remote: \n","remote: Create a pull request for 'T967215' on GitHub by visiting:\u001b[K\n","remote:      https://github.com/sparsh-ai/incremental-learning/pull/new/T967215\u001b[K\n","remote: \n","To https://github.com/sparsh-ai/incremental-learning.git\n"," * [new branch]      T967215 -> T967215\n"]}]},{"cell_type":"code","metadata":{"id":"evKxFrICIpy_"},"source":["# !mv /content/ckpts .\n","# !mv /content/soba_4mth_2014_1neg_30seq_1.parquet.snappy ."],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BXJY8c9d4Xi5"},"source":["### Installations"]},{"cell_type":"markdown","metadata":{"id":"BK-ZCkf00xZt"},"source":["### Dataset"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5eJSFty70xW-","executionInfo":{"status":"ok","timestamp":1636620661898,"user_tz":-330,"elapsed":4398,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}},"outputId":"01c07006-458b-42dc-8d89-596871e2f0a4"},"source":["!wget -q --show-progress https://github.com/RecoHut-Datasets/sobazaar/raw/main/Data/Sobazaar-hashID.csv.gz\n","!wget -q --show-progress https://github.com/sparsh-ai/incremental-learning/raw/T644011/soba_4mth_2014_1neg_30seq_1.parquet.snappy"],"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Sobazaar-hashID.csv 100%[===================>]  17.11M  99.6MB/s    in 0.2s    \n","soba_4mth_2014_1neg 100%[===================>]  35.27M   126MB/s    in 0.3s    \n"]}]},{"cell_type":"markdown","metadata":{"id":"GB_yDppW3_Yt"},"source":["### Imports"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3lFs8AyO1IWc","executionInfo":{"status":"ok","timestamp":1636620662736,"user_tz":-330,"elapsed":864,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}},"outputId":"d8699e86-0805-4952-cbaf-ad8549dfa689"},"source":["%tensorflow_version 1.x"],"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["TensorFlow 1.x selected.\n"]}]},{"cell_type":"code","metadata":{"id":"vrEmNkAAsQlM","executionInfo":{"status":"ok","timestamp":1636620666859,"user_tz":-330,"elapsed":4126,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}}},"source":["import numpy as np\n","from tqdm.notebook import tqdm\n","import sys\n","import os\n","import logging\n","import pandas as pd\n","from os import path as osp\n","from pathlib import Path\n","import random\n","import datetime\n","import time\n","import glob\n","\n","import bz2\n","import pickle\n","import _pickle as cPickle\n","\n","import tensorflow as tf"],"execution_count":5,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NyxCtlrJ3_Ta"},"source":["### Params"]},{"cell_type":"code","metadata":{"id":"MXBwnUCD3_RD","executionInfo":{"status":"ok","timestamp":1636620666861,"user_tz":-330,"elapsed":10,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}}},"source":["class Args:\n","    path_bronze = '/content'\n","    path_silver = '/content'\n","\n","args = Args()"],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"id":"K5cAMUaO2H8W","executionInfo":{"status":"ok","timestamp":1636620667896,"user_tz":-330,"elapsed":1043,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}}},"source":["random.seed(1234)\n","np.random.seed(1234)\n","tf.set_random_seed(123)"],"execution_count":7,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Q40X4lHf4JHw"},"source":["### Logger"]},{"cell_type":"code","metadata":{"id":"cibwpV5L4JFb","executionInfo":{"status":"ok","timestamp":1636620667897,"user_tz":-330,"elapsed":13,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}}},"source":["logging.basicConfig(stream=sys.stdout,\n","                    level = logging.DEBUG,\n","                    format='%(asctime)s [%(levelname)s] : %(message)s',\n","                    datefmt='%d-%b-%y %H:%M:%S')\n","\n","logger = logging.getLogger('Logger')"],"execution_count":8,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2M0-cN2ZzWE-"},"source":["## Modules"]},{"cell_type":"markdown","metadata":{"id":"qY9Y0q2sz1MS"},"source":["### Utils"]},{"cell_type":"code","metadata":{"id":"tH7lmOJbAOIf","executionInfo":{"status":"ok","timestamp":1636620667897,"user_tz":-330,"elapsed":13,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}}},"source":["def save_pickle(data, title):\n"," with bz2.BZ2File(title + '.pbz2', 'w') as f: \n","    cPickle.dump(data, f)\n","\n","def load_pickle(path):\n","    data = bz2.BZ2File(path+'.pbz2', 'rb')\n","    data = cPickle.load(data)\n","    return data"],"execution_count":9,"outputs":[]},{"cell_type":"code","metadata":{"id":"5lHX1pHU7fvN","executionInfo":{"status":"ok","timestamp":1636620667898,"user_tz":-330,"elapsed":12,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}}},"source":["class BatchLoader:\n","    \"\"\"\n","    batch data loader by batch size\n","    return: [[users], [items], np.array(item_seqs_matrix), [seq_lens], [labels]] in batch iterator\n","    \"\"\"\n","\n","    def __init__(self, data_df, batch_size):\n","\n","        self.data_df = data_df.reset_index(drop=True)  # df ['userId', 'itemId', 'label']\n","        self.data_df['index'] = self.data_df.index\n","        self.data_df['batch'] = self.data_df['index'].apply(lambda x: int(x / batch_size) + 1)\n","        self.num_batches = self.data_df['batch'].max()\n","\n","    def get_batch(self, batch_id):\n","\n","        batch = self.data_df[self.data_df['batch'] == batch_id]\n","        users = batch['userId'].tolist()\n","        items = batch['itemId'].tolist()\n","        labels = batch['label'].tolist()\n","        seq_lens = batch['itemSeq'].apply(len).tolist()\n","\n","        item_seqs_matrix = np.zeros([len(batch), 30], np.int32)\n","\n","        i = 0\n","        for itemSeq in batch['itemSeq'].tolist():\n","            for j in range(len(itemSeq)):\n","                item_seqs_matrix[i][j] = itemSeq[j]  # convert list of itemSeq into a matrix with zero padding\n","            i += 1\n","\n","        return [users, items, item_seqs_matrix, seq_lens, labels]\n","\n","\n","def cal_roc_auc(scores, labels):\n","\n","    arr = sorted(zip(scores, labels), key=lambda d: d[0], reverse=True)\n","    pos, neg = 0., 0.\n","    for record in arr:\n","        if record[1] == 1.:\n","            pos += 1\n","        else:\n","            neg += 1\n","\n","    if pos == 0 or neg == 0:\n","        return None\n","\n","    fp, tp = 0., 0.\n","    xy_arr = []\n","    for record in arr:\n","        if record[1] == 1.:\n","            tp += 1\n","        else:\n","            fp += 1\n","        xy_arr.append([fp/neg, tp/pos])\n","\n","    auc = 0.\n","    prev_x = 0.\n","    prev_y = 0.\n","    for x, y in xy_arr:\n","        auc += ((x - prev_x) * (y + prev_y) / 2.)\n","        prev_x = x\n","        prev_y = y\n","    return auc\n","\n","\n","def cal_roc_gauc(users, scores, labels):\n","    # weighted sum of individual auc\n","    df = pd.DataFrame({'user': users,\n","                       'score': scores,\n","                       'label': labels})\n","\n","    df_gb = df.groupby('user').agg(lambda x: x.tolist())\n","\n","    auc_ls = []  # collect auc for all users\n","    user_imp_ls = []\n","\n","    for row in df_gb.itertuples():\n","        auc = cal_roc_auc(row.score, row.label)\n","        if auc is None:\n","            pass\n","        else:\n","            auc_ls.append(auc)\n","            user_imp = len(row.label)\n","            user_imp_ls.append(user_imp)\n","\n","    total_imp = sum(user_imp_ls)\n","    weighted_auc_ls = [auc * user_imp / total_imp for auc, user_imp in zip(auc_ls, user_imp_ls)]\n","\n","    return sum(weighted_auc_ls)"],"execution_count":10,"outputs":[]},{"cell_type":"code","metadata":{"id":"oVSMgDeANiLH","executionInfo":{"status":"ok","timestamp":1636621467385,"user_tz":-330,"elapsed":12,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}}},"source":["class BatchLoaderYsoft:\n","    \"\"\"\n","    batch data loader by batch size with y_soft\n","    return: [[users], [items], np.array(item_seqs_matrix), [seq_lens], [labels], [labels_soft]] in batch iterator\n","    \"\"\"\n","\n","    def __init__(self, data_df, batch_size):\n","\n","        self.data_df = data_df.reset_index(drop=True)  # df ['userId', 'itemId', 'label']\n","        self.data_df['index'] = self.data_df.index\n","        self.data_df['batch'] = self.data_df['index'].apply(lambda x: int(x / batch_size) + 1)\n","        self.num_batches = self.data_df['batch'].max()\n","\n","    def get_batch(self, batch_id):\n","\n","        batch = self.data_df[self.data_df['batch'] == batch_id]\n","        users = batch['userId'].tolist()\n","        items = batch['itemId'].tolist()\n","        labels = batch['label'].tolist()\n","        labels_soft = batch['label_soft'].tolist()\n","        seq_lens = batch['itemSeq'].apply(len).tolist()\n","\n","        item_seqs_matrix = np.zeros([len(batch), 30], np.int32)\n","\n","        i = 0\n","        for itemSeq in batch['itemSeq'].tolist():\n","            for j in range(len(itemSeq)):\n","                item_seqs_matrix[i][j] = itemSeq[j]  # convert list of itemSeq into a matrix with zero padding\n","            i += 1\n","\n","        return [users, items, item_seqs_matrix, seq_lens, labels, labels_soft]"],"execution_count":26,"outputs":[]},{"cell_type":"code","metadata":{"id":"SvNkS4mP7T7m","executionInfo":{"status":"ok","timestamp":1636620667898,"user_tz":-330,"elapsed":12,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}}},"source":["def average_pooling(emb, seq_len):\n","    mask = tf.sequence_mask(seq_len, tf.shape(emb)[-2], dtype=tf.float32)  # [B, T]\n","    mask = tf.expand_dims(mask, -1)  # [B, T, 1]\n","    emb *= mask  # [B, T, H]\n","    sum_pool = tf.reduce_sum(emb, -2)  # [B, H]\n","    avg_pool = tf.div(sum_pool, tf.expand_dims(tf.cast(seq_len, tf.float32), -1) + 1e-8)  # [B, H]\n","    return avg_pool"],"execution_count":11,"outputs":[]},{"cell_type":"code","metadata":{"id":"WQjMHSPwCTgU","executionInfo":{"status":"ok","timestamp":1636620667899,"user_tz":-330,"elapsed":12,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}}},"source":["def search_ckpt(search_alias, mode='last'):\n","    ckpt_ls = glob.glob(search_alias)\n","\n","    if mode == 'best logloss':\n","        metrics_ls = [float(ckpt.split('.ckpt')[0].split('_')[-1].split('TestLOGLOSS')[-1]) for ckpt in ckpt_ls]  # logloss\n","        selected_metrics_pos_ls = [i for i, x in enumerate(metrics_ls) if x == min(metrics_ls)]  # find all positions of the selected ckpts\n","    elif mode == 'best auc':\n","        metrics_ls = [float(ckpt.split('.ckpt')[0].split('_')[-2].split('TestAUC')[-1]) for ckpt in ckpt_ls]  # auc\n","        selected_metrics_pos_ls = [i for i, x in enumerate(metrics_ls) if x == max(metrics_ls)]  # find all positions of the selected ckpts\n","    else:  # mode == 'last'\n","        metrics_ls = [float(ckpt.split('.ckpt')[0].split('_')[-3].split('Epoch')[-1]) for ckpt in ckpt_ls]  # epoch no.\n","        selected_metrics_pos_ls = [i for i, x in enumerate(metrics_ls) if x == max(metrics_ls)]  # find all positions of the selected ckpts\n","    ckpt = ckpt_ls[max(selected_metrics_pos_ls)]  # get the full path of the last selected ckpt\n","\n","    ckpt = ckpt.split('.ckpt')[0]  # get the path name before .ckpt\n","    ckpt = ckpt + '.ckpt'  # get the path with .ckpt\n","    return ckpt"],"execution_count":12,"outputs":[]},{"cell_type":"code","metadata":{"id":"RASF-mMAHU4u","executionInfo":{"status":"ok","timestamp":1636620667900,"user_tz":-330,"elapsed":13,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}}},"source":["def parquet_to_csv(path):\n","    save_path = path.split('.parquet')[0]+'.csv'\n","    pd.read_parquet(path).to_csv(save_path)\n","    logger.info('csv file saved at {}'.format(save_path))"],"execution_count":13,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"PguTj6gN2oj8"},"source":["### Dataset"]},{"cell_type":"code","metadata":{"id":"XQbXoMDO26pN","executionInfo":{"status":"ok","timestamp":1636620667900,"user_tz":-330,"elapsed":12,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}}},"source":["def _gen_neg(num_items, pos_ls, num_neg):\n","    neg_ls = []\n","    for n in range(num_neg):  # generate num_neg\n","        neg = pos_ls[0]\n","        while neg in pos_ls:\n","            neg = random.randint(0, num_items - 1)\n","        neg_ls.append(neg)\n","    return neg_ls"],"execution_count":14,"outputs":[]},{"cell_type":"code","metadata":{"id":"jnzORRSw2p_v","executionInfo":{"status":"ok","timestamp":1636620667901,"user_tz":-330,"elapsed":13,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}}},"source":["def preprocess_sobazaar():\n","    # convert csv into pandas dataframe\n","    data_path = osp.join(args.path_bronze,'Sobazaar-hashID.csv.gz')\n","    save_path = osp.join(args.path_silver,'soba_4mth_2014_1neg_30seq_1.csv')\n","\n","    if not osp.exists(save_path):\n","        df = pd.read_csv(data_path)\n","        \n","        # preprocess\n","        df['date'] = df['Timestamp'].apply(lambda x: int(''.join(c for c in x.split('T')[0] if c.isdigit())))  # extract date and convert to int\n","        df['timestamp'] = df['Timestamp'].apply(lambda x: int(datetime.datetime.strptime(x.split('.')[0], '%Y-%m-%dT%H:%M:%S').timestamp()))\n","        df = df.drop(['Action', 'Timestamp'], axis=1)  # drop useless\n","        df.columns = ['itemId', 'userId', 'date', 'timestamp']  # rename\n","        df = df[['userId', 'itemId', 'date', 'timestamp']]  # switch columns\n","\n","        # remap id\n","        user_id = sorted(df['userId'].unique().tolist())  # sort column\n","        user_map = dict(zip(user_id, range(len(user_id))))  # create map, key is original id, value is mapped id starting from 0\n","        df['userId'] = df['userId'].map(lambda x: user_map[x])  # map key to value in df\n","\n","        item_id = sorted(df['itemId'].unique().tolist())  # sort column\n","        item_map = dict(zip(item_id, range(len(item_id))))  # create map, key is original id, value is mapped id starting from 0\n","        df['itemId'] = df['itemId'].map(lambda x: item_map[x])  # map key to value in df\n","\n","        logger.info('dataframe head - {}'.format(df.head(20)))\n","        logger.info('num_users: {}'.format(len(user_map)))  # 17126\n","        logger.info('num_items: {}'.format(len(item_map)))  # 24785\n","        logger.info('num_records: {}'.format(len(df)))  # 842660\n","\n","        # collect user history\n","        df_gb = df.groupby(['userId'])\n","        neg_lss = []\n","        num_neg = 1\n","        item_seqs = []\n","        max_len = 30\n","        count = 0\n","        for row in tqdm(df.itertuples(), total=df.shape[0]):\n","            user_df = df_gb.get_group(row.userId)\n","            user_history_df = user_df[user_df['timestamp'] <= row.timestamp].sort_values(['timestamp'], ascending=False).reset_index(drop=True)\n","            userHist = user_history_df['itemId'].unique().tolist()\n","            neg_lss.append(_gen_neg(len(item_map), userHist, num_neg))\n","\n","            user_history_df = user_history_df[user_history_df['timestamp'] < row.timestamp].sort_values(['timestamp'], ascending=False).reset_index(drop=True)\n","            item_seq_ls = user_history_df['itemId'][:max_len].tolist()\n","            itemSeq = '#'.join(str(i) for i in item_seq_ls)\n","            item_seqs.append(itemSeq)\n","\n","            count += 1\n","            if count % 100000 == 0:\n","                logger.info('done row {}'.format(count))\n","\n","        df['neg_itemId_ls'] = neg_lss\n","        df['itemSeq'] = item_seqs\n","\n","        users, itemseqs, items, labels, dates, timestamps = [], [], [], [], [], []\n","        for row in tqdm(df.itertuples(), total=df.shape[0]):\n","            users.append(row.userId)\n","            itemseqs.append(row.itemSeq)\n","            items.append(row.itemId)\n","            labels.append(1)  # positive samples have label 1\n","            dates.append(row.date)\n","            timestamps.append(row.timestamp)\n","            for j in range(num_neg):\n","                users.append(row.userId)\n","                itemseqs.append(row.itemSeq)\n","                items.append(row.neg_itemId_ls[j])\n","                labels.append(0)  # negative samples have label 0\n","                dates.append(row.date)\n","                timestamps.append(row.timestamp)\n","\n","        df = pd.DataFrame({'userId': users,\n","                        'itemSeq': itemseqs,\n","                        'itemId': items,\n","                        'label': labels,\n","                        'date': dates,\n","                        'timestamp': timestamps})\n","\n","        logger.info('dataframe head - {}'.format(df.head(20)))\n","        logger.info(len(df))  # 1685320\n","\n","        # save csv and pickle\n","        # ['userId', 'itemSeq', 'itemId', 'label', 'date', 'timestamp']\n","        df.to_csv(save_path, index=False)\n","        logger.info('processed data saved at {}'.format(save_path))\n","    else:\n","        logger.info('File already exists at {}'.format(save_path))"],"execution_count":15,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"x5we-S657T-E"},"source":["### Pretraining"]},{"cell_type":"code","metadata":{"id":"nxC7vY0i-DtE","executionInfo":{"status":"ok","timestamp":1636620667901,"user_tz":-330,"elapsed":12,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}}},"source":["class EmbMLPnocate(object):\n","    \"\"\"\n","        Embedding&MLP base model without item category\n","    \"\"\"\n","    def __init__(self, hyperparams, train_config=None):\n","\n","        self.train_config = train_config\n","\n","        # create placeholder\n","        self.u = tf.placeholder(tf.int32, [None])  # [B]\n","        self.i = tf.placeholder(tf.int32, [None])  # [B]\n","        self.hist_i = tf.placeholder(tf.int32, [None, None])  # [B, T]\n","        self.hist_len = tf.placeholder(tf.int32, [None])  # [B]\n","        self.y = tf.placeholder(tf.float32, [None])  # [B]\n","        self.base_lr = tf.placeholder(tf.float32, [], name='base_lr')  # scalar\n","\n","        # -- create emb begin -------\n","        user_emb_w = tf.get_variable(\"user_emb_w\", [hyperparams['num_users'], hyperparams['user_embed_dim']])\n","        item_emb_w = tf.get_variable(\"item_emb_w\", [hyperparams['num_items'], hyperparams['item_embed_dim']])\n","        # -- create emb end -------\n","\n","        # -- create mlp begin ---\n","        concat_dim = hyperparams['user_embed_dim'] + hyperparams['item_embed_dim'] * 2\n","        with tf.variable_scope('fcn1'):\n","            fcn1_kernel = tf.get_variable(name='kernel', shape=[concat_dim, hyperparams['layers'][1]])\n","            fcn1_bias = tf.get_variable(name='bias', shape=[hyperparams['layers'][1]])\n","        with tf.variable_scope('fcn2'):\n","            fcn2_kernel = tf.get_variable(name='kernel', shape=[hyperparams['layers'][1], hyperparams['layers'][2]])\n","            fcn2_bias = tf.get_variable(name='bias', shape=[hyperparams['layers'][2]])\n","        with tf.variable_scope('fcn3'):\n","            fcn3_kernel = tf.get_variable(name='kernel', shape=[hyperparams['layers'][2], 1])\n","            fcn3_bias = tf.get_variable(name='bias', shape=[1])\n","        # -- create mlp end ---\n","\n","        # -- emb begin -------\n","        u_emb = tf.nn.embedding_lookup(user_emb_w, self.u)  # [B, H]\n","        i_emb = tf.nn.embedding_lookup(item_emb_w, self.i)  # [B, H]\n","        h_emb = tf.nn.embedding_lookup(item_emb_w, self.hist_i)  # [B, T, H]\n","        u_hist = average_pooling(h_emb, self.hist_len)  # [B, H]\n","        # -- emb end -------\n","\n","        # -- mlp begin -------\n","        fcn = tf.concat([u_emb, u_hist, i_emb], axis=-1)  # [B, H x 3]\n","        fcn_layer_1 = tf.nn.relu(tf.matmul(fcn, fcn1_kernel) + fcn1_bias)  # [B, l1]\n","        fcn_layer_2 = tf.nn.relu(tf.matmul(fcn_layer_1, fcn2_kernel) + fcn2_bias)  # [B, l2]\n","        fcn_layer_3 = tf.matmul(fcn_layer_2, fcn3_kernel) + fcn3_bias  # [B, 1]\n","        # -- mlp end -------\n","\n","        logits = tf.reshape(fcn_layer_3, [-1])  # [B]\n","        self.scores = tf.sigmoid(logits)  # [B]\n","\n","        # return same dimension as input tensors, let x = logits, z = labels, z * -log(sigmoid(x)) + (1 - z) * -log(1 - sigmoid(x))\n","        self.losses = tf.nn.sigmoid_cross_entropy_with_logits(logits=logits, labels=self.y)\n","        self.loss = tf.reduce_mean(self.losses)\n","\n","        # base_optimizer\n","        if train_config['base_optimizer'] == 'adam':\n","            base_optimizer = tf.train.AdamOptimizer(learning_rate=self.base_lr)\n","        elif train_config['base_optimizer'] == 'rmsprop':\n","            base_optimizer = tf.train.RMSPropOptimizer(learning_rate=self.base_lr)\n","        else:\n","            base_optimizer = tf.train.GradientDescentOptimizer(learning_rate=self.base_lr)\n","\n","        trainable_params = tf.trainable_variables()\n","\n","        # update base model\n","        update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n","        with tf.control_dependencies(update_ops):\n","            base_grads = tf.gradients(self.loss, trainable_params)  # return a list of gradients (A list of `sum(dy/dx)` for each x in `xs`)\n","            base_grads_tuples = zip(base_grads, trainable_params)\n","            self.train_base_op = base_optimizer.apply_gradients(base_grads_tuples)\n","\n","    def train_base(self, sess, batch):\n","        loss, _ = sess.run([self.loss, self.train_base_op], feed_dict={\n","            self.u: batch[0],\n","            self.i: batch[1],\n","            self.hist_i: batch[2],\n","            self.hist_len: batch[3],\n","            self.y: batch[4],\n","            self.base_lr: self.train_config['base_lr'],\n","        })\n","        return loss\n","\n","    def inference(self, sess, batch):\n","        scores, losses = sess.run([self.scores, self.losses], feed_dict={\n","            self.u: batch[0],\n","            self.i: batch[1],\n","            self.hist_i: batch[2],\n","            self.hist_len: batch[3],\n","            self.y: batch[4],\n","        })\n","        return scores, losses"],"execution_count":16,"outputs":[]},{"cell_type":"code","metadata":{"id":"rzeAyivB-DrL","executionInfo":{"status":"ok","timestamp":1636620667902,"user_tz":-330,"elapsed":13,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}}},"source":["class Engine(object):\n","    \"\"\"\n","    Training epoch and test\n","    \"\"\"\n","\n","    def __init__(self, sess, model):\n","\n","        self.sess = sess\n","        self.model = model\n","\n","    def base_train_an_epoch(self, epoch_id, cur_set, train_config):\n","\n","        train_start_time = time.time()\n","\n","        if train_config['shuffle']:\n","            cur_set = cur_set.sample(frac=1)\n","\n","        cur_batch_loader = BatchLoader(cur_set, train_config['base_bs'])\n","\n","        base_loss_cur_sum = 0\n","\n","        for i in range(1, cur_batch_loader.num_batches + 1):\n","\n","            cur_batch = cur_batch_loader.get_batch(batch_id=i)\n","\n","            base_loss_cur = self.model.train_base(self.sess, cur_batch)  # sess.run\n","\n","            if (i - 1) % 100 == 0:\n","                print('[Epoch {} Batch {}] base_loss_cur {:.4f}, time elapsed {}'.format(epoch_id,\n","                                                                                         i,\n","                                                                                         base_loss_cur,\n","                                                                                         time.strftime('%H:%M:%S',\n","                                                                                                       time.gmtime(\n","                                                                                                           time.time() - train_start_time))))\n","\n","            base_loss_cur_sum += base_loss_cur\n","\n","        # epoch done, compute average loss\n","        base_loss_cur_avg = base_loss_cur_sum / cur_batch_loader.num_batches\n","\n","        return base_loss_cur_avg\n","\n","    def test(self, test_set, train_config):\n","\n","        test_batch_loader = BatchLoader(test_set, train_config['base_bs'])\n","\n","        scores, losses, labels = [], [], []\n","        for i in range(1, test_batch_loader.num_batches + 1):\n","            test_batch = test_batch_loader.get_batch(batch_id=i)\n","            batch_scores, batch_losses = self.model.inference(self.sess, test_batch)  # sees.run\n","            scores.extend(batch_scores.tolist())\n","            losses.extend(batch_losses.tolist())\n","            labels.extend(test_batch[4])\n","\n","        test_auc = cal_roc_auc(scores, labels)\n","        test_logloss = sum(losses) / len(losses)\n","\n","        return test_auc, test_logloss"],"execution_count":17,"outputs":[]},{"cell_type":"code","metadata":{"id":"YECKrjGc-Dow","executionInfo":{"status":"ok","timestamp":1636620667903,"user_tz":-330,"elapsed":14,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}}},"source":["def pretrain_sobazaar():\n","    # load data to df\n","    start_time = time.time()\n","\n","    load_path = osp.join(args.path_silver,'soba_4mth_2014_1neg_30seq_1.csv')\n","    data_df = pd.read_csv(load_path)\n","\n","    data_df['itemSeq'] = data_df['itemSeq'].fillna('')  # empty seq are NaN\n","    data_df['itemSeq'] = data_df['itemSeq'].apply(lambda x: [int(item) for item in x.split('#') if item != ''])\n","\n","    logger.info('Done loading data! time elapsed: {}'.format(time.strftime('%H:%M:%S', time.gmtime(time.time() - start_time))))\n","\n","    num_users = data_df['userId'].max() + 1\n","    num_items = data_df['itemId'].max() + 1\n","\n","    train_config = {'method': 'pretrain',\n","                    'dir_name': 'pretrain_train1-10_test11_10epoch',  # edit train test period range, number of epochs\n","                    'start_date': 20140901,  # overall train start date\n","                    'end_date': 20141231,  # overall train end date\n","                    'num_periods': 31,  # number of periods divided into\n","                    'train_start_period': 1,\n","                    'train_end_period': 10,\n","                    'test_period': 11,\n","                    'train_set_size': None,\n","                    'test_set_size': None,\n","\n","                    'base_optimizer': 'adam',  # base model optimizer: adam, rmsprop, sgd\n","                    'base_lr': None,  # base model learning rate\n","                    'base_bs': 256,  # base model batch size\n","                    'base_num_epochs': 10,  # base model number of epochs\n","                    'shuffle': True,  # whether to shuffle the dataset for each epoch\n","                    }\n","\n","    EmbMLPnocate_hyperparams = {'num_users': num_users,\n","                                'num_items': num_items,\n","                                'user_embed_dim': 8,\n","                                'item_embed_dim': 8,\n","                                'layers': [24, 12, 6, 1]  # input dim is user_embed_dim + item_embed_dim x 2\n","                                }\n","\n","    # sort train data into periods based on num_periods\n","    data_df = data_df[(data_df['date'] >= train_config['start_date']) & (data_df['date'] <= train_config['end_date'])]\n","    data_df = data_df.sort_values(['timestamp']).reset_index(drop=True)\n","    records_per_period = int(len(data_df) / train_config['num_periods'])\n","    data_df['index'] = data_df.index\n","    data_df['period'] = data_df['index'].apply(lambda x: int(x / records_per_period) + 1)\n","    data_df = data_df[data_df.period != train_config['num_periods'] + 1]  # delete last extra period\n","    period_df = data_df.groupby('period')['date'].agg(['count', 'min', 'max'])\n","    data_df = data_df.drop(['index', 'date', 'timestamp'], axis=1)\n","\n","    # build base model computation graph\n","    base_model = EmbMLPnocate(EmbMLPnocate_hyperparams, train_config=train_config)\n","\n","    # create session\n","    sess = tf.Session()\n","\n","    # create saver\n","    saver = tf.train.Saver(max_to_keep=80)\n","\n","    orig_dir_name = train_config['dir_name']\n","\n","    for base_lr in [1e-3]:\n","\n","        print('')\n","        print('base_lr', base_lr)\n","\n","        train_config['base_lr'] = base_lr\n","\n","        train_config['dir_name'] = orig_dir_name + '_' + str(base_lr)\n","        print('dir_name: ', train_config['dir_name'])\n","\n","        # create current and next set\n","        train_set = data_df[(data_df['period'] >= train_config['train_start_period']) &\n","                            (data_df['period'] <= train_config['train_end_period'])]\n","        test_set = data_df[data_df['period'] == train_config['test_period']]\n","        train_config['train_set_size'] = len(train_set)\n","        train_config['test_set_size'] = len(test_set)\n","        print('train set size', len(train_set), 'test set size', len(test_set))\n","\n","        # checkpoints directory\n","        checkpoints_dir = os.path.join('ckpts', train_config['dir_name'])\n","        if not os.path.exists(checkpoints_dir):\n","            os.makedirs(checkpoints_dir)\n","\n","        # write train_config to text file\n","        with open(os.path.join(checkpoints_dir, 'config.txt'), mode='w') as f:\n","            f.write('train_config: ' + str(train_config) + '\\n')\n","            f.write('\\n')\n","            f.write('EmbMLPnocate_hyperparams: ' + str(EmbMLPnocate_hyperparams) + '\\n')\n","\n","        sess.run([tf.global_variables_initializer(), tf.local_variables_initializer()])\n","\n","        # create an engine instance\n","        engine = Engine(sess, base_model)\n","\n","        train_start_time = time.time()\n","\n","        for epoch_id in range(1, train_config['base_num_epochs'] + 1):\n","\n","            print('Training Base Model Epoch {} Start!'.format(epoch_id))\n","\n","            base_loss_cur_avg = engine.base_train_an_epoch(epoch_id, train_set, train_config)\n","            print('Epoch {} Done! time elapsed: {}, base_loss_cur_avg {:.4f}'.format(\n","                epoch_id,\n","                time.strftime('%H:%M:%S', time.gmtime(time.time() - train_start_time)),\n","                base_loss_cur_avg))\n","\n","            test_auc, test_logloss = engine.test(test_set, train_config)\n","            print('test_auc {:.4f}, test_logloss {:.4f}'.format(\n","                test_auc,\n","                test_logloss))\n","            print('time elapsed {}'.format(time.strftime('%H:%M:%S', time.gmtime(time.time() - train_start_time))))\n","\n","            print('')\n","\n","            # save checkpoint\n","            checkpoint_alias = 'Epoch{}_TestAUC{:.4f}_TestLOGLOSS{:.4f}.ckpt'.format(\n","                epoch_id,\n","                test_auc,\n","                test_logloss)\n","            checkpoint_path = os.path.join(checkpoints_dir, checkpoint_alias)\n","            saver.save(sess, checkpoint_path)"],"execution_count":18,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hr7X8V0h-DmW"},"source":["### Incremental Update"]},{"cell_type":"code","metadata":{"id":"l41GttlhB0Ry","executionInfo":{"status":"ok","timestamp":1636620668793,"user_tz":-330,"elapsed":903,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}}},"source":["def iu_sobazaar():\n","    # load data to df\n","    start_time = time.time()\n","\n","    load_path = osp.join(args.path_silver,'soba_4mth_2014_1neg_30seq_1.csv')\n","    data_df = pd.read_csv(load_path)\n","\n","    data_df['itemSeq'] = data_df['itemSeq'].fillna('')  # empty seq are NaN\n","    data_df['itemSeq'] = data_df['itemSeq'].apply(lambda x: [int(item) for item in x.split('#') if item != ''])\n","\n","    logger.info('Done loading data! time elapsed: {}'.format(time.strftime('%H:%M:%S', time.gmtime(time.time() - start_time))))\n","\n","    num_users = data_df['userId'].max() + 1\n","    num_items = data_df['itemId'].max() + 1\n","\n","    train_config = {'method': 'IU_by_period',\n","                    'dir_name': 'IU_train11-23_test24-30_1epoch',  # edit train test period, number of epochs\n","                    'pretrain_model': 'pretrain_train1-10_test11_10epoch_0.001',\n","                    'start_date': 20140901,  # overall train start date\n","                    'end_date': 20141231,  # overall train end date\n","                    'num_periods': 31,  # number of periods divided into\n","                    'train_start_period': 11,\n","                    'test_start_period': 24,\n","                    'cur_period': None,  # current incremental period\n","                    'next_period': None,  # next incremental period\n","                    'cur_set_size': None,  # current incremental dataset size\n","                    'next_set_size': None,  # next incremental dataset size\n","                    'period_alias': None,  # individual period directory alias to save ckpts\n","                    'restored_ckpt_mode': 'best auc',  # mode to search the checkpoint to restore, 'best auc', 'best gauc', 'last'\n","                    'restored_ckpt': None,  # configure in the for loop\n","\n","                    'base_optimizer': 'adam',  # base model optimizer: adam, rmsprop, sgd\n","                    'base_lr': None,  # base model learning rate\n","                    'base_bs': 256,  # base model batch size\n","                    'base_num_epochs': 1,  # base model number of epochs\n","                    'shuffle': True,  # whether to shuffle the dataset for each epoch\n","                    }\n","\n","    EmbMLPnocate_hyperparams = {'num_users': num_users,\n","                                'num_items': num_items,\n","                                'user_embed_dim': 8,\n","                                'item_embed_dim': 8,\n","                                'layers': [24, 12, 6, 1]  # input dim is user_embed_dim + item_embed_dim x 2\n","                                }\n","\n","    # sort train data into periods based on num_periods\n","    data_df = data_df[(data_df['date'] >= train_config['start_date']) & (data_df['date'] <= train_config['end_date'])]\n","    data_df = data_df.sort_values(['timestamp']).reset_index(drop=True)\n","    records_per_period = int(len(data_df) / train_config['num_periods'])\n","    data_df['index'] = data_df.index\n","    data_df['period'] = data_df['index'].apply(lambda x: int(x / records_per_period) + 1)\n","    data_df = data_df[data_df.period != train_config['num_periods'] + 1]  # delete last extra period\n","    period_df = data_df.groupby('period')['date'].agg(['count', 'min', 'max'])\n","    data_df = data_df.drop(['index', 'date', 'timestamp'], axis=1)\n","\n","    orig_dir_name = train_config['dir_name']\n","\n","    for base_lr in [1e-3]:\n","\n","        print('')\n","        print('base_lr', base_lr)\n","\n","        train_config['base_lr'] = base_lr\n","\n","        train_config['dir_name'] = orig_dir_name + '_' + str(base_lr)\n","        print('dir_name: ', train_config['dir_name'])\n","\n","        test_aucs = []\n","        test_loglosses = []\n","\n","        for i in range(train_config['train_start_period'], train_config['num_periods']):\n","\n","            # configure cur_period, next_period\n","            train_config['cur_period'] = i\n","            train_config['next_period'] = i + 1\n","            print('')\n","            print('current period: {}, next period: {}'.format(\n","                train_config['cur_period'],\n","                train_config['next_period']))\n","            print('')\n","\n","            # create current and next set\n","            cur_set = data_df[data_df['period'] == train_config['cur_period']]\n","            next_set = data_df[data_df['period'] == train_config['next_period']]\n","            train_config['cur_set_size'] = len(cur_set)\n","            train_config['next_set_size'] = len(next_set)\n","            print('current set size', len(cur_set), 'next set size', len(next_set))\n","\n","            train_config['period_alias'] = 'period' + str(i)\n","\n","            # checkpoints directory\n","            ckpts_dir = os.path.join('ckpts', train_config['dir_name'], train_config['period_alias'])\n","            if not os.path.exists(ckpts_dir):\n","                os.makedirs(ckpts_dir)\n","\n","            if i == train_config['train_start_period']:\n","                search_alias = os.path.join('ckpts', train_config['pretrain_model'], 'Epoch*')\n","                train_config['restored_ckpt'] = search_ckpt(search_alias, mode=train_config['restored_ckpt_mode'])\n","            else:\n","                prev_period_alias = 'period' + str(i - 1)\n","                search_alias = os.path.join('ckpts', train_config['dir_name'], prev_period_alias, 'Epoch*')\n","                train_config['restored_ckpt'] = search_ckpt(search_alias, mode=train_config['restored_ckpt_mode'])\n","            print('restored checkpoint: {}'.format(train_config['restored_ckpt']))\n","\n","            # write train_config to text file\n","            with open(os.path.join(ckpts_dir, 'config.txt'), mode='w') as f:\n","                f.write('train_config: ' + str(train_config) + '\\n')\n","                f.write('\\n')\n","                f.write('EmbMLPnocate_hyperparams: ' + str(EmbMLPnocate_hyperparams) + '\\n')\n","\n","            # build base model computation graph\n","            tf.reset_default_graph()\n","            base_model = EmbMLPnocate(EmbMLPnocate_hyperparams, train_config=train_config)\n","\n","            # create session\n","            with tf.Session() as sess:\n","                \n","                saver = tf.train.Saver()\n","                saver.restore(sess, train_config['restored_ckpt'])\n","                # create an engine instance with base_model\n","                engine = Engine(sess, base_model)\n","                train_start_time = time.time()\n","                max_auc = 0\n","                best_logloss = 0\n","\n","                for epoch_id in range(1, train_config['base_num_epochs'] + 1):\n","                    print('Training Base Model Epoch {} Start!'.format(epoch_id))\n","                    base_loss_cur_avg = engine.base_train_an_epoch(epoch_id, cur_set, train_config)\n","                    print('Epoch {} Done! time elapsed: {}, base_loss_cur_avg {:.4f}'.format(\n","                        epoch_id,\n","                        time.strftime('%H:%M:%S', time.gmtime(time.time() - train_start_time)),\n","                        base_loss_cur_avg))\n","                    cur_auc, cur_logloss = engine.test(cur_set, train_config)\n","                    next_auc, next_logloss = engine.test(next_set, train_config)\n","                    print('cur_auc {:.4f}, cur_logloss {:.4f}, next_auc {:.4f}, next_logloss {:.4f}'.format(\n","                        cur_auc,\n","                        cur_logloss,\n","                        next_auc,\n","                        next_logloss))\n","                    print('time elapsed {}'.format(time.strftime('%H:%M:%S', time.gmtime(time.time() - train_start_time))))\n","                    print('')\n","                    # save checkpoint\n","                    checkpoint_alias = 'Epoch{}_TestAUC{:.4f}_TestLOGLOSS{:.4f}.ckpt'.format(\n","                        epoch_id,\n","                        next_auc,\n","                        next_logloss)\n","                    checkpoint_path = os.path.join(ckpts_dir, checkpoint_alias)\n","                    saver.save(sess, checkpoint_path)\n","                    if next_auc > max_auc:\n","                        max_auc = next_auc\n","                        best_logloss = next_logloss\n","\n","                if i >= train_config['test_start_period']:\n","                    test_aucs.append(max_auc)\n","                    test_loglosses.append(best_logloss)\n","\n","            if i >= train_config['test_start_period']:\n","                average_auc = sum(test_aucs) / len(test_aucs)\n","                average_logloss = sum(test_loglosses) / len(test_loglosses)\n","                print('test aucs', test_aucs)\n","                print('average auc', average_auc)\n","                print('')\n","                print('test loglosses', test_loglosses)\n","                print('average logloss', average_logloss)\n","\n","                # write metrics to text file\n","                with open(os.path.join(ckpts_dir, 'test_metrics.txt'), mode='w') as f:\n","                    f.write('test_aucs: ' + str(test_aucs) + '\\n')\n","                    f.write('average_auc: ' + str(average_auc) + '\\n')\n","                    f.write('test_loglosses: ' + str(test_loglosses) + '\\n')\n","                    f.write('average_logloss: ' + str(average_logloss) + '\\n')"],"execution_count":19,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"sD75F5a3GbNR"},"source":["### Batch Update"]},{"cell_type":"code","metadata":{"id":"_q1TeLujGbKx","executionInfo":{"status":"ok","timestamp":1636620668794,"user_tz":-330,"elapsed":10,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}}},"source":["def bu_sobazaar():\n","    # load data to df\n","    start_time = time.time()\n","\n","    load_path = osp.join(args.path_silver,'soba_4mth_2014_1neg_30seq_1.csv')\n","    data_df = pd.read_csv(load_path)\n","\n","    data_df['itemSeq'] = data_df['itemSeq'].fillna('')  # empty seq are NaN\n","    data_df['itemSeq'] = data_df['itemSeq'].apply(lambda x: [int(item) for item in x.split('#') if item != ''])\n","\n","    logger.info('Done loading data! time elapsed: {}'.format(time.strftime('%H:%M:%S', time.gmtime(time.time() - start_time))))\n","\n","    num_users = data_df['userId'].max() + 1\n","    num_items = data_df['itemId'].max() + 1\n","\n","    train_config = {'method': 'BU_by_period',\n","                    'dir_name': 'BU_train11-23_test24-30_7_1epoch',  # edit train test period, window size, number of epochs\n","                    'pretrain_model': 'pretrain_train1-10_test11_10epoch_0.001',\n","                    'start_date': 20140901,  # overall train start date\n","                    'end_date': 20141231,  # overall train end date\n","                    'num_periods': 31,  # number of periods divided into\n","                    'train_start_period': 11,\n","                    'test_start_period': 24,\n","                    'window_size': 7,  # number of periods or 'full' for full retraining\n","                    'cur_periods': None,  # current batch periods\n","                    'next_period': None,  # next incremental period\n","                    'cur_set_size': None,  # current batch dataset size\n","                    'next_set_size': None,  # next incremental dataset size\n","                    'period_alias': None,  # individual period directory alias to save ckpts\n","                    'restored_ckpt_mode': 'best auc',  # mode to search the checkpoint to restore, 'best auc', 'best gauc', 'last'\n","                    'restored_ckpt': None,  # configure in the for loop\n","\n","                    'base_optimizer': 'adam',  # base model optimizer: adam, rmsprop, sgd\n","                    'base_lr': None,  # base model learning rate\n","                    'base_bs': 256,  # base model batch size\n","                    'base_num_epochs': 1,  # base model number of epochs\n","                    'shuffle': True,  # whether to shuffle the dataset for each epoch\n","                    }\n","\n","    EmbMLPnocate_hyperparams = {'num_users': num_users,\n","                                'num_items': num_items,\n","                                'user_embed_dim': 8,\n","                                'item_embed_dim': 8,\n","                                'layers': [24, 12, 6, 1]  # input dim is user_embed_dim + item_embed_dim x 2\n","                                }\n","\n","    # sort train data into periods based on num_periods\n","    data_df = data_df[(data_df['date'] >= train_config['start_date']) & (data_df['date'] <= train_config['end_date'])]\n","    data_df = data_df.sort_values(['timestamp']).reset_index(drop=True)\n","    records_per_period = int(len(data_df) / train_config['num_periods'])\n","    data_df['index'] = data_df.index\n","    data_df['period'] = data_df['index'].apply(lambda x: int(x / records_per_period) + 1)\n","    data_df = data_df[data_df.period != train_config['num_periods'] + 1]  # delete last extra period\n","    period_df = data_df.groupby('period')['date'].agg(['count', 'min', 'max'])\n","    data_df = data_df.drop(['index', 'date', 'timestamp'], axis=1)\n","\n","    orig_dir_name = train_config['dir_name']\n","\n","    for base_lr in [1e-3]:\n","\n","        print('')\n","        print('base_lr', base_lr)\n","\n","        train_config['base_lr'] = base_lr\n","\n","        train_config['dir_name'] = orig_dir_name + '_' + str(base_lr)\n","        print('dir_name: ', train_config['dir_name'])\n","\n","        test_aucs = []\n","        test_loglosses = []\n","\n","        for i in range(train_config['train_start_period'], train_config['num_periods']):\n","\n","            # configure cur_periods, next_period\n","            if train_config['window_size'] == 'full':\n","                train_config['cur_periods'] = [i - prev_num for prev_num in reversed(range(i))]\n","            else:\n","                train_config['cur_periods'] = [i - prev_num for prev_num in reversed(range(train_config['window_size']))]\n","            train_config['next_period'] = i + 1\n","            print('')\n","            print('current periods: {}, next period: {}'.format(\n","                train_config['cur_periods'],\n","                train_config['next_period']))\n","            print('')\n","\n","            # create current and next set\n","            cur_set = data_df[data_df['period'].isin(train_config['cur_periods'])]\n","            next_set = data_df[data_df['period'] == train_config['next_period']]\n","            train_config['cur_set_size'] = len(cur_set)\n","            train_config['next_set_size'] = len(next_set)\n","            print('current set size', len(cur_set), 'next set size', len(next_set))\n","\n","            train_config['period_alias'] = 'period' + str(i)\n","\n","            # checkpoints directory\n","            ckpts_dir = os.path.join('ckpts', train_config['dir_name'], train_config['period_alias'])\n","            if not os.path.exists(ckpts_dir):\n","                os.makedirs(ckpts_dir)\n","\n","            if i == train_config['train_start_period']:\n","                search_alias = os.path.join('ckpts', train_config['pretrain_model'], 'Epoch*')\n","                train_config['restored_ckpt'] = search_ckpt(search_alias, mode=train_config['restored_ckpt_mode'])\n","            else:\n","                prev_period_alias = 'period' + str(i - 1)\n","                search_alias = os.path.join('ckpts', train_config['dir_name'], prev_period_alias, 'Epoch*')\n","                train_config['restored_ckpt'] = search_ckpt(search_alias, mode=train_config['restored_ckpt_mode'])\n","            print('restored checkpoint: {}'.format(train_config['restored_ckpt']))\n","\n","            # write train_config to text file\n","            with open(os.path.join(ckpts_dir, 'config.txt'), mode='w') as f:\n","                f.write('train_config: ' + str(train_config) + '\\n')\n","                f.write('\\n')\n","                f.write('EmbMLPnocate_hyperparams: ' + str(EmbMLPnocate_hyperparams) + '\\n')\n","\n","            # build base model computation graph\n","            tf.reset_default_graph()\n","            base_model = EmbMLPnocate(EmbMLPnocate_hyperparams, train_config=train_config)\n","\n","            # create session\n","            with tf.Session() as sess:\n","                saver = tf.train.Saver()\n","                saver.restore(sess, train_config['restored_ckpt'])\n","\n","                # create an engine instance with base_model\n","                engine = Engine(sess, base_model)\n","                train_start_time = time.time()\n","                max_auc = 0\n","                best_logloss = 0\n","\n","                for epoch_id in range(1, train_config['base_num_epochs'] + 1):\n","                    print('Training Base Model Epoch {} Start!'.format(epoch_id))\n","                    base_loss_cur_avg = engine.base_train_an_epoch(epoch_id, cur_set, train_config)\n","                    print('Epoch {} Done! time elapsed: {}, base_loss_cur_avg {:.4f}'.format(\n","                        epoch_id,\n","                        time.strftime('%H:%M:%S', time.gmtime(time.time() - train_start_time)),\n","                        base_loss_cur_avg))\n","                    cur_auc, cur_logloss = engine.test(cur_set, train_config)\n","                    next_auc, next_logloss = engine.test(next_set, train_config)\n","                    print('cur_auc {:.4f}, cur_logloss {:.4f}, next_auc {:.4f}, next_logloss {:.4f}'.format(\n","                        cur_auc,\n","                        cur_logloss,\n","                        next_auc,\n","                        next_logloss))\n","                    print('time elapsed {}'.format(time.strftime('%H:%M:%S', time.gmtime(time.time() - train_start_time))))\n","                    print('')\n","                    # save checkpoint\n","                    checkpoint_alias = 'Epoch{}_TestAUC{:.4f}_TestLOGLOSS{:.4f}.ckpt'.format(\n","                        epoch_id,\n","                        next_auc,\n","                        next_logloss)\n","                    checkpoint_path = os.path.join(ckpts_dir, checkpoint_alias)\n","                    saver.save(sess, checkpoint_path)\n","\n","                    if next_auc > max_auc:\n","                        max_auc = next_auc\n","                        best_logloss = next_logloss\n","\n","                if i >= train_config['test_start_period']:\n","                    test_aucs.append(max_auc)\n","                    test_loglosses.append(best_logloss)\n","\n","            if i >= train_config['test_start_period']:\n","                average_auc = sum(test_aucs) / len(test_aucs)\n","                average_logloss = sum(test_loglosses) / len(test_loglosses)\n","                print('test aucs', test_aucs)\n","                print('average auc', average_auc)\n","                print('')\n","                print('test loglosses', test_loglosses)\n","                print('average logloss', average_logloss)\n","\n","                # write metrics to text file\n","                with open(os.path.join(ckpts_dir, 'test_metrics.txt'), mode='w') as f:\n","                    f.write('test_aucs: ' + str(test_aucs) + '\\n')\n","                    f.write('average_auc: ' + str(average_auc) + '\\n')\n","                    f.write('test_loglosses: ' + str(test_loglosses) + '\\n')\n","                    f.write('average_logloss: ' + str(average_logloss) + '\\n')"],"execution_count":20,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"QisHmVRqK7Bf"},"source":["### SPMF"]},{"cell_type":"code","metadata":{"id":"A8IzVQtPK6-2","executionInfo":{"status":"ok","timestamp":1636622923604,"user_tz":-330,"elapsed":1472,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}}},"source":["def spmf_sobazaar():\n","    # load data to df\n","    start_time = time.time()\n","\n","    load_path = osp.join(args.path_silver,'soba_4mth_2014_1neg_30seq_1.csv')\n","    data_df = pd.read_csv(load_path)\n","\n","    data_df['itemSeq'] = data_df['itemSeq'].fillna('')  # empty seq are NaN\n","    data_df['itemSeq'] = data_df['itemSeq'].apply(lambda x: [int(item) for item in x.split('#') if item != ''])\n","\n","    logger.info('Done loading data! time elapsed: {}'.format(time.strftime('%H:%M:%S', time.gmtime(time.time() - start_time))))\n","\n","    num_users = data_df['userId'].max() + 1\n","    num_items = data_df['itemId'].max() + 1\n","\n","    train_config = {'method': 'SPMF_by_period',\n","                    'dir_name': 'SPMF_2_train11-23_test24-30_1epoch',  # edit strategy type, train test period, number of epochs\n","                    'pretrain_model': 'pretrain_train1-10_test11_10epoch_0.001',\n","                    'start_date': 20140901,  # overall train start date\n","                    'end_date': 20141231,  # overall train end date\n","                    'num_periods': 31,  # number of periods divided into\n","                    'train_start_period': 11,\n","                    'test_start_period': 24,\n","                    'cur_period': None,  # current incremental period\n","                    'next_period': None,  # next incremental period\n","                    'cur_set_size': None,  # current incremental dataset size\n","                    'next_set_size': None,  # next incremental dataset size\n","                    'period_alias': None,  # individual period directory alias to save ckpts\n","                    'restored_ckpt_mode': 'best auc',  # mode to search the checkpoint to restore, 'best auc', 'best gauc', 'last'\n","                    'restored_ckpt': None,  # configure in the for loop\n","\n","                    'strategy': 2,  # two different sampling strategies\n","                    'frac_of_pretrain_D': None,  # reservoir size as a fraction of pretrain dataset, less than or equal to 1\n","                    'res_cur_ratio': None,  # the ratio of reservoir sample to current set, only for strategy 2\n","\n","                    'base_optimizer': 'adam',  # base model optimizer: adam, rmsprop, sgd\n","                    'base_lr': None,  # base model learning rate\n","                    'base_bs': 256,  # base model batch size\n","                    'base_num_epochs': 1,  # base model number of epochs\n","                    'shuffle': False,  # whether to shuffle the dataset for each epoch\n","                    }\n","\n","    EmbMLPnocate_hyperparams = {'num_users': num_users,\n","                                'num_items': num_items,\n","                                'user_embed_dim': 8,\n","                                'item_embed_dim': 8,\n","                                'layers': [24, 12, 6, 1]  # input dim is user_embed_dim + item_embed_dim x 2\n","                                }\n","\n","    # sort train data into periods based on num_periods\n","    data_df = data_df[(data_df['date'] >= train_config['start_date']) & (data_df['date'] <= train_config['end_date'])]\n","    data_df = data_df.sort_values(['timestamp']).reset_index(drop=True)\n","    records_per_period = int(len(data_df) / train_config['num_periods'])\n","    data_df['index'] = data_df.index\n","    data_df['period'] = data_df['index'].apply(lambda x: int(x / records_per_period) + 1)\n","    data_df = data_df[data_df.period != train_config['num_periods'] + 1]  # delete last extra period\n","    period_df = data_df.groupby('period')['date'].agg(['count', 'min', 'max'])\n","    data_df = data_df.drop(['index', 'date', 'timestamp'], axis=1)\n","\n","    orig_dir_name = train_config['dir_name']\n","\n","    for frac in [1]:\n","\n","        for ratio in [0.1]:\n","\n","            for base_lr in [1e-3]:\n","\n","                print('')\n","                print('frac_of_pretrain_D', frac, 'res_cur_ratio', ratio, 'base_lr', base_lr)\n","\n","                train_config['frac_of_pretrain_D'] = frac\n","                train_config['res_cur_ratio'] = ratio\n","                train_config['base_lr'] = base_lr\n","\n","                train_config['dir_name'] = orig_dir_name + '_' + str(frac) + '_' + str(ratio) + '_' + str(base_lr)  # for strategy 2\n","                # train_config['dir_name'] = orig_dir_name + '_' + str(frac) + '_' + str(base_lr)  # for strategy 1\n","                print('dir_name: ', train_config['dir_name'])\n","\n","                test_aucs = []\n","                test_loglosses = []\n","\n","                for i in range(train_config['train_start_period'], train_config['num_periods']):\n","\n","                    # configure cur_period, next_period\n","                    train_config['cur_period'] = i\n","                    train_config['next_period'] = i + 1\n","                    print('')\n","                    print('current period: {}, next period: {}'.format(\n","                        train_config['cur_period'],\n","                        train_config['next_period']))\n","                    print('')\n","\n","                    # create current and next set\n","                    cur_set = data_df[data_df['period'] == train_config['cur_period']]\n","                    next_set = data_df[data_df['period'] == train_config['next_period']]\n","                    train_config['cur_set_size'] = len(cur_set)\n","                    train_config['next_set_size'] = len(next_set)\n","                    print('current set size', len(cur_set), 'next set size', len(next_set))\n","\n","                    # create train\n","                    pos_cur_set = cur_set[cur_set['label'] == 1]\n","                    neg_cur_set = cur_set[cur_set['label'] == 0]\n","\n","                    if i == train_config['train_start_period']:\n","                        pos_pretrain_set = data_df[(data_df['period'] < train_config['train_start_period']) & (data_df['label'] == 1)]\n","                        reservoir_size = int(len(pos_pretrain_set) * train_config['frac_of_pretrain_D'])\n","                        reservoir = pos_pretrain_set.sample(n=reservoir_size)\n","\n","                        neg_pretrain_set = data_df[(data_df['period'] < train_config['train_start_period']) & (data_df['label'] == 0)]\n","                        neg_reservoir_size = int(len(neg_pretrain_set) * train_config['frac_of_pretrain_D'])\n","                        neg_reservoir = neg_pretrain_set.sample(n=neg_reservoir_size)\n","\n","                    train_config['period_alias'] = 'period' + str(i)\n","\n","                    # checkpoints directory\n","                    ckpts_dir = os.path.join('ckpts', train_config['dir_name'], train_config['period_alias'])\n","                    if not os.path.exists(ckpts_dir):\n","                        os.makedirs(ckpts_dir)\n","\n","                    if i == train_config['train_start_period']:\n","                        search_alias = os.path.join('ckpts', train_config['pretrain_model'], 'Epoch*')\n","                        train_config['restored_ckpt'] = search_ckpt(search_alias, mode=train_config['restored_ckpt_mode'])\n","                    else:\n","                        prev_period_alias = 'period' + str(i - 1)\n","                        search_alias = os.path.join('ckpts', train_config['dir_name'], prev_period_alias, 'Epoch*')\n","                        train_config['restored_ckpt'] = search_ckpt(search_alias, mode=train_config['restored_ckpt_mode'])\n","                    print('restored checkpoint: {}'.format(train_config['restored_ckpt']))\n","\n","                    # write train_config to text file\n","                    with open(os.path.join(ckpts_dir, 'config.txt'), mode='w') as f:\n","                        f.write('train_config: ' + str(train_config) + '\\n')\n","                        f.write('\\n')\n","                        f.write('EmbMLPnocate_hyperparams: ' + str(EmbMLPnocate_hyperparams) + '\\n')\n","\n","                    # build base model computation graph\n","                    tf.reset_default_graph()\n","                    base_model = EmbMLPnocate(EmbMLPnocate_hyperparams, train_config=train_config)\n","\n","                    # create session\n","                    with tf.Session() as sess:\n","                        saver = tf.train.Saver()\n","                        saver.restore(sess, train_config['restored_ckpt'])\n","\n","                        def compute_prob_and_gen_set_and_update_reservoir():\n","\n","                            \"\"\"\n","                            this strategy follows exactly the method from the paper \"Streaming ranking based recommender systems\"\n","                            train_set = samples of (current_set + reservoir)\n","                            \"\"\"\n","                            compute_prob_start_time = time.time()\n","\n","                            pos_train_set = pd.concat([reservoir, pos_cur_set], ignore_index=False)  # combine R and W\n","                            neg_train_set = pd.concat([neg_reservoir, neg_cur_set], ignore_index=False)  # combine R and W\n","\n","                            # compute prob\n","                            pos_train_batch_loader = BatchLoader(pos_train_set, train_config['base_bs'])\n","\n","                            scores = []\n","                            for i in range(1, pos_train_batch_loader.num_batches + 1):\n","                                pos_train_batch = pos_train_batch_loader.get_batch(batch_id=i)\n","                                batch_scores, batch_losses = base_model.inference(sess, pos_train_batch)  # sess.run\n","                                scores.extend(batch_scores)\n","\n","                            ordered_pos_train_set = pos_train_set\n","                            ordered_pos_train_set['score'] = scores\n","                            ordered_pos_train_set = ordered_pos_train_set.sort_values(['score'], ascending=False).reset_index(drop=True)  # edit\n","                            ordered_pos_train_set['rank'] = np.arange(len(ordered_pos_train_set))\n","                            total_num = len(pos_train_set)\n","                            ordered_pos_train_set['weight'] = ordered_pos_train_set['rank'].apply(lambda x: np.exp(x / total_num))\n","                            total_weights = ordered_pos_train_set['weight'].sum()\n","                            ordered_pos_train_set['prob'] = ordered_pos_train_set['weight'].apply(lambda x: x / total_weights)\n","                            ordered_pos_train_set = ordered_pos_train_set.drop(['score', 'rank', 'weight'], axis=1)\n","\n","                            # generate train set\n","                            sampled_pos_train_set = ordered_pos_train_set.sample(n=len(pos_cur_set), replace=False, weights='prob')\n","                            sampled_pos_train_set = sampled_pos_train_set.drop(['prob'], axis=1)\n","                            sampled_neg_train_set = neg_train_set.sample(n=len(neg_cur_set), replace=False)\n","                            sampled_train_set = pd.concat([sampled_pos_train_set, sampled_neg_train_set], ignore_index=False)\n","                            sampled_train_set = sampled_train_set.sort_values(['period']).reset_index(drop=True)\n","\n","                            # update pos reservoir\n","                            t = len(data_df[(data_df['period'] < train_config['cur_period']) & (data_df['label'] == 1)])\n","                            probs_to_res = len(reservoir) / (t + np.arange(len(pos_cur_set)) + 1)\n","                            random_probs = np.random.rand(len(pos_cur_set))\n","                            selected_pos_cur_set = pos_cur_set[probs_to_res > random_probs]\n","                            num_left_in_res = len(reservoir) - len(selected_pos_cur_set)\n","                            updated_reservoir = pd.concat([reservoir.sample(n=num_left_in_res), selected_pos_cur_set], ignore_index=False)\n","                            print('selected_pos_cur_set size', len(selected_pos_cur_set))\n","                            # print('num_in_res', len(reservoir))\n","                            # print('num_left_in_res', num_left_in_res)\n","                            # print('num_in_updated_res', len(updated_reservoir))\n","\n","                            # update neg reservoir\n","                            t = len(data_df[(data_df['period'] < train_config['cur_period']) & (data_df['label'] == 0)])\n","                            probs_to_res = len(neg_reservoir) / (t + np.arange(len(neg_cur_set)) + 1)\n","                            random_probs = np.random.rand(len(neg_cur_set))\n","                            selected_neg_cur_set = neg_cur_set[probs_to_res > random_probs]\n","                            num_left_in_res = len(neg_reservoir) - len(selected_neg_cur_set)\n","                            updated_neg_reservoir = pd.concat([neg_reservoir.sample(n=num_left_in_res), selected_neg_cur_set], ignore_index=False)\n","                            print('selected_neg_cur_set size', len(selected_neg_cur_set))\n","                            # print('num_in_neg_res', len(neg_reservoir))\n","                            # print('num_left_in_neg_res', num_left_in_res)\n","                            # print('num_in_updated_neg_res', len(updated_neg_reservoir))\n","\n","                            print('compute prob and generate train set and update reservoir time elapsed: {}'.format(\n","                                time.strftime('%H:%M:%S', time.gmtime(time.time() - compute_prob_start_time))))\n","\n","                            return sampled_train_set, updated_reservoir, updated_neg_reservoir\n","\n","\n","                        def compute_prob_and_gen_set_and_update_reservoir2():\n","                            \"\"\"\n","                            this strategy modify slightly the method from paper \"Streaming ranking based recommender systems\"\n","                            train_set = current_set + samples of reservoir (need to set ratio of reservoir sample to current set)\n","                            \"\"\"\n","                            compute_prob_start_time = time.time()\n","\n","                            # compute prob\n","                            reservoir_batch_loader = BatchLoader(reservoir, train_config['base_bs'])\n","\n","                            scores = []\n","                            for i in range(1, reservoir_batch_loader.num_batches + 1):\n","                                reservoir_batch = reservoir_batch_loader.get_batch(batch_id=i)\n","                                batch_scores, batch_losses = base_model.inference(sess, reservoir_batch)  # sess.run\n","                                scores.extend(batch_scores.tolist())\n","\n","                            ordered_reservoir = reservoir\n","                            ordered_reservoir['score'] = scores\n","                            ordered_reservoir = ordered_reservoir.sort_values(['score'], ascending=False).reset_index(drop=True)  # edit\n","                            ordered_reservoir['rank'] = np.arange(len(ordered_reservoir))\n","                            total_num = len(reservoir)\n","                            ordered_reservoir['weight'] = ordered_reservoir['rank'].apply(lambda x: np.exp(x / total_num))\n","                            total_weights = ordered_reservoir['weight'].sum()\n","                            ordered_reservoir['prob'] = ordered_reservoir['weight'].apply(lambda x: x / total_weights)\n","                            ordered_reservoir = ordered_reservoir.drop(['score', 'rank', 'weight'], axis=1)\n","\n","                            # generate train set\n","                            sampled_pos_reservoir = ordered_reservoir.sample(n=int(len(pos_cur_set) * train_config['res_cur_ratio']), replace=False, weights='prob')\n","                            sampled_pos_reservoir = sampled_pos_reservoir.drop(['prob'], axis=1)\n","                            sampled_neg_reservoir = neg_reservoir.sample(n=int(len(neg_cur_set) * train_config['res_cur_ratio']), replace=False)\n","                            sampled_reservoir = pd.concat([sampled_pos_reservoir, sampled_neg_reservoir], ignore_index=False)\n","                            sampled_train_set = pd.concat([sampled_reservoir, cur_set], ignore_index=False)\n","                            sampled_train_set = sampled_train_set.sort_values(['period']).reset_index(drop=True)\n","                            print('sampled_reservoir size', len(sampled_reservoir))\n","                            # print('sampled_train_set size', len(sampled_train_set))\n","\n","                            # update reservoir\n","                            t = len(data_df[(data_df['period'] < train_config['cur_period']) & (data_df['label'] == 1)])\n","                            probs_to_res = len(reservoir) / (t + np.arange(len(pos_cur_set)) + 1)\n","                            random_probs = np.random.rand(len(pos_cur_set))\n","                            selected_pos_cur_set = pos_cur_set[probs_to_res > random_probs]\n","                            num_left_in_res = len(reservoir) - len(selected_pos_cur_set)\n","                            updated_reservoir = pd.concat([reservoir.sample(n=num_left_in_res), selected_pos_cur_set], ignore_index=False)\n","                            print('selected_pos_current_set size', len(selected_pos_cur_set))\n","                            # print('num_in_res', len(reservoir))\n","                            # print('num_left_in_res', num_left_in_res)\n","                            # print('num_in_updated_res', len(updated_reservoir))\n","\n","                            # update neg reservoir\n","                            t = len(data_df[(data_df['period'] < train_config['cur_period']) & (data_df['label'] == 0)])\n","                            probs_to_res = len(neg_reservoir) / (t + np.arange(len(neg_cur_set)) + 1)\n","                            random_probs = np.random.rand(len(neg_cur_set))\n","                            selected_neg_cur_set = neg_cur_set[probs_to_res > random_probs]\n","                            num_left_in_res = len(neg_reservoir) - len(selected_neg_cur_set)\n","                            updated_neg_reservoir = pd.concat([neg_reservoir.sample(n=num_left_in_res), selected_neg_cur_set], ignore_index=False)\n","                            print('selected_neg_cur_set size', len(selected_neg_cur_set))\n","                            # print('num_in_neg_res', len(neg_reservoir))\n","                            # print('num_left_in_neg_res', num_left_in_res)\n","                            # print('num_in_updated_neg_res', len(updated_neg_reservoir))\n","\n","                            print('compute prob and generate train set and update reservoir time elapsed: {}'.format(\n","                                time.strftime('%H:%M:%S', time.gmtime(time.time() - compute_prob_start_time))))\n","\n","                            return sampled_train_set, updated_reservoir, updated_neg_reservoir\n","\n","                        if train_config['strategy'] == 1:\n","                            cur_set, reservoir, neg_reservoir = compute_prob_and_gen_set_and_update_reservoir()\n","                        else:  # train_config['strategy'] == 2\n","                            cur_set, reservoir, neg_reservoir = compute_prob_and_gen_set_and_update_reservoir2()\n","\n","                        # create an engine instance with base_model\n","                        engine = Engine(sess, base_model)\n","\n","                        train_start_time = time.time()\n","\n","                        max_auc = 0\n","                        best_logloss = 0\n","\n","                        for epoch_id in range(1, train_config['base_num_epochs'] + 1):\n","\n","                            print('Training Base Model Epoch {} Start!'.format(epoch_id))\n","\n","                            base_loss_cur_avg = engine.base_train_an_epoch(epoch_id, cur_set, train_config)\n","                            print('Epoch {} Done! time elapsed: {}, base_loss_cur_avg {:.4f}'.format(\n","                                epoch_id,\n","                                time.strftime('%H:%M:%S', time.gmtime(time.time() - train_start_time)),\n","                                base_loss_cur_avg))\n","\n","                            cur_auc, cur_logloss = engine.test(cur_set, train_config)\n","                            next_auc, next_logloss = engine.test(next_set, train_config)\n","                            print('cur_auc {:.4f}, cur_logloss {:.4f}, next_auc {:.4f}, next_logloss {:.4f}'.format(\n","                                cur_auc,\n","                                cur_logloss,\n","                                next_auc,\n","                                next_logloss))\n","                            \n","                            print('time elapsed {}'.format(time.strftime('%H:%M:%S', time.gmtime(time.time() - train_start_time))))\n","                            print('')\n","\n","                            # save checkpoint\n","                            checkpoint_alias = 'Epoch{}_TestAUC{:.4f}_TestLOGLOSS{:.4f}.ckpt'.format(\n","                                epoch_id,\n","                                next_auc,\n","                                next_logloss)\n","                            checkpoint_path = os.path.join(ckpts_dir, checkpoint_alias)\n","                            saver.save(sess, checkpoint_path)\n","\n","                            if next_auc > max_auc:\n","                                max_auc = next_auc\n","                                best_logloss = next_logloss\n","\n","                        if i >= train_config['test_start_period']:\n","                            test_aucs.append(max_auc)\n","                            test_loglosses.append(best_logloss)\n","\n","                    if i >= train_config['test_start_period']:\n","                        average_auc = sum(test_aucs) / len(test_aucs)\n","                        average_logloss = sum(test_loglosses) / len(test_loglosses)\n","                        print('test aucs', test_aucs)\n","                        print('average auc', average_auc)\n","                        print('')\n","                        print('test loglosses', test_loglosses)\n","                        print('average logloss', average_logloss)\n","\n","                        # write metrics to text file\n","                        with open(os.path.join(ckpts_dir, 'test_metrics.txt'), mode='w') as f:\n","                            f.write('test_aucs: ' + str(test_aucs) + '\\n')\n","                            f.write('average_auc: ' + str(average_auc) + '\\n')\n","                            f.write('test_loglosses: ' + str(test_loglosses) + '\\n')\n","                            f.write('average_logloss: ' + str(average_logloss) + '\\n')"],"execution_count":40,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DAbNpqZ4NPq_"},"source":["### IncCTR"]},{"cell_type":"code","metadata":{"id":"LctucMACNPnE","executionInfo":{"status":"ok","timestamp":1636623321604,"user_tz":-330,"elapsed":801,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}}},"source":["def incctr_sobazaar():\n","    # load data to df\n","    start_time = time.time()\n","\n","    load_path = osp.join(args.path_silver,'soba_4mth_2014_1neg_30seq_1.csv')\n","    data_df = pd.read_csv(load_path)\n","\n","    data_df['itemSeq'] = data_df['itemSeq'].fillna('')  # empty seq are NaN\n","    data_df['itemSeq'] = data_df['itemSeq'].apply(lambda x: [int(item) for item in x.split('#') if item != ''])\n","\n","    logger.info('Done loading data! time elapsed: {}'.format(time.strftime('%H:%M:%S', time.gmtime(time.time() - start_time))))\n","\n","    num_users = data_df['userId'].max() + 1\n","    num_items = data_df['itemId'].max() + 1\n","\n","    train_config = {'method': 'IncCTR_by_period',\n","                    'dir_name': 'IncCTR_train11-23_test24-30_1epoch',  # edit train test period, number of epochs\n","                    'pretrain_model': 'pretrain_train1-10_test11_10epoch_0.001',\n","                    'start_date': 20140901,  # overall train start date\n","                    'end_date': 20141231,  # overall train end date\n","                    'num_periods': 31,  # number of periods divided into\n","                    'train_start_period': 11,\n","                    'test_start_period': 24,\n","                    'cur_period': None,  # current incremental period\n","                    'next_period': None,  # next incremental period\n","                    'cur_set_size': None,  # current incremental dataset size\n","                    'next_set_size': None,  # next incremental dataset size\n","                    'period_alias': None,  # individual period directory alias to save ckpts\n","                    'restored_ckpt_mode': 'best auc',  # mode to search the checkpoint to restore, 'best auc', 'best gauc', 'last'\n","                    'restored_ckpt': None,  # configure in the for loop\n","\n","                    'lambda': None,  # weight assigned to knowledge distillation loss\n","\n","                    'base_optimizer': 'adam',  # base model optimizer: adam, rmsprop, sgd\n","                    'base_lr': None,  # base model learning rate\n","                    'base_bs': 256,  # base model batch size\n","                    'base_num_epochs': 1,  # base model number of epochs\n","                    'shuffle': True,  # whether to shuffle the dataset for each epoch\n","                    }\n","\n","    EmbMLPnocate_hyperparams = {'num_users': num_users,\n","                                'num_items': num_items,\n","                                'user_embed_dim': 8,\n","                                'item_embed_dim': 8,\n","                                'layers': [24, 12, 6, 1]  # input dim is user_embed_dim + item_embed_dim x 2\n","                                }\n","\n","    # sort train data into periods based on num_periods\n","    data_df = data_df[(data_df['date'] >= train_config['start_date']) & (data_df['date'] <= train_config['end_date'])]\n","    data_df = data_df.sort_values(['timestamp']).reset_index(drop=True)\n","    records_per_period = int(len(data_df) / train_config['num_periods'])\n","    data_df['index'] = data_df.index\n","    data_df['period'] = data_df['index'].apply(lambda x: int(x / records_per_period) + 1)\n","    data_df = data_df[data_df.period != train_config['num_periods'] + 1]  # delete last extra period\n","    period_df = data_df.groupby('period')['date'].agg(['count', 'min', 'max'])\n","    data_df = data_df.drop(['index', 'date', 'timestamp'], axis=1)\n","\n","    orig_dir_name = train_config['dir_name']\n","\n","    for lam in [0.1]:\n","\n","        for base_lr in [1e-3]:\n","\n","            print('')\n","            print('lambda', lam, 'base_lr', base_lr)\n","\n","            train_config['lambda'] = lam\n","            train_config['base_lr'] = base_lr\n","\n","            train_config['dir_name'] = orig_dir_name + '_' + str(lam) + '_' + str(base_lr)\n","            print('dir_name: ', train_config['dir_name'])\n","\n","            test_aucs = []\n","            test_loglosses = []\n","\n","            for i in range(train_config['train_start_period'], train_config['num_periods']):\n","\n","                # configure cur_period, next_period\n","                train_config['cur_period'] = i\n","                train_config['next_period'] = i + 1\n","                print('')\n","                print('current period: {}, next period: {}'.format(\n","                    train_config['cur_period'],\n","                    train_config['next_period']))\n","                print('')\n","\n","                # create current and next set\n","                cur_set = data_df[data_df['period'] == train_config['cur_period']]\n","                next_set = data_df[data_df['period'] == train_config['next_period']]\n","                train_config['cur_set_size'] = len(cur_set)\n","                train_config['next_set_size'] = len(next_set)\n","                print('current set size', len(cur_set), 'next set size', len(next_set))\n","\n","                train_config['period_alias'] = 'period' + str(i)\n","\n","                # checkpoints directory\n","                ckpts_dir = os.path.join('ckpts', train_config['dir_name'], train_config['period_alias'])\n","                if not os.path.exists(ckpts_dir):\n","                    os.makedirs(ckpts_dir)\n","\n","                if i == train_config['train_start_period']:\n","                    search_alias = os.path.join('ckpts', train_config['pretrain_model'], 'Epoch*')\n","                    train_config['restored_ckpt'] = search_ckpt(search_alias, mode=train_config['restored_ckpt_mode'])\n","                else:\n","                    prev_period_alias = 'period' + str(i - 1)\n","                    search_alias = os.path.join('ckpts', train_config['dir_name'], prev_period_alias, 'Epoch*')\n","                    train_config['restored_ckpt'] = search_ckpt(search_alias, mode=train_config['restored_ckpt_mode'])\n","                print('restored checkpoint: {}'.format(train_config['restored_ckpt']))\n","\n","                # write train_config to text file\n","                with open(os.path.join(ckpts_dir, 'config.txt'), mode='w') as f:\n","                    f.write('train_config: ' + str(train_config) + '\\n')\n","                    f.write('\\n')\n","                    f.write('EmbMLPnocate_hyperparams: ' + str(EmbMLPnocate_hyperparams) + '\\n')\n","\n","                # build base model computation graph\n","                tf.reset_default_graph()\n","                base_model = EmbMLPnocate(EmbMLPnocate_hyperparams, train_config=train_config)\n","\n","                # create session\n","                with tf.Session() as sess:\n","                    saver = tf.train.Saver()\n","                    saver.restore(sess, train_config['restored_ckpt'])\n","\n","                    def infer_prev_base():\n","                        infer_start_time = time.time()\n","                        infer_batch_loader = BatchLoader(cur_set, train_config['base_bs'])  # load batch test set\n","                        scores = []\n","                        for i in range(1, infer_batch_loader.num_batches + 1):\n","                            infer_batch = infer_batch_loader.get_batch(batch_id=i)\n","                            batch_scores, batch_losses = base_model.inference(sess, infer_batch)  # sees.run\n","                            scores.extend(batch_scores.tolist())\n","                        print('Inference Done! time elapsed: {}'.format(time.strftime('%H:%M:%S', time.gmtime(time.time() - infer_start_time))))\n","                        return scores\n","\n","                    cur_set['label_soft'] = infer_prev_base()\n","                    \n","                    # create an engine instance with base_model\n","                    engine = Engine(sess, base_model)\n","\n","                    train_start_time = time.time()\n","\n","                    max_auc = 0\n","                    best_logloss = 0\n","\n","                    for epoch_id in range(1, train_config['base_num_epochs'] + 1):\n","\n","                        print('Training Base Model Epoch {} Start!'.format(epoch_id))\n","\n","                        base_loss_cur_avg = engine.base_train_an_epoch(epoch_id, cur_set, train_config)\n","                        print('Epoch {} Done! time elapsed: {}, base_loss_cur_avg {:.4f}'.format(\n","                            epoch_id,\n","                            time.strftime('%H:%M:%S', time.gmtime(time.time() - train_start_time)),\n","                            base_loss_cur_avg))\n","\n","                        cur_auc, cur_logloss = engine.test(cur_set, train_config)\n","                        next_auc, next_logloss = engine.test(next_set, train_config)\n","                        print('cur_auc {:.4f}, cur_logloss {:.4f}, next_auc {:.4f}, next_logloss {:.4f}'.format(\n","                            cur_auc,\n","                            cur_logloss,\n","                            next_auc,\n","                            next_logloss))\n","                        print('time elapsed {}'.format(time.strftime('%H:%M:%S', time.gmtime(time.time() - train_start_time))))\n","\n","                        print('')\n","\n","                        # save checkpoint\n","                        checkpoint_alias = 'Epoch{}_TestAUC{:.4f}_TestLOGLOSS{:.4f}.ckpt'.format(\n","                            epoch_id,\n","                            next_auc,\n","                            next_logloss)\n","                        checkpoint_path = os.path.join(ckpts_dir, checkpoint_alias)\n","                        saver.save(sess, checkpoint_path)\n","\n","                        if next_auc > max_auc:\n","                            max_auc = next_auc\n","                            best_logloss = next_logloss\n","\n","                    if i >= train_config['test_start_period']:\n","                        test_aucs.append(max_auc)\n","                        test_loglosses.append(best_logloss)\n","\n","                if i >= train_config['test_start_period']:\n","                    average_auc = sum(test_aucs) / len(test_aucs)\n","                    average_logloss = sum(test_loglosses) / len(test_loglosses)\n","                    print('test aucs', test_aucs)\n","                    print('average auc', average_auc)\n","                    print('')\n","                    print('test loglosses', test_loglosses)\n","                    print('average logloss', average_logloss)\n","\n","                    # write metrics to text file\n","                    with open(os.path.join(ckpts_dir, 'test_metrics.txt'), mode='w') as f:\n","                        f.write('test_aucs: ' + str(test_aucs) + '\\n')\n","                        f.write('average_auc: ' + str(average_auc) + '\\n')\n","                        f.write('test_loglosses: ' + str(test_loglosses) + '\\n')\n","                        f.write('average_logloss: ' + str(average_logloss) + '\\n')"],"execution_count":44,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_vlw927jV7Bw"},"source":["### SML"]},{"cell_type":"code","metadata":{"id":"aV7fwSblWNtc","executionInfo":{"status":"ok","timestamp":1636623752513,"user_tz":-330,"elapsed":5,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}}},"source":["class SMLEngine(object):\n","    \"\"\"\n","    Training epoch and test\n","    \"\"\"\n","\n","    def __init__(self, sess, model):\n","\n","        self.sess = sess\n","        self.model = model\n","\n","    def base_train_an_epoch(self, epoch_id, cur_set, train_config):\n","\n","        train_start_time = time.time()\n","\n","        if train_config['shuffle']:\n","            cur_set = cur_set.sample(frac=1)\n","\n","        cur_batch_loader = BatchLoader(cur_set, train_config['base_bs'])\n","\n","        base_loss_cur_sum = 0\n","\n","        for i in range(1, cur_batch_loader.num_batches + 1):\n","\n","            cur_batch = cur_batch_loader.get_batch(batch_id=i)\n","\n","            base_loss_cur = self.model.train_base(self.sess, cur_batch)  # sess.run\n","\n","            if (i - 1) % 100 == 0:\n","                print('[Epoch {} Batch {}] base_loss_cur {:.4f}, time elapsed {}'.format(epoch_id,\n","                                                                                         i,\n","                                                                                         base_loss_cur,\n","                                                                                         time.strftime('%H:%M:%S', time.gmtime(time.time() - train_start_time))))\n","\n","            base_loss_cur_sum += base_loss_cur\n","\n","        # epoch done, compute average loss\n","        base_loss_cur_avg = base_loss_cur_sum / cur_batch_loader.num_batches\n","\n","        return base_loss_cur_avg\n","\n","    def transfer_train_an_epoch(self, epoch_id, next_set, train_config):\n","\n","        train_start_time = time.time()\n","\n","        if train_config['shuffle']:\n","            next_set = next_set.sample(frac=1)\n","\n","        next_batch_loader = BatchLoader(next_set, train_config['transfer_bs'])\n","\n","        transfer_loss_next_sum = 0\n","\n","        for i in range(1, next_batch_loader.num_batches + 1):\n","\n","            next_batch = next_batch_loader.get_batch(batch_id=i)\n","\n","            transfer_loss_next = self.model.train_transfer(self.sess, next_batch)  # sess.run\n","\n","            if (i - 1) % 100 == 0:\n","                print('[Epoch {} Batch {}] transfer_loss_next {:.4f}, time elapsed {}'.format(epoch_id,\n","                                                                                              i,\n","                                                                                              transfer_loss_next,\n","                                                                                              time.strftime('%H:%M:%S', time.gmtime(time.time() - train_start_time))))\n","\n","                # test the performance of transferred model (can comment out if not needed)\n","                next_auc, next_logloss = self.test(next_set, train_config)\n","                print('next_auc {:.4f}, next_logloss {:.4f}'.format(\n","                    next_auc,\n","                    next_logloss))\n","                print('time elapsed {}'.format(time.strftime('%H:%M:%S', time.gmtime(time.time() - train_start_time))))\n","\n","                print('')\n","\n","            transfer_loss_next_sum += transfer_loss_next\n","\n","        # epoch done, compute average loss\n","        transfer_loss_next_avg = transfer_loss_next_sum / next_batch_loader.num_batches\n","\n","        return transfer_loss_next_avg\n","\n","    def test(self, test_set, train_config):\n","\n","        test_batch_loader = BatchLoader(test_set, train_config['base_bs'])\n","\n","        scores, losses, labels = [], [], []\n","        for i in range(1, test_batch_loader.num_batches + 1):\n","            test_batch = test_batch_loader.get_batch(batch_id=i)\n","            batch_scores, batch_losses = self.model.inference(self.sess, test_batch)  # sees.run\n","            scores.extend(batch_scores.tolist())\n","            losses.extend(batch_losses.tolist())\n","            labels.extend(test_batch[4])\n","\n","        test_auc = cal_roc_auc(scores, labels)\n","        test_logloss = sum(losses) / len(losses)\n","\n","        return test_auc, test_logloss"],"execution_count":50,"outputs":[]},{"cell_type":"code","metadata":{"id":"HZcrkhPbWUH-","executionInfo":{"status":"ok","timestamp":1636623755395,"user_tz":-330,"elapsed":1831,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}}},"source":["def gelu(input_tensor):\n","    cdf = 0.5 * (1.0 + tf.erf(input_tensor / tf.sqrt(2.0)))\n","    return input_tensor * cdf\n","\n","\n","def transfer_emb(name, emb_prev, emb_upd, n1=10, n2=5, l1=20):\n","\n","    with tf.variable_scope(name):\n","        embed_dim = emb_upd.get_shape().as_list()[-1]  # H\n","        embeds_norm = tf.sqrt(tf.reduce_sum(emb_prev * emb_prev, axis=-1))  # [num]\n","        embeds_dot = tf.div(emb_prev * emb_upd, tf.expand_dims(embeds_norm, -1) + tf.constant(1e-15))  # [num, H]\n","        stack_embeds = tf.stack([emb_prev, emb_upd, embeds_dot], axis=1)  # [num, 3, H]\n","\n","        input1 = tf.expand_dims(stack_embeds, -1)  # [num, 3, H, 1]\n","        filter1 = tf.get_variable(name=\"cnn_filter1\", shape=[3, 1, 1, n1])  # [3, 1, 1, n1]\n","        output1 = tf.nn.conv2d(input1, filter1, strides=[1, 1, 1, 1], padding='VALID')  # [num, 1, H, n1]\n","        output1 = gelu(output1)  # [num, 1, H, n1]\n","\n","        input2 = tf.transpose(output1, perm=[0, 3, 2, 1])  # [num, n1, H, 1]\n","        filter2 = tf.get_variable(name=\"cnn_filter2\", shape=[n1, 1, 1, n2])  # [n1, 1, 1, n2]\n","        output2 = tf.nn.conv2d(input2, filter2, strides=[1, 1, 1, 1], padding='VALID')  # [num, 1, H, n2]\n","        output2 = gelu(output2)  # [num, 1, H, n2]\n","\n","        cnn_output = tf.transpose(output2, perm=[0, 3, 2, 1])  # [num, n2, H, 1]\n","        cnn_output = tf.reshape(cnn_output, shape=[-1, n2 * embed_dim])  # [num, n2 x H]\n","\n","        with tf.variable_scope('fcn1'):\n","            fcn1_kernel = tf.get_variable(name='kernel', shape=[n2 * embed_dim, l1])  # [n2 x H, l1]\n","            fcn1_bias = tf.get_variable(name='bias', shape=[l1])  # [l1]\n","        with tf.variable_scope('fcn2'):\n","            fcn2_kernel = tf.get_variable(name='kernel', shape=[l1, embed_dim])  # [l1, H]\n","            fcn2_bias = tf.get_variable(name='bias', shape=[embed_dim])  # [H]\n","\n","        fcn1 = gelu(tf.matmul(cnn_output, fcn1_kernel) + fcn1_bias)  # [num, l1]\n","        fcn2 = tf.matmul(fcn1, fcn2_kernel) + fcn2_bias  # [num, H]\n","\n","    return fcn2\n","\n","\n","def transfer_mlp(name, param_prev, param_upd, param_shape, n1=5, n2=3, l1=40):\n","\n","    with tf.variable_scope(name):\n","        param_prev = tf.reshape(param_prev, [-1])  # [dim]\n","        param_upd = tf.reshape(param_upd, [-1])  # [dim]\n","        param_dim = param_upd.get_shape().as_list()[-1]  # max_dim: 24 x 12 = 288\n","        param_norm = tf.sqrt(tf.reduce_sum(param_prev * param_prev))  # scalar\n","        param_dot = tf.div(param_prev * param_upd, param_norm + tf.constant(1e-15))  # [dim] / [] = [dim]\n","        stack_param = tf.stack([param_prev, param_upd, param_dot], axis=0)  # [3, dim]\n","\n","        input1 = tf.expand_dims(tf.expand_dims(stack_param, -1), 0)  # [1, 3, dim, 1]\n","        filter1 = tf.get_variable(name=\"cnn_filter1\", shape=[3, 1, 1, n1])  # [3, 1, 1, n1]\n","        output1 = tf.nn.conv2d(input1, filter1, strides=[1, 1, 1, 1], padding='VALID')  # [1, 1, dim, n1]\n","        output1 = gelu(output1)  # [1, 1, dim, n1]\n","\n","        input2 = tf.transpose(output1, perm=[0, 3, 2, 1])  # [1, n1, dim, 1]\n","        filter2 = tf.get_variable(name=\"cnn_filter2\", shape=[n1, 1, 1, n2])  # [n1, 1, 1, n2]\n","        output2 = tf.nn.conv2d(input2, filter2, strides=[1, 1, 1, 1], padding='VALID')  # [1, 1, dim, n2]\n","        output2 = gelu(output2)  # [1, 1, dim, n2]\n","\n","        cnn_output = tf.transpose(output2, perm=[0, 3, 2, 1])  # [1, n2, dim, 1]\n","        cnn_output = tf.reshape(cnn_output, shape=[1, -1])  # [1, n2 x dim]\n","\n","        with tf.variable_scope('fcn1'):\n","            fcn1_kernel = tf.get_variable(name='kernel', shape=[n2 * param_dim, l1])  # [n2 x dim, l1]\n","            fcn1_bias = tf.get_variable(name='bias', shape=[l1])  # [l1]\n","        with tf.variable_scope('fcn2'):\n","            fcn2_kernel = tf.get_variable(name='kernel', shape=[l1, param_dim])  # [l1, dim]\n","            fcn2_bias = tf.get_variable(name='bias', shape=[param_dim])  # [dim]\n","\n","        fcn1 = gelu(tf.matmul(cnn_output, fcn1_kernel) + fcn1_bias)  # [1, l1]\n","        fcn2 = tf.matmul(fcn1, fcn2_kernel) + fcn2_bias  # [1, dim]\n","        output = tf.reshape(fcn2, shape=param_shape)  # [dim1, dim2, ...]\n","\n","    return output\n","\n","\n","class SML(object):\n","\n","    def __init__(self, hyperparams, prev_emb_dict, prev_mlp_dict, train_config=None):\n","\n","        self.train_config = train_config\n","\n","        # create placeholder\n","        self.u = tf.placeholder(tf.int32, [None])  # [B]\n","        self.i = tf.placeholder(tf.int32, [None])  # [B]\n","        self.hist_i = tf.placeholder(tf.int32, [None, None])  # [B, T]\n","        self.hist_len = tf.placeholder(tf.int32, [None])  # [B]\n","        self.y = tf.placeholder(tf.float32, [None])  # [B]\n","        self.base_lr = tf.placeholder(tf.float32, [], name='base_lr')  # scalar\n","        self.transfer_lr = tf.placeholder(tf.float32, [], name='transfer_lr')  # scalar\n","\n","        if train_config['transfer_emb']:\n","            # -- create emb_w_upd begin -------\n","            user_emb_w_upd = tf.get_variable(\"user_emb_w\", [hyperparams['num_users'], hyperparams['user_embed_dim']])\n","            item_emb_w_upd = tf.get_variable(\"item_emb_w\", [hyperparams['num_items'], hyperparams['item_embed_dim']])\n","            # -- create emb_w_upd end -------\n","\n","            # -- create emb_w_prev begin ----\n","            user_emb_w_prev = tf.convert_to_tensor(prev_emb_dict['user_emb_w'], tf.float32)\n","            item_emb_w_prev = tf.convert_to_tensor(prev_emb_dict['item_emb_w'], tf.float32)\n","            # -- create emb_w_prev end ----\n","\n","            # -- transfer emb_w begin ----\n","            with tf.variable_scope('transfer_emb'):\n","                user_emb_w = transfer_emb(name='user_emb_w',\n","                                          emb_prev=user_emb_w_prev,\n","                                          emb_upd=user_emb_w_upd,\n","                                          n1=train_config['emb_n1'],\n","                                          n2=train_config['emb_n2'],\n","                                          l1=train_config['emb_l1'])\n","                item_emb_w = transfer_emb(name='item_emb_w',\n","                                          emb_prev=item_emb_w_prev,\n","                                          emb_upd=item_emb_w_upd,\n","                                          n1=train_config['emb_n1'],\n","                                          n2=train_config['emb_n2'],\n","                                          l1=train_config['emb_l1'])\n","            # -- transfer emb end ----\n","\n","            # -- update op begin -------\n","            self.user_emb_w_upd_op = user_emb_w_upd.assign(user_emb_w)\n","            self.item_emb_w_upd_op = item_emb_w_upd.assign(item_emb_w)\n","            # -- update op end -------\n","\n","        else:\n","            # -- create emb_w begin -------\n","            user_emb_w = tf.get_variable(\"user_emb_w\", [hyperparams['num_users'], hyperparams['user_embed_dim']])\n","            item_emb_w = tf.get_variable(\"item_emb_w\", [hyperparams['num_items'], hyperparams['item_embed_dim']])\n","            # -- create emb_w end -------\n","\n","        if train_config['transfer_mlp']:\n","            # -- create mlp_upd begin ---\n","            concat_dim = hyperparams['user_embed_dim'] + hyperparams['item_embed_dim'] * 2\n","            with tf.variable_scope('fcn1'):\n","                fcn1_kernel_upd = tf.get_variable('kernel', [concat_dim, hyperparams['layers'][1]])\n","                fcn1_bias_upd = tf.get_variable('bias', [hyperparams['layers'][1]])\n","            with tf.variable_scope('fcn2'):\n","                fcn2_kernel_upd = tf.get_variable('kernel', [hyperparams['layers'][1], hyperparams['layers'][2]])\n","                fcn2_bias_upd = tf.get_variable('bias', [hyperparams['layers'][2]])\n","            with tf.variable_scope('fcn3'):\n","                fcn3_kernel_upd = tf.get_variable('kernel', [hyperparams['layers'][2], 1])\n","                fcn3_bias_upd = tf.get_variable('bias', [1])\n","            # -- create mlp_upd end ---\n","\n","            # -- create mlp_prev begin ----\n","            fcn1_kernel_prev = tf.convert_to_tensor(prev_mlp_dict['fcn1/kernel'], tf.float32)\n","            fcn1_bias_prev = tf.convert_to_tensor(prev_mlp_dict['fcn1/bias'], tf.float32)\n","            fcn2_kernel_prev = tf.convert_to_tensor(prev_mlp_dict['fcn2/kernel'], tf.float32)\n","            fcn2_bias_prev = tf.convert_to_tensor(prev_mlp_dict['fcn2/bias'], tf.float32)\n","            fcn3_kernel_prev = tf.convert_to_tensor(prev_mlp_dict['fcn3/kernel'], tf.float32)\n","            fcn3_bias_prev = tf.convert_to_tensor(prev_mlp_dict['fcn3/bias'], tf.float32)\n","            # -- create mlp_prev end ----\n","\n","            # -- transfer mlp begin ----\n","            with tf.variable_scope('transfer_mlp'):\n","                with tf.variable_scope('fcn1'):\n","                    fcn1_kernel = transfer_mlp(name='kernel',\n","                                               param_prev=fcn1_kernel_prev,\n","                                               param_upd=fcn1_kernel_upd,\n","                                               param_shape=[concat_dim, hyperparams['layers'][1]],\n","                                               n1=train_config['mlp_n1'],\n","                                               n2=train_config['mlp_n2'],\n","                                               l1=train_config['mlp_l1_dict']['fcn1/kernel'])\n","                    fcn1_bias = transfer_mlp(name='bias',\n","                                             param_prev=fcn1_bias_prev,\n","                                             param_upd=fcn1_bias_upd,\n","                                             param_shape=[hyperparams['layers'][1]],\n","                                             n1=train_config['mlp_n1'],\n","                                             n2=train_config['mlp_n2'],\n","                                             l1=train_config['mlp_l1_dict']['fcn1/bias'])\n","                with tf.variable_scope('fcn2'):\n","                    fcn2_kernel = transfer_mlp(name='kernel',\n","                                               param_prev=fcn2_kernel_prev,\n","                                               param_upd=fcn2_kernel_upd,\n","                                               param_shape=[hyperparams['layers'][1], hyperparams['layers'][2]],\n","                                               n1=train_config['mlp_n1'],\n","                                               n2=train_config['mlp_n2'],\n","                                               l1=train_config['mlp_l1_dict']['fcn2/kernel'])\n","                    fcn2_bias = transfer_mlp(name='bias',\n","                                             param_prev=fcn2_bias_prev,\n","                                             param_upd=fcn2_bias_upd,\n","                                             param_shape=[hyperparams['layers'][2]],\n","                                             n1=train_config['mlp_n1'],\n","                                             n2=train_config['mlp_n2'],\n","                                             l1=train_config['mlp_l1_dict']['fcn2/bias'])\n","                with tf.variable_scope('fcn3'):\n","                    fcn3_kernel = transfer_mlp(name='kernel',\n","                                               param_prev=fcn3_kernel_prev,\n","                                               param_upd=fcn3_kernel_upd,\n","                                               param_shape=[hyperparams['layers'][2], 1],\n","                                               n1=train_config['mlp_n1'],\n","                                               n2=train_config['mlp_n2'],\n","                                               l1=train_config['mlp_l1_dict']['fcn3/kernel'])\n","                    fcn3_bias = transfer_mlp(name='bias',\n","                                             param_prev=fcn3_bias_prev,\n","                                             param_upd=fcn3_bias_upd,\n","                                             param_shape=[1],\n","                                             n1=train_config['mlp_n1'],\n","                                             n2=train_config['mlp_n2'],\n","                                             l1=train_config['mlp_l1_dict']['fcn3/bias'])\n","            # -- transfer mlp end ----\n","\n","            # -- update op begin -------\n","            self.fcn1_kernel_upd_op = fcn1_kernel_upd.assign(fcn1_kernel)\n","            self.fcn1_bias_upd_op = fcn1_bias_upd.assign(fcn1_bias)\n","            self.fcn2_kernel_upd_op = fcn2_kernel_upd.assign(fcn2_kernel)\n","            self.fcn2_bias_upd_op = fcn2_bias_upd.assign(fcn2_bias)\n","            self.fcn3_kernel_upd_op = fcn3_kernel_upd.assign(fcn3_kernel)\n","            self.fcn3_bias_upd_op = fcn3_bias_upd.assign(fcn3_bias)\n","            # -- update op end -------\n","\n","        else:\n","            # -- create mlp begin ---\n","            concat_dim = hyperparams['user_embed_dim'] + hyperparams['item_embed_dim'] * 2\n","            with tf.variable_scope('fcn1'):\n","                fcn1_kernel = tf.get_variable('kernel', [concat_dim, hyperparams['layers'][1]])\n","                fcn1_bias = tf.get_variable('bias', [hyperparams['layers'][1]])\n","            with tf.variable_scope('fcn2'):\n","                fcn2_kernel = tf.get_variable('kernel', [hyperparams['layers'][1], hyperparams['layers'][2]])\n","                fcn2_bias = tf.get_variable('bias', [hyperparams['layers'][2]])\n","            with tf.variable_scope('fcn3'):\n","                fcn3_kernel = tf.get_variable('kernel', [hyperparams['layers'][2], 1])\n","                fcn3_bias = tf.get_variable('bias', [1])\n","            # -- create mlp end ---\n","\n","        # -- emb begin -------\n","        u_emb = tf.nn.embedding_lookup(user_emb_w, self.u)  # [B, H]\n","        i_emb = tf.nn.embedding_lookup(item_emb_w, self.i)  # [B, H]\n","        h_emb = tf.nn.embedding_lookup(item_emb_w, self.hist_i)  # [B, T, H]\n","        u_hist = average_pooling(h_emb, self.hist_len)  # [B, H]\n","        # -- emb end -------\n","\n","        # -- mlp begin -------\n","        fcn = tf.concat([u_emb, u_hist, i_emb], axis=-1)  # [B, H x 3]\n","        fcn_layer_1 = tf.nn.relu(tf.matmul(fcn, fcn1_kernel) + fcn1_bias)  # [B, l1]\n","        fcn_layer_2 = tf.nn.relu(tf.matmul(fcn_layer_1, fcn2_kernel) + fcn2_bias)  # [B, l2]\n","        fcn_layer_3 = tf.matmul(fcn_layer_2, fcn3_kernel) + fcn3_bias  # [B, 1]\n","        # -- mlp end -------\n","\n","        logits = tf.reshape(fcn_layer_3, [-1])  # [B]\n","        self.scores = tf.sigmoid(logits)  # [B]\n","\n","        # return same dimension as input tensors, let x = logits, z = labels, z * -log(sigmoid(x)) + (1 - z) * -log(1 - sigmoid(x))\n","        self.losses = tf.nn.sigmoid_cross_entropy_with_logits(logits=logits, labels=self.y)\n","        self.loss = tf.reduce_mean(self.losses)\n","\n","        # base_optimizer\n","        if train_config['base_optimizer'] == 'adam':\n","            base_optimizer = tf.train.AdamOptimizer(learning_rate=self.base_lr)\n","        elif train_config['base_optimizer'] == 'rmsprop':\n","            base_optimizer = tf.train.RMSPropOptimizer(learning_rate=self.base_lr)\n","        else:\n","            base_optimizer = tf.train.GradientDescentOptimizer(learning_rate=self.base_lr)\n","\n","        # transfer_optimizer\n","        if train_config['transfer_optimizer'] == 'adam':\n","            transfer_optimizer = tf.train.AdamOptimizer(learning_rate=self.transfer_lr)\n","        elif train_config['transfer_optimizer'] == 'rmsprop':\n","            transfer_optimizer = tf.train.RMSPropOptimizer(learning_rate=self.transfer_lr)\n","        else:\n","            transfer_optimizer = tf.train.GradientDescentOptimizer(learning_rate=self.transfer_lr)\n","\n","        trainable_params = tf.trainable_variables()\n","        base_params = [v for v in trainable_params if 'transfer' not in v.name]\n","        transfer_params = [v for v in trainable_params if 'transfer' in v.name]\n","\n","        # update base model and transfer module\n","        update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n","        with tf.control_dependencies(update_ops):\n","            base_grads = tf.gradients(self.loss, base_params)  # return a list of gradients (A list of `sum(dy/dx)` for each x in `xs`)\n","            base_grads_tuples = zip(base_grads, base_params)\n","            self.train_base_op = base_optimizer.apply_gradients(base_grads_tuples)\n","\n","            transfer_grads = tf.gradients(self.loss, transfer_params)\n","            transfer_grads_tuples = zip(transfer_grads, transfer_params)\n","            with tf.variable_scope('transfer_opt'):\n","                self.train_transfer_op = transfer_optimizer.apply_gradients(transfer_grads_tuples)\n","\n","    def train_base(self, sess, batch):\n","        loss, _ = sess.run([self.loss, self.train_base_op], feed_dict={\n","            self.u: batch[0],\n","            self.i: batch[1],\n","            self.hist_i: batch[2],\n","            self.hist_len: batch[3],\n","            self.y: batch[4],\n","            self.base_lr: self.train_config['base_lr'],\n","        })\n","        return loss\n","\n","    def train_transfer(self, sess, batch):\n","        loss, _, = sess.run([self.loss, self.train_transfer_op], feed_dict={\n","            self.u: batch[0],\n","            self.i: batch[1],\n","            self.hist_i: batch[2],\n","            self.hist_len: batch[3],\n","            self.y: batch[4],\n","            self.transfer_lr: self.train_config['transfer_lr'],\n","        })\n","        return loss\n","\n","    def update(self, sess):\n","        if self.train_config['transfer_emb']:\n","            sess.run([self.user_emb_w_upd_op,\n","                      self.item_emb_w_upd_op])\n","        if self.train_config['transfer_mlp']:\n","            sess.run([self.fcn1_kernel_upd_op,\n","                      self.fcn1_bias_upd_op,\n","                      self.fcn2_kernel_upd_op,\n","                      self.fcn2_bias_upd_op,\n","                      self.fcn3_kernel_upd_op,\n","                      self.fcn3_bias_upd_op])\n","\n","    def inference(self, sess, batch):\n","        scores, losses = sess.run([self.scores, self.losses], feed_dict={\n","            self.u: batch[0],\n","            self.i: batch[1],\n","            self.hist_i: batch[2],\n","            self.hist_len: batch[3],\n","            self.y: batch[4],\n","        })\n","        return scores, losses"],"execution_count":51,"outputs":[]},{"cell_type":"code","metadata":{"id":"pd5O3RncV6_S","executionInfo":{"status":"ok","timestamp":1636623985215,"user_tz":-330,"elapsed":1390,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}}},"source":["def sml_sobazaar():\n","    # load data to df\n","    start_time = time.time()\n","\n","    load_path = osp.join(args.path_silver,'soba_4mth_2014_1neg_30seq_1.csv')\n","    data_df = pd.read_csv(load_path)\n","\n","    data_df['itemSeq'] = data_df['itemSeq'].fillna('')  # empty seq are NaN\n","    data_df['itemSeq'] = data_df['itemSeq'].apply(lambda x: [int(item) for item in x.split('#') if item != ''])\n","\n","    logger.info('Done loading data! time elapsed: {}'.format(time.strftime('%H:%M:%S', time.gmtime(time.time() - start_time))))\n","\n","    num_users = data_df['userId'].max() + 1\n","    num_items = data_df['itemId'].max() + 1\n","\n","    train_config = {'method': 'SML_by_period',\n","                    'dir_name': 'SML_emb&mlp_train11-23_test24-30_1epoch_1epoch',  # edit parameter to transfer, train test period, transfer number of epochs, base number of epochs\n","                    'pretrain_model': 'pretrain_train1-10_test11_10epoch_0.001',  # pretrained base model\n","                    'start_date': 20140901,  # overall train start date\n","                    'end_date': 20141231,  # overall train end date\n","                    'num_periods': 31,  # number of periods divided into\n","                    'train_start_period': 11,\n","                    'test_start_period': 24,\n","                    'cur_period': None,  # current incremental period\n","                    'next_period': None,  # next incremental period\n","                    'cur_set_size': None,  # current incremental dataset size\n","                    'next_set_size': None,  # next incremental dataset size\n","                    'period_alias': None,  # individual period directory alias to save ckpts\n","                    'restored_ckpt_mode': 'best auc',  # mode to search the ckpt to restore: 'best auc', 'best logloss', 'last'\n","                    'restored_ckpt': None,  # restored sml model checkpoint\n","\n","                    'transfer_emb': True,\n","                    'emb_n1': 10,\n","                    'emb_n2': 5,\n","                    'emb_l1': 20,\n","                    'transfer_mlp': True,\n","                    'mlp_n1': 5,\n","                    'mlp_n2': 3,\n","                    'mlp_l1_dict': {'fcn1/kernel': 40,\n","                                    'fcn1/bias': 20,\n","                                    'fcn2/kernel': 20,\n","                                    'fcn2/bias': 10,\n","                                    'fcn3/kernel': 10,\n","                                    'fcn3/bias': 1},\n","\n","                    'transfer_optimizer': 'adam',  # transfer module optimizer: adam, rmsprop, sgd\n","                    'transfer_lr': None,  # transfer module learning rate\n","                    'transfer_bs': 256,  # transfer module batch size\n","                    'transfer_num_epochs': 1,  # transfer module number of epochs\n","                    'test_stop_train': False,  # whether to stop updating transfer module during test periods\n","\n","                    'base_optimizer': 'adam',  # base model optimizer: adam, rmsprop, sgd\n","                    'base_lr': None,  # base model learning rate\n","                    'base_bs': 256,  # base model batch size\n","                    'base_num_epochs': 1,  # base model number of epochs\n","                    'shuffle': True,  # whether to shuffle the dataset for each epoch\n","                    }\n","\n","    EmbMLPnocate_hyperparams = {'num_users': num_users,\n","                                'num_items': num_items,\n","                                'user_embed_dim': 8,\n","                                'item_embed_dim': 8,\n","                                'layers': [24, 12, 6, 1]  # input dim is user_embed_dim + item_embed_dim x 2\n","                                }\n","\n","    # sort train data into periods based on num_periods\n","    data_df = data_df[(data_df['date'] >= train_config['start_date']) & (data_df['date'] <= train_config['end_date'])]\n","    data_df = data_df.sort_values(['timestamp']).reset_index(drop=True)\n","    records_per_period = int(len(data_df) / train_config['num_periods'])\n","    data_df['index'] = data_df.index\n","    data_df['period'] = data_df['index'].apply(lambda x: int(x / records_per_period) + 1)\n","    data_df = data_df[data_df.period != train_config['num_periods'] + 1]  # delete last extra period\n","    period_df = data_df.groupby('period')['date'].agg(['count', 'min', 'max'])\n","    data_df = data_df.drop(['index', 'date', 'timestamp'], axis=1)\n","\n","    orig_dir_name = train_config['dir_name']\n","\n","    for transfer_lr in [1e-3]:\n","\n","        for base_lr in [1e-2]:\n","\n","            print('')\n","            print('transfer_lr', transfer_lr, 'base_lr', base_lr)\n","\n","            train_config['transfer_lr'] = transfer_lr\n","            train_config['base_lr'] = base_lr\n","\n","            train_config['dir_name'] = orig_dir_name + '_' + str(transfer_lr) + '_' + str(base_lr)\n","            print('dir_name: ', train_config['dir_name'])\n","\n","            test_aucs = []\n","            test_loglosses = []\n","\n","            for i in range(train_config['train_start_period'], train_config['num_periods']):\n","\n","                # configure cur_period, next_period\n","                train_config['cur_period'] = i\n","                train_config['next_period'] = i + 1\n","                print('')\n","                print('current period: {}, next period: {}'.format(\n","                    train_config['cur_period'],\n","                    train_config['next_period']))\n","                print('')\n","\n","                # create current and next set\n","                cur_set = data_df[data_df['period'] == train_config['cur_period']]\n","                next_set = data_df[data_df['period'] == train_config['next_period']]\n","                train_config['cur_set_size'] = len(cur_set)\n","                train_config['next_set_size'] = len(next_set)\n","                print('current set size', len(cur_set), 'next set size', len(next_set))\n","\n","                train_config['period_alias'] = 'period' + str(i)\n","\n","                # checkpoints directory\n","                ckpts_dir = os.path.join('ckpts', train_config['dir_name'], train_config['period_alias'])\n","                if not os.path.exists(ckpts_dir):\n","                    os.makedirs(ckpts_dir)\n","\n","                if i == train_config['train_start_period']:\n","                    search_alias = os.path.join('ckpts', train_config['pretrain_model'], 'Epoch*')\n","                    train_config['restored_ckpt'] = search_ckpt(search_alias, mode=train_config['restored_ckpt_mode'])\n","                else:\n","                    prev_period_alias = 'period' + str(i - 1)\n","                    search_alias = os.path.join('ckpts', train_config['dir_name'], prev_period_alias, 'Epoch*')\n","                    train_config['restored_ckpt'] = search_ckpt(search_alias, mode=train_config['restored_ckpt_mode'])\n","                print('restored checkpoint: {}'.format(train_config['restored_ckpt']))\n","\n","                # write train_config to text file\n","                with open(os.path.join(ckpts_dir, 'config.txt'), mode='w') as f:\n","                    f.write('train_config: ' + str(train_config) + '\\n')\n","                    f.write('\\n')\n","                    f.write('EmbMLPnocate_hyperparams: ' + str(EmbMLPnocate_hyperparams) + '\\n')\n","\n","                def collect_params():\n","                    \"\"\"\n","                    collect previous period model parameters\n","                    :return: prev_emb_dict, prev_mlp_dict\n","                    \"\"\"\n","                    collect_params_start_time = time.time()\n","                    emb_ls = ['user_emb_w', 'item_emb_w']\n","                    mlp_ls = ['fcn1/kernel', 'fcn2/kernel', 'fcn3/kernel', 'fcn3/bias', 'fcn1/bias', 'fcn2/bias']\n","                    prev_emb_dict_ = {name: tf.train.load_checkpoint(train_config['restored_ckpt']).get_tensor(name)\n","                                    for name, _ in tf.train.list_variables(train_config['restored_ckpt']) if name in emb_ls}\n","                    prev_mlp_dict_ = {name: tf.train.load_checkpoint(train_config['restored_ckpt']).get_tensor(name)\n","                                    for name, _ in tf.train.list_variables(train_config['restored_ckpt']) if name in mlp_ls}\n","                    print('collect params time elapsed: {}'.format(\n","                        time.strftime('%H:%M:%S', time.gmtime(time.time() - collect_params_start_time))))\n","                    return prev_emb_dict_, prev_mlp_dict_\n","\n","                # collect previous period model parameters\n","                prev_emb_dict, prev_mlp_dict = collect_params()\n","\n","                # build sml model computation graph\n","                tf.reset_default_graph()\n","                sml_model = SML(EmbMLPnocate_hyperparams, prev_emb_dict, prev_mlp_dict, train_config=train_config)\n","\n","                # create session\n","                with tf.Session() as sess:\n","\n","                    def train_base():\n","                        # create an engine instance with sml_model\n","                        engine = SMLEngine(sess, sml_model)\n","                        train_start_time = time.time()\n","                        max_auc = 0\n","                        best_logloss = 0\n","                        for epoch_id in range(1, train_config['base_num_epochs'] + 1):\n","                            print('Training Base Model Epoch {} Start!'.format(epoch_id))\n","                            base_loss_cur_avg = engine.base_train_an_epoch(epoch_id, cur_set, train_config)\n","                            print('Epoch {} Done! time elapsed: {}, base_loss_cur_avg {:.4f}'.format(\n","                                epoch_id,\n","                                time.strftime('%H:%M:%S', time.gmtime(time.time() - train_start_time)),\n","                                base_loss_cur_avg))\n","                            cur_auc, cur_logloss = engine.test(cur_set, train_config)\n","                            next_auc, next_logloss = engine.test(next_set, train_config)\n","                            print('cur_auc {:.4f}, cur_logloss {:.4f}, next_auc {:.4f}, next_logloss {:.4f}'.format(\n","                                cur_auc,\n","                                cur_logloss,\n","                                next_auc,\n","                                next_logloss))\n","                            print('time elapsed {}'.format(time.strftime('%H:%M:%S', time.gmtime(time.time() - train_start_time))))\n","                            print('')\n","                            # save checkpoint\n","                            if i >= train_config['test_start_period'] and train_config['test_stop_train']:\n","                                checkpoint_alias = 'Epoch{}_TestAUC{:.4f}_TestLOGLOSS{:.4f}.ckpt'.format(\n","                                    epoch_id,\n","                                    next_auc,\n","                                    next_logloss)\n","                                checkpoint_path = os.path.join(ckpts_dir, checkpoint_alias)\n","                                saver.save(sess, checkpoint_path)\n","                            if next_auc > max_auc:\n","                                max_auc = next_auc\n","                                best_logloss = next_logloss\n","                        if i >= train_config['test_start_period']:\n","                            test_aucs.append(max_auc)\n","                            test_loglosses.append(best_logloss)\n","\n","                    def train_transfer():\n","                        # create an engine instance with sml_model\n","                        engine = SMLEngine(sess, sml_model)\n","                        train_start_time = time.time()\n","                        for epoch_id in range(1, train_config['transfer_num_epochs'] + 1):\n","                            print('Training Transfer Module Epoch {} Start!'.format(epoch_id))\n","                            transfer_loss_next_avg = engine.transfer_train_an_epoch(epoch_id, next_set, train_config)\n","                            print('Epoch {} Done! time elapsed: {}, transfer_loss_next_avg {:.4f}'.format(\n","                                epoch_id,\n","                                time.strftime('%H:%M:%S', time.gmtime(time.time() - train_start_time)),\n","                                transfer_loss_next_avg))\n","                            cur_auc, cur_logloss = engine.test(cur_set, train_config)\n","                            next_auc, next_logloss = engine.test(next_set, train_config)\n","                            print('cur_auc {:.4f}, cur_logloss {:.4f}, next_auc {:.4f}, next_logloss {:.4f}'.format(\n","                                cur_auc,\n","                                cur_logloss,\n","                                next_auc,\n","                                next_logloss))\n","                            print('time elapsed {}'.format(time.strftime('%H:%M:%S', time.gmtime(time.time() - train_start_time))))\n","                            print('')\n","                            # update transferred params\n","                            sml_model.update(sess)\n","                            # save checkpoint\n","                            checkpoint_alias = 'Epoch{}_TestAUC{:.4f}_TestLOGLOSS{:.4f}.ckpt'.format(\n","                                epoch_id,\n","                                next_auc,\n","                                next_logloss)\n","                            checkpoint_path = os.path.join(ckpts_dir, checkpoint_alias)\n","                            saver.save(sess, checkpoint_path)\n","\n","                    # restore sml model (transfer module and base model)\n","                    if i == train_config['train_start_period']:\n","                        sess.run([tf.global_variables_initializer(), tf.local_variables_initializer()])  # initialize transfer module\n","                        restorer = tf.train.Saver(var_list=[v for v in tf.global_variables() if 'transfer' not in v.name])  # restore base model\n","                        restorer.restore(sess, train_config['restored_ckpt'])\n","                    else:\n","                        restorer = tf.train.Saver()  # restore transfer module and base model\n","                        restorer.restore(sess, train_config['restored_ckpt'])\n","                    saver = tf.train.Saver()\n","\n","                    # test transfer module by training base model with it\n","                    train_base()\n","\n","                    # train transfer module\n","                    if i < train_config['test_start_period'] or not train_config['test_stop_train']:\n","                        train_transfer()\n","\n","                if i >= train_config['test_start_period']:\n","                    average_auc = sum(test_aucs) / len(test_aucs)\n","                    average_logloss = sum(test_loglosses) / len(test_loglosses)\n","                    print('test aucs', test_aucs)\n","                    print('average auc', average_auc)\n","                    print('')\n","                    print('test loglosses', test_loglosses)\n","                    print('average logloss', average_logloss)\n","\n","                    # write metrics to text file\n","                    with open(os.path.join(ckpts_dir, 'test_metrics.txt'), mode='w') as f:\n","                        f.write('test_aucs: ' + str(test_aucs) + '\\n')\n","                        f.write('average_auc: ' + str(average_auc) + '\\n')\n","                        f.write('test_loglosses: ' + str(test_loglosses) + '\\n')\n","                        f.write('average_logloss: ' + str(average_logloss) + '\\n')"],"execution_count":56,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"P0nc1kyjzX9T"},"source":["## Jobs"]},{"cell_type":"code","metadata":{"id":"2y8mdDjds6dr","colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["83c9f2d6bc6a42109250d55a39b4b3e7","6af9c2d14cd344408ebeb165aecd5812","14a3c2eab70348fd8336ae11dbbfcffd","1f7c997368b34aedb5ecafc1ad6b1121","712fb93cec584db29a18f17f48ce85dc","6149942fe6ad4e0ea119545de5a8d242","f4349bb599ab413c9c087714eb7ba11f","7b90eb42b83641b4890847c92356a8f6","8d3fb8b1ab5742ec983d41c822319729","27e8cfe72e7a4a93b1f2e1f705a551a5","74ce21ef75514a30a12822ebc5165352","30c470ccdef34a2999ab74aad3503ba7","7ef1f03aed684fdbbc6602ec174cd2cd","45c6daf562174f79ad42975093c84210","d3fc9033cc8f45c9a9bc114a6c590fd9","53e9c4c383124944a89e833d7f0a56df","e69ef6e531314efe9a9ee6dc58e5998b","813103a929a84c78aa67e72419563cb7","0fa28afc080a4e98a0f5ccd7d083e3b3","60fe4254494e4bc589c971c5e287ec88","bb7538a3fc0045c28b51edc7f7cbbc00","d30f88908fe54f779659b41bd1e5ba6f"]},"collapsed":true,"executionInfo":{"status":"ok","timestamp":1636550822729,"user_tz":-330,"elapsed":2141725,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}},"outputId":"af2586b2-0874-4753-f5f5-7da803ca06aa"},"source":["# logger.info('JOB START: PREPROCESS_DATASET')\n","# preprocess_sobazaar()\n","# logger.info('JOB END: PREPROCESS_DATASET')"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["10-Nov-21 12:51:22 [INFO] : JOB START: PREPROCESS_DATASET\n","10-Nov-21 12:51:37 [INFO] : dataframe head -     userId  itemId      date   timestamp\n","0     3192   14808  20140901  1409596736\n","1     3192   14808  20140901  1409596739\n","2     3192   18402  20140901  1409596746\n","3    13749    4020  20140901  1409596746\n","4     9380    5935  20140901  1409596753\n","5     3192   18402  20140901  1409596756\n","6    11381    1603  20140901  1409596756\n","7    11381   20984  20140901  1409596762\n","8     6149    6092  20140901  1409596766\n","9    11381    1204  20140901  1409596772\n","10   11381    1204  20140901  1409596774\n","11    9380    4576  20140901  1409596774\n","12   11381    1204  20140901  1409596774\n","13    3192   11779  20140901  1409596789\n","14   10241    9656  20140901  1409596795\n","15    9380   18238  20140901  1409596813\n","16   10241   16933  20140901  1409596816\n","17   13749    3756  20140901  1409596820\n","18   13749    3756  20140901  1409596822\n","19    6149   20603  20140901  1409596825\n","10-Nov-21 12:51:37 [INFO] : num_users: 17126\n","10-Nov-21 12:51:37 [INFO] : num_items: 24785\n","10-Nov-21 12:51:37 [INFO] : num_records: 842660\n"]},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"83c9f2d6bc6a42109250d55a39b4b3e7","version_minor":0,"version_major":2},"text/plain":["  0%|          | 0/842660 [00:00<?, ?it/s]"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["10-Nov-21 12:51:49 [INFO] : NumExpr defaulting to 2 threads.\n","10-Nov-21 12:55:52 [INFO] : done row 100000\n","10-Nov-21 12:59:54 [INFO] : done row 200000\n","10-Nov-21 13:03:55 [INFO] : done row 300000\n","10-Nov-21 13:07:59 [INFO] : done row 400000\n","10-Nov-21 13:12:17 [INFO] : done row 500000\n","10-Nov-21 13:16:34 [INFO] : done row 600000\n","10-Nov-21 13:20:47 [INFO] : done row 700000\n","10-Nov-21 13:25:00 [INFO] : done row 800000\n"]},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"30c470ccdef34a2999ab74aad3503ba7","version_minor":0,"version_major":2},"text/plain":["  0%|          | 0/842660 [00:00<?, ?it/s]"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["10-Nov-21 13:26:54 [INFO] : dataframe head -     userId  ...   timestamp\n","0     3192  ...  1409596736\n","1     3192  ...  1409596736\n","2     3192  ...  1409596739\n","3     3192  ...  1409596739\n","4     3192  ...  1409596746\n","5     3192  ...  1409596746\n","6    13749  ...  1409596746\n","7    13749  ...  1409596746\n","8     9380  ...  1409596753\n","9     9380  ...  1409596753\n","10    3192  ...  1409596756\n","11    3192  ...  1409596756\n","12   11381  ...  1409596756\n","13   11381  ...  1409596756\n","14   11381  ...  1409596762\n","15   11381  ...  1409596762\n","16    6149  ...  1409596766\n","17    6149  ...  1409596766\n","18   11381  ...  1409596772\n","19   11381  ...  1409596772\n","\n","[20 rows x 6 columns]\n","10-Nov-21 13:26:54 [INFO] : 1685320\n","10-Nov-21 13:27:03 [INFO] : processed data saved at /content/soba_4mth_2014_1neg_30seq_1.csv\n","10-Nov-21 13:27:03 [INFO] : JOB END: PREPROCESS_DATASET\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0A3Bwm7DHyc4","executionInfo":{"status":"ok","timestamp":1636620685985,"user_tz":-330,"elapsed":11366,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}},"outputId":"549d6d2a-20b2-482c-fb65-6379ccb8a12e"},"source":["logger.info('JOB START: CONVERT_PARQUET_TO_CSV')\n","parquet_to_csv('/content/soba_4mth_2014_1neg_30seq_1.parquet.snappy')\n","logger.info('JOB END: CONVERT_PARQUET_TO_CSV')"],"execution_count":23,"outputs":[{"output_type":"stream","name":"stdout","text":["11-Nov-21 08:51:16 [INFO] : JOB START: CONVERT_PARQUET_TO_CSV\n","11-Nov-21 08:51:26 [INFO] : csv file saved at /content/soba_4mth_2014_1neg_30seq_1.csv\n","11-Nov-21 08:51:26 [INFO] : JOB END: CONVERT_PARQUET_TO_CSV\n"]}]},{"cell_type":"code","metadata":{"id":"3ig3tPpB2Fx-","colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"executionInfo":{"status":"ok","timestamp":1636620920481,"user_tz":-330,"elapsed":231988,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}},"outputId":"3a50d93c-353a-489c-8a68-682a72717f36"},"source":["logger.info('JOB START: EMBEDMLP_PRETRAINING')\n","pretrain_sobazaar()\n","logger.info('JOB END: EMBEDMLP_PRETRAINING')"],"execution_count":24,"outputs":[{"output_type":"stream","name":"stdout","text":["11-Nov-21 08:51:29 [INFO] : JOB START: EMBEDMLP_PRETRAINING\n","11-Nov-21 08:51:49 [INFO] : Done loading data! time elapsed: 00:00:20\n","11-Nov-21 08:51:49 [INFO] : NumExpr defaulting to 2 threads.\n","WARNING:tensorflow:From <ipython-input-11-dafab32e42db>:6: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Deprecated in favor of operator or tf.math.divide.\n","11-Nov-21 08:51:51 [WARNING] : From <ipython-input-11-dafab32e42db>:6: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Deprecated in favor of operator or tf.math.divide.\n","WARNING:tensorflow:From /tensorflow-1.15.2/python3.7/tensorflow_core/python/ops/nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use tf.where in 2.0, which has the same broadcast rule as np.where\n","11-Nov-21 08:51:51 [WARNING] : From /tensorflow-1.15.2/python3.7/tensorflow_core/python/ops/nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use tf.where in 2.0, which has the same broadcast rule as np.where\n","\n","base_lr 0.001\n","dir_name:  pretrain_train1-10_test11_10epoch_0.001\n","train set size 543650 test set size 54365\n","Training Base Model Epoch 1 Start!\n","[Epoch 1 Batch 1] base_loss_cur 0.6939, time elapsed 00:00:02\n","[Epoch 1 Batch 101] base_loss_cur 0.6703, time elapsed 00:00:02\n","[Epoch 1 Batch 201] base_loss_cur 0.5353, time elapsed 00:00:03\n","[Epoch 1 Batch 301] base_loss_cur 0.4332, time elapsed 00:00:04\n","[Epoch 1 Batch 401] base_loss_cur 0.4005, time elapsed 00:00:05\n","[Epoch 1 Batch 501] base_loss_cur 0.3474, time elapsed 00:00:06\n","[Epoch 1 Batch 601] base_loss_cur 0.3893, time elapsed 00:00:07\n","[Epoch 1 Batch 701] base_loss_cur 0.3396, time elapsed 00:00:07\n","[Epoch 1 Batch 801] base_loss_cur 0.3628, time elapsed 00:00:08\n","[Epoch 1 Batch 901] base_loss_cur 0.4041, time elapsed 00:00:09\n","[Epoch 1 Batch 1001] base_loss_cur 0.3448, time elapsed 00:00:10\n","[Epoch 1 Batch 1101] base_loss_cur 0.3669, time elapsed 00:00:11\n","[Epoch 1 Batch 1201] base_loss_cur 0.3555, time elapsed 00:00:12\n","[Epoch 1 Batch 1301] base_loss_cur 0.3406, time elapsed 00:00:13\n","[Epoch 1 Batch 1401] base_loss_cur 0.3405, time elapsed 00:00:13\n","[Epoch 1 Batch 1501] base_loss_cur 0.4251, time elapsed 00:00:14\n","[Epoch 1 Batch 1601] base_loss_cur 0.3260, time elapsed 00:00:15\n","[Epoch 1 Batch 1701] base_loss_cur 0.3318, time elapsed 00:00:16\n","[Epoch 1 Batch 1801] base_loss_cur 0.3288, time elapsed 00:00:17\n","[Epoch 1 Batch 1901] base_loss_cur 0.4090, time elapsed 00:00:18\n","[Epoch 1 Batch 2001] base_loss_cur 0.3260, time elapsed 00:00:19\n","[Epoch 1 Batch 2101] base_loss_cur 0.3462, time elapsed 00:00:19\n","Epoch 1 Done! time elapsed: 00:00:20, base_loss_cur_avg 0.3936\n","test_auc 0.8528, test_logloss 0.5142\n","time elapsed 00:00:21\n","\n","Training Base Model Epoch 2 Start!\n","[Epoch 2 Batch 1] base_loss_cur 0.3243, time elapsed 00:00:00\n","[Epoch 2 Batch 101] base_loss_cur 0.3127, time elapsed 00:00:01\n","[Epoch 2 Batch 201] base_loss_cur 0.3547, time elapsed 00:00:02\n","[Epoch 2 Batch 301] base_loss_cur 0.3064, time elapsed 00:00:03\n","[Epoch 2 Batch 401] base_loss_cur 0.3165, time elapsed 00:00:03\n","[Epoch 2 Batch 501] base_loss_cur 0.3262, time elapsed 00:00:04\n","[Epoch 2 Batch 601] base_loss_cur 0.3763, time elapsed 00:00:05\n","[Epoch 2 Batch 701] base_loss_cur 0.3056, time elapsed 00:00:06\n","[Epoch 2 Batch 801] base_loss_cur 0.2880, time elapsed 00:00:07\n","[Epoch 2 Batch 901] base_loss_cur 0.3219, time elapsed 00:00:08\n","[Epoch 2 Batch 1001] base_loss_cur 0.2759, time elapsed 00:00:08\n","[Epoch 2 Batch 1101] base_loss_cur 0.3201, time elapsed 00:00:09\n","[Epoch 2 Batch 1201] base_loss_cur 0.3337, time elapsed 00:00:10\n","[Epoch 2 Batch 1301] base_loss_cur 0.3145, time elapsed 00:00:11\n","[Epoch 2 Batch 1401] base_loss_cur 0.3682, time elapsed 00:00:12\n","[Epoch 2 Batch 1501] base_loss_cur 0.3286, time elapsed 00:00:13\n","[Epoch 2 Batch 1601] base_loss_cur 0.3164, time elapsed 00:00:14\n","[Epoch 2 Batch 1701] base_loss_cur 0.3597, time elapsed 00:00:14\n","[Epoch 2 Batch 1801] base_loss_cur 0.3571, time elapsed 00:00:15\n","[Epoch 2 Batch 1901] base_loss_cur 0.3071, time elapsed 00:00:16\n","[Epoch 2 Batch 2001] base_loss_cur 0.2966, time elapsed 00:00:17\n","[Epoch 2 Batch 2101] base_loss_cur 0.2883, time elapsed 00:00:18\n","Epoch 2 Done! time elapsed: 00:00:40, base_loss_cur_avg 0.3185\n","test_auc 0.8535, test_logloss 0.5567\n","time elapsed 00:00:41\n","\n","Training Base Model Epoch 3 Start!\n","[Epoch 3 Batch 1] base_loss_cur 0.2297, time elapsed 00:00:00\n","[Epoch 3 Batch 101] base_loss_cur 0.2426, time elapsed 00:00:01\n","[Epoch 3 Batch 201] base_loss_cur 0.2801, time elapsed 00:00:02\n","[Epoch 3 Batch 301] base_loss_cur 0.3203, time elapsed 00:00:03\n","[Epoch 3 Batch 401] base_loss_cur 0.3332, time elapsed 00:00:03\n","[Epoch 3 Batch 501] base_loss_cur 0.3419, time elapsed 00:00:04\n","[Epoch 3 Batch 601] base_loss_cur 0.2740, time elapsed 00:00:05\n","[Epoch 3 Batch 701] base_loss_cur 0.3079, time elapsed 00:00:06\n","[Epoch 3 Batch 801] base_loss_cur 0.3384, time elapsed 00:00:07\n","[Epoch 3 Batch 901] base_loss_cur 0.3308, time elapsed 00:00:08\n","[Epoch 3 Batch 1001] base_loss_cur 0.2447, time elapsed 00:00:08\n","[Epoch 3 Batch 1101] base_loss_cur 0.3366, time elapsed 00:00:09\n","[Epoch 3 Batch 1201] base_loss_cur 0.2559, time elapsed 00:00:10\n","[Epoch 3 Batch 1301] base_loss_cur 0.2109, time elapsed 00:00:11\n","[Epoch 3 Batch 1401] base_loss_cur 0.2451, time elapsed 00:00:12\n","[Epoch 3 Batch 1501] base_loss_cur 0.2806, time elapsed 00:00:13\n","[Epoch 3 Batch 1601] base_loss_cur 0.3383, time elapsed 00:00:14\n","[Epoch 3 Batch 1701] base_loss_cur 0.3422, time elapsed 00:00:14\n","[Epoch 3 Batch 1801] base_loss_cur 0.3411, time elapsed 00:00:15\n","[Epoch 3 Batch 1901] base_loss_cur 0.2903, time elapsed 00:00:16\n","[Epoch 3 Batch 2001] base_loss_cur 0.2936, time elapsed 00:00:17\n","[Epoch 3 Batch 2101] base_loss_cur 0.2989, time elapsed 00:00:18\n","Epoch 3 Done! time elapsed: 00:00:59, base_loss_cur_avg 0.3067\n","test_auc 0.8508, test_logloss 0.5885\n","time elapsed 00:01:01\n","\n","Training Base Model Epoch 4 Start!\n","[Epoch 4 Batch 1] base_loss_cur 0.2528, time elapsed 00:00:00\n","[Epoch 4 Batch 101] base_loss_cur 0.2795, time elapsed 00:00:01\n","[Epoch 4 Batch 201] base_loss_cur 0.3284, time elapsed 00:00:02\n","[Epoch 4 Batch 301] base_loss_cur 0.2934, time elapsed 00:00:03\n","[Epoch 4 Batch 401] base_loss_cur 0.2742, time elapsed 00:00:03\n","[Epoch 4 Batch 501] base_loss_cur 0.2916, time elapsed 00:00:04\n","[Epoch 4 Batch 601] base_loss_cur 0.2705, time elapsed 00:00:05\n","[Epoch 4 Batch 701] base_loss_cur 0.2780, time elapsed 00:00:06\n","[Epoch 4 Batch 801] base_loss_cur 0.3400, time elapsed 00:00:07\n","[Epoch 4 Batch 901] base_loss_cur 0.2782, time elapsed 00:00:08\n","[Epoch 4 Batch 1001] base_loss_cur 0.2787, time elapsed 00:00:09\n","[Epoch 4 Batch 1101] base_loss_cur 0.2652, time elapsed 00:00:09\n","[Epoch 4 Batch 1201] base_loss_cur 0.2965, time elapsed 00:00:10\n","[Epoch 4 Batch 1301] base_loss_cur 0.2610, time elapsed 00:00:11\n","[Epoch 4 Batch 1401] base_loss_cur 0.2993, time elapsed 00:00:12\n","[Epoch 4 Batch 1501] base_loss_cur 0.2811, time elapsed 00:00:13\n","[Epoch 4 Batch 1601] base_loss_cur 0.2814, time elapsed 00:00:14\n","[Epoch 4 Batch 1701] base_loss_cur 0.2698, time elapsed 00:00:14\n","[Epoch 4 Batch 1801] base_loss_cur 0.3055, time elapsed 00:00:15\n","[Epoch 4 Batch 1901] base_loss_cur 0.2286, time elapsed 00:00:16\n","[Epoch 4 Batch 2001] base_loss_cur 0.2646, time elapsed 00:00:17\n","[Epoch 4 Batch 2101] base_loss_cur 0.3039, time elapsed 00:00:18\n","Epoch 4 Done! time elapsed: 00:01:19, base_loss_cur_avg 0.2962\n","test_auc 0.8497, test_logloss 0.6384\n","time elapsed 00:01:22\n","\n","Training Base Model Epoch 5 Start!\n","[Epoch 5 Batch 1] base_loss_cur 0.2720, time elapsed 00:00:00\n","[Epoch 5 Batch 101] base_loss_cur 0.2973, time elapsed 00:00:01\n","[Epoch 5 Batch 201] base_loss_cur 0.2413, time elapsed 00:00:02\n","[Epoch 5 Batch 301] base_loss_cur 0.2520, time elapsed 00:00:03\n","[Epoch 5 Batch 401] base_loss_cur 0.3262, time elapsed 00:00:03\n","[Epoch 5 Batch 501] base_loss_cur 0.3188, time elapsed 00:00:04\n","[Epoch 5 Batch 601] base_loss_cur 0.2682, time elapsed 00:00:05\n","[Epoch 5 Batch 701] base_loss_cur 0.2722, time elapsed 00:00:06\n","[Epoch 5 Batch 801] base_loss_cur 0.2888, time elapsed 00:00:07\n","[Epoch 5 Batch 901] base_loss_cur 0.2457, time elapsed 00:00:08\n","[Epoch 5 Batch 1001] base_loss_cur 0.2701, time elapsed 00:00:09\n","[Epoch 5 Batch 1101] base_loss_cur 0.2657, time elapsed 00:00:09\n","[Epoch 5 Batch 1201] base_loss_cur 0.2984, time elapsed 00:00:10\n","[Epoch 5 Batch 1301] base_loss_cur 0.3744, time elapsed 00:00:11\n","[Epoch 5 Batch 1401] base_loss_cur 0.2712, time elapsed 00:00:12\n","[Epoch 5 Batch 1501] base_loss_cur 0.2615, time elapsed 00:00:13\n","[Epoch 5 Batch 1601] base_loss_cur 0.2942, time elapsed 00:00:14\n","[Epoch 5 Batch 1701] base_loss_cur 0.2866, time elapsed 00:00:15\n","[Epoch 5 Batch 1801] base_loss_cur 0.2733, time elapsed 00:00:15\n","[Epoch 5 Batch 1901] base_loss_cur 0.3323, time elapsed 00:00:16\n","[Epoch 5 Batch 2001] base_loss_cur 0.3173, time elapsed 00:00:17\n","[Epoch 5 Batch 2101] base_loss_cur 0.2494, time elapsed 00:00:18\n","Epoch 5 Done! time elapsed: 00:01:40, base_loss_cur_avg 0.2846\n","test_auc 0.8466, test_logloss 0.7079\n","time elapsed 00:01:42\n","\n","Training Base Model Epoch 6 Start!\n","[Epoch 6 Batch 1] base_loss_cur 0.3014, time elapsed 00:00:00\n","[Epoch 6 Batch 101] base_loss_cur 0.2717, time elapsed 00:00:01\n","[Epoch 6 Batch 201] base_loss_cur 0.2761, time elapsed 00:00:02\n","[Epoch 6 Batch 301] base_loss_cur 0.2497, time elapsed 00:00:03\n","[Epoch 6 Batch 401] base_loss_cur 0.2603, time elapsed 00:00:03\n","[Epoch 6 Batch 501] base_loss_cur 0.2544, time elapsed 00:00:04\n","[Epoch 6 Batch 601] base_loss_cur 0.3015, time elapsed 00:00:05\n","[Epoch 6 Batch 701] base_loss_cur 0.3097, time elapsed 00:00:06\n","[Epoch 6 Batch 801] base_loss_cur 0.2273, time elapsed 00:00:07\n","[Epoch 6 Batch 901] base_loss_cur 0.2591, time elapsed 00:00:08\n","[Epoch 6 Batch 1001] base_loss_cur 0.2897, time elapsed 00:00:09\n","[Epoch 6 Batch 1101] base_loss_cur 0.2223, time elapsed 00:00:09\n","[Epoch 6 Batch 1201] base_loss_cur 0.2968, time elapsed 00:00:10\n","[Epoch 6 Batch 1301] base_loss_cur 0.2344, time elapsed 00:00:11\n","[Epoch 6 Batch 1401] base_loss_cur 0.2667, time elapsed 00:00:12\n","[Epoch 6 Batch 1501] base_loss_cur 0.2528, time elapsed 00:00:13\n","[Epoch 6 Batch 1601] base_loss_cur 0.2944, time elapsed 00:00:14\n","[Epoch 6 Batch 1701] base_loss_cur 0.2863, time elapsed 00:00:15\n","[Epoch 6 Batch 1801] base_loss_cur 0.2708, time elapsed 00:00:15\n","[Epoch 6 Batch 1901] base_loss_cur 0.3139, time elapsed 00:00:16\n","[Epoch 6 Batch 2001] base_loss_cur 0.2636, time elapsed 00:00:17\n","[Epoch 6 Batch 2101] base_loss_cur 0.2210, time elapsed 00:00:18\n","Epoch 6 Done! time elapsed: 00:02:00, base_loss_cur_avg 0.2722\n","test_auc 0.8435, test_logloss 0.7923\n","time elapsed 00:02:02\n","\n","Training Base Model Epoch 7 Start!\n","[Epoch 7 Batch 1] base_loss_cur 0.2293, time elapsed 00:00:00\n","[Epoch 7 Batch 101] base_loss_cur 0.1941, time elapsed 00:00:01\n","[Epoch 7 Batch 201] base_loss_cur 0.2905, time elapsed 00:00:02\n","[Epoch 7 Batch 301] base_loss_cur 0.2283, time elapsed 00:00:03\n","[Epoch 7 Batch 401] base_loss_cur 0.2692, time elapsed 00:00:03\n","[Epoch 7 Batch 501] base_loss_cur 0.2346, time elapsed 00:00:04\n","[Epoch 7 Batch 601] base_loss_cur 0.2098, time elapsed 00:00:05\n","[Epoch 7 Batch 701] base_loss_cur 0.2128, time elapsed 00:00:06\n","[Epoch 7 Batch 801] base_loss_cur 0.2176, time elapsed 00:00:07\n","[Epoch 7 Batch 901] base_loss_cur 0.2637, time elapsed 00:00:08\n","[Epoch 7 Batch 1001] base_loss_cur 0.3040, time elapsed 00:00:08\n","[Epoch 7 Batch 1101] base_loss_cur 0.2305, time elapsed 00:00:09\n","[Epoch 7 Batch 1201] base_loss_cur 0.2393, time elapsed 00:00:10\n","[Epoch 7 Batch 1301] base_loss_cur 0.2599, time elapsed 00:00:11\n","[Epoch 7 Batch 1401] base_loss_cur 0.2929, time elapsed 00:00:12\n","[Epoch 7 Batch 1501] base_loss_cur 0.2653, time elapsed 00:00:13\n","[Epoch 7 Batch 1601] base_loss_cur 0.3084, time elapsed 00:00:14\n","[Epoch 7 Batch 1701] base_loss_cur 0.2768, time elapsed 00:00:14\n","[Epoch 7 Batch 1801] base_loss_cur 0.2338, time elapsed 00:00:15\n","[Epoch 7 Batch 1901] base_loss_cur 0.2200, time elapsed 00:00:16\n","[Epoch 7 Batch 2001] base_loss_cur 0.3261, time elapsed 00:00:17\n","[Epoch 7 Batch 2101] base_loss_cur 0.2202, time elapsed 00:00:18\n","Epoch 7 Done! time elapsed: 00:02:20, base_loss_cur_avg 0.2597\n","test_auc 0.8402, test_logloss 0.8806\n","time elapsed 00:02:21\n","\n","Training Base Model Epoch 8 Start!\n","[Epoch 8 Batch 1] base_loss_cur 0.2258, time elapsed 00:00:00\n","[Epoch 8 Batch 101] base_loss_cur 0.2806, time elapsed 00:00:01\n","[Epoch 8 Batch 201] base_loss_cur 0.2187, time elapsed 00:00:02\n","[Epoch 8 Batch 301] base_loss_cur 0.2538, time elapsed 00:00:03\n","[Epoch 8 Batch 401] base_loss_cur 0.2543, time elapsed 00:00:03\n","[Epoch 8 Batch 501] base_loss_cur 0.2795, time elapsed 00:00:04\n","[Epoch 8 Batch 601] base_loss_cur 0.2216, time elapsed 00:00:05\n","[Epoch 8 Batch 701] base_loss_cur 0.1971, time elapsed 00:00:06\n","[Epoch 8 Batch 801] base_loss_cur 0.2239, time elapsed 00:00:07\n","[Epoch 8 Batch 901] base_loss_cur 0.2567, time elapsed 00:00:08\n","[Epoch 8 Batch 1001] base_loss_cur 0.2558, time elapsed 00:00:08\n","[Epoch 8 Batch 1101] base_loss_cur 0.2636, time elapsed 00:00:09\n","[Epoch 8 Batch 1201] base_loss_cur 0.1934, time elapsed 00:00:10\n","[Epoch 8 Batch 1301] base_loss_cur 0.2704, time elapsed 00:00:11\n","[Epoch 8 Batch 1401] base_loss_cur 0.2204, time elapsed 00:00:12\n","[Epoch 8 Batch 1501] base_loss_cur 0.2427, time elapsed 00:00:13\n","[Epoch 8 Batch 1601] base_loss_cur 0.2693, time elapsed 00:00:14\n","[Epoch 8 Batch 1701] base_loss_cur 0.2970, time elapsed 00:00:14\n","[Epoch 8 Batch 1801] base_loss_cur 0.2468, time elapsed 00:00:15\n","[Epoch 8 Batch 1901] base_loss_cur 0.2200, time elapsed 00:00:16\n","[Epoch 8 Batch 2001] base_loss_cur 0.2729, time elapsed 00:00:17\n","[Epoch 8 Batch 2101] base_loss_cur 0.2765, time elapsed 00:00:18\n","Epoch 8 Done! time elapsed: 00:02:40, base_loss_cur_avg 0.2483\n","test_auc 0.8377, test_logloss 0.9704\n","time elapsed 00:02:41\n","\n","Training Base Model Epoch 9 Start!\n","[Epoch 9 Batch 1] base_loss_cur 0.1957, time elapsed 00:00:00\n","[Epoch 9 Batch 101] base_loss_cur 0.2171, time elapsed 00:00:01\n","[Epoch 9 Batch 201] base_loss_cur 0.2276, time elapsed 00:00:02\n","[Epoch 9 Batch 301] base_loss_cur 0.1777, time elapsed 00:00:02\n","[Epoch 9 Batch 401] base_loss_cur 0.2379, time elapsed 00:00:03\n","[Epoch 9 Batch 501] base_loss_cur 0.2813, time elapsed 00:00:04\n","[Epoch 9 Batch 601] base_loss_cur 0.2182, time elapsed 00:00:05\n","[Epoch 9 Batch 701] base_loss_cur 0.1665, time elapsed 00:00:06\n","[Epoch 9 Batch 801] base_loss_cur 0.2024, time elapsed 00:00:07\n","[Epoch 9 Batch 901] base_loss_cur 0.2285, time elapsed 00:00:08\n","[Epoch 9 Batch 1001] base_loss_cur 0.1879, time elapsed 00:00:08\n","[Epoch 9 Batch 1101] base_loss_cur 0.2683, time elapsed 00:00:09\n","[Epoch 9 Batch 1201] base_loss_cur 0.2329, time elapsed 00:00:10\n","[Epoch 9 Batch 1301] base_loss_cur 0.2683, time elapsed 00:00:11\n","[Epoch 9 Batch 1401] base_loss_cur 0.3015, time elapsed 00:00:12\n","[Epoch 9 Batch 1501] base_loss_cur 0.2333, time elapsed 00:00:13\n","[Epoch 9 Batch 1601] base_loss_cur 0.2187, time elapsed 00:00:14\n","[Epoch 9 Batch 1701] base_loss_cur 0.2239, time elapsed 00:00:14\n","[Epoch 9 Batch 1801] base_loss_cur 0.2825, time elapsed 00:00:15\n","[Epoch 9 Batch 1901] base_loss_cur 0.2615, time elapsed 00:00:16\n","[Epoch 9 Batch 2001] base_loss_cur 0.2468, time elapsed 00:00:17\n","[Epoch 9 Batch 2101] base_loss_cur 0.2420, time elapsed 00:00:18\n","Epoch 9 Done! time elapsed: 00:03:00, base_loss_cur_avg 0.2377\n","test_auc 0.8345, test_logloss 1.0346\n","time elapsed 00:03:01\n","\n","Training Base Model Epoch 10 Start!\n","[Epoch 10 Batch 1] base_loss_cur 0.1952, time elapsed 00:00:00\n","[Epoch 10 Batch 101] base_loss_cur 0.2032, time elapsed 00:00:01\n","[Epoch 10 Batch 201] base_loss_cur 0.2423, time elapsed 00:00:02\n","[Epoch 10 Batch 301] base_loss_cur 0.2472, time elapsed 00:00:03\n","[Epoch 10 Batch 401] base_loss_cur 0.1492, time elapsed 00:00:03\n","[Epoch 10 Batch 501] base_loss_cur 0.2474, time elapsed 00:00:04\n","[Epoch 10 Batch 601] base_loss_cur 0.2899, time elapsed 00:00:05\n","[Epoch 10 Batch 701] base_loss_cur 0.2133, time elapsed 00:00:06\n","[Epoch 10 Batch 801] base_loss_cur 0.1988, time elapsed 00:00:07\n","[Epoch 10 Batch 901] base_loss_cur 0.2084, time elapsed 00:00:08\n","[Epoch 10 Batch 1001] base_loss_cur 0.2702, time elapsed 00:00:08\n","[Epoch 10 Batch 1101] base_loss_cur 0.2158, time elapsed 00:00:09\n","[Epoch 10 Batch 1201] base_loss_cur 0.2717, time elapsed 00:00:10\n","[Epoch 10 Batch 1301] base_loss_cur 0.1655, time elapsed 00:00:11\n","[Epoch 10 Batch 1401] base_loss_cur 0.2001, time elapsed 00:00:12\n","[Epoch 10 Batch 1501] base_loss_cur 0.2610, time elapsed 00:00:13\n","[Epoch 10 Batch 1601] base_loss_cur 0.2329, time elapsed 00:00:14\n","[Epoch 10 Batch 1701] base_loss_cur 0.2128, time elapsed 00:00:14\n","[Epoch 10 Batch 1801] base_loss_cur 0.1974, time elapsed 00:00:15\n","[Epoch 10 Batch 1901] base_loss_cur 0.3023, time elapsed 00:00:16\n","[Epoch 10 Batch 2001] base_loss_cur 0.2363, time elapsed 00:00:17\n","[Epoch 10 Batch 2101] base_loss_cur 0.2282, time elapsed 00:00:18\n","Epoch 10 Done! time elapsed: 00:03:20, base_loss_cur_avg 0.2280\n","test_auc 0.8298, test_logloss 1.1370\n","time elapsed 00:03:21\n","\n","11-Nov-21 08:55:21 [INFO] : JOB END: EMBEDMLP_PRETRAINING\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Y3quOeuqFHwF","collapsed":true,"executionInfo":{"status":"ok","timestamp":1636621464860,"user_tz":-330,"elapsed":544424,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}},"outputId":"f4a6c221-0e6e-499c-e78a-066abf8da6bb"},"source":["logger.info('JOB START: BATCH_UPDATE')\n","bu_sobazaar()\n","logger.info('JOB END: BATCH_UPDATE')"],"execution_count":25,"outputs":[{"output_type":"stream","name":"stdout","text":["11-Nov-21 08:55:21 [INFO] : JOB START: BATCH_UPDATE\n","11-Nov-21 08:55:41 [INFO] : Done loading data! time elapsed: 00:00:20\n","\n","base_lr 0.001\n","dir_name:  BU_train11-23_test24-30_7_1epoch_0.001\n","\n","current periods: [5, 6, 7, 8, 9, 10, 11], next period: 12\n","\n","current set size 380555 next set size 54365\n","restored checkpoint: ckpts/pretrain_train1-10_test11_10epoch_0.001/Epoch2_TestAUC0.8535_TestLOGLOSS0.5567.ckpt\n","INFO:tensorflow:Restoring parameters from ckpts/pretrain_train1-10_test11_10epoch_0.001/Epoch2_TestAUC0.8535_TestLOGLOSS0.5567.ckpt\n","11-Nov-21 08:55:43 [INFO] : Restoring parameters from ckpts/pretrain_train1-10_test11_10epoch_0.001/Epoch2_TestAUC0.8535_TestLOGLOSS0.5567.ckpt\n","Training Base Model Epoch 1 Start!\n","[Epoch 1 Batch 1] base_loss_cur 0.3012, time elapsed 00:00:00\n","[Epoch 1 Batch 101] base_loss_cur 0.3759, time elapsed 00:00:01\n","[Epoch 1 Batch 201] base_loss_cur 0.3072, time elapsed 00:00:02\n","[Epoch 1 Batch 301] base_loss_cur 0.2780, time elapsed 00:00:02\n","[Epoch 1 Batch 401] base_loss_cur 0.3333, time elapsed 00:00:03\n","[Epoch 1 Batch 501] base_loss_cur 0.2950, time elapsed 00:00:04\n","[Epoch 1 Batch 601] base_loss_cur 0.3739, time elapsed 00:00:05\n","[Epoch 1 Batch 701] base_loss_cur 0.4402, time elapsed 00:00:06\n","[Epoch 1 Batch 801] base_loss_cur 0.2771, time elapsed 00:00:06\n","[Epoch 1 Batch 901] base_loss_cur 0.2760, time elapsed 00:00:07\n","[Epoch 1 Batch 1001] base_loss_cur 0.3149, time elapsed 00:00:08\n","[Epoch 1 Batch 1101] base_loss_cur 0.2818, time elapsed 00:00:09\n","[Epoch 1 Batch 1201] base_loss_cur 0.3244, time elapsed 00:00:10\n","[Epoch 1 Batch 1301] base_loss_cur 0.3660, time elapsed 00:00:11\n","[Epoch 1 Batch 1401] base_loss_cur 0.3258, time elapsed 00:00:11\n","Epoch 1 Done! time elapsed: 00:00:12, base_loss_cur_avg 0.3126\n","cur_auc 0.9468, cur_logloss 0.2778, next_auc 0.8025, next_logloss 0.8145\n","time elapsed 00:00:24\n","\n","\n","current periods: [6, 7, 8, 9, 10, 11, 12], next period: 13\n","\n","current set size 380555 next set size 54365\n","restored checkpoint: ckpts/BU_train11-23_test24-30_7_1epoch_0.001/period11/Epoch1_TestAUC0.8025_TestLOGLOSS0.8145.ckpt\n","INFO:tensorflow:Restoring parameters from ckpts/BU_train11-23_test24-30_7_1epoch_0.001/period11/Epoch1_TestAUC0.8025_TestLOGLOSS0.8145.ckpt\n","11-Nov-21 08:56:08 [INFO] : Restoring parameters from ckpts/BU_train11-23_test24-30_7_1epoch_0.001/period11/Epoch1_TestAUC0.8025_TestLOGLOSS0.8145.ckpt\n","Training Base Model Epoch 1 Start!\n","[Epoch 1 Batch 1] base_loss_cur 0.3988, time elapsed 00:00:00\n","[Epoch 1 Batch 101] base_loss_cur 0.4014, time elapsed 00:00:01\n","[Epoch 1 Batch 201] base_loss_cur 0.3279, time elapsed 00:00:02\n","[Epoch 1 Batch 301] base_loss_cur 0.3181, time elapsed 00:00:02\n","[Epoch 1 Batch 401] base_loss_cur 0.3056, time elapsed 00:00:03\n","[Epoch 1 Batch 501] base_loss_cur 0.2414, time elapsed 00:00:04\n","[Epoch 1 Batch 601] base_loss_cur 0.2488, time elapsed 00:00:05\n","[Epoch 1 Batch 701] base_loss_cur 0.2902, time elapsed 00:00:06\n","[Epoch 1 Batch 801] base_loss_cur 0.3233, time elapsed 00:00:07\n","[Epoch 1 Batch 901] base_loss_cur 0.2944, time elapsed 00:00:07\n","[Epoch 1 Batch 1001] base_loss_cur 0.2441, time elapsed 00:00:08\n","[Epoch 1 Batch 1101] base_loss_cur 0.3132, time elapsed 00:00:09\n","[Epoch 1 Batch 1201] base_loss_cur 0.2597, time elapsed 00:00:10\n","[Epoch 1 Batch 1301] base_loss_cur 0.2713, time elapsed 00:00:11\n","[Epoch 1 Batch 1401] base_loss_cur 0.2601, time elapsed 00:00:12\n","Epoch 1 Done! time elapsed: 00:00:12, base_loss_cur_avg 0.3120\n","cur_auc 0.9506, cur_logloss 0.2718, next_auc 0.8668, next_logloss 0.5196\n","time elapsed 00:00:24\n","\n","\n","current periods: [7, 8, 9, 10, 11, 12, 13], next period: 14\n","\n","current set size 380555 next set size 54365\n","restored checkpoint: ckpts/BU_train11-23_test24-30_7_1epoch_0.001/period12/Epoch1_TestAUC0.8668_TestLOGLOSS0.5196.ckpt\n","INFO:tensorflow:Restoring parameters from ckpts/BU_train11-23_test24-30_7_1epoch_0.001/period12/Epoch1_TestAUC0.8668_TestLOGLOSS0.5196.ckpt\n","11-Nov-21 08:56:34 [INFO] : Restoring parameters from ckpts/BU_train11-23_test24-30_7_1epoch_0.001/period12/Epoch1_TestAUC0.8668_TestLOGLOSS0.5196.ckpt\n","Training Base Model Epoch 1 Start!\n","[Epoch 1 Batch 1] base_loss_cur 0.4059, time elapsed 00:00:00\n","[Epoch 1 Batch 101] base_loss_cur 0.3276, time elapsed 00:00:01\n","[Epoch 1 Batch 201] base_loss_cur 0.3321, time elapsed 00:00:02\n","[Epoch 1 Batch 301] base_loss_cur 0.2956, time elapsed 00:00:02\n","[Epoch 1 Batch 401] base_loss_cur 0.3665, time elapsed 00:00:03\n","[Epoch 1 Batch 501] base_loss_cur 0.2780, time elapsed 00:00:04\n","[Epoch 1 Batch 601] base_loss_cur 0.3000, time elapsed 00:00:05\n","[Epoch 1 Batch 701] base_loss_cur 0.2801, time elapsed 00:00:06\n","[Epoch 1 Batch 801] base_loss_cur 0.3212, time elapsed 00:00:07\n","[Epoch 1 Batch 901] base_loss_cur 0.3291, time elapsed 00:00:08\n","[Epoch 1 Batch 1001] base_loss_cur 0.2548, time elapsed 00:00:08\n","[Epoch 1 Batch 1101] base_loss_cur 0.2824, time elapsed 00:00:09\n","[Epoch 1 Batch 1201] base_loss_cur 0.3003, time elapsed 00:00:10\n","[Epoch 1 Batch 1301] base_loss_cur 0.2571, time elapsed 00:00:11\n","[Epoch 1 Batch 1401] base_loss_cur 0.3226, time elapsed 00:00:12\n","Epoch 1 Done! time elapsed: 00:00:13, base_loss_cur_avg 0.2948\n","cur_auc 0.9560, cur_logloss 0.2574, next_auc 0.8662, next_logloss 0.5410\n","time elapsed 00:00:23\n","\n","\n","current periods: [8, 9, 10, 11, 12, 13, 14], next period: 15\n","\n","current set size 380555 next set size 54365\n","restored checkpoint: ckpts/BU_train11-23_test24-30_7_1epoch_0.001/period13/Epoch1_TestAUC0.8662_TestLOGLOSS0.5410.ckpt\n","INFO:tensorflow:Restoring parameters from ckpts/BU_train11-23_test24-30_7_1epoch_0.001/period13/Epoch1_TestAUC0.8662_TestLOGLOSS0.5410.ckpt\n","11-Nov-21 08:56:58 [INFO] : Restoring parameters from ckpts/BU_train11-23_test24-30_7_1epoch_0.001/period13/Epoch1_TestAUC0.8662_TestLOGLOSS0.5410.ckpt\n","Training Base Model Epoch 1 Start!\n","[Epoch 1 Batch 1] base_loss_cur 0.2730, time elapsed 00:00:00\n","[Epoch 1 Batch 101] base_loss_cur 0.2377, time elapsed 00:00:01\n","[Epoch 1 Batch 201] base_loss_cur 0.3075, time elapsed 00:00:02\n","[Epoch 1 Batch 301] base_loss_cur 0.2972, time elapsed 00:00:02\n","[Epoch 1 Batch 401] base_loss_cur 0.3023, time elapsed 00:00:03\n","[Epoch 1 Batch 501] base_loss_cur 0.2898, time elapsed 00:00:04\n","[Epoch 1 Batch 601] base_loss_cur 0.2848, time elapsed 00:00:05\n","[Epoch 1 Batch 701] base_loss_cur 0.2429, time elapsed 00:00:06\n","[Epoch 1 Batch 801] base_loss_cur 0.2612, time elapsed 00:00:07\n","[Epoch 1 Batch 901] base_loss_cur 0.2985, time elapsed 00:00:07\n","[Epoch 1 Batch 1001] base_loss_cur 0.3092, time elapsed 00:00:08\n","[Epoch 1 Batch 1101] base_loss_cur 0.2904, time elapsed 00:00:09\n","[Epoch 1 Batch 1201] base_loss_cur 0.3306, time elapsed 00:00:10\n","[Epoch 1 Batch 1301] base_loss_cur 0.2927, time elapsed 00:00:11\n","[Epoch 1 Batch 1401] base_loss_cur 0.2325, time elapsed 00:00:12\n","Epoch 1 Done! time elapsed: 00:00:12, base_loss_cur_avg 0.2834\n","cur_auc 0.9600, cur_logloss 0.2446, next_auc 0.8888, next_logloss 0.4770\n","time elapsed 00:00:25\n","\n","\n","current periods: [9, 10, 11, 12, 13, 14, 15], next period: 16\n","\n","current set size 380555 next set size 54365\n","restored checkpoint: ckpts/BU_train11-23_test24-30_7_1epoch_0.001/period14/Epoch1_TestAUC0.8888_TestLOGLOSS0.4770.ckpt\n","INFO:tensorflow:Restoring parameters from ckpts/BU_train11-23_test24-30_7_1epoch_0.001/period14/Epoch1_TestAUC0.8888_TestLOGLOSS0.4770.ckpt\n","11-Nov-21 08:57:24 [INFO] : Restoring parameters from ckpts/BU_train11-23_test24-30_7_1epoch_0.001/period14/Epoch1_TestAUC0.8888_TestLOGLOSS0.4770.ckpt\n","Training Base Model Epoch 1 Start!\n","[Epoch 1 Batch 1] base_loss_cur 0.2824, time elapsed 00:00:00\n","[Epoch 1 Batch 101] base_loss_cur 0.2793, time elapsed 00:00:01\n","[Epoch 1 Batch 201] base_loss_cur 0.2052, time elapsed 00:00:02\n","[Epoch 1 Batch 301] base_loss_cur 0.3083, time elapsed 00:00:03\n","[Epoch 1 Batch 401] base_loss_cur 0.2636, time elapsed 00:00:03\n","[Epoch 1 Batch 501] base_loss_cur 0.2596, time elapsed 00:00:04\n","[Epoch 1 Batch 601] base_loss_cur 0.2998, time elapsed 00:00:05\n","[Epoch 1 Batch 701] base_loss_cur 0.2888, time elapsed 00:00:06\n","[Epoch 1 Batch 801] base_loss_cur 0.2489, time elapsed 00:00:07\n","[Epoch 1 Batch 901] base_loss_cur 0.2563, time elapsed 00:00:08\n","[Epoch 1 Batch 1001] base_loss_cur 0.3097, time elapsed 00:00:09\n","[Epoch 1 Batch 1101] base_loss_cur 0.2703, time elapsed 00:00:09\n","[Epoch 1 Batch 1201] base_loss_cur 0.2802, time elapsed 00:00:10\n","[Epoch 1 Batch 1301] base_loss_cur 0.2387, time elapsed 00:00:11\n","[Epoch 1 Batch 1401] base_loss_cur 0.2725, time elapsed 00:00:12\n","Epoch 1 Done! time elapsed: 00:00:13, base_loss_cur_avg 0.2755\n","cur_auc 0.9625, cur_logloss 0.2372, next_auc 0.7779, next_logloss 1.0132\n","time elapsed 00:00:25\n","\n","\n","current periods: [10, 11, 12, 13, 14, 15, 16], next period: 17\n","\n","current set size 380555 next set size 54365\n","restored checkpoint: ckpts/BU_train11-23_test24-30_7_1epoch_0.001/period15/Epoch1_TestAUC0.7779_TestLOGLOSS1.0132.ckpt\n","INFO:tensorflow:Restoring parameters from ckpts/BU_train11-23_test24-30_7_1epoch_0.001/period15/Epoch1_TestAUC0.7779_TestLOGLOSS1.0132.ckpt\n","11-Nov-21 08:57:50 [INFO] : Restoring parameters from ckpts/BU_train11-23_test24-30_7_1epoch_0.001/period15/Epoch1_TestAUC0.7779_TestLOGLOSS1.0132.ckpt\n","Training Base Model Epoch 1 Start!\n","[Epoch 1 Batch 1] base_loss_cur 0.3963, time elapsed 00:00:00\n","[Epoch 1 Batch 101] base_loss_cur 0.2702, time elapsed 00:00:01\n","[Epoch 1 Batch 201] base_loss_cur 0.3060, time elapsed 00:00:02\n","[Epoch 1 Batch 301] base_loss_cur 0.2969, time elapsed 00:00:03\n","[Epoch 1 Batch 401] base_loss_cur 0.2628, time elapsed 00:00:03\n","[Epoch 1 Batch 501] base_loss_cur 0.2579, time elapsed 00:00:04\n","[Epoch 1 Batch 601] base_loss_cur 0.2146, time elapsed 00:00:05\n","[Epoch 1 Batch 701] base_loss_cur 0.2495, time elapsed 00:00:06\n","[Epoch 1 Batch 801] base_loss_cur 0.3598, time elapsed 00:00:07\n","[Epoch 1 Batch 901] base_loss_cur 0.2867, time elapsed 00:00:08\n","[Epoch 1 Batch 1001] base_loss_cur 0.2585, time elapsed 00:00:09\n","[Epoch 1 Batch 1101] base_loss_cur 0.3372, time elapsed 00:00:09\n","[Epoch 1 Batch 1201] base_loss_cur 0.3097, time elapsed 00:00:10\n","[Epoch 1 Batch 1301] base_loss_cur 0.3284, time elapsed 00:00:11\n","[Epoch 1 Batch 1401] base_loss_cur 0.2456, time elapsed 00:00:12\n","Epoch 1 Done! time elapsed: 00:00:13, base_loss_cur_avg 0.2970\n","cur_auc 0.9597, cur_logloss 0.2495, next_auc 0.8123, next_logloss 0.6530\n","time elapsed 00:00:25\n","\n","\n","current periods: [11, 12, 13, 14, 15, 16, 17], next period: 18\n","\n","current set size 380555 next set size 54365\n","restored checkpoint: ckpts/BU_train11-23_test24-30_7_1epoch_0.001/period16/Epoch1_TestAUC0.8123_TestLOGLOSS0.6530.ckpt\n","INFO:tensorflow:Restoring parameters from ckpts/BU_train11-23_test24-30_7_1epoch_0.001/period16/Epoch1_TestAUC0.8123_TestLOGLOSS0.6530.ckpt\n","11-Nov-21 08:58:16 [INFO] : Restoring parameters from ckpts/BU_train11-23_test24-30_7_1epoch_0.001/period16/Epoch1_TestAUC0.8123_TestLOGLOSS0.6530.ckpt\n","Training Base Model Epoch 1 Start!\n","[Epoch 1 Batch 1] base_loss_cur 0.4127, time elapsed 00:00:00\n","[Epoch 1 Batch 101] base_loss_cur 0.3474, time elapsed 00:00:01\n","[Epoch 1 Batch 201] base_loss_cur 0.3763, time elapsed 00:00:02\n","[Epoch 1 Batch 301] base_loss_cur 0.3215, time elapsed 00:00:03\n","[Epoch 1 Batch 401] base_loss_cur 0.2746, time elapsed 00:00:03\n","[Epoch 1 Batch 501] base_loss_cur 0.3079, time elapsed 00:00:04\n","[Epoch 1 Batch 601] base_loss_cur 0.2758, time elapsed 00:00:05\n","[Epoch 1 Batch 701] base_loss_cur 0.3504, time elapsed 00:00:06\n","[Epoch 1 Batch 801] base_loss_cur 0.3793, time elapsed 00:00:07\n","[Epoch 1 Batch 901] base_loss_cur 0.2618, time elapsed 00:00:08\n","[Epoch 1 Batch 1001] base_loss_cur 0.2929, time elapsed 00:00:09\n","[Epoch 1 Batch 1101] base_loss_cur 0.3300, time elapsed 00:00:10\n","[Epoch 1 Batch 1201] base_loss_cur 0.3158, time elapsed 00:00:10\n","[Epoch 1 Batch 1301] base_loss_cur 0.2198, time elapsed 00:00:11\n","[Epoch 1 Batch 1401] base_loss_cur 0.3216, time elapsed 00:00:12\n","Epoch 1 Done! time elapsed: 00:00:13, base_loss_cur_avg 0.2994\n","cur_auc 0.9586, cur_logloss 0.2534, next_auc 0.8275, next_logloss 0.5985\n","time elapsed 00:00:25\n","\n","\n","current periods: [12, 13, 14, 15, 16, 17, 18], next period: 19\n","\n","current set size 380555 next set size 54365\n","restored checkpoint: ckpts/BU_train11-23_test24-30_7_1epoch_0.001/period17/Epoch1_TestAUC0.8275_TestLOGLOSS0.5985.ckpt\n","INFO:tensorflow:Restoring parameters from ckpts/BU_train11-23_test24-30_7_1epoch_0.001/period17/Epoch1_TestAUC0.8275_TestLOGLOSS0.5985.ckpt\n","11-Nov-21 08:58:43 [INFO] : Restoring parameters from ckpts/BU_train11-23_test24-30_7_1epoch_0.001/period17/Epoch1_TestAUC0.8275_TestLOGLOSS0.5985.ckpt\n","Training Base Model Epoch 1 Start!\n","[Epoch 1 Batch 1] base_loss_cur 0.2675, time elapsed 00:00:00\n","[Epoch 1 Batch 101] base_loss_cur 0.2917, time elapsed 00:00:01\n","[Epoch 1 Batch 201] base_loss_cur 0.3747, time elapsed 00:00:02\n","[Epoch 1 Batch 301] base_loss_cur 0.3519, time elapsed 00:00:03\n","[Epoch 1 Batch 401] base_loss_cur 0.3036, time elapsed 00:00:03\n","[Epoch 1 Batch 501] base_loss_cur 0.3376, time elapsed 00:00:04\n","[Epoch 1 Batch 601] base_loss_cur 0.2974, time elapsed 00:00:05\n","[Epoch 1 Batch 701] base_loss_cur 0.2930, time elapsed 00:00:06\n","[Epoch 1 Batch 801] base_loss_cur 0.2808, time elapsed 00:00:07\n","[Epoch 1 Batch 901] base_loss_cur 0.3496, time elapsed 00:00:08\n","[Epoch 1 Batch 1001] base_loss_cur 0.3679, time elapsed 00:00:09\n","[Epoch 1 Batch 1101] base_loss_cur 0.3333, time elapsed 00:00:09\n","[Epoch 1 Batch 1201] base_loss_cur 0.2963, time elapsed 00:00:10\n","[Epoch 1 Batch 1301] base_loss_cur 0.2831, time elapsed 00:00:11\n","[Epoch 1 Batch 1401] base_loss_cur 0.2582, time elapsed 00:00:12\n","Epoch 1 Done! time elapsed: 00:00:13, base_loss_cur_avg 0.3012\n","cur_auc 0.9579, cur_logloss 0.2554, next_auc 0.7612, next_logloss 0.7718\n","time elapsed 00:00:25\n","\n","\n","current periods: [13, 14, 15, 16, 17, 18, 19], next period: 20\n","\n","current set size 380555 next set size 54365\n","restored checkpoint: ckpts/BU_train11-23_test24-30_7_1epoch_0.001/period18/Epoch1_TestAUC0.7612_TestLOGLOSS0.7718.ckpt\n","INFO:tensorflow:Restoring parameters from ckpts/BU_train11-23_test24-30_7_1epoch_0.001/period18/Epoch1_TestAUC0.7612_TestLOGLOSS0.7718.ckpt\n","11-Nov-21 08:59:09 [INFO] : Restoring parameters from ckpts/BU_train11-23_test24-30_7_1epoch_0.001/period18/Epoch1_TestAUC0.7612_TestLOGLOSS0.7718.ckpt\n","Training Base Model Epoch 1 Start!\n","[Epoch 1 Batch 1] base_loss_cur 0.3559, time elapsed 00:00:00\n","[Epoch 1 Batch 101] base_loss_cur 0.3719, time elapsed 00:00:01\n","[Epoch 1 Batch 201] base_loss_cur 0.2948, time elapsed 00:00:02\n","[Epoch 1 Batch 301] base_loss_cur 0.2596, time elapsed 00:00:03\n","[Epoch 1 Batch 401] base_loss_cur 0.3397, time elapsed 00:00:04\n","[Epoch 1 Batch 501] base_loss_cur 0.3583, time elapsed 00:00:04\n","[Epoch 1 Batch 601] base_loss_cur 0.3185, time elapsed 00:00:05\n","[Epoch 1 Batch 701] base_loss_cur 0.3023, time elapsed 00:00:06\n","[Epoch 1 Batch 801] base_loss_cur 0.2838, time elapsed 00:00:07\n","[Epoch 1 Batch 901] base_loss_cur 0.3110, time elapsed 00:00:08\n","[Epoch 1 Batch 1001] base_loss_cur 0.3590, time elapsed 00:00:09\n","[Epoch 1 Batch 1101] base_loss_cur 0.3245, time elapsed 00:00:10\n","[Epoch 1 Batch 1201] base_loss_cur 0.2929, time elapsed 00:00:11\n","[Epoch 1 Batch 1301] base_loss_cur 0.3911, time elapsed 00:00:11\n","[Epoch 1 Batch 1401] base_loss_cur 0.2928, time elapsed 00:00:12\n","Epoch 1 Done! time elapsed: 00:00:13, base_loss_cur_avg 0.3198\n","cur_auc 0.9538, cur_logloss 0.2685, next_auc 0.8044, next_logloss 0.6301\n","time elapsed 00:00:24\n","\n","\n","current periods: [14, 15, 16, 17, 18, 19, 20], next period: 21\n","\n","current set size 380555 next set size 54365\n","restored checkpoint: ckpts/BU_train11-23_test24-30_7_1epoch_0.001/period19/Epoch1_TestAUC0.8044_TestLOGLOSS0.6301.ckpt\n","INFO:tensorflow:Restoring parameters from ckpts/BU_train11-23_test24-30_7_1epoch_0.001/period19/Epoch1_TestAUC0.8044_TestLOGLOSS0.6301.ckpt\n","11-Nov-21 08:59:34 [INFO] : Restoring parameters from ckpts/BU_train11-23_test24-30_7_1epoch_0.001/period19/Epoch1_TestAUC0.8044_TestLOGLOSS0.6301.ckpt\n","Training Base Model Epoch 1 Start!\n","[Epoch 1 Batch 1] base_loss_cur 0.3356, time elapsed 00:00:00\n","[Epoch 1 Batch 101] base_loss_cur 0.3665, time elapsed 00:00:01\n","[Epoch 1 Batch 201] base_loss_cur 0.3004, time elapsed 00:00:02\n","[Epoch 1 Batch 301] base_loss_cur 0.3492, time elapsed 00:00:03\n","[Epoch 1 Batch 401] base_loss_cur 0.2882, time elapsed 00:00:03\n","[Epoch 1 Batch 501] base_loss_cur 0.3554, time elapsed 00:00:04\n","[Epoch 1 Batch 601] base_loss_cur 0.3060, time elapsed 00:00:05\n","[Epoch 1 Batch 701] base_loss_cur 0.2533, time elapsed 00:00:06\n","[Epoch 1 Batch 801] base_loss_cur 0.3361, time elapsed 00:00:07\n","[Epoch 1 Batch 901] base_loss_cur 0.3659, time elapsed 00:00:08\n","[Epoch 1 Batch 1001] base_loss_cur 0.3199, time elapsed 00:00:09\n","[Epoch 1 Batch 1101] base_loss_cur 0.3144, time elapsed 00:00:10\n","[Epoch 1 Batch 1201] base_loss_cur 0.3233, time elapsed 00:00:10\n","[Epoch 1 Batch 1301] base_loss_cur 0.3149, time elapsed 00:00:11\n","[Epoch 1 Batch 1401] base_loss_cur 0.2871, time elapsed 00:00:12\n","Epoch 1 Done! time elapsed: 00:00:13, base_loss_cur_avg 0.3211\n","cur_auc 0.9539, cur_logloss 0.2699, next_auc 0.7943, next_logloss 0.6656\n","time elapsed 00:00:25\n","\n","\n","current periods: [15, 16, 17, 18, 19, 20, 21], next period: 22\n","\n","current set size 380555 next set size 54365\n","restored checkpoint: ckpts/BU_train11-23_test24-30_7_1epoch_0.001/period20/Epoch1_TestAUC0.7943_TestLOGLOSS0.6656.ckpt\n","INFO:tensorflow:Restoring parameters from ckpts/BU_train11-23_test24-30_7_1epoch_0.001/period20/Epoch1_TestAUC0.7943_TestLOGLOSS0.6656.ckpt\n","11-Nov-21 09:00:01 [INFO] : Restoring parameters from ckpts/BU_train11-23_test24-30_7_1epoch_0.001/period20/Epoch1_TestAUC0.7943_TestLOGLOSS0.6656.ckpt\n","Training Base Model Epoch 1 Start!\n","[Epoch 1 Batch 1] base_loss_cur 0.3223, time elapsed 00:00:00\n","[Epoch 1 Batch 101] base_loss_cur 0.3581, time elapsed 00:00:01\n","[Epoch 1 Batch 201] base_loss_cur 0.3367, time elapsed 00:00:02\n","[Epoch 1 Batch 301] base_loss_cur 0.2890, time elapsed 00:00:03\n","[Epoch 1 Batch 401] base_loss_cur 0.3817, time elapsed 00:00:03\n","[Epoch 1 Batch 501] base_loss_cur 0.2727, time elapsed 00:00:04\n","[Epoch 1 Batch 601] base_loss_cur 0.3528, time elapsed 00:00:05\n","[Epoch 1 Batch 701] base_loss_cur 0.3767, time elapsed 00:00:06\n","[Epoch 1 Batch 801] base_loss_cur 0.2895, time elapsed 00:00:07\n","[Epoch 1 Batch 901] base_loss_cur 0.3194, time elapsed 00:00:08\n","[Epoch 1 Batch 1001] base_loss_cur 0.3049, time elapsed 00:00:09\n","[Epoch 1 Batch 1101] base_loss_cur 0.3247, time elapsed 00:00:10\n","[Epoch 1 Batch 1201] base_loss_cur 0.2888, time elapsed 00:00:10\n","[Epoch 1 Batch 1301] base_loss_cur 0.3281, time elapsed 00:00:11\n","[Epoch 1 Batch 1401] base_loss_cur 0.3476, time elapsed 00:00:12\n","Epoch 1 Done! time elapsed: 00:00:13, base_loss_cur_avg 0.3273\n","cur_auc 0.9524, cur_logloss 0.2753, next_auc 0.7940, next_logloss 0.6647\n","time elapsed 00:00:25\n","\n","\n","current periods: [16, 17, 18, 19, 20, 21, 22], next period: 23\n","\n","current set size 380555 next set size 54365\n","restored checkpoint: ckpts/BU_train11-23_test24-30_7_1epoch_0.001/period21/Epoch1_TestAUC0.7940_TestLOGLOSS0.6647.ckpt\n","INFO:tensorflow:Restoring parameters from ckpts/BU_train11-23_test24-30_7_1epoch_0.001/period21/Epoch1_TestAUC0.7940_TestLOGLOSS0.6647.ckpt\n","11-Nov-21 09:00:27 [INFO] : Restoring parameters from ckpts/BU_train11-23_test24-30_7_1epoch_0.001/period21/Epoch1_TestAUC0.7940_TestLOGLOSS0.6647.ckpt\n","Training Base Model Epoch 1 Start!\n","[Epoch 1 Batch 1] base_loss_cur 0.3398, time elapsed 00:00:00\n","[Epoch 1 Batch 101] base_loss_cur 0.3451, time elapsed 00:00:01\n","[Epoch 1 Batch 201] base_loss_cur 0.3244, time elapsed 00:00:02\n","[Epoch 1 Batch 301] base_loss_cur 0.3805, time elapsed 00:00:03\n","[Epoch 1 Batch 401] base_loss_cur 0.3848, time elapsed 00:00:03\n","[Epoch 1 Batch 501] base_loss_cur 0.2820, time elapsed 00:00:04\n","[Epoch 1 Batch 601] base_loss_cur 0.2669, time elapsed 00:00:05\n","[Epoch 1 Batch 701] base_loss_cur 0.3378, time elapsed 00:00:06\n","[Epoch 1 Batch 801] base_loss_cur 0.3274, time elapsed 00:00:07\n","[Epoch 1 Batch 901] base_loss_cur 0.3134, time elapsed 00:00:08\n","[Epoch 1 Batch 1001] base_loss_cur 0.2864, time elapsed 00:00:09\n","[Epoch 1 Batch 1101] base_loss_cur 0.3305, time elapsed 00:00:09\n","[Epoch 1 Batch 1201] base_loss_cur 0.3029, time elapsed 00:00:10\n","[Epoch 1 Batch 1301] base_loss_cur 0.3190, time elapsed 00:00:11\n","[Epoch 1 Batch 1401] base_loss_cur 0.3151, time elapsed 00:00:12\n","Epoch 1 Done! time elapsed: 00:00:13, base_loss_cur_avg 0.3369\n","cur_auc 0.9489, cur_logloss 0.2851, next_auc 0.7836, next_logloss 0.6920\n","time elapsed 00:00:25\n","\n","\n","current periods: [17, 18, 19, 20, 21, 22, 23], next period: 24\n","\n","current set size 380555 next set size 54365\n","restored checkpoint: ckpts/BU_train11-23_test24-30_7_1epoch_0.001/period22/Epoch1_TestAUC0.7836_TestLOGLOSS0.6920.ckpt\n","INFO:tensorflow:Restoring parameters from ckpts/BU_train11-23_test24-30_7_1epoch_0.001/period22/Epoch1_TestAUC0.7836_TestLOGLOSS0.6920.ckpt\n","11-Nov-21 09:00:54 [INFO] : Restoring parameters from ckpts/BU_train11-23_test24-30_7_1epoch_0.001/period22/Epoch1_TestAUC0.7836_TestLOGLOSS0.6920.ckpt\n","Training Base Model Epoch 1 Start!\n","[Epoch 1 Batch 1] base_loss_cur 0.3443, time elapsed 00:00:00\n","[Epoch 1 Batch 101] base_loss_cur 0.3196, time elapsed 00:00:01\n","[Epoch 1 Batch 201] base_loss_cur 0.3352, time elapsed 00:00:02\n","[Epoch 1 Batch 301] base_loss_cur 0.3271, time elapsed 00:00:03\n","[Epoch 1 Batch 401] base_loss_cur 0.3182, time elapsed 00:00:03\n","[Epoch 1 Batch 501] base_loss_cur 0.3177, time elapsed 00:00:04\n","[Epoch 1 Batch 601] base_loss_cur 0.3664, time elapsed 00:00:05\n","[Epoch 1 Batch 701] base_loss_cur 0.3269, time elapsed 00:00:06\n","[Epoch 1 Batch 801] base_loss_cur 0.3207, time elapsed 00:00:07\n","[Epoch 1 Batch 901] base_loss_cur 0.3283, time elapsed 00:00:08\n","[Epoch 1 Batch 1001] base_loss_cur 0.3638, time elapsed 00:00:09\n","[Epoch 1 Batch 1101] base_loss_cur 0.3918, time elapsed 00:00:10\n","[Epoch 1 Batch 1201] base_loss_cur 0.3888, time elapsed 00:00:11\n","[Epoch 1 Batch 1301] base_loss_cur 0.3422, time elapsed 00:00:11\n","[Epoch 1 Batch 1401] base_loss_cur 0.3500, time elapsed 00:00:12\n","Epoch 1 Done! time elapsed: 00:00:13, base_loss_cur_avg 0.3451\n","cur_auc 0.9458, cur_logloss 0.2941, next_auc 0.8146, next_logloss 0.6122\n","time elapsed 00:00:25\n","\n","\n","current periods: [18, 19, 20, 21, 22, 23, 24], next period: 25\n","\n","current set size 380555 next set size 54365\n","restored checkpoint: ckpts/BU_train11-23_test24-30_7_1epoch_0.001/period23/Epoch1_TestAUC0.8146_TestLOGLOSS0.6122.ckpt\n","INFO:tensorflow:Restoring parameters from ckpts/BU_train11-23_test24-30_7_1epoch_0.001/period23/Epoch1_TestAUC0.8146_TestLOGLOSS0.6122.ckpt\n","11-Nov-21 09:01:20 [INFO] : Restoring parameters from ckpts/BU_train11-23_test24-30_7_1epoch_0.001/period23/Epoch1_TestAUC0.8146_TestLOGLOSS0.6122.ckpt\n","Training Base Model Epoch 1 Start!\n","[Epoch 1 Batch 1] base_loss_cur 0.3485, time elapsed 00:00:00\n","[Epoch 1 Batch 101] base_loss_cur 0.3044, time elapsed 00:00:01\n","[Epoch 1 Batch 201] base_loss_cur 0.3372, time elapsed 00:00:02\n","[Epoch 1 Batch 301] base_loss_cur 0.3450, time elapsed 00:00:03\n","[Epoch 1 Batch 401] base_loss_cur 0.3123, time elapsed 00:00:03\n","[Epoch 1 Batch 501] base_loss_cur 0.3364, time elapsed 00:00:04\n","[Epoch 1 Batch 601] base_loss_cur 0.3579, time elapsed 00:00:05\n","[Epoch 1 Batch 701] base_loss_cur 0.3219, time elapsed 00:00:06\n","[Epoch 1 Batch 801] base_loss_cur 0.3598, time elapsed 00:00:07\n","[Epoch 1 Batch 901] base_loss_cur 0.3426, time elapsed 00:00:08\n","[Epoch 1 Batch 1001] base_loss_cur 0.3200, time elapsed 00:00:09\n","[Epoch 1 Batch 1101] base_loss_cur 0.3487, time elapsed 00:00:09\n","[Epoch 1 Batch 1201] base_loss_cur 0.3617, time elapsed 00:00:10\n","[Epoch 1 Batch 1301] base_loss_cur 0.2864, time elapsed 00:00:11\n","[Epoch 1 Batch 1401] base_loss_cur 0.3533, time elapsed 00:00:12\n","Epoch 1 Done! time elapsed: 00:00:13, base_loss_cur_avg 0.3453\n","cur_auc 0.9455, cur_logloss 0.2948, next_auc 0.7734, next_logloss 0.7131\n","time elapsed 00:00:25\n","\n","test aucs [0.7733711649782167]\n","average auc 0.7733711649782167\n","\n","test loglosses [0.7130960004359019]\n","average logloss 0.7130960004359019\n","\n","current periods: [19, 20, 21, 22, 23, 24, 25], next period: 26\n","\n","current set size 380555 next set size 54365\n","restored checkpoint: ckpts/BU_train11-23_test24-30_7_1epoch_0.001/period24/Epoch1_TestAUC0.7734_TestLOGLOSS0.7131.ckpt\n","INFO:tensorflow:Restoring parameters from ckpts/BU_train11-23_test24-30_7_1epoch_0.001/period24/Epoch1_TestAUC0.7734_TestLOGLOSS0.7131.ckpt\n","11-Nov-21 09:01:46 [INFO] : Restoring parameters from ckpts/BU_train11-23_test24-30_7_1epoch_0.001/period24/Epoch1_TestAUC0.7734_TestLOGLOSS0.7131.ckpt\n","Training Base Model Epoch 1 Start!\n","[Epoch 1 Batch 1] base_loss_cur 0.3443, time elapsed 00:00:00\n","[Epoch 1 Batch 101] base_loss_cur 0.3128, time elapsed 00:00:01\n","[Epoch 1 Batch 201] base_loss_cur 0.3704, time elapsed 00:00:02\n","[Epoch 1 Batch 301] base_loss_cur 0.4597, time elapsed 00:00:02\n","[Epoch 1 Batch 401] base_loss_cur 0.3935, time elapsed 00:00:03\n","[Epoch 1 Batch 501] base_loss_cur 0.3777, time elapsed 00:00:04\n","[Epoch 1 Batch 601] base_loss_cur 0.3916, time elapsed 00:00:05\n","[Epoch 1 Batch 701] base_loss_cur 0.3650, time elapsed 00:00:06\n","[Epoch 1 Batch 801] base_loss_cur 0.3634, time elapsed 00:00:07\n","[Epoch 1 Batch 901] base_loss_cur 0.3394, time elapsed 00:00:08\n","[Epoch 1 Batch 1001] base_loss_cur 0.3173, time elapsed 00:00:08\n","[Epoch 1 Batch 1101] base_loss_cur 0.3615, time elapsed 00:00:09\n","[Epoch 1 Batch 1201] base_loss_cur 0.3173, time elapsed 00:00:10\n","[Epoch 1 Batch 1301] base_loss_cur 0.2945, time elapsed 00:00:11\n","[Epoch 1 Batch 1401] base_loss_cur 0.3733, time elapsed 00:00:12\n","Epoch 1 Done! time elapsed: 00:00:13, base_loss_cur_avg 0.3560\n","cur_auc 0.9423, cur_logloss 0.3031, next_auc 0.7849, next_logloss 0.6751\n","time elapsed 00:00:24\n","\n","test aucs [0.7733711649782167, 0.7848744462332831]\n","average auc 0.7791228056057499\n","\n","test loglosses [0.7130960004359019, 0.6750665854410918]\n","average logloss 0.6940812929384969\n","\n","current periods: [20, 21, 22, 23, 24, 25, 26], next period: 27\n","\n","current set size 380555 next set size 54365\n","restored checkpoint: ckpts/BU_train11-23_test24-30_7_1epoch_0.001/period25/Epoch1_TestAUC0.7849_TestLOGLOSS0.6751.ckpt\n","INFO:tensorflow:Restoring parameters from ckpts/BU_train11-23_test24-30_7_1epoch_0.001/period25/Epoch1_TestAUC0.7849_TestLOGLOSS0.6751.ckpt\n","11-Nov-21 09:02:11 [INFO] : Restoring parameters from ckpts/BU_train11-23_test24-30_7_1epoch_0.001/period25/Epoch1_TestAUC0.7849_TestLOGLOSS0.6751.ckpt\n","Training Base Model Epoch 1 Start!\n","[Epoch 1 Batch 1] base_loss_cur 0.3282, time elapsed 00:00:00\n","[Epoch 1 Batch 101] base_loss_cur 0.3807, time elapsed 00:00:01\n","[Epoch 1 Batch 201] base_loss_cur 0.4162, time elapsed 00:00:02\n","[Epoch 1 Batch 301] base_loss_cur 0.3806, time elapsed 00:00:03\n","[Epoch 1 Batch 401] base_loss_cur 0.3861, time elapsed 00:00:03\n","[Epoch 1 Batch 501] base_loss_cur 0.3806, time elapsed 00:00:04\n","[Epoch 1 Batch 601] base_loss_cur 0.4004, time elapsed 00:00:05\n","[Epoch 1 Batch 701] base_loss_cur 0.4507, time elapsed 00:00:06\n","[Epoch 1 Batch 801] base_loss_cur 0.3880, time elapsed 00:00:07\n","[Epoch 1 Batch 901] base_loss_cur 0.3299, time elapsed 00:00:08\n","[Epoch 1 Batch 1001] base_loss_cur 0.3707, time elapsed 00:00:09\n","[Epoch 1 Batch 1101] base_loss_cur 0.4115, time elapsed 00:00:10\n","[Epoch 1 Batch 1201] base_loss_cur 0.2799, time elapsed 00:00:10\n","[Epoch 1 Batch 1301] base_loss_cur 0.3835, time elapsed 00:00:11\n","[Epoch 1 Batch 1401] base_loss_cur 0.3631, time elapsed 00:00:12\n","Epoch 1 Done! time elapsed: 00:00:13, base_loss_cur_avg 0.3568\n","cur_auc 0.9419, cur_logloss 0.3059, next_auc 0.7503, next_logloss 0.7431\n","time elapsed 00:00:25\n","\n","test aucs [0.7733711649782167, 0.7848744462332831, 0.7503173179195032]\n","average auc 0.769520976377001\n","\n","test loglosses [0.7130960004359019, 0.6750665854410918, 0.7431252074347245]\n","average logloss 0.7104292644372393\n","\n","current periods: [21, 22, 23, 24, 25, 26, 27], next period: 28\n","\n","current set size 380555 next set size 54365\n","restored checkpoint: ckpts/BU_train11-23_test24-30_7_1epoch_0.001/period26/Epoch1_TestAUC0.7503_TestLOGLOSS0.7431.ckpt\n","INFO:tensorflow:Restoring parameters from ckpts/BU_train11-23_test24-30_7_1epoch_0.001/period26/Epoch1_TestAUC0.7503_TestLOGLOSS0.7431.ckpt\n","11-Nov-21 09:02:38 [INFO] : Restoring parameters from ckpts/BU_train11-23_test24-30_7_1epoch_0.001/period26/Epoch1_TestAUC0.7503_TestLOGLOSS0.7431.ckpt\n","Training Base Model Epoch 1 Start!\n","[Epoch 1 Batch 1] base_loss_cur 0.3695, time elapsed 00:00:00\n","[Epoch 1 Batch 101] base_loss_cur 0.3425, time elapsed 00:00:01\n","[Epoch 1 Batch 201] base_loss_cur 0.3554, time elapsed 00:00:02\n","[Epoch 1 Batch 301] base_loss_cur 0.3587, time elapsed 00:00:03\n","[Epoch 1 Batch 401] base_loss_cur 0.3662, time elapsed 00:00:03\n","[Epoch 1 Batch 501] base_loss_cur 0.3705, time elapsed 00:00:04\n","[Epoch 1 Batch 601] base_loss_cur 0.4241, time elapsed 00:00:05\n","[Epoch 1 Batch 701] base_loss_cur 0.3744, time elapsed 00:00:06\n","[Epoch 1 Batch 801] base_loss_cur 0.4185, time elapsed 00:00:07\n","[Epoch 1 Batch 901] base_loss_cur 0.3692, time elapsed 00:00:08\n","[Epoch 1 Batch 1001] base_loss_cur 0.3543, time elapsed 00:00:09\n","[Epoch 1 Batch 1101] base_loss_cur 0.3645, time elapsed 00:00:10\n","[Epoch 1 Batch 1201] base_loss_cur 0.3485, time elapsed 00:00:11\n","[Epoch 1 Batch 1301] base_loss_cur 0.4004, time elapsed 00:00:11\n","[Epoch 1 Batch 1401] base_loss_cur 0.3585, time elapsed 00:00:12\n","Epoch 1 Done! time elapsed: 00:00:13, base_loss_cur_avg 0.3696\n","cur_auc 0.9378, cur_logloss 0.3163, next_auc 0.7771, next_logloss 0.6741\n","time elapsed 00:00:26\n","\n","test aucs [0.7733711649782167, 0.7848744462332831, 0.7503173179195032, 0.7771110043176401]\n","average auc 0.7714184833621608\n","\n","test loglosses [0.7130960004359019, 0.6750665854410918, 0.7431252074347245, 0.674109708162875]\n","average logloss 0.7013493753686483\n","\n","current periods: [22, 23, 24, 25, 26, 27, 28], next period: 29\n","\n","current set size 380555 next set size 54365\n","restored checkpoint: ckpts/BU_train11-23_test24-30_7_1epoch_0.001/period27/Epoch1_TestAUC0.7771_TestLOGLOSS0.6741.ckpt\n","INFO:tensorflow:Restoring parameters from ckpts/BU_train11-23_test24-30_7_1epoch_0.001/period27/Epoch1_TestAUC0.7771_TestLOGLOSS0.6741.ckpt\n","11-Nov-21 09:03:04 [INFO] : Restoring parameters from ckpts/BU_train11-23_test24-30_7_1epoch_0.001/period27/Epoch1_TestAUC0.7771_TestLOGLOSS0.6741.ckpt\n","Training Base Model Epoch 1 Start!\n","[Epoch 1 Batch 1] base_loss_cur 0.4562, time elapsed 00:00:00\n","[Epoch 1 Batch 101] base_loss_cur 0.3675, time elapsed 00:00:01\n","[Epoch 1 Batch 201] base_loss_cur 0.3624, time elapsed 00:00:02\n","[Epoch 1 Batch 301] base_loss_cur 0.3300, time elapsed 00:00:03\n","[Epoch 1 Batch 401] base_loss_cur 0.4055, time elapsed 00:00:04\n","[Epoch 1 Batch 501] base_loss_cur 0.3829, time elapsed 00:00:04\n","[Epoch 1 Batch 601] base_loss_cur 0.3747, time elapsed 00:00:05\n","[Epoch 1 Batch 701] base_loss_cur 0.4390, time elapsed 00:00:06\n","[Epoch 1 Batch 801] base_loss_cur 0.3263, time elapsed 00:00:07\n","[Epoch 1 Batch 901] base_loss_cur 0.3979, time elapsed 00:00:08\n","[Epoch 1 Batch 1001] base_loss_cur 0.4144, time elapsed 00:00:09\n","[Epoch 1 Batch 1101] base_loss_cur 0.3809, time elapsed 00:00:10\n","[Epoch 1 Batch 1201] base_loss_cur 0.3563, time elapsed 00:00:11\n","[Epoch 1 Batch 1301] base_loss_cur 0.3917, time elapsed 00:00:12\n","[Epoch 1 Batch 1401] base_loss_cur 0.3698, time elapsed 00:00:12\n","Epoch 1 Done! time elapsed: 00:00:13, base_loss_cur_avg 0.3724\n","cur_auc 0.9371, cur_logloss 0.3198, next_auc 0.7741, next_logloss 0.6496\n","time elapsed 00:00:26\n","\n","test aucs [0.7733711649782167, 0.7848744462332831, 0.7503173179195032, 0.7771110043176401, 0.774118925558211]\n","average auc 0.7719585718013708\n","\n","test loglosses [0.7130960004359019, 0.6750665854410918, 0.7431252074347245, 0.674109708162875, 0.6496325144643643]\n","average logloss 0.6910060031877915\n","\n","current periods: [23, 24, 25, 26, 27, 28, 29], next period: 30\n","\n","current set size 380555 next set size 54365\n","restored checkpoint: ckpts/BU_train11-23_test24-30_7_1epoch_0.001/period28/Epoch1_TestAUC0.7741_TestLOGLOSS0.6496.ckpt\n","INFO:tensorflow:Restoring parameters from ckpts/BU_train11-23_test24-30_7_1epoch_0.001/period28/Epoch1_TestAUC0.7741_TestLOGLOSS0.6496.ckpt\n","11-Nov-21 09:03:31 [INFO] : Restoring parameters from ckpts/BU_train11-23_test24-30_7_1epoch_0.001/period28/Epoch1_TestAUC0.7741_TestLOGLOSS0.6496.ckpt\n","Training Base Model Epoch 1 Start!\n","[Epoch 1 Batch 1] base_loss_cur 0.3928, time elapsed 00:00:00\n","[Epoch 1 Batch 101] base_loss_cur 0.4181, time elapsed 00:00:01\n","[Epoch 1 Batch 201] base_loss_cur 0.3887, time elapsed 00:00:02\n","[Epoch 1 Batch 301] base_loss_cur 0.3816, time elapsed 00:00:03\n","[Epoch 1 Batch 401] base_loss_cur 0.3988, time elapsed 00:00:03\n","[Epoch 1 Batch 501] base_loss_cur 0.3166, time elapsed 00:00:04\n","[Epoch 1 Batch 601] base_loss_cur 0.4144, time elapsed 00:00:05\n","[Epoch 1 Batch 701] base_loss_cur 0.4026, time elapsed 00:00:06\n","[Epoch 1 Batch 801] base_loss_cur 0.3573, time elapsed 00:00:07\n","[Epoch 1 Batch 901] base_loss_cur 0.4093, time elapsed 00:00:08\n","[Epoch 1 Batch 1001] base_loss_cur 0.4145, time elapsed 00:00:09\n","[Epoch 1 Batch 1101] base_loss_cur 0.4159, time elapsed 00:00:10\n","[Epoch 1 Batch 1201] base_loss_cur 0.3493, time elapsed 00:00:10\n","[Epoch 1 Batch 1301] base_loss_cur 0.3859, time elapsed 00:00:11\n","[Epoch 1 Batch 1401] base_loss_cur 0.3317, time elapsed 00:00:12\n","Epoch 1 Done! time elapsed: 00:00:13, base_loss_cur_avg 0.3756\n","cur_auc 0.9366, cur_logloss 0.3211, next_auc 0.7987, next_logloss 0.6117\n","time elapsed 00:00:26\n","\n","test aucs [0.7733711649782167, 0.7848744462332831, 0.7503173179195032, 0.7771110043176401, 0.774118925558211, 0.7986522539443252]\n","average auc 0.7764075188251965\n","\n","test loglosses [0.7130960004359019, 0.6750665854410918, 0.7431252074347245, 0.674109708162875, 0.6496325144643643, 0.6116537211273294]\n","average logloss 0.6777806228443811\n","\n","current periods: [24, 25, 26, 27, 28, 29, 30], next period: 31\n","\n","current set size 380555 next set size 54365\n","restored checkpoint: ckpts/BU_train11-23_test24-30_7_1epoch_0.001/period29/Epoch1_TestAUC0.7987_TestLOGLOSS0.6117.ckpt\n","INFO:tensorflow:Restoring parameters from ckpts/BU_train11-23_test24-30_7_1epoch_0.001/period29/Epoch1_TestAUC0.7987_TestLOGLOSS0.6117.ckpt\n","11-Nov-21 09:03:58 [INFO] : Restoring parameters from ckpts/BU_train11-23_test24-30_7_1epoch_0.001/period29/Epoch1_TestAUC0.7987_TestLOGLOSS0.6117.ckpt\n","Training Base Model Epoch 1 Start!\n","[Epoch 1 Batch 1] base_loss_cur 0.3858, time elapsed 00:00:00\n","[Epoch 1 Batch 101] base_loss_cur 0.3795, time elapsed 00:00:01\n","[Epoch 1 Batch 201] base_loss_cur 0.3077, time elapsed 00:00:02\n","[Epoch 1 Batch 301] base_loss_cur 0.3171, time elapsed 00:00:03\n","[Epoch 1 Batch 401] base_loss_cur 0.3633, time elapsed 00:00:04\n","[Epoch 1 Batch 501] base_loss_cur 0.3962, time elapsed 00:00:04\n","[Epoch 1 Batch 601] base_loss_cur 0.3192, time elapsed 00:00:05\n","[Epoch 1 Batch 701] base_loss_cur 0.3360, time elapsed 00:00:06\n","[Epoch 1 Batch 801] base_loss_cur 0.3971, time elapsed 00:00:07\n","[Epoch 1 Batch 901] base_loss_cur 0.3538, time elapsed 00:00:08\n","[Epoch 1 Batch 1001] base_loss_cur 0.4139, time elapsed 00:00:09\n","[Epoch 1 Batch 1101] base_loss_cur 0.3995, time elapsed 00:00:10\n","[Epoch 1 Batch 1201] base_loss_cur 0.3274, time elapsed 00:00:11\n","[Epoch 1 Batch 1301] base_loss_cur 0.4411, time elapsed 00:00:12\n","[Epoch 1 Batch 1401] base_loss_cur 0.3321, time elapsed 00:00:13\n","Epoch 1 Done! time elapsed: 00:00:13, base_loss_cur_avg 0.3728\n","cur_auc 0.9378, cur_logloss 0.3182, next_auc 0.7889, next_logloss 0.6412\n","time elapsed 00:00:26\n","\n","test aucs [0.7733711649782167, 0.7848744462332831, 0.7503173179195032, 0.7771110043176401, 0.774118925558211, 0.7986522539443252, 0.7888996229424762]\n","average auc 0.7781921051276651\n","\n","test loglosses [0.7130960004359019, 0.6750665854410918, 0.7431252074347245, 0.674109708162875, 0.6496325144643643, 0.6116537211273294, 0.6412282284996546]\n","average logloss 0.6725588522237059\n","11-Nov-21 09:04:25 [INFO] : JOB END: BATCH_UPDATE\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"id":"ZvFN3zuYJq-U","executionInfo":{"status":"ok","timestamp":1636623218955,"user_tz":-330,"elapsed":291436,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}},"outputId":"72fa099b-f84c-42c0-bf93-c6ee4b83c8ad"},"source":["logger.info('JOB START: SPMF_MODEL_TRAINING')\n","spmf_sobazaar()\n","logger.info('JOB END: SPMF_MODEL_TRAINING')"],"execution_count":41,"outputs":[{"output_type":"stream","name":"stdout","text":["11-Nov-21 09:28:48 [INFO] : JOB START: SPMF_MODEL_TRAINING\n","11-Nov-21 09:29:11 [INFO] : Done loading data! time elapsed: 00:00:22\n","\n","frac_of_pretrain_D 1 res_cur_ratio 0.1 base_lr 0.001\n","dir_name:  SPMF_2_train11-23_test24-30_1epoch_1_0.1_0.001\n","\n","current period: 11, next period: 12\n","\n","current set size 54365 next set size 54365\n","restored checkpoint: ckpts/pretrain_train1-10_test11_10epoch_0.001/Epoch2_TestAUC0.8535_TestLOGLOSS0.5567.ckpt\n","INFO:tensorflow:Restoring parameters from ckpts/pretrain_train1-10_test11_10epoch_0.001/Epoch2_TestAUC0.8535_TestLOGLOSS0.5567.ckpt\n","11-Nov-21 09:29:13 [INFO] : Restoring parameters from ckpts/pretrain_train1-10_test11_10epoch_0.001/Epoch2_TestAUC0.8535_TestLOGLOSS0.5567.ckpt\n","sampled_reservoir size 5436\n","selected_pos_current_set size 25934\n","selected_neg_cur_set size 25881\n","compute prob and generate train set and update reservoir time elapsed: 00:00:07\n","Training Base Model Epoch 1 Start!\n","[Epoch 1 Batch 1] base_loss_cur 0.3788, time elapsed 00:00:00\n","[Epoch 1 Batch 101] base_loss_cur 0.4012, time elapsed 00:00:00\n","[Epoch 1 Batch 201] base_loss_cur 0.4253, time elapsed 00:00:01\n","Epoch 1 Done! time elapsed: 00:00:01, base_loss_cur_avg 0.4234\n","cur_auc 0.9217, cur_logloss 0.3455, next_auc 0.8009, next_logloss 0.6433\n","time elapsed 00:00:04\n","\n","\n","current period: 12, next period: 13\n","\n","current set size 54365 next set size 54365\n","restored checkpoint: ckpts/SPMF_2_train11-23_test24-30_1epoch_1_0.1_0.001/period11/Epoch1_TestAUC0.8009_TestLOGLOSS0.6433.ckpt\n","INFO:tensorflow:Restoring parameters from ckpts/SPMF_2_train11-23_test24-30_1epoch_1_0.1_0.001/period11/Epoch1_TestAUC0.8009_TestLOGLOSS0.6433.ckpt\n","11-Nov-21 09:29:25 [INFO] : Restoring parameters from ckpts/SPMF_2_train11-23_test24-30_1epoch_1_0.1_0.001/period11/Epoch1_TestAUC0.8009_TestLOGLOSS0.6433.ckpt\n","sampled_reservoir size 5436\n","selected_pos_current_set size 23633\n","selected_neg_cur_set size 23567\n","compute prob and generate train set and update reservoir time elapsed: 00:00:07\n","Training Base Model Epoch 1 Start!\n","[Epoch 1 Batch 1] base_loss_cur 0.3152, time elapsed 00:00:00\n","[Epoch 1 Batch 101] base_loss_cur 0.5949, time elapsed 00:00:00\n","[Epoch 1 Batch 201] base_loss_cur 0.4696, time elapsed 00:00:01\n","Epoch 1 Done! time elapsed: 00:00:01, base_loss_cur_avg 0.4804\n","cur_auc 0.9015, cur_logloss 0.3903, next_auc 0.8507, next_logloss 0.4848\n","time elapsed 00:00:04\n","\n","\n","current period: 13, next period: 14\n","\n","current set size 54365 next set size 54365\n","restored checkpoint: ckpts/SPMF_2_train11-23_test24-30_1epoch_1_0.1_0.001/period12/Epoch1_TestAUC0.8507_TestLOGLOSS0.4848.ckpt\n","INFO:tensorflow:Restoring parameters from ckpts/SPMF_2_train11-23_test24-30_1epoch_1_0.1_0.001/period12/Epoch1_TestAUC0.8507_TestLOGLOSS0.4848.ckpt\n","11-Nov-21 09:29:38 [INFO] : Restoring parameters from ckpts/SPMF_2_train11-23_test24-30_1epoch_1_0.1_0.001/period12/Epoch1_TestAUC0.8507_TestLOGLOSS0.4848.ckpt\n","sampled_reservoir size 5436\n","selected_pos_current_set size 21869\n","selected_neg_cur_set size 21679\n","compute prob and generate train set and update reservoir time elapsed: 00:00:07\n","Training Base Model Epoch 1 Start!\n","[Epoch 1 Batch 1] base_loss_cur 0.4018, time elapsed 00:00:00\n","[Epoch 1 Batch 101] base_loss_cur 0.3889, time elapsed 00:00:00\n","[Epoch 1 Batch 201] base_loss_cur 0.3718, time elapsed 00:00:01\n","Epoch 1 Done! time elapsed: 00:00:01, base_loss_cur_avg 0.4277\n","cur_auc 0.9297, cur_logloss 0.3408, next_auc 0.8567, next_logloss 0.4780\n","time elapsed 00:00:04\n","\n","\n","current period: 14, next period: 15\n","\n","current set size 54365 next set size 54365\n","restored checkpoint: ckpts/SPMF_2_train11-23_test24-30_1epoch_1_0.1_0.001/period13/Epoch1_TestAUC0.8567_TestLOGLOSS0.4780.ckpt\n","INFO:tensorflow:Restoring parameters from ckpts/SPMF_2_train11-23_test24-30_1epoch_1_0.1_0.001/period13/Epoch1_TestAUC0.8567_TestLOGLOSS0.4780.ckpt\n","11-Nov-21 09:29:51 [INFO] : Restoring parameters from ckpts/SPMF_2_train11-23_test24-30_1epoch_1_0.1_0.001/period13/Epoch1_TestAUC0.8567_TestLOGLOSS0.4780.ckpt\n","sampled_reservoir size 5436\n","selected_pos_current_set size 20108\n","selected_neg_cur_set size 20246\n","compute prob and generate train set and update reservoir time elapsed: 00:00:07\n","Training Base Model Epoch 1 Start!\n","[Epoch 1 Batch 1] base_loss_cur 0.3730, time elapsed 00:00:00\n","[Epoch 1 Batch 101] base_loss_cur 0.3718, time elapsed 00:00:00\n","[Epoch 1 Batch 201] base_loss_cur 0.3961, time elapsed 00:00:01\n","Epoch 1 Done! time elapsed: 00:00:02, base_loss_cur_avg 0.4089\n","cur_auc 0.9492, cur_logloss 0.2997, next_auc 0.8917, next_logloss 0.4145\n","time elapsed 00:00:04\n","\n","\n","current period: 15, next period: 16\n","\n","current set size 54365 next set size 54365\n","restored checkpoint: ckpts/SPMF_2_train11-23_test24-30_1epoch_1_0.1_0.001/period14/Epoch1_TestAUC0.8917_TestLOGLOSS0.4145.ckpt\n","INFO:tensorflow:Restoring parameters from ckpts/SPMF_2_train11-23_test24-30_1epoch_1_0.1_0.001/period14/Epoch1_TestAUC0.8917_TestLOGLOSS0.4145.ckpt\n","11-Nov-21 09:30:03 [INFO] : Restoring parameters from ckpts/SPMF_2_train11-23_test24-30_1epoch_1_0.1_0.001/period14/Epoch1_TestAUC0.8917_TestLOGLOSS0.4145.ckpt\n","sampled_reservoir size 5436\n","selected_pos_current_set size 18770\n","selected_neg_cur_set size 18728\n","compute prob and generate train set and update reservoir time elapsed: 00:00:07\n","Training Base Model Epoch 1 Start!\n","[Epoch 1 Batch 1] base_loss_cur 0.4765, time elapsed 00:00:00\n","[Epoch 1 Batch 101] base_loss_cur 0.4193, time elapsed 00:00:00\n","[Epoch 1 Batch 201] base_loss_cur 0.3835, time elapsed 00:00:01\n","Epoch 1 Done! time elapsed: 00:00:02, base_loss_cur_avg 0.3887\n","cur_auc 0.9485, cur_logloss 0.2919, next_auc 0.7899, next_logloss 0.6595\n","time elapsed 00:00:04\n","\n","\n","current period: 16, next period: 17\n","\n","current set size 54365 next set size 54365\n","restored checkpoint: ckpts/SPMF_2_train11-23_test24-30_1epoch_1_0.1_0.001/period15/Epoch1_TestAUC0.7899_TestLOGLOSS0.6595.ckpt\n","INFO:tensorflow:Restoring parameters from ckpts/SPMF_2_train11-23_test24-30_1epoch_1_0.1_0.001/period15/Epoch1_TestAUC0.7899_TestLOGLOSS0.6595.ckpt\n","11-Nov-21 09:30:16 [INFO] : Restoring parameters from ckpts/SPMF_2_train11-23_test24-30_1epoch_1_0.1_0.001/period15/Epoch1_TestAUC0.7899_TestLOGLOSS0.6595.ckpt\n","sampled_reservoir size 5436\n","selected_pos_current_set size 17657\n","selected_neg_cur_set size 17533\n","compute prob and generate train set and update reservoir time elapsed: 00:00:07\n","Training Base Model Epoch 1 Start!\n","[Epoch 1 Batch 1] base_loss_cur 0.4750, time elapsed 00:00:00\n","[Epoch 1 Batch 101] base_loss_cur 0.4294, time elapsed 00:00:01\n","[Epoch 1 Batch 201] base_loss_cur 0.4541, time elapsed 00:00:01\n","Epoch 1 Done! time elapsed: 00:00:02, base_loss_cur_avg 0.4847\n","cur_auc 0.9299, cur_logloss 0.3516, next_auc 0.8277, next_logloss 0.5130\n","time elapsed 00:00:04\n","\n","\n","current period: 17, next period: 18\n","\n","current set size 54365 next set size 54365\n","restored checkpoint: ckpts/SPMF_2_train11-23_test24-30_1epoch_1_0.1_0.001/period16/Epoch1_TestAUC0.8277_TestLOGLOSS0.5130.ckpt\n","INFO:tensorflow:Restoring parameters from ckpts/SPMF_2_train11-23_test24-30_1epoch_1_0.1_0.001/period16/Epoch1_TestAUC0.8277_TestLOGLOSS0.5130.ckpt\n","11-Nov-21 09:30:29 [INFO] : Restoring parameters from ckpts/SPMF_2_train11-23_test24-30_1epoch_1_0.1_0.001/period16/Epoch1_TestAUC0.8277_TestLOGLOSS0.5130.ckpt\n","sampled_reservoir size 5436\n","selected_pos_current_set size 16484\n","selected_neg_cur_set size 16544\n","compute prob and generate train set and update reservoir time elapsed: 00:00:07\n","Training Base Model Epoch 1 Start!\n","[Epoch 1 Batch 1] base_loss_cur 0.5766, time elapsed 00:00:00\n","[Epoch 1 Batch 101] base_loss_cur 0.5659, time elapsed 00:00:00\n","[Epoch 1 Batch 201] base_loss_cur 0.4206, time elapsed 00:00:01\n","Epoch 1 Done! time elapsed: 00:00:02, base_loss_cur_avg 0.4655\n","cur_auc 0.9304, cur_logloss 0.3520, next_auc 0.8363, next_logloss 0.5041\n","time elapsed 00:00:06\n","\n","\n","current period: 18, next period: 19\n","\n","current set size 54365 next set size 54365\n","restored checkpoint: ckpts/SPMF_2_train11-23_test24-30_1epoch_1_0.1_0.001/period17/Epoch1_TestAUC0.8363_TestLOGLOSS0.5041.ckpt\n","INFO:tensorflow:Restoring parameters from ckpts/SPMF_2_train11-23_test24-30_1epoch_1_0.1_0.001/period17/Epoch1_TestAUC0.8363_TestLOGLOSS0.5041.ckpt\n","11-Nov-21 09:30:44 [INFO] : Restoring parameters from ckpts/SPMF_2_train11-23_test24-30_1epoch_1_0.1_0.001/period17/Epoch1_TestAUC0.8363_TestLOGLOSS0.5041.ckpt\n","sampled_reservoir size 5436\n","selected_pos_current_set size 15528\n","selected_neg_cur_set size 15438\n","compute prob and generate train set and update reservoir time elapsed: 00:00:07\n","Training Base Model Epoch 1 Start!\n","[Epoch 1 Batch 1] base_loss_cur 0.6269, time elapsed 00:00:00\n","[Epoch 1 Batch 101] base_loss_cur 0.4534, time elapsed 00:00:00\n","[Epoch 1 Batch 201] base_loss_cur 0.4096, time elapsed 00:00:01\n","Epoch 1 Done! time elapsed: 00:00:02, base_loss_cur_avg 0.4610\n","cur_auc 0.9286, cur_logloss 0.3512, next_auc 0.7742, next_logloss 0.5960\n","time elapsed 00:00:04\n","\n","\n","current period: 19, next period: 20\n","\n","current set size 54365 next set size 54365\n","restored checkpoint: ckpts/SPMF_2_train11-23_test24-30_1epoch_1_0.1_0.001/period18/Epoch1_TestAUC0.7742_TestLOGLOSS0.5960.ckpt\n","INFO:tensorflow:Restoring parameters from ckpts/SPMF_2_train11-23_test24-30_1epoch_1_0.1_0.001/period18/Epoch1_TestAUC0.7742_TestLOGLOSS0.5960.ckpt\n","11-Nov-21 09:30:57 [INFO] : Restoring parameters from ckpts/SPMF_2_train11-23_test24-30_1epoch_1_0.1_0.001/period18/Epoch1_TestAUC0.7742_TestLOGLOSS0.5960.ckpt\n","sampled_reservoir size 5436\n","selected_pos_current_set size 14734\n","selected_neg_cur_set size 14672\n","compute prob and generate train set and update reservoir time elapsed: 00:00:07\n","Training Base Model Epoch 1 Start!\n","[Epoch 1 Batch 1] base_loss_cur 0.5900, time elapsed 00:00:00\n","[Epoch 1 Batch 101] base_loss_cur 0.4137, time elapsed 00:00:00\n","[Epoch 1 Batch 201] base_loss_cur 0.4921, time elapsed 00:00:01\n","Epoch 1 Done! time elapsed: 00:00:02, base_loss_cur_avg 0.5135\n","cur_auc 0.9088, cur_logloss 0.3994, next_auc 0.8276, next_logloss 0.5105\n","time elapsed 00:00:04\n","\n","\n","current period: 20, next period: 21\n","\n","current set size 54365 next set size 54365\n","restored checkpoint: ckpts/SPMF_2_train11-23_test24-30_1epoch_1_0.1_0.001/period19/Epoch1_TestAUC0.8276_TestLOGLOSS0.5105.ckpt\n","INFO:tensorflow:Restoring parameters from ckpts/SPMF_2_train11-23_test24-30_1epoch_1_0.1_0.001/period19/Epoch1_TestAUC0.8276_TestLOGLOSS0.5105.ckpt\n","11-Nov-21 09:31:10 [INFO] : Restoring parameters from ckpts/SPMF_2_train11-23_test24-30_1epoch_1_0.1_0.001/period19/Epoch1_TestAUC0.8276_TestLOGLOSS0.5105.ckpt\n","sampled_reservoir size 5436\n","selected_pos_current_set size 13972\n","selected_neg_cur_set size 13988\n","compute prob and generate train set and update reservoir time elapsed: 00:00:07\n","Training Base Model Epoch 1 Start!\n","[Epoch 1 Batch 1] base_loss_cur 0.5730, time elapsed 00:00:00\n","[Epoch 1 Batch 101] base_loss_cur 0.4574, time elapsed 00:00:00\n","[Epoch 1 Batch 201] base_loss_cur 0.3809, time elapsed 00:00:01\n","Epoch 1 Done! time elapsed: 00:00:01, base_loss_cur_avg 0.4614\n","cur_auc 0.9255, cur_logloss 0.3611, next_auc 0.8184, next_logloss 0.5310\n","time elapsed 00:00:04\n","\n","\n","current period: 21, next period: 22\n","\n","current set size 54365 next set size 54365\n","restored checkpoint: ckpts/SPMF_2_train11-23_test24-30_1epoch_1_0.1_0.001/period20/Epoch1_TestAUC0.8184_TestLOGLOSS0.5310.ckpt\n","INFO:tensorflow:Restoring parameters from ckpts/SPMF_2_train11-23_test24-30_1epoch_1_0.1_0.001/period20/Epoch1_TestAUC0.8184_TestLOGLOSS0.5310.ckpt\n","11-Nov-21 09:31:23 [INFO] : Restoring parameters from ckpts/SPMF_2_train11-23_test24-30_1epoch_1_0.1_0.001/period20/Epoch1_TestAUC0.8184_TestLOGLOSS0.5310.ckpt\n","sampled_reservoir size 5436\n","selected_pos_current_set size 13210\n","selected_neg_cur_set size 13169\n","compute prob and generate train set and update reservoir time elapsed: 00:00:07\n","Training Base Model Epoch 1 Start!\n","[Epoch 1 Batch 1] base_loss_cur 0.6428, time elapsed 00:00:00\n","[Epoch 1 Batch 101] base_loss_cur 0.5059, time elapsed 00:00:00\n","[Epoch 1 Batch 201] base_loss_cur 0.3654, time elapsed 00:00:01\n","Epoch 1 Done! time elapsed: 00:00:02, base_loss_cur_avg 0.4804\n","cur_auc 0.9155, cur_logloss 0.3830, next_auc 0.8192, next_logloss 0.5235\n","time elapsed 00:00:04\n","\n","\n","current period: 22, next period: 23\n","\n","current set size 54365 next set size 54365\n","restored checkpoint: ckpts/SPMF_2_train11-23_test24-30_1epoch_1_0.1_0.001/period21/Epoch1_TestAUC0.8192_TestLOGLOSS0.5235.ckpt\n","INFO:tensorflow:Restoring parameters from ckpts/SPMF_2_train11-23_test24-30_1epoch_1_0.1_0.001/period21/Epoch1_TestAUC0.8192_TestLOGLOSS0.5235.ckpt\n","11-Nov-21 09:31:36 [INFO] : Restoring parameters from ckpts/SPMF_2_train11-23_test24-30_1epoch_1_0.1_0.001/period21/Epoch1_TestAUC0.8192_TestLOGLOSS0.5235.ckpt\n","sampled_reservoir size 5436\n","selected_pos_current_set size 12559\n","selected_neg_cur_set size 12588\n","compute prob and generate train set and update reservoir time elapsed: 00:00:07\n","Training Base Model Epoch 1 Start!\n","[Epoch 1 Batch 1] base_loss_cur 0.6612, time elapsed 00:00:00\n","[Epoch 1 Batch 101] base_loss_cur 0.5632, time elapsed 00:00:01\n","[Epoch 1 Batch 201] base_loss_cur 0.4889, time elapsed 00:00:01\n","Epoch 1 Done! time elapsed: 00:00:02, base_loss_cur_avg 0.4931\n","cur_auc 0.9057, cur_logloss 0.4033, next_auc 0.8082, next_logloss 0.5376\n","time elapsed 00:00:04\n","\n","\n","current period: 23, next period: 24\n","\n","current set size 54365 next set size 54365\n","restored checkpoint: ckpts/SPMF_2_train11-23_test24-30_1epoch_1_0.1_0.001/period22/Epoch1_TestAUC0.8082_TestLOGLOSS0.5376.ckpt\n","INFO:tensorflow:Restoring parameters from ckpts/SPMF_2_train11-23_test24-30_1epoch_1_0.1_0.001/period22/Epoch1_TestAUC0.8082_TestLOGLOSS0.5376.ckpt\n","11-Nov-21 09:31:49 [INFO] : Restoring parameters from ckpts/SPMF_2_train11-23_test24-30_1epoch_1_0.1_0.001/period22/Epoch1_TestAUC0.8082_TestLOGLOSS0.5376.ckpt\n","sampled_reservoir size 5436\n","selected_pos_current_set size 11973\n","selected_neg_cur_set size 12054\n","compute prob and generate train set and update reservoir time elapsed: 00:00:07\n","Training Base Model Epoch 1 Start!\n","[Epoch 1 Batch 1] base_loss_cur 0.5467, time elapsed 00:00:00\n","[Epoch 1 Batch 101] base_loss_cur 0.5224, time elapsed 00:00:00\n","[Epoch 1 Batch 201] base_loss_cur 0.4579, time elapsed 00:00:01\n","Epoch 1 Done! time elapsed: 00:00:02, base_loss_cur_avg 0.5043\n","cur_auc 0.8997, cur_logloss 0.4152, next_auc 0.8323, next_logloss 0.5064\n","time elapsed 00:00:04\n","\n","\n","current period: 24, next period: 25\n","\n","current set size 54365 next set size 54365\n","restored checkpoint: ckpts/SPMF_2_train11-23_test24-30_1epoch_1_0.1_0.001/period23/Epoch1_TestAUC0.8323_TestLOGLOSS0.5064.ckpt\n","INFO:tensorflow:Restoring parameters from ckpts/SPMF_2_train11-23_test24-30_1epoch_1_0.1_0.001/period23/Epoch1_TestAUC0.8323_TestLOGLOSS0.5064.ckpt\n","11-Nov-21 09:32:02 [INFO] : Restoring parameters from ckpts/SPMF_2_train11-23_test24-30_1epoch_1_0.1_0.001/period23/Epoch1_TestAUC0.8323_TestLOGLOSS0.5064.ckpt\n","sampled_reservoir size 5436\n","selected_pos_current_set size 11469\n","selected_neg_cur_set size 11538\n","compute prob and generate train set and update reservoir time elapsed: 00:00:07\n","Training Base Model Epoch 1 Start!\n","[Epoch 1 Batch 1] base_loss_cur 0.6026, time elapsed 00:00:00\n","[Epoch 1 Batch 101] base_loss_cur 0.4747, time elapsed 00:00:00\n","[Epoch 1 Batch 201] base_loss_cur 0.5832, time elapsed 00:00:01\n","Epoch 1 Done! time elapsed: 00:00:01, base_loss_cur_avg 0.4857\n","cur_auc 0.9028, cur_logloss 0.4063, next_auc 0.7932, next_logloss 0.5578\n","time elapsed 00:00:04\n","\n","test aucs [0.7931901834700298]\n","average auc 0.7931901834700298\n","\n","test loglosses [0.5577758781666139]\n","average logloss 0.5577758781666139\n","\n","current period: 25, next period: 26\n","\n","current set size 54365 next set size 54365\n","restored checkpoint: ckpts/SPMF_2_train11-23_test24-30_1epoch_1_0.1_0.001/period24/Epoch1_TestAUC0.7932_TestLOGLOSS0.5578.ckpt\n","INFO:tensorflow:Restoring parameters from ckpts/SPMF_2_train11-23_test24-30_1epoch_1_0.1_0.001/period24/Epoch1_TestAUC0.7932_TestLOGLOSS0.5578.ckpt\n","11-Nov-21 09:32:15 [INFO] : Restoring parameters from ckpts/SPMF_2_train11-23_test24-30_1epoch_1_0.1_0.001/period24/Epoch1_TestAUC0.7932_TestLOGLOSS0.5578.ckpt\n","sampled_reservoir size 5436\n","selected_pos_current_set size 11079\n","selected_neg_cur_set size 11005\n","compute prob and generate train set and update reservoir time elapsed: 00:00:07\n","Training Base Model Epoch 1 Start!\n","[Epoch 1 Batch 1] base_loss_cur 0.6849, time elapsed 00:00:00\n","[Epoch 1 Batch 101] base_loss_cur 0.6284, time elapsed 00:00:00\n","[Epoch 1 Batch 201] base_loss_cur 0.5629, time elapsed 00:00:01\n","Epoch 1 Done! time elapsed: 00:00:02, base_loss_cur_avg 0.5249\n","cur_auc 0.8848, cur_logloss 0.4398, next_auc 0.8050, next_logloss 0.5374\n","time elapsed 00:00:06\n","\n","test aucs [0.7931901834700298, 0.8050342875503502]\n","average auc 0.7991122355101901\n","\n","test loglosses [0.5577758781666139, 0.5374437732583568]\n","average logloss 0.5476098257124853\n","\n","current period: 26, next period: 27\n","\n","current set size 54365 next set size 54365\n","restored checkpoint: ckpts/SPMF_2_train11-23_test24-30_1epoch_1_0.1_0.001/period25/Epoch1_TestAUC0.8050_TestLOGLOSS0.5374.ckpt\n","INFO:tensorflow:Restoring parameters from ckpts/SPMF_2_train11-23_test24-30_1epoch_1_0.1_0.001/period25/Epoch1_TestAUC0.8050_TestLOGLOSS0.5374.ckpt\n","11-Nov-21 09:32:30 [INFO] : Restoring parameters from ckpts/SPMF_2_train11-23_test24-30_1epoch_1_0.1_0.001/period25/Epoch1_TestAUC0.8050_TestLOGLOSS0.5374.ckpt\n","sampled_reservoir size 5436\n","selected_pos_current_set size 10619\n","selected_neg_cur_set size 10682\n","compute prob and generate train set and update reservoir time elapsed: 00:00:07\n","Training Base Model Epoch 1 Start!\n","[Epoch 1 Batch 1] base_loss_cur 0.6671, time elapsed 00:00:00\n","[Epoch 1 Batch 101] base_loss_cur 0.4844, time elapsed 00:00:00\n","[Epoch 1 Batch 201] base_loss_cur 0.5987, time elapsed 00:00:01\n","Epoch 1 Done! time elapsed: 00:00:02, base_loss_cur_avg 0.5016\n","cur_auc 0.8932, cur_logloss 0.4234, next_auc 0.7757, next_logloss 0.5752\n","time elapsed 00:00:05\n","\n","test aucs [0.7931901834700298, 0.8050342875503502, 0.77569081868783]\n","average auc 0.7913050965694034\n","\n","test loglosses [0.5577758781666139, 0.5374437732583568, 0.575169055326951]\n","average logloss 0.5567962355839738\n","\n","current period: 27, next period: 28\n","\n","current set size 54365 next set size 54365\n","restored checkpoint: ckpts/SPMF_2_train11-23_test24-30_1epoch_1_0.1_0.001/period26/Epoch1_TestAUC0.7757_TestLOGLOSS0.5752.ckpt\n","INFO:tensorflow:Restoring parameters from ckpts/SPMF_2_train11-23_test24-30_1epoch_1_0.1_0.001/period26/Epoch1_TestAUC0.7757_TestLOGLOSS0.5752.ckpt\n","11-Nov-21 09:32:44 [INFO] : Restoring parameters from ckpts/SPMF_2_train11-23_test24-30_1epoch_1_0.1_0.001/period26/Epoch1_TestAUC0.7757_TestLOGLOSS0.5752.ckpt\n","sampled_reservoir size 5436\n","selected_pos_current_set size 10300\n","selected_neg_cur_set size 10189\n","compute prob and generate train set and update reservoir time elapsed: 00:00:07\n","Training Base Model Epoch 1 Start!\n","[Epoch 1 Batch 1] base_loss_cur 0.5630, time elapsed 00:00:00\n","[Epoch 1 Batch 101] base_loss_cur 0.4733, time elapsed 00:00:00\n","[Epoch 1 Batch 201] base_loss_cur 0.4756, time elapsed 00:00:01\n","Epoch 1 Done! time elapsed: 00:00:02, base_loss_cur_avg 0.5314\n","cur_auc 0.8819, cur_logloss 0.4476, next_auc 0.8058, next_logloss 0.5365\n","time elapsed 00:00:05\n","\n","test aucs [0.7931901834700298, 0.8050342875503502, 0.77569081868783, 0.8058306387808231]\n","average auc 0.7949364821222583\n","\n","test loglosses [0.5577758781666139, 0.5374437732583568, 0.575169055326951, 0.5365459692556691]\n","average logloss 0.5517336690018977\n","\n","current period: 28, next period: 29\n","\n","current set size 54365 next set size 54365\n","restored checkpoint: ckpts/SPMF_2_train11-23_test24-30_1epoch_1_0.1_0.001/period27/Epoch1_TestAUC0.8058_TestLOGLOSS0.5365.ckpt\n","INFO:tensorflow:Restoring parameters from ckpts/SPMF_2_train11-23_test24-30_1epoch_1_0.1_0.001/period27/Epoch1_TestAUC0.8058_TestLOGLOSS0.5365.ckpt\n","11-Nov-21 09:32:57 [INFO] : Restoring parameters from ckpts/SPMF_2_train11-23_test24-30_1epoch_1_0.1_0.001/period27/Epoch1_TestAUC0.8058_TestLOGLOSS0.5365.ckpt\n","sampled_reservoir size 5436\n","selected_pos_current_set size 9812\n","selected_neg_cur_set size 10038\n","compute prob and generate train set and update reservoir time elapsed: 00:00:07\n","Training Base Model Epoch 1 Start!\n","[Epoch 1 Batch 1] base_loss_cur 0.6145, time elapsed 00:00:00\n","[Epoch 1 Batch 101] base_loss_cur 0.5333, time elapsed 00:00:00\n","[Epoch 1 Batch 201] base_loss_cur 0.4702, time elapsed 00:00:01\n","Epoch 1 Done! time elapsed: 00:00:02, base_loss_cur_avg 0.5182\n","cur_auc 0.8834, cur_logloss 0.4432, next_auc 0.7934, next_logloss 0.5514\n","time elapsed 00:00:04\n","\n","test aucs [0.7931901834700298, 0.8050342875503502, 0.77569081868783, 0.8058306387808231, 0.7934098642871424]\n","average auc 0.7946311585552351\n","\n","test loglosses [0.5577758781666139, 0.5374437732583568, 0.575169055326951, 0.5365459692556691, 0.5514386151552647]\n","average logloss 0.5516746582325711\n","\n","current period: 29, next period: 30\n","\n","current set size 54365 next set size 54365\n","restored checkpoint: ckpts/SPMF_2_train11-23_test24-30_1epoch_1_0.1_0.001/period28/Epoch1_TestAUC0.7934_TestLOGLOSS0.5514.ckpt\n","INFO:tensorflow:Restoring parameters from ckpts/SPMF_2_train11-23_test24-30_1epoch_1_0.1_0.001/period28/Epoch1_TestAUC0.7934_TestLOGLOSS0.5514.ckpt\n","11-Nov-21 09:33:10 [INFO] : Restoring parameters from ckpts/SPMF_2_train11-23_test24-30_1epoch_1_0.1_0.001/period28/Epoch1_TestAUC0.7934_TestLOGLOSS0.5514.ckpt\n","sampled_reservoir size 5436\n","selected_pos_current_set size 9623\n","selected_neg_cur_set size 9565\n","compute prob and generate train set and update reservoir time elapsed: 00:00:07\n","Training Base Model Epoch 1 Start!\n","[Epoch 1 Batch 1] base_loss_cur 0.6400, time elapsed 00:00:00\n","[Epoch 1 Batch 101] base_loss_cur 0.5184, time elapsed 00:00:00\n","[Epoch 1 Batch 201] base_loss_cur 0.5264, time elapsed 00:00:01\n","Epoch 1 Done! time elapsed: 00:00:02, base_loss_cur_avg 0.5282\n","cur_auc 0.8793, cur_logloss 0.4503, next_auc 0.8110, next_logloss 0.5309\n","time elapsed 00:00:04\n","\n","test aucs [0.7931901834700298, 0.8050342875503502, 0.77569081868783, 0.8058306387808231, 0.7934098642871424, 0.8110147611403699]\n","average auc 0.7973617589860909\n","\n","test loglosses [0.5577758781666139, 0.5374437732583568, 0.575169055326951, 0.5365459692556691, 0.5514386151552647, 0.5308791764772957]\n","average logloss 0.5482087446066918\n","\n","current period: 30, next period: 31\n","\n","current set size 54365 next set size 54365\n","restored checkpoint: ckpts/SPMF_2_train11-23_test24-30_1epoch_1_0.1_0.001/period29/Epoch1_TestAUC0.8110_TestLOGLOSS0.5309.ckpt\n","INFO:tensorflow:Restoring parameters from ckpts/SPMF_2_train11-23_test24-30_1epoch_1_0.1_0.001/period29/Epoch1_TestAUC0.8110_TestLOGLOSS0.5309.ckpt\n","11-Nov-21 09:33:24 [INFO] : Restoring parameters from ckpts/SPMF_2_train11-23_test24-30_1epoch_1_0.1_0.001/period29/Epoch1_TestAUC0.8110_TestLOGLOSS0.5309.ckpt\n","sampled_reservoir size 5436\n","selected_pos_current_set size 9174\n","selected_neg_cur_set size 9130\n","compute prob and generate train set and update reservoir time elapsed: 00:00:07\n","Training Base Model Epoch 1 Start!\n","[Epoch 1 Batch 1] base_loss_cur 0.6114, time elapsed 00:00:00\n","[Epoch 1 Batch 101] base_loss_cur 0.5288, time elapsed 00:00:00\n","[Epoch 1 Batch 201] base_loss_cur 0.4154, time elapsed 00:00:01\n","Epoch 1 Done! time elapsed: 00:00:02, base_loss_cur_avg 0.5126\n","cur_auc 0.8864, cur_logloss 0.4374, next_auc 0.8011, next_logloss 0.5437\n","time elapsed 00:00:04\n","\n","test aucs [0.7931901834700298, 0.8050342875503502, 0.77569081868783, 0.8058306387808231, 0.7934098642871424, 0.8110147611403699, 0.8010893543631188]\n","average auc 0.7978942726113806\n","\n","test loglosses [0.5577758781666139, 0.5374437732583568, 0.575169055326951, 0.5365459692556691, 0.5514386151552647, 0.5308791764772957, 0.5437131018636775]\n","average logloss 0.5475665099291184\n","11-Nov-21 09:33:39 [INFO] : JOB END: SPMF_MODEL_TRAINING\n"]}]},{"cell_type":"code","metadata":{"collapsed":true,"colab":{"base_uri":"https://localhost:8080/"},"id":"G7LFV1L2SHQ8","executionInfo":{"status":"ok","timestamp":1636623488608,"user_tz":-330,"elapsed":159072,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}},"outputId":"916d9322-47f5-48a4-c541-dbe02f663b3c"},"source":["logger.info('JOB START: INCCTR_MODEL_TRAINING')\n","incctr_sobazaar()\n","logger.info('JOB END: INCCTR_MODEL_TRAINING')"],"execution_count":45,"outputs":[{"output_type":"stream","name":"stdout","text":["11-Nov-21 09:35:31 [INFO] : JOB START: INCCTR_MODEL_TRAINING\n","11-Nov-21 09:35:53 [INFO] : Done loading data! time elapsed: 00:00:22\n","\n","lambda 0.1 base_lr 0.001\n","dir_name:  IncCTR_train11-23_test24-30_1epoch_0.1_0.001\n","\n","current period: 11, next period: 12\n","\n","current set size 54365 next set size 54365\n","restored checkpoint: ckpts/pretrain_train1-10_test11_10epoch_0.001/Epoch2_TestAUC0.8535_TestLOGLOSS0.5567.ckpt\n","INFO:tensorflow:Restoring parameters from ckpts/pretrain_train1-10_test11_10epoch_0.001/Epoch2_TestAUC0.8535_TestLOGLOSS0.5567.ckpt\n","11-Nov-21 09:35:55 [INFO] : Restoring parameters from ckpts/pretrain_train1-10_test11_10epoch_0.001/Epoch2_TestAUC0.8535_TestLOGLOSS0.5567.ckpt\n","Inference Done! time elapsed: 00:00:01\n","Training Base Model Epoch 1 Start!\n","[Epoch 1 Batch 1] base_loss_cur 0.6108, time elapsed 00:00:00\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:136: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame.\n","Try using .loc[row_indexer,col_indexer] = value instead\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"]},{"output_type":"stream","name":"stdout","text":["[Epoch 1 Batch 101] base_loss_cur 0.3518, time elapsed 00:00:00\n","[Epoch 1 Batch 201] base_loss_cur 0.3989, time elapsed 00:00:01\n","Epoch 1 Done! time elapsed: 00:00:01, base_loss_cur_avg 0.4121\n","cur_auc 0.9277, cur_logloss 0.3294, next_auc 0.8024, next_logloss 0.6749\n","time elapsed 00:00:04\n","\n","\n","current period: 12, next period: 13\n","\n","current set size 54365 next set size 54365\n","restored checkpoint: ckpts/IncCTR_train11-23_test24-30_1epoch_0.1_0.001/period11/Epoch1_TestAUC0.8024_TestLOGLOSS0.6749.ckpt\n","INFO:tensorflow:Restoring parameters from ckpts/IncCTR_train11-23_test24-30_1epoch_0.1_0.001/period11/Epoch1_TestAUC0.8024_TestLOGLOSS0.6749.ckpt\n","11-Nov-21 09:36:01 [INFO] : Restoring parameters from ckpts/IncCTR_train11-23_test24-30_1epoch_0.1_0.001/period11/Epoch1_TestAUC0.8024_TestLOGLOSS0.6749.ckpt\n","Inference Done! time elapsed: 00:00:01\n","Training Base Model Epoch 1 Start!\n","[Epoch 1 Batch 1] base_loss_cur 0.7337, time elapsed 00:00:00\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:136: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame.\n","Try using .loc[row_indexer,col_indexer] = value instead\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"]},{"output_type":"stream","name":"stdout","text":["[Epoch 1 Batch 101] base_loss_cur 0.4322, time elapsed 00:00:01\n","[Epoch 1 Batch 201] base_loss_cur 0.4076, time elapsed 00:00:01\n","Epoch 1 Done! time elapsed: 00:00:01, base_loss_cur_avg 0.4551\n","cur_auc 0.9249, cur_logloss 0.3455, next_auc 0.8667, next_logloss 0.4723\n","time elapsed 00:00:05\n","\n","\n","current period: 13, next period: 14\n","\n","current set size 54365 next set size 54365\n","restored checkpoint: ckpts/IncCTR_train11-23_test24-30_1epoch_0.1_0.001/period12/Epoch1_TestAUC0.8667_TestLOGLOSS0.4723.ckpt\n","INFO:tensorflow:Restoring parameters from ckpts/IncCTR_train11-23_test24-30_1epoch_0.1_0.001/period12/Epoch1_TestAUC0.8667_TestLOGLOSS0.4723.ckpt\n","11-Nov-21 09:36:09 [INFO] : Restoring parameters from ckpts/IncCTR_train11-23_test24-30_1epoch_0.1_0.001/period12/Epoch1_TestAUC0.8667_TestLOGLOSS0.4723.ckpt\n","Inference Done! time elapsed: 00:00:01\n","Training Base Model Epoch 1 Start!\n","[Epoch 1 Batch 1] base_loss_cur 0.4363, time elapsed 00:00:00\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:136: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame.\n","Try using .loc[row_indexer,col_indexer] = value instead\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"]},{"output_type":"stream","name":"stdout","text":["[Epoch 1 Batch 101] base_loss_cur 0.3777, time elapsed 00:00:01\n","[Epoch 1 Batch 201] base_loss_cur 0.3216, time elapsed 00:00:01\n","Epoch 1 Done! time elapsed: 00:00:01, base_loss_cur_avg 0.3769\n","cur_auc 0.9618, cur_logloss 0.2590, next_auc 0.8768, next_logloss 0.4561\n","time elapsed 00:00:04\n","\n","\n","current period: 14, next period: 15\n","\n","current set size 54365 next set size 54365\n","restored checkpoint: ckpts/IncCTR_train11-23_test24-30_1epoch_0.1_0.001/period13/Epoch1_TestAUC0.8768_TestLOGLOSS0.4561.ckpt\n","INFO:tensorflow:Restoring parameters from ckpts/IncCTR_train11-23_test24-30_1epoch_0.1_0.001/period13/Epoch1_TestAUC0.8768_TestLOGLOSS0.4561.ckpt\n","11-Nov-21 09:36:15 [INFO] : Restoring parameters from ckpts/IncCTR_train11-23_test24-30_1epoch_0.1_0.001/period13/Epoch1_TestAUC0.8768_TestLOGLOSS0.4561.ckpt\n","Inference Done! time elapsed: 00:00:01\n","Training Base Model Epoch 1 Start!\n","[Epoch 1 Batch 1] base_loss_cur 0.4747, time elapsed 00:00:00\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:136: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame.\n","Try using .loc[row_indexer,col_indexer] = value instead\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"]},{"output_type":"stream","name":"stdout","text":["[Epoch 1 Batch 101] base_loss_cur 0.3308, time elapsed 00:00:01\n","[Epoch 1 Batch 201] base_loss_cur 0.2655, time elapsed 00:00:01\n","Epoch 1 Done! time elapsed: 00:00:02, base_loss_cur_avg 0.3559\n","cur_auc 0.9639, cur_logloss 0.2409, next_auc 0.8948, next_logloss 0.4233\n","time elapsed 00:00:04\n","\n","\n","current period: 15, next period: 16\n","\n","current set size 54365 next set size 54365\n","restored checkpoint: ckpts/IncCTR_train11-23_test24-30_1epoch_0.1_0.001/period14/Epoch1_TestAUC0.8948_TestLOGLOSS0.4233.ckpt\n","INFO:tensorflow:Restoring parameters from ckpts/IncCTR_train11-23_test24-30_1epoch_0.1_0.001/period14/Epoch1_TestAUC0.8948_TestLOGLOSS0.4233.ckpt\n","11-Nov-21 09:36:22 [INFO] : Restoring parameters from ckpts/IncCTR_train11-23_test24-30_1epoch_0.1_0.001/period14/Epoch1_TestAUC0.8948_TestLOGLOSS0.4233.ckpt\n","Inference Done! time elapsed: 00:00:01\n","Training Base Model Epoch 1 Start!\n","[Epoch 1 Batch 1] base_loss_cur 0.3801, time elapsed 00:00:00\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:136: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame.\n","Try using .loc[row_indexer,col_indexer] = value instead\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"]},{"output_type":"stream","name":"stdout","text":["[Epoch 1 Batch 101] base_loss_cur 0.3736, time elapsed 00:00:01\n","[Epoch 1 Batch 201] base_loss_cur 0.2350, time elapsed 00:00:01\n","Epoch 1 Done! time elapsed: 00:00:01, base_loss_cur_avg 0.3519\n","cur_auc 0.9595, cur_logloss 0.2524, next_auc 0.7880, next_logloss 0.7285\n","time elapsed 00:00:04\n","\n","\n","current period: 16, next period: 17\n","\n","current set size 54365 next set size 54365\n","restored checkpoint: ckpts/IncCTR_train11-23_test24-30_1epoch_0.1_0.001/period15/Epoch1_TestAUC0.7880_TestLOGLOSS0.7285.ckpt\n","INFO:tensorflow:Restoring parameters from ckpts/IncCTR_train11-23_test24-30_1epoch_0.1_0.001/period15/Epoch1_TestAUC0.7880_TestLOGLOSS0.7285.ckpt\n","11-Nov-21 09:36:28 [INFO] : Restoring parameters from ckpts/IncCTR_train11-23_test24-30_1epoch_0.1_0.001/period15/Epoch1_TestAUC0.7880_TestLOGLOSS0.7285.ckpt\n","Inference Done! time elapsed: 00:00:01\n","Training Base Model Epoch 1 Start!\n","[Epoch 1 Batch 1] base_loss_cur 0.6457, time elapsed 00:00:00\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:136: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame.\n","Try using .loc[row_indexer,col_indexer] = value instead\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"]},{"output_type":"stream","name":"stdout","text":["[Epoch 1 Batch 101] base_loss_cur 0.4370, time elapsed 00:00:01\n","[Epoch 1 Batch 201] base_loss_cur 0.3459, time elapsed 00:00:01\n","Epoch 1 Done! time elapsed: 00:00:01, base_loss_cur_avg 0.4429\n","cur_auc 0.9480, cur_logloss 0.2979, next_auc 0.8313, next_logloss 0.5272\n","time elapsed 00:00:04\n","\n","\n","current period: 17, next period: 18\n","\n","current set size 54365 next set size 54365\n","restored checkpoint: ckpts/IncCTR_train11-23_test24-30_1epoch_0.1_0.001/period16/Epoch1_TestAUC0.8313_TestLOGLOSS0.5272.ckpt\n","INFO:tensorflow:Restoring parameters from ckpts/IncCTR_train11-23_test24-30_1epoch_0.1_0.001/period16/Epoch1_TestAUC0.8313_TestLOGLOSS0.5272.ckpt\n","11-Nov-21 09:36:35 [INFO] : Restoring parameters from ckpts/IncCTR_train11-23_test24-30_1epoch_0.1_0.001/period16/Epoch1_TestAUC0.8313_TestLOGLOSS0.5272.ckpt\n","Inference Done! time elapsed: 00:00:01\n","Training Base Model Epoch 1 Start!\n","[Epoch 1 Batch 1] base_loss_cur 0.5131, time elapsed 00:00:00\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:136: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame.\n","Try using .loc[row_indexer,col_indexer] = value instead\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"]},{"output_type":"stream","name":"stdout","text":["[Epoch 1 Batch 101] base_loss_cur 0.4290, time elapsed 00:00:01\n","[Epoch 1 Batch 201] base_loss_cur 0.3235, time elapsed 00:00:01\n","Epoch 1 Done! time elapsed: 00:00:01, base_loss_cur_avg 0.4257\n","cur_auc 0.9454, cur_logloss 0.3055, next_auc 0.8388, next_logloss 0.5136\n","time elapsed 00:00:05\n","\n","\n","current period: 18, next period: 19\n","\n","current set size 54365 next set size 54365\n","restored checkpoint: ckpts/IncCTR_train11-23_test24-30_1epoch_0.1_0.001/period17/Epoch1_TestAUC0.8388_TestLOGLOSS0.5136.ckpt\n","INFO:tensorflow:Restoring parameters from ckpts/IncCTR_train11-23_test24-30_1epoch_0.1_0.001/period17/Epoch1_TestAUC0.8388_TestLOGLOSS0.5136.ckpt\n","11-Nov-21 09:36:42 [INFO] : Restoring parameters from ckpts/IncCTR_train11-23_test24-30_1epoch_0.1_0.001/period17/Epoch1_TestAUC0.8388_TestLOGLOSS0.5136.ckpt\n","Inference Done! time elapsed: 00:00:01\n","Training Base Model Epoch 1 Start!\n","[Epoch 1 Batch 1] base_loss_cur 0.5519, time elapsed 00:00:00\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:136: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame.\n","Try using .loc[row_indexer,col_indexer] = value instead\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"]},{"output_type":"stream","name":"stdout","text":["[Epoch 1 Batch 101] base_loss_cur 0.4004, time elapsed 00:00:00\n","[Epoch 1 Batch 201] base_loss_cur 0.4503, time elapsed 00:00:01\n","Epoch 1 Done! time elapsed: 00:00:01, base_loss_cur_avg 0.4251\n","cur_auc 0.9421, cur_logloss 0.3116, next_auc 0.7712, next_logloss 0.6423\n","time elapsed 00:00:04\n","\n","\n","current period: 19, next period: 20\n","\n","current set size 54365 next set size 54365\n","restored checkpoint: ckpts/IncCTR_train11-23_test24-30_1epoch_0.1_0.001/period18/Epoch1_TestAUC0.7712_TestLOGLOSS0.6423.ckpt\n","INFO:tensorflow:Restoring parameters from ckpts/IncCTR_train11-23_test24-30_1epoch_0.1_0.001/period18/Epoch1_TestAUC0.7712_TestLOGLOSS0.6423.ckpt\n","11-Nov-21 09:36:49 [INFO] : Restoring parameters from ckpts/IncCTR_train11-23_test24-30_1epoch_0.1_0.001/period18/Epoch1_TestAUC0.7712_TestLOGLOSS0.6423.ckpt\n","Inference Done! time elapsed: 00:00:01\n","Training Base Model Epoch 1 Start!\n","[Epoch 1 Batch 1] base_loss_cur 0.6295, time elapsed 00:00:00\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:136: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame.\n","Try using .loc[row_indexer,col_indexer] = value instead\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"]},{"output_type":"stream","name":"stdout","text":["[Epoch 1 Batch 101] base_loss_cur 0.3998, time elapsed 00:00:01\n","[Epoch 1 Batch 201] base_loss_cur 0.4599, time elapsed 00:00:01\n","Epoch 1 Done! time elapsed: 00:00:01, base_loss_cur_avg 0.4832\n","cur_auc 0.9262, cur_logloss 0.3539, next_auc 0.8322, next_logloss 0.5116\n","time elapsed 00:00:04\n","\n","\n","current period: 20, next period: 21\n","\n","current set size 54365 next set size 54365\n","restored checkpoint: ckpts/IncCTR_train11-23_test24-30_1epoch_0.1_0.001/period19/Epoch1_TestAUC0.8322_TestLOGLOSS0.5116.ckpt\n","INFO:tensorflow:Restoring parameters from ckpts/IncCTR_train11-23_test24-30_1epoch_0.1_0.001/period19/Epoch1_TestAUC0.8322_TestLOGLOSS0.5116.ckpt\n","11-Nov-21 09:36:56 [INFO] : Restoring parameters from ckpts/IncCTR_train11-23_test24-30_1epoch_0.1_0.001/period19/Epoch1_TestAUC0.8322_TestLOGLOSS0.5116.ckpt\n","Inference Done! time elapsed: 00:00:01\n","Training Base Model Epoch 1 Start!\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:136: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame.\n","Try using .loc[row_indexer,col_indexer] = value instead\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"]},{"output_type":"stream","name":"stdout","text":["[Epoch 1 Batch 1] base_loss_cur 0.4785, time elapsed 00:00:00\n","[Epoch 1 Batch 101] base_loss_cur 0.3758, time elapsed 00:00:01\n","[Epoch 1 Batch 201] base_loss_cur 0.3611, time elapsed 00:00:01\n","Epoch 1 Done! time elapsed: 00:00:01, base_loss_cur_avg 0.4261\n","cur_auc 0.9424, cur_logloss 0.3151, next_auc 0.8183, next_logloss 0.5510\n","time elapsed 00:00:04\n","\n","\n","current period: 21, next period: 22\n","\n","current set size 54365 next set size 54365\n","restored checkpoint: ckpts/IncCTR_train11-23_test24-30_1epoch_0.1_0.001/period20/Epoch1_TestAUC0.8183_TestLOGLOSS0.5510.ckpt\n","INFO:tensorflow:Restoring parameters from ckpts/IncCTR_train11-23_test24-30_1epoch_0.1_0.001/period20/Epoch1_TestAUC0.8183_TestLOGLOSS0.5510.ckpt\n","11-Nov-21 09:37:02 [INFO] : Restoring parameters from ckpts/IncCTR_train11-23_test24-30_1epoch_0.1_0.001/period20/Epoch1_TestAUC0.8183_TestLOGLOSS0.5510.ckpt\n","Inference Done! time elapsed: 00:00:01\n","Training Base Model Epoch 1 Start!\n","[Epoch 1 Batch 1] base_loss_cur 0.4580, time elapsed 00:00:00\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:136: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame.\n","Try using .loc[row_indexer,col_indexer] = value instead\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"]},{"output_type":"stream","name":"stdout","text":["[Epoch 1 Batch 101] base_loss_cur 0.4443, time elapsed 00:00:01\n","[Epoch 1 Batch 201] base_loss_cur 0.3934, time elapsed 00:00:01\n","Epoch 1 Done! time elapsed: 00:00:01, base_loss_cur_avg 0.4481\n","cur_auc 0.9330, cur_logloss 0.3378, next_auc 0.8198, next_logloss 0.5354\n","time elapsed 00:00:04\n","\n","\n","current period: 22, next period: 23\n","\n","current set size 54365 next set size 54365\n","restored checkpoint: ckpts/IncCTR_train11-23_test24-30_1epoch_0.1_0.001/period21/Epoch1_TestAUC0.8198_TestLOGLOSS0.5354.ckpt\n","INFO:tensorflow:Restoring parameters from ckpts/IncCTR_train11-23_test24-30_1epoch_0.1_0.001/period21/Epoch1_TestAUC0.8198_TestLOGLOSS0.5354.ckpt\n","11-Nov-21 09:37:09 [INFO] : Restoring parameters from ckpts/IncCTR_train11-23_test24-30_1epoch_0.1_0.001/period21/Epoch1_TestAUC0.8198_TestLOGLOSS0.5354.ckpt\n","Inference Done! time elapsed: 00:00:01\n","Training Base Model Epoch 1 Start!\n","[Epoch 1 Batch 1] base_loss_cur 0.5454, time elapsed 00:00:00\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:136: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame.\n","Try using .loc[row_indexer,col_indexer] = value instead\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"]},{"output_type":"stream","name":"stdout","text":["[Epoch 1 Batch 101] base_loss_cur 0.4759, time elapsed 00:00:01\n","[Epoch 1 Batch 201] base_loss_cur 0.4386, time elapsed 00:00:01\n","Epoch 1 Done! time elapsed: 00:00:01, base_loss_cur_avg 0.4620\n","cur_auc 0.9258, cur_logloss 0.3576, next_auc 0.8093, next_logloss 0.5477\n","time elapsed 00:00:05\n","\n","\n","current period: 23, next period: 24\n","\n","current set size 54365 next set size 54365\n","restored checkpoint: ckpts/IncCTR_train11-23_test24-30_1epoch_0.1_0.001/period22/Epoch1_TestAUC0.8093_TestLOGLOSS0.5477.ckpt\n","INFO:tensorflow:Restoring parameters from ckpts/IncCTR_train11-23_test24-30_1epoch_0.1_0.001/period22/Epoch1_TestAUC0.8093_TestLOGLOSS0.5477.ckpt\n","11-Nov-21 09:37:16 [INFO] : Restoring parameters from ckpts/IncCTR_train11-23_test24-30_1epoch_0.1_0.001/period22/Epoch1_TestAUC0.8093_TestLOGLOSS0.5477.ckpt\n","Inference Done! time elapsed: 00:00:01\n","Training Base Model Epoch 1 Start!\n","[Epoch 1 Batch 1] base_loss_cur 0.6210, time elapsed 00:00:00\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:136: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame.\n","Try using .loc[row_indexer,col_indexer] = value instead\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"]},{"output_type":"stream","name":"stdout","text":["[Epoch 1 Batch 101] base_loss_cur 0.4136, time elapsed 00:00:01\n","[Epoch 1 Batch 201] base_loss_cur 0.4538, time elapsed 00:00:01\n","Epoch 1 Done! time elapsed: 00:00:01, base_loss_cur_avg 0.4715\n","cur_auc 0.9210, cur_logloss 0.3670, next_auc 0.8330, next_logloss 0.5103\n","time elapsed 00:00:04\n","\n","\n","current period: 24, next period: 25\n","\n","current set size 54365 next set size 54365\n","restored checkpoint: ckpts/IncCTR_train11-23_test24-30_1epoch_0.1_0.001/period23/Epoch1_TestAUC0.8330_TestLOGLOSS0.5103.ckpt\n","INFO:tensorflow:Restoring parameters from ckpts/IncCTR_train11-23_test24-30_1epoch_0.1_0.001/period23/Epoch1_TestAUC0.8330_TestLOGLOSS0.5103.ckpt\n","11-Nov-21 09:37:23 [INFO] : Restoring parameters from ckpts/IncCTR_train11-23_test24-30_1epoch_0.1_0.001/period23/Epoch1_TestAUC0.8330_TestLOGLOSS0.5103.ckpt\n","Inference Done! time elapsed: 00:00:01\n","Training Base Model Epoch 1 Start!\n","[Epoch 1 Batch 1] base_loss_cur 0.5012, time elapsed 00:00:00\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:136: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame.\n","Try using .loc[row_indexer,col_indexer] = value instead\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"]},{"output_type":"stream","name":"stdout","text":["[Epoch 1 Batch 101] base_loss_cur 0.4440, time elapsed 00:00:01\n","[Epoch 1 Batch 201] base_loss_cur 0.4436, time elapsed 00:00:01\n","Epoch 1 Done! time elapsed: 00:00:01, base_loss_cur_avg 0.4577\n","cur_auc 0.9217, cur_logloss 0.3630, next_auc 0.7946, next_logloss 0.5716\n","time elapsed 00:00:04\n","\n","test aucs [0.7946248806920454]\n","average auc 0.7946248806920454\n","\n","test loglosses [0.5715699686791471]\n","average logloss 0.5715699686791471\n","\n","current period: 25, next period: 26\n","\n","current set size 54365 next set size 54365\n","restored checkpoint: ckpts/IncCTR_train11-23_test24-30_1epoch_0.1_0.001/period24/Epoch1_TestAUC0.7946_TestLOGLOSS0.5716.ckpt\n","INFO:tensorflow:Restoring parameters from ckpts/IncCTR_train11-23_test24-30_1epoch_0.1_0.001/period24/Epoch1_TestAUC0.7946_TestLOGLOSS0.5716.ckpt\n","11-Nov-21 09:37:29 [INFO] : Restoring parameters from ckpts/IncCTR_train11-23_test24-30_1epoch_0.1_0.001/period24/Epoch1_TestAUC0.7946_TestLOGLOSS0.5716.ckpt\n","Inference Done! time elapsed: 00:00:01\n","Training Base Model Epoch 1 Start!\n","[Epoch 1 Batch 1] base_loss_cur 0.5406, time elapsed 00:00:00\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:136: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame.\n","Try using .loc[row_indexer,col_indexer] = value instead\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"]},{"output_type":"stream","name":"stdout","text":["[Epoch 1 Batch 101] base_loss_cur 0.4948, time elapsed 00:00:01\n","[Epoch 1 Batch 201] base_loss_cur 0.4321, time elapsed 00:00:01\n","Epoch 1 Done! time elapsed: 00:00:01, base_loss_cur_avg 0.4951\n","cur_auc 0.9072, cur_logloss 0.3966, next_auc 0.8054, next_logloss 0.5466\n","time elapsed 00:00:04\n","\n","test aucs [0.7946248806920454, 0.8054429244140696]\n","average auc 0.8000339025530575\n","\n","test loglosses [0.5715699686791471, 0.5465883919764487]\n","average logloss 0.5590791803277979\n","\n","current period: 26, next period: 27\n","\n","current set size 54365 next set size 54365\n","restored checkpoint: ckpts/IncCTR_train11-23_test24-30_1epoch_0.1_0.001/period25/Epoch1_TestAUC0.8054_TestLOGLOSS0.5466.ckpt\n","INFO:tensorflow:Restoring parameters from ckpts/IncCTR_train11-23_test24-30_1epoch_0.1_0.001/period25/Epoch1_TestAUC0.8054_TestLOGLOSS0.5466.ckpt\n","11-Nov-21 09:37:35 [INFO] : Restoring parameters from ckpts/IncCTR_train11-23_test24-30_1epoch_0.1_0.001/period25/Epoch1_TestAUC0.8054_TestLOGLOSS0.5466.ckpt\n","Inference Done! time elapsed: 00:00:01\n","Training Base Model Epoch 1 Start!\n","[Epoch 1 Batch 1] base_loss_cur 0.4921, time elapsed 00:00:00\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:136: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame.\n","Try using .loc[row_indexer,col_indexer] = value instead\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"]},{"output_type":"stream","name":"stdout","text":["[Epoch 1 Batch 101] base_loss_cur 0.4632, time elapsed 00:00:00\n","[Epoch 1 Batch 201] base_loss_cur 0.4447, time elapsed 00:00:01\n","Epoch 1 Done! time elapsed: 00:00:01, base_loss_cur_avg 0.4704\n","cur_auc 0.9142, cur_logloss 0.3813, next_auc 0.7787, next_logloss 0.5831\n","time elapsed 00:00:05\n","\n","test aucs [0.7946248806920454, 0.8054429244140696, 0.7786713435954691]\n","average auc 0.7929130495671947\n","\n","test loglosses [0.5715699686791471, 0.5465883919764487, 0.5831081405705112]\n","average logloss 0.5670888337420357\n","\n","current period: 27, next period: 28\n","\n","current set size 54365 next set size 54365\n","restored checkpoint: ckpts/IncCTR_train11-23_test24-30_1epoch_0.1_0.001/period26/Epoch1_TestAUC0.7787_TestLOGLOSS0.5831.ckpt\n","INFO:tensorflow:Restoring parameters from ckpts/IncCTR_train11-23_test24-30_1epoch_0.1_0.001/period26/Epoch1_TestAUC0.7787_TestLOGLOSS0.5831.ckpt\n","11-Nov-21 09:37:43 [INFO] : Restoring parameters from ckpts/IncCTR_train11-23_test24-30_1epoch_0.1_0.001/period26/Epoch1_TestAUC0.7787_TestLOGLOSS0.5831.ckpt\n","Inference Done! time elapsed: 00:00:01\n","Training Base Model Epoch 1 Start!\n","[Epoch 1 Batch 1] base_loss_cur 0.5095, time elapsed 00:00:00\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:136: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame.\n","Try using .loc[row_indexer,col_indexer] = value instead\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"]},{"output_type":"stream","name":"stdout","text":["[Epoch 1 Batch 101] base_loss_cur 0.5123, time elapsed 00:00:01\n","[Epoch 1 Batch 201] base_loss_cur 0.4601, time elapsed 00:00:01\n","Epoch 1 Done! time elapsed: 00:00:01, base_loss_cur_avg 0.5027\n","cur_auc 0.9053, cur_logloss 0.4049, next_auc 0.8103, next_logloss 0.5359\n","time elapsed 00:00:04\n","\n","test aucs [0.7946248806920454, 0.8054429244140696, 0.7786713435954691, 0.8102949400590483]\n","average auc 0.797258522190158\n","\n","test loglosses [0.5715699686791471, 0.5465883919764487, 0.5831081405705112, 0.5358890365127177]\n","average logloss 0.5592888844347061\n","\n","current period: 28, next period: 29\n","\n","current set size 54365 next set size 54365\n","restored checkpoint: ckpts/IncCTR_train11-23_test24-30_1epoch_0.1_0.001/period27/Epoch1_TestAUC0.8103_TestLOGLOSS0.5359.ckpt\n","INFO:tensorflow:Restoring parameters from ckpts/IncCTR_train11-23_test24-30_1epoch_0.1_0.001/period27/Epoch1_TestAUC0.8103_TestLOGLOSS0.5359.ckpt\n","11-Nov-21 09:37:50 [INFO] : Restoring parameters from ckpts/IncCTR_train11-23_test24-30_1epoch_0.1_0.001/period27/Epoch1_TestAUC0.8103_TestLOGLOSS0.5359.ckpt\n","Inference Done! time elapsed: 00:00:01\n","Training Base Model Epoch 1 Start!\n","[Epoch 1 Batch 1] base_loss_cur 0.5161, time elapsed 00:00:00\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:136: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame.\n","Try using .loc[row_indexer,col_indexer] = value instead\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"]},{"output_type":"stream","name":"stdout","text":["[Epoch 1 Batch 101] base_loss_cur 0.4377, time elapsed 00:00:01\n","[Epoch 1 Batch 201] base_loss_cur 0.4483, time elapsed 00:00:01\n","Epoch 1 Done! time elapsed: 00:00:01, base_loss_cur_avg 0.4885\n","cur_auc 0.9082, cur_logloss 0.3984, next_auc 0.7960, next_logloss 0.5541\n","time elapsed 00:00:04\n","\n","test aucs [0.7946248806920454, 0.8054429244140696, 0.7786713435954691, 0.8102949400590483, 0.7960391114811731]\n","average auc 0.7970146400483611\n","\n","test loglosses [0.5715699686791471, 0.5465883919764487, 0.5831081405705112, 0.5358890365127177, 0.5541467515375439]\n","average logloss 0.5582604578552737\n","\n","current period: 29, next period: 30\n","\n","current set size 54365 next set size 54365\n","restored checkpoint: ckpts/IncCTR_train11-23_test24-30_1epoch_0.1_0.001/period28/Epoch1_TestAUC0.7960_TestLOGLOSS0.5541.ckpt\n","INFO:tensorflow:Restoring parameters from ckpts/IncCTR_train11-23_test24-30_1epoch_0.1_0.001/period28/Epoch1_TestAUC0.7960_TestLOGLOSS0.5541.ckpt\n","11-Nov-21 09:37:56 [INFO] : Restoring parameters from ckpts/IncCTR_train11-23_test24-30_1epoch_0.1_0.001/period28/Epoch1_TestAUC0.7960_TestLOGLOSS0.5541.ckpt\n","Inference Done! time elapsed: 00:00:01\n","Training Base Model Epoch 1 Start!\n","[Epoch 1 Batch 1] base_loss_cur 0.6168, time elapsed 00:00:00\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:136: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame.\n","Try using .loc[row_indexer,col_indexer] = value instead\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"]},{"output_type":"stream","name":"stdout","text":["[Epoch 1 Batch 101] base_loss_cur 0.5403, time elapsed 00:00:00\n","[Epoch 1 Batch 201] base_loss_cur 0.4435, time elapsed 00:00:01\n","Epoch 1 Done! time elapsed: 00:00:01, base_loss_cur_avg 0.5002\n","cur_auc 0.9047, cur_logloss 0.4047, next_auc 0.8140, next_logloss 0.5314\n","time elapsed 00:00:04\n","\n","test aucs [0.7946248806920454, 0.8054429244140696, 0.7786713435954691, 0.8102949400590483, 0.7960391114811731, 0.8139521239086794]\n","average auc 0.7998375540250807\n","\n","test loglosses [0.5715699686791471, 0.5465883919764487, 0.5831081405705112, 0.5358890365127177, 0.5541467515375439, 0.5314080350701775]\n","average logloss 0.5537850540577577\n","\n","current period: 30, next period: 31\n","\n","current set size 54365 next set size 54365\n","restored checkpoint: ckpts/IncCTR_train11-23_test24-30_1epoch_0.1_0.001/period29/Epoch1_TestAUC0.8140_TestLOGLOSS0.5314.ckpt\n","INFO:tensorflow:Restoring parameters from ckpts/IncCTR_train11-23_test24-30_1epoch_0.1_0.001/period29/Epoch1_TestAUC0.8140_TestLOGLOSS0.5314.ckpt\n","11-Nov-21 09:38:03 [INFO] : Restoring parameters from ckpts/IncCTR_train11-23_test24-30_1epoch_0.1_0.001/period29/Epoch1_TestAUC0.8140_TestLOGLOSS0.5314.ckpt\n","Inference Done! time elapsed: 00:00:01\n","Training Base Model Epoch 1 Start!\n","[Epoch 1 Batch 1] base_loss_cur 0.4854, time elapsed 00:00:00\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:136: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame.\n","Try using .loc[row_indexer,col_indexer] = value instead\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"]},{"output_type":"stream","name":"stdout","text":["[Epoch 1 Batch 101] base_loss_cur 0.4747, time elapsed 00:00:00\n","[Epoch 1 Batch 201] base_loss_cur 0.4304, time elapsed 00:00:01\n","Epoch 1 Done! time elapsed: 00:00:01, base_loss_cur_avg 0.4840\n","cur_auc 0.9112, cur_logloss 0.3914, next_auc 0.8053, next_logloss 0.5437\n","time elapsed 00:00:04\n","\n","test aucs [0.7946248806920454, 0.8054429244140696, 0.7786713435954691, 0.8102949400590483, 0.7960391114811731, 0.8139521239086794, 0.8052611581052571]\n","average auc 0.800612354607963\n","\n","test loglosses [0.5715699686791471, 0.5465883919764487, 0.5831081405705112, 0.5358890365127177, 0.5541467515375439, 0.5314080350701775, 0.543682206829815]\n","average logloss 0.5523417901680515\n","11-Nov-21 09:38:09 [INFO] : JOB END: INCCTR_MODEL_TRAINING\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"S5HsNVFXYOjE","executionInfo":{"status":"ok","timestamp":1636624812859,"user_tz":-330,"elapsed":823665,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}},"outputId":"ee691545-f06e-4e9f-e9d2-44b4a0413802"},"source":["logger.info('JOB START: SML_MODEL_TRAINING')\n","sml_sobazaar()\n","logger.info('JOB END: SML_MODEL_TRAINING')"],"execution_count":57,"outputs":[{"output_type":"stream","name":"stdout","text":["11-Nov-21 09:46:30 [INFO] : JOB START: SML_MODEL_TRAINING\n","11-Nov-21 09:46:52 [INFO] : Done loading data! time elapsed: 00:00:22\n","\n","transfer_lr 0.001 base_lr 0.01\n","dir_name:  SML_emb&mlp_train11-23_test24-30_1epoch_1epoch_0.001_0.01\n","\n","current period: 11, next period: 12\n","\n","current set size 54365 next set size 54365\n","restored checkpoint: ckpts/pretrain_train1-10_test11_10epoch_0.001/Epoch2_TestAUC0.8535_TestLOGLOSS0.5567.ckpt\n","collect params time elapsed: 00:00:00\n","INFO:tensorflow:Restoring parameters from ckpts/pretrain_train1-10_test11_10epoch_0.001/Epoch2_TestAUC0.8535_TestLOGLOSS0.5567.ckpt\n","11-Nov-21 09:46:57 [INFO] : Restoring parameters from ckpts/pretrain_train1-10_test11_10epoch_0.001/Epoch2_TestAUC0.8535_TestLOGLOSS0.5567.ckpt\n","Training Base Model Epoch 1 Start!\n","[Epoch 1 Batch 1] base_loss_cur 0.7986, time elapsed 00:00:05\n","[Epoch 1 Batch 101] base_loss_cur 0.6944, time elapsed 00:00:07\n","[Epoch 1 Batch 201] base_loss_cur 0.6928, time elapsed 00:00:10\n","Epoch 1 Done! time elapsed: 00:00:10, base_loss_cur_avg 0.7086\n","cur_auc 0.5656, cur_logloss 0.6931, next_auc 0.6318, next_logloss 0.6931\n","time elapsed 00:00:16\n","\n","Training Transfer Module Epoch 1 Start!\n","[Epoch 1 Batch 1] transfer_loss_next 0.6929, time elapsed 00:00:00\n","next_auc 0.6410, next_logloss 0.6960\n","time elapsed 00:00:03\n","\n","[Epoch 1 Batch 101] transfer_loss_next 0.5588, time elapsed 00:00:07\n","next_auc 0.7752, next_logloss 0.5728\n","time elapsed 00:00:09\n","\n","[Epoch 1 Batch 201] transfer_loss_next 0.5485, time elapsed 00:00:13\n","next_auc 0.7805, next_logloss 0.5584\n","time elapsed 00:00:15\n","\n","Epoch 1 Done! time elapsed: 00:00:16, transfer_loss_next_avg 0.6003\n","cur_auc 0.8551, cur_logloss 0.4996, next_auc 0.7811, next_logloss 0.5616\n","time elapsed 00:00:21\n","\n","\n","current period: 12, next period: 13\n","\n","current set size 54365 next set size 54365\n","restored checkpoint: ckpts/SML_emb&mlp_train11-23_test24-30_1epoch_1epoch_0.001_0.01/period11/Epoch1_TestAUC0.7811_TestLOGLOSS0.5616.ckpt\n","collect params time elapsed: 00:00:00\n","INFO:tensorflow:Restoring parameters from ckpts/SML_emb&mlp_train11-23_test24-30_1epoch_1epoch_0.001_0.01/period11/Epoch1_TestAUC0.7811_TestLOGLOSS0.5616.ckpt\n","11-Nov-21 09:47:40 [INFO] : Restoring parameters from ckpts/SML_emb&mlp_train11-23_test24-30_1epoch_1epoch_0.001_0.01/period11/Epoch1_TestAUC0.7811_TestLOGLOSS0.5616.ckpt\n","Training Base Model Epoch 1 Start!\n","[Epoch 1 Batch 1] base_loss_cur 0.7361, time elapsed 00:00:00\n","[Epoch 1 Batch 101] base_loss_cur 0.6798, time elapsed 00:00:03\n","[Epoch 1 Batch 201] base_loss_cur 0.6455, time elapsed 00:00:06\n","Epoch 1 Done! time elapsed: 00:00:06, base_loss_cur_avg 0.6875\n","cur_auc 0.8444, cur_logloss 0.6068, next_auc 0.6268, next_logloss 0.6582\n","time elapsed 00:00:14\n","\n","Training Transfer Module Epoch 1 Start!\n","[Epoch 1 Batch 1] transfer_loss_next 0.6588, time elapsed 00:00:00\n","next_auc 0.6304, next_logloss 0.6578\n","time elapsed 00:00:03\n","\n","[Epoch 1 Batch 101] transfer_loss_next 0.4883, time elapsed 00:00:07\n","next_auc 0.8621, next_logloss 0.4675\n","time elapsed 00:00:09\n","\n","[Epoch 1 Batch 201] transfer_loss_next 0.4863, time elapsed 00:00:13\n","next_auc 0.8661, next_logloss 0.4614\n","time elapsed 00:00:15\n","\n","Epoch 1 Done! time elapsed: 00:00:16, transfer_loss_next_avg 0.4877\n","cur_auc 0.9412, cur_logloss 0.3440, next_auc 0.8669, next_logloss 0.4693\n","time elapsed 00:00:21\n","\n","\n","current period: 13, next period: 14\n","\n","current set size 54365 next set size 54365\n","restored checkpoint: ckpts/SML_emb&mlp_train11-23_test24-30_1epoch_1epoch_0.001_0.01/period12/Epoch1_TestAUC0.8669_TestLOGLOSS0.4693.ckpt\n","collect params time elapsed: 00:00:00\n","INFO:tensorflow:Restoring parameters from ckpts/SML_emb&mlp_train11-23_test24-30_1epoch_1epoch_0.001_0.01/period12/Epoch1_TestAUC0.8669_TestLOGLOSS0.4693.ckpt\n","11-Nov-21 09:48:20 [INFO] : Restoring parameters from ckpts/SML_emb&mlp_train11-23_test24-30_1epoch_1epoch_0.001_0.01/period12/Epoch1_TestAUC0.8669_TestLOGLOSS0.4693.ckpt\n","Training Base Model Epoch 1 Start!\n","[Epoch 1 Batch 1] base_loss_cur 0.7207, time elapsed 00:00:00\n","[Epoch 1 Batch 101] base_loss_cur 0.4074, time elapsed 00:00:03\n","[Epoch 1 Batch 201] base_loss_cur 0.3397, time elapsed 00:00:06\n","Epoch 1 Done! time elapsed: 00:00:06, base_loss_cur_avg 0.3913\n","cur_auc 0.9674, cur_logloss 0.2336, next_auc 0.8513, next_logloss 0.4990\n","time elapsed 00:00:12\n","\n","Training Transfer Module Epoch 1 Start!\n","[Epoch 1 Batch 1] transfer_loss_next 0.3947, time elapsed 00:00:00\n","next_auc 0.8513, next_logloss 0.4977\n","time elapsed 00:00:03\n","\n","[Epoch 1 Batch 101] transfer_loss_next 0.4808, time elapsed 00:00:07\n","next_auc 0.8648, next_logloss 0.4617\n","time elapsed 00:00:09\n","\n","[Epoch 1 Batch 201] transfer_loss_next 0.4477, time elapsed 00:00:13\n","next_auc 0.8677, next_logloss 0.4562\n","time elapsed 00:00:15\n","\n","Epoch 1 Done! time elapsed: 00:00:16, transfer_loss_next_avg 0.4646\n","cur_auc 0.9623, cur_logloss 0.2755, next_auc 0.8678, next_logloss 0.4572\n","time elapsed 00:00:21\n","\n","\n","current period: 14, next period: 15\n","\n","current set size 54365 next set size 54365\n","restored checkpoint: ckpts/SML_emb&mlp_train11-23_test24-30_1epoch_1epoch_0.001_0.01/period13/Epoch1_TestAUC0.8678_TestLOGLOSS0.4572.ckpt\n","collect params time elapsed: 00:00:00\n","INFO:tensorflow:Restoring parameters from ckpts/SML_emb&mlp_train11-23_test24-30_1epoch_1epoch_0.001_0.01/period13/Epoch1_TestAUC0.8678_TestLOGLOSS0.4572.ckpt\n","11-Nov-21 09:48:59 [INFO] : Restoring parameters from ckpts/SML_emb&mlp_train11-23_test24-30_1epoch_1epoch_0.001_0.01/period13/Epoch1_TestAUC0.8678_TestLOGLOSS0.4572.ckpt\n","Training Base Model Epoch 1 Start!\n","[Epoch 1 Batch 1] base_loss_cur 0.6464, time elapsed 00:00:00\n","[Epoch 1 Batch 101] base_loss_cur 0.3914, time elapsed 00:00:03\n","[Epoch 1 Batch 201] base_loss_cur 0.3627, time elapsed 00:00:06\n","Epoch 1 Done! time elapsed: 00:00:06, base_loss_cur_avg 0.4016\n","cur_auc 0.9535, cur_logloss 0.2841, next_auc 0.8879, next_logloss 0.4261\n","time elapsed 00:00:12\n","\n","Training Transfer Module Epoch 1 Start!\n","[Epoch 1 Batch 1] transfer_loss_next 0.4212, time elapsed 00:00:00\n","next_auc 0.8879, next_logloss 0.4258\n","time elapsed 00:00:03\n","\n","[Epoch 1 Batch 101] transfer_loss_next 0.4199, time elapsed 00:00:07\n","next_auc 0.8889, next_logloss 0.4221\n","time elapsed 00:00:11\n","\n","[Epoch 1 Batch 201] transfer_loss_next 0.4106, time elapsed 00:00:15\n","next_auc 0.8893, next_logloss 0.4208\n","time elapsed 00:00:17\n","\n","Epoch 1 Done! time elapsed: 00:00:18, transfer_loss_next_avg 0.4228\n","cur_auc 0.9503, cur_logloss 0.3065, next_auc 0.8893, next_logloss 0.4217\n","time elapsed 00:00:23\n","\n","\n","current period: 15, next period: 16\n","\n","current set size 54365 next set size 54365\n","restored checkpoint: ckpts/SML_emb&mlp_train11-23_test24-30_1epoch_1epoch_0.001_0.01/period14/Epoch1_TestAUC0.8893_TestLOGLOSS0.4217.ckpt\n","collect params time elapsed: 00:00:00\n","INFO:tensorflow:Restoring parameters from ckpts/SML_emb&mlp_train11-23_test24-30_1epoch_1epoch_0.001_0.01/period14/Epoch1_TestAUC0.8893_TestLOGLOSS0.4217.ckpt\n","11-Nov-21 09:49:40 [INFO] : Restoring parameters from ckpts/SML_emb&mlp_train11-23_test24-30_1epoch_1epoch_0.001_0.01/period14/Epoch1_TestAUC0.8893_TestLOGLOSS0.4217.ckpt\n","Training Base Model Epoch 1 Start!\n","[Epoch 1 Batch 1] base_loss_cur 0.6896, time elapsed 00:00:00\n","[Epoch 1 Batch 101] base_loss_cur 0.3843, time elapsed 00:00:03\n","[Epoch 1 Batch 201] base_loss_cur 0.3698, time elapsed 00:00:06\n","Epoch 1 Done! time elapsed: 00:00:06, base_loss_cur_avg 0.4010\n","cur_auc 0.9439, cur_logloss 0.3048, next_auc 0.7924, next_logloss 0.5907\n","time elapsed 00:00:12\n","\n","Training Transfer Module Epoch 1 Start!\n","[Epoch 1 Batch 1] transfer_loss_next 0.5837, time elapsed 00:00:00\n","next_auc 0.7926, next_logloss 0.5818\n","time elapsed 00:00:03\n","\n","[Epoch 1 Batch 101] transfer_loss_next 0.5109, time elapsed 00:00:06\n","next_auc 0.8086, next_logloss 0.5286\n","time elapsed 00:00:09\n","\n","[Epoch 1 Batch 201] transfer_loss_next 0.4905, time elapsed 00:00:13\n","next_auc 0.8095, next_logloss 0.5322\n","time elapsed 00:00:15\n","\n","Epoch 1 Done! time elapsed: 00:00:16, transfer_loss_next_avg 0.5326\n","cur_auc 0.9417, cur_logloss 0.3527, next_auc 0.8104, next_logloss 0.5267\n","time elapsed 00:00:21\n","\n","\n","current period: 16, next period: 17\n","\n","current set size 54365 next set size 54365\n","restored checkpoint: ckpts/SML_emb&mlp_train11-23_test24-30_1epoch_1epoch_0.001_0.01/period15/Epoch1_TestAUC0.8104_TestLOGLOSS0.5267.ckpt\n","collect params time elapsed: 00:00:00\n","INFO:tensorflow:Restoring parameters from ckpts/SML_emb&mlp_train11-23_test24-30_1epoch_1epoch_0.001_0.01/period15/Epoch1_TestAUC0.8104_TestLOGLOSS0.5267.ckpt\n","11-Nov-21 09:50:19 [INFO] : Restoring parameters from ckpts/SML_emb&mlp_train11-23_test24-30_1epoch_1epoch_0.001_0.01/period15/Epoch1_TestAUC0.8104_TestLOGLOSS0.5267.ckpt\n","Training Base Model Epoch 1 Start!\n","[Epoch 1 Batch 1] base_loss_cur 0.6934, time elapsed 00:00:00\n","[Epoch 1 Batch 101] base_loss_cur 0.4979, time elapsed 00:00:03\n","[Epoch 1 Batch 201] base_loss_cur 0.4402, time elapsed 00:00:06\n","Epoch 1 Done! time elapsed: 00:00:06, base_loss_cur_avg 0.5273\n","cur_auc 0.8748, cur_logloss 0.4616, next_auc 0.8004, next_logloss 0.5434\n","time elapsed 00:00:12\n","\n","Training Transfer Module Epoch 1 Start!\n","[Epoch 1 Batch 1] transfer_loss_next 0.5478, time elapsed 00:00:00\n","next_auc 0.8003, next_logloss 0.5439\n","time elapsed 00:00:03\n","\n","[Epoch 1 Batch 101] transfer_loss_next 0.5265, time elapsed 00:00:07\n","next_auc 0.8236, next_logloss 0.5188\n","time elapsed 00:00:10\n","\n","[Epoch 1 Batch 201] transfer_loss_next 0.5433, time elapsed 00:00:13\n","next_auc 0.8280, next_logloss 0.5148\n","time elapsed 00:00:18\n","\n","Epoch 1 Done! time elapsed: 00:00:18, transfer_loss_next_avg 0.5183\n","cur_auc 0.9237, cur_logloss 0.3763, next_auc 0.8275, next_logloss 0.5163\n","time elapsed 00:00:23\n","\n","\n","current period: 17, next period: 18\n","\n","current set size 54365 next set size 54365\n","restored checkpoint: ckpts/SML_emb&mlp_train11-23_test24-30_1epoch_1epoch_0.001_0.01/period16/Epoch1_TestAUC0.8275_TestLOGLOSS0.5163.ckpt\n","collect params time elapsed: 00:00:00\n","INFO:tensorflow:Restoring parameters from ckpts/SML_emb&mlp_train11-23_test24-30_1epoch_1epoch_0.001_0.01/period16/Epoch1_TestAUC0.8275_TestLOGLOSS0.5163.ckpt\n","11-Nov-21 09:51:00 [INFO] : Restoring parameters from ckpts/SML_emb&mlp_train11-23_test24-30_1epoch_1epoch_0.001_0.01/period16/Epoch1_TestAUC0.8275_TestLOGLOSS0.5163.ckpt\n","Training Base Model Epoch 1 Start!\n","[Epoch 1 Batch 1] base_loss_cur 0.7361, time elapsed 00:00:00\n","[Epoch 1 Batch 101] base_loss_cur 0.4262, time elapsed 00:00:03\n","[Epoch 1 Batch 201] base_loss_cur 0.4025, time elapsed 00:00:06\n","Epoch 1 Done! time elapsed: 00:00:06, base_loss_cur_avg 0.4707\n","cur_auc 0.9412, cur_logloss 0.3500, next_auc 0.8284, next_logloss 0.5107\n","time elapsed 00:00:12\n","\n","Training Transfer Module Epoch 1 Start!\n","[Epoch 1 Batch 1] transfer_loss_next 0.4867, time elapsed 00:00:00\n","next_auc 0.8282, next_logloss 0.5106\n","time elapsed 00:00:03\n","\n","[Epoch 1 Batch 101] transfer_loss_next 0.4401, time elapsed 00:00:07\n","next_auc 0.8358, next_logloss 0.5017\n","time elapsed 00:00:09\n","\n","[Epoch 1 Batch 201] transfer_loss_next 0.5008, time elapsed 00:00:13\n","next_auc 0.8369, next_logloss 0.5001\n","time elapsed 00:00:15\n","\n","Epoch 1 Done! time elapsed: 00:00:16, transfer_loss_next_avg 0.5046\n","cur_auc 0.9348, cur_logloss 0.3473, next_auc 0.8377, next_logloss 0.4997\n","time elapsed 00:00:21\n","\n","\n","current period: 18, next period: 19\n","\n","current set size 54365 next set size 54365\n","restored checkpoint: ckpts/SML_emb&mlp_train11-23_test24-30_1epoch_1epoch_0.001_0.01/period17/Epoch1_TestAUC0.8377_TestLOGLOSS0.4997.ckpt\n","collect params time elapsed: 00:00:00\n","INFO:tensorflow:Restoring parameters from ckpts/SML_emb&mlp_train11-23_test24-30_1epoch_1epoch_0.001_0.01/period17/Epoch1_TestAUC0.8377_TestLOGLOSS0.4997.ckpt\n","11-Nov-21 09:51:39 [INFO] : Restoring parameters from ckpts/SML_emb&mlp_train11-23_test24-30_1epoch_1epoch_0.001_0.01/period17/Epoch1_TestAUC0.8377_TestLOGLOSS0.4997.ckpt\n","Training Base Model Epoch 1 Start!\n","[Epoch 1 Batch 1] base_loss_cur 0.7132, time elapsed 00:00:00\n","[Epoch 1 Batch 101] base_loss_cur 0.4791, time elapsed 00:00:03\n","[Epoch 1 Batch 201] base_loss_cur 0.4091, time elapsed 00:00:06\n","Epoch 1 Done! time elapsed: 00:00:06, base_loss_cur_avg 0.4588\n","cur_auc 0.9260, cur_logloss 0.3607, next_auc 0.7726, next_logloss 0.5842\n","time elapsed 00:00:12\n","\n","Training Transfer Module Epoch 1 Start!\n","[Epoch 1 Batch 1] transfer_loss_next 0.6141, time elapsed 00:00:00\n","next_auc 0.7727, next_logloss 0.5822\n","time elapsed 00:00:03\n","\n","[Epoch 1 Batch 101] transfer_loss_next 0.5703, time elapsed 00:00:07\n","next_auc 0.7755, next_logloss 0.5665\n","time elapsed 00:00:09\n","\n","[Epoch 1 Batch 201] transfer_loss_next 0.5600, time elapsed 00:00:13\n","next_auc 0.7765, next_logloss 0.5640\n","time elapsed 00:00:15\n","\n","Epoch 1 Done! time elapsed: 00:00:16, transfer_loss_next_avg 0.5677\n","cur_auc 0.9248, cur_logloss 0.3997, next_auc 0.7766, next_logloss 0.5638\n","time elapsed 00:00:23\n","\n","\n","current period: 19, next period: 20\n","\n","current set size 54365 next set size 54365\n","restored checkpoint: ckpts/SML_emb&mlp_train11-23_test24-30_1epoch_1epoch_0.001_0.01/period18/Epoch1_TestAUC0.7766_TestLOGLOSS0.5638.ckpt\n","collect params time elapsed: 00:00:00\n","INFO:tensorflow:Restoring parameters from ckpts/SML_emb&mlp_train11-23_test24-30_1epoch_1epoch_0.001_0.01/period18/Epoch1_TestAUC0.7766_TestLOGLOSS0.5638.ckpt\n","11-Nov-21 09:52:20 [INFO] : Restoring parameters from ckpts/SML_emb&mlp_train11-23_test24-30_1epoch_1epoch_0.001_0.01/period18/Epoch1_TestAUC0.7766_TestLOGLOSS0.5638.ckpt\n","Training Base Model Epoch 1 Start!\n","[Epoch 1 Batch 1] base_loss_cur 0.7055, time elapsed 00:00:00\n","[Epoch 1 Batch 101] base_loss_cur 0.4851, time elapsed 00:00:03\n","[Epoch 1 Batch 201] base_loss_cur 0.4230, time elapsed 00:00:06\n","Epoch 1 Done! time elapsed: 00:00:06, base_loss_cur_avg 0.5222\n","cur_auc 0.8958, cur_logloss 0.4323, next_auc 0.8303, next_logloss 0.5076\n","time elapsed 00:00:12\n","\n","Training Transfer Module Epoch 1 Start!\n","[Epoch 1 Batch 1] transfer_loss_next 0.5536, time elapsed 00:00:00\n","next_auc 0.8304, next_logloss 0.5072\n","time elapsed 00:00:03\n","\n","[Epoch 1 Batch 101] transfer_loss_next 0.4402, time elapsed 00:00:06\n","next_auc 0.8398, next_logloss 0.4957\n","time elapsed 00:00:09\n","\n","[Epoch 1 Batch 201] transfer_loss_next 0.4513, time elapsed 00:00:13\n","next_auc 0.8406, next_logloss 0.4972\n","time elapsed 00:00:15\n","\n","Epoch 1 Done! time elapsed: 00:00:16, transfer_loss_next_avg 0.4977\n","cur_auc 0.9174, cur_logloss 0.3909, next_auc 0.8405, next_logloss 0.4942\n","time elapsed 00:00:21\n","\n","\n","current period: 20, next period: 21\n","\n","current set size 54365 next set size 54365\n","restored checkpoint: ckpts/SML_emb&mlp_train11-23_test24-30_1epoch_1epoch_0.001_0.01/period19/Epoch1_TestAUC0.8405_TestLOGLOSS0.4942.ckpt\n","collect params time elapsed: 00:00:00\n","INFO:tensorflow:Restoring parameters from ckpts/SML_emb&mlp_train11-23_test24-30_1epoch_1epoch_0.001_0.01/period19/Epoch1_TestAUC0.8405_TestLOGLOSS0.4942.ckpt\n","11-Nov-21 09:52:59 [INFO] : Restoring parameters from ckpts/SML_emb&mlp_train11-23_test24-30_1epoch_1epoch_0.001_0.01/period19/Epoch1_TestAUC0.8405_TestLOGLOSS0.4942.ckpt\n","Training Base Model Epoch 1 Start!\n","[Epoch 1 Batch 1] base_loss_cur 0.6436, time elapsed 00:00:00\n","[Epoch 1 Batch 101] base_loss_cur 0.4252, time elapsed 00:00:03\n","[Epoch 1 Batch 201] base_loss_cur 0.3757, time elapsed 00:00:06\n","Epoch 1 Done! time elapsed: 00:00:06, base_loss_cur_avg 0.4350\n","cur_auc 0.9506, cur_logloss 0.3139, next_auc 0.8230, next_logloss 0.5154\n","time elapsed 00:00:12\n","\n","Training Transfer Module Epoch 1 Start!\n","[Epoch 1 Batch 1] transfer_loss_next 0.5213, time elapsed 00:00:00\n","next_auc 0.8226, next_logloss 0.5151\n","time elapsed 00:00:03\n","\n","[Epoch 1 Batch 101] transfer_loss_next 0.4261, time elapsed 00:00:07\n","next_auc 0.8262, next_logloss 0.5143\n","time elapsed 00:00:09\n","\n","[Epoch 1 Batch 201] transfer_loss_next 0.5567, time elapsed 00:00:13\n","next_auc 0.8272, next_logloss 0.5075\n","time elapsed 00:00:15\n","\n","Epoch 1 Done! time elapsed: 00:00:16, transfer_loss_next_avg 0.5112\n","cur_auc 0.9466, cur_logloss 0.3434, next_auc 0.8268, next_logloss 0.5081\n","time elapsed 00:00:21\n","\n","\n","current period: 21, next period: 22\n","\n","current set size 54365 next set size 54365\n","restored checkpoint: ckpts/SML_emb&mlp_train11-23_test24-30_1epoch_1epoch_0.001_0.01/period20/Epoch1_TestAUC0.8268_TestLOGLOSS0.5081.ckpt\n","collect params time elapsed: 00:00:00\n","INFO:tensorflow:Restoring parameters from ckpts/SML_emb&mlp_train11-23_test24-30_1epoch_1epoch_0.001_0.01/period20/Epoch1_TestAUC0.8268_TestLOGLOSS0.5081.ckpt\n","11-Nov-21 09:53:40 [INFO] : Restoring parameters from ckpts/SML_emb&mlp_train11-23_test24-30_1epoch_1epoch_0.001_0.01/period20/Epoch1_TestAUC0.8268_TestLOGLOSS0.5081.ckpt\n","Training Base Model Epoch 1 Start!\n","[Epoch 1 Batch 1] base_loss_cur 0.6581, time elapsed 00:00:00\n","[Epoch 1 Batch 101] base_loss_cur 0.4457, time elapsed 00:00:03\n","[Epoch 1 Batch 201] base_loss_cur 0.4194, time elapsed 00:00:06\n","Epoch 1 Done! time elapsed: 00:00:06, base_loss_cur_avg 0.4661\n","cur_auc 0.9410, cur_logloss 0.3513, next_auc 0.8203, next_logloss 0.5196\n","time elapsed 00:00:12\n","\n","Training Transfer Module Epoch 1 Start!\n","[Epoch 1 Batch 1] transfer_loss_next 0.5382, time elapsed 00:00:00\n","next_auc 0.8205, next_logloss 0.5193\n","time elapsed 00:00:03\n","\n","[Epoch 1 Batch 101] transfer_loss_next 0.4811, time elapsed 00:00:07\n","next_auc 0.8225, next_logloss 0.5168\n","time elapsed 00:00:10\n","\n","[Epoch 1 Batch 201] transfer_loss_next 0.5121, time elapsed 00:00:13\n","next_auc 0.8232, next_logloss 0.5175\n","time elapsed 00:00:16\n","\n","Epoch 1 Done! time elapsed: 00:00:16, transfer_loss_next_avg 0.5166\n","cur_auc 0.9329, cur_logloss 0.3693, next_auc 0.8241, next_logloss 0.5145\n","time elapsed 00:00:22\n","\n","\n","current period: 22, next period: 23\n","\n","current set size 54365 next set size 54365\n","restored checkpoint: ckpts/SML_emb&mlp_train11-23_test24-30_1epoch_1epoch_0.001_0.01/period21/Epoch1_TestAUC0.8241_TestLOGLOSS0.5145.ckpt\n","collect params time elapsed: 00:00:00\n","INFO:tensorflow:Restoring parameters from ckpts/SML_emb&mlp_train11-23_test24-30_1epoch_1epoch_0.001_0.01/period21/Epoch1_TestAUC0.8241_TestLOGLOSS0.5145.ckpt\n","11-Nov-21 09:54:19 [INFO] : Restoring parameters from ckpts/SML_emb&mlp_train11-23_test24-30_1epoch_1epoch_0.001_0.01/period21/Epoch1_TestAUC0.8241_TestLOGLOSS0.5145.ckpt\n","Training Base Model Epoch 1 Start!\n","[Epoch 1 Batch 1] base_loss_cur 0.6764, time elapsed 00:00:00\n","[Epoch 1 Batch 101] base_loss_cur 0.4978, time elapsed 00:00:03\n","[Epoch 1 Batch 201] base_loss_cur 0.4324, time elapsed 00:00:06\n","Epoch 1 Done! time elapsed: 00:00:06, base_loss_cur_avg 0.4907\n","cur_auc 0.9209, cur_logloss 0.3860, next_auc 0.8107, next_logloss 0.5309\n","time elapsed 00:00:12\n","\n","Training Transfer Module Epoch 1 Start!\n","[Epoch 1 Batch 1] transfer_loss_next 0.5214, time elapsed 00:00:00\n","next_auc 0.8104, next_logloss 0.5308\n","time elapsed 00:00:03\n","\n","[Epoch 1 Batch 101] transfer_loss_next 0.5601, time elapsed 00:00:07\n","next_auc 0.8108, next_logloss 0.5309\n","time elapsed 00:00:09\n","\n","[Epoch 1 Batch 201] transfer_loss_next 0.5443, time elapsed 00:00:13\n","next_auc 0.8122, next_logloss 0.5275\n","time elapsed 00:00:16\n","\n","Epoch 1 Done! time elapsed: 00:00:16, transfer_loss_next_avg 0.5303\n","cur_auc 0.9191, cur_logloss 0.3915, next_auc 0.8121, next_logloss 0.5281\n","time elapsed 00:00:21\n","\n","\n","current period: 23, next period: 24\n","\n","current set size 54365 next set size 54365\n","restored checkpoint: ckpts/SML_emb&mlp_train11-23_test24-30_1epoch_1epoch_0.001_0.01/period22/Epoch1_TestAUC0.8121_TestLOGLOSS0.5281.ckpt\n","collect params time elapsed: 00:00:00\n","INFO:tensorflow:Restoring parameters from ckpts/SML_emb&mlp_train11-23_test24-30_1epoch_1epoch_0.001_0.01/period22/Epoch1_TestAUC0.8121_TestLOGLOSS0.5281.ckpt\n","11-Nov-21 09:54:58 [INFO] : Restoring parameters from ckpts/SML_emb&mlp_train11-23_test24-30_1epoch_1epoch_0.001_0.01/period22/Epoch1_TestAUC0.8121_TestLOGLOSS0.5281.ckpt\n","Training Base Model Epoch 1 Start!\n","[Epoch 1 Batch 1] base_loss_cur 0.6606, time elapsed 00:00:00\n","[Epoch 1 Batch 101] base_loss_cur 0.4739, time elapsed 00:00:03\n","[Epoch 1 Batch 201] base_loss_cur 0.4624, time elapsed 00:00:06\n","Epoch 1 Done! time elapsed: 00:00:06, base_loss_cur_avg 0.4936\n","cur_auc 0.9160, cur_logloss 0.3955, next_auc 0.8332, next_logloss 0.5057\n","time elapsed 00:00:12\n","\n","Training Transfer Module Epoch 1 Start!\n","[Epoch 1 Batch 1] transfer_loss_next 0.4680, time elapsed 00:00:00\n","next_auc 0.8332, next_logloss 0.5055\n","time elapsed 00:00:03\n","\n","[Epoch 1 Batch 101] transfer_loss_next 0.5223, time elapsed 00:00:07\n","next_auc 0.8339, next_logloss 0.5037\n","time elapsed 00:00:09\n","\n","[Epoch 1 Batch 201] transfer_loss_next 0.5434, time elapsed 00:00:13\n","next_auc 0.8337, next_logloss 0.5043\n","time elapsed 00:00:15\n","\n","Epoch 1 Done! time elapsed: 00:00:16, transfer_loss_next_avg 0.5054\n","cur_auc 0.9120, cur_logloss 0.4043, next_auc 0.8334, next_logloss 0.5058\n","time elapsed 00:00:21\n","\n","\n","current period: 24, next period: 25\n","\n","current set size 54365 next set size 54365\n","restored checkpoint: ckpts/SML_emb&mlp_train11-23_test24-30_1epoch_1epoch_0.001_0.01/period23/Epoch1_TestAUC0.8334_TestLOGLOSS0.5058.ckpt\n","collect params time elapsed: 00:00:00\n","INFO:tensorflow:Restoring parameters from ckpts/SML_emb&mlp_train11-23_test24-30_1epoch_1epoch_0.001_0.01/period23/Epoch1_TestAUC0.8334_TestLOGLOSS0.5058.ckpt\n","11-Nov-21 09:55:40 [INFO] : Restoring parameters from ckpts/SML_emb&mlp_train11-23_test24-30_1epoch_1epoch_0.001_0.01/period23/Epoch1_TestAUC0.8334_TestLOGLOSS0.5058.ckpt\n","Training Base Model Epoch 1 Start!\n","[Epoch 1 Batch 1] base_loss_cur 0.6221, time elapsed 00:00:01\n","[Epoch 1 Batch 101] base_loss_cur 0.5155, time elapsed 00:00:03\n","[Epoch 1 Batch 201] base_loss_cur 0.4564, time elapsed 00:00:06\n","Epoch 1 Done! time elapsed: 00:00:06, base_loss_cur_avg 0.4888\n","cur_auc 0.9098, cur_logloss 0.3981, next_auc 0.7991, next_logloss 0.5447\n","time elapsed 00:00:12\n","\n","Training Transfer Module Epoch 1 Start!\n","[Epoch 1 Batch 1] transfer_loss_next 0.5486, time elapsed 00:00:00\n","next_auc 0.7991, next_logloss 0.5449\n","time elapsed 00:00:03\n","\n","[Epoch 1 Batch 101] transfer_loss_next 0.5780, time elapsed 00:00:07\n","next_auc 0.8018, next_logloss 0.5398\n","time elapsed 00:00:09\n","\n","[Epoch 1 Batch 201] transfer_loss_next 0.5629, time elapsed 00:00:13\n","next_auc 0.8025, next_logloss 0.5377\n","time elapsed 00:00:16\n","\n","Epoch 1 Done! time elapsed: 00:00:16, transfer_loss_next_avg 0.5400\n","cur_auc 0.9098, cur_logloss 0.4127, next_auc 0.8024, next_logloss 0.5378\n","time elapsed 00:00:21\n","\n","test aucs [0.79914774832016]\n","average auc 0.79914774832016\n","\n","test loglosses [0.5447032297292195]\n","average logloss 0.5447032297292195\n","\n","current period: 25, next period: 26\n","\n","current set size 54365 next set size 54365\n","restored checkpoint: ckpts/SML_emb&mlp_train11-23_test24-30_1epoch_1epoch_0.001_0.01/period24/Epoch1_TestAUC0.8024_TestLOGLOSS0.5378.ckpt\n","collect params time elapsed: 00:00:00\n","INFO:tensorflow:Restoring parameters from ckpts/SML_emb&mlp_train11-23_test24-30_1epoch_1epoch_0.001_0.01/period24/Epoch1_TestAUC0.8024_TestLOGLOSS0.5378.ckpt\n","11-Nov-21 09:56:19 [INFO] : Restoring parameters from ckpts/SML_emb&mlp_train11-23_test24-30_1epoch_1epoch_0.001_0.01/period24/Epoch1_TestAUC0.8024_TestLOGLOSS0.5378.ckpt\n","Training Base Model Epoch 1 Start!\n","[Epoch 1 Batch 1] base_loss_cur 0.6694, time elapsed 00:00:00\n","[Epoch 1 Batch 101] base_loss_cur 0.5321, time elapsed 00:00:03\n","[Epoch 1 Batch 201] base_loss_cur 0.4787, time elapsed 00:00:06\n","Epoch 1 Done! time elapsed: 00:00:06, base_loss_cur_avg 0.5195\n","cur_auc 0.9046, cur_logloss 0.4197, next_auc 0.8124, next_logloss 0.5282\n","time elapsed 00:00:14\n","\n","Training Transfer Module Epoch 1 Start!\n","[Epoch 1 Batch 1] transfer_loss_next 0.5461, time elapsed 00:00:01\n","next_auc 0.8124, next_logloss 0.5284\n","time elapsed 00:00:03\n","\n","[Epoch 1 Batch 101] transfer_loss_next 0.5263, time elapsed 00:00:07\n","next_auc 0.8154, next_logloss 0.5239\n","time elapsed 00:00:09\n","\n","[Epoch 1 Batch 201] transfer_loss_next 0.4979, time elapsed 00:00:13\n","next_auc 0.8167, next_logloss 0.5223\n","time elapsed 00:00:16\n","\n","Epoch 1 Done! time elapsed: 00:00:16, transfer_loss_next_avg 0.5255\n","cur_auc 0.9078, cur_logloss 0.4037, next_auc 0.8160, next_logloss 0.5267\n","time elapsed 00:00:21\n","\n","test aucs [0.79914774832016, 0.8123655187473042]\n","average auc 0.8057566335337321\n","\n","test loglosses [0.5447032297292195, 0.5281697354429911]\n","average logloss 0.5364364825861052\n","\n","current period: 26, next period: 27\n","\n","current set size 54365 next set size 54365\n","restored checkpoint: ckpts/SML_emb&mlp_train11-23_test24-30_1epoch_1epoch_0.001_0.01/period25/Epoch1_TestAUC0.8160_TestLOGLOSS0.5267.ckpt\n","collect params time elapsed: 00:00:00\n","INFO:tensorflow:Restoring parameters from ckpts/SML_emb&mlp_train11-23_test24-30_1epoch_1epoch_0.001_0.01/period25/Epoch1_TestAUC0.8160_TestLOGLOSS0.5267.ckpt\n","11-Nov-21 09:57:00 [INFO] : Restoring parameters from ckpts/SML_emb&mlp_train11-23_test24-30_1epoch_1epoch_0.001_0.01/period25/Epoch1_TestAUC0.8160_TestLOGLOSS0.5267.ckpt\n","Training Base Model Epoch 1 Start!\n","[Epoch 1 Batch 1] base_loss_cur 0.6531, time elapsed 00:00:00\n","[Epoch 1 Batch 101] base_loss_cur 0.4043, time elapsed 00:00:03\n","[Epoch 1 Batch 201] base_loss_cur 0.4160, time elapsed 00:00:06\n","Epoch 1 Done! time elapsed: 00:00:06, base_loss_cur_avg 0.4849\n","cur_auc 0.9252, cur_logloss 0.3690, next_auc 0.7811, next_logloss 0.5649\n","time elapsed 00:00:12\n","\n","Training Transfer Module Epoch 1 Start!\n","[Epoch 1 Batch 1] transfer_loss_next 0.5766, time elapsed 00:00:00\n","next_auc 0.7812, next_logloss 0.5641\n","time elapsed 00:00:04\n","\n","[Epoch 1 Batch 101] transfer_loss_next 0.5028, time elapsed 00:00:07\n","next_auc 0.7854, next_logloss 0.5541\n","time elapsed 00:00:10\n","\n","[Epoch 1 Batch 201] transfer_loss_next 0.5551, time elapsed 00:00:13\n","next_auc 0.7865, next_logloss 0.5536\n","time elapsed 00:00:16\n","\n","Epoch 1 Done! time elapsed: 00:00:16, transfer_loss_next_avg 0.5560\n","cur_auc 0.9142, cur_logloss 0.4042, next_auc 0.7864, next_logloss 0.5551\n","time elapsed 00:00:22\n","\n","test aucs [0.79914774832016, 0.8123655187473042, 0.7811364117055042]\n","average auc 0.7975498929243227\n","\n","test loglosses [0.5447032297292195, 0.5281697354429911, 0.5649350871167629]\n","average logloss 0.5459360174296578\n","\n","current period: 27, next period: 28\n","\n","current set size 54365 next set size 54365\n","restored checkpoint: ckpts/SML_emb&mlp_train11-23_test24-30_1epoch_1epoch_0.001_0.01/period26/Epoch1_TestAUC0.7864_TestLOGLOSS0.5551.ckpt\n","collect params time elapsed: 00:00:00\n","INFO:tensorflow:Restoring parameters from ckpts/SML_emb&mlp_train11-23_test24-30_1epoch_1epoch_0.001_0.01/period26/Epoch1_TestAUC0.7864_TestLOGLOSS0.5551.ckpt\n","11-Nov-21 09:57:40 [INFO] : Restoring parameters from ckpts/SML_emb&mlp_train11-23_test24-30_1epoch_1epoch_0.001_0.01/period26/Epoch1_TestAUC0.7864_TestLOGLOSS0.5551.ckpt\n","Training Base Model Epoch 1 Start!\n","[Epoch 1 Batch 1] base_loss_cur 0.6772, time elapsed 00:00:00\n","[Epoch 1 Batch 101] base_loss_cur 0.4978, time elapsed 00:00:03\n","[Epoch 1 Batch 201] base_loss_cur 0.4958, time elapsed 00:00:06\n","Epoch 1 Done! time elapsed: 00:00:06, base_loss_cur_avg 0.5156\n","cur_auc 0.9094, cur_logloss 0.4117, next_auc 0.8097, next_logloss 0.5300\n","time elapsed 00:00:12\n","\n","Training Transfer Module Epoch 1 Start!\n","[Epoch 1 Batch 1] transfer_loss_next 0.5416, time elapsed 00:00:00\n","next_auc 0.8098, next_logloss 0.5300\n","time elapsed 00:00:03\n","\n","[Epoch 1 Batch 101] transfer_loss_next 0.5042, time elapsed 00:00:06\n","next_auc 0.8108, next_logloss 0.5304\n","time elapsed 00:00:11\n","\n","[Epoch 1 Batch 201] transfer_loss_next 0.5566, time elapsed 00:00:14\n","next_auc 0.8111, next_logloss 0.5289\n","time elapsed 00:00:17\n","\n","Epoch 1 Done! time elapsed: 00:00:17, transfer_loss_next_avg 0.5305\n","cur_auc 0.9088, cur_logloss 0.4053, next_auc 0.8108, next_logloss 0.5310\n","time elapsed 00:00:23\n","\n","test aucs [0.79914774832016, 0.8123655187473042, 0.7811364117055042, 0.8097162287738812]\n","average auc 0.8005914768867123\n","\n","test loglosses [0.5447032297292195, 0.5281697354429911, 0.5649350871167629, 0.5300051507447209]\n","average logloss 0.5419533007584235\n","\n","current period: 28, next period: 29\n","\n","current set size 54365 next set size 54365\n","restored checkpoint: ckpts/SML_emb&mlp_train11-23_test24-30_1epoch_1epoch_0.001_0.01/period27/Epoch1_TestAUC0.8108_TestLOGLOSS0.5310.ckpt\n","collect params time elapsed: 00:00:00\n","INFO:tensorflow:Restoring parameters from ckpts/SML_emb&mlp_train11-23_test24-30_1epoch_1epoch_0.001_0.01/period27/Epoch1_TestAUC0.8108_TestLOGLOSS0.5310.ckpt\n","11-Nov-21 09:58:20 [INFO] : Restoring parameters from ckpts/SML_emb&mlp_train11-23_test24-30_1epoch_1epoch_0.001_0.01/period27/Epoch1_TestAUC0.8108_TestLOGLOSS0.5310.ckpt\n","Training Base Model Epoch 1 Start!\n","[Epoch 1 Batch 1] base_loss_cur 0.6474, time elapsed 00:00:00\n","[Epoch 1 Batch 101] base_loss_cur 0.5015, time elapsed 00:00:03\n","[Epoch 1 Batch 201] base_loss_cur 0.4764, time elapsed 00:00:06\n","Epoch 1 Done! time elapsed: 00:00:06, base_loss_cur_avg 0.5011\n","cur_auc 0.9099, cur_logloss 0.4038, next_auc 0.7963, next_logloss 0.5480\n","time elapsed 00:00:12\n","\n","Training Transfer Module Epoch 1 Start!\n","[Epoch 1 Batch 1] transfer_loss_next 0.5638, time elapsed 00:00:00\n","next_auc 0.7963, next_logloss 0.5479\n","time elapsed 00:00:03\n","\n","[Epoch 1 Batch 101] transfer_loss_next 0.6071, time elapsed 00:00:06\n","next_auc 0.7970, next_logloss 0.5458\n","time elapsed 00:00:09\n","\n","[Epoch 1 Batch 201] transfer_loss_next 0.5051, time elapsed 00:00:12\n","next_auc 0.7974, next_logloss 0.5488\n","time elapsed 00:00:15\n","\n","Epoch 1 Done! time elapsed: 00:00:15, transfer_loss_next_avg 0.5469\n","cur_auc 0.9038, cur_logloss 0.4258, next_auc 0.7972, next_logloss 0.5487\n","time elapsed 00:00:21\n","\n","test aucs [0.79914774832016, 0.8123655187473042, 0.7811364117055042, 0.8097162287738812, 0.796266431360377]\n","average auc 0.7997264677814452\n","\n","test loglosses [0.5447032297292195, 0.5281697354429911, 0.5649350871167629, 0.5300051507447209, 0.5479879395377347]\n","average logloss 0.5431602285142858\n","\n","current period: 29, next period: 30\n","\n","current set size 54365 next set size 54365\n","restored checkpoint: ckpts/SML_emb&mlp_train11-23_test24-30_1epoch_1epoch_0.001_0.01/period28/Epoch1_TestAUC0.7972_TestLOGLOSS0.5487.ckpt\n","collect params time elapsed: 00:00:00\n","INFO:tensorflow:Restoring parameters from ckpts/SML_emb&mlp_train11-23_test24-30_1epoch_1epoch_0.001_0.01/period28/Epoch1_TestAUC0.7972_TestLOGLOSS0.5487.ckpt\n","11-Nov-21 09:58:58 [INFO] : Restoring parameters from ckpts/SML_emb&mlp_train11-23_test24-30_1epoch_1epoch_0.001_0.01/period28/Epoch1_TestAUC0.7972_TestLOGLOSS0.5487.ckpt\n","Training Base Model Epoch 1 Start!\n","[Epoch 1 Batch 1] base_loss_cur 0.6072, time elapsed 00:00:00\n","[Epoch 1 Batch 101] base_loss_cur 0.5045, time elapsed 00:00:03\n","[Epoch 1 Batch 201] base_loss_cur 0.4560, time elapsed 00:00:06\n","Epoch 1 Done! time elapsed: 00:00:06, base_loss_cur_avg 0.5235\n","cur_auc 0.8957, cur_logloss 0.4281, next_auc 0.8159, next_logloss 0.5245\n","time elapsed 00:00:12\n","\n","Training Transfer Module Epoch 1 Start!\n","[Epoch 1 Batch 1] transfer_loss_next 0.5822, time elapsed 00:00:00\n","next_auc 0.8163, next_logloss 0.5240\n","time elapsed 00:00:03\n","\n","[Epoch 1 Batch 101] transfer_loss_next 0.5036, time elapsed 00:00:06\n","next_auc 0.8162, next_logloss 0.5254\n","time elapsed 00:00:09\n","\n","[Epoch 1 Batch 201] transfer_loss_next 0.5409, time elapsed 00:00:12\n","next_auc 0.8173, next_logloss 0.5227\n","time elapsed 00:00:17\n","\n","Epoch 1 Done! time elapsed: 00:00:17, transfer_loss_next_avg 0.5248\n","cur_auc 0.8934, cur_logloss 0.4321, next_auc 0.8163, next_logloss 0.5239\n","time elapsed 00:00:23\n","\n","test aucs [0.79914774832016, 0.8123655187473042, 0.7811364117055042, 0.8097162287738812, 0.796266431360377, 0.8158835633270971]\n","average auc 0.8024193170390538\n","\n","test loglosses [0.5447032297292195, 0.5281697354429911, 0.5649350871167629, 0.5300051507447209, 0.5479879395377347, 0.5245247099331529]\n","average logloss 0.5400543087507637\n","\n","current period: 30, next period: 31\n","\n","current set size 54365 next set size 54365\n","restored checkpoint: ckpts/SML_emb&mlp_train11-23_test24-30_1epoch_1epoch_0.001_0.01/period29/Epoch1_TestAUC0.8163_TestLOGLOSS0.5239.ckpt\n","collect params time elapsed: 00:00:00\n","INFO:tensorflow:Restoring parameters from ckpts/SML_emb&mlp_train11-23_test24-30_1epoch_1epoch_0.001_0.01/period29/Epoch1_TestAUC0.8163_TestLOGLOSS0.5239.ckpt\n","11-Nov-21 09:59:38 [INFO] : Restoring parameters from ckpts/SML_emb&mlp_train11-23_test24-30_1epoch_1epoch_0.001_0.01/period29/Epoch1_TestAUC0.8163_TestLOGLOSS0.5239.ckpt\n","Training Base Model Epoch 1 Start!\n","[Epoch 1 Batch 1] base_loss_cur 0.6376, time elapsed 00:00:00\n","[Epoch 1 Batch 101] base_loss_cur 0.4944, time elapsed 00:00:03\n","[Epoch 1 Batch 201] base_loss_cur 0.4405, time elapsed 00:00:06\n","Epoch 1 Done! time elapsed: 00:00:06, base_loss_cur_avg 0.5071\n","cur_auc 0.9110, cur_logloss 0.4028, next_auc 0.8051, next_logloss 0.5372\n","time elapsed 00:00:12\n","\n","Training Transfer Module Epoch 1 Start!\n","[Epoch 1 Batch 1] transfer_loss_next 0.5094, time elapsed 00:00:00\n","next_auc 0.8054, next_logloss 0.5367\n","time elapsed 00:00:03\n","\n","[Epoch 1 Batch 101] transfer_loss_next 0.5466, time elapsed 00:00:06\n","next_auc 0.8065, next_logloss 0.5355\n","time elapsed 00:00:09\n","\n","[Epoch 1 Batch 201] transfer_loss_next 0.5035, time elapsed 00:00:12\n","next_auc 0.8079, next_logloss 0.5406\n","time elapsed 00:00:15\n","\n","Epoch 1 Done! time elapsed: 00:00:15, transfer_loss_next_avg 0.5366\n","cur_auc 0.9098, cur_logloss 0.4012, next_auc 0.8076, next_logloss 0.5358\n","time elapsed 00:00:20\n","\n","test aucs [0.79914774832016, 0.8123655187473042, 0.7811364117055042, 0.8097162287738812, 0.796266431360377, 0.8158835633270971, 0.8050643678748467]\n","average auc 0.8027971814441671\n","\n","test loglosses [0.5447032297292195, 0.5281697354429911, 0.5649350871167629, 0.5300051507447209, 0.5479879395377347, 0.5245247099331529, 0.537159813973833]\n","average logloss 0.5396408094969164\n","11-Nov-21 10:00:13 [INFO] : JOB END: SML_MODEL_TRAINING\n"]}]},{"cell_type":"code","metadata":{"id":"JHHTsxBsYRfF","executionInfo":{"status":"ok","timestamp":1636624882399,"user_tz":-330,"elapsed":1614,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}}},"source":["!cp -r /content/ckpts /content/T967215"],"execution_count":59,"outputs":[]}]}