{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"T643979 | Learning Node Representations from Multiple Social Contexts","provenance":[],"collapsed_sections":[],"toc_visible":true,"authorship_tag":"ABX9TyNQwkI6PWeFNz3LNn+Yg43J"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"88eb015e26694d2c851e441d1f1bee50":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_30fe03db016e432fbc8a332cb50210e3","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_8ac890dfa873414f9bcc36d320b71eb7","IPY_MODEL_2664dbc2b11d4d5c920e4eae1c9e7b11","IPY_MODEL_16bacab8fd6c41bcaf6b49086834bd11"]}},"30fe03db016e432fbc8a332cb50210e3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"8ac890dfa873414f9bcc36d320b71eb7":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_b9bd884368c84ea9ae71069698c68f61","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"100%","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_126b55bae38d4a5791974495c9411b40"}},"2664dbc2b11d4d5c920e4eae1c9e7b11":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_4c91bffbae13491cb981f4c3526619ac","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":2708,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":2708,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_046f8b70cea54486b54b815e21a394cb"}},"16bacab8fd6c41bcaf6b49086834bd11":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_224fc7f353ae452282d2c4646d897546","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 2708/2708 [00:03&lt;00:00, 936.20it/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_b612687527b64294a41b98c5f94e897f"}},"b9bd884368c84ea9ae71069698c68f61":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"126b55bae38d4a5791974495c9411b40":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"4c91bffbae13491cb981f4c3526619ac":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"046f8b70cea54486b54b815e21a394cb":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"224fc7f353ae452282d2c4646d897546":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"b612687527b64294a41b98c5f94e897f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}}},"cells":[{"cell_type":"markdown","metadata":{"id":"QxZ7RARQFPCc"},"source":["# SPLITTER: Learning Node Representations from Multiple Contexts"]},{"cell_type":"markdown","metadata":{"id":"0ygKEzgBFB4X"},"source":["<p><center><img src='https://s3.us-west-2.amazonaws.com/secure.notion-static.com/0097c9b8-cf21-4e70-a7e6-47c1608e05d9/Untitled.png?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAT73L2G45O3KS52Y5%2F20211011%2Fus-west-2%2Fs3%2Faws4_request&X-Amz-Date=20211011T155845Z&X-Amz-Expires=86400&X-Amz-Signature=51c89f9f8e7484cb49ef55d55b955d2e96ebea5f0011e28afffe718c43604d51&X-Amz-SignedHeaders=host&response-content-disposition=filename%20%3D%22Untitled.png%22' width=50%></p></center>"]},{"cell_type":"markdown","metadata":{"id":"VF3ME_7kAYSw"},"source":["## CLI Run"]},{"cell_type":"code","metadata":{"id":"Kio4HciT-LYr"},"source":["!git clone https://github.com/benedekrozemberczki/Splitter.git"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"J7GITBzn_XcE","executionInfo":{"status":"ok","timestamp":1633966545412,"user_tz":-330,"elapsed":490,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}},"outputId":"586e67c6-0a36-4ddc-a20c-46d3e56dbe9c"},"source":["%%writefile requirements.txt\n","networkx==1.11\n","tqdm==4.28.1\n","numpy==1.15.4\n","pandas==0.23.4\n","texttable==1.5.0\n","scipy==1.1.0\n","argparse==1.1.0\n","torch==1.1.0\n","gensim==3.6.0"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Writing requirements.txt\n"]}]},{"cell_type":"code","metadata":{"id":"sh8xmRO4_tVP"},"source":["!pip install -r requirements.txt"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7E1iM-o6_vxk"},"source":["!pip install -q -U numpy networkx"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"d_5BuifZ_R72","executionInfo":{"status":"ok","timestamp":1633970608440,"user_tz":-330,"elapsed":433,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}},"outputId":"5daf7ca5-22a3-4036-b83d-3cb93cf237aa"},"source":["%cd Splitter"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/Splitter\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"C8jsSul2_S4z","executionInfo":{"status":"ok","timestamp":1633970303424,"user_tz":-330,"elapsed":2920179,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}},"outputId":"6dce6d03-ef35-4b55-d850-629a393d67c1"},"source":["!python src/main.py"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["+-----------------------+----------------------------------+\n","|      Dimensions       |               128                |\n","+=======================+==================================+\n","| Edge path             | ./input/chameleon_edges.csv      |\n","+-----------------------+----------------------------------+\n","| Embedding output path | ./output/chameleon_embedding.csv |\n","+-----------------------+----------------------------------+\n","| Lambd                 | 0.100                            |\n","+-----------------------+----------------------------------+\n","| Learning rate         | 0.025                            |\n","+-----------------------+----------------------------------+\n","| Negative samples      | 5                                |\n","+-----------------------+----------------------------------+\n","| Number of walks       | 10                               |\n","+-----------------------+----------------------------------+\n","| Persona output path   | ./output/chameleon_personas.json |\n","+-----------------------+----------------------------------+\n","| Seed                  | 42                               |\n","+-----------------------+----------------------------------+\n","| Walk length           | 40                               |\n","+-----------------------+----------------------------------+\n","| Window size           | 5                                |\n","+-----------------------+----------------------------------+\n","| Workers               | 4                                |\n","+-----------------------+----------------------------------+\n","\n","Doing base random walks.\n","\n","100% 2277/2277 [00:04<00:00, 469.39it/s]\n","\n","Learning the base model.\n","\n","\n","Deleting the base walker.\n","\n","Creating egonets.\n","100% 2277/2277 [00:04<00:00, 466.21it/s]\n","Creating the persona graph.\n","100% 31371/31371 [00:00<00:00, 717488.57it/s]\n","Clustering the persona graph.\n","\n","Doing persona random walks.\n","\n","100% 3979/3979 [00:05<00:00, 761.52it/s] \n","\n","Learning the joint model.\n","\n","Splitter (Loss=0.6933): 100% 39790/39790 [48:14<00:00, 12.41it/s]\n","\n","\n","Saving the model.\n","\n"]}]},{"cell_type":"markdown","metadata":{"id":"buflXVGiAziM"},"source":["## API Exploration"]},{"cell_type":"code","metadata":{"id":"1YLyLPuEPpy3"},"source":["!pip install -q -U tqdm"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"D6pFeyggDcr2"},"source":["### Walker"]},{"cell_type":"code","metadata":{"id":"7GzZAdF5DcqP"},"source":["\"\"\"DeepWalker class.\"\"\"\n","\n","import random\n","import numpy as np\n","from tqdm.notebook import tqdm\n","import networkx as nx\n","from gensim.models import Word2Vec\n","\n","class DeepWalker(object):\n","    \"\"\"\n","    DeepWalk node embedding learner object.\n","    A barebones implementation of \"DeepWalk: Online Learning of Social Representations\".\n","    Paper: https://arxiv.org/abs/1403.6652\n","    Video: https://www.youtube.com/watch?v=aZNtHJwfIVg\n","    \"\"\"\n","    def __init__(self, graph, args):\n","        \"\"\"\n","        :param graph: NetworkX graph.\n","        :param args: Arguments object.\n","        \"\"\"\n","        self.graph = graph\n","        self.args = args\n","\n","    def do_walk(self, node):\n","        \"\"\"\n","        Doing a single truncated random walk from a source node.\n","        :param node: Source node of the truncated random walk.\n","        :return walk: A single random walk.\n","        \"\"\"\n","        walk = [node]\n","        while len(walk) < self.args.walk_length:\n","            nebs = [n for n in nx.neighbors(self.graph, walk[-1])]\n","            if len(nebs) == 0:\n","                break\n","            walk.append(random.choice(nebs))\n","        return walk\n","\n","    def create_features(self):\n","        \"\"\"\n","        Creating random walks from each node.\n","        \"\"\"\n","        self.paths = []\n","        for node in tqdm(self.graph.nodes()):\n","            for _ in range(self.args.number_of_walks):\n","                walk = self.do_walk(node)\n","                self.paths.append(walk)\n","\n","    def learn_base_embedding(self):\n","        \"\"\"\n","        Learning an embedding of nodes in the base graph.\n","        :return self.embedding: Embedding of nodes in the latent space.\n","        \"\"\"\n","        self.paths = [[str(node) for node in walk] for walk in self.paths]\n","\n","        model = Word2Vec(self.paths,\n","                         size=self.args.dimensions,\n","                         window=self.args.window_size,\n","                         min_count=1,\n","                         sg=1,\n","                         workers=self.args.workers,\n","                         iter=1)\n","\n","        self.embedding = np.array([list(model[str(n)]) for n in self.graph.nodes()])\n","        return self.embedding"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1fxNmdBwDcoo"},"source":["### Ego Splitter"]},{"cell_type":"code","metadata":{"id":"5n9hWT8KDcmo"},"source":["\"\"\"Ego-Splitter class\"\"\"\n","\n","# import community\n","import community.community_louvain as community\n","import networkx as nx\n","from tqdm.notebook import tqdm\n","\n","\n","class EgoNetSplitter(object):\n","    \"\"\"An implementation of `\"Ego-Splitting\" see:\n","    https://www.eecs.yorku.ca/course_archive/2017-18/F/6412/reading/kdd17p145.pdf\n","    From the KDD '17 paper \"Ego-Splitting Framework: from Non-Overlapping to Overlapping Clusters\".\n","    The tool first creates the egonets of nodes.\n","    A persona-graph is created which is clustered by the Louvain method.\n","    The resulting overlapping cluster memberships are stored as a dictionary.\n","    Args:\n","        resolution (float): Resolution parameter of Python Louvain. Default 1.0.\n","    \"\"\"\n","    def __init__(self, resolution=1.0):\n","        self.resolution = resolution\n","\n","    def _create_egonet(self, node):\n","        \"\"\"\n","        Creating an ego net, extracting personas and partitioning it.\n","        Args:\n","            node: Node ID for egonet (ego node).\n","        \"\"\"\n","        ego_net_minus_ego = self.graph.subgraph(self.graph.neighbors(node))\n","        components = {i: n for i, n in enumerate(nx.connected_components(ego_net_minus_ego))}\n","        new_mapping = {}\n","        personalities = []\n","        for k, v in components.items():\n","            personalities.append(self.index)\n","            for other_node in v:\n","                new_mapping[other_node] = self.index\n","            self.index = self.index+1\n","        self.components[node] = new_mapping\n","        self.personalities[node] = personalities\n","\n","    def _create_egonets(self):\n","        \"\"\"\n","        Creating an egonet for each node.\n","        \"\"\"\n","        self.components = {}\n","        self.personalities = {}\n","        self.index = 0\n","        print(\"Creating egonets.\")\n","        for node in tqdm(self.graph.nodes()):\n","            self._create_egonet(node)\n","\n","    def _map_personalities(self):\n","        \"\"\"\n","        Mapping the personas to new nodes.\n","        \"\"\"\n","        self.personality_map = {p: n for n in self.graph.nodes() for p in self.personalities[n]}\n","\n","    def _get_new_edge_ids(self, edge):\n","        \"\"\"\n","        Getting the new edge identifiers.\n","        Args:\n","            edge: Edge being mapped to the new identifiers.\n","        \"\"\"\n","        return (self.components[edge[0]][edge[1]], self.components[edge[1]][edge[0]])\n","\n","    def _create_persona_graph(self):\n","        \"\"\"\n","        Create a persona graph using the egonet components.\n","        \"\"\"\n","        print(\"Creating the persona graph.\")\n","        self.persona_graph_edges = [self._get_new_edge_ids(e) for e in tqdm(self.graph.edges())]\n","        self.persona_graph = nx.from_edgelist(self.persona_graph_edges)\n","\n","    def _create_partitions(self):\n","        \"\"\"\n","        Creating a non-overlapping clustering of nodes in the persona graph.\n","        \"\"\"\n","        print(\"Clustering the persona graph.\")\n","        self.partitions = community.best_partition(self.persona_graph, resolution=self.resolution)\n","        self.overlapping_partitions = {node: [] for node in self.graph.nodes()}\n","        for node, membership in self.partitions.items():\n","            self.overlapping_partitions[self.personality_map[node]].append(membership)\n","\n","    def fit(self, graph):\n","        \"\"\"\n","        Fitting an Ego-Splitter clustering model.\n","        Arg types:\n","            * **graph** *(NetworkX graph)* - The graph to be clustered.\n","        \"\"\"\n","        self.graph = graph\n","        self._create_egonets()\n","        self._map_personalities()\n","        self._create_persona_graph()\n","        self._create_partitions()\n","\n","    def get_memberships(self):\n","        r\"\"\"Getting the cluster membership of nodes.\n","        Return types:\n","            * **memberships** *(dictionary of lists)* - Cluster memberships.\n","        \"\"\"\n","        return self.overlapping_partitions"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"seS2ZumdDcja"},"source":["### Splitter"]},{"cell_type":"code","metadata":{"id":"pdKSnuoADcfX"},"source":["\"\"\"Splitter Class.\"\"\"\n","\n","import json\n","import torch\n","import random\n","import numpy as np\n","import pandas as pd\n","from tqdm.notebook import trange\n","# from walkers import DeepWalker\n","# from ego_splitting import EgoNetSplitter\n","\n","class Splitter(torch.nn.Module):\n","    \"\"\"\n","    An implementation of \"Splitter: Learning Node Representations\n","    that Capture Multiple Social Contexts\" (WWW 2019).\n","    Paper: http://epasto.org/papers/www2019splitter.pdf\n","    \"\"\"\n","    def __init__(self, args, base_node_count, node_count):\n","        \"\"\"\n","        Splitter set up.\n","        :param args: Arguments object.\n","        :param base_node_count: Number of nodes in the source graph.\n","        :param node_count: Number of nodes in the persona graph.\n","        \"\"\"\n","        super(Splitter, self).__init__()\n","        self.args = args\n","        self.base_node_count = base_node_count\n","        self.node_count = node_count\n","\n","    def create_weights(self):\n","        \"\"\"\n","        Creating weights for embedding.\n","        \"\"\"\n","        self.base_node_embedding = torch.nn.Embedding(self.base_node_count,\n","                                                      self.args.dimensions,\n","                                                      padding_idx=0)\n","\n","        self.node_embedding = torch.nn.Embedding(self.node_count,\n","                                                 self.args.dimensions,\n","                                                 padding_idx=0)\n","\n","        self.node_noise_embedding = torch.nn.Embedding(self.node_count,\n","                                                       self.args.dimensions,\n","                                                       padding_idx=0)\n","\n","    def initialize_weights(self, base_node_embedding, mapping):\n","        \"\"\"\n","        Using the base embedding and the persona mapping for initializing the embeddings.\n","        :param base_node_embedding: Node embedding of the source graph.\n","        :param mapping: Mapping of personas to nodes.\n","        \"\"\"\n","        persona_embedding = np.array([base_node_embedding[n] for _, n in mapping.items()])\n","        self.node_embedding.weight.data = torch.nn.Parameter(torch.Tensor(persona_embedding))\n","        self.node_noise_embedding.weight.data = torch.nn.Parameter(torch.Tensor(persona_embedding))\n","        self.base_node_embedding.weight.data = torch.nn.Parameter(torch.Tensor(base_node_embedding),\n","                                                                  requires_grad=False)\n","\n","    def calculate_main_loss(self, sources, contexts, targets):\n","        \"\"\"\n","        Calculating the main embedding loss.\n","        :param sources: Source node vector.\n","        :param contexts: Context node vector.\n","        :param targets: Binary target vector.\n","        :return main_loss: Loss value.\n","        \"\"\"\n","        node_f = self.node_embedding(sources)\n","        node_f = torch.nn.functional.normalize(node_f, p=2, dim=1)\n","        feature_f = self.node_noise_embedding(contexts)\n","        feature_f = torch.nn.functional.normalize(feature_f, p=2, dim=1)\n","        scores = torch.sum(node_f*feature_f, dim=1)\n","        scores = torch.sigmoid(scores)\n","        main_loss = targets*torch.log(scores)+(1-targets)*torch.log(1-scores)\n","        main_loss = -torch.mean(main_loss)\n","        return main_loss\n","\n","    def calculate_regularization(self, pure_sources, personas):\n","        \"\"\"\n","        Calculating the regularization loss.\n","        :param pure_sources: Source nodes in persona graph.\n","        :param personas: Context node vector.\n","        :return regularization_loss: Loss value.\n","        \"\"\"\n","        source_f = self.node_embedding(pure_sources)\n","        original_f = self.base_node_embedding(personas)\n","        scores = torch.clamp(torch.sum(source_f*original_f, dim=1), -15, 15)\n","        scores = torch.sigmoid(scores)\n","        regularization_loss = -torch.mean(torch.log(scores))\n","        return regularization_loss\n","\n","    def forward(self, sources, contexts, targets, personas, pure_sources):\n","        \"\"\"\n","        Doing a forward pass.\n","        :param sources: Source node vector.\n","        :param contexts: Context node vector.\n","        :param targets: Binary target vector.\n","        :param pure_sources: Source nodes in persona graph.\n","        :param personas: Context node vector.\n","        :return loss: Loss value.\n","        \"\"\"\n","        main_loss = self.calculate_main_loss(sources, contexts, targets)\n","        regularization_loss = self.calculate_regularization(pure_sources, personas)\n","        loss = main_loss + self.args.lambd*regularization_loss\n","        return loss\n","\n","class SplitterTrainer(object):\n","    \"\"\"\n","    Class for training a Splitter.\n","    \"\"\"\n","    def __init__(self, graph, args):\n","        \"\"\"\n","        :param graph: NetworkX graph object.\n","        :param args: Arguments object.\n","        \"\"\"\n","        self.graph = graph\n","        self.args = args\n","        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","    def create_noises(self):\n","        \"\"\"\n","        Creating node noise distribution for negative sampling.\n","        \"\"\"\n","        self.downsampled_degrees = {}\n","        for n in self.egonet_splitter.persona_graph.nodes():\n","            self.downsampled_degrees[n] = int(1+self.egonet_splitter.persona_graph.degree(n)**0.75)\n","        self.noises = [k for k, v in self.downsampled_degrees.items() for i in range(v)]\n","\n","    def base_model_fit(self):\n","        \"\"\"\n","        Fitting DeepWalk on base model.\n","        \"\"\"\n","        self.base_walker = DeepWalker(self.graph, self.args)\n","        print(\"\\nDoing base random walks.\\n\")\n","        self.base_walker.create_features()\n","        print(\"\\nLearning the base model.\\n\")\n","        self.base_node_embedding = self.base_walker.learn_base_embedding()\n","        print(\"\\nDeleting the base walker.\\n\")\n","        del self.base_walker\n","\n","    def create_split(self):\n","        \"\"\"\n","        Creating an EgoNetSplitter.\n","        \"\"\"\n","        self.egonet_splitter = EgoNetSplitter()\n","        self.egonet_splitter.fit(self.graph)\n","        self.persona_walker = DeepWalker(self.egonet_splitter.persona_graph, self.args)\n","        print(\"\\nDoing persona random walks.\\n\")\n","        self.persona_walker.create_features()\n","        self.create_noises()\n","\n","    def setup_model(self):\n","        \"\"\"\n","        Creating a model and doing a transfer to GPU.\n","        \"\"\"\n","        base_node_count = self.graph.number_of_nodes()\n","        persona_node_count = self.egonet_splitter.persona_graph.number_of_nodes()\n","        self.model = Splitter(self.args, base_node_count, persona_node_count)\n","        self.model.create_weights()\n","        self.model.initialize_weights(self.base_node_embedding,\n","                                      self.egonet_splitter.personality_map)\n","        self.model = self.model.to(self.device)\n","\n","    def transfer_batch(self, source_nodes, context_nodes, targets, persona_nodes, pure_source_nodes):\n","        \"\"\"\n","        Transfering the batch to GPU.\n","        \"\"\"\n","        self.sources = torch.LongTensor(source_nodes).to(self.device)\n","        self.contexts = torch.LongTensor(context_nodes).to(self.device)\n","        self.targets = torch.FloatTensor(targets).to(self.device)\n","        self.personas = torch.LongTensor(persona_nodes).to(self.device)\n","        self.pure_sources = torch.LongTensor(pure_source_nodes).to(self.device)\n","\n","    def optimize(self):\n","        \"\"\"\n","        Doing a weight update.\n","        \"\"\"\n","        loss = self.model(self.sources, self.contexts,\n","                          self.targets, self.personas, self.pure_sources)\n","        loss.backward()\n","        self.optimizer.step()\n","        self.optimizer.zero_grad()\n","        return loss.item()\n","\n","    def process_walk(self, walk):\n","        \"\"\"\n","        Process random walk (source, context) pairs.\n","        Sample negative instances and create persona node list.\n","        :param walk: Random walk sequence.\n","        \"\"\"\n","        left_nodes = [walk[i] for i in range(len(walk)-self.args.window_size) for j in range(1, self.args.window_size+1)]\n","        right_nodes = [walk[i+j] for i in range(len(walk)-self.args.window_size) for j in range(1, self.args.window_size+1)]\n","        node_pair_count = len(left_nodes)\n","        source_nodes = left_nodes + right_nodes\n","        context_nodes = right_nodes + left_nodes\n","        persona_nodes = np.array([self.egonet_splitter.personality_map[source_node] for source_node in source_nodes])\n","        pure_source_nodes = np.array(source_nodes)\n","        source_nodes = np.array((self.args.negative_samples+1)*source_nodes)\n","        noises = np.random.choice(self.noises, node_pair_count*2*self.args.negative_samples)\n","        context_nodes = np.concatenate((np.array(context_nodes), noises))\n","        positives = [1.0 for node in range(node_pair_count*2)]\n","        negatives = [0.0 for node in range(node_pair_count*self.args.negative_samples*2)]\n","        targets = np.array(positives + negatives)\n","        self.transfer_batch(source_nodes, context_nodes, targets, persona_nodes, pure_source_nodes)\n","\n","    def update_average_loss(self, loss_score):\n","        \"\"\"\n","        Updating the average loss and the description of the time remains bar.\n","        :param loss_score: Loss on the sample.\n","        \"\"\"\n","        self.cummulative_loss = self.cummulative_loss + loss_score\n","        self.steps = self.steps + 1\n","        average_loss = self.cummulative_loss/self.steps\n","        self.walk_steps.set_description(\"Splitter (Loss=%g)\" % round(average_loss, 4))\n","\n","    def reset_average_loss(self, step):\n","        \"\"\"\n","        Doing a reset on the average loss.\n","        :param step: Current number of walks processed.\n","        \"\"\"\n","        if step % 100 == 0:\n","            self.cummulative_loss = 0\n","            self.steps = 0\n","\n","    def fit(self):\n","        \"\"\"\n","        Fitting a model.\n","        \"\"\"\n","        self.base_model_fit()\n","        self.create_split()\n","        self.setup_model()\n","        self.model.train()\n","        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=self.args.learning_rate)\n","        self.optimizer.zero_grad()\n","        print(\"\\nLearning the joint model.\\n\")\n","        random.shuffle(self.persona_walker.paths)\n","        self.walk_steps = trange(len(self.persona_walker.paths), desc=\"Loss\")\n","        for step in self.walk_steps:\n","            self.reset_average_loss(step)\n","            walk = self.persona_walker.paths[step]\n","            self.process_walk(walk)\n","            loss_score = self.optimize()\n","            self.update_average_loss(loss_score)\n","\n","    def save_embedding(self):\n","        \"\"\"\n","        Saving the node embedding.\n","        \"\"\"\n","        print(\"\\n\\nSaving the model.\\n\")\n","        nodes = [node for node in self.egonet_splitter.persona_graph.nodes()]\n","        nodes.sort()\n","        nodes = torch.LongTensor(nodes).to(self.device)\n","        embedding = self.model.node_embedding(nodes).cpu().detach().numpy()\n","        embedding_header = [\"id\"] + [\"x_\" + str(x) for x in range(self.args.dimensions)]\n","        embedding = [np.array(range(embedding.shape[0])).reshape(-1, 1), embedding]\n","        embedding = np.concatenate(embedding, axis=1)\n","        embedding = pd.DataFrame(embedding, columns=embedding_header)\n","        embedding.to_csv(self.args.embedding_output_path, index=None)\n","\n","    def save_persona_graph_mapping(self):\n","        \"\"\"\n","        Saving the persona map.\n","        \"\"\"\n","        with open(self.args.persona_output_path, \"w\") as f:\n","            json.dump(self.egonet_splitter.personality_map, f)                     "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"FPYSDKdTEExT"},"source":["### Utils"]},{"cell_type":"code","metadata":{"id":"JQiv-uZ0EEsU"},"source":["\"\"\"Data reading and printing utils.\"\"\"\n","\n","import pandas as pd\n","import networkx as nx\n","from texttable import Texttable\n","\n","def tab_printer(args):\n","    \"\"\"\n","    Function to print the logs in a nice tabular format.\n","    :param args: Parameters used for the model.\n","    \"\"\"\n","    args = vars(args)\n","    keys = sorted(args.keys())\n","    t = Texttable()\n","    t.add_rows([[\"Parameter\", \"Value\"]])\n","    t.add_rows([[k.replace(\"_\", \" \").capitalize(), args[k]] for k in keys])\n","    print(t.draw())\n","\n","def graph_reader(path):\n","    \"\"\"\n","    Function to read the graph from the path.\n","    :param path: Path to the edge list.\n","    :return graph: NetworkX object returned.\n","    \"\"\"\n","    graph = nx.from_edgelist(pd.read_csv(path).values.tolist())\n","    graph.remove_edges_from(nx.selfloop_edges(graph))\n","    return graph"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-8ppLeZUOrev"},"source":["### Dataset"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NqX_PCosOskk","executionInfo":{"status":"ok","timestamp":1633970763918,"user_tz":-330,"elapsed":654,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}},"outputId":"7b925fd0-c4e4-4ee4-e9ec-91a1405666f3"},"source":["!head ./input/cora_edges.csv"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["id_1,id_2\n","0,633\n","0,1862\n","0,2582\n","1,2\n","1,652\n","1,654\n","2,1\n","2,332\n","2,1454\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"m_5s3ykwOwGm","executionInfo":{"status":"ok","timestamp":1633970763920,"user_tz":-330,"elapsed":19,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}},"outputId":"b86b91ac-7a3c-451f-c605-f9fb1c83da3c"},"source":["!head ./input/cora_target.csv"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["id,target\n","0,3\n","1,4\n","2,4\n","3,0\n","4,3\n","5,2\n","6,0\n","7,3\n","8,3\n"]}]},{"cell_type":"markdown","metadata":{"id":"6TJtEjJdDngP"},"source":["### Params"]},{"cell_type":"code","metadata":{"id":"RzA3V-7vDcci"},"source":["\"\"\"Parameter parsing.\"\"\"\n","\n","import argparse\n","\n","def parameter_parser():\n","    \"\"\"\n","    A method to parse up command line parameters.\n","    By default it trains on the coras dataset.\n","    The default hyperparameters give a good quality representation without grid search.\n","    \"\"\"\n","    parser = argparse.ArgumentParser(description=\"Run Splitter.\")\n","\n","    parser.add_argument(\"--edge-path\",\n","                        nargs=\"?\",\n","                        default=\"./input/cora_edges.csv\",\n","\t                help=\"Edge list csv.\")\n","\n","    parser.add_argument(\"--embedding-output-path\",\n","                        nargs=\"?\",\n","                        default=\"./output/cora_embedding.csv\",\n","\t                help=\"Embedding output path.\")\n","\n","    parser.add_argument(\"--persona-output-path\",\n","                        nargs=\"?\",\n","                        default=\"./output/cora_personas.json\",\n","\t                help=\"Persona output path.\")\n","\n","    parser.add_argument(\"--number-of-walks\",\n","                        type=int,\n","                        default=10,\n","\t                help=\"Number of random walks per source node. Default is 10.\")\n","\n","    parser.add_argument(\"--window-size\",\n","                        type=int,\n","                        default=5,\n","\t                help=\"Skip-gram window size. Default is 5.\")\n","\n","    parser.add_argument(\"--negative-samples\",\n","                        type=int,\n","                        default=5,\n","\t                help=\"Negative sample number. Default is 5.\")\n","\n","    parser.add_argument(\"--walk-length\",\n","                        type=int,\n","                        default=40,\n","\t                help=\"Truncated random walk length. Default is 40.\")\n","\n","    parser.add_argument(\"--seed\",\n","                        type=int,\n","                        default=42,\n","\t                help=\"Random seed for PyTorch. Default is 42.\")\n","\n","    parser.add_argument(\"--learning-rate\",\n","                        type=float,\n","                        default=0.025,\n","\t                help=\"Learning rate. Default is 0.025.\")\n","\n","    parser.add_argument(\"--lambd\",\n","                        type=float,\n","                        default=0.1,\n","\t                help=\"Regularization parameter. Default is 0.1.\")\n","\n","    parser.add_argument(\"--dimensions\",\n","                        type=int,\n","                        default=128,\n","\t                help=\"Embedding dimensions. Default is 128.\")\n","\n","    parser.add_argument('--workers',\n","                        type=int,\n","                        default=4,\n","\t                help='Number of parallel workers. Default is 4.')\n","\n","    return parser.parse_args(args={})"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"oW9kBXppDWPN"},"source":["### Main"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":945,"referenced_widgets":["88eb015e26694d2c851e441d1f1bee50","30fe03db016e432fbc8a332cb50210e3","8ac890dfa873414f9bcc36d320b71eb7","2664dbc2b11d4d5c920e4eae1c9e7b11","16bacab8fd6c41bcaf6b49086834bd11","b9bd884368c84ea9ae71069698c68f61","126b55bae38d4a5791974495c9411b40","4c91bffbae13491cb981f4c3526619ac","046f8b70cea54486b54b815e21a394cb","224fc7f353ae452282d2c4646d897546","b612687527b64294a41b98c5f94e897f","72370da8470340afbe5df829d07c6cd5","eb3fc1bab35f420f9c3a882491732de9","f5aced473ae74b67964b35ce2615ec3f","e352a9d727b8471aa805b2b3ebb34760"]},"id":"qh8LUk1yCnxJ","outputId":"580f4aa6-7cf7-4b81-8ca2-90f52c88e996"},"source":["\"\"\"Running the Splitter.\"\"\"\n","\n","import torch\n","# from param_parser import parameter_parser\n","# from splitter import SplitterTrainer\n","# from utils import tab_printer, graph_reader\n","\n","def main():\n","    \"\"\"\n","    Parsing command line parameters.\n","    Reading data, embedding base graph, creating persona graph and learning a splitter.\n","    Saving the persona mapping and the embedding.\n","    \"\"\"\n","    args = parameter_parser()\n","    torch.manual_seed(args.seed)\n","    tab_printer(args)\n","    graph = graph_reader(args.edge_path)\n","    trainer = SplitterTrainer(graph, args)\n","    trainer.fit()\n","    trainer.save_embedding()\n","    trainer.save_persona_graph_mapping()\n","\n","if __name__ == \"__main__\":\n","    main()"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["+-----------------------+-----------------------------+\n","|      Dimensions       |             128             |\n","+=======================+=============================+\n","| Edge path             | ./input/cora_edges.csv      |\n","+-----------------------+-----------------------------+\n","| Embedding output path | ./output/cora_embedding.csv |\n","+-----------------------+-----------------------------+\n","| Lambd                 | 0.100                       |\n","+-----------------------+-----------------------------+\n","| Learning rate         | 0.025                       |\n","+-----------------------+-----------------------------+\n","| Negative samples      | 5                           |\n","+-----------------------+-----------------------------+\n","| Number of walks       | 10                          |\n","+-----------------------+-----------------------------+\n","| Persona output path   | ./output/cora_personas.json |\n","+-----------------------+-----------------------------+\n","| Seed                  | 42                          |\n","+-----------------------+-----------------------------+\n","| Walk length           | 40                          |\n","+-----------------------+-----------------------------+\n","| Window size           | 5                           |\n","+-----------------------+-----------------------------+\n","| Workers               | 4                           |\n","+-----------------------+-----------------------------+\n","\n","Doing base random walks.\n","\n"]},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"88eb015e26694d2c851e441d1f1bee50","version_minor":0,"version_major":2},"text/plain":["  0%|          | 0/2708 [00:00<?, ?it/s]"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["\n","Learning the base model.\n","\n","\n","Deleting the base walker.\n","\n","Creating egonets.\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:63: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"]},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"72370da8470340afbe5df829d07c6cd5","version_minor":0,"version_major":2},"text/plain":["  0%|          | 0/2708 [00:00<?, ?it/s]"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Creating the persona graph.\n"]},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"eb3fc1bab35f420f9c3a882491732de9","version_minor":0,"version_major":2},"text/plain":["  0%|          | 0/5278 [00:00<?, ?it/s]"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Clustering the persona graph.\n","\n","Doing persona random walks.\n","\n"]},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"f5aced473ae74b67964b35ce2615ec3f","version_minor":0,"version_major":2},"text/plain":["  0%|          | 0/6589 [00:00<?, ?it/s]"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["\n","Learning the joint model.\n","\n"]},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"e352a9d727b8471aa805b2b3ebb34760","version_minor":0,"version_major":2},"text/plain":["Loss:   0%|          | 0/65890 [00:00<?, ?it/s]"]},"metadata":{}}]}]}