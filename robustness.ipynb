{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install recbole==0.2.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into '/content/x'...\n",
      "remote: Enumerating objects: 25, done.\u001b[K\n",
      "remote: Counting objects: 100% (25/25), done.\u001b[K\n",
      "remote: Compressing objects: 100% (15/15), done.\u001b[K\n",
      "remote: Total 25 (delta 6), reused 24 (delta 5), pack-reused 0\u001b[K\n",
      "Unpacking objects: 100% (25/25), done.\n",
      "/content/x\n"
     ]
    }
   ],
   "source": [
    "!mkdir /content/x && git clone https://github.com/RecoHut-Stanzas/RGRecSys.git /content/x\n",
    "%cd /content/x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p saved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import pandas as pd\n",
    "from sklearn.utils import shuffle as sk_shuffle\n",
    "from collections.abc import Iterable\n",
    "from collections import Counter\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "from logging import getLogger, shutdown\n",
    "import importlib\n",
    "import pprint as pprint\n",
    "import pickle\n",
    "\n",
    "from recbole.data.dataset import Dataset\n",
    "from recbole.data.utils import get_data_loader\n",
    "from recbole.data import save_split_dataloaders\n",
    "from recbole.utils.utils import set_color, ModelType, init_seed\n",
    "from recbole.utils.enum_type import FeatureType\n",
    "from recbole.utils import ModelType, init_logger, get_model, get_trainer, init_seed, InputType\n",
    "from recbole.utils.utils import set_color\n",
    "from recbole.config import Config, EvalSetting\n",
    "from recbole.sampler import Sampler, RepeatableSampler, KGSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOG_DIR = \"./log\"\n",
    "RESULTS_DIR = \"./results\"\n",
    "SAVED_DIR = \"./saved\"\n",
    "DATASETS_DIR = \".datasets/\"\n",
    "\n",
    "GENERAL_MODELS = [\"Pop\", \"ItemKNN\", \"BPR\", \"NeuMF\", \"ConvNCF\", \"DMF\", \"FISM\", \"NAIS\", \"SpectralCF\", \"GCMC\",\n",
    "                  \"NGCF\", \"LightGCN\", \"DGCF\", \"LINE\", \"MultiVAE\", \"MultiDAE\", \"MacridVAE\", \"CDAE\", \"ENMF\",\n",
    "                  \"NNCF\", \"RaCT\", \"RecVAE\", \"EASE\", \"SLIMElastic\"]\n",
    "\n",
    "CONTEXT_MODELS = [\"LR\", \"FM\", \"NFM\", \"DeepFM\", \"xDeepFM\", \"AFM\", \"FFM\", \"FwFM\", \"FNN\", \"PNN\", \"DSSM\", \"WideDeep\",\n",
    "                  \"DCN\", \"AutoInt\"]\n",
    "\n",
    "KNOWLEDGE_MODELS = [\"CKE\", \"CFKG\", \"KTUP\", \"KGAT\", \"RippleNet\", \"MKR\", \"KGCN\", \"KGNNLS\"]\n",
    "\n",
    "SEQUENTIAL_MODELS = [\"FPMC\", \"GRU4REC\", \"NARM\", \"STAMP\", \"Caser\", \"NextItNet\", \"TransRec\", \"SASRec\", \"BERT4Rec\",\n",
    "                     \"SRGNN\", \"GCSAN\", \"GRU4RecF\", \"SASRecF\", \"FDSA\", \"S3Rec\", \"GRU4RecKG\", \"KSR\", \"FOSSIL\",\n",
    "                     \"SHAN\", \"RepeatNet\", \"HGN\", \"HRM\", \"NPE\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RobustnessGymDataset(Dataset):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "\n",
    "    def copy(self, new_inter_feat):\n",
    "        nxt = copy.deepcopy(self)\n",
    "        nxt.inter_feat = new_inter_feat\n",
    "        return nxt\n",
    "\n",
    "    def _data_filtering(self):\n",
    "        \"\"\"\n",
    "        Filters data by removing nans, removing duplications,\n",
    "        updating interaction if nans/duplications removed,\n",
    "        and resetting index.\n",
    "        \"\"\"\n",
    "        self._filter_nan_user_or_item()\n",
    "        self._remove_duplication()\n",
    "        self._filter_inter_by_user_or_item()\n",
    "        self._reset_index()\n",
    "\n",
    "    def split_by_ratio(self, ratios, group_by=None):\n",
    "        self.logger.debug(f'split by ratios [{ratios}], group_by=[{group_by}]')\n",
    "        tot_ratio = sum(ratios)\n",
    "        ratios = [_ / tot_ratio for _ in ratios]\n",
    "\n",
    "        if group_by is None:\n",
    "            tot_cnt = self.__len__()\n",
    "            split_ids = self._calcu_split_ids(tot=tot_cnt, ratios=ratios)\n",
    "            next_index = [range(start, end) for start, end in zip([0] + split_ids, split_ids + [tot_cnt])]\n",
    "        else:\n",
    "            grouped_inter_feat_index = self._grouped_index(self.inter_feat[group_by].to_numpy())\n",
    "            next_index = [[] for _ in range(len(ratios))]\n",
    "            for grouped_index in grouped_inter_feat_index:\n",
    "                tot_cnt = len(grouped_index)\n",
    "                split_ids = self._calcu_split_ids(tot=tot_cnt, ratios=ratios)\n",
    "                for index, start, end in zip(next_index, [0] + split_ids, split_ids + [tot_cnt]):\n",
    "                    index.extend(grouped_index[start:end])\n",
    "\n",
    "        self._drop_unused_col()\n",
    "        next_df = [self.inter_feat.iloc[index] for index in next_index]\n",
    "        next_ds = [self.copy(_) for _ in next_df]\n",
    "        return next_ds\n",
    "\n",
    "    def leave_one_out(self, group_by, leave_one_num=1):\n",
    "        self.logger.debug(f'leave one out, group_by=[{group_by}], leave_one_num=[{leave_one_num}]')\n",
    "        if group_by is None:\n",
    "            raise ValueError('leave one out strategy require a group field')\n",
    "\n",
    "        grouped_inter_feat_index = self._grouped_index(self.inter_feat[group_by].numpy())\n",
    "        next_index = self._split_index_by_leave_one_out(grouped_inter_feat_index, leave_one_num)\n",
    "\n",
    "        self._drop_unused_col()\n",
    "        next_df = [self.inter_feat.iloc[index] for index in next_index]\n",
    "        next_ds = [self.copy(_) for _ in next_df]\n",
    "        return next_ds\n",
    "\n",
    "    def _transform_by_field_value_random(self):\n",
    "        \"\"\"\n",
    "        Transforms x% of feature/field values by removing the current value and\n",
    "        replacing with random value selected from set of all possible values.\n",
    "\n",
    "        Returns:\n",
    "\n",
    "        \"\"\"\n",
    "        transform_percents = self.config['transform_val']\n",
    "        if transform_percents is None:\n",
    "            return []\n",
    "\n",
    "        self.logger.debug(set_color('transform_by_field_value', 'blue') + f': val={transform_percents}')\n",
    "        for field in transform_percents:\n",
    "            if field not in self.field2type:\n",
    "                raise ValueError(f'Field [{field}] not defined in dataset.')\n",
    "            for feat_name in self.feat_name_list:\n",
    "                feat = getattr(self, feat_name)\n",
    "                if field in feat:\n",
    "                    # gather all possible field values\n",
    "                    field_values = []\n",
    "                    for index, row in feat.iterrows():\n",
    "                        if not isinstance(row[field], Iterable) and row[field] != 0 and row[field] not in field_values:\n",
    "                            field_values.append(row[field])\n",
    "                        elif isinstance(row[field], Iterable) and len(row[field]) != 0:\n",
    "                            for i in row[field]:\n",
    "                                if i not in field_values:\n",
    "                                    field_values.append(i)\n",
    "                    random_indices = random.sample(range(1, len(feat) - 1),\n",
    "                                                   round(transform_percents[field] * len(feat) - 1))\n",
    "                    for i in random_indices:\n",
    "                        field_value_choices = field_values[:]\n",
    "                        if not isinstance(feat.iloc[i, feat.columns.get_loc(field)], Iterable):\n",
    "                            # remove current value and replace with another chosen at random\n",
    "                            field_value_choices.remove(feat.iloc[i, feat.columns.get_loc(field)])\n",
    "                            feat.iloc[i, feat.columns.get_loc(field)] = random.choice(field_value_choices)\n",
    "                        elif isinstance(feat.iloc[i, feat.columns.get_loc(field)], Iterable):\n",
    "                            for j in feat.iloc[i, feat.columns.get_loc(field)]:\n",
    "                                field_value_choices.remove(j)\n",
    "                            # remove iterable and replace with ONE randomly chosen value\n",
    "                            feat.iloc[i, feat.columns.get_loc(field)] = np.array([[random.choice(field_value_choices)]])\n",
    "        return field_values\n",
    "\n",
    "    def _transform_by_field_value_structured(self):\n",
    "        \"\"\"\n",
    "        Transforms field/feature in structured manner.\n",
    "\n",
    "        (1) If feature value is a single value (float, int), then the value is replaced with a value within x% of the\n",
    "        current value. For example, age = 30, x = 10% --> may be replaced with age = 32.\n",
    "        (2) If feature value is an iterable (list, numpy array), then x% of the values are dropped.\n",
    "        For example, genre = [Horror, Drama, Romance], x = 33% --> may be replaced with genre = [Horror, Romance]\n",
    "        \"\"\"\n",
    "        transform_percents = self.config['DropeFraction_or_variance_transform_val']\n",
    "\n",
    "        if transform_percents is None:\n",
    "            return []\n",
    "        self.logger.debug(set_color('_transform_by_field_value', 'blue') + f': val={transform_percents}')\n",
    "\n",
    "        for field in transform_percents:\n",
    "            if field not in self.field2type:\n",
    "                raise ValueError(f'Field [{field}] not defined in dataset.')\n",
    "            for feat_name in self.feat_name_list:\n",
    "                feat = getattr(self, feat_name)\n",
    "                if field in feat:\n",
    "                    random_indices = random.sample(range(1, len(feat) - 1),\n",
    "                                                   round(transform_percents[field] * len(feat) - 1))\n",
    "                    for i in random_indices:\n",
    "                        if not isinstance(feat.iloc[i, feat.columns.get_loc(field)], Iterable):\n",
    "                            # replaces current value with random integer within x% of current value\n",
    "                            random_value = random.randint(\n",
    "                                round((1 - transform_percents[field]) * feat.iloc[i, feat.columns.get_loc(field)]),\n",
    "                                round((1 + transform_percents[field]) * feat.iloc[i, feat.columns.get_loc(field)]))\n",
    "                            feat.iloc[i, feat.columns.get_loc(field)] = random_value\n",
    "                        elif isinstance(feat.iloc[i, feat.columns.get_loc(field)], Iterable) and len(\n",
    "                                feat.iloc[i, feat.columns.get_loc(field)]) > 1:\n",
    "                            # randomly sample x% from iterable/list and remove them\n",
    "                            dropped_values = random.sample(list(feat.iloc[i, feat.columns.get_loc(field)]),\n",
    "                                                           round(transform_percents[field] *\n",
    "                                                                 len(feat.iloc[i, feat.columns.get_loc(field)])))\n",
    "                            for item in dropped_values:\n",
    "                                feat.iat[i, feat.columns.get_loc(field)] = np.array(\n",
    "                                    feat.iloc[i, feat.columns.get_loc(field)][\n",
    "                                        feat.iloc[i, feat.columns.get_loc(field)] != item])\n",
    "\n",
    "    def _transform_by_field_value_delete_feat(self):\n",
    "        \"\"\"\n",
    "        Transforms field by \"deleting\" x% of feature values. Since the feature value cannot be truly deleted,\n",
    "        we instead remove x% of feature values and replace with the average value of the feature.\n",
    "        \"\"\"\n",
    "        delete_percent = self.config['DeleteFraction_transform_val']\n",
    "        if delete_percent is None:\n",
    "            return []\n",
    "\n",
    "        self.logger.debug(set_color('_transform_by_field_value', 'blue') + f': val={delete_percent}')\n",
    "        for field in delete_percent:\n",
    "            if field not in self.field2type:\n",
    "                raise ValueError(f'Field [{field}] not defined in dataset.')\n",
    "            value_list = []\n",
    "            for feat_name in self.feat_name_list:\n",
    "                feat = getattr(self, feat_name)\n",
    "                if field in feat:\n",
    "                    # compute average value of feature/field\n",
    "                    for i in range(len(feat)):\n",
    "                        value_list.append(feat.iloc[i, feat.columns.get_loc(field)])\n",
    "                    avg_value = np.mean(value_list)\n",
    "\n",
    "            for feat_name in self.feat_name_list:\n",
    "                feat = getattr(self, feat_name)\n",
    "                if field in feat:\n",
    "                    random_indices = random.sample(range(1, len(feat) - 1),\n",
    "                                                   round(delete_percent[field] * len(feat) - 1))\n",
    "                    for i in random_indices:\n",
    "                        if not isinstance(feat.iloc[i, feat.columns.get_loc(field)], Iterable):\n",
    "                            # replace with average value of feature\n",
    "                            feat.iloc[i, feat.columns.get_loc(field)] = avg_value\n",
    "\n",
    "    def _make_data_more_sparse(self):\n",
    "        val1 = self.config['selected_user_spars_data']\n",
    "        val2 = self.config['fraction_spars_data']\n",
    "        user_D = {}\n",
    "        item_D = {}\n",
    "\n",
    "        for line in range(len(self.inter_feat)):\n",
    "            user_id = self.inter_feat.iloc[line][\"user_id\"]\n",
    "            item_id = self.inter_feat.iloc[line][\"item_id\"]\n",
    "\n",
    "            if user_id not in user_D:\n",
    "                user_D[user_id] = []\n",
    "            user_D[user_id].append(item_id)\n",
    "            if item_id not in item_D:\n",
    "                item_D[item_id] = []\n",
    "            item_D[item_id].append(user_id)\n",
    "\n",
    "        for user_id in user_D:\n",
    "            if len(user_D[user_id]) > val1:\n",
    "                selected_item_id = random.sample(user_D[user_id], round(val2 * len(user_D[user_id])))\n",
    "                for item in selected_item_id:\n",
    "                    self.inter_feat.drop(self.inter_feat.loc[self.inter_feat['user_id'] == user_id].loc[\n",
    "                                             self.inter_feat['item_id'] == item].index, inplace=True)\n",
    "\n",
    "    def _transform_interactions_random(self):\n",
    "        transform_fraction = self.config['transform_inter']\n",
    "        if transform_fraction is None:\n",
    "            return []\n",
    "\n",
    "        random_rating = 0\n",
    "        possible_values = [0.0, 1.0]\n",
    "        random_rows = random.sample(list(self.inter_feat.index), round(transform_fraction * len(self.inter_feat)))\n",
    "        for index in random_rows:\n",
    "            if self.config['MODEL_TYPE'] == ModelType.GENERAL or self.config['MODEL_TYPE'] == ModelType.TRADITIONAL:\n",
    "                transform_col = \"rating\"\n",
    "                get_random_rating = True\n",
    "                while get_random_rating:\n",
    "                    random_rating = round(random.uniform(possible_values[0], possible_values[1]), 2)\n",
    "                    if random_rating != self.inter_feat[transform_col].loc[index]:\n",
    "                        get_random_rating = False\n",
    "                self.inter_feat[transform_col].loc[index] = random_rating\n",
    "            if self.config['MODEL_TYPE'] == ModelType.CONTEXT:\n",
    "                transform_col = \"label\"\n",
    "                if self.inter_feat[transform_col].loc[index] == 1.0:\n",
    "                    self.inter_feat[transform_col].loc[index] = 0.0\n",
    "                else:\n",
    "                    self.inter_feat[transform_col].loc[index] = 1.0\n",
    "\n",
    "    @staticmethod\n",
    "    def _get_user_or_item_subset(feat_file, field, val_list):\n",
    "        return {val: list(feat_file[feat_file[field] == val]) for val in val_list}\n",
    "\n",
    "    def create_distribution(self):\n",
    "        dist_shift = self.config['distribution_shift']\n",
    "        if dist_shift is None:\n",
    "            return []\n",
    "\n",
    "        for field in dist_shift:\n",
    "            distribution_dict = dist_shift[field]\n",
    "            # supports distribution dict of size 2 only\n",
    "            assert (len(distribution_dict) == 2)\n",
    "            if field not in self.field2type:\n",
    "                raise ValueError(f'Field [{field}] not defined in dataset.')\n",
    "            if sum(list(distribution_dict.values())) != 1:\n",
    "                raise ValueError(f'Distribution needs to add up to 1.')\n",
    "            if self.field2type[field] not in {FeatureType.TOKEN}:\n",
    "                raise ValueError(f'Currently only works for Token types.')\n",
    "            for feat_name in self.feat_name_list:\n",
    "                feat = getattr(self, feat_name)\n",
    "                if field in feat:\n",
    "                    user_val_dict = {}\n",
    "                    user_val_counts = {}\n",
    "                    user_val_original_proportions = {}\n",
    "                    unique_vals = list(feat[field].unique())\n",
    "                    for val in unique_vals:\n",
    "                        user_val_dict[val] = list(feat[feat[field] == val][self.uid_field])\n",
    "                        user_val_counts[val] = len(\n",
    "                            [i for i in self.inter_feat[self.uid_field] if i in user_val_dict[val]])\n",
    "                    for val, proportion in distribution_dict.items():\n",
    "                        if val != 0.0:\n",
    "                            token_val = self.field2token_id[field][val]\n",
    "                            user_val_original_proportions[val] = user_val_counts[token_val] / len(self.inter_feat)\n",
    "                    no_change_val = 0\n",
    "                    no_change_quantity = 0\n",
    "                    for val, proportion in distribution_dict.items():\n",
    "                        token_val = self.field2token_id[field][val]\n",
    "                        if proportion >= user_val_original_proportions[val]:\n",
    "                            no_change_val = val\n",
    "                            no_change_new_proportion = proportion\n",
    "                            no_change_quantity = user_val_counts[token_val]\n",
    "                    num_new_test = int(no_change_quantity / no_change_new_proportion)\n",
    "                    num_other_class = num_new_test - no_change_quantity\n",
    "                    for val, proportion in distribution_dict.items():\n",
    "                        token_val = self.field2token_id[field][val]\n",
    "                        if val != no_change_val:\n",
    "                            original_val = user_val_counts[token_val]\n",
    "                            drop_indices = np.random.choice(\n",
    "                                self.inter_feat.index[self.inter_feat[self.uid_field].isin(user_val_dict[token_val])],\n",
    "                                original_val - num_other_class, replace=False)\n",
    "                            self.inter_feat = self.inter_feat.drop(drop_indices)\n",
    "                            new_quantity = len(\n",
    "                                [i for i in self.inter_feat[self.uid_field] if i in user_val_dict[token_val]])\n",
    "\n",
    "    @staticmethod\n",
    "    def create_distribution_slice(train, test):\n",
    "        print(\"Preparing distributional test slice.\")\n",
    "        train.get_training_distribution_statistics()\n",
    "        slice_test = copy.deepcopy(test)\n",
    "        slice_test.create_distribution()\n",
    "        # slice_test.get_training_distribution_statistics()\n",
    "        # slice_test._filter_inter_by_user_or_item()\n",
    "        slice_test._reset_index()\n",
    "        slice_test._user_item_feat_preparation()\n",
    "        return slice_test\n",
    "\n",
    "    def get_training_distribution_statistics(self):\n",
    "        dist_slice = self.config['distribution_shift']\n",
    "        if dist_slice is None:\n",
    "            print(\"No Training Stats Computed\")\n",
    "            return []\n",
    "\n",
    "        for field in dist_slice:\n",
    "            user_dict = {}\n",
    "            for feat_name in self.feat_name_list:\n",
    "                feat = getattr(self, feat_name)\n",
    "                if field in feat:\n",
    "                    unique_vals = list(feat[field].unique())\n",
    "                    for val in unique_vals:\n",
    "                        user_dict[val] = list(feat[feat[field] == val][self.uid_field])\n",
    "            dist = {}\n",
    "            for val in user_dict:\n",
    "                if val != 0.0:\n",
    "                    dist[val] = len(self.inter_feat[self.inter_feat[self.uid_field].isin(user_dict[val])])\n",
    "            print(\"Training Distribution:\")\n",
    "            for val in user_dict:\n",
    "                if val != 0.0:\n",
    "                    print(\"Val: \", self.field2id_token[field][int(val)], \"Percent: \",\n",
    "                          dist[val] / sum(list(dist.values())))\n",
    "\n",
    "    def get_attack_statistics(self, train):\n",
    "        print(\"Interaction Transformation Robustness Test Summary\")\n",
    "\n",
    "    def get_distribution_shift_statistics(self, train, test):\n",
    "        print(\"Distribution Shift Robustness Test Summary\")\n",
    "\n",
    "    def get_transformation_statistics(self, test):\n",
    "        # TODO: improve printed information\n",
    "        print(\"Transformation of Features Robustness Test Summary\")\n",
    "        print(\"Original Test Size: \", len(test.inter_feat))\n",
    "        print(\"Original Test Users: \", len(test.inter_feat[self.uid_field].unique()))\n",
    "        print(\"Original Test Features Distribution\")\n",
    "\n",
    "        print(\"Transformed Test Size: \", len(self.inter_feat))\n",
    "        print(\"Transformed Test Users: \", len(self.inter_feat[self.uid_field].unique()))\n",
    "        print(\"Transformed Test Features Distribution\")\n",
    "\n",
    "    def get_sparsity_statistics(self, train):\n",
    "        print(\"Sparsity Robustness Test Summary\")\n",
    "        print(\"Original Train Size: \", len(train.inter_feat))\n",
    "        print(\"Original Train Users: \", len(train.inter_feat[self.uid_field].unique()))\n",
    "        print(\"Sparsified Train Size: \", len(self.inter_feat))\n",
    "        print(\"Sparsified Train Users: \", len(self.inter_feat[self.uid_field].unique()))\n",
    "\n",
    "    @staticmethod\n",
    "    def create_transformed_test(test):\n",
    "        print(\"Preparing test set transformation.\")\n",
    "        transformed_test = copy.deepcopy(test)\n",
    "        transformed_test.read_transform_features()\n",
    "        transformed_test._transform_by_field_value_random()\n",
    "        transformed_test._transform_by_field_value_structured()\n",
    "        transformed_test._transform_by_field_value_delete_feat()\n",
    "        transformed_test.get_transformation_statistics(test)\n",
    "        return transformed_test\n",
    "\n",
    "    @staticmethod\n",
    "    def create_transformed_train(train):\n",
    "        print(\"Preparing training set transformation.\")\n",
    "        transformed_train = copy.deepcopy(train)\n",
    "        transformed_train.read_transform_interactions()\n",
    "        transformed_train._transform_interactions_random()\n",
    "        transformed_train.get_attack_statistics(train)\n",
    "        return transformed_train\n",
    "\n",
    "    def read_transform_interactions(self):\n",
    "        transform_config = self.config.final_config_dict[\"transform_interactions\"]\n",
    "\n",
    "        if transform_config is None:\n",
    "            print(\"No transformation configs.\")\n",
    "            return None\n",
    "\n",
    "        if \"fraction_transformed\" in transform_config:\n",
    "            self.config.final_config_dict[\"transform_inter\"] = transform_config[\"fraction_transformed\"]\n",
    "        else:\n",
    "            print(\"No transformation percent specified.\")\n",
    "            return None\n",
    "\n",
    "    def read_sparsify(self):\n",
    "        sparsify_config = self.config.final_config_dict[\"sparsify\"]\n",
    "\n",
    "        if sparsify_config is None:\n",
    "            print(\"No sparsity configs.\")\n",
    "            return None\n",
    "\n",
    "        if \"min_user_inter\" in sparsify_config:\n",
    "            min_val = sparsify_config[\"min_user_inter\"]\n",
    "            self.config.final_config_dict['selected_user_spars_data'] = min_val\n",
    "        else:\n",
    "            self.config.final_config_dict['selected_user_spars_data'] = 0\n",
    "\n",
    "        if \"fraction_removed\" in sparsify_config:\n",
    "            fraction = sparsify_config[\"fraction_removed\"]\n",
    "            self.config.final_config_dict[\"fraction_spars_data\"] = fraction\n",
    "        else:\n",
    "            print(\"No sparsity fraction specified.\")\n",
    "            return None\n",
    "\n",
    "    @staticmethod\n",
    "    def create_sparse_train(train):\n",
    "        print(\"Preparing sparsified training data set.\")\n",
    "        sparse_train = copy.deepcopy(train)\n",
    "        sparse_train.read_sparsify()\n",
    "        sparse_train._make_data_more_sparse()\n",
    "        sparse_train.get_sparsity_statistics(train)\n",
    "        return sparse_train\n",
    "\n",
    "    def _filter_by_inter_num(self, train):\n",
    "        ban_users = self._get_illegal_ids_by_inter_num(dataset=train, field=self.uid_field, feat=self.user_feat,\n",
    "                                                       max_num=self.config['max_user_inter_num'],\n",
    "                                                       min_num=self.config['min_user_inter_num'])\n",
    "        ban_items = self._get_illegal_ids_by_inter_num(dataset=train, field=self.iid_field, feat=self.item_feat,\n",
    "                                                       max_num=self.config['max_item_inter_num'],\n",
    "                                                       min_num=self.config['min_item_inter_num'])\n",
    "\n",
    "        if len(ban_users) == 0 and len(ban_items) == 0:\n",
    "            return\n",
    "\n",
    "        if self.user_feat is not None:\n",
    "            dropped_user = self.user_feat[self.uid_field].isin(ban_users)\n",
    "            self.user_feat.drop(self.user_feat.index[dropped_user], inplace=True)\n",
    "\n",
    "        if self.item_feat is not None:\n",
    "            dropped_item = self.item_feat[self.iid_field].isin(ban_items)\n",
    "            self.item_feat.drop(self.item_feat.index[dropped_item], inplace=True)\n",
    "\n",
    "        dropped_inter = pd.Series(False, index=self.inter_feat.index)\n",
    "        if self.uid_field:\n",
    "            dropped_inter |= self.inter_feat[self.uid_field].isin(ban_users)\n",
    "        if self.iid_field:\n",
    "            dropped_inter |= self.inter_feat[self.iid_field].isin(ban_items)\n",
    "        self.logger.debug('[{}] dropped interactions'.format(len(dropped_inter)))\n",
    "        self.inter_feat.drop(self.inter_feat.index[dropped_inter], inplace=True)\n",
    "\n",
    "    def _get_illegal_ids_by_inter_num(self, dataset, field, feat, max_num=None, min_num=None):\n",
    "        \"\"\"\n",
    "        Overloaded from RecBole. This version uses *train* interactions for slicing.\n",
    "        \"\"\"\n",
    "        self.logger.debug('\\n get_illegal_ids_by_inter_num:\\n\\t field=[{}], max_num=[{}], min_num=[{}]'.format(\n",
    "            field, max_num, min_num\n",
    "        ))\n",
    "\n",
    "        if field is None:\n",
    "            return set()\n",
    "        if max_num is None and min_num is None:\n",
    "            return set()\n",
    "\n",
    "        max_num = max_num or np.inf\n",
    "        min_num = min_num or -1\n",
    "\n",
    "        ids = dataset[field].values\n",
    "        inter_num = Counter(ids)\n",
    "        ids = {id_ for id_ in inter_num if inter_num[id_] < min_num or inter_num[id_] > max_num}\n",
    "\n",
    "        if feat is not None:\n",
    "            for id_ in feat[field].values:\n",
    "                if inter_num[id_] < min_num:\n",
    "                    ids.add(id_)\n",
    "        self.logger.debug('[{}] illegal_ids_by_inter_num, field=[{}]'.format(len(ids), field))\n",
    "        return ids\n",
    "\n",
    "    def _drop_by_value(self, val, cmp):\n",
    "        \"\"\"\n",
    "        Overloaded _drop_by_value function from RecBole Dataset base class.\n",
    "        Here we enable filtering for any field type (not just floats). We also\n",
    "        enable dropping of categorical features. This function is called by\n",
    "        _filter_by_field_value() in RecBole.\n",
    "\n",
    "        Args:\n",
    "            val (dict):\n",
    "            cmp (Callable):\n",
    "\n",
    "        Returns:\n",
    "            filter_field (list): field names used in comparison.\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        if val is None:\n",
    "            return []\n",
    "\n",
    "        self.logger.debug(set_color('drop_by_value', 'blue') + f': val={val}')\n",
    "        filter_field = []\n",
    "        for field in val:\n",
    "            if field not in self.field2type:\n",
    "                raise ValueError(f'Field [{field}] not defined in dataset.')\n",
    "            for feat_name in self.feat_name_list:\n",
    "                feat = getattr(self, feat_name)\n",
    "                if field in feat:\n",
    "                    if self.field2type[field] == FeatureType.TOKEN_SEQ:\n",
    "                        raise NotImplementedError\n",
    "                    if self.field2type[field] == FeatureType.TOKEN:\n",
    "                        # tokens are mapped to new values by __init__()\n",
    "                        if isinstance(val[field], str):\n",
    "                            feat.drop(feat.index[cmp(feat[field].values, self.field2token_id[field][val[field]])],\n",
    "                                      inplace=True)\n",
    "                        else:\n",
    "                            def convert_to_orig_val(x):\n",
    "                                if int(x) == 0:\n",
    "                                    return 0.0\n",
    "                                else:\n",
    "                                    try:\n",
    "                                        return float(self.field2id_token[field][int(x)])\n",
    "                                    except:\n",
    "                                        return 0.0\n",
    "\n",
    "                            original_tokens = np.array([convert_to_orig_val(i) for i in feat[field].values])\n",
    "                            feat.drop(feat.index[cmp(original_tokens, float(val[field]))], inplace=True)\n",
    "                    if self.field2type[field] in {FeatureType.FLOAT, FeatureType.FLOAT_SEQ}:\n",
    "                        feat.drop(feat.index[cmp(feat[field].values, val[field])], inplace=True)\n",
    "            filter_field.append(field)\n",
    "        return filter_field\n",
    "\n",
    "    def get_slice_statistics(self, test):\n",
    "        print(\"Slice Robustness Test Summary\")\n",
    "        print(\"Original Test Size: \", len(test.inter_feat))\n",
    "        print(\"Original Test Users: \", len(test.inter_feat[self.uid_field].unique()))\n",
    "        print(\"Subpopulation Size: \", len(self.inter_feat))\n",
    "        print(\"Subpopulation Users: \", len(self.inter_feat[self.uid_field].unique()))\n",
    "\n",
    "    def create_slice(self, test, train):\n",
    "        slice_config = self.config.final_config_dict[\"slice\"]\n",
    "        slice_test = copy.deepcopy(test)\n",
    "        print(\"Preparing subpopulation of Test set.\")\n",
    "        if \"by_feature\" in slice_config:\n",
    "            slice_test = self.create_slice_by_feature(slice_test)\n",
    "        if \"by_inter\" in slice_config:\n",
    "            slice_test = self.create_slice_by_inter(slice_test, train)\n",
    "        slice_test._reset_index()\n",
    "        slice_test._user_item_feat_preparation()\n",
    "        slice_test.get_slice_statistics(test)\n",
    "        return slice_test\n",
    "\n",
    "    def create_slice_by_inter(self, slice_test, train):\n",
    "        print(\"Preparing test set slice based on training set interactions.\")\n",
    "        slice_test.read_slice_by_inter()\n",
    "        slice_test._filter_by_inter_num(train)\n",
    "        return slice_test\n",
    "\n",
    "    def read_slice_by_inter(self):\n",
    "        feature_config = self.config.final_config_dict[\"slice\"][\"by_inter\"]\n",
    "\n",
    "        if feature_config is None:\n",
    "            print(\"No interaction subset specified.\")\n",
    "            return None\n",
    "\n",
    "        if \"user\" in feature_config:\n",
    "            user_inter = feature_config[\"user\"]\n",
    "            assert (type(user_inter) == dict)\n",
    "            if \"min\" in user_inter:\n",
    "                min_val = user_inter[\"min\"]\n",
    "                self.config.final_config_dict[\"min_user_inter_num\"] = min_val\n",
    "            if \"max\" in user_inter:\n",
    "                max_val = user_inter[\"max\"]\n",
    "                self.config.final_config_dict[\"max_user_inter_num\"] = max_val\n",
    "        if \"item\" in feature_config:\n",
    "            item_inter = feature_config[\"item\"]\n",
    "            assert (type(item_inter) == dict)\n",
    "            if \"min\" in item_inter:\n",
    "                min_val = item_inter[\"min\"]\n",
    "                self.config.final_config_dict[\"min_item_inter_num\"] = min_val\n",
    "            if \"max\" in item_inter:\n",
    "                max_val = item_inter[\"max\"]\n",
    "                self.config.final_config_dict[\"max_item_inter_num\"] = max_val\n",
    "\n",
    "    def create_slice_by_feature(self, slice_test):\n",
    "        print(\"Preparing test set slice based on feature values.\")\n",
    "        slice_test.read_slice_by_feature()\n",
    "        slice_test._filter_by_field_value()\n",
    "        slice_test._filter_inter_by_user_or_item()\n",
    "        return slice_test\n",
    "\n",
    "    def read_slice_by_feature(self):\n",
    "        feature_config = self.config.final_config_dict[\"slice\"][\"by_feature\"]\n",
    "\n",
    "        if feature_config is None:\n",
    "            print(\"No feature values specified.\")\n",
    "            return None\n",
    "\n",
    "        for field in feature_config:\n",
    "            for feat_name in self.feat_name_list:\n",
    "                feat = getattr(self, feat_name)\n",
    "                if field in feat:\n",
    "                    if field not in self.field2type:\n",
    "                        raise ValueError(f'Field [{field}] not defined in dataset.')\n",
    "                    slice_specs = feature_config[field]\n",
    "                    if type(slice_specs) == dict:\n",
    "                        if \"min\" in slice_specs:\n",
    "                            min_dict = {field: slice_specs[\"min\"]}\n",
    "                            if self.config.final_config_dict[\"lowest_val\"] is None:\n",
    "                                self.config.final_config_dict[\"lowest_val\"] = min_dict\n",
    "                            else:\n",
    "                                self.config.final_config_dict[\"lowest_val\"].update(min_dict)\n",
    "                        if \"max\" in slice_specs:\n",
    "                            max_dict = {field: slice_specs[\"max\"]}\n",
    "                            if self.config.final_config_dict[\"highest_val\"] is None:\n",
    "                                self.config.final_config_dict[\"highest_val\"] = max_dict\n",
    "                            else:\n",
    "                                self.config.final_config_dict[\"highest_val\"].update(max_dict)\n",
    "                        if \"equal\" in slice_specs:\n",
    "                            equal_dict = {field: slice_specs[\"equal\"]}\n",
    "                            if self.config.final_config_dict[\"equal_val\"] is None:\n",
    "                                self.config.final_config_dict[\"equal_val\"] = equal_dict\n",
    "                            else:\n",
    "                                self.config.final_config_dict[\"equal_val\"].update(equal_dict)\n",
    "                    else:\n",
    "                        print(\"Incorrect config format.\")\n",
    "                        return None\n",
    "\n",
    "    def read_transform_features(self):\n",
    "        feature_config = self.config.final_config_dict[\"transform_features\"]\n",
    "\n",
    "        if feature_config is None:\n",
    "            print(\"No feature transformation specified.\")\n",
    "            return None\n",
    "\n",
    "        if \"structured\" in feature_config:\n",
    "            self.config.final_config_dict['DropeFraction_or_variance_transform_val'] = {}\n",
    "            for field in feature_config[\"structured\"]:\n",
    "                percent = feature_config[\"structured\"][field]\n",
    "                self.config.final_config_dict['DropeFraction_or_variance_transform_val'].update({field: percent})\n",
    "        elif \"random\" in feature_config:\n",
    "            self.config.final_config_dict['transform_val'] = {}\n",
    "            for field in feature_config[\"random\"]:\n",
    "                percent = feature_config[\"random\"][field]\n",
    "                self.config.final_config_dict['transform_val'].update({field: percent})\n",
    "        else:\n",
    "            print(\"Transformation of features incorrectly specified.\")\n",
    "            return None\n",
    "\n",
    "    def create_robustness_datasets(self, train, valid, test):\n",
    "        final_config = self.config.final_config_dict\n",
    "        robustness_testing_datasets = {}\n",
    "\n",
    "        if \"slice\" in final_config:\n",
    "            robustness_testing_datasets[\"slice\"] = self.create_slice(test, train)\n",
    "\n",
    "        if \"sparsify\" in final_config:\n",
    "            robustness_testing_datasets[\"sparsity\"] = self.create_sparse_train(train)\n",
    "\n",
    "        if \"transform_features\" in final_config:\n",
    "            robustness_testing_datasets['transformation_test'] = self.create_transformed_test(test)\n",
    "\n",
    "        if \"transform_interactions\" in final_config:\n",
    "            robustness_testing_datasets['transformation_train'] = self.create_transformed_train(train)\n",
    "\n",
    "        if \"distribution_shift\" in final_config:\n",
    "            robustness_testing_datasets['distributional_slice'] = self.create_distribution_slice(train, test)\n",
    "\n",
    "        return robustness_testing_datasets\n",
    "\n",
    "    def build(self, eval_setting):\n",
    "        \"\"\"\n",
    "        Overloads RecBole build. Our version builds train, valid, test\n",
    "        and modified versions of train, valid, test as needed according to the\n",
    "        robustness tests requested in the robustness_dict.\n",
    "        Args:\n",
    "            eval_setting (EvalSetting):\n",
    "\n",
    "        Returns:\n",
    "            original_datasets (list): list containing original train, valid, test datasets\n",
    "            robustness_testing_datasets (dict): {robustness test name: modified dataset} key value pairs\n",
    "\n",
    "        \"\"\"\n",
    "        if self.benchmark_filename_list is not None:\n",
    "            raise NotImplementedError()\n",
    "\n",
    "        ordering_args = eval_setting.ordering_args\n",
    "        if ordering_args['strategy'] == 'shuffle':\n",
    "            self.inter_feat = sk_shuffle(self.inter_feat)\n",
    "            self.inter_feat = self.inter_feat.reset_index(drop=True)\n",
    "        elif ordering_args['strategy'] == 'by':\n",
    "            raise NotImplementedError()\n",
    "\n",
    "        group_field = eval_setting.group_field\n",
    "        split_args = eval_setting.split_args\n",
    "\n",
    "        if split_args['strategy'] == 'by_ratio':\n",
    "            original_datasets = self.split_by_ratio(split_args['ratios'], group_by=group_field)\n",
    "        elif split_args['strategy'] == 'by_value':\n",
    "            raise NotImplementedError()\n",
    "        elif split_args['strategy'] == 'loo':\n",
    "            original_datasets = self.leave_one_out(group_by=group_field, leave_one_num=split_args['leave_one_num'])\n",
    "        else:\n",
    "            original_datasets = self\n",
    "\n",
    "        train, valid, test = original_datasets\n",
    "        robustness_testing_datasets = self.create_robustness_datasets(train, valid, test)\n",
    "\n",
    "        for data in list(robustness_testing_datasets.values()) + original_datasets:\n",
    "            if data is not None:\n",
    "                data.inter_feat = data.inter_feat.reset_index(drop=True)\n",
    "                data._change_feat_format()\n",
    "                if ordering_args['strategy'] == 'shuffle':\n",
    "                    torch.manual_seed(self.config['seed'])\n",
    "                    data.shuffle()\n",
    "                elif ordering_args['strategy'] == 'by':\n",
    "                    data.sort(by=ordering_args['field'], ascending=ordering_args['ascending'])\n",
    "\n",
    "        return original_datasets, robustness_testing_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing subpopulation of Test set.\n",
      "Preparing test set slice based on feature values.\n",
      "Slice Robustness Test Summary\n",
      "Original Test Size:  10000\n",
      "Original Test Users:  927\n",
      "Subpopulation Size:  2637\n",
      "Subpopulation Users:  286\n",
      "Preparing sparsified training data set.\n",
      "Sparsity Robustness Test Summary\n",
      "Original Train Size:  80000\n",
      "Original Train Users:  943\n",
      "Sparsified Train Size:  75994\n",
      "Sparsified Train Users:  943\n",
      "Preparing training set transformation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/pandas/core/indexing.py:670: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  iloc._setitem_with_indexer(indexer, value)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interaction Transformation Robustness Test Summary\n",
      "Preparing distributional test slice.\n",
      "Training Distribution:\n",
      "Val:  M Percent:  0.74395\n",
      "Val:  F Percent:  0.25605\n",
      "{'slice': \u001b[1;35mml-100k\u001b[0m\n",
      "\u001b[1;34mThe number of users\u001b[0m: 944\n",
      "\u001b[1;34mAverage actions of users\u001b[0m: 9.22027972027972\n",
      "\u001b[1;34mThe number of items\u001b[0m: 1683\n",
      "\u001b[1;34mAverage actions of items\u001b[0m: 3.124407582938389\n",
      "\u001b[1;34mThe number of inters\u001b[0m: 2637\n",
      "\u001b[1;34mThe sparsity of the dataset\u001b[0m: 99.8340206652769%\n",
      "\u001b[1;34mRemain Fields\u001b[0m: ['user_id', 'item_id', 'timestamp', 'age', 'gender', 'occupation', 'release_year', 'class', 'label'], 'sparsity': \u001b[1;35mml-100k\u001b[0m\n",
      "\u001b[1;34mThe number of users\u001b[0m: 944\n",
      "\u001b[1;34mAverage actions of users\u001b[0m: 80.58748674443267\n",
      "\u001b[1;34mThe number of items\u001b[0m: 1683\n",
      "\u001b[1;34mAverage actions of items\u001b[0m: 46.14086217364906\n",
      "\u001b[1;34mThe number of inters\u001b[0m: 75994\n",
      "\u001b[1;34mThe sparsity of the dataset\u001b[0m: 95.21674874366799%\n",
      "\u001b[1;34mRemain Fields\u001b[0m: ['user_id', 'item_id', 'timestamp', 'age', 'gender', 'occupation', 'release_year', 'class', 'label'], 'transformation_train': \u001b[1;35mml-100k\u001b[0m\n",
      "\u001b[1;34mThe number of users\u001b[0m: 944\n",
      "\u001b[1;34mAverage actions of users\u001b[0m: 84.83563096500531\n",
      "\u001b[1;34mThe number of items\u001b[0m: 1683\n",
      "\u001b[1;34mAverage actions of items\u001b[0m: 48.338368580060425\n",
      "\u001b[1;34mThe number of inters\u001b[0m: 80000\n",
      "\u001b[1;34mThe sparsity of the dataset\u001b[0m: 94.96460114605678%\n",
      "\u001b[1;34mRemain Fields\u001b[0m: ['user_id', 'item_id', 'timestamp', 'age', 'gender', 'occupation', 'release_year', 'class', 'label'], 'distributional_slice': \u001b[1;35mml-100k\u001b[0m\n",
      "\u001b[1;34mThe number of users\u001b[0m: 944\n",
      "\u001b[1;34mAverage actions of users\u001b[0m: 9.58246828143022\n",
      "\u001b[1;34mThe number of items\u001b[0m: 1683\n",
      "\u001b[1;34mAverage actions of items\u001b[0m: 7.028764805414552\n",
      "\u001b[1;34mThe number of inters\u001b[0m: 8308\n",
      "\u001b[1;34mThe sparsity of the dataset\u001b[0m: 99.477073829018%\n",
      "\u001b[1;34mRemain Fields\u001b[0m: ['user_id', 'item_id', 'timestamp', 'age', 'gender', 'occupation', 'release_year', 'class', 'label']}\n"
     ]
    }
   ],
   "source": [
    "config = Config(model=\"DCN\", dataset=\"ml-100k\",\n",
    "                config_dict = {\n",
    "                    'distribution_shift': {'gender': {\"M\": .9, \"F\": .1}},\n",
    "                    'slice': {'by_feature': {'age': {'min': 40}}},\n",
    "                    'sparsify': {'fraction_removed': 0.05},\n",
    "                    # 'transform_features': {'fraction_removed': {'occupation': 0.2}},\n",
    "                    'transform_interactions': {'fraction_transformed': 0.2},\n",
    "                    })\n",
    "init_seed(config['seed'], config['reproducibility'])\n",
    "data = RobustnessGymDataset(config)\n",
    "datasets, robust_dict = data.build(EvalSetting(config))\n",
    "print(robust_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(config):\n",
    "    \"\"\"\n",
    "    Initializes RobustnessGymDataset for each recommendation system type in RecBole.\n",
    "    Args:\n",
    "        config (Config): Config file indicating MODEL_TYPE and model.\n",
    "\n",
    "    Returns:\n",
    "        RobustnessGymDataset instance.\n",
    "    \"\"\"\n",
    "    dataset_module = importlib.import_module('recbole.data.dataset')\n",
    "    if hasattr(dataset_module, config['model'] + 'Dataset'):\n",
    "        return getattr(dataset_module, config['model'] + 'Dataset')(config)\n",
    "    else:\n",
    "        model_type = config['MODEL_TYPE']\n",
    "        if model_type == ModelType.SEQUENTIAL:\n",
    "            from recbole.data.dataset import SequentialDataset\n",
    "            SequentialDataset.__bases__ = (RobustnessGymDataset,)\n",
    "            return SequentialDataset(config)\n",
    "        elif model_type == ModelType.KNOWLEDGE:\n",
    "            from recbole.data.dataset import KnowledgeBasedDataset\n",
    "            KnowledgeBasedDataset.__bases__ = (RobustnessGymDataset,)\n",
    "            return KnowledgeBasedDataset(config)\n",
    "        elif model_type == ModelType.SOCIAL:\n",
    "            from recbole.data.dataset import SocialDataset\n",
    "            SocialDataset.__bases__ = (RobustnessGymDataset,)\n",
    "            return SocialDataset(config)\n",
    "        elif model_type == ModelType.DECISIONTREE:\n",
    "            from recbole.data.dataset import DecisionTreeDataset\n",
    "            DecisionTreeDataset.__bases__ = (RobustnessGymDataset,)\n",
    "            return DecisionTreeDataset(config)\n",
    "        else:\n",
    "            return RobustnessGymDataset(config)\n",
    "\n",
    "\n",
    "def get_transformed_train(config, train_kwargs, train_dataloader, robustness_testing_datasets):\n",
    "    \"\"\"\n",
    "    Converts training data set created by transformations into dataloader object. Uses same config\n",
    "    settings as original training data.\n",
    "\n",
    "    Args:\n",
    "        train_kwargs (dict): Training dataset config\n",
    "        train_dataloader (Dataloader): Training dataloader\n",
    "        config (Config): General config\n",
    "        robustness_testing_datasets (dict): Modified datasets resulting from robustness tests\n",
    "\n",
    "    Returns:\n",
    "        transformed_train (Dataloader)\n",
    "    \"\"\"\n",
    "    transformed_train = None\n",
    "    if \"transformation_train\" in robustness_testing_datasets:\n",
    "        transformation_kwargs = {\n",
    "            'config': config,\n",
    "            'dataset': robustness_testing_datasets['transformation_train'],\n",
    "            'batch_size': config['train_batch_size'],\n",
    "            'dl_format': config['MODEL_INPUT_TYPE'],\n",
    "            'shuffle': True,\n",
    "        }\n",
    "        try:\n",
    "            transformation_kwargs['sampler'] = train_kwargs['sampler']\n",
    "            transformation_kwargs['neg_sample_args'] = train_kwargs['neg_sample_args']\n",
    "            transformed_train = train_dataloader(**transformation_kwargs)\n",
    "        except:\n",
    "            transformed_train = train_dataloader(**transformation_kwargs)\n",
    "\n",
    "    return transformed_train\n",
    "\n",
    "\n",
    "def get_sparsity_train(config, train_kwargs, train_dataloader, robustness_testing_datasets):\n",
    "    \"\"\"\n",
    "    Converts training data set created by sparsity into dataloader object. Uses same config\n",
    "    settings as original training data.\n",
    "\n",
    "    Args:\n",
    "        train_kwargs (dict): Training dataset config\n",
    "        train_dataloader (Dataloader): Training dataloader\n",
    "        config (Config): General config\n",
    "        robustness_testing_datasets (dict): Modified datasets resulting from robustness tests\n",
    "\n",
    "    Returns:\n",
    "        sparsity_train (Dataloader)\n",
    "\n",
    "    \"\"\"\n",
    "    sparsity_train = None\n",
    "    if \"sparsity\" in robustness_testing_datasets:\n",
    "        sparsity_kwargs = {\n",
    "            'config': config,\n",
    "            'dataset': robustness_testing_datasets['sparsity'],\n",
    "            'batch_size': config['train_batch_size'],\n",
    "            'dl_format': config['MODEL_INPUT_TYPE'],\n",
    "            'shuffle': True,\n",
    "        }\n",
    "        try:\n",
    "            sparsity_kwargs['sampler'] = train_kwargs['sampler']\n",
    "            sparsity_kwargs['neg_sample_args'] = train_kwargs['neg_sample_args']\n",
    "            sparsity_train = train_dataloader(**sparsity_kwargs)\n",
    "        except:\n",
    "            sparsity_train = train_dataloader(**sparsity_kwargs)\n",
    "\n",
    "    return sparsity_train\n",
    "\n",
    "\n",
    "def get_distributional_slice_test(eval_kwargs, test_kwargs, test_dataloader, robustness_testing_datasets):\n",
    "    \"\"\"\n",
    "\n",
    "    Args:\n",
    "        test_dataloader:\n",
    "        test_kwargs:\n",
    "        eval_kwargs (dict):\n",
    "        test_dataloader (Dataloader):\n",
    "        robustness_testing_datasets (dict):\n",
    "\n",
    "    Returns:\n",
    "\n",
    "    \"\"\"\n",
    "    slice_test = None\n",
    "    if 'distributional_slice' in robustness_testing_datasets:\n",
    "        slice_kwargs = {'dataset': robustness_testing_datasets['distributional_slice']}\n",
    "        if 'sampler' in test_kwargs:\n",
    "            slice_kwargs['sampler'] = test_kwargs['sampler']\n",
    "        slice_kwargs.update(eval_kwargs)\n",
    "        slice_test = test_dataloader(**slice_kwargs)\n",
    "\n",
    "    return slice_test\n",
    "\n",
    "\n",
    "def get_slice_test(eval_kwargs, test_kwargs, test_dataloader, robustness_testing_datasets):\n",
    "    \"\"\"\n",
    "\n",
    "    Args:\n",
    "        test_dataloader:\n",
    "        test_kwargs:\n",
    "        eval_kwargs (dict):\n",
    "        test_dataloader (Dataloader):\n",
    "        robustness_testing_datasets (dict):\n",
    "\n",
    "    Returns:\n",
    "\n",
    "    \"\"\"\n",
    "    slice_test = None\n",
    "    if 'slice' in robustness_testing_datasets:\n",
    "        slice_kwargs = {'dataset': robustness_testing_datasets['slice']}\n",
    "        if 'sampler' in test_kwargs:\n",
    "            slice_kwargs['sampler'] = test_kwargs['sampler']\n",
    "        slice_kwargs.update(eval_kwargs)\n",
    "        slice_test = test_dataloader(**slice_kwargs)\n",
    "\n",
    "    return slice_test\n",
    "\n",
    "\n",
    "def get_transformation_test(eval_kwargs, test_kwargs, test_dataloader, robustness_testing_datasets):\n",
    "    \"\"\"\n",
    "\n",
    "    Args:\n",
    "        test_dataloader:\n",
    "        test_kwargs:\n",
    "        eval_kwargs (dict):\n",
    "        test_dataloader (Dataloader):\n",
    "        robustness_testing_datasets (dict):\n",
    "\n",
    "    Returns:\n",
    "\n",
    "    \"\"\"\n",
    "    transformation_test = None\n",
    "    if 'transformation' in robustness_testing_datasets:\n",
    "        transformation_kwargs = {'dataset': robustness_testing_datasets['transformation']}\n",
    "        if 'sampler' in test_kwargs:\n",
    "            transformation_kwargs['sampler'] = test_kwargs['sampler']\n",
    "        transformation_kwargs.update(eval_kwargs)\n",
    "        transformation_test = test_dataloader(**transformation_kwargs)\n",
    "\n",
    "    return transformation_test\n",
    "\n",
    "\n",
    "def data_preparation(config, dataset, save=False):\n",
    "    \"\"\"\n",
    "    Builds datasets, including datasets built by applying robustness tests, configures train, validation, test\n",
    "    sets, converts to tensors. Overloads RecBole data_preparation - we include the preparation of the robustness test\n",
    "    train/test/valid sets here.\n",
    "\n",
    "    Args:\n",
    "        config (Config):\n",
    "        dataset (RobustnessGymDataset):\n",
    "        save (bool):\n",
    "\n",
    "    Returns:\n",
    "\n",
    "    \"\"\"\n",
    "    model_type = config['MODEL_TYPE']\n",
    "    model = config['model']\n",
    "    es = EvalSetting(config)\n",
    "\n",
    "    original_datasets, robustness_testing_datasets = dataset.build(es)\n",
    "    train_dataset, valid_dataset, test_dataset = original_datasets\n",
    "    phases = ['train', 'valid', 'test']\n",
    "    sampler = None\n",
    "    logger = getLogger()\n",
    "    train_neg_sample_args = config['train_neg_sample_args']\n",
    "    eval_neg_sample_args = es.neg_sample_args\n",
    "\n",
    "    # Training\n",
    "    train_kwargs = {\n",
    "        'config': config,\n",
    "        'dataset': train_dataset,\n",
    "        'batch_size': config['train_batch_size'],\n",
    "        'dl_format': config['MODEL_INPUT_TYPE'],\n",
    "        'shuffle': True,\n",
    "    }\n",
    "\n",
    "    if train_neg_sample_args['strategy'] != 'none':\n",
    "        if dataset.label_field in dataset.inter_feat:\n",
    "            raise ValueError(\n",
    "                f'`training_neg_sample_num` should be 0 '\n",
    "                f'if inter_feat have label_field [{dataset.label_field}].'\n",
    "            )\n",
    "        if model_type != ModelType.SEQUENTIAL:\n",
    "            sampler = Sampler(phases, original_datasets, train_neg_sample_args['distribution'])\n",
    "        else:\n",
    "            sampler = RepeatableSampler(phases, dataset, train_neg_sample_args['distribution'])\n",
    "        if model not in [\"MultiVAE\", \"MultiDAE\", \"MacridVAE\", \"CDAE\", \"ENMF\", \"RaCT\", \"RecVAE\"]:\n",
    "            train_kwargs['sampler'] = sampler.set_phase('train')\n",
    "            train_kwargs['neg_sample_args'] = train_neg_sample_args\n",
    "        if model_type == ModelType.KNOWLEDGE:\n",
    "            kg_sampler = KGSampler(dataset, train_neg_sample_args['distribution'])\n",
    "            train_kwargs['kg_sampler'] = kg_sampler\n",
    "\n",
    "    dataloader = get_data_loader('train', config, train_neg_sample_args)\n",
    "    logger.info(\n",
    "        set_color('Build', 'pink') + set_color(f' [{dataloader.__name__}]', 'yellow') + ' for ' +\n",
    "        set_color('[train]', 'yellow') + ' with format ' + set_color(f'[{train_kwargs[\"dl_format\"]}]', 'yellow')\n",
    "    )\n",
    "    if train_neg_sample_args['strategy'] != 'none':\n",
    "        logger.info(\n",
    "            set_color('[train]', 'pink') + set_color(' Negative Sampling', 'blue') + f': {train_neg_sample_args}'\n",
    "        )\n",
    "    else:\n",
    "        logger.info(set_color('[train]', 'pink') + set_color(' No Negative Sampling', 'yellow'))\n",
    "    logger.info(\n",
    "        set_color('[train]', 'pink') + set_color(' batch_size', 'cyan') + ' = ' +\n",
    "        set_color(f'[{train_kwargs[\"batch_size\"]}]', 'yellow') + ', ' + set_color('shuffle', 'cyan') + ' = ' +\n",
    "        set_color(f'[{train_kwargs[\"shuffle\"]}]\\n', 'yellow')\n",
    "    )\n",
    "\n",
    "    train_data = dataloader(**train_kwargs)\n",
    "    transformed_train = get_transformed_train(config, train_kwargs, dataloader, robustness_testing_datasets)\n",
    "    sparsity_train = get_sparsity_train(config, train_kwargs, dataloader, robustness_testing_datasets)\n",
    "\n",
    "    # Evaluation\n",
    "    eval_kwargs = {\n",
    "        'config': config,\n",
    "        'batch_size': config['eval_batch_size'],\n",
    "        'dl_format': InputType.POINTWISE,\n",
    "        'shuffle': False,\n",
    "    }\n",
    "    valid_kwargs = {'dataset': valid_dataset}\n",
    "    test_kwargs = {'dataset': test_dataset}\n",
    "\n",
    "    if eval_neg_sample_args['strategy'] != 'none':\n",
    "        if dataset.label_field in dataset.inter_feat:\n",
    "            raise ValueError(\n",
    "                f'It can not validate with `{es.es_str[1]}` '\n",
    "                f'when inter_feat have label_field [{dataset.label_field}].'\n",
    "            )\n",
    "        if sampler is None:\n",
    "            if model_type != ModelType.SEQUENTIAL:\n",
    "                sampler = Sampler(phases, original_datasets, eval_neg_sample_args['distribution'])\n",
    "            else:\n",
    "                sampler = RepeatableSampler(phases, dataset, eval_neg_sample_args['distribution'])\n",
    "        else:\n",
    "            sampler.set_distribution(eval_neg_sample_args['distribution'])\n",
    "        eval_kwargs['neg_sample_args'] = eval_neg_sample_args\n",
    "        valid_kwargs['sampler'] = sampler.set_phase('valid')\n",
    "        test_kwargs['sampler'] = sampler.set_phase('test')\n",
    "\n",
    "    valid_kwargs.update(eval_kwargs)\n",
    "    test_kwargs.update(eval_kwargs)\n",
    "\n",
    "    dataloader = get_data_loader('evaluation', config, eval_neg_sample_args)\n",
    "    logger.info(\n",
    "        set_color('Build', 'pink') + set_color(f' [{dataloader.__name__}]', 'yellow') + ' for ' +\n",
    "        set_color('[evaluation]', 'yellow') + ' with format ' + set_color(f'[{eval_kwargs[\"dl_format\"]}]', 'yellow')\n",
    "    )\n",
    "    logger.info(es)\n",
    "    logger.info(\n",
    "        set_color('[evaluation]', 'pink') + set_color(' batch_size', 'cyan') + ' = ' +\n",
    "        set_color(f'[{eval_kwargs[\"batch_size\"]}]', 'yellow') + ', ' + set_color('shuffle', 'cyan') + ' = ' +\n",
    "        set_color(f'[{eval_kwargs[\"shuffle\"]}]\\n', 'yellow')\n",
    "    )\n",
    "\n",
    "    valid_data = dataloader(**valid_kwargs)\n",
    "    test_data = dataloader(**test_kwargs)\n",
    "\n",
    "    transformed_test = None\n",
    "    if 'transformation_test' in robustness_testing_datasets:\n",
    "        transformed_test_kwargs = test_kwargs\n",
    "        transformed_test_kwargs['dataset'] = robustness_testing_datasets['transformation_test']\n",
    "        transformed_test = dataloader(**transformed_test_kwargs)\n",
    "\n",
    "    slice_test = get_slice_test(eval_kwargs, test_kwargs, dataloader, robustness_testing_datasets)\n",
    "    distributional_slice_test = get_distributional_slice_test(eval_kwargs, test_kwargs, dataloader,\n",
    "                                                              robustness_testing_datasets)\n",
    "\n",
    "    if save:\n",
    "        save_split_dataloaders(config, dataloaders=(train_data, valid_data, test_data))\n",
    "\n",
    "    robustness_testing_data = {'slice': slice_test,\n",
    "                               'distributional_slice': distributional_slice_test,\n",
    "                               'transformation_train': transformed_train,\n",
    "                               'transformation_test': transformed_test,\n",
    "                               'sparsity': sparsity_train}\n",
    "\n",
    "    return train_data, valid_data, test_data, robustness_testing_data\n",
    "\n",
    "\n",
    "def get_config_dict(robustness_tests, base_config_dict):\n",
    "    \"\"\"\n",
    "    Combines robustness_test and train_config_dict into a single config_dict.\n",
    "\n",
    "    Args:\n",
    "        robustness_tests (dict): robustness test config dict\n",
    "        base_config_dict (dict): train/data/eval/model/hyperparam config dict\n",
    "\n",
    "    Returns:\n",
    "        config_dict (dict): config dict\n",
    "    \"\"\"\n",
    "    config_dict = {}\n",
    "    if robustness_tests is not None:\n",
    "        if base_config_dict is not None:\n",
    "            config_dict = {**robustness_tests, **base_config_dict}\n",
    "        else:\n",
    "            config_dict = robustness_tests\n",
    "    else:\n",
    "        if base_config_dict is not None:\n",
    "            config_dict = base_config_dict\n",
    "    return config_dict\n",
    "\n",
    "\n",
    "def train_and_test(model, dataset, robustness_tests=None, base_config_dict=None, save_model=True):\n",
    "    \"\"\"\n",
    "    Train a recommendation model and run robustness tests.\n",
    "    Args:\n",
    "        model (str): Name of model to be trained.\n",
    "        dataset (str): Dataset name; must match the dataset's folder name located in 'data_path' path.\n",
    "        base_config_dict: Configuration dictionary. If no config passed, takes default values.\n",
    "        save_model (bool): Determines whether or not to externally save the model after training.\n",
    "        robustness_tests (dict): Configuration dictionary for robustness tests.\n",
    "\n",
    "    Returns:\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    config_dict = get_config_dict(robustness_tests, base_config_dict)\n",
    "    config = Config(model=model, dataset=dataset, config_dict=config_dict)\n",
    "    init_seed(config['seed'], config['reproducibility'])\n",
    "\n",
    "    logger = getLogger()\n",
    "    if len(logger.handlers) != 0:\n",
    "        logger.removeHandler(logger.handlers[1])\n",
    "    init_logger(config)\n",
    "\n",
    "    logger.info(config)\n",
    "\n",
    "    # dataset filtering\n",
    "    dataset = create_dataset(config)\n",
    "    logger.info(dataset)\n",
    "\n",
    "    # dataset splitting\n",
    "    train_data, valid_data, test_data, robustness_testing_data = data_preparation(config, dataset, save=True)\n",
    "\n",
    "    for robustness_test in robustness_testing_data:\n",
    "        if robustness_testing_data[robustness_test] is not None:\n",
    "            logger.info(set_color('Robustness Test', 'yellow') + f': {robustness_test}')\n",
    "\n",
    "    # model loading and initialization\n",
    "    model = get_model(config['model'])(config, train_data).to(config['device'])\n",
    "    logger.info(model)\n",
    "\n",
    "    # trainer loading and initialization\n",
    "    trainer = get_trainer(config['MODEL_TYPE'], config['model'])(config, model)\n",
    "\n",
    "    # model training\n",
    "    best_valid_score, best_valid_result = trainer.fit(\n",
    "        train_data, valid_data, saved=save_model, show_progress=config['show_progress']\n",
    "    )\n",
    "\n",
    "    # model evaluation\n",
    "    test_result = trainer.evaluate(test_data, load_best_model=save_model,\n",
    "                                   show_progress=config['show_progress'])\n",
    "    logger.info(set_color('best valid ', 'yellow') + f': {best_valid_result}')\n",
    "    logger.info(set_color('test result', 'yellow') + f': {test_result}')\n",
    "\n",
    "    test_result_transformation, test_result_sparsity, \\\n",
    "    test_result_slice, test_result_distributional_slice = None, None, None, None\n",
    "\n",
    "    if robustness_testing_data['slice'] is not None:\n",
    "        test_result_slice = trainer.evaluate(robustness_testing_data['slice'], load_best_model=save_model,\n",
    "                                             show_progress=config['show_progress'])\n",
    "        logger.info(set_color('test result for slice', 'yellow') + f': {test_result_slice}')\n",
    "\n",
    "    if robustness_testing_data['distributional_slice'] is not None:\n",
    "        test_result_distributional_slice = trainer.evaluate(robustness_testing_data['distributional_slice'],\n",
    "                                                            load_best_model=save_model,\n",
    "                                                            show_progress=config['show_progress'])\n",
    "        logger.info(set_color('test result for distributional slice', 'yellow') + f': '\n",
    "                                                                                  f'{test_result_distributional_slice}')\n",
    "\n",
    "    if robustness_testing_data['transformation_test'] is not None:\n",
    "        test_result_transformation = trainer.evaluate(robustness_testing_data['transformation_test'],\n",
    "                                                      load_best_model=save_model,\n",
    "                                                      show_progress=config['show_progress'])\n",
    "        logger.info(set_color('test result for transformation on test', 'yellow') + f': {test_result_transformation}')\n",
    "\n",
    "    if robustness_testing_data['transformation_train'] is not None:\n",
    "        transformation_model = get_model(config['model'])(config, robustness_testing_data['transformation_train']).to(\n",
    "            config['device'])\n",
    "        logger.info(transformation_model)\n",
    "        transformation_trainer = get_trainer(config['MODEL_TYPE'], config['model'])(config, transformation_model)\n",
    "        best_valid_score_transformation, best_valid_result_transformation = transformation_trainer.fit(\n",
    "            robustness_testing_data['transformation_train'], valid_data, saved=save_model,\n",
    "            show_progress=config['show_progress'])\n",
    "        test_result_transformation = transformation_trainer.evaluate(test_data, load_best_model=save_model,\n",
    "                                                                     show_progress=config['show_progress'])\n",
    "        logger.info(\n",
    "            set_color('best valid for transformed training set', 'yellow') + f': {best_valid_result_transformation}')\n",
    "        logger.info(set_color('test result for transformed training set', 'yellow') + f': {test_result_transformation}')\n",
    "\n",
    "    if robustness_testing_data['sparsity'] is not None:\n",
    "        sparsity_model = get_model(config['model'])(config, robustness_testing_data['sparsity']).to(config['device'])\n",
    "        logger.info(sparsity_model)\n",
    "        sparsity_trainer = get_trainer(config['MODEL_TYPE'], config['model'])(config, sparsity_model)\n",
    "        best_valid_score_sparsity, best_valid_result_sparsity = sparsity_trainer.fit(\n",
    "            robustness_testing_data['sparsity'], valid_data, saved=save_model,\n",
    "            show_progress=config['show_progress'])\n",
    "        test_result_sparsity = sparsity_trainer.evaluate(test_data, load_best_model=save_model,\n",
    "                                                         show_progress=config['show_progress'])\n",
    "        logger.info(set_color('best valid for sparsified training set', 'yellow') + f': {best_valid_result_sparsity}')\n",
    "        logger.info(set_color('test result for sparsified training set', 'yellow') + f': {test_result_sparsity}')\n",
    "\n",
    "    logger.handlers.clear()\n",
    "    shutdown()\n",
    "    del logger\n",
    "\n",
    "    return {\n",
    "        'test_result': test_result,\n",
    "        'distributional_test_result': test_result_distributional_slice,\n",
    "        'transformation_test_result': test_result_transformation,\n",
    "        'sparsity_test_result': test_result_sparsity,\n",
    "        'slice_test_result': test_result_slice\n",
    "    }\n",
    "\n",
    "\n",
    "def test(model, dataset, model_path, dataloader_path=None, robustness_tests=None, base_config_dict=None):\n",
    "    \"\"\"\n",
    "    Test a pre-trained model from file path. Note that the only robustness test applicable here\n",
    "    is slicing.\n",
    "    Args:\n",
    "        model (str): Name of model.\n",
    "        dataset (str): Name of dataset.\n",
    "        model_path (str): Path to saved model.\n",
    "        robustness_tests (dict): Configuration dictionary for robustness tests.\n",
    "        base_config_dict (dict): Configuration dictionary for data/model/training/evaluation.\n",
    "\n",
    "    Returns:\n",
    "\n",
    "    \"\"\"\n",
    "    config_dict = get_config_dict(robustness_tests, base_config_dict)\n",
    "    config = Config(model=model, dataset=dataset, config_dict=config_dict)\n",
    "    init_seed(config['seed'], config['reproducibility'])\n",
    "\n",
    "    # logger initialization\n",
    "    logger = getLogger()\n",
    "    if len(logger.handlers) != 0:\n",
    "        logger.removeHandler(logger.handlers[1])\n",
    "    init_logger(config)\n",
    "\n",
    "    # dataset filtering\n",
    "    dataset = create_dataset(config)\n",
    "    logger.info(dataset)\n",
    "\n",
    "    # dataset splitting\n",
    "    if dataloader_path is None:\n",
    "        train_data, _, test_data, robustness_testing_data = data_preparation(config, dataset, save=False)\n",
    "    else:\n",
    "        train_data, valid_data, test_data = pickle.load(open(SAVED_DIR + dataloader_path, \"rb\"))\n",
    "        robustness_testing_data = {\"slice\": None, \"transformation\": None, \"sparsity\": None}\n",
    "\n",
    "    # model loading and initialization\n",
    "    model = get_model(config['model'])(config, train_data).to(config['device'])\n",
    "    logger.info(model)\n",
    "\n",
    "    # trainer loading and initialization\n",
    "    trainer = get_trainer(config['MODEL_TYPE'], config['model'])(config, model)\n",
    "\n",
    "    # model evaluation\n",
    "    test_result = trainer.evaluate(test_data, load_best_model=True, model_file=model_path,\n",
    "                                   show_progress=config['show_progress'])\n",
    "    logger.info(set_color('test result', 'yellow') + f': {test_result}')\n",
    "\n",
    "    test_result_slice = None\n",
    "    if robustness_testing_data['slice'] is not None:\n",
    "        test_result_slice = trainer.evaluate(robustness_testing_data['slice'], load_best_model=True,\n",
    "                                             model_file=model_path,\n",
    "                                             show_progress=config['show_progress'])\n",
    "        logger.info(set_color('test result for slice', 'yellow') + f': {test_result_slice}')\n",
    "\n",
    "    return {\n",
    "        'test_result': test_result,\n",
    "        'slice_test_result': test_result_slice\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "14 Jan 06:48    INFO  \n",
      "General Hyper Parameters:\n",
      "gpu_id = 0\n",
      "use_gpu = True\n",
      "seed = 2020\n",
      "state = INFO\n",
      "reproducibility = True\n",
      "data_path = /usr/local/lib/python3.7/dist-packages/recbole/config/../dataset_example/ml-100k\n",
      "show_progress = False\n",
      "\n",
      "Training Hyper Parameters:\n",
      "checkpoint_dir = saved\n",
      "epochs = 300\n",
      "train_batch_size = 2048\n",
      "learner = adam\n",
      "learning_rate = 0.001\n",
      "training_neg_sample_num = 1\n",
      "training_neg_sample_distribution = uniform\n",
      "eval_step = 1\n",
      "stopping_step = 10\n",
      "clip_grad_norm = None\n",
      "weight_decay = 0.0\n",
      "draw_loss_pic = False\n",
      "loss_decimal_place = 4\n",
      "\n",
      "Evaluation Hyper Parameters:\n",
      "eval_setting = RO_RS,full\n",
      "group_by_user = True\n",
      "split_ratio = [0.8, 0.1, 0.1]\n",
      "leave_one_num = 2\n",
      "real_time_process = False\n",
      "metrics = ['Recall', 'MRR', 'NDCG', 'Hit', 'Precision']\n",
      "topk = [10]\n",
      "valid_metric = MRR@10\n",
      "eval_batch_size = 4096\n",
      "metric_decimal_place = 4\n",
      "\n",
      "Dataset Hyper Parameters:\n",
      "field_separator = \t\n",
      "seq_separator =  \n",
      "USER_ID_FIELD = user_id\n",
      "ITEM_ID_FIELD = item_id\n",
      "RATING_FIELD = rating\n",
      "TIME_FIELD = timestamp\n",
      "seq_len = None\n",
      "LABEL_FIELD = label\n",
      "threshold = None\n",
      "NEG_PREFIX = neg_\n",
      "load_col = {'inter': ['user_id', 'item_id', 'rating', 'timestamp'], 'user': ['user_id', 'age', 'gender', 'occupation'], 'item': ['item_id', 'release_year', 'class']}\n",
      "unload_col = None\n",
      "unused_col = None\n",
      "additional_feat_suffix = None\n",
      "rm_dup_inter = None\n",
      "lowest_val = None\n",
      "highest_val = None\n",
      "equal_val = None\n",
      "not_equal_val = None\n",
      "filter_inter_by_user_or_item = True\n",
      "max_user_inter_num = None\n",
      "min_user_inter_num = None\n",
      "max_item_inter_num = None\n",
      "min_item_inter_num = None\n",
      "fields_in_same_space = None\n",
      "preload_weight = None\n",
      "normalize_field = None\n",
      "normalize_all = True\n",
      "ITEM_LIST_LENGTH_FIELD = item_length\n",
      "LIST_SUFFIX = _list\n",
      "MAX_ITEM_LIST_LENGTH = 50\n",
      "POSITION_FIELD = position_id\n",
      "HEAD_ENTITY_ID_FIELD = head_id\n",
      "TAIL_ENTITY_ID_FIELD = tail_id\n",
      "RELATION_ID_FIELD = relation_id\n",
      "ENTITY_ID_FIELD = entity_id\n",
      "\n",
      "Other Hyper Parameters: \n",
      "embedding_size = 64\n",
      "SOURCE_ID_FIELD = source_id\n",
      "TARGET_ID_FIELD = target_id\n",
      "benchmark_filename = None\n",
      "MODEL_TYPE = ModelType.GENERAL\n",
      "distribution_shift = {'gender': {'M': 0.9, 'F': 0.1}}\n",
      "slice = {'by_feature': {'age': {'min': 40}}}\n",
      "sparsify = {'fraction_removed': 0.05}\n",
      "transform_interactions = {'fraction_transformed': 0.2}\n",
      "save_dataset = True\n",
      "MODEL_INPUT_TYPE = InputType.PAIRWISE\n",
      "eval_type = EvaluatorType.RANKING\n",
      "valid_metric_bigger = True\n",
      "device = cpu\n",
      "train_neg_sample_args = {'strategy': 'by', 'by': 1, 'distribution': 'uniform'}\n",
      "\n",
      "\n",
      "14 Jan 06:48    INFO  ml-100k\n",
      "The number of users: 944\n",
      "Average actions of users: 106.04453870625663\n",
      "The number of items: 1683\n",
      "Average actions of items: 59.45303210463734\n",
      "The number of inters: 100000\n",
      "The sparsity of the dataset: 93.70575143257098%\n",
      "Remain Fields: ['user_id', 'item_id', 'rating', 'timestamp', 'age', 'gender', 'occupation', 'release_year', 'class']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing subpopulation of Test set.\n",
      "Preparing test set slice based on feature values.\n",
      "Slice Robustness Test Summary\n",
      "Original Test Size:  9596\n",
      "Original Test Users:  943\n",
      "Subpopulation Size:  2535\n",
      "Subpopulation Users:  293\n",
      "Preparing sparsified training data set.\n",
      "Sparsity Robustness Test Summary\n",
      "Original Train Size:  80808\n",
      "Original Train Users:  943\n",
      "Sparsified Train Size:  76783\n",
      "Sparsified Train Users:  943\n",
      "Preparing training set transformation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/pandas/core/indexing.py:670: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  iloc._setitem_with_indexer(indexer, value)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interaction Transformation Robustness Test Summary\n",
      "Preparing distributional test slice.\n",
      "Training Distribution:\n",
      "Val:  M Percent:  0.7423274923274923\n",
      "Val:  F Percent:  0.25767250767250766\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "14 Jan 06:49    INFO  Build [GeneralNegSampleDataLoader] for [train] with format [InputType.PAIRWISE]\n",
      "14 Jan 06:49    INFO  [train] Negative Sampling: {'strategy': 'by', 'by': 1, 'distribution': 'uniform'}\n",
      "14 Jan 06:49    INFO  [train] batch_size = [2048], shuffle = [True]\n",
      "\n",
      "14 Jan 06:49    INFO  Build [GeneralFullDataLoader] for [evaluation] with format [InputType.POINTWISE]\n",
      "14 Jan 06:49    INFO  Evaluation Setting:\n",
      "\tGroup by user_id\n",
      "\tOrdering: {'strategy': 'shuffle'}\n",
      "\tSplitting: {'strategy': 'by_ratio', 'ratios': [0.8, 0.1, 0.1]}\n",
      "\tNegative Sampling: {'strategy': 'full', 'distribution': 'uniform'}\n",
      "14 Jan 06:49    INFO  [evaluation] batch_size = [4096], shuffle = [False]\n",
      "\n",
      "14 Jan 06:49    INFO  Saved split dataloaders: saved/ml-100k-for-BPR-dataloader.pth\n",
      "14 Jan 06:49    INFO  Robustness Test: slice\n",
      "14 Jan 06:49    INFO  Robustness Test: distributional_slice\n",
      "14 Jan 06:49    INFO  Robustness Test: transformation_train\n",
      "14 Jan 06:49    INFO  Robustness Test: sparsity\n",
      "14 Jan 06:49    INFO  BPR(\n",
      "  (user_embedding): Embedding(944, 64)\n",
      "  (item_embedding): Embedding(1683, 64)\n",
      "  (loss): BPRLoss()\n",
      ")\n",
      "Trainable parameters: 168128\n",
      "14 Jan 06:49    INFO  epoch 0 training [time: 0.35s, train loss: 27.7247]\n",
      "14 Jan 06:49    INFO  epoch 0 evaluating [time: 0.26s, valid_score: 0.021600]\n",
      "14 Jan 06:49    INFO  valid result: \n",
      "recall@10 : 0.0077    mrr@10 : 0.0216    ndcg@10 : 0.0094    hit@10 : 0.0689    precision@10 : 0.0077    \n",
      "14 Jan 06:49    INFO  Saving current best: saved/BPR-Jan-14-2022_06-49-06.pth\n",
      "14 Jan 06:49    INFO  epoch 1 training [time: 0.26s, train loss: 27.6270]\n",
      "14 Jan 06:49    INFO  epoch 1 evaluating [time: 0.23s, valid_score: 0.055900]\n",
      "14 Jan 06:49    INFO  valid result: \n",
      "recall@10 : 0.0154    mrr@10 : 0.0559    ndcg@10 : 0.0231    hit@10 : 0.1591    precision@10 : 0.0203    \n",
      "14 Jan 06:49    INFO  Saving current best: saved/BPR-Jan-14-2022_06-49-06.pth\n",
      "14 Jan 06:49    INFO  epoch 2 training [time: 0.26s, train loss: 27.2801]\n",
      "14 Jan 06:49    INFO  epoch 2 evaluating [time: 0.26s, valid_score: 0.168100]\n",
      "14 Jan 06:49    INFO  valid result: \n",
      "recall@10 : 0.0515    mrr@10 : 0.1681    ndcg@10 : 0.0764    hit@10 : 0.3637    precision@10 : 0.0633    \n",
      "14 Jan 06:49    INFO  Saving current best: saved/BPR-Jan-14-2022_06-49-06.pth\n",
      "14 Jan 06:49    INFO  epoch 3 training [time: 0.25s, train loss: 25.8362]\n",
      "14 Jan 06:49    INFO  epoch 3 evaluating [time: 0.26s, valid_score: 0.233200]\n",
      "14 Jan 06:49    INFO  valid result: \n",
      "recall@10 : 0.0852    mrr@10 : 0.2332    ndcg@10 : 0.1135    hit@10 : 0.4825    precision@10 : 0.0888    \n",
      "14 Jan 06:49    INFO  Saving current best: saved/BPR-Jan-14-2022_06-49-06.pth\n",
      "14 Jan 06:49    INFO  epoch 4 training [time: 0.26s, train loss: 22.2847]\n",
      "14 Jan 06:49    INFO  epoch 4 evaluating [time: 0.26s, valid_score: 0.264800]\n",
      "14 Jan 06:49    INFO  valid result: \n",
      "recall@10 : 0.1069    mrr@10 : 0.2648    ndcg@10 : 0.1333    hit@10 : 0.5451    precision@10 : 0.0994    \n",
      "14 Jan 06:49    INFO  Saving current best: saved/BPR-Jan-14-2022_06-49-06.pth\n",
      "14 Jan 06:49    INFO  epoch 5 training [time: 0.27s, train loss: 18.0862]\n",
      "14 Jan 06:49    INFO  epoch 5 evaluating [time: 0.26s, valid_score: 0.276600]\n",
      "14 Jan 06:49    INFO  valid result: \n",
      "recall@10 : 0.1138    mrr@10 : 0.2766    ndcg@10 : 0.1402    hit@10 : 0.5589    precision@10 : 0.1018    \n",
      "14 Jan 06:49    INFO  Saving current best: saved/BPR-Jan-14-2022_06-49-06.pth\n",
      "14 Jan 06:49    INFO  epoch 6 training [time: 0.28s, train loss: 15.4435]\n",
      "14 Jan 06:49    INFO  epoch 6 evaluating [time: 0.25s, valid_score: 0.283600]\n",
      "14 Jan 06:49    INFO  valid result: \n",
      "recall@10 : 0.1194    mrr@10 : 0.2836    ndcg@10 : 0.1453    hit@10 : 0.5779    precision@10 : 0.1052    \n",
      "14 Jan 06:49    INFO  Saving current best: saved/BPR-Jan-14-2022_06-49-06.pth\n",
      "14 Jan 06:49    INFO  epoch 7 training [time: 0.26s, train loss: 14.0628]\n",
      "14 Jan 06:49    INFO  epoch 7 evaluating [time: 0.26s, valid_score: 0.293300]\n",
      "14 Jan 06:49    INFO  valid result: \n",
      "recall@10 : 0.1257    mrr@10 : 0.2933    ndcg@10 : 0.1511    hit@10 : 0.5854    precision@10 : 0.1066    \n",
      "14 Jan 06:49    INFO  Saving current best: saved/BPR-Jan-14-2022_06-49-06.pth\n",
      "14 Jan 06:49    INFO  epoch 8 training [time: 0.27s, train loss: 13.3594]\n",
      "14 Jan 06:49    INFO  epoch 8 evaluating [time: 0.29s, valid_score: 0.298700]\n",
      "14 Jan 06:49    INFO  valid result: \n",
      "recall@10 : 0.1342    mrr@10 : 0.2987    ndcg@10 : 0.1567    hit@10 : 0.5992    precision@10 : 0.1095    \n",
      "14 Jan 06:49    INFO  Saving current best: saved/BPR-Jan-14-2022_06-49-06.pth\n",
      "14 Jan 06:49    INFO  epoch 9 training [time: 0.27s, train loss: 12.8072]\n",
      "14 Jan 06:49    INFO  epoch 9 evaluating [time: 0.26s, valid_score: 0.313500]\n",
      "14 Jan 06:49    INFO  valid result: \n",
      "recall@10 : 0.1416    mrr@10 : 0.3135    ndcg@10 : 0.1648    hit@10 : 0.6151    precision@10 : 0.1126    \n",
      "14 Jan 06:49    INFO  Saving current best: saved/BPR-Jan-14-2022_06-49-06.pth\n",
      "14 Jan 06:49    INFO  epoch 10 training [time: 0.27s, train loss: 12.2494]\n",
      "14 Jan 06:49    INFO  epoch 10 evaluating [time: 0.27s, valid_score: 0.322600]\n",
      "14 Jan 06:49    INFO  valid result: \n",
      "recall@10 : 0.1491    mrr@10 : 0.3226    ndcg@10 : 0.1712    hit@10 : 0.6363    precision@10 : 0.1163    \n",
      "14 Jan 06:49    INFO  Saving current best: saved/BPR-Jan-14-2022_06-49-06.pth\n",
      "14 Jan 06:49    INFO  epoch 11 training [time: 0.28s, train loss: 11.8306]\n",
      "14 Jan 06:49    INFO  epoch 11 evaluating [time: 0.25s, valid_score: 0.326700]\n",
      "14 Jan 06:49    INFO  valid result: \n",
      "recall@10 : 0.1549    mrr@10 : 0.3267    ndcg@10 : 0.176    hit@10 : 0.6416    precision@10 : 0.1193    \n",
      "14 Jan 06:49    INFO  Saving current best: saved/BPR-Jan-14-2022_06-49-06.pth\n",
      "14 Jan 06:49    INFO  epoch 12 training [time: 0.29s, train loss: 11.3695]\n",
      "14 Jan 06:49    INFO  epoch 12 evaluating [time: 0.26s, valid_score: 0.330200]\n",
      "14 Jan 06:49    INFO  valid result: \n",
      "recall@10 : 0.1621    mrr@10 : 0.3302    ndcg@10 : 0.1801    hit@10 : 0.6585    precision@10 : 0.123    \n",
      "14 Jan 06:49    INFO  Saving current best: saved/BPR-Jan-14-2022_06-49-06.pth\n",
      "14 Jan 06:49    INFO  epoch 13 training [time: 0.26s, train loss: 10.9777]\n",
      "14 Jan 06:49    INFO  epoch 13 evaluating [time: 0.26s, valid_score: 0.338000]\n",
      "14 Jan 06:49    INFO  valid result: \n",
      "recall@10 : 0.1678    mrr@10 : 0.338    ndcg@10 : 0.1853    hit@10 : 0.6638    precision@10 : 0.1257    \n",
      "14 Jan 06:49    INFO  Saving current best: saved/BPR-Jan-14-2022_06-49-06.pth\n",
      "14 Jan 06:49    INFO  epoch 14 training [time: 0.28s, train loss: 10.5918]\n",
      "14 Jan 06:49    INFO  epoch 14 evaluating [time: 0.25s, valid_score: 0.338000]\n",
      "14 Jan 06:49    INFO  valid result: \n",
      "recall@10 : 0.1724    mrr@10 : 0.338    ndcg@10 : 0.1898    hit@10 : 0.6808    precision@10 : 0.1303    \n",
      "14 Jan 06:49    INFO  epoch 15 training [time: 0.29s, train loss: 10.2395]\n",
      "14 Jan 06:49    INFO  epoch 15 evaluating [time: 0.28s, valid_score: 0.343900]\n",
      "14 Jan 06:49    INFO  valid result: \n",
      "recall@10 : 0.178    mrr@10 : 0.3439    ndcg@10 : 0.1946    hit@10 : 0.6957    precision@10 : 0.1344    \n",
      "14 Jan 06:49    INFO  Saving current best: saved/BPR-Jan-14-2022_06-49-06.pth\n",
      "14 Jan 06:49    INFO  epoch 16 training [time: 0.28s, train loss: 9.8758]\n",
      "14 Jan 06:49    INFO  epoch 16 evaluating [time: 0.25s, valid_score: 0.347300]\n",
      "14 Jan 06:49    INFO  valid result: \n",
      "recall@10 : 0.1817    mrr@10 : 0.3473    ndcg@10 : 0.1957    hit@10 : 0.6957    precision@10 : 0.1338    \n",
      "14 Jan 06:49    INFO  Saving current best: saved/BPR-Jan-14-2022_06-49-06.pth\n",
      "14 Jan 06:49    INFO  epoch 17 training [time: 0.28s, train loss: 9.6919]\n",
      "14 Jan 06:49    INFO  epoch 17 evaluating [time: 0.26s, valid_score: 0.346100]\n",
      "14 Jan 06:49    INFO  valid result: \n",
      "recall@10 : 0.1843    mrr@10 : 0.3461    ndcg@10 : 0.1959    hit@10 : 0.702    precision@10 : 0.1352    \n",
      "14 Jan 06:49    INFO  epoch 18 training [time: 0.25s, train loss: 9.3386]\n",
      "14 Jan 06:49    INFO  epoch 18 evaluating [time: 0.28s, valid_score: 0.347900]\n",
      "14 Jan 06:49    INFO  valid result: \n",
      "recall@10 : 0.187    mrr@10 : 0.3479    ndcg@10 : 0.1983    hit@10 : 0.7031    precision@10 : 0.1369    \n",
      "14 Jan 06:49    INFO  Saving current best: saved/BPR-Jan-14-2022_06-49-06.pth\n",
      "14 Jan 06:49    INFO  epoch 19 training [time: 0.27s, train loss: 9.1299]\n",
      "14 Jan 06:49    INFO  epoch 19 evaluating [time: 0.26s, valid_score: 0.353900]\n",
      "14 Jan 06:49    INFO  valid result: \n",
      "recall@10 : 0.1897    mrr@10 : 0.3539    ndcg@10 : 0.2029    hit@10 : 0.7084    precision@10 : 0.1397    \n",
      "14 Jan 06:49    INFO  Saving current best: saved/BPR-Jan-14-2022_06-49-06.pth\n",
      "14 Jan 06:49    INFO  epoch 20 training [time: 0.28s, train loss: 8.9662]\n",
      "14 Jan 06:49    INFO  epoch 20 evaluating [time: 0.28s, valid_score: 0.353800]\n",
      "14 Jan 06:49    INFO  valid result: \n",
      "recall@10 : 0.1913    mrr@10 : 0.3538    ndcg@10 : 0.2035    hit@10 : 0.7094    precision@10 : 0.1413    \n",
      "14 Jan 06:49    INFO  epoch 21 training [time: 0.26s, train loss: 8.7419]\n",
      "14 Jan 06:49    INFO  epoch 21 evaluating [time: 0.25s, valid_score: 0.355700]\n",
      "14 Jan 06:49    INFO  valid result: \n",
      "recall@10 : 0.1969    mrr@10 : 0.3557    ndcg@10 : 0.2071    hit@10 : 0.72    precision@10 : 0.1435    \n",
      "14 Jan 06:49    INFO  Saving current best: saved/BPR-Jan-14-2022_06-49-06.pth\n",
      "14 Jan 06:49    INFO  epoch 22 training [time: 0.27s, train loss: 8.6105]\n",
      "14 Jan 06:49    INFO  epoch 22 evaluating [time: 0.26s, valid_score: 0.359500]\n",
      "14 Jan 06:49    INFO  valid result: \n",
      "recall@10 : 0.1972    mrr@10 : 0.3595    ndcg@10 : 0.2096    hit@10 : 0.72    precision@10 : 0.1446    \n",
      "14 Jan 06:49    INFO  Saving current best: saved/BPR-Jan-14-2022_06-49-06.pth\n",
      "14 Jan 06:49    INFO  epoch 23 training [time: 0.27s, train loss: 8.4964]\n",
      "14 Jan 06:49    INFO  epoch 23 evaluating [time: 0.27s, valid_score: 0.360800]\n",
      "14 Jan 06:49    INFO  valid result: \n",
      "recall@10 : 0.1966    mrr@10 : 0.3608    ndcg@10 : 0.2093    hit@10 : 0.7232    precision@10 : 0.1439    \n",
      "14 Jan 06:49    INFO  Saving current best: saved/BPR-Jan-14-2022_06-49-06.pth\n",
      "14 Jan 06:49    INFO  epoch 24 training [time: 0.27s, train loss: 8.3219]\n",
      "14 Jan 06:49    INFO  epoch 24 evaluating [time: 0.26s, valid_score: 0.363500]\n",
      "14 Jan 06:49    INFO  valid result: \n",
      "recall@10 : 0.198    mrr@10 : 0.3635    ndcg@10 : 0.2104    hit@10 : 0.7264    precision@10 : 0.1449    \n",
      "14 Jan 06:49    INFO  Saving current best: saved/BPR-Jan-14-2022_06-49-06.pth\n",
      "14 Jan 06:49    INFO  epoch 25 training [time: 0.27s, train loss: 8.1444]\n",
      "14 Jan 06:49    INFO  epoch 25 evaluating [time: 0.26s, valid_score: 0.363400]\n",
      "14 Jan 06:49    INFO  valid result: \n",
      "recall@10 : 0.1979    mrr@10 : 0.3634    ndcg@10 : 0.2108    hit@10 : 0.7285    precision@10 : 0.145    \n",
      "14 Jan 06:49    INFO  epoch 26 training [time: 0.26s, train loss: 8.0956]\n",
      "14 Jan 06:49    INFO  epoch 26 evaluating [time: 0.25s, valid_score: 0.362800]\n",
      "14 Jan 06:49    INFO  valid result: \n",
      "recall@10 : 0.196    mrr@10 : 0.3628    ndcg@10 : 0.2113    hit@10 : 0.7243    precision@10 : 0.1455    \n",
      "14 Jan 06:49    INFO  epoch 27 training [time: 0.24s, train loss: 7.9825]\n",
      "14 Jan 06:49    INFO  epoch 27 evaluating [time: 0.29s, valid_score: 0.360700]\n",
      "14 Jan 06:49    INFO  valid result: \n",
      "recall@10 : 0.1952    mrr@10 : 0.3607    ndcg@10 : 0.2114    hit@10 : 0.7222    precision@10 : 0.1469    \n",
      "14 Jan 06:49    INFO  epoch 28 training [time: 0.25s, train loss: 7.7779]\n",
      "14 Jan 06:49    INFO  epoch 28 evaluating [time: 0.25s, valid_score: 0.367900]\n",
      "14 Jan 06:49    INFO  valid result: \n",
      "recall@10 : 0.1989    mrr@10 : 0.3679    ndcg@10 : 0.2148    hit@10 : 0.7328    precision@10 : 0.1483    \n",
      "14 Jan 06:49    INFO  Saving current best: saved/BPR-Jan-14-2022_06-49-06.pth\n",
      "14 Jan 06:49    INFO  epoch 29 training [time: 0.27s, train loss: 7.7366]\n",
      "14 Jan 06:49    INFO  epoch 29 evaluating [time: 0.26s, valid_score: 0.370100]\n",
      "14 Jan 06:49    INFO  valid result: \n",
      "recall@10 : 0.1984    mrr@10 : 0.3701    ndcg@10 : 0.2151    hit@10 : 0.7317    precision@10 : 0.148    \n",
      "14 Jan 06:49    INFO  Saving current best: saved/BPR-Jan-14-2022_06-49-06.pth\n",
      "14 Jan 06:49    INFO  epoch 30 training [time: 0.25s, train loss: 7.5695]\n",
      "14 Jan 06:49    INFO  epoch 30 evaluating [time: 0.26s, valid_score: 0.371100]\n",
      "14 Jan 06:49    INFO  valid result: \n",
      "recall@10 : 0.1985    mrr@10 : 0.3711    ndcg@10 : 0.2155    hit@10 : 0.7306    precision@10 : 0.1488    \n",
      "14 Jan 06:49    INFO  Saving current best: saved/BPR-Jan-14-2022_06-49-06.pth\n",
      "14 Jan 06:49    INFO  epoch 31 training [time: 0.25s, train loss: 7.5426]\n",
      "14 Jan 06:49    INFO  epoch 31 evaluating [time: 0.26s, valid_score: 0.371800]\n",
      "14 Jan 06:49    INFO  valid result: \n",
      "recall@10 : 0.2041    mrr@10 : 0.3718    ndcg@10 : 0.2182    hit@10 : 0.7349    precision@10 : 0.1519    \n",
      "14 Jan 06:49    INFO  Saving current best: saved/BPR-Jan-14-2022_06-49-06.pth\n",
      "14 Jan 06:49    INFO  epoch 32 training [time: 0.26s, train loss: 7.3615]\n",
      "14 Jan 06:49    INFO  epoch 32 evaluating [time: 0.27s, valid_score: 0.374100]\n",
      "14 Jan 06:49    INFO  valid result: \n",
      "recall@10 : 0.2012    mrr@10 : 0.3741    ndcg@10 : 0.2172    hit@10 : 0.7296    precision@10 : 0.1499    \n",
      "14 Jan 06:49    INFO  Saving current best: saved/BPR-Jan-14-2022_06-49-06.pth\n",
      "14 Jan 06:49    INFO  epoch 33 training [time: 0.25s, train loss: 7.2530]\n",
      "14 Jan 06:49    INFO  epoch 33 evaluating [time: 0.26s, valid_score: 0.374300]\n",
      "14 Jan 06:49    INFO  valid result: \n",
      "recall@10 : 0.2021    mrr@10 : 0.3743    ndcg@10 : 0.2184    hit@10 : 0.7275    precision@10 : 0.1502    \n",
      "14 Jan 06:49    INFO  Saving current best: saved/BPR-Jan-14-2022_06-49-06.pth\n",
      "14 Jan 06:49    INFO  epoch 34 training [time: 0.26s, train loss: 7.1397]\n",
      "14 Jan 06:49    INFO  epoch 34 evaluating [time: 0.26s, valid_score: 0.372100]\n",
      "14 Jan 06:49    INFO  valid result: \n",
      "recall@10 : 0.1986    mrr@10 : 0.3721    ndcg@10 : 0.2168    hit@10 : 0.7222    precision@10 : 0.1488    \n",
      "14 Jan 06:49    INFO  epoch 35 training [time: 0.26s, train loss: 7.0707]\n",
      "14 Jan 06:49    INFO  epoch 35 evaluating [time: 0.26s, valid_score: 0.374300]\n",
      "14 Jan 06:49    INFO  valid result: \n",
      "recall@10 : 0.1998    mrr@10 : 0.3743    ndcg@10 : 0.2176    hit@10 : 0.7275    precision@10 : 0.1489    \n",
      "14 Jan 06:49    INFO  epoch 36 training [time: 0.29s, train loss: 6.9954]\n",
      "14 Jan 06:49    INFO  epoch 36 evaluating [time: 0.25s, valid_score: 0.376400]\n",
      "14 Jan 06:49    INFO  valid result: \n",
      "recall@10 : 0.2027    mrr@10 : 0.3764    ndcg@10 : 0.2198    hit@10 : 0.7296    precision@10 : 0.1505    \n",
      "14 Jan 06:49    INFO  Saving current best: saved/BPR-Jan-14-2022_06-49-06.pth\n",
      "14 Jan 06:49    INFO  epoch 37 training [time: 0.27s, train loss: 6.9096]\n",
      "14 Jan 06:49    INFO  epoch 37 evaluating [time: 0.26s, valid_score: 0.379800]\n",
      "14 Jan 06:49    INFO  valid result: \n",
      "recall@10 : 0.2037    mrr@10 : 0.3798    ndcg@10 : 0.2217    hit@10 : 0.7349    precision@10 : 0.1524    \n",
      "14 Jan 06:49    INFO  Saving current best: saved/BPR-Jan-14-2022_06-49-06.pth\n",
      "14 Jan 06:49    INFO  epoch 38 training [time: 0.27s, train loss: 6.7997]\n",
      "14 Jan 06:49    INFO  epoch 38 evaluating [time: 0.27s, valid_score: 0.378400]\n",
      "14 Jan 06:49    INFO  valid result: \n",
      "recall@10 : 0.2046    mrr@10 : 0.3784    ndcg@10 : 0.2205    hit@10 : 0.7359    precision@10 : 0.151    \n",
      "14 Jan 06:49    INFO  epoch 39 training [time: 0.25s, train loss: 6.7170]\n",
      "14 Jan 06:49    INFO  epoch 39 evaluating [time: 0.27s, valid_score: 0.379300]\n",
      "14 Jan 06:49    INFO  valid result: \n",
      "recall@10 : 0.2075    mrr@10 : 0.3793    ndcg@10 : 0.2222    hit@10 : 0.7402    precision@10 : 0.1525    \n",
      "14 Jan 06:49    INFO  epoch 40 training [time: 0.26s, train loss: 6.6869]\n",
      "14 Jan 06:49    INFO  epoch 40 evaluating [time: 0.24s, valid_score: 0.379300]\n",
      "14 Jan 06:49    INFO  valid result: \n",
      "recall@10 : 0.2118    mrr@10 : 0.3793    ndcg@10 : 0.2244    hit@10 : 0.7434    precision@10 : 0.1551    \n",
      "14 Jan 06:49    INFO  epoch 41 training [time: 0.26s, train loss: 6.5183]\n",
      "14 Jan 06:49    INFO  epoch 41 evaluating [time: 0.27s, valid_score: 0.382800]\n",
      "14 Jan 06:49    INFO  valid result: \n",
      "recall@10 : 0.2103    mrr@10 : 0.3828    ndcg@10 : 0.2248    hit@10 : 0.7466    precision@10 : 0.1538    \n",
      "14 Jan 06:49    INFO  Saving current best: saved/BPR-Jan-14-2022_06-49-06.pth\n",
      "14 Jan 06:49    INFO  epoch 42 training [time: 0.27s, train loss: 6.4468]\n",
      "14 Jan 06:49    INFO  epoch 42 evaluating [time: 0.26s, valid_score: 0.382000]\n",
      "14 Jan 06:49    INFO  valid result: \n",
      "recall@10 : 0.2074    mrr@10 : 0.382    ndcg@10 : 0.2242    hit@10 : 0.7381    precision@10 : 0.154    \n",
      "14 Jan 06:49    INFO  epoch 43 training [time: 0.27s, train loss: 6.3495]\n",
      "14 Jan 06:49    INFO  epoch 43 evaluating [time: 0.25s, valid_score: 0.382300]\n",
      "14 Jan 06:49    INFO  valid result: \n",
      "recall@10 : 0.2107    mrr@10 : 0.3823    ndcg@10 : 0.2253    hit@10 : 0.7423    precision@10 : 0.1546    \n",
      "14 Jan 06:49    INFO  epoch 44 training [time: 0.27s, train loss: 6.2826]\n",
      "14 Jan 06:49    INFO  epoch 44 evaluating [time: 0.25s, valid_score: 0.383300]\n",
      "14 Jan 06:49    INFO  valid result: \n",
      "recall@10 : 0.2099    mrr@10 : 0.3833    ndcg@10 : 0.2247    hit@10 : 0.7402    precision@10 : 0.1551    \n",
      "14 Jan 06:49    INFO  Saving current best: saved/BPR-Jan-14-2022_06-49-06.pth\n",
      "14 Jan 06:49    INFO  epoch 45 training [time: 0.27s, train loss: 6.2454]\n",
      "14 Jan 06:49    INFO  epoch 45 evaluating [time: 0.25s, valid_score: 0.383900]\n",
      "14 Jan 06:49    INFO  valid result: \n",
      "recall@10 : 0.2098    mrr@10 : 0.3839    ndcg@10 : 0.2256    hit@10 : 0.7466    precision@10 : 0.156    \n",
      "14 Jan 06:49    INFO  Saving current best: saved/BPR-Jan-14-2022_06-49-06.pth\n",
      "14 Jan 06:49    INFO  epoch 46 training [time: 0.28s, train loss: 6.0882]\n",
      "14 Jan 06:49    INFO  epoch 46 evaluating [time: 0.26s, valid_score: 0.383600]\n",
      "14 Jan 06:49    INFO  valid result: \n",
      "recall@10 : 0.2097    mrr@10 : 0.3836    ndcg@10 : 0.2256    hit@10 : 0.7476    precision@10 : 0.1564    \n",
      "14 Jan 06:49    INFO  epoch 47 training [time: 0.27s, train loss: 5.9947]\n",
      "14 Jan 06:49    INFO  epoch 47 evaluating [time: 0.27s, valid_score: 0.383800]\n",
      "14 Jan 06:49    INFO  valid result: \n",
      "recall@10 : 0.2122    mrr@10 : 0.3838    ndcg@10 : 0.227    hit@10 : 0.7508    precision@10 : 0.1576    \n",
      "14 Jan 06:49    INFO  epoch 48 training [time: 0.26s, train loss: 6.0526]\n",
      "14 Jan 06:49    INFO  epoch 48 evaluating [time: 0.27s, valid_score: 0.390600]\n",
      "14 Jan 06:49    INFO  valid result: \n",
      "recall@10 : 0.2122    mrr@10 : 0.3906    ndcg@10 : 0.2283    hit@10 : 0.7519    precision@10 : 0.1576    \n",
      "14 Jan 06:49    INFO  Saving current best: saved/BPR-Jan-14-2022_06-49-06.pth\n",
      "14 Jan 06:49    INFO  epoch 49 training [time: 0.27s, train loss: 5.8431]\n",
      "14 Jan 06:49    INFO  epoch 49 evaluating [time: 0.28s, valid_score: 0.393400]\n",
      "14 Jan 06:49    INFO  valid result: \n",
      "recall@10 : 0.2161    mrr@10 : 0.3934    ndcg@10 : 0.2321    hit@10 : 0.7656    precision@10 : 0.1601    \n",
      "14 Jan 06:49    INFO  Saving current best: saved/BPR-Jan-14-2022_06-49-06.pth\n",
      "14 Jan 06:49    INFO  epoch 50 training [time: 0.27s, train loss: 5.8239]\n",
      "14 Jan 06:49    INFO  epoch 50 evaluating [time: 0.27s, valid_score: 0.394700]\n",
      "14 Jan 06:49    INFO  valid result: \n",
      "recall@10 : 0.2158    mrr@10 : 0.3947    ndcg@10 : 0.232    hit@10 : 0.7582    precision@10 : 0.1593    \n",
      "14 Jan 06:49    INFO  Saving current best: saved/BPR-Jan-14-2022_06-49-06.pth\n",
      "14 Jan 06:49    INFO  epoch 51 training [time: 0.26s, train loss: 5.8228]\n",
      "14 Jan 06:49    INFO  epoch 51 evaluating [time: 0.25s, valid_score: 0.387500]\n",
      "14 Jan 06:49    INFO  valid result: \n",
      "recall@10 : 0.2154    mrr@10 : 0.3875    ndcg@10 : 0.2312    hit@10 : 0.7572    precision@10 : 0.1606    \n",
      "14 Jan 06:49    INFO  epoch 52 training [time: 0.27s, train loss: 5.6998]\n",
      "14 Jan 06:49    INFO  epoch 52 evaluating [time: 0.26s, valid_score: 0.388300]\n",
      "14 Jan 06:49    INFO  valid result: \n",
      "recall@10 : 0.2143    mrr@10 : 0.3883    ndcg@10 : 0.2303    hit@10 : 0.755    precision@10 : 0.1601    \n",
      "14 Jan 06:49    INFO  epoch 53 training [time: 0.26s, train loss: 5.6517]\n",
      "14 Jan 06:49    INFO  epoch 53 evaluating [time: 0.26s, valid_score: 0.387100]\n",
      "14 Jan 06:49    INFO  valid result: \n",
      "recall@10 : 0.2135    mrr@10 : 0.3871    ndcg@10 : 0.23    hit@10 : 0.7561    precision@10 : 0.1602    \n",
      "14 Jan 06:49    INFO  epoch 54 training [time: 0.26s, train loss: 5.5603]\n",
      "14 Jan 06:49    INFO  epoch 54 evaluating [time: 0.26s, valid_score: 0.391800]\n",
      "14 Jan 06:49    INFO  valid result: \n",
      "recall@10 : 0.2152    mrr@10 : 0.3918    ndcg@10 : 0.2327    hit@10 : 0.7582    precision@10 : 0.162    \n",
      "14 Jan 06:49    INFO  epoch 55 training [time: 0.26s, train loss: 5.4378]\n",
      "14 Jan 06:49    INFO  epoch 55 evaluating [time: 0.27s, valid_score: 0.390600]\n",
      "14 Jan 06:49    INFO  valid result: \n",
      "recall@10 : 0.2167    mrr@10 : 0.3906    ndcg@10 : 0.2331    hit@10 : 0.7593    precision@10 : 0.1617    \n",
      "14 Jan 06:49    INFO  epoch 56 training [time: 0.26s, train loss: 5.3834]\n",
      "14 Jan 06:49    INFO  epoch 56 evaluating [time: 0.26s, valid_score: 0.388200]\n",
      "14 Jan 06:49    INFO  valid result: \n",
      "recall@10 : 0.218    mrr@10 : 0.3882    ndcg@10 : 0.2327    hit@10 : 0.7603    precision@10 : 0.162    \n",
      "14 Jan 06:49    INFO  epoch 57 training [time: 0.26s, train loss: 5.3686]\n",
      "14 Jan 06:49    INFO  epoch 57 evaluating [time: 0.25s, valid_score: 0.385800]\n",
      "14 Jan 06:49    INFO  valid result: \n",
      "recall@10 : 0.2171    mrr@10 : 0.3858    ndcg@10 : 0.2322    hit@10 : 0.7582    precision@10 : 0.1621    \n",
      "14 Jan 06:49    INFO  epoch 58 training [time: 0.23s, train loss: 5.3178]\n",
      "14 Jan 06:49    INFO  epoch 58 evaluating [time: 0.27s, valid_score: 0.390500]\n",
      "14 Jan 06:49    INFO  valid result: \n",
      "recall@10 : 0.2169    mrr@10 : 0.3905    ndcg@10 : 0.2326    hit@10 : 0.7572    precision@10 : 0.1619    \n",
      "14 Jan 06:49    INFO  epoch 59 training [time: 0.25s, train loss: 5.2570]\n",
      "14 Jan 06:49    INFO  epoch 59 evaluating [time: 0.26s, valid_score: 0.387600]\n",
      "14 Jan 06:49    INFO  valid result: \n",
      "recall@10 : 0.2191    mrr@10 : 0.3876    ndcg@10 : 0.2318    hit@10 : 0.7593    precision@10 : 0.1616    \n",
      "14 Jan 06:49    INFO  epoch 60 training [time: 0.27s, train loss: 5.1364]\n",
      "14 Jan 06:49    INFO  epoch 60 evaluating [time: 0.25s, valid_score: 0.387700]\n",
      "14 Jan 06:49    INFO  valid result: \n",
      "recall@10 : 0.2176    mrr@10 : 0.3877    ndcg@10 : 0.2307    hit@10 : 0.7582    precision@10 : 0.161    \n",
      "14 Jan 06:49    INFO  epoch 61 training [time: 0.28s, train loss: 5.0872]\n",
      "14 Jan 06:49    INFO  epoch 61 evaluating [time: 0.27s, valid_score: 0.387700]\n",
      "14 Jan 06:49    INFO  valid result: \n",
      "recall@10 : 0.2178    mrr@10 : 0.3877    ndcg@10 : 0.2305    hit@10 : 0.7635    precision@10 : 0.1612    \n",
      "14 Jan 06:49    INFO  Finished training, best eval result in epoch 50\n",
      "14 Jan 06:49    INFO  Loading model structure and parameters from saved/BPR-Jan-14-2022_06-49-06.pth\n",
      "14 Jan 06:49    INFO  best valid : {'recall@10': 0.2158, 'mrr@10': 0.3947, 'ndcg@10': 0.232, 'hit@10': 0.7582, 'precision@10': 0.1593}\n",
      "14 Jan 06:49    INFO  test result: {'recall@10': 0.2357, 'mrr@10': 0.46, 'ndcg@10': 0.2811, 'hit@10': 0.755, 'precision@10': 0.1933}\n",
      "14 Jan 06:49    INFO  Loading model structure and parameters from saved/BPR-Jan-14-2022_06-49-06.pth\n",
      "14 Jan 06:49    INFO  test result for slice: {'recall@10': 0.2497, 'mrr@10': 0.4078, 'ndcg@10': 0.2644, 'hit@10': 0.727, 'precision@10': 0.1737}\n",
      "14 Jan 06:49    INFO  Loading model structure and parameters from saved/BPR-Jan-14-2022_06-49-06.pth\n",
      "14 Jan 06:49    INFO  test result for distributional slice: {'recall@10': 0.2333, 'mrr@10': 0.4116, 'ndcg@10': 0.2593, 'hit@10': 0.6877, 'precision@10': 0.174}\n",
      "14 Jan 06:49    INFO  BPR(\n",
      "  (user_embedding): Embedding(944, 64)\n",
      "  (item_embedding): Embedding(1683, 64)\n",
      "  (loss): BPRLoss()\n",
      ")\n",
      "Trainable parameters: 168128\n",
      "14 Jan 06:49    INFO  epoch 0 training [time: 0.23s, train loss: 27.7239]\n",
      "14 Jan 06:49    INFO  epoch 0 evaluating [time: 0.26s, valid_score: 0.018400]\n",
      "14 Jan 06:49    INFO  valid result: \n",
      "recall@10 : 0.0077    mrr@10 : 0.0184    ndcg@10 : 0.0086    hit@10 : 0.07    precision@10 : 0.0078    \n",
      "14 Jan 06:49    INFO  Saving current best: saved/BPR-Jan-14-2022_06-49-44.pth\n",
      "14 Jan 06:49    INFO  epoch 1 training [time: 0.27s, train loss: 27.6282]\n",
      "14 Jan 06:49    INFO  epoch 1 evaluating [time: 0.26s, valid_score: 0.046300]\n",
      "14 Jan 06:49    INFO  valid result: \n",
      "recall@10 : 0.016    mrr@10 : 0.0463    ndcg@10 : 0.0207    hit@10 : 0.1474    precision@10 : 0.018    \n",
      "14 Jan 06:49    INFO  Saving current best: saved/BPR-Jan-14-2022_06-49-44.pth\n",
      "14 Jan 06:49    INFO  epoch 2 training [time: 0.25s, train loss: 27.3143]\n",
      "14 Jan 06:49    INFO  epoch 2 evaluating [time: 0.26s, valid_score: 0.145800]\n",
      "14 Jan 06:49    INFO  valid result: \n",
      "recall@10 : 0.0572    mrr@10 : 0.1458    ndcg@10 : 0.073    hit@10 : 0.3807    precision@10 : 0.0613    \n",
      "14 Jan 06:49    INFO  Saving current best: saved/BPR-Jan-14-2022_06-49-44.pth\n",
      "14 Jan 06:49    INFO  epoch 3 training [time: 0.28s, train loss: 26.0582]\n",
      "14 Jan 06:49    INFO  epoch 3 evaluating [time: 0.25s, valid_score: 0.250100]\n",
      "14 Jan 06:49    INFO  valid result: \n",
      "recall@10 : 0.1139    mrr@10 : 0.2501    ndcg@10 : 0.133    hit@10 : 0.5567    precision@10 : 0.1005    \n",
      "14 Jan 06:49    INFO  Saving current best: saved/BPR-Jan-14-2022_06-49-44.pth\n",
      "14 Jan 06:49    INFO  epoch 4 training [time: 0.27s, train loss: 22.8583]\n",
      "14 Jan 06:49    INFO  epoch 4 evaluating [time: 0.26s, valid_score: 0.291400]\n",
      "14 Jan 06:49    INFO  valid result: \n",
      "recall@10 : 0.1321    mrr@10 : 0.2914    ndcg@10 : 0.1554    hit@10 : 0.6045    precision@10 : 0.1108    \n",
      "14 Jan 06:49    INFO  Saving current best: saved/BPR-Jan-14-2022_06-49-44.pth\n",
      "14 Jan 06:49    INFO  epoch 5 training [time: 0.26s, train loss: 18.6668]\n",
      "14 Jan 06:49    INFO  epoch 5 evaluating [time: 0.25s, valid_score: 0.314700]\n",
      "14 Jan 06:49    INFO  valid result: \n",
      "recall@10 : 0.1396    mrr@10 : 0.3147    ndcg@10 : 0.1664    hit@10 : 0.6257    precision@10 : 0.1143    \n",
      "14 Jan 06:49    INFO  Saving current best: saved/BPR-Jan-14-2022_06-49-44.pth\n",
      "14 Jan 06:49    INFO  epoch 6 training [time: 0.25s, train loss: 15.6845]\n",
      "14 Jan 06:49    INFO  epoch 6 evaluating [time: 0.27s, valid_score: 0.318400]\n",
      "14 Jan 06:49    INFO  valid result: \n",
      "recall@10 : 0.1451    mrr@10 : 0.3184    ndcg@10 : 0.1703    hit@10 : 0.6278    precision@10 : 0.1168    \n",
      "14 Jan 06:49    INFO  Saving current best: saved/BPR-Jan-14-2022_06-49-44.pth\n",
      "14 Jan 06:49    INFO  epoch 7 training [time: 0.24s, train loss: 14.1002]\n",
      "14 Jan 06:49    INFO  epoch 7 evaluating [time: 0.26s, valid_score: 0.323500]\n",
      "14 Jan 06:49    INFO  valid result: \n",
      "recall@10 : 0.153    mrr@10 : 0.3235    ndcg@10 : 0.1758    hit@10 : 0.6511    precision@10 : 0.1192    \n",
      "14 Jan 06:49    INFO  Saving current best: saved/BPR-Jan-14-2022_06-49-44.pth\n",
      "14 Jan 06:49    INFO  epoch 8 training [time: 0.26s, train loss: 13.1083]\n",
      "14 Jan 06:49    INFO  epoch 8 evaluating [time: 0.27s, valid_score: 0.330900]\n",
      "14 Jan 06:49    INFO  valid result: \n",
      "recall@10 : 0.161    mrr@10 : 0.3309    ndcg@10 : 0.182    hit@10 : 0.6596    precision@10 : 0.1244    \n",
      "14 Jan 06:49    INFO  Saving current best: saved/BPR-Jan-14-2022_06-49-44.pth\n",
      "14 Jan 06:49    INFO  epoch 9 training [time: 0.27s, train loss: 12.4948]\n",
      "14 Jan 06:49    INFO  epoch 9 evaluating [time: 0.26s, valid_score: 0.332400]\n",
      "14 Jan 06:49    INFO  valid result: \n",
      "recall@10 : 0.1656    mrr@10 : 0.3324    ndcg@10 : 0.1844    hit@10 : 0.667    precision@10 : 0.1261    \n",
      "14 Jan 06:49    INFO  Saving current best: saved/BPR-Jan-14-2022_06-49-44.pth\n",
      "14 Jan 06:49    INFO  epoch 10 training [time: 0.28s, train loss: 11.9265]\n",
      "14 Jan 06:49    INFO  epoch 10 evaluating [time: 0.26s, valid_score: 0.332900]\n",
      "14 Jan 06:49    INFO  valid result: \n",
      "recall@10 : 0.1676    mrr@10 : 0.3329    ndcg@10 : 0.186    hit@10 : 0.6723    precision@10 : 0.1278    \n",
      "14 Jan 06:49    INFO  Saving current best: saved/BPR-Jan-14-2022_06-49-44.pth\n",
      "14 Jan 06:49    INFO  epoch 11 training [time: 0.27s, train loss: 11.4771]\n",
      "14 Jan 06:49    INFO  epoch 11 evaluating [time: 0.27s, valid_score: 0.328500]\n",
      "14 Jan 06:49    INFO  valid result: \n",
      "recall@10 : 0.1744    mrr@10 : 0.3285    ndcg@10 : 0.1877    hit@10 : 0.6861    precision@10 : 0.1311    \n",
      "14 Jan 06:49    INFO  epoch 12 training [time: 0.26s, train loss: 11.0317]\n",
      "14 Jan 06:49    INFO  epoch 12 evaluating [time: 0.26s, valid_score: 0.331100]\n",
      "14 Jan 06:49    INFO  valid result: \n",
      "recall@10 : 0.1736    mrr@10 : 0.3311    ndcg@10 : 0.1883    hit@10 : 0.6766    precision@10 : 0.1306    \n",
      "14 Jan 06:49    INFO  epoch 13 training [time: 0.24s, train loss: 10.5698]\n",
      "14 Jan 06:49    INFO  epoch 13 evaluating [time: 0.28s, valid_score: 0.335200]\n",
      "14 Jan 06:49    INFO  valid result: \n",
      "recall@10 : 0.1759    mrr@10 : 0.3352    ndcg@10 : 0.1909    hit@10 : 0.6787    precision@10 : 0.1314    \n",
      "14 Jan 06:49    INFO  Saving current best: saved/BPR-Jan-14-2022_06-49-44.pth\n",
      "14 Jan 06:49    INFO  epoch 14 training [time: 0.27s, train loss: 10.2972]\n",
      "14 Jan 06:49    INFO  epoch 14 evaluating [time: 0.24s, valid_score: 0.338300]\n",
      "14 Jan 06:49    INFO  valid result: \n",
      "recall@10 : 0.179    mrr@10 : 0.3383    ndcg@10 : 0.1919    hit@10 : 0.6872    precision@10 : 0.1324    \n",
      "14 Jan 06:49    INFO  Saving current best: saved/BPR-Jan-14-2022_06-49-44.pth\n",
      "14 Jan 06:49    INFO  epoch 15 training [time: 0.28s, train loss: 10.0025]\n",
      "14 Jan 06:49    INFO  epoch 15 evaluating [time: 0.27s, valid_score: 0.340700]\n",
      "14 Jan 06:49    INFO  valid result: \n",
      "recall@10 : 0.1798    mrr@10 : 0.3407    ndcg@10 : 0.1937    hit@10 : 0.6978    precision@10 : 0.1338    \n",
      "14 Jan 06:49    INFO  Saving current best: saved/BPR-Jan-14-2022_06-49-44.pth\n",
      "14 Jan 06:49    INFO  epoch 16 training [time: 0.28s, train loss: 9.6900]\n",
      "14 Jan 06:49    INFO  epoch 16 evaluating [time: 0.28s, valid_score: 0.344300]\n",
      "14 Jan 06:49    INFO  valid result: \n",
      "recall@10 : 0.1814    mrr@10 : 0.3443    ndcg@10 : 0.1958    hit@10 : 0.7052    precision@10 : 0.1355    \n",
      "14 Jan 06:49    INFO  Saving current best: saved/BPR-Jan-14-2022_06-49-44.pth\n",
      "14 Jan 06:49    INFO  epoch 17 training [time: 0.27s, train loss: 9.4378]\n",
      "14 Jan 06:49    INFO  epoch 17 evaluating [time: 0.28s, valid_score: 0.346600]\n",
      "14 Jan 06:49    INFO  valid result: \n",
      "recall@10 : 0.1831    mrr@10 : 0.3466    ndcg@10 : 0.1972    hit@10 : 0.7052    precision@10 : 0.1371    \n",
      "14 Jan 06:49    INFO  Saving current best: saved/BPR-Jan-14-2022_06-49-44.pth\n",
      "14 Jan 06:49    INFO  epoch 18 training [time: 0.27s, train loss: 9.2183]\n",
      "14 Jan 06:49    INFO  epoch 18 evaluating [time: 0.28s, valid_score: 0.346600]\n",
      "14 Jan 06:49    INFO  valid result: \n",
      "recall@10 : 0.1829    mrr@10 : 0.3466    ndcg@10 : 0.1981    hit@10 : 0.7031    precision@10 : 0.1382    \n",
      "14 Jan 06:49    INFO  epoch 19 training [time: 0.24s, train loss: 9.0680]\n",
      "14 Jan 06:49    INFO  epoch 19 evaluating [time: 0.24s, valid_score: 0.353600]\n",
      "14 Jan 06:49    INFO  valid result: \n",
      "recall@10 : 0.1831    mrr@10 : 0.3536    ndcg@10 : 0.1998    hit@10 : 0.7063    precision@10 : 0.1381    \n",
      "14 Jan 06:49    INFO  Saving current best: saved/BPR-Jan-14-2022_06-49-44.pth\n",
      "14 Jan 06:49    INFO  epoch 20 training [time: 0.28s, train loss: 8.8051]\n",
      "14 Jan 06:49    INFO  epoch 20 evaluating [time: 0.26s, valid_score: 0.353900]\n",
      "14 Jan 06:49    INFO  valid result: \n",
      "recall@10 : 0.1851    mrr@10 : 0.3539    ndcg@10 : 0.2019    hit@10 : 0.7137    precision@10 : 0.1404    \n",
      "14 Jan 06:49    INFO  Saving current best: saved/BPR-Jan-14-2022_06-49-44.pth\n",
      "14 Jan 06:49    INFO  epoch 21 training [time: 0.26s, train loss: 8.7360]\n",
      "14 Jan 06:49    INFO  epoch 21 evaluating [time: 0.26s, valid_score: 0.349600]\n",
      "14 Jan 06:49    INFO  valid result: \n",
      "recall@10 : 0.1825    mrr@10 : 0.3496    ndcg@10 : 0.1998    hit@10 : 0.7073    precision@10 : 0.1386    \n",
      "14 Jan 06:49    INFO  epoch 22 training [time: 0.25s, train loss: 8.5636]\n",
      "14 Jan 06:49    INFO  epoch 22 evaluating [time: 0.25s, valid_score: 0.349300]\n",
      "14 Jan 06:49    INFO  valid result: \n",
      "recall@10 : 0.1872    mrr@10 : 0.3493    ndcg@10 : 0.2021    hit@10 : 0.7222    precision@10 : 0.1404    \n",
      "14 Jan 06:49    INFO  epoch 23 training [time: 0.26s, train loss: 8.4371]\n",
      "14 Jan 06:49    INFO  epoch 23 evaluating [time: 0.28s, valid_score: 0.351700]\n",
      "14 Jan 06:49    INFO  valid result: \n",
      "recall@10 : 0.1882    mrr@10 : 0.3517    ndcg@10 : 0.2031    hit@10 : 0.719    precision@10 : 0.1408    \n",
      "14 Jan 06:49    INFO  epoch 24 training [time: 0.29s, train loss: 8.2739]\n",
      "14 Jan 06:49    INFO  epoch 24 evaluating [time: 0.27s, valid_score: 0.356900]\n",
      "14 Jan 06:49    INFO  valid result: \n",
      "recall@10 : 0.1883    mrr@10 : 0.3569    ndcg@10 : 0.2049    hit@10 : 0.7222    precision@10 : 0.1417    \n",
      "14 Jan 06:49    INFO  Saving current best: saved/BPR-Jan-14-2022_06-49-44.pth\n",
      "14 Jan 06:49    INFO  epoch 25 training [time: 0.29s, train loss: 8.1332]\n",
      "14 Jan 06:49    INFO  epoch 25 evaluating [time: 0.26s, valid_score: 0.354000]\n",
      "14 Jan 06:49    INFO  valid result: \n",
      "recall@10 : 0.1877    mrr@10 : 0.354    ndcg@10 : 0.204    hit@10 : 0.7179    precision@10 : 0.1418    \n",
      "14 Jan 06:49    INFO  epoch 26 training [time: 0.26s, train loss: 7.9816]\n",
      "14 Jan 06:50    INFO  epoch 26 evaluating [time: 0.26s, valid_score: 0.361500]\n",
      "14 Jan 06:50    INFO  valid result: \n",
      "recall@10 : 0.1902    mrr@10 : 0.3615    ndcg@10 : 0.2076    hit@10 : 0.7211    precision@10 : 0.1437    \n",
      "14 Jan 06:50    INFO  Saving current best: saved/BPR-Jan-14-2022_06-49-44.pth\n",
      "14 Jan 06:50    INFO  epoch 27 training [time: 0.28s, train loss: 7.9198]\n",
      "14 Jan 06:50    INFO  epoch 27 evaluating [time: 0.27s, valid_score: 0.357900]\n",
      "14 Jan 06:50    INFO  valid result: \n",
      "recall@10 : 0.1951    mrr@10 : 0.3579    ndcg@10 : 0.2095    hit@10 : 0.7296    precision@10 : 0.1456    \n",
      "14 Jan 06:50    INFO  epoch 28 training [time: 0.29s, train loss: 7.7519]\n",
      "14 Jan 06:50    INFO  epoch 28 evaluating [time: 0.27s, valid_score: 0.357600]\n",
      "14 Jan 06:50    INFO  valid result: \n",
      "recall@10 : 0.1932    mrr@10 : 0.3576    ndcg@10 : 0.2088    hit@10 : 0.7275    precision@10 : 0.1449    \n",
      "14 Jan 06:50    INFO  epoch 29 training [time: 0.28s, train loss: 7.7402]\n",
      "14 Jan 06:50    INFO  epoch 29 evaluating [time: 0.27s, valid_score: 0.355000]\n",
      "14 Jan 06:50    INFO  valid result: \n",
      "recall@10 : 0.1952    mrr@10 : 0.355    ndcg@10 : 0.2096    hit@10 : 0.7285    precision@10 : 0.1462    \n",
      "14 Jan 06:50    INFO  epoch 30 training [time: 0.29s, train loss: 7.5507]\n",
      "14 Jan 06:50    INFO  epoch 30 evaluating [time: 0.25s, valid_score: 0.354800]\n",
      "14 Jan 06:50    INFO  valid result: \n",
      "recall@10 : 0.1953    mrr@10 : 0.3548    ndcg@10 : 0.2098    hit@10 : 0.737    precision@10 : 0.1474    \n",
      "14 Jan 06:50    INFO  epoch 31 training [time: 0.28s, train loss: 7.4626]\n",
      "14 Jan 06:50    INFO  epoch 31 evaluating [time: 0.26s, valid_score: 0.363200]\n",
      "14 Jan 06:50    INFO  valid result: \n",
      "recall@10 : 0.1953    mrr@10 : 0.3632    ndcg@10 : 0.2111    hit@10 : 0.7328    precision@10 : 0.1467    \n",
      "14 Jan 06:50    INFO  Saving current best: saved/BPR-Jan-14-2022_06-49-44.pth\n",
      "14 Jan 06:50    INFO  epoch 32 training [time: 0.31s, train loss: 7.2930]\n",
      "14 Jan 06:50    INFO  epoch 32 evaluating [time: 0.30s, valid_score: 0.363200]\n",
      "14 Jan 06:50    INFO  valid result: \n",
      "recall@10 : 0.1948    mrr@10 : 0.3632    ndcg@10 : 0.2111    hit@10 : 0.7349    precision@10 : 0.146    \n",
      "14 Jan 06:50    INFO  epoch 33 training [time: 0.31s, train loss: 7.2444]\n",
      "14 Jan 06:50    INFO  epoch 33 evaluating [time: 0.27s, valid_score: 0.369400]\n",
      "14 Jan 06:50    INFO  valid result: \n",
      "recall@10 : 0.1992    mrr@10 : 0.3694    ndcg@10 : 0.2148    hit@10 : 0.7349    precision@10 : 0.1477    \n",
      "14 Jan 06:50    INFO  Saving current best: saved/BPR-Jan-14-2022_06-49-44.pth\n",
      "14 Jan 06:50    INFO  epoch 34 training [time: 0.30s, train loss: 7.0941]\n",
      "14 Jan 06:50    INFO  epoch 34 evaluating [time: 0.26s, valid_score: 0.370200]\n",
      "14 Jan 06:50    INFO  valid result: \n",
      "recall@10 : 0.2008    mrr@10 : 0.3702    ndcg@10 : 0.2158    hit@10 : 0.7349    precision@10 : 0.1481    \n",
      "14 Jan 06:50    INFO  Saving current best: saved/BPR-Jan-14-2022_06-49-44.pth\n",
      "14 Jan 06:50    INFO  epoch 35 training [time: 0.31s, train loss: 7.0790]\n",
      "14 Jan 06:50    INFO  epoch 35 evaluating [time: 0.27s, valid_score: 0.372200]\n",
      "14 Jan 06:50    INFO  valid result: \n",
      "recall@10 : 0.2007    mrr@10 : 0.3722    ndcg@10 : 0.217    hit@10 : 0.7391    precision@10 : 0.149    \n",
      "14 Jan 06:50    INFO  Saving current best: saved/BPR-Jan-14-2022_06-49-44.pth\n",
      "14 Jan 06:50    INFO  epoch 36 training [time: 0.29s, train loss: 6.9289]\n",
      "14 Jan 06:50    INFO  epoch 36 evaluating [time: 0.26s, valid_score: 0.373000]\n",
      "14 Jan 06:50    INFO  valid result: \n",
      "recall@10 : 0.2014    mrr@10 : 0.373    ndcg@10 : 0.2173    hit@10 : 0.7413    precision@10 : 0.1491    \n",
      "14 Jan 06:50    INFO  Saving current best: saved/BPR-Jan-14-2022_06-49-44.pth\n",
      "14 Jan 06:50    INFO  epoch 37 training [time: 0.31s, train loss: 6.8099]\n",
      "14 Jan 06:50    INFO  epoch 37 evaluating [time: 0.26s, valid_score: 0.378200]\n",
      "14 Jan 06:50    INFO  valid result: \n",
      "recall@10 : 0.2019    mrr@10 : 0.3782    ndcg@10 : 0.2191    hit@10 : 0.7413    precision@10 : 0.1489    \n",
      "14 Jan 06:50    INFO  Saving current best: saved/BPR-Jan-14-2022_06-49-44.pth\n",
      "14 Jan 06:50    INFO  epoch 38 training [time: 0.32s, train loss: 6.7252]\n",
      "14 Jan 06:50    INFO  epoch 38 evaluating [time: 0.26s, valid_score: 0.377700]\n",
      "14 Jan 06:50    INFO  valid result: \n",
      "recall@10 : 0.1993    mrr@10 : 0.3777    ndcg@10 : 0.2186    hit@10 : 0.7349    precision@10 : 0.1498    \n",
      "14 Jan 06:50    INFO  epoch 39 training [time: 0.29s, train loss: 6.5944]\n",
      "14 Jan 06:50    INFO  epoch 39 evaluating [time: 0.30s, valid_score: 0.380300]\n",
      "14 Jan 06:50    INFO  valid result: \n",
      "recall@10 : 0.203    mrr@10 : 0.3803    ndcg@10 : 0.2205    hit@10 : 0.7423    precision@10 : 0.1517    \n",
      "14 Jan 06:50    INFO  Saving current best: saved/BPR-Jan-14-2022_06-49-44.pth\n",
      "14 Jan 06:50    INFO  epoch 40 training [time: 0.30s, train loss: 6.5411]\n",
      "14 Jan 06:50    INFO  epoch 40 evaluating [time: 0.27s, valid_score: 0.386900]\n",
      "14 Jan 06:50    INFO  valid result: \n",
      "recall@10 : 0.2048    mrr@10 : 0.3869    ndcg@10 : 0.2222    hit@10 : 0.7423    precision@10 : 0.1514    \n",
      "14 Jan 06:50    INFO  Saving current best: saved/BPR-Jan-14-2022_06-49-44.pth\n",
      "14 Jan 06:50    INFO  epoch 41 training [time: 0.29s, train loss: 6.5481]\n",
      "14 Jan 06:50    INFO  epoch 41 evaluating [time: 0.36s, valid_score: 0.384100]\n",
      "14 Jan 06:50    INFO  valid result: \n",
      "recall@10 : 0.2048    mrr@10 : 0.3841    ndcg@10 : 0.2212    hit@10 : 0.7423    precision@10 : 0.1516    \n",
      "14 Jan 06:50    INFO  epoch 42 training [time: 0.29s, train loss: 6.4236]\n",
      "14 Jan 06:50    INFO  epoch 42 evaluating [time: 0.26s, valid_score: 0.386400]\n",
      "14 Jan 06:50    INFO  valid result: \n",
      "recall@10 : 0.2033    mrr@10 : 0.3864    ndcg@10 : 0.2218    hit@10 : 0.7434    precision@10 : 0.1524    \n",
      "14 Jan 06:50    INFO  epoch 43 training [time: 0.29s, train loss: 6.3544]\n",
      "14 Jan 06:50    INFO  epoch 43 evaluating [time: 0.26s, valid_score: 0.390500]\n",
      "14 Jan 06:50    INFO  valid result: \n",
      "recall@10 : 0.2017    mrr@10 : 0.3905    ndcg@10 : 0.2224    hit@10 : 0.7381    precision@10 : 0.1513    \n",
      "14 Jan 06:50    INFO  Saving current best: saved/BPR-Jan-14-2022_06-49-44.pth\n",
      "14 Jan 06:50    INFO  epoch 44 training [time: 0.30s, train loss: 6.2723]\n",
      "14 Jan 06:50    INFO  epoch 44 evaluating [time: 0.26s, valid_score: 0.384400]\n",
      "14 Jan 06:50    INFO  valid result: \n",
      "recall@10 : 0.2047    mrr@10 : 0.3844    ndcg@10 : 0.2232    hit@10 : 0.7413    precision@10 : 0.1524    \n",
      "14 Jan 06:50    INFO  epoch 45 training [time: 0.30s, train loss: 6.2452]\n",
      "14 Jan 06:50    INFO  epoch 45 evaluating [time: 0.29s, valid_score: 0.388500]\n",
      "14 Jan 06:50    INFO  valid result: \n",
      "recall@10 : 0.2045    mrr@10 : 0.3885    ndcg@10 : 0.2231    hit@10 : 0.7444    precision@10 : 0.1515    \n",
      "14 Jan 06:50    INFO  epoch 46 training [time: 0.28s, train loss: 6.1502]\n",
      "14 Jan 06:50    INFO  epoch 46 evaluating [time: 0.26s, valid_score: 0.390600]\n",
      "14 Jan 06:50    INFO  valid result: \n",
      "recall@10 : 0.2102    mrr@10 : 0.3906    ndcg@10 : 0.2272    hit@10 : 0.7529    precision@10 : 0.156    \n",
      "14 Jan 06:50    INFO  Saving current best: saved/BPR-Jan-14-2022_06-49-44.pth\n",
      "14 Jan 06:50    INFO  epoch 47 training [time: 0.28s, train loss: 5.9913]\n",
      "14 Jan 06:50    INFO  epoch 47 evaluating [time: 0.27s, valid_score: 0.389100]\n",
      "14 Jan 06:50    INFO  valid result: \n",
      "recall@10 : 0.2079    mrr@10 : 0.3891    ndcg@10 : 0.2255    hit@10 : 0.7455    precision@10 : 0.1546    \n",
      "14 Jan 06:50    INFO  epoch 48 training [time: 0.30s, train loss: 5.9595]\n",
      "14 Jan 06:50    INFO  epoch 48 evaluating [time: 0.27s, valid_score: 0.385800]\n",
      "14 Jan 06:50    INFO  valid result: \n",
      "recall@10 : 0.2079    mrr@10 : 0.3858    ndcg@10 : 0.225    hit@10 : 0.7476    precision@10 : 0.1548    \n",
      "14 Jan 06:50    INFO  epoch 49 training [time: 0.30s, train loss: 5.9405]\n",
      "14 Jan 06:50    INFO  epoch 49 evaluating [time: 0.26s, valid_score: 0.385400]\n",
      "14 Jan 06:50    INFO  valid result: \n",
      "recall@10 : 0.2054    mrr@10 : 0.3854    ndcg@10 : 0.2236    hit@10 : 0.7444    precision@10 : 0.1538    \n",
      "14 Jan 06:50    INFO  epoch 50 training [time: 0.29s, train loss: 5.8108]\n",
      "14 Jan 06:50    INFO  epoch 50 evaluating [time: 0.27s, valid_score: 0.382200]\n",
      "14 Jan 06:50    INFO  valid result: \n",
      "recall@10 : 0.209    mrr@10 : 0.3822    ndcg@10 : 0.2247    hit@10 : 0.7455    precision@10 : 0.1556    \n",
      "14 Jan 06:50    INFO  epoch 51 training [time: 0.29s, train loss: 5.7302]\n",
      "14 Jan 06:50    INFO  epoch 51 evaluating [time: 0.27s, valid_score: 0.385000]\n",
      "14 Jan 06:50    INFO  valid result: \n",
      "recall@10 : 0.208    mrr@10 : 0.385    ndcg@10 : 0.2249    hit@10 : 0.7434    precision@10 : 0.1545    \n",
      "14 Jan 06:50    INFO  epoch 52 training [time: 0.30s, train loss: 5.6632]\n",
      "14 Jan 06:50    INFO  epoch 52 evaluating [time: 0.26s, valid_score: 0.384300]\n",
      "14 Jan 06:50    INFO  valid result: \n",
      "recall@10 : 0.2087    mrr@10 : 0.3843    ndcg@10 : 0.2254    hit@10 : 0.7455    precision@10 : 0.1544    \n",
      "14 Jan 06:50    INFO  epoch 53 training [time: 0.30s, train loss: 5.6636]\n",
      "14 Jan 06:50    INFO  epoch 53 evaluating [time: 0.26s, valid_score: 0.390000]\n",
      "14 Jan 06:50    INFO  valid result: \n",
      "recall@10 : 0.2089    mrr@10 : 0.39    ndcg@10 : 0.2272    hit@10 : 0.7455    precision@10 : 0.155    \n",
      "14 Jan 06:50    INFO  epoch 54 training [time: 0.28s, train loss: 5.5362]\n",
      "14 Jan 06:50    INFO  epoch 54 evaluating [time: 0.26s, valid_score: 0.389300]\n",
      "14 Jan 06:50    INFO  valid result: \n",
      "recall@10 : 0.2106    mrr@10 : 0.3893    ndcg@10 : 0.2272    hit@10 : 0.7497    precision@10 : 0.1556    \n",
      "14 Jan 06:50    INFO  epoch 55 training [time: 0.30s, train loss: 5.4795]\n",
      "14 Jan 06:50    INFO  epoch 55 evaluating [time: 0.26s, valid_score: 0.388000]\n",
      "14 Jan 06:50    INFO  valid result: \n",
      "recall@10 : 0.211    mrr@10 : 0.388    ndcg@10 : 0.2273    hit@10 : 0.7466    precision@10 : 0.1556    \n",
      "14 Jan 06:50    INFO  epoch 56 training [time: 0.28s, train loss: 5.4265]\n",
      "14 Jan 06:50    INFO  epoch 56 evaluating [time: 0.26s, valid_score: 0.387500]\n",
      "14 Jan 06:50    INFO  valid result: \n",
      "recall@10 : 0.2111    mrr@10 : 0.3875    ndcg@10 : 0.228    hit@10 : 0.7455    precision@10 : 0.1557    \n",
      "14 Jan 06:50    INFO  epoch 57 training [time: 0.27s, train loss: 5.4008]\n",
      "14 Jan 06:50    INFO  epoch 57 evaluating [time: 0.26s, valid_score: 0.392900]\n",
      "14 Jan 06:50    INFO  valid result: \n",
      "recall@10 : 0.2113    mrr@10 : 0.3929    ndcg@10 : 0.2299    hit@10 : 0.7423    precision@10 : 0.1552    \n",
      "14 Jan 06:50    INFO  Saving current best: saved/BPR-Jan-14-2022_06-49-44.pth\n",
      "14 Jan 06:50    INFO  epoch 58 training [time: 0.27s, train loss: 5.3204]\n",
      "14 Jan 06:50    INFO  epoch 58 evaluating [time: 0.27s, valid_score: 0.392000]\n",
      "14 Jan 06:50    INFO  valid result: \n",
      "recall@10 : 0.2124    mrr@10 : 0.392    ndcg@10 : 0.2298    hit@10 : 0.7508    precision@10 : 0.1552    \n",
      "14 Jan 06:50    INFO  epoch 59 training [time: 0.27s, train loss: 5.2175]\n",
      "14 Jan 06:50    INFO  epoch 59 evaluating [time: 0.27s, valid_score: 0.394200]\n",
      "14 Jan 06:50    INFO  valid result: \n",
      "recall@10 : 0.2128    mrr@10 : 0.3942    ndcg@10 : 0.2304    hit@10 : 0.754    precision@10 : 0.1561    \n",
      "14 Jan 06:50    INFO  Saving current best: saved/BPR-Jan-14-2022_06-49-44.pth\n",
      "14 Jan 06:50    INFO  epoch 60 training [time: 0.31s, train loss: 5.1855]\n",
      "14 Jan 06:50    INFO  epoch 60 evaluating [time: 0.26s, valid_score: 0.395400]\n",
      "14 Jan 06:50    INFO  valid result: \n",
      "recall@10 : 0.2115    mrr@10 : 0.3954    ndcg@10 : 0.2303    hit@10 : 0.7529    precision@10 : 0.156    \n",
      "14 Jan 06:50    INFO  Saving current best: saved/BPR-Jan-14-2022_06-49-44.pth\n",
      "14 Jan 06:50    INFO  epoch 61 training [time: 0.28s, train loss: 5.1096]\n",
      "14 Jan 06:50    INFO  epoch 61 evaluating [time: 0.27s, valid_score: 0.392800]\n",
      "14 Jan 06:50    INFO  valid result: \n",
      "recall@10 : 0.2127    mrr@10 : 0.3928    ndcg@10 : 0.2303    hit@10 : 0.754    precision@10 : 0.1565    \n",
      "14 Jan 06:50    INFO  epoch 62 training [time: 0.30s, train loss: 5.1510]\n",
      "14 Jan 06:50    INFO  epoch 62 evaluating [time: 0.27s, valid_score: 0.391200]\n",
      "14 Jan 06:50    INFO  valid result: \n",
      "recall@10 : 0.212    mrr@10 : 0.3912    ndcg@10 : 0.2302    hit@10 : 0.7476    precision@10 : 0.1572    \n",
      "14 Jan 06:50    INFO  epoch 63 training [time: 0.28s, train loss: 5.0026]\n",
      "14 Jan 06:50    INFO  epoch 63 evaluating [time: 0.27s, valid_score: 0.383500]\n",
      "14 Jan 06:50    INFO  valid result: \n",
      "recall@10 : 0.2121    mrr@10 : 0.3835    ndcg@10 : 0.2283    hit@10 : 0.7519    precision@10 : 0.1567    \n",
      "14 Jan 06:50    INFO  epoch 64 training [time: 0.27s, train loss: 4.9368]\n",
      "14 Jan 06:50    INFO  epoch 64 evaluating [time: 0.27s, valid_score: 0.390200]\n",
      "14 Jan 06:50    INFO  valid result: \n",
      "recall@10 : 0.2167    mrr@10 : 0.3902    ndcg@10 : 0.2312    hit@10 : 0.7561    precision@10 : 0.1576    \n",
      "14 Jan 06:50    INFO  epoch 65 training [time: 0.29s, train loss: 4.8780]\n",
      "14 Jan 06:50    INFO  epoch 65 evaluating [time: 0.26s, valid_score: 0.381800]\n",
      "14 Jan 06:50    INFO  valid result: \n",
      "recall@10 : 0.216    mrr@10 : 0.3818    ndcg@10 : 0.2299    hit@10 : 0.7572    precision@10 : 0.1586    \n",
      "14 Jan 06:50    INFO  epoch 66 training [time: 0.27s, train loss: 4.8280]\n",
      "14 Jan 06:50    INFO  epoch 66 evaluating [time: 0.25s, valid_score: 0.387200]\n",
      "14 Jan 06:50    INFO  valid result: \n",
      "recall@10 : 0.2146    mrr@10 : 0.3872    ndcg@10 : 0.2302    hit@10 : 0.754    precision@10 : 0.1574    \n",
      "14 Jan 06:50    INFO  epoch 67 training [time: 0.29s, train loss: 4.8800]\n",
      "14 Jan 06:50    INFO  epoch 67 evaluating [time: 0.26s, valid_score: 0.386400]\n",
      "14 Jan 06:50    INFO  valid result: \n",
      "recall@10 : 0.2149    mrr@10 : 0.3864    ndcg@10 : 0.2298    hit@10 : 0.7603    precision@10 : 0.1578    \n",
      "14 Jan 06:50    INFO  epoch 68 training [time: 0.29s, train loss: 4.7220]\n",
      "14 Jan 06:50    INFO  epoch 68 evaluating [time: 0.27s, valid_score: 0.384800]\n",
      "14 Jan 06:50    INFO  valid result: \n",
      "recall@10 : 0.2144    mrr@10 : 0.3848    ndcg@10 : 0.229    hit@10 : 0.7614    precision@10 : 0.1569    \n",
      "14 Jan 06:50    INFO  epoch 69 training [time: 0.29s, train loss: 4.7086]\n",
      "14 Jan 06:50    INFO  epoch 69 evaluating [time: 0.26s, valid_score: 0.385600]\n",
      "14 Jan 06:50    INFO  valid result: \n",
      "recall@10 : 0.2158    mrr@10 : 0.3856    ndcg@10 : 0.23    hit@10 : 0.7572    precision@10 : 0.1576    \n",
      "14 Jan 06:50    INFO  epoch 70 training [time: 0.26s, train loss: 4.6354]\n",
      "14 Jan 06:50    INFO  epoch 70 evaluating [time: 0.27s, valid_score: 0.386200]\n",
      "14 Jan 06:50    INFO  valid result: \n",
      "recall@10 : 0.2153    mrr@10 : 0.3862    ndcg@10 : 0.229    hit@10 : 0.7529    precision@10 : 0.1563    \n",
      "14 Jan 06:50    INFO  epoch 71 training [time: 0.28s, train loss: 4.6148]\n",
      "14 Jan 06:50    INFO  epoch 71 evaluating [time: 0.26s, valid_score: 0.387100]\n",
      "14 Jan 06:50    INFO  valid result: \n",
      "recall@10 : 0.2137    mrr@10 : 0.3871    ndcg@10 : 0.2283    hit@10 : 0.7508    precision@10 : 0.156    \n",
      "14 Jan 06:50    INFO  Finished training, best eval result in epoch 60\n",
      "14 Jan 06:50    INFO  Loading model structure and parameters from saved/BPR-Jan-14-2022_06-49-44.pth\n",
      "14 Jan 06:50    INFO  best valid for transformed training set: {'recall@10': 0.2115, 'mrr@10': 0.3954, 'ndcg@10': 0.2303, 'hit@10': 0.7529, 'precision@10': 0.156}\n",
      "14 Jan 06:50    INFO  test result for transformed training set: {'recall@10': 0.2404, 'mrr@10': 0.4621, 'ndcg@10': 0.2825, 'hit@10': 0.7646, 'precision@10': 0.194}\n",
      "14 Jan 06:50    INFO  BPR(\n",
      "  (user_embedding): Embedding(944, 64)\n",
      "  (item_embedding): Embedding(1683, 64)\n",
      "  (loss): BPRLoss()\n",
      ")\n",
      "Trainable parameters: 168128\n",
      "14 Jan 06:50    INFO  epoch 0 training [time: 0.28s, train loss: 26.3398]\n",
      "14 Jan 06:50    INFO  epoch 0 evaluating [time: 0.24s, valid_score: 0.019300]\n",
      "14 Jan 06:50    INFO  valid result: \n",
      "recall@10 : 0.0071    mrr@10 : 0.0193    ndcg@10 : 0.0081    hit@10 : 0.0647    precision@10 : 0.0071    \n",
      "14 Jan 06:50    INFO  Saving current best: saved/BPR-Jan-14-2022_06-50-28.pth\n",
      "14 Jan 06:50    INFO  epoch 1 training [time: 0.28s, train loss: 26.2592]\n",
      "14 Jan 06:50    INFO  epoch 1 evaluating [time: 0.26s, valid_score: 0.039600]\n",
      "14 Jan 06:50    INFO  valid result: \n",
      "recall@10 : 0.0121    mrr@10 : 0.0396    ndcg@10 : 0.0164    hit@10 : 0.1156    precision@10 : 0.0138    \n",
      "14 Jan 06:50    INFO  Saving current best: saved/BPR-Jan-14-2022_06-50-28.pth\n",
      "14 Jan 06:50    INFO  epoch 2 training [time: 0.30s, train loss: 26.0275]\n",
      "14 Jan 06:50    INFO  epoch 2 evaluating [time: 0.25s, valid_score: 0.129200]\n",
      "14 Jan 06:50    INFO  valid result: \n",
      "recall@10 : 0.0405    mrr@10 : 0.1292    ndcg@10 : 0.0595    hit@10 : 0.3128    precision@10 : 0.0494    \n",
      "14 Jan 06:50    INFO  Saving current best: saved/BPR-Jan-14-2022_06-50-28.pth\n",
      "14 Jan 06:50    INFO  epoch 3 training [time: 0.26s, train loss: 25.1009]\n",
      "14 Jan 06:50    INFO  epoch 3 evaluating [time: 0.25s, valid_score: 0.205700]\n",
      "14 Jan 06:50    INFO  valid result: \n",
      "recall@10 : 0.082    mrr@10 : 0.2057    ndcg@10 : 0.1081    hit@10 : 0.4581    precision@10 : 0.0864    \n",
      "14 Jan 06:50    INFO  Saving current best: saved/BPR-Jan-14-2022_06-50-28.pth\n",
      "14 Jan 06:50    INFO  epoch 4 training [time: 0.24s, train loss: 22.5234]\n",
      "14 Jan 06:50    INFO  epoch 4 evaluating [time: 0.27s, valid_score: 0.257400]\n",
      "14 Jan 06:50    INFO  valid result: \n",
      "recall@10 : 0.1039    mrr@10 : 0.2574    ndcg@10 : 0.1345    hit@10 : 0.526    precision@10 : 0.0994    \n",
      "14 Jan 06:50    INFO  Saving current best: saved/BPR-Jan-14-2022_06-50-28.pth\n",
      "14 Jan 06:50    INFO  epoch 5 training [time: 0.25s, train loss: 18.6407]\n",
      "14 Jan 06:50    INFO  epoch 5 evaluating [time: 0.26s, valid_score: 0.284400]\n",
      "14 Jan 06:50    INFO  valid result: \n",
      "recall@10 : 0.1191    mrr@10 : 0.2844    ndcg@10 : 0.148    hit@10 : 0.5705    precision@10 : 0.1063    \n",
      "14 Jan 06:50    INFO  Saving current best: saved/BPR-Jan-14-2022_06-50-28.pth\n",
      "14 Jan 06:50    INFO  epoch 6 training [time: 0.24s, train loss: 15.5415]\n",
      "14 Jan 06:50    INFO  epoch 6 evaluating [time: 0.28s, valid_score: 0.292300]\n",
      "14 Jan 06:50    INFO  valid result: \n",
      "recall@10 : 0.1258    mrr@10 : 0.2923    ndcg@10 : 0.153    hit@10 : 0.5811    precision@10 : 0.1083    \n",
      "14 Jan 06:50    INFO  Saving current best: saved/BPR-Jan-14-2022_06-50-28.pth\n",
      "14 Jan 06:50    INFO  epoch 7 training [time: 0.24s, train loss: 13.8529]\n",
      "14 Jan 06:50    INFO  epoch 7 evaluating [time: 0.27s, valid_score: 0.301700]\n",
      "14 Jan 06:50    INFO  valid result: \n",
      "recall@10 : 0.1315    mrr@10 : 0.3017    ndcg@10 : 0.1574    hit@10 : 0.6013    precision@10 : 0.1107    \n",
      "14 Jan 06:50    INFO  Saving current best: saved/BPR-Jan-14-2022_06-50-28.pth\n",
      "14 Jan 06:50    INFO  epoch 8 training [time: 0.25s, train loss: 12.9565]\n",
      "14 Jan 06:50    INFO  epoch 8 evaluating [time: 0.25s, valid_score: 0.305100]\n",
      "14 Jan 06:50    INFO  valid result: \n",
      "recall@10 : 0.1377    mrr@10 : 0.3051    ndcg@10 : 0.1615    hit@10 : 0.6119    precision@10 : 0.1122    \n",
      "14 Jan 06:50    INFO  Saving current best: saved/BPR-Jan-14-2022_06-50-28.pth\n",
      "14 Jan 06:50    INFO  epoch 9 training [time: 0.24s, train loss: 12.3408]\n",
      "14 Jan 06:50    INFO  epoch 9 evaluating [time: 0.26s, valid_score: 0.311100]\n",
      "14 Jan 06:50    INFO  valid result: \n",
      "recall@10 : 0.1428    mrr@10 : 0.3111    ndcg@10 : 0.166    hit@10 : 0.6235    precision@10 : 0.1145    \n",
      "14 Jan 06:50    INFO  Saving current best: saved/BPR-Jan-14-2022_06-50-28.pth\n",
      "14 Jan 06:50    INFO  epoch 10 training [time: 0.27s, train loss: 11.8648]\n",
      "14 Jan 06:50    INFO  epoch 10 evaluating [time: 0.27s, valid_score: 0.314900]\n",
      "14 Jan 06:50    INFO  valid result: \n",
      "recall@10 : 0.1503    mrr@10 : 0.3149    ndcg@10 : 0.1721    hit@10 : 0.6299    precision@10 : 0.1189    \n",
      "14 Jan 06:50    INFO  Saving current best: saved/BPR-Jan-14-2022_06-50-28.pth\n",
      "14 Jan 06:50    INFO  epoch 11 training [time: 0.24s, train loss: 11.3977]\n",
      "14 Jan 06:50    INFO  epoch 11 evaluating [time: 0.27s, valid_score: 0.327100]\n",
      "14 Jan 06:50    INFO  valid result: \n",
      "recall@10 : 0.1564    mrr@10 : 0.3271    ndcg@10 : 0.1782    hit@10 : 0.6522    precision@10 : 0.1227    \n",
      "14 Jan 06:50    INFO  Saving current best: saved/BPR-Jan-14-2022_06-50-28.pth\n",
      "14 Jan 06:50    INFO  epoch 12 training [time: 0.27s, train loss: 11.0332]\n",
      "14 Jan 06:50    INFO  epoch 12 evaluating [time: 0.27s, valid_score: 0.329600]\n",
      "14 Jan 06:50    INFO  valid result: \n",
      "recall@10 : 0.1601    mrr@10 : 0.3296    ndcg@10 : 0.1813    hit@10 : 0.6596    precision@10 : 0.1251    \n",
      "14 Jan 06:50    INFO  Saving current best: saved/BPR-Jan-14-2022_06-50-28.pth\n",
      "14 Jan 06:50    INFO  epoch 13 training [time: 0.26s, train loss: 10.6654]\n",
      "14 Jan 06:50    INFO  epoch 13 evaluating [time: 0.28s, valid_score: 0.334800]\n",
      "14 Jan 06:50    INFO  valid result: \n",
      "recall@10 : 0.1627    mrr@10 : 0.3348    ndcg@10 : 0.1845    hit@10 : 0.6575    precision@10 : 0.1269    \n",
      "14 Jan 06:50    INFO  Saving current best: saved/BPR-Jan-14-2022_06-50-28.pth\n",
      "14 Jan 06:50    INFO  epoch 14 training [time: 0.24s, train loss: 10.2685]\n",
      "14 Jan 06:50    INFO  epoch 14 evaluating [time: 0.26s, valid_score: 0.333700]\n",
      "14 Jan 06:50    INFO  valid result: \n",
      "recall@10 : 0.1691    mrr@10 : 0.3337    ndcg@10 : 0.1875    hit@10 : 0.6691    precision@10 : 0.1296    \n",
      "14 Jan 06:50    INFO  epoch 15 training [time: 0.25s, train loss: 9.9933]\n",
      "14 Jan 06:50    INFO  epoch 15 evaluating [time: 0.26s, valid_score: 0.336600]\n",
      "14 Jan 06:50    INFO  valid result: \n",
      "recall@10 : 0.1709    mrr@10 : 0.3366    ndcg@10 : 0.1898    hit@10 : 0.6702    precision@10 : 0.1316    \n",
      "14 Jan 06:50    INFO  Saving current best: saved/BPR-Jan-14-2022_06-50-28.pth\n",
      "14 Jan 06:50    INFO  epoch 16 training [time: 0.26s, train loss: 9.6991]\n",
      "14 Jan 06:50    INFO  epoch 16 evaluating [time: 0.26s, valid_score: 0.341000]\n",
      "14 Jan 06:50    INFO  valid result: \n",
      "recall@10 : 0.1734    mrr@10 : 0.341    ndcg@10 : 0.1924    hit@10 : 0.6776    precision@10 : 0.1324    \n",
      "14 Jan 06:50    INFO  Saving current best: saved/BPR-Jan-14-2022_06-50-28.pth\n",
      "14 Jan 06:50    INFO  epoch 17 training [time: 0.27s, train loss: 9.3947]\n",
      "14 Jan 06:50    INFO  epoch 17 evaluating [time: 0.25s, valid_score: 0.344300]\n",
      "14 Jan 06:50    INFO  valid result: \n",
      "recall@10 : 0.1761    mrr@10 : 0.3443    ndcg@10 : 0.1949    hit@10 : 0.6829    precision@10 : 0.134    \n",
      "14 Jan 06:50    INFO  Saving current best: saved/BPR-Jan-14-2022_06-50-28.pth\n",
      "14 Jan 06:50    INFO  epoch 18 training [time: 0.24s, train loss: 9.2361]\n",
      "14 Jan 06:50    INFO  epoch 18 evaluating [time: 0.27s, valid_score: 0.347100]\n",
      "14 Jan 06:50    INFO  valid result: \n",
      "recall@10 : 0.1777    mrr@10 : 0.3471    ndcg@10 : 0.1973    hit@10 : 0.6882    precision@10 : 0.1363    \n",
      "14 Jan 06:50    INFO  Saving current best: saved/BPR-Jan-14-2022_06-50-28.pth\n",
      "14 Jan 06:50    INFO  epoch 19 training [time: 0.25s, train loss: 8.9687]\n",
      "14 Jan 06:50    INFO  epoch 19 evaluating [time: 0.25s, valid_score: 0.343200]\n",
      "14 Jan 06:50    INFO  valid result: \n",
      "recall@10 : 0.1824    mrr@10 : 0.3432    ndcg@10 : 0.1979    hit@10 : 0.6946    precision@10 : 0.1371    \n",
      "14 Jan 06:50    INFO  epoch 20 training [time: 0.25s, train loss: 8.8119]\n",
      "14 Jan 06:50    INFO  epoch 20 evaluating [time: 0.27s, valid_score: 0.342000]\n",
      "14 Jan 06:50    INFO  valid result: \n",
      "recall@10 : 0.1808    mrr@10 : 0.342    ndcg@10 : 0.1976    hit@10 : 0.6946    precision@10 : 0.1374    \n",
      "14 Jan 06:50    INFO  epoch 21 training [time: 0.25s, train loss: 8.5039]\n",
      "14 Jan 06:50    INFO  epoch 21 evaluating [time: 0.26s, valid_score: 0.343200]\n",
      "14 Jan 06:50    INFO  valid result: \n",
      "recall@10 : 0.1834    mrr@10 : 0.3432    ndcg@10 : 0.1983    hit@10 : 0.7031    precision@10 : 0.1389    \n",
      "14 Jan 06:50    INFO  epoch 22 training [time: 0.26s, train loss: 8.3649]\n",
      "14 Jan 06:50    INFO  epoch 22 evaluating [time: 0.27s, valid_score: 0.346900]\n",
      "14 Jan 06:50    INFO  valid result: \n",
      "recall@10 : 0.1837    mrr@10 : 0.3469    ndcg@10 : 0.2005    hit@10 : 0.702    precision@10 : 0.1394    \n",
      "14 Jan 06:50    INFO  epoch 23 training [time: 0.26s, train loss: 8.2701]\n",
      "14 Jan 06:50    INFO  epoch 23 evaluating [time: 0.28s, valid_score: 0.354100]\n",
      "14 Jan 06:50    INFO  valid result: \n",
      "recall@10 : 0.1882    mrr@10 : 0.3541    ndcg@10 : 0.2049    hit@10 : 0.7094    precision@10 : 0.1421    \n",
      "14 Jan 06:50    INFO  Saving current best: saved/BPR-Jan-14-2022_06-50-28.pth\n",
      "14 Jan 06:50    INFO  epoch 24 training [time: 0.27s, train loss: 8.0260]\n",
      "14 Jan 06:50    INFO  epoch 24 evaluating [time: 0.26s, valid_score: 0.358800]\n",
      "14 Jan 06:50    INFO  valid result: \n",
      "recall@10 : 0.1872    mrr@10 : 0.3588    ndcg@10 : 0.2058    hit@10 : 0.7126    precision@10 : 0.1417    \n",
      "14 Jan 06:50    INFO  Saving current best: saved/BPR-Jan-14-2022_06-50-28.pth\n",
      "14 Jan 06:50    INFO  epoch 25 training [time: 0.26s, train loss: 7.9360]\n",
      "14 Jan 06:50    INFO  epoch 25 evaluating [time: 0.26s, valid_score: 0.356400]\n",
      "14 Jan 06:50    INFO  valid result: \n",
      "recall@10 : 0.1887    mrr@10 : 0.3564    ndcg@10 : 0.2065    hit@10 : 0.7094    precision@10 : 0.1414    \n",
      "14 Jan 06:50    INFO  epoch 26 training [time: 0.24s, train loss: 7.7365]\n",
      "14 Jan 06:50    INFO  epoch 26 evaluating [time: 0.26s, valid_score: 0.359200]\n",
      "14 Jan 06:50    INFO  valid result: \n",
      "recall@10 : 0.1851    mrr@10 : 0.3592    ndcg@10 : 0.2062    hit@10 : 0.702    precision@10 : 0.1409    \n",
      "14 Jan 06:50    INFO  Saving current best: saved/BPR-Jan-14-2022_06-50-28.pth\n",
      "14 Jan 06:50    INFO  epoch 27 training [time: 0.26s, train loss: 7.6582]\n",
      "14 Jan 06:50    INFO  epoch 27 evaluating [time: 0.27s, valid_score: 0.363200]\n",
      "14 Jan 06:50    INFO  valid result: \n",
      "recall@10 : 0.1859    mrr@10 : 0.3632    ndcg@10 : 0.2074    hit@10 : 0.7094    precision@10 : 0.1414    \n",
      "14 Jan 06:50    INFO  Saving current best: saved/BPR-Jan-14-2022_06-50-28.pth\n",
      "14 Jan 06:50    INFO  epoch 28 training [time: 0.25s, train loss: 7.4516]\n",
      "14 Jan 06:50    INFO  epoch 28 evaluating [time: 0.28s, valid_score: 0.367800]\n",
      "14 Jan 06:50    INFO  valid result: \n",
      "recall@10 : 0.19    mrr@10 : 0.3678    ndcg@10 : 0.2109    hit@10 : 0.7169    precision@10 : 0.1437    \n",
      "14 Jan 06:50    INFO  Saving current best: saved/BPR-Jan-14-2022_06-50-28.pth\n",
      "14 Jan 06:50    INFO  epoch 29 training [time: 0.26s, train loss: 7.4171]\n",
      "14 Jan 06:50    INFO  epoch 29 evaluating [time: 0.26s, valid_score: 0.371300]\n",
      "14 Jan 06:50    INFO  valid result: \n",
      "recall@10 : 0.1923    mrr@10 : 0.3713    ndcg@10 : 0.2131    hit@10 : 0.7222    precision@10 : 0.1454    \n",
      "14 Jan 06:50    INFO  Saving current best: saved/BPR-Jan-14-2022_06-50-28.pth\n",
      "14 Jan 06:50    INFO  epoch 30 training [time: 0.29s, train loss: 7.2952]\n",
      "14 Jan 06:50    INFO  epoch 30 evaluating [time: 0.27s, valid_score: 0.366200]\n",
      "14 Jan 06:50    INFO  valid result: \n",
      "recall@10 : 0.1922    mrr@10 : 0.3662    ndcg@10 : 0.2116    hit@10 : 0.7169    precision@10 : 0.1449    \n",
      "14 Jan 06:50    INFO  epoch 31 training [time: 0.28s, train loss: 7.1175]\n",
      "14 Jan 06:50    INFO  epoch 31 evaluating [time: 0.26s, valid_score: 0.364600]\n",
      "14 Jan 06:50    INFO  valid result: \n",
      "recall@10 : 0.1941    mrr@10 : 0.3646    ndcg@10 : 0.2129    hit@10 : 0.7179    precision@10 : 0.1467    \n",
      "14 Jan 06:50    INFO  epoch 32 training [time: 0.27s, train loss: 7.0413]\n",
      "14 Jan 06:50    INFO  epoch 32 evaluating [time: 0.27s, valid_score: 0.358700]\n",
      "14 Jan 06:50    INFO  valid result: \n",
      "recall@10 : 0.1939    mrr@10 : 0.3587    ndcg@10 : 0.212    hit@10 : 0.7137    precision@10 : 0.1462    \n",
      "14 Jan 06:50    INFO  epoch 33 training [time: 0.26s, train loss: 6.9500]\n",
      "14 Jan 06:50    INFO  epoch 33 evaluating [time: 0.27s, valid_score: 0.363500]\n",
      "14 Jan 06:50    INFO  valid result: \n",
      "recall@10 : 0.1948    mrr@10 : 0.3635    ndcg@10 : 0.2134    hit@10 : 0.7253    precision@10 : 0.1471    \n",
      "14 Jan 06:50    INFO  epoch 34 training [time: 0.27s, train loss: 6.8463]\n",
      "14 Jan 06:50    INFO  epoch 34 evaluating [time: 0.28s, valid_score: 0.364300]\n",
      "14 Jan 06:50    INFO  valid result: \n",
      "recall@10 : 0.1952    mrr@10 : 0.3643    ndcg@10 : 0.2144    hit@10 : 0.7169    precision@10 : 0.1476    \n",
      "14 Jan 06:50    INFO  epoch 35 training [time: 0.23s, train loss: 6.7471]\n",
      "14 Jan 06:50    INFO  epoch 35 evaluating [time: 0.27s, valid_score: 0.367400]\n",
      "14 Jan 06:50    INFO  valid result: \n",
      "recall@10 : 0.2011    mrr@10 : 0.3674    ndcg@10 : 0.218    hit@10 : 0.7317    precision@10 : 0.1515    \n",
      "14 Jan 06:50    INFO  epoch 36 training [time: 0.26s, train loss: 6.6636]\n",
      "14 Jan 06:50    INFO  epoch 36 evaluating [time: 0.25s, valid_score: 0.368800]\n",
      "14 Jan 06:50    INFO  valid result: \n",
      "recall@10 : 0.2013    mrr@10 : 0.3688    ndcg@10 : 0.2191    hit@10 : 0.7285    precision@10 : 0.1515    \n",
      "14 Jan 06:50    INFO  epoch 37 training [time: 0.26s, train loss: 6.5694]\n",
      "14 Jan 06:50    INFO  epoch 37 evaluating [time: 0.27s, valid_score: 0.369400]\n",
      "14 Jan 06:50    INFO  valid result: \n",
      "recall@10 : 0.2005    mrr@10 : 0.3694    ndcg@10 : 0.2199    hit@10 : 0.7275    precision@10 : 0.1512    \n",
      "14 Jan 06:50    INFO  epoch 38 training [time: 0.25s, train loss: 6.4700]\n",
      "14 Jan 06:50    INFO  epoch 38 evaluating [time: 0.26s, valid_score: 0.373400]\n",
      "14 Jan 06:50    INFO  valid result: \n",
      "recall@10 : 0.2    mrr@10 : 0.3734    ndcg@10 : 0.2198    hit@10 : 0.7306    precision@10 : 0.1504    \n",
      "14 Jan 06:50    INFO  Saving current best: saved/BPR-Jan-14-2022_06-50-28.pth\n",
      "14 Jan 06:50    INFO  epoch 39 training [time: 0.25s, train loss: 6.3704]\n",
      "14 Jan 06:50    INFO  epoch 39 evaluating [time: 0.27s, valid_score: 0.371200]\n",
      "14 Jan 06:50    INFO  valid result: \n",
      "recall@10 : 0.2005    mrr@10 : 0.3712    ndcg@10 : 0.2195    hit@10 : 0.7275    precision@10 : 0.1504    \n",
      "14 Jan 06:50    INFO  epoch 40 training [time: 0.23s, train loss: 6.2624]\n",
      "14 Jan 06:50    INFO  epoch 40 evaluating [time: 0.26s, valid_score: 0.372500]\n",
      "14 Jan 06:50    INFO  valid result: \n",
      "recall@10 : 0.2018    mrr@10 : 0.3725    ndcg@10 : 0.2195    hit@10 : 0.7338    precision@10 : 0.1501    \n",
      "14 Jan 06:50    INFO  epoch 41 training [time: 0.26s, train loss: 6.2573]\n",
      "14 Jan 06:50    INFO  epoch 41 evaluating [time: 0.27s, valid_score: 0.372800]\n",
      "14 Jan 06:50    INFO  valid result: \n",
      "recall@10 : 0.2042    mrr@10 : 0.3728    ndcg@10 : 0.2205    hit@10 : 0.7296    precision@10 : 0.1508    \n",
      "14 Jan 06:50    INFO  epoch 42 training [time: 0.25s, train loss: 6.1730]\n",
      "14 Jan 06:50    INFO  epoch 42 evaluating [time: 0.26s, valid_score: 0.372100]\n",
      "14 Jan 06:50    INFO  valid result: \n",
      "recall@10 : 0.2036    mrr@10 : 0.3721    ndcg@10 : 0.2198    hit@10 : 0.7328    precision@10 : 0.1512    \n",
      "14 Jan 06:50    INFO  epoch 43 training [time: 0.25s, train loss: 6.0315]\n",
      "14 Jan 06:50    INFO  epoch 43 evaluating [time: 0.27s, valid_score: 0.370900]\n",
      "14 Jan 06:50    INFO  valid result: \n",
      "recall@10 : 0.2053    mrr@10 : 0.3709    ndcg@10 : 0.2207    hit@10 : 0.7402    precision@10 : 0.1526    \n",
      "14 Jan 06:50    INFO  epoch 44 training [time: 0.23s, train loss: 5.9795]\n",
      "14 Jan 06:50    INFO  epoch 44 evaluating [time: 0.26s, valid_score: 0.374300]\n",
      "14 Jan 06:50    INFO  valid result: \n",
      "recall@10 : 0.2056    mrr@10 : 0.3743    ndcg@10 : 0.2222    hit@10 : 0.7381    precision@10 : 0.153    \n",
      "14 Jan 06:50    INFO  Saving current best: saved/BPR-Jan-14-2022_06-50-28.pth\n",
      "14 Jan 06:50    INFO  epoch 45 training [time: 0.25s, train loss: 5.8497]\n",
      "14 Jan 06:50    INFO  epoch 45 evaluating [time: 0.24s, valid_score: 0.378700]\n",
      "14 Jan 06:50    INFO  valid result: \n",
      "recall@10 : 0.2042    mrr@10 : 0.3787    ndcg@10 : 0.2217    hit@10 : 0.7349    precision@10 : 0.1514    \n",
      "14 Jan 06:50    INFO  Saving current best: saved/BPR-Jan-14-2022_06-50-28.pth\n",
      "14 Jan 06:50    INFO  epoch 46 training [time: 0.25s, train loss: 5.8550]\n",
      "14 Jan 06:50    INFO  epoch 46 evaluating [time: 0.27s, valid_score: 0.370300]\n",
      "14 Jan 06:50    INFO  valid result: \n",
      "recall@10 : 0.2067    mrr@10 : 0.3703    ndcg@10 : 0.2201    hit@10 : 0.7338    precision@10 : 0.1522    \n",
      "14 Jan 06:50    INFO  epoch 47 training [time: 0.27s, train loss: 5.7652]\n",
      "14 Jan 06:50    INFO  epoch 47 evaluating [time: 0.28s, valid_score: 0.371200]\n",
      "14 Jan 06:50    INFO  valid result: \n",
      "recall@10 : 0.2081    mrr@10 : 0.3712    ndcg@10 : 0.221    hit@10 : 0.7391    precision@10 : 0.1537    \n",
      "14 Jan 06:50    INFO  epoch 48 training [time: 0.26s, train loss: 5.6495]\n",
      "14 Jan 06:50    INFO  epoch 48 evaluating [time: 0.26s, valid_score: 0.376800]\n",
      "14 Jan 06:50    INFO  valid result: \n",
      "recall@10 : 0.2065    mrr@10 : 0.3768    ndcg@10 : 0.2219    hit@10 : 0.7391    precision@10 : 0.1529    \n",
      "14 Jan 06:50    INFO  epoch 49 training [time: 0.25s, train loss: 5.5199]\n",
      "14 Jan 06:50    INFO  epoch 49 evaluating [time: 0.27s, valid_score: 0.379900]\n",
      "14 Jan 06:50    INFO  valid result: \n",
      "recall@10 : 0.2065    mrr@10 : 0.3799    ndcg@10 : 0.223    hit@10 : 0.7391    precision@10 : 0.1537    \n",
      "14 Jan 06:50    INFO  Saving current best: saved/BPR-Jan-14-2022_06-50-28.pth\n",
      "14 Jan 06:50    INFO  epoch 50 training [time: 0.27s, train loss: 5.4894]\n",
      "14 Jan 06:50    INFO  epoch 50 evaluating [time: 0.27s, valid_score: 0.381500]\n",
      "14 Jan 06:50    INFO  valid result: \n",
      "recall@10 : 0.2101    mrr@10 : 0.3815    ndcg@10 : 0.2255    hit@10 : 0.7434    precision@10 : 0.1559    \n",
      "14 Jan 06:50    INFO  Saving current best: saved/BPR-Jan-14-2022_06-50-28.pth\n",
      "14 Jan 06:50    INFO  epoch 51 training [time: 0.26s, train loss: 5.4563]\n",
      "14 Jan 06:50    INFO  epoch 51 evaluating [time: 0.27s, valid_score: 0.377900]\n",
      "14 Jan 06:50    INFO  valid result: \n",
      "recall@10 : 0.2098    mrr@10 : 0.3779    ndcg@10 : 0.2251    hit@10 : 0.7423    precision@10 : 0.1557    \n",
      "14 Jan 06:50    INFO  epoch 52 training [time: 0.27s, train loss: 5.4235]\n",
      "14 Jan 06:50    INFO  epoch 52 evaluating [time: 0.25s, valid_score: 0.378700]\n",
      "14 Jan 06:50    INFO  valid result: \n",
      "recall@10 : 0.2123    mrr@10 : 0.3787    ndcg@10 : 0.2261    hit@10 : 0.754    precision@10 : 0.1573    \n",
      "14 Jan 06:50    INFO  epoch 53 training [time: 0.25s, train loss: 5.3876]\n",
      "14 Jan 06:50    INFO  epoch 53 evaluating [time: 0.26s, valid_score: 0.376800]\n",
      "14 Jan 06:50    INFO  valid result: \n",
      "recall@10 : 0.2126    mrr@10 : 0.3768    ndcg@10 : 0.2265    hit@10 : 0.7497    precision@10 : 0.1574    \n",
      "14 Jan 06:51    INFO  epoch 54 training [time: 0.26s, train loss: 5.2602]\n",
      "14 Jan 06:51    INFO  epoch 54 evaluating [time: 0.25s, valid_score: 0.376900]\n",
      "14 Jan 06:51    INFO  valid result: \n",
      "recall@10 : 0.2087    mrr@10 : 0.3769    ndcg@10 : 0.2244    hit@10 : 0.7497    precision@10 : 0.1556    \n",
      "14 Jan 06:51    INFO  epoch 55 training [time: 0.27s, train loss: 5.2106]\n",
      "14 Jan 06:51    INFO  epoch 55 evaluating [time: 0.25s, valid_score: 0.382800]\n",
      "14 Jan 06:51    INFO  valid result: \n",
      "recall@10 : 0.2072    mrr@10 : 0.3828    ndcg@10 : 0.2255    hit@10 : 0.7497    precision@10 : 0.1554    \n",
      "14 Jan 06:51    INFO  Saving current best: saved/BPR-Jan-14-2022_06-50-28.pth\n",
      "14 Jan 06:51    INFO  epoch 56 training [time: 0.27s, train loss: 5.2269]\n",
      "14 Jan 06:51    INFO  epoch 56 evaluating [time: 0.27s, valid_score: 0.383300]\n",
      "14 Jan 06:51    INFO  valid result: \n",
      "recall@10 : 0.2091    mrr@10 : 0.3833    ndcg@10 : 0.2261    hit@10 : 0.7508    precision@10 : 0.155    \n",
      "14 Jan 06:51    INFO  Saving current best: saved/BPR-Jan-14-2022_06-50-28.pth\n",
      "14 Jan 06:51    INFO  epoch 57 training [time: 0.25s, train loss: 5.1029]\n",
      "14 Jan 06:51    INFO  epoch 57 evaluating [time: 0.26s, valid_score: 0.386600]\n",
      "14 Jan 06:51    INFO  valid result: \n",
      "recall@10 : 0.2117    mrr@10 : 0.3866    ndcg@10 : 0.2282    hit@10 : 0.7508    precision@10 : 0.1571    \n",
      "14 Jan 06:51    INFO  Saving current best: saved/BPR-Jan-14-2022_06-50-28.pth\n",
      "14 Jan 06:51    INFO  epoch 58 training [time: 0.25s, train loss: 5.0363]\n",
      "14 Jan 06:51    INFO  epoch 58 evaluating [time: 0.25s, valid_score: 0.383000]\n",
      "14 Jan 06:51    INFO  valid result: \n",
      "recall@10 : 0.209    mrr@10 : 0.383    ndcg@10 : 0.2266    hit@10 : 0.7497    precision@10 : 0.1565    \n",
      "14 Jan 06:51    INFO  epoch 59 training [time: 0.24s, train loss: 4.9661]\n",
      "14 Jan 06:51    INFO  epoch 59 evaluating [time: 0.26s, valid_score: 0.388100]\n",
      "14 Jan 06:51    INFO  valid result: \n",
      "recall@10 : 0.2111    mrr@10 : 0.3881    ndcg@10 : 0.2284    hit@10 : 0.7434    precision@10 : 0.1571    \n",
      "14 Jan 06:51    INFO  Saving current best: saved/BPR-Jan-14-2022_06-50-28.pth\n",
      "14 Jan 06:51    INFO  epoch 60 training [time: 0.24s, train loss: 4.8730]\n",
      "14 Jan 06:51    INFO  epoch 60 evaluating [time: 0.27s, valid_score: 0.386800]\n",
      "14 Jan 06:51    INFO  valid result: \n",
      "recall@10 : 0.2119    mrr@10 : 0.3868    ndcg@10 : 0.2289    hit@10 : 0.7466    precision@10 : 0.1574    \n",
      "14 Jan 06:51    INFO  epoch 61 training [time: 0.25s, train loss: 4.8049]\n",
      "14 Jan 06:51    INFO  epoch 61 evaluating [time: 0.25s, valid_score: 0.385600]\n",
      "14 Jan 06:51    INFO  valid result: \n",
      "recall@10 : 0.2087    mrr@10 : 0.3856    ndcg@10 : 0.227    hit@10 : 0.7444    precision@10 : 0.1558    \n",
      "14 Jan 06:51    INFO  epoch 62 training [time: 0.24s, train loss: 4.7418]\n",
      "14 Jan 06:51    INFO  epoch 62 evaluating [time: 0.26s, valid_score: 0.388100]\n",
      "14 Jan 06:51    INFO  valid result: \n",
      "recall@10 : 0.2096    mrr@10 : 0.3881    ndcg@10 : 0.2275    hit@10 : 0.7455    precision@10 : 0.1561    \n",
      "14 Jan 06:51    INFO  epoch 63 training [time: 0.26s, train loss: 4.7274]\n",
      "14 Jan 06:51    INFO  epoch 63 evaluating [time: 0.25s, valid_score: 0.387400]\n",
      "14 Jan 06:51    INFO  valid result: \n",
      "recall@10 : 0.2101    mrr@10 : 0.3874    ndcg@10 : 0.2273    hit@10 : 0.7466    precision@10 : 0.1562    \n",
      "14 Jan 06:51    INFO  epoch 64 training [time: 0.27s, train loss: 4.6720]\n",
      "14 Jan 06:51    INFO  epoch 64 evaluating [time: 0.27s, valid_score: 0.385700]\n",
      "14 Jan 06:51    INFO  valid result: \n",
      "recall@10 : 0.2127    mrr@10 : 0.3857    ndcg@10 : 0.2284    hit@10 : 0.7497    precision@10 : 0.1564    \n",
      "14 Jan 06:51    INFO  epoch 65 training [time: 0.26s, train loss: 4.7336]\n",
      "14 Jan 06:51    INFO  epoch 65 evaluating [time: 0.28s, valid_score: 0.387700]\n",
      "14 Jan 06:51    INFO  valid result: \n",
      "recall@10 : 0.2137    mrr@10 : 0.3877    ndcg@10 : 0.2297    hit@10 : 0.7582    precision@10 : 0.1579    \n",
      "14 Jan 06:51    INFO  epoch 66 training [time: 0.26s, train loss: 4.5039]\n",
      "14 Jan 06:51    INFO  epoch 66 evaluating [time: 0.27s, valid_score: 0.378100]\n",
      "14 Jan 06:51    INFO  valid result: \n",
      "recall@10 : 0.2109    mrr@10 : 0.3781    ndcg@10 : 0.2264    hit@10 : 0.7497    precision@10 : 0.1571    \n",
      "14 Jan 06:51    INFO  epoch 67 training [time: 0.25s, train loss: 4.4633]\n",
      "14 Jan 06:51    INFO  epoch 67 evaluating [time: 0.27s, valid_score: 0.380500]\n",
      "14 Jan 06:51    INFO  valid result: \n",
      "recall@10 : 0.2104    mrr@10 : 0.3805    ndcg@10 : 0.2271    hit@10 : 0.7466    precision@10 : 0.1569    \n",
      "14 Jan 06:51    INFO  epoch 68 training [time: 0.26s, train loss: 4.5054]\n",
      "14 Jan 06:51    INFO  epoch 68 evaluating [time: 0.27s, valid_score: 0.382300]\n",
      "14 Jan 06:51    INFO  valid result: \n",
      "recall@10 : 0.2125    mrr@10 : 0.3823    ndcg@10 : 0.2279    hit@10 : 0.7519    precision@10 : 0.1578    \n",
      "14 Jan 06:51    INFO  epoch 69 training [time: 0.27s, train loss: 4.3791]\n",
      "14 Jan 06:51    INFO  epoch 69 evaluating [time: 0.26s, valid_score: 0.381500]\n",
      "14 Jan 06:51    INFO  valid result: \n",
      "recall@10 : 0.212    mrr@10 : 0.3815    ndcg@10 : 0.2273    hit@10 : 0.7508    precision@10 : 0.1579    \n",
      "14 Jan 06:51    INFO  epoch 70 training [time: 0.25s, train loss: 4.3704]\n",
      "14 Jan 06:51    INFO  epoch 70 evaluating [time: 0.26s, valid_score: 0.381400]\n",
      "14 Jan 06:51    INFO  valid result: \n",
      "recall@10 : 0.2145    mrr@10 : 0.3814    ndcg@10 : 0.2295    hit@10 : 0.7508    precision@10 : 0.1602    \n",
      "14 Jan 06:51    INFO  Finished training, best eval result in epoch 59\n",
      "14 Jan 06:51    INFO  Loading model structure and parameters from saved/BPR-Jan-14-2022_06-50-28.pth\n",
      "14 Jan 06:51    INFO  best valid for sparsified training set: {'recall@10': 0.2111, 'mrr@10': 0.3881, 'ndcg@10': 0.2284, 'hit@10': 0.7434, 'precision@10': 0.1571}\n",
      "14 Jan 06:51    INFO  test result for sparsified training set: {'recall@10': 0.2362, 'mrr@10': 0.4637, 'ndcg@10': 0.282, 'hit@10': 0.7529, 'precision@10': 0.1931}\n"
     ]
    }
   ],
   "source": [
    "all_results = {}\n",
    "for model in [\"BPR\"]:\n",
    "    dataset = \"ml-100k\"\n",
    "    base_config_dict = {\n",
    "        'data_path': DATASETS_DIR,\n",
    "        'show_progress': False,\n",
    "        'save_dataset': True,\n",
    "        'load_col': {'inter': ['user_id', 'item_id', 'rating', 'timestamp'],\n",
    "                        'user': ['user_id', 'age', 'gender', 'occupation'],\n",
    "                        'item': ['item_id', 'release_year', 'class']}\n",
    "    }\n",
    "    robustness_dict = {\n",
    "                    'distribution_shift': {'gender': {\"M\": .9, \"F\": .1}},\n",
    "                    'slice': {'by_feature': {'age': {'min': 40}}},\n",
    "                    'sparsify': {'fraction_removed': 0.05},\n",
    "                    # 'transform_features': {'fraction_removed': {'occupation': 0.2}},\n",
    "                    'transform_interactions': {'fraction_transformed': 0.2},\n",
    "                    }\n",
    "    results = train_and_test(model=model, dataset=dataset, robustness_tests=robustness_dict,\n",
    "                                base_config_dict=base_config_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'distributional_test_result': {'hit@10': 0.6877,\n",
       "  'mrr@10': 0.4116,\n",
       "  'ndcg@10': 0.2593,\n",
       "  'precision@10': 0.174,\n",
       "  'recall@10': 0.2333},\n",
       " 'slice_test_result': {'hit@10': 0.727,\n",
       "  'mrr@10': 0.4078,\n",
       "  'ndcg@10': 0.2644,\n",
       "  'precision@10': 0.1737,\n",
       "  'recall@10': 0.2497},\n",
       " 'sparsity_test_result': {'hit@10': 0.7529,\n",
       "  'mrr@10': 0.4637,\n",
       "  'ndcg@10': 0.282,\n",
       "  'precision@10': 0.1931,\n",
       "  'recall@10': 0.2362},\n",
       " 'test_result': {'hit@10': 0.755,\n",
       "  'mrr@10': 0.46,\n",
       "  'ndcg@10': 0.2811,\n",
       "  'precision@10': 0.1933,\n",
       "  'recall@10': 0.2357},\n",
       " 'transformation_test_result': {'hit@10': 0.7646,\n",
       "  'mrr@10': 0.4621,\n",
       "  'ndcg@10': 0.2825,\n",
       "  'precision@10': 0.194,\n",
       "  'recall@10': 0.2404}}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **References**\n",
    "1. Zohreh Ovaisi, Shelby Heinecke, Jia Li, Yongfeng Zhang, Elena Zheleva, Caiming Xiong, RGRecSys: A Toolkit for Robustness Evaluation of Recommender Systems. WSDM, 2022 - https://arxiv.org/abs/2201.04399.\n",
    "2. Source code - https://github.com/salesforce/RGRecSys."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
