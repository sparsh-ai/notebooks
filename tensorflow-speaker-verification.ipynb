{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Speaker verification with GE2E loss (Text independent version)\n",
    "\n",
    "Mainly based on https://github.com/Janghyun1230/Speaker_Verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/device:GPU:0'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import librosa\n",
    "import time\n",
    "import random\n",
    "from tensorflow.contrib import rnn\n",
    "\n",
    "audio_path = \"../input/vctk-corpus/VCTK-Corpus/wav48\"\n",
    "\n",
    "class hyperparam:\n",
    "    train_path = \"train\"\n",
    "    test_path = \"test\"\n",
    "    model_path = \"model\"\n",
    "    \n",
    "    # spectrogram params\n",
    "    tisv_frame = 180 # max frame number of utterances of tdsv\n",
    "    hop = 0.01 # hop size (s)\n",
    "    window = 0.025 # window length (s)\n",
    "    sr = 8000 # sampling rate\n",
    "    \n",
    "    # model params\n",
    "    num_layer = 3 # number of lstm layers\n",
    "    hidden = 128 # hidden state dimension of lstm\n",
    "    proj = 64 # projection dimension of lstm\n",
    "    \n",
    "    # training params\n",
    "    N = 4 # number of speakers of batch \n",
    "    M = 5 # number of utterances per speaker\n",
    "    train = True\n",
    "    optim = 'sgd' # optimizer type\n",
    "    loss = 'softmax' # loss type (softmax or contrast)\n",
    "\n",
    "    iteration = 10000\n",
    "    lr = 1e-2\n",
    "    \n",
    "    # test params\n",
    "    tdsv = False # text dependent or not\n",
    "    model_num = 1 # number of ckpt file to load\n",
    "    \n",
    "config = hyperparam()\n",
    "\n",
    "tf.test.gpu_device_name()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data preparation\n",
    "\n",
    "We use pack [librosa](http://librosa.github.io/librosa/) to preprocess the audio.\n",
    "\n",
    "The functions we use:\n",
    "* [librosa.effects.trim](https://librosa.github.io/librosa/generated/librosa.effects.trim.html?highlight=librosa%20effects%20trim#librosa.effects.trim)\n",
    "\n",
    "    Trim leading and trailing silence from an audio signal.\n",
    "\n",
    "* [librosa.core.load](https://librosa.github.io/librosa/generated/librosa.core.load.html?highlight=librosa%20core%20load#librosa.core.load)\n",
    "    \n",
    "    Load an audio file as a floating point time series.\n",
    "    \n",
    "    Audio will be automatically resampled to the given rate (default sr=22050).\n",
    "\n",
    "    To preserve the native sampling rate of the file, use sr=None.\n",
    "\n",
    "* [librosa.feature.mfcc](https://librosa.github.io/librosa/generated/librosa.feature.mfcc.html#librosa-feature-mfcc)\n",
    "\n",
    "    Mel-frequency cepstral coefficients. Return a MFCC sequence with shape [n_mfcc, t].\n",
    "    \n",
    "For simplicity, we just use the first 30 files with its first 100 items in the datasets. You can adjust parameter `cutted` and `cut_utter` to change this setting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "def save_spectrogram_tisv(audio_path, cutting = True, cutted = 30, cut_utter = 100):\n",
    "    \"\"\" Full preprocess of text independent utterance. The log-mel-spectrogram is saved as numpy file.\n",
    "        Each partial utterance is splitted by voice detection using DB\n",
    "        and the first and the last 180 frames from each partial utterance are saved. \n",
    "        Need : utterance data set (VTCK)\n",
    "    \"\"\"\n",
    "    print(\"start text independent utterance feature extraction\")\n",
    "    os.makedirs(config.train_path, exist_ok=True)   # make folder to save train file\n",
    "    os.makedirs(config.test_path, exist_ok=True)    # make folder to save test file\n",
    "\n",
    "    total_speaker_num = len(os.listdir(audio_path))\n",
    "    \n",
    "    if cutting:\n",
    "        train_speaker_num= (cutted//10)*8 # for cutting set\n",
    "    else:\n",
    "        train_speaker_num= (total_speaker_num//10)*9  # split total data 90% train and 10% test\n",
    "    \n",
    "    \n",
    "    print(\"total speaker number : %d\"%total_speaker_num)\n",
    "    if cutting:\n",
    "        print(\"train : %d, test : %d\"%(train_speaker_num, cutted+1-train_speaker_num))\n",
    "    else:\n",
    "        print(\"train : %d, test : %d\"%(train_speaker_num, total_speaker_num-train_speaker_num))\n",
    "    print(os.listdir(audio_path))\n",
    "    \n",
    "    for i, folder in enumerate(os.listdir(audio_path)):\n",
    "        speaker_path = os.path.join(audio_path, folder)     # path of each speaker\n",
    "        print(\"%d th speaker processing...\" % i)\n",
    "        utterances_spec = []\n",
    "        k=0\n",
    "        for j, utter_name in enumerate(os.listdir(speaker_path)):\n",
    "            utter_path = os.path.join(speaker_path, utter_name)         # path of each utterance\n",
    "            utter, sr = librosa.core.load(utter_path, config.sr)        # load utterance audio\n",
    "            # trim the silence in the audio. the interval of utter_trim corresponding to the non-silent region\n",
    "            utter_trim, index = librosa.effects.trim(utter, top_db=20)\n",
    "            \n",
    "            cur_slide = 0\n",
    "            mfcc_win_sample = int(config.sr*config.hop*config.tisv_frame) # the length of a mfcc window\n",
    "            while(True):\n",
    "                if(cur_slide + mfcc_win_sample > utter_trim.shape[0]):\n",
    "                    break\n",
    "                slide_win = utter_trim[cur_slide : cur_slide+mfcc_win_sample]\n",
    "\n",
    "                S = librosa.feature.mfcc(y=slide_win, sr=config.sr, n_mfcc=40)\n",
    "                utterances_spec.append(S)\n",
    "                cur_slide += int(mfcc_win_sample/2)\n",
    "            if cutting:\n",
    "                if j > cut_utter:\n",
    "                    break\n",
    "                \n",
    "        utterances_spec = np.array(utterances_spec)\n",
    "        print('utterances_spec.shape = {}'.format(utterances_spec.shape))\n",
    "\n",
    "        if i<train_speaker_num:      # save spectrogram as numpy file\n",
    "            np.save(os.path.join(config.train_path, \"speaker%d.npy\"%i), utterances_spec)\n",
    "        else:\n",
    "            np.save(os.path.join(config.test_path, \"speaker%d.npy\"%(i-train_speaker_num)), utterances_spec)\n",
    "        \n",
    "        if cutting:\n",
    "            if i > cutted-1:\n",
    "                break\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Organize batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_batch(shuffle=True, noise_filenum=None, utter_start=0):\n",
    "    \"\"\" Generate 1 batch.\n",
    "        For TD-SV, noise is added to each utterance.\n",
    "        For TI-SV, random frame length is applied to each batch of utterances (140-180 frames)\n",
    "        speaker_num : number of speaker of each batch\n",
    "        utter_num : number of utterance per speaker of each batch\n",
    "        shuffle : random sampling or not\n",
    "        noise_filenum : specify noise file or not (TD-SV)\n",
    "        utter_start : start point of slicing (TI-SV)\n",
    "    :return: 1 random numpy batch (frames x batch(NM) x n_mels)\n",
    "    \"\"\"\n",
    "    speaker_num = config.N\n",
    "    utter_num = config.M \n",
    "    # data path\n",
    "    if config.train:\n",
    "        path = config.train_path\n",
    "    else:\n",
    "        path = config.test_path\n",
    "    \n",
    "    # TI-SV 本实验使用： 与文本无关说话人鉴别\n",
    "    np_file_list = os.listdir(path)\n",
    "    total_speaker = len(np_file_list)\n",
    "\n",
    "    if shuffle:\n",
    "        selected_files = random.sample(np_file_list, speaker_num)  # select random N speakers\n",
    "    else:\n",
    "        selected_files = np_file_list[:speaker_num]                # select first N speakers\n",
    "            \n",
    "    utter_batch = []\n",
    "    for file in selected_files:\n",
    "        utters  =  np.load(os.path.join(path, file))        # load utterance spectrogram of selected speaker\n",
    "        if shuffle:\n",
    "            utter_index = np.random.randint(0, utters.shape[0], utter_num)   # select M utterances per speaker\n",
    "            utter_batch.append(utters[utter_index])       # each speaker's utterance [M, n_mels, frames] is appended\n",
    "        else:\n",
    "            utter_batch.append(utters[utter_start: utter_start+utter_num])\n",
    "\n",
    "    utter_batch = np.concatenate(utter_batch, axis=0)     # utterance batch [batch(NM), n_mels, frames]\n",
    "\n",
    "    if config.train:\n",
    "        frame_slice = np.random.randint(140,181)          # for train session, random slicing of input batch\n",
    "        utter_batch = utter_batch[:,:,:frame_slice]\n",
    "    else:\n",
    "        utter_batch = utter_batch[:,:,:160]               # for train session, fixed length slicing of input batch\n",
    "\n",
    "    utter_batch = np.transpose(utter_batch, axes=(2,0,1))     # transpose [frames, batch, n_mels]\n",
    "\n",
    "    return utter_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Define model and training session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(path):\n",
    "    tf.reset_default_graph()    # reset graph\n",
    "    # draw graph\n",
    "    batch = tf.placeholder(shape= [None, config.N*config.M, 40], dtype=tf.float32)  # input batch (time x batch x n_mel)\n",
    "    lr = tf.placeholder(dtype= tf.float32)  # learning rate\n",
    "    global_step = tf.Variable(0, name='global_step', trainable=False)\n",
    "    w = tf.get_variable(\"w\", initializer= np.array([10], dtype=np.float32))\n",
    "    b = tf.get_variable(\"b\", initializer= np.array([-5], dtype=np.float32))\n",
    "\n",
    "    # embedding lstm (3-layer default)\n",
    "    with tf.variable_scope(\"lstm\"):\n",
    "        lstm_cells = [tf.contrib.rnn.LSTMCell(num_units=config.hidden, num_proj=config.proj) for i in range(config.num_layer)]\n",
    "        lstm = tf.contrib.rnn.MultiRNNCell(lstm_cells)    # define lstm op and variables\n",
    "        outputs, _ = tf.nn.dynamic_rnn(cell=lstm, inputs=batch, dtype=tf.float32, time_major=True)   # for TI-VS must use dynamic rnn\n",
    "        embedded = outputs[-1]                            # the last ouput is the embedded d-vector\n",
    "        embedded = normalize(embedded)                    # normalize\n",
    "    print(\"embedded size: \", embedded.shape)\n",
    "    # loss\n",
    "    sim_matrix = similarity(embedded, w, b)\n",
    "    print(\"similarity matrix size: \", sim_matrix.shape)\n",
    "    loss = loss_cal(sim_matrix, type=config.loss)\n",
    "\n",
    "    # optimizer operation\n",
    "    trainable_vars= tf.trainable_variables()                # get variable list\n",
    "    optimizer= optim(lr)                                    # get optimizer (type is determined by configuration)\n",
    "    grads, vars= zip(*optimizer.compute_gradients(loss))    # compute gradients of variables with respect to loss\n",
    "    grads_clip, _ = tf.clip_by_global_norm(grads, 3.0)      # l2 norm clipping by 3\n",
    "    grads_rescale= [0.01*grad for grad in grads_clip[:2]] + grads_clip[2:]   # smaller gradient scale for w, b\n",
    "    train_op= optimizer.apply_gradients(zip(grads_rescale, vars), global_step= global_step)   # gradient update operation\n",
    "\n",
    "    # check variables memory\n",
    "    variable_count = np.sum(np.array([np.prod(np.array(v.get_shape().as_list())) for v in trainable_vars]))\n",
    "    print(\"total variables :\", variable_count)\n",
    "\n",
    "    # record loss\n",
    "    loss_summary = tf.summary.scalar(\"loss\", loss)\n",
    "    merged = tf.summary.merge_all()\n",
    "    saver = tf.train.Saver()\n",
    "\n",
    "    # training session\n",
    "    with tf.Session() as sess:\n",
    "        tf.global_variables_initializer().run()\n",
    "\n",
    "#        if(os.path.exists(path)):\n",
    "#            print(\"Restore from {}\".format(os.path.join(path, \"Check_Point/model.ckpt-2\")))\n",
    "#            saver.restore(sess, os.path.join(path, \"Check_Point/model.ckpt-2\"))  # restore variables from selected ckpt file\n",
    "#        else:\n",
    "#            os.makedirs(os.path.join(path, \"Check_Point\"), exist_ok=True)  # make folder to save model\n",
    "#            os.makedirs(os.path.join(path, \"logs\"), exist_ok=True)          # make folder to save log\n",
    "\n",
    "        os.makedirs(os.path.join(path, \"Check_Point\"), exist_ok=True)  # make folder to save model\n",
    "        os.makedirs(os.path.join(path, \"logs\"), exist_ok=True)          # make folder to save log\n",
    "        \n",
    "        writer = tf.summary.FileWriter(os.path.join(path, \"logs\"), sess.graph)\n",
    "        epoch = 0\n",
    "        lr_factor = 1   # lr decay factor ( 1/2 per 10000 iteration)\n",
    "        loss_acc = 0    # accumulated loss ( for running average of loss)\n",
    "\n",
    "        for iter in range(config.iteration):\n",
    "            # run forward and backward propagation and update parameters\n",
    "            _, loss_cur, summary = sess.run([train_op, loss, merged],\n",
    "                                  feed_dict={batch: random_batch(), lr: config.lr*lr_factor})\n",
    "            loss_acc += loss_cur    # accumulated loss for each 100 iteration\n",
    "            if iter % 10 == 0:\n",
    "                writer.add_summary(summary, iter)   # write at tensorboard\n",
    "            if (iter+1) % 100 == 0:\n",
    "                print(\"(iter : %d) loss: %.4f\" % ((iter+1),loss_acc/100))\n",
    "                loss_acc = 0                        # reset accumulated loss\n",
    "            if (iter+1) % 2000 == 0:\n",
    "                lr_factor /= 2                      # lr decay\n",
    "                print(\"learning rate is decayed! current lr : \", config.lr*lr_factor)\n",
    "            if (iter+1) % 5000 == 0:\n",
    "                saver.save(sess, os.path.join(path, \"./Check_Point/model.ckpt\"), global_step=iter//5000)\n",
    "                print(\"model is saved!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Define the loss\n",
    "and other utilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def similarity(embedded, w, b, N=4, M=5, P=64, center=None):\n",
    "    \"\"\" Calculate similarity matrix from embedded utterance batch (NM x embed_dim) eq. (9)\n",
    "        Input center to test enrollment. (embedded for verification)\n",
    "    :return: tf similarity matrix (NM x N)\n",
    "    \"\"\"\n",
    "    embedded_split = tf.reshape(embedded, shape=[N, M, P])\n",
    "\n",
    "    if center is None:\n",
    "        center = normalize(tf.reduce_mean(embedded_split, axis=1))              # [N,P] normalized center vectors eq.(1)\n",
    "        center_except = normalize(tf.reshape(tf.reduce_sum(embedded_split, axis=1, keep_dims=True)\n",
    "                                             - embedded_split, shape=[N*M,P]))  # [NM,P] center vectors eq.(8)\n",
    "        # make similarity matrix eq.(9)\n",
    "        S = tf.concat(\n",
    "            [tf.concat([tf.reduce_sum(center_except[i*M:(i+1)*M,:]*embedded_split[j,:,:], axis=1, keep_dims=True) if i==j\n",
    "                        else tf.reduce_sum(center[i:(i+1),:]*embedded_split[j,:,:], axis=1, keep_dims=True) for i in range(N)],\n",
    "                       axis=1) for j in range(N)], axis=0)\n",
    "    else :\n",
    "        # If center(enrollment) exist, use it.\n",
    "        S = tf.concat(\n",
    "            [tf.concat([tf.reduce_sum(center[i:(i + 1), :] * embedded_split[j, :, :], axis=1, keep_dims=True) for i\n",
    "                        in range(N)],\n",
    "                       axis=1) for j in range(N)], axis=0)\n",
    "\n",
    "    S = tf.abs(w)*S+b   # rescaling\n",
    "\n",
    "    return S\n",
    "\n",
    "\n",
    "def normalize(x):\n",
    "    \"\"\" normalize the last dimension vector of the input matrix\n",
    "    :return: normalized input\n",
    "    \"\"\"\n",
    "    return x/tf.sqrt(tf.reduce_sum(x**2, axis=-1, keep_dims=True)+1e-6)\n",
    "\n",
    "\n",
    "def cossim(x,y, normalized=True):\n",
    "    \"\"\" calculate similarity between tensors\n",
    "    :return: cos similarity tf op node\n",
    "    \"\"\"\n",
    "    if normalized:\n",
    "        return tf.reduce_sum(x*y)\n",
    "    else:\n",
    "        x_norm = tf.sqrt(tf.reduce_sum(x**2)+1e-6)\n",
    "        y_norm = tf.sqrt(tf.reduce_sum(y**2)+1e-6)\n",
    "        return tf.reduce_sum(x*y)/x_norm/y_norm\n",
    "    \n",
    "def loss_cal(S, type=\"softmax\", N=config.N, M=config.M):\n",
    "    \"\"\" calculate loss with similarity matrix(S) eq.(6) (7) \n",
    "    :type: \"softmax\" or \"contrast\"\n",
    "    :return: loss\n",
    "    \"\"\"\n",
    "    S_correct = tf.concat([S[i*M:(i+1)*M, i:(i+1)] for i in range(N)], axis=0)  # colored entries in Fig.1\n",
    "\n",
    "    if type == \"softmax\":\n",
    "        total = -tf.reduce_sum(S_correct-tf.log(tf.reduce_sum(tf.exp(S), axis=1, keep_dims=True) + 1e-6))\n",
    "    elif type == \"contrast\":\n",
    "        S_sig = tf.sigmoid(S)\n",
    "        S_sig = tf.concat([tf.concat([0*S_sig[i*M:(i+1)*M, j:(j+1)] if i==j\n",
    "                              else S_sig[i*M:(i+1)*M, j:(j+1)] for j in range(N)], axis=1)\n",
    "                             for i in range(N)], axis=0)\n",
    "        total = tf.reduce_sum(1-tf.sigmoid(S_correct)+tf.reduce_max(S_sig, axis=1, keep_dims=True))\n",
    "    else:\n",
    "        raise AssertionError(\"loss type should be softmax or contrast !\")\n",
    "\n",
    "    return total\n",
    "\n",
    "def optim(lr):\n",
    "    \"\"\" return optimizer determined by configuration\n",
    "    :return: tf optimizer\n",
    "    \"\"\"\n",
    "    if config.optim == \"sgd\":\n",
    "        return tf.train.GradientDescentOptimizer(lr)\n",
    "    elif config.optim == \"rmsprop\":\n",
    "        return tf.train.RMSPropOptimizer(lr)\n",
    "    elif config.optim == \"adam\":\n",
    "        return tf.train.AdamOptimizer(lr, beta1=config.beta1, beta2=config.beta2)\n",
    "    else:\n",
    "        raise AssertionError(\"Wrong optimizer type!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Test session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Session\n",
    "def test(path):\n",
    "    tf.reset_default_graph()\n",
    "\n",
    "    # draw graph\n",
    "    enroll = tf.placeholder(shape=[None, config.N*config.M, 40], dtype=tf.float32) # enrollment batch (time x batch x n_mel)\n",
    "    verif = tf.placeholder(shape=[None, config.N*config.M, 40], dtype=tf.float32)  # verification batch (time x batch x n_mel)\n",
    "    batch = tf.concat([enroll, verif], axis=1)\n",
    "\n",
    "    # embedding lstm (3-layer default)\n",
    "    with tf.variable_scope(\"lstm\"):\n",
    "        lstm_cells = [tf.contrib.rnn.LSTMCell(num_units=config.hidden, num_proj=config.proj) for i in range(config.num_layer)]\n",
    "        lstm = tf.contrib.rnn.MultiRNNCell(lstm_cells)    # make lstm op and variables\n",
    "        outputs, _ = tf.nn.dynamic_rnn(cell=lstm, inputs=batch, dtype=tf.float32, time_major=True)   # for TI-VS must use dynamic rnn\n",
    "        embedded = outputs[-1]                            # the last ouput is the embedded d-vector\n",
    "        embedded = normalize(embedded)                    # normalize\n",
    "\n",
    "    print(\"embedded size: \", embedded.shape)\n",
    "\n",
    "    # enrollment embedded vectors (speaker model)\n",
    "    enroll_embed = normalize(tf.reduce_mean(tf.reshape(embedded[:config.N*config.M, :], shape= [config.N, config.M, -1]), axis=1))\n",
    "    # verification embedded vectors\n",
    "    verif_embed = embedded[config.N*config.M:, :]\n",
    "\n",
    "    similarity_matrix = similarity(embedded=verif_embed, w=1., b=0., center=enroll_embed)\n",
    "\n",
    "    saver = tf.train.Saver(var_list=tf.global_variables())\n",
    "    with tf.Session() as sess:\n",
    "        tf.global_variables_initializer().run()\n",
    "        \n",
    "        # load model\n",
    "        print(\"model path :\", path)\n",
    "        ckpt = tf.train.get_checkpoint_state(checkpoint_dir=os.path.join(path, \"Check_Point\"))\n",
    "        ckpt_list = ckpt.all_model_checkpoint_paths\n",
    "        loaded = 0\n",
    "        for model in ckpt_list:\n",
    "            if config.model_num == int(model[-1]):    # find ckpt file which matches configuration model number\n",
    "                print(\"ckpt file is loaded !\", model)\n",
    "                loaded = 1\n",
    "                saver.restore(sess, model)  # restore variables from selected ckpt file\n",
    "                break\n",
    "\n",
    "        if loaded == 0:\n",
    "            raise AssertionError(\"ckpt file does not exist! Check config.model_num or config.model_path.\")\n",
    "\n",
    "        print(\"test file path : \", config.test_path)\n",
    "        '''\n",
    "            test speaker:p225--p243\n",
    "        '''\n",
    "        # return similarity matrix after enrollment and verification\n",
    "        time1 = time.time() # for check inference time\n",
    "        if config.tdsv:\n",
    "            S = sess.run(similarity_matrix, feed_dict={enroll:random_batch(shuffle=False, noise_filenum=1),\n",
    "                                                       verif:random_batch(shuffle=False, noise_filenum=2)})\n",
    "        else:\n",
    "            S = sess.run(similarity_matrix, feed_dict={enroll:random_batch(shuffle=False),\n",
    "                                                       verif:random_batch(shuffle=False, utter_start=config.M)})\n",
    "        S = S.reshape([config.N, config.M, -1])\n",
    "        time2 = time.time()\n",
    "\n",
    "        np.set_printoptions(precision=2)\n",
    "        print(\"inference time for %d utterences : %0.2fs\"%(2*config.M*config.N, time2-time1))\n",
    "        print(S)    # print similarity matrix\n",
    "\n",
    "        # calculating EER\n",
    "        diff = 1; EER=0; EER_thres = 0; EER_FAR=0; EER_FRR=0\n",
    "\n",
    "        # through thresholds calculate false acceptance ratio (FAR) and false reject ratio (FRR)\n",
    "        for thres in [0.01*i+0.5 for i in range(50)]:\n",
    "            S_thres = S>thres\n",
    "\n",
    "            # False acceptance ratio = false acceptance / mismatched population (enroll speaker != verification speaker)\n",
    "            FAR = sum([np.sum(S_thres[i])-np.sum(S_thres[i,:,i]) for i in range(config.N)])/(config.N-1)/config.M/config.N\n",
    "\n",
    "            # False reject ratio = false reject / matched population (enroll speaker = verification speaker)\n",
    "            FRR = sum([config.M-np.sum(S_thres[i][:,i]) for i in range(config.N)])/config.M/config.N\n",
    "\n",
    "            # Save threshold when FAR = FRR (=EER)\n",
    "            if diff> abs(FAR-FRR):\n",
    "                diff = abs(FAR-FRR)\n",
    "                EER = (FAR+FRR)/2\n",
    "                EER_thres = thres\n",
    "                EER_FAR = FAR\n",
    "                EER_FRR = FRR\n",
    "\n",
    "        print(\"\\nEER : %0.2f (thres:%0.2f, FAR:%0.2f, FRR:%0.2f)\"%(EER,EER_thres,EER_FAR,EER_FRR))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start text independent utterance feature extraction\n",
      "total speaker number : 109\n",
      "train : 24, test : 7\n",
      "['p251', 'p272', 'p248', 'p237', 'p277', 'p311', 'p361', 'p262', 'p316', 'p287', 'p264', 'p310', 'p339', 'p307', 'p283', 'p268', 'p226', 'p336', 'p254', 'p236', 'p265', 'p256', 'p301', 'p269', 'p230', 'p335', 'p334', 'p258', 'p314', 'p333', 'p228', 'p280', 'p351', 'p326', 'p329', 'p300', 'p271', 'p288', 'p281', 'p302', 'p270', 'p260', 'p306', 'p249', 'p341', 'p305', 'p252', 'p304', 'p227', 'p313', 'p364', 'p340', 'p255', 'p294', 'p376', 'p279', 'p343', 'p234', 'p360', 'p293', 'p299', 'p315', 'p273', 'p261', 'p318', 'p225', 'p244', 'p239', 'p257', 'p278', 'p308', 'p317', 'p362', 'p345', 'p259', 'p284', 'p282', 'p374', 'p285', 'p292', 'p267', 'p233', 'p250', 'p298', 'p297', 'p240', 'p276', 'p263', 'p312', 'p253', 'p363', 'p247', 'p295', 'p275', 'p238', 'p286', 'p330', 'p266', 'p245', 'p303', 'p232', 'p243', 'p347', 'p231', 'p229', 'p241', 'p323', 'p274', 'p246']\n",
      "0 th speaker processing...\n",
      "utterances_spec.shape = (98, 40, 29)\n",
      "1 th speaker processing...\n",
      "utterances_spec.shape = (126, 40, 29)\n",
      "2 th speaker processing...\n",
      "utterances_spec.shape = (116, 40, 29)\n",
      "3 th speaker processing...\n",
      "utterances_spec.shape = (65, 40, 29)\n",
      "4 th speaker processing...\n",
      "utterances_spec.shape = (91, 40, 29)\n",
      "5 th speaker processing...\n",
      "utterances_spec.shape = (60, 40, 29)\n",
      "6 th speaker processing...\n",
      "utterances_spec.shape = (37, 40, 29)\n",
      "7 th speaker processing...\n",
      "utterances_spec.shape = (114, 40, 29)\n",
      "8 th speaker processing...\n",
      "utterances_spec.shape = (87, 40, 29)\n",
      "9 th speaker processing...\n",
      "utterances_spec.shape = (64, 40, 29)\n",
      "10 th speaker processing...\n",
      "utterances_spec.shape = (105, 40, 29)\n",
      "11 th speaker processing...\n",
      "utterances_spec.shape = (83, 40, 29)\n",
      "12 th speaker processing...\n",
      "utterances_spec.shape = (57, 40, 29)\n",
      "13 th speaker processing...\n",
      "utterances_spec.shape = (102, 40, 29)\n",
      "14 th speaker processing...\n",
      "utterances_spec.shape = (80, 40, 29)\n",
      "15 th speaker processing...\n",
      "utterances_spec.shape = (99, 40, 29)\n",
      "16 th speaker processing...\n",
      "utterances_spec.shape = (119, 40, 29)\n",
      "17 th speaker processing...\n",
      "utterances_spec.shape = (74, 40, 29)\n",
      "18 th speaker processing...\n",
      "utterances_spec.shape = (104, 40, 29)\n",
      "19 th speaker processing...\n",
      "utterances_spec.shape = (70, 40, 29)\n",
      "20 th speaker processing...\n",
      "utterances_spec.shape = (200, 40, 29)\n",
      "21 th speaker processing...\n",
      "utterances_spec.shape = (115, 40, 29)\n",
      "22 th speaker processing...\n",
      "utterances_spec.shape = (96, 40, 29)\n",
      "23 th speaker processing...\n",
      "utterances_spec.shape = (98, 40, 29)\n",
      "24 th speaker processing...\n",
      "utterances_spec.shape = (145, 40, 29)\n",
      "25 th speaker processing...\n",
      "utterances_spec.shape = (121, 40, 29)\n",
      "26 th speaker processing...\n",
      "utterances_spec.shape = (95, 40, 29)\n",
      "27 th speaker processing...\n",
      "utterances_spec.shape = (74, 40, 29)\n",
      "28 th speaker processing...\n",
      "utterances_spec.shape = (88, 40, 29)\n",
      "29 th speaker processing...\n",
      "utterances_spec.shape = (67, 40, 29)\n",
      "30 th speaker processing...\n",
      "utterances_spec.shape = (153, 40, 29)\n",
      "\n",
      "Training Session\n",
      "embedded size:  (20, 64)\n",
      "similarity matrix size:  (20, 4)\n",
      "total variables : 210434\n",
      "(iter : 100) loss: 9.6017\n",
      "(iter : 200) loss: 5.1765\n",
      "(iter : 300) loss: 3.7387\n",
      "(iter : 400) loss: 3.1812\n",
      "(iter : 500) loss: 3.3882\n",
      "(iter : 600) loss: 2.9728\n",
      "(iter : 700) loss: 2.2826\n",
      "(iter : 800) loss: 2.4424\n",
      "(iter : 900) loss: 2.4680\n",
      "(iter : 1000) loss: 1.7115\n",
      "(iter : 1100) loss: 1.8038\n",
      "(iter : 1200) loss: 1.9859\n",
      "(iter : 1300) loss: 1.9667\n",
      "(iter : 1400) loss: 2.1122\n",
      "(iter : 1500) loss: 1.6643\n",
      "(iter : 1600) loss: 1.2962\n",
      "(iter : 1700) loss: 1.4502\n",
      "(iter : 1800) loss: 1.2926\n",
      "(iter : 1900) loss: 1.0679\n",
      "(iter : 2000) loss: 1.3665\n",
      "learning rate is decayed! current lr :  0.005\n",
      "(iter : 2100) loss: 0.9250\n",
      "(iter : 2200) loss: 0.7609\n",
      "(iter : 2300) loss: 0.7718\n",
      "(iter : 2400) loss: 0.5820\n",
      "(iter : 2500) loss: 0.8600\n",
      "(iter : 2600) loss: 0.6450\n",
      "(iter : 2700) loss: 0.5754\n",
      "(iter : 2800) loss: 0.6623\n",
      "(iter : 2900) loss: 0.6806\n",
      "(iter : 3000) loss: 0.5741\n",
      "(iter : 3100) loss: 0.5977\n",
      "(iter : 3200) loss: 0.5964\n",
      "(iter : 3300) loss: 0.5315\n",
      "(iter : 3400) loss: 0.7147\n",
      "(iter : 3500) loss: 0.5198\n",
      "(iter : 3600) loss: 0.5180\n",
      "(iter : 3700) loss: 0.4523\n",
      "(iter : 3800) loss: 0.4682\n",
      "(iter : 3900) loss: 0.5457\n",
      "(iter : 4000) loss: 0.4067\n",
      "learning rate is decayed! current lr :  0.0025\n",
      "(iter : 4100) loss: 0.3697\n",
      "(iter : 4200) loss: 0.3359\n",
      "(iter : 4300) loss: 0.3105\n",
      "(iter : 4400) loss: 0.3568\n",
      "(iter : 4500) loss: 0.3374\n",
      "(iter : 4600) loss: 0.2879\n",
      "(iter : 4700) loss: 0.2614\n",
      "(iter : 4800) loss: 0.4282\n",
      "(iter : 4900) loss: 0.3101\n",
      "(iter : 5000) loss: 0.3878\n",
      "model is saved!\n",
      "(iter : 5100) loss: 0.3359\n",
      "(iter : 5200) loss: 0.2811\n",
      "(iter : 5300) loss: 0.2453\n",
      "(iter : 5400) loss: 0.3852\n",
      "(iter : 5500) loss: 0.2289\n",
      "(iter : 5600) loss: 0.1940\n",
      "(iter : 5700) loss: 0.2760\n",
      "(iter : 5800) loss: 0.2145\n",
      "(iter : 5900) loss: 0.2818\n",
      "(iter : 6000) loss: 0.3370\n",
      "learning rate is decayed! current lr :  0.00125\n",
      "(iter : 6100) loss: 0.2539\n",
      "(iter : 6200) loss: 0.1787\n",
      "(iter : 6300) loss: 0.2275\n",
      "(iter : 6400) loss: 0.2120\n",
      "(iter : 6500) loss: 0.1857\n",
      "(iter : 6600) loss: 0.1875\n",
      "(iter : 6700) loss: 0.2221\n",
      "(iter : 6800) loss: 0.1381\n",
      "(iter : 6900) loss: 0.1782\n",
      "(iter : 7000) loss: 0.1891\n",
      "(iter : 7100) loss: 0.1786\n",
      "(iter : 7200) loss: 0.2166\n",
      "(iter : 7300) loss: 0.1901\n",
      "(iter : 7400) loss: 0.2367\n",
      "(iter : 7500) loss: 0.2153\n",
      "(iter : 7600) loss: 0.2911\n",
      "(iter : 7700) loss: 0.2591\n",
      "(iter : 7800) loss: 0.1629\n",
      "(iter : 7900) loss: 0.2070\n",
      "(iter : 8000) loss: 0.2113\n",
      "learning rate is decayed! current lr :  0.000625\n",
      "(iter : 8100) loss: 0.2136\n",
      "(iter : 8200) loss: 0.1663\n",
      "(iter : 8300) loss: 0.2094\n",
      "(iter : 8400) loss: 0.2353\n",
      "(iter : 8500) loss: 0.1621\n",
      "(iter : 8600) loss: 0.2102\n",
      "(iter : 8700) loss: 0.1428\n",
      "(iter : 8800) loss: 0.1383\n",
      "(iter : 8900) loss: 0.1816\n",
      "(iter : 9000) loss: 0.1769\n",
      "(iter : 9100) loss: 0.1348\n",
      "(iter : 9200) loss: 0.2574\n",
      "(iter : 9300) loss: 0.1069\n",
      "(iter : 9400) loss: 0.1853\n",
      "(iter : 9500) loss: 0.1538\n",
      "(iter : 9600) loss: 0.1908\n",
      "(iter : 9700) loss: 0.1690\n",
      "(iter : 9800) loss: 0.1528\n",
      "(iter : 9900) loss: 0.1847\n",
      "(iter : 10000) loss: 0.2037\n",
      "learning rate is decayed! current lr :  0.0003125\n",
      "model is saved!\n",
      "\n",
      "Test session\n",
      "embedded size:  (40, 64)\n",
      "model path : model\n",
      "ckpt file is loaded ! model/Check_Point/model.ckpt-1\n",
      "test file path :  test\n",
      "inference time for 40 utterences : 0.12s\n",
      "[[[ 0.69 -0.24  0.3   0.23]\n",
      "  [ 0.71 -0.28  0.24  0.34]\n",
      "  [ 0.87 -0.4   0.32  0.33]\n",
      "  [ 0.82 -0.48  0.52  0.71]\n",
      "  [ 0.84 -0.43  0.52  0.65]]\n",
      "\n",
      " [[-0.43  0.87 -0.06 -0.45]\n",
      "  [-0.23  0.69  0.02 -0.29]\n",
      "  [-0.47  0.81 -0.3  -0.53]\n",
      "  [-0.4   0.86  0.01 -0.5 ]\n",
      "  [-0.26  0.64 -0.38 -0.56]]\n",
      "\n",
      " [[ 0.27 -0.01  0.58  0.38]\n",
      "  [ 0.53 -0.16  0.82  0.3 ]\n",
      "  [ 0.47 -0.22  0.79  0.26]\n",
      "  [ 0.74 -0.14  0.75  0.58]\n",
      "  [ 0.47 -0.15  0.73  0.77]]\n",
      "\n",
      " [[ 0.78 -0.59  0.69  0.87]\n",
      "  [ 0.57 -0.62  0.49  0.84]\n",
      "  [ 0.13 -0.36  0.19  0.77]\n",
      "  [ 0.36 -0.29  0.17  0.87]\n",
      "  [ 0.59 -0.45  0.55  0.87]]]\n",
      "\n",
      "EER : 0.10 (thres:0.65, FAR:0.10, FRR:0.10)\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    save_spectrogram_tisv(audio_path, cutting = True)\n",
    "    config.train = True\n",
    "    # start training\n",
    "    print(\"\\nTraining Session\")\n",
    "    with tf.device(\"/gpu:0\"):\n",
    "        train(config.model_path)\n",
    "            \n",
    "    # start test\n",
    "    config.train = False\n",
    "    print(\"\\nTest session\")\n",
    "    if os.path.isdir(config.model_path):\n",
    "        with tf.device(\"/gpu:0\"):\n",
    "            test(config.model_path)\n",
    "    else:\n",
    "        raise AssertionError(\"model path doesn't exist!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
