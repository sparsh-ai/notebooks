{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"T948705 | Sparsely-gated Mixture of Experts","provenance":[{"file_id":"1NIIRgT5RBZsiyOXCP50xE974JElemsxV","timestamp":1635268017176}],"collapsed_sections":[],"authorship_tag":"ABX9TyN9dIzIcUz1G4p5In5IL5PB"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"1a14a77a376b4892ad2ccbba543aeac1":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_8003e3eb521149ac86066798acf1aa6b","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_ee0d26e63b8649eabf9be40886db83c3","IPY_MODEL_eeddb363983440c99e56532b051c3d09","IPY_MODEL_63f0c0ab3cc748bf90b5a47c1319ca7f"]}},"8003e3eb521149ac86066798acf1aa6b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"ee0d26e63b8649eabf9be40886db83c3":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_6e5a671d75c64630853164d83366d6d3","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_19b74d9654c24a34926c63c5f59da3ae"}},"eeddb363983440c99e56532b051c3d09":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_017f5ab15f454ec3ba2cce157ab7ff7a","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":170498071,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":170498071,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_fea6f37c9fac47578d493ad661402d2d"}},"63f0c0ab3cc748bf90b5a47c1319ca7f":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_66720267afbc4d87946ff29e74a740f9","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 170499072/? [00:03&lt;00:00, 57698228.15it/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_3173d4397e9c445abdd7fa3b379163bb"}},"6e5a671d75c64630853164d83366d6d3":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"19b74d9654c24a34926c63c5f59da3ae":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"017f5ab15f454ec3ba2cce157ab7ff7a":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"fea6f37c9fac47578d493ad661402d2d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"66720267afbc4d87946ff29e74a740f9":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"3173d4397e9c445abdd7fa3b379163bb":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}}},"cells":[{"cell_type":"markdown","metadata":{"id":"oy2u5YZVej_A"},"source":["# Sparsely-gated Mixture-of-Experts"]},{"cell_type":"markdown","metadata":{"id":"V-RQWsJQc7GF"},"source":["<p><center><img src='https://s3.us-west-2.amazonaws.com/secure.notion-static.com/bc27926b-46a1-4699-b456-d439f3484256/Untitled.png?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAT73L2G45O3KS52Y5%2F20211027%2Fus-west-2%2Fs3%2Faws4_request&X-Amz-Date=20211027T062821Z&X-Amz-Expires=86400&X-Amz-Signature=01ec9e3caff091d3c6f2289e1c1963dbec4f638a7753a142d15461954dc9e237&X-Amz-SignedHeaders=host&response-content-disposition=filename%20%3D%22Untitled.png%22'></center></p>"]},{"cell_type":"markdown","metadata":{"id":"FftlGp1Nk7ZM"},"source":["## Setup"]},{"cell_type":"code","metadata":{"id":"0C74UBefepVZ"},"source":["import torch\n","from torch import nn\n","import torchvision\n","import torch.nn.functional as F\n","import torch.optim as optim\n","from torch.optim import Adam\n","import torchvision.transforms as transforms\n","from torch.distributions.normal import Normal\n","\n","import numpy as np"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"uBtgh1Nzk3_c"},"source":["## Models"]},{"cell_type":"code","metadata":{"id":"MZwEQz7te7cs","executionInfo":{"status":"ok","timestamp":1635316105967,"user_tz":-330,"elapsed":6,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}}},"source":["class MLP(nn.Module):\n","    def __init__(self, input_size, output_size, hidden_size):\n","        super(MLP, self).__init__()\n","        self.fc1 = nn.Linear(input_size, hidden_size)\n","        self.fc2 = nn.Linear(hidden_size, output_size)\n","        self.relu = nn.ReLU()\n","        self.log_soft = nn.LogSoftmax(1)\n","\n","    def forward(self, x):\n","        out = self.fc1(x)\n","        out = self.relu(out)\n","        out = self.fc2(out)\n","        out = self.log_soft(out)\n","        return out"],"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"id":"AlmLcwywezkH","executionInfo":{"status":"ok","timestamp":1635316106477,"user_tz":-330,"elapsed":515,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}}},"source":["class SparseDispatcher(object):\n","    \"\"\"Helper for implementing a mixture of experts.\n","    The purpose of this class is to create input minibatches for the\n","    experts and to combine the results of the experts to form a unified\n","    output tensor.\n","    There are two functions:\n","    dispatch - take an input Tensor and create input Tensors for each expert.\n","    combine - take output Tensors from each expert and form a combined output\n","      Tensor.  Outputs from different experts for the same batch element are\n","      summed together, weighted by the provided \"gates\".\n","    The class is initialized with a \"gates\" Tensor, which specifies which\n","    batch elements go to which experts, and the weights to use when combining\n","    the outputs.  Batch element b is sent to expert e iff gates[b, e] != 0.\n","    The inputs and outputs are all two-dimensional [batch, depth].\n","    Caller is responsible for collapsing additional dimensions prior to\n","    calling this class and reshaping the output to the original shape.\n","    See common_layers.reshape_like().\n","    Example use:\n","    gates: a float32 `Tensor` with shape `[batch_size, num_experts]`\n","    inputs: a float32 `Tensor` with shape `[batch_size, input_size]`\n","    experts: a list of length `num_experts` containing sub-networks.\n","    dispatcher = SparseDispatcher(num_experts, gates)\n","    expert_inputs = dispatcher.dispatch(inputs)\n","    expert_outputs = [experts[i](expert_inputs[i]) for i in range(num_experts)]\n","    outputs = dispatcher.combine(expert_outputs)\n","    The preceding code sets the output for a particular example b to:\n","    output[b] = Sum_i(gates[b, i] * experts[i](inputs[b]))\n","    This class takes advantage of sparsity in the gate matrix by including in the\n","    `Tensor`s for expert i only the batch elements for which `gates[b, i] > 0`.\n","    \"\"\"\n","\n","    def __init__(self, num_experts, gates):\n","        \"\"\"Create a SparseDispatcher.\"\"\"\n","\n","        self._gates = gates\n","        self._num_experts = num_experts\n","        # sort experts\n","        sorted_experts, index_sorted_experts = torch.nonzero(gates).sort(0)\n","        # drop indices\n","        _, self._expert_index = sorted_experts.split(1, dim=1)\n","        # get according batch index for each expert\n","        self._batch_index = sorted_experts[index_sorted_experts[:, 1],0]\n","        # calculate num samples that each expert gets\n","        self._part_sizes = list((gates > 0).sum(0).numpy())\n","        # expand gates to match with self._batch_index\n","        gates_exp = gates[self._batch_index.flatten()]\n","        self._nonzero_gates = torch.gather(gates_exp, 1, self._expert_index)\n","\n","    def dispatch(self, inp):\n","        \"\"\"Create one input Tensor for each expert.\n","        The `Tensor` for a expert `i` contains the slices of `inp` corresponding\n","        to the batch elements `b` where `gates[b, i] > 0`.\n","        Args:\n","          inp: a `Tensor` of shape \"[batch_size, <extra_input_dims>]`\n","        Returns:\n","          a list of `num_experts` `Tensor`s with shapes\n","            `[expert_batch_size_i, <extra_input_dims>]`.\n","        \"\"\"\n","\n","        # assigns samples to experts whose gate is nonzero\n","\n","        # expand according to batch index so we can just split by _part_sizes\n","        inp_exp = inp[self._batch_index].squeeze(1)\n","        return torch.split(inp_exp, self._part_sizes, dim=0)\n","\n","\n","    def combine(self, expert_out, multiply_by_gates=True):\n","        \"\"\"Sum together the expert output, weighted by the gates.\n","        The slice corresponding to a particular batch element `b` is computed\n","        as the sum over all experts `i` of the expert output, weighted by the\n","        corresponding gate values.  If `multiply_by_gates` is set to False, the\n","        gate values are ignored.\n","        Args:\n","          expert_out: a list of `num_experts` `Tensor`s, each with shape\n","            `[expert_batch_size_i, <extra_output_dims>]`.\n","          multiply_by_gates: a boolean\n","        Returns:\n","          a `Tensor` with shape `[batch_size, <extra_output_dims>]`.\n","        \"\"\"\n","        # apply exp to expert outputs, so we are not longer in log space\n","        stitched = torch.cat(expert_out, 0).exp()\n","\n","        if multiply_by_gates:\n","            stitched = stitched.mul(self._nonzero_gates)\n","        zeros = torch.zeros(self._gates.size(0), expert_out[-1].size(1), requires_grad=True)\n","        # combine samples that have been processed by the same k experts\n","        combined = zeros.index_add(0, self._batch_index, stitched.float())\n","        # add eps to all zero values in order to avoid nans when going back to log space\n","        combined[combined == 0] = np.finfo(float).eps\n","        # back to log space\n","        return combined.log()\n","\n","\n","    def expert_to_gates(self):\n","        \"\"\"Gate values corresponding to the examples in the per-expert `Tensor`s.\n","        Returns:\n","          a list of `num_experts` one-dimensional `Tensor`s with type `tf.float32`\n","              and shapes `[expert_batch_size_i]`\n","        \"\"\"\n","        # split nonzero gates for each expert\n","        return torch.split(self._nonzero_gates, self._part_sizes, dim=0)\n","\n","\n","\n","\n","class MoE(nn.Module):\n","\n","    \"\"\"Call a Sparsely gated mixture of experts layer with 1-layer Feed-Forward networks as experts.\n","    Args:\n","    input_size: integer - size of the input\n","    output_size: integer - size of the input\n","    num_experts: an integer - number of experts\n","    hidden_size: an integer - hidden size of the experts\n","    noisy_gating: a boolean\n","    k: an integer - how many experts to use for each batch element\n","    \"\"\"\n","\n","    def __init__(self, input_size, output_size, num_experts, hidden_size, noisy_gating=True, k=4):\n","        super(MoE, self).__init__()\n","        self.noisy_gating = noisy_gating\n","        self.num_experts = num_experts\n","        self.output_size = output_size\n","        self.input_size = input_size\n","        self.hidden_size = hidden_size\n","        self.k = k\n","        # instantiate experts\n","        self.experts = nn.ModuleList([MLP(self.input_size, self.output_size, self.hidden_size) for i in range(self.num_experts)])\n","        self.w_gate = nn.Parameter(torch.zeros(input_size, num_experts), requires_grad=True)\n","        self.w_noise = nn.Parameter(torch.zeros(input_size, num_experts), requires_grad=True)\n","\n","        self.softplus = nn.Softplus()\n","        self.softmax = nn.Softmax(1)\n","        self.normal = Normal(torch.tensor([0.0]), torch.tensor([1.0]))\n","\n","        assert(self.k <= self.num_experts)\n","\n","    def cv_squared(self, x):\n","        \"\"\"The squared coefficient of variation of a sample.\n","        Useful as a loss to encourage a positive distribution to be more uniform.\n","        Epsilons added for numerical stability.\n","        Returns 0 for an empty Tensor.\n","        Args:\n","        x: a `Tensor`.\n","        Returns:\n","        a `Scalar`.\n","        \"\"\"\n","        eps = 1e-10\n","        # if only num_experts = 1\n","        if x.shape[0] == 1:\n","            return torch.Tensor([0])\n","        return x.float().var() / (x.float().mean()**2 + eps)\n","\n","\n","    def _gates_to_load(self, gates):\n","        \"\"\"Compute the true load per expert, given the gates.\n","        The load is the number of examples for which the corresponding gate is >0.\n","        Args:\n","        gates: a `Tensor` of shape [batch_size, n]\n","        Returns:\n","        a float32 `Tensor` of shape [n]\n","        \"\"\"\n","        return (gates > 0).sum(0)\n","\n","\n","\n","\n","    def _prob_in_top_k(self, clean_values, noisy_values, noise_stddev, noisy_top_values):\n","        \"\"\"Helper function to NoisyTopKGating.\n","        Computes the probability that value is in top k, given different random noise.\n","        This gives us a way of backpropagating from a loss that balances the number\n","        of times each expert is in the top k experts per example.\n","        In the case of no noise, pass in None for noise_stddev, and the result will\n","        not be differentiable.\n","        Args:\n","        clean_values: a `Tensor` of shape [batch, n].\n","        noisy_values: a `Tensor` of shape [batch, n].  Equal to clean values plus\n","          normally distributed noise with standard deviation noise_stddev.\n","        noise_stddev: a `Tensor` of shape [batch, n], or None\n","        noisy_top_values: a `Tensor` of shape [batch, m].\n","           \"values\" Output of tf.top_k(noisy_top_values, m).  m >= k+1\n","        Returns:\n","        a `Tensor` of shape [batch, n].\n","        \"\"\"\n","\n","        batch = clean_values.size(0)\n","        m = noisy_top_values.size(1)\n","        top_values_flat = noisy_top_values.flatten()\n","        threshold_positions_if_in = torch.arange(batch) * m + self.k\n","        threshold_if_in = torch.unsqueeze(torch.gather(top_values_flat, 0, threshold_positions_if_in), 1)\n","        is_in = torch.gt(noisy_values, threshold_if_in)\n","        threshold_positions_if_out = threshold_positions_if_in - 1\n","        threshold_if_out = torch.unsqueeze(torch.gather(top_values_flat,0 , threshold_positions_if_out), 1)\n","        # is each value currently in the top k.\n","        prob_if_in = self.normal.cdf((clean_values - threshold_if_in)/noise_stddev)\n","        prob_if_out = self.normal.cdf((clean_values - threshold_if_out)/noise_stddev)\n","        prob = torch.where(is_in, prob_if_in, prob_if_out)\n","        return prob\n","\n","\n","    def noisy_top_k_gating(self, x, train, noise_epsilon=1e-2):\n","        \"\"\"Noisy top-k gating.\n","          See paper: https://arxiv.org/abs/1701.06538.\n","          Args:\n","            x: input Tensor with shape [batch_size, input_size]\n","            train: a boolean - we only add noise at training time.\n","            noise_epsilon: a float\n","          Returns:\n","            gates: a Tensor with shape [batch_size, num_experts]\n","            load: a Tensor with shape [num_experts]\n","        \"\"\"\n","        clean_logits = x @ self.w_gate\n","        if self.noisy_gating:\n","            raw_noise_stddev = x @ self.w_noise\n","            noise_stddev = ((self.softplus(raw_noise_stddev) + noise_epsilon) * train)\n","            noisy_logits = clean_logits + ( torch.randn_like(clean_logits) * noise_stddev)\n","            logits = noisy_logits\n","        else:\n","            logits = clean_logits\n","\n","        # calculate topk + 1 that will be needed for the noisy gates\n","        top_logits, top_indices = logits.topk(min(self.k + 1, self.num_experts), dim=1)\n","        top_k_logits = top_logits[:, :self.k]\n","        top_k_indices = top_indices[:, :self.k]\n","        top_k_gates = self.softmax(top_k_logits)\n","\n","        zeros = torch.zeros_like(logits, requires_grad=True)\n","        gates = zeros.scatter(1, top_k_indices, top_k_gates)\n","\n","        if self.noisy_gating and self.k < self.num_experts:\n","            load = (self._prob_in_top_k(clean_logits, noisy_logits, noise_stddev, top_logits)).sum(0)\n","        else:\n","            load = self._gates_to_load(gates)\n","        return gates, load\n","\n","\n","\n","    def forward(self, x, train=True, loss_coef=1e-2):\n","        \"\"\"Args:\n","        x: tensor shape [batch_size, input_size]\n","        train: a boolean scalar.\n","        loss_coef: a scalar - multiplier on load-balancing losses\n","        Returns:\n","        y: a tensor with shape [batch_size, output_size].\n","        extra_training_loss: a scalar.  This should be added into the overall\n","        training loss of the model.  The backpropagation of this loss\n","        encourages all experts to be approximately equally used across a batch.\n","        \"\"\"\n","        gates, load = self.noisy_top_k_gating(x, train)\n","        # calculate importance loss\n","        importance = gates.sum(0)\n","        #\n","        loss = self.cv_squared(importance) + self.cv_squared(load)\n","        loss *= loss_coef\n","\n","        dispatcher = SparseDispatcher(self.num_experts, gates)\n","        expert_inputs = dispatcher.dispatch(x)\n","        gates = dispatcher.expert_to_gates()\n","        expert_outputs = [self.experts[i](expert_inputs[i]) for i in range(self.num_experts)]\n","        y = dispatcher.combine(expert_outputs)\n","        return y, loss"],"execution_count":3,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4RyYGM1Rfwco"},"source":["## Dummy example\n","\n","a minimal working example illustrating how to train and evaluate the MoE layer with dummy inputs and targets."]},{"cell_type":"markdown","metadata":{"id":"oAprN9tbcZAl"},"source":["> Note: If dummy example doesn't work, try in `torch==1.0.0`"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"O7VsrYAUcmih","executionInfo":{"status":"ok","timestamp":1634544948503,"user_tz":-330,"elapsed":12,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}},"outputId":"0e6d079c-83a5-474a-8082-56b3d9ba519d"},"source":["def train(x,y, model, loss_fn, optim):\n","    # model returns the prediction and the loss that encourages all experts to have equal importance and load\n","    y_hat, aux_loss = model(x.float())\n","    # calculate prediction loss\n","    loss = loss_fn(y_hat, y)\n","    # combine losses\n","    total_loss = loss + aux_loss\n","    optim.zero_grad()\n","    total_loss.backward()\n","    optim.step()\n","\n","    print(\"Training Results - loss: {:.2f}, aux_loss: {:.3f}\".format(loss.item(), aux_loss.item()))\n","    return model\n","\n","def eval(x, y, model, loss_fn):\n","    model.eval()\n","    # model returns the prediction and the loss that encourages all experts to have equal importance and load\n","    y_hat, aux_loss = model(x.float(), train=False)\n","    loss = loss_fn(y_hat, y)\n","    total_loss = loss + aux_loss\n","    print(\"Evaluation Results - loss: {:.2f}, aux_loss: {:.3f}\".format(loss.item(), aux_loss.item()))\n","\n","\n","\n","def dummy_data(batch_size, input_size, num_classes):\n","    # dummy input\n","    x = torch.rand(batch_size, input_size)\n","\n","    # dummy target\n","    y = torch.randint(num_classes, (batch_size, 1)).squeeze(1)\n","    return x,y\n","\n","\n","# arguments\n","input_size = 1000\n","num_classes = 20\n","num_experts = 10\n","hidden_size = 64\n","batch_size = 5\n","k = 4\n","\n","# instantiate the MoE layer\n","model = MoE(input_size, num_classes, num_experts,hidden_size, k=k, noisy_gating=True)\n","\n","loss_fn = nn.NLLLoss()\n","optim = Adam(model.parameters())\n","\n","x, y = dummy_data(batch_size, input_size, num_classes)\n","\n","# train\n","model = train(x, y, model, loss_fn, optim)\n","# evaluate\n","x, y = dummy_data(batch_size, input_size, num_classes)\n","eval(x, y, model, loss_fn)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Training Results - loss: 3.01, aux_loss: 0.002\n","Evaluation Results - loss: 3.00, aux_loss: 0.033\n"]}]},{"cell_type":"markdown","metadata":{"id":"fFG0tXWOf2AQ"},"source":["## CIFAR-10 example\n","\n","a minimal working example of the CIFAR 10 dataset. It achieves an accuracy of 39% with arbitrary hyper-parameters and not fully converged."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":372,"referenced_widgets":["1a14a77a376b4892ad2ccbba543aeac1","8003e3eb521149ac86066798acf1aa6b","ee0d26e63b8649eabf9be40886db83c3","eeddb363983440c99e56532b051c3d09","63f0c0ab3cc748bf90b5a47c1319ca7f","6e5a671d75c64630853164d83366d6d3","19b74d9654c24a34926c63c5f59da3ae","017f5ab15f454ec3ba2cce157ab7ff7a","fea6f37c9fac47578d493ad661402d2d","66720267afbc4d87946ff29e74a740f9","3173d4397e9c445abdd7fa3b379163bb"]},"id":"kL_yIDpxfBzT","executionInfo":{"status":"ok","timestamp":1635266865862,"user_tz":-330,"elapsed":125229,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}},"outputId":"b634622b-24b7-4875-8f77-9fe01431abe8"},"source":["transform = transforms.Compose(\n","    [transforms.ToTensor(),\n","     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n","\n","trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n","                                        download=True, transform=transform)\n","trainloader = torch.utils.data.DataLoader(trainset, batch_size=64,\n","                                          shuffle=True, num_workers=2)\n","\n","testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n","                                       download=True, transform=transform)\n","testloader = torch.utils.data.DataLoader(testset, batch_size=64,\n","                                         shuffle=False, num_workers=2)\n","\n","classes = ('plane', 'car', 'bird', 'cat',\n","           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n","\n","\n","device = torch.device('cpu')\n","net = MoE(input_size=3072,output_size= 10, num_experts=10, hidden_size=256, noisy_gating=True, k=4)\n","net = net.to(device)\n","\n","criterion = nn.CrossEntropyLoss()\n","optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n","\n","\n","for epoch in range(2):  # loop over the dataset multiple times\n","\n","    running_loss = 0.0\n","    for i, data in enumerate(trainloader, 0):\n","        # get the inputs; data is a list of [inputs, labels]\n","        inputs, labels = data\n","        inputs, labels = inputs.to(device), labels.to(device)\n","\n","        # zero the parameter gradients\n","        optimizer.zero_grad()\n","\n","        # forward + backward + optimize\n","        inputs = inputs.view(inputs.shape[0], -1)\n","        outputs, aux_loss = net(inputs)\n","        loss = criterion(outputs, labels)\n","        total_loss = loss + aux_loss\n","        total_loss.backward()\n","        optimizer.step()\n","\n","        # print statistics\n","        running_loss += loss.item()\n","        if i % 100 == 99:    # print every 2000 mini-batches\n","            print('[%d, %5d] loss: %.3f' %\n","                  (epoch + 1, i + 1, running_loss / 100))\n","            running_loss = 0.0\n","\n","print('Finished Training')\n","\n","\n","correct = 0\n","total = 0\n","with torch.no_grad():\n","    for data in testloader:\n","        images, labels = data\n","        outputs, _ = net(images.view(images.shape[0], -1))\n","        _, predicted = torch.max(outputs.data, 1)\n","        total += labels.size(0)\n","        correct += (predicted == labels).sum().item()\n","\n","print('Accuracy of the network on the 10000 test images: %d %%' % (\n","    100 * correct / total))"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"]},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"1a14a77a376b4892ad2ccbba543aeac1","version_minor":0,"version_major":2},"text/plain":["  0%|          | 0/170498071 [00:00<?, ?it/s]"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Extracting ./data/cifar-10-python.tar.gz to ./data\n","Files already downloaded and verified\n","[1,   100] loss: 2.284\n","[1,   200] loss: 2.229\n","[1,   300] loss: 2.154\n","[1,   400] loss: 2.089\n","[1,   500] loss: 2.056\n","[1,   600] loss: 1.999\n","[1,   700] loss: 1.958\n","[2,   100] loss: 1.898\n","[2,   200] loss: 1.865\n","[2,   300] loss: 1.859\n","[2,   400] loss: 1.829\n","[2,   500] loss: 1.798\n","[2,   600] loss: 1.789\n","[2,   700] loss: 1.761\n","Finished Training\n","Accuracy of the network on the 10000 test images: 39 %\n"]}]}]}