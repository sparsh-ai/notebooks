{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "project_name = \"reco-tut-mlh\"; branch = \"main\"; account = \"sparsh-ai\"\n",
    "project_path = os.path.join('/content', project_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(project_path):\n",
    "    !cp /content/drive/MyDrive/mykeys.py /content\n",
    "    import mykeys\n",
    "    !rm /content/mykeys.py\n",
    "    path = \"/content/\" + project_name; \n",
    "    !mkdir \"{path}\"\n",
    "    %cd \"{path}\"\n",
    "    import sys; sys.path.append(path)\n",
    "    !git config --global user.email \"recotut@recohut.com\"\n",
    "    !git config --global user.name  \"reco-tut\"\n",
    "    !git init\n",
    "    !git remote add origin https://\"{mykeys.git_token}\":x-oauth-basic@github.com/\"{account}\"/\"{project_name}\".git\n",
    "    !git pull origin \"{branch}\"\n",
    "    !git checkout main\n",
    "else:\n",
    "    %cd \"{project_path}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git pull --rebase origin main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git add . && git commit -m 'commit' && git push origin \"{branch}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0,'./code')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Standard Variational Autoencoder (SVAE)\n",
    "\n",
    "The Standard Variational Autoencoder (SVAE), SVAE uses an autoencoder to generate a salient feature representation of users, learning a latent vector for each user. The decoder then takes this latent representation and outputs a probability distribution over all items; we get probabilities of all the movies being watched by each user."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "from utils import numpy_stratified_split\n",
    "import build_features\n",
    "import metrics\n",
    "from models import SVAE\n",
    "from tensorflow.python.framework.ops import disable_eager_execution\n",
    "\n",
    "disable_eager_execution()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fp = os.path.join('./data/bronze', 'u.data')\n",
    "raw_data = pd.read_csv(fp, sep='\\t', names=['userId', 'movieId', 'rating', 'timestamp'])\n",
    "print(f'Shape: {raw_data.shape}')\n",
    "raw_data.sample(5, random_state=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Binarize the data (only keep ratings >= 4)\n",
    "df_preferred = raw_data[raw_data['rating'] > 3.5]\n",
    "print (df_preferred.shape)\n",
    "df_low_rating = raw_data[raw_data['rating'] <= 3.5]\n",
    "\n",
    "df_preferred.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep users who clicked on at least 5 movies\n",
    "df = df_preferred.groupby('userId').filter(lambda x: len(x) >= 5)\n",
    "\n",
    "# Keep movies that were clicked on by at least on 1 user\n",
    "df = df.groupby('movieId').filter(lambda x: len(x) >= 1)\n",
    "\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain both usercount and itemcount after filtering\n",
    "usercount = df[['userId']].groupby('userId', as_index = False).size()\n",
    "itemcount = df[['movieId']].groupby('movieId', as_index = False).size()\n",
    "\n",
    "# Compute sparsity after filtering\n",
    "sparsity = 1. * raw_data.shape[0] / (usercount.shape[0] * itemcount.shape[0])\n",
    "\n",
    "print(\"After filtering, there are %d watching events from %d users and %d movies (sparsity: %.3f%%)\" % \n",
    "      (raw_data.shape[0], usercount.shape[0], itemcount.shape[0], sparsity * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_users =sorted(df.userId.unique())\n",
    "np.random.seed(123)\n",
    "unique_users = np.random.permutation(unique_users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HELDOUT_USERS = 200\n",
    "\n",
    "# Create train/validation/test users\n",
    "n_users = len(unique_users)\n",
    "print(\"Number of unique users:\", n_users)\n",
    "\n",
    "train_users = unique_users[:(n_users - HELDOUT_USERS * 2)]\n",
    "print(\"\\nNumber of training users:\", len(train_users))\n",
    "\n",
    "val_users = unique_users[(n_users - HELDOUT_USERS * 2) : (n_users - HELDOUT_USERS)]\n",
    "print(\"\\nNumber of validation users:\", len(val_users))\n",
    "\n",
    "test_users = unique_users[(n_users - HELDOUT_USERS):]\n",
    "print(\"\\nNumber of test users:\", len(test_users))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For training set keep only users that are in train_users list\n",
    "train_set = df.loc[df['userId'].isin(train_users)]\n",
    "print(\"Number of training observations: \", train_set.shape[0])\n",
    "\n",
    "# For validation set keep only users that are in val_users list\n",
    "val_set = df.loc[df['userId'].isin(val_users)]\n",
    "print(\"\\nNumber of validation observations: \", val_set.shape[0])\n",
    "\n",
    "# For test set keep only users that are in test_users list\n",
    "test_set = df.loc[df['userId'].isin(test_users)]\n",
    "print(\"\\nNumber of test observations: \", test_set.shape[0])\n",
    "\n",
    "# train_set/val_set/test_set contain user - movie interactions with rating 4 or 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain list of unique movies used in training set\n",
    "unique_train_items = pd.unique(train_set['movieId'])\n",
    "print(\"Number of unique movies that rated in training set\", unique_train_items.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For validation set keep only movies that used in training set\n",
    "val_set = val_set.loc[val_set['movieId'].isin(unique_train_items)]\n",
    "print(\"Number of validation observations after filtering: \", val_set.shape[0])\n",
    "\n",
    "# For test set keep only movies that used in training set\n",
    "test_set = test_set.loc[test_set['movieId'].isin(unique_train_items)]\n",
    "print(\"\\nNumber of test observations after filtering: \", test_set.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the sparse matrix generation for train, validation and test sets\n",
    "# use list of unique items from training set for all sets\n",
    "am_train = build_features.AffinityMatrix(df=train_set, items_list=unique_train_items)\n",
    "\n",
    "am_val = build_features.AffinityMatrix(df=val_set, items_list=unique_train_items)\n",
    "\n",
    "am_test = build_features.AffinityMatrix(df=test_set, items_list=unique_train_items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain the sparse matrix for train, validation and test sets\n",
    "train_data, _, _ = am_train.gen_affinity_matrix()\n",
    "print(train_data.shape)\n",
    "\n",
    "val_data, val_map_users, val_map_items = am_val.gen_affinity_matrix()\n",
    "print(val_data.shape)\n",
    "\n",
    "test_data, test_map_users, test_map_items = am_test.gen_affinity_matrix()\n",
    "print(test_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split validation and test data into training and testing parts\n",
    "val_data_tr, val_data_te = numpy_stratified_split(val_data, ratio=0.75, seed=123)\n",
    "test_data_tr, test_data_te = numpy_stratified_split(test_data, ratio=0.75, seed=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Binarize train, validation and test data\n",
    "train_data = np.where(train_data > 3.5, 1.0, 0.0)\n",
    "val_data = np.where(val_data > 3.5, 1.0, 0.0)\n",
    "test_data = np.where(test_data > 3.5, 1.0, 0.0)\n",
    "\n",
    "# Binarize validation data: training part  \n",
    "val_data_tr = np.where(val_data_tr > 3.5, 1.0, 0.0)\n",
    "# Binarize validation data: testing part (save non-binary version in the separate object, will be used for calculating NDCG)\n",
    "val_data_te_ratings = val_data_te.copy()\n",
    "val_data_te = np.where(val_data_te > 3.5, 1.0, 0.0)\n",
    "\n",
    "# Binarize test data: training part \n",
    "test_data_tr = np.where(test_data_tr > 3.5, 1.0, 0.0)\n",
    "\n",
    "# Binarize test data: testing part (save non-binary version in the separate object, will be used for calculating NDCG)\n",
    "test_data_te_ratings = test_data_te.copy()\n",
    "test_data_te = np.where(test_data_te > 3.5, 1.0, 0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieve real ratings from initial dataset \n",
    "\n",
    "test_data_te_ratings=pd.DataFrame(test_data_te_ratings)\n",
    "val_data_te_ratings=pd.DataFrame(val_data_te_ratings)\n",
    "\n",
    "for index,i in df_low_rating.iterrows():\n",
    "    user_old= i['userId'] # old value \n",
    "    item_old=i['movieId'] # old value \n",
    "\n",
    "    if (test_map_users.get(user_old) is not None)  and (test_map_items.get(item_old) is not None) :\n",
    "        user_new=test_map_users.get(user_old) # new value \n",
    "        item_new=test_map_items.get(item_old) # new value \n",
    "        rating=i['rating'] \n",
    "        test_data_te_ratings.at[user_new,item_new]= rating   \n",
    "\n",
    "    if (val_map_users.get(user_old) is not None)  and (val_map_items.get(item_old) is not None) :\n",
    "        user_new=val_map_users.get(user_old) # new value \n",
    "        item_new=val_map_items.get(item_old) # new value \n",
    "        rating=i['rating'] \n",
    "        val_data_te_ratings.at[user_new,item_new]= rating   \n",
    "\n",
    "\n",
    "val_data_te_ratings=val_data_te_ratings.to_numpy()    \n",
    "test_data_te_ratings=test_data_te_ratings.to_numpy()    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INTERMEDIATE_DIM = 200\n",
    "LATENT_DIM = 64\n",
    "EPOCHS = 400\n",
    "BATCH_SIZE = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SVAE.StandardVAE(n_users=train_data.shape[0], # Number of unique users in the training set\n",
    "                                   original_dim=train_data.shape[1], # Number of unique items in the training set\n",
    "                                   intermediate_dim=INTERMEDIATE_DIM, \n",
    "                                   latent_dim=LATENT_DIM, \n",
    "                                   n_epochs=EPOCHS, \n",
    "                                   batch_size=BATCH_SIZE, \n",
    "                                   k=10,\n",
    "                                   verbose=0,\n",
    "                                   seed=123,\n",
    "                                   drop_encoder=0.5,\n",
    "                                   drop_decoder=0.5,\n",
    "                                   annealing=False,\n",
    "                                   beta=1.0\n",
    "                                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "model.fit(x_train=train_data,\n",
    "          x_valid=val_data,\n",
    "          x_val_tr=val_data_tr,\n",
    "          x_val_te=val_data_te_ratings, # with the original ratings\n",
    "          mapper=am_val\n",
    "          )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recommend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model prediction on the training part of test set \n",
    "top_k =  model.recommend_k_items(x=test_data_tr,k=10,remove_seen=True)\n",
    "\n",
    "# Convert sparse matrix back to df\n",
    "recommendations = am_test.map_back_sparse(top_k, kind='prediction')\n",
    "test_df = am_test.map_back_sparse(test_data_te_ratings, kind='ratings') # use test_data_te_, with the original ratings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create column with the predicted movie's rank for each user \n",
    "top_k = recommendations.copy()\n",
    "top_k['rank'] = recommendations.groupby('userId', sort=False).cumcount() + 1  # For each user, only include movies recommendations that are also in the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision_at_k = metrics.precision_at_k(top_k, test_df, 'userId', 'movieId', 'rank')\n",
    "recall_at_k = metrics.recall_at_k(top_k, test_df, 'userId', 'movieId', 'rank')\n",
    "mean_average_precision = metrics.mean_average_precision(top_k, test_df, 'userId', 'movieId', 'rank')\n",
    "ndcg = metrics.ndcg(top_k, test_df, 'userId', 'movieId', 'rank')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Precision: {precision_at_k:.6f}',\n",
    "      f'Recall: {recall_at_k:.6f}',\n",
    "      f'MAP: {mean_average_precision:.6f} ',\n",
    "      f'NDCG: {ndcg:.6f}', sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "\n",
    "1.   Kilol Gupta, Mukunds Y. Raghuprasad, Pankhuri Kumar, A Hybrid Variational Autoencoder for Collaborative Filtering, 2018, https://arxiv.org/pdf/1808.01006.pdf\n",
    "\n",
    "2.   Microsoft SVAE implementation: https://github.com/microsoft/recommenders/blob/main/examples/02_model_collaborative_filtering/standard_vae_deep_dive.ipynb\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPZEOjWIuHceGRKTXhyvWVv",
   "collapsed_sections": [],
   "mount_file_id": "17Scuoh_xL6c7q1JIKFzaSo8sCWFPADv6",
   "name": "reco-tut-mlh-02-svae.ipynb",
   "provenance": [
    {
     "file_id": "1hpz-977VUa3vPqE_nh6mVDC5jOxu4ypm",
     "timestamp": 1628675091827
    },
    {
     "file_id": "1AqVI1vgtpjVbcx5XdPgFnkJCmWXMAg7G",
     "timestamp": 1628674309073
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
