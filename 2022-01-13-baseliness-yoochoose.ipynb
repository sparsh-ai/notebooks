{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"2022-01-13-baseliness-yoochoose.ipynb","provenance":[{"file_id":"https://github.com/recohut/nbs/blob/main/raw/P409152%20%7C%20Training%20AR%2C%20SR%2C%20S-POP%2C%20and%20VSKNN%20models%20on%20Yoochoose%20dataset.ipynb","timestamp":1644611789696},{"file_id":"1FEZmnoLGIsTsGiK2gi1TsIHLAaWCXF_a","timestamp":1641012885831}],"collapsed_sections":[],"toc_visible":true,"mount_file_id":"1FEZmnoLGIsTsGiK2gi1TsIHLAaWCXF_a","authorship_tag":"ABX9TyMCo1+cvbrr2NsJabrCvae5"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Training AR, SR, S-POP, and VSKNN models on Yoochoose dataset"],"metadata":{"id":"RD5Z0MFEGDxN"}},{"cell_type":"markdown","source":["## Yoochoose Dataset"],"metadata":{"id":"_a5mQwhZADCf"}},{"cell_type":"code","source":["!pip install -qq recohut==0.0.9"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3BJTggv4AfLp","executionInfo":{"status":"ok","timestamp":1641013225139,"user_tz":-330,"elapsed":6815,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}},"outputId":"93f01e5b-3d89-4695-b424-c8b838d08296"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[?25l\r\u001b[K     |██                              | 10 kB 17.7 MB/s eta 0:00:01\r\u001b[K     |████▏                           | 20 kB 3.4 MB/s eta 0:00:01\r\u001b[K     |██████▎                         | 30 kB 3.7 MB/s eta 0:00:01\r\u001b[K     |████████▍                       | 40 kB 3.5 MB/s eta 0:00:01\r\u001b[K     |██████████▌                     | 51 kB 3.4 MB/s eta 0:00:01\r\u001b[K     |████████████▋                   | 61 kB 3.6 MB/s eta 0:00:01\r\u001b[K     |██████████████▊                 | 71 kB 3.5 MB/s eta 0:00:01\r\u001b[K     |████████████████▉               | 81 kB 4.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 92 kB 4.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 102 kB 3.8 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▏        | 112 kB 3.8 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▎      | 122 kB 3.8 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▍    | 133 kB 3.8 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▌  | 143 kB 3.8 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▋| 153 kB 3.8 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 155 kB 3.8 MB/s \n","\u001b[?25h"]}]},{"cell_type":"code","source":["from recohut.utils.common_utils import download_url"],"metadata":{"id":"3Z0xeHBh_9bc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["data_root = '/content/data'\n","download_url('https://github.com/RecoHut-Datasets/yoochoose/raw/v4/yoochoose_train.txt', data_root)\n","download_url('https://github.com/RecoHut-Datasets/yoochoose/raw/v4/yoochoose_valid.txt', data_root)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":70},"id":"IIcwSWSa1j7S","executionInfo":{"status":"ok","timestamp":1641013297948,"user_tz":-330,"elapsed":2381,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}},"outputId":"ae220435-0079-4d5c-b11c-0bdd7f6107bf"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Downloading https://github.com/RecoHut-Datasets/yoochoose/raw/v4/yoochoose_train.txt\n","Downloading https://github.com/RecoHut-Datasets/yoochoose/raw/v4/yoochoose_valid.txt\n"]},{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'/content/data/yoochoose_valid.txt'"]},"metadata":{},"execution_count":6}]},{"cell_type":"markdown","source":["## Association Rules (AR)\n","\n","Simple Association Rules (AR) are a simplified version of the association rule mining technique [Agrawal et al. 1993] with a maximum rule size of two. The method is designed to capture the frequency of two co-occurring events, e.g., “Customers who bought . . . also bought”. Algorithmically, the rules and their corresponding importance are “learned” by counting how often the items i and j occurred together in a session of any user. Let a session s be a chronologically ordered tuple of item click events s = ($s_1$,$s_2$,$s_3$, . . . ,$s_m$) and $S_p$ the set of all past sessions. Given a user’s current session s with $s_{|s|}$ being the last item interaction in s, we can define the score for a recommendable item i as follows, where the indicator function $1_{EQ}(a,b)$ is 1 in case a and b refer to the same item and 0 otherwise.\n","\n","$$score_{AR}(i,s) = \\dfrac{1}{\\sum_{p \\in S_p}\\sum_{x=1}^{|p|}1_{EQ}(s_{|s|},p_x)\\cdot(|p|-1)}\\sum_{p \\in s_p}\\sum_{x=1}^{|p|}\\sum_{y=1}^{|p|}1_{EQ}(s_{|s|},p_x)\\cdot1_{EQ}(i,p_y)$$\n","\n","In the above equation, the sums at the right-hand side represent the counting scheme. The term at the left-hand side normalizes the score by the number of total rule occurrences originating from the current item $s_{|s|}$. A list of recommendations returned by the ar method then contains the items with the highest scores in descending order. No minimum support or confidence thresholds are applied.\n","\n","> References\n","- [https://arxiv.org/pdf/1803.09587.pdf](https://arxiv.org/pdf/1803.09587.pdf)\n","- [http://www.rakesh.agrawal-family.com/papers/sigmod93assoc.pdf](http://www.rakesh.agrawal-family.com/papers/sigmod93assoc.pdf)\n","- [https://github.com/mmaher22/iCV-SBR/tree/master/Source Codes/AR%26SR_Python](https://github.com/mmaher22/iCV-SBR/tree/master/Source%20Codes/AR%26SR_Python)"],"metadata":{"id":"IIqP4BuO_jXp"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"DPCu6bnQ1j7R"},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","import collections as col"]},{"cell_type":"code","source":["class AssosiationRules: \n","    '''\n","    AssosiationRules(pruning=10, session_key='SessionId', item_keys=['ItemId'])\n","    Parameters\n","    --------\n","    pruning : int\n","        Prune the results per item to a list of the top N co-occurrences. (Default value: 10)\n","    session_key : string\n","        The data frame key for the session identifier. (Default value: SessionId)\n","    item_keys : string\n","        The data frame list of keys for the item identifier as first item in list \n","        and features keys next. (Default value: [ItemId])\n","    '''\n","    def __init__( self, pruning=10, session_key='SessionID', item_keys=['ItemID'] ):\n","        self.pruning = pruning\n","        self.session_key = session_key\n","        self.item_keys = item_keys\n","        self.items_features = {}\n","        self.predict_for_item_ids = []\n","        \n","    def fit( self, data):\n","        '''\n","        Trains the predictor.\n","        \n","        Parameters\n","        --------\n","        data: pandas.DataFrame\n","            Training data. It contains the transactions of the sessions. \n","            It has one column for session IDs, one for item IDs and many for the\n","            item features if exist.\n","            It must have a header. Column names are arbitrary, but must \n","            correspond to the ones you set during the initialization of the \n","            network (session_key, item_keys).\n","        '''\n","        cur_session = -1\n","        last_items = []\n","        all_rules = []\n","        indices_item = []\n","        for i in self.item_keys:\n","            all_rules.append(dict())\n","            indices_item.append( data.columns.get_loc(i) )\n","\n","        data.sort_values(self.session_key, inplace=True)\n","        index_session = data.columns.get_loc(self.session_key)\n","        \n","        #Create Dictionary of items and their features\n","        for row in data.itertuples( index=False ):\n","            item_id = row[indices_item[0]]\n","            if not item_id in self.items_features.keys() :\n","                self.items_features[item_id] = []\n","                for i in indices_item:\n","                    self.items_features[item_id].append(row[i])\n","              \n","        for i in range(len(self.item_keys)):\n","            rules = all_rules[i]\n","            index_item = indices_item[i]\n","            for row in data.itertuples( index=False ):\n","                session_id, item_id = row[index_session], row[index_item]\n","                if session_id != cur_session:\n","                    cur_session = session_id\n","                    last_items = []\n","                else: \n","                    for item_id2 in last_items:                \n","                        if not item_id in rules :\n","                            rules[item_id] = dict()                \n","                        if not item_id2 in rules :\n","                            rules[item_id2] = dict()                \n","                        if not item_id in rules[item_id2]:\n","                            rules[item_id2][item_id] = 0           \n","                        if not item_id2 in rules[item_id]:\n","                            rules[item_id][item_id2] = 0\n","                        \n","                        rules[item_id][item_id2] += 1\n","                        rules[item_id2][item_id] += 1\n","                        \n","                last_items.append(item_id)\n","                \n","            if self.pruning > 0:\n","                rules = self.prune(rules) \n","                \n","            all_rules[i] = rules\n","        self.all_rules = all_rules\n","        self.predict_for_item_ids = list(self.all_rules[0].keys())\n","    def predict_next(self, session_items, k = 20):\n","        '''\n","        Gives predicton scores for a selected set of items on how likely they be the next item in the session.\n","                \n","        Parameters\n","        --------\n","        session_items : List\n","            Items IDs in current session.\n","        k : Integer\n","            How many items to recommend\n","        Returns\n","        --------\n","        out : pandas.Series\n","            Prediction scores for selected items on how likely to be the next item of this session. \n","            Indexed by the item IDs.\n","        \n","        '''\n","        all_len = len(self.predict_for_item_ids)\n","        input_item_id = session_items[-1]\n","        preds = np.zeros( all_len ) \n","             \n","        if input_item_id in self.all_rules[0].keys():\n","            for k_ind in range(all_len):\n","                key = self.predict_for_item_ids[k_ind]\n","                if key in session_items:\n","                    continue\n","                try:\n","                    preds[ k_ind ] += self.all_rules[0][input_item_id][key]\n","                except:\n","                    pass\n","                for i in range(1, len(self.all_rules)):\n","                    input_item_feature = self.items_features[input_item_id][i]\n","                    key_feature = self.items_features[key][i]\n","                    try:\n","                        preds[ k_ind ] += self.all_rules[i][input_item_feature][key_feature]\n","                    except:\n","                        pass\n","        \n","        series = pd.Series(data=preds, index=self.predict_for_item_ids)\n","        series = series / series.max()\n","        \n","        return series.nlargest(k).index.values\n","    \n","    def prune(self, rules): \n","        '''\n","        Gives predicton scores for a selected set of items on how likely they be the next item in the session.\n","        Parameters\n","            --------\n","            rules : dict of dicts\n","                The rules mined from the training data\n","        '''\n","        for k1 in rules:\n","            tmp = rules[k1]\n","            if self.pruning < 1:\n","                keep = len(tmp) - int( len(tmp) * self.pruning )\n","            elif self.pruning >= 1:\n","                keep = self.pruning\n","            counter = col.Counter( tmp )\n","            rules[k1] = dict()\n","            for k2, v in counter.most_common( keep ):\n","                rules[k1][k2] = v\n","        return rules"],"metadata":{"id":"ItJSYYDU9o7z"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import os\n","import time\n","import argparse\n","import pandas as pd"],"metadata":{"id":"ZMoCIzvw9mhZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["parser = argparse.ArgumentParser()\n","parser.add_argument('--prune', type=int, default=10, help=\"Association Rules Pruning Parameter\")\n","parser.add_argument('--K', type=int, default=20, help=\"K items to be used in Recall@K and MRR@K\")\n","parser.add_argument('--itemid', default='sid', type=str)\n","parser.add_argument('--sessionid', default='uid', type=str)\n","parser.add_argument('--item_feats', default='', type=str, \n","                    help=\"Names of Columns containing items features separated by #\")\n","parser.add_argument('--valid_data', default='yoochoose_valid.txt', type=str)\n","parser.add_argument('--train_data', default='yoochoose_train.txt', type=str)\n","parser.add_argument('--data_folder', default=data_root, type=str)\n","\n","# Get the arguments\n","args = parser.parse_args([])\n","train_data = os.path.join(args.data_folder, args.train_data)\n","x_train = pd.read_csv(train_data)\n","valid_data = os.path.join(args.data_folder, args.valid_data)\n","x_valid = pd.read_csv(valid_data)\n","x_valid.sort_values(args.sessionid, inplace=True)\n","\n","items_feats = [args.itemid]\n","ffeats = args.item_feats.strip().split(\"#\")\n","if ffeats[0] != '':\n","    items_feats.extend(ffeats)\n","\n","print('Finished Reading Data.')\n","# Fitting AR Model\n","print('Start Model Fitting...')\n","t1 = time.time()\n","model = AssosiationRules(session_key = args.sessionid, item_keys = items_feats, pruning=args.prune)\n","model.fit(x_train)\n","t2 = time.time()\n","print('End Model Fitting with total time =', t2 - t1)\n","\n","print('Start Predictions...')\n","# Test Set Evaluation\n","test_size = 0.0\n","hit = 0.0\n","MRR = 0.0\n","cur_length = 0\n","cur_session = -1\n","last_items = []\n","t1 = time.time()\n","index_item = x_valid.columns.get_loc(args.itemid)\n","index_session = x_valid.columns.get_loc(args.sessionid)\n","train_items = model.items_features.keys()\n","counter = 0\n","for row in x_valid.itertuples(index=False):\n","    counter += 1\n","    if counter % 5000 == 0:\n","        print('Finished Prediction for ', counter, 'items.')\n","    session_id, item_id = row[index_session], row[index_item]\n","    if session_id != cur_session:\n","        cur_session = session_id\n","        last_items = []\n","        cur_length = 0\n","    \n","    if not item_id in last_items and item_id in train_items:\n","        if len(last_items) > cur_length: #make prediction\n","            cur_length += 1\n","            test_size += 1\n","            # Predict the most similar items to items\n","            predictions = model.predict_next(last_items, k = args.K)\n","            #print('preds:', predictions)\n","            # Evaluation\n","            rank = 0\n","            for predicted_item in predictions:\n","                rank += 1\n","                if predicted_item == item_id:\n","                    hit += 1.0\n","                    MRR += 1/rank\n","                    break\n","        \n","        last_items.append(item_id)\n","t2 = time.time()\n","print('Recall: {}'.format(hit / test_size))\n","print ('\\nMRR: {}'.format(MRR / test_size))\n","print('End Model Predictions with total time =', t2 - t1)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4JK1xoL59WBm","executionInfo":{"status":"ok","timestamp":1641017921756,"user_tz":-330,"elapsed":159820,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}},"outputId":"1ef77be3-cce8-471a-c8fc-7779d1931371"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Finished Reading Data.\n","Start Model Fitting...\n","End Model Fitting with total time = 47.870760679244995\n","Start Predictions...\n","Finished Prediction for  5000 items.\n","Recall: 0.26574500768049153\n","\n","MRR: 0.1308005998098165\n","End Model Predictions with total time = 110.56373572349548\n"]}]},{"cell_type":"markdown","source":["## Sequential Rules\n","\n","The SR method as proposed in [Kamehkhosh et al. 2017] is a variation of MC and AR. It also takes the order of actions into account, but in a less restrictive manner. In contrast to the MC method, we create a rule when an item q appeared after an item p in a session even when other events happened between p and q. When assigning weights to the rules, we consider the number of elements appearing between p and q in the session. Specifically, we use the weight function $w_{SR}(x)$ = 1/(x), where x corresponds to the number of steps between the two items. Given the current session s, the sr method calculates the score for the target item i as follows:\n","\n","$$score_{SR}(i,s) = \\dfrac{1}{\\sum_{p \\in S_p}\\sum_{x=2}^{|p|}1_{EQ}(s_{|s|},p_x)\\cdot x}\\sum_{p \\in s_p}\\sum_{x=2}^{|p|}\\sum_{y=1}^{x-1}1_{EQ}(s_{|s|},p_y)\\cdot1_{EQ}(i,p_x)\\cdot w_{SR}(x-y)$$\n","\n","In contrast to the equation for AR, the third inner sum only considers indices of previous item view events for each session p. In addition, the weighting function $w_{SR}(x)$ is added. Again, we normalize the absolute score by the total number of rule occurrences for the current item $s_{|s|}$.\n","\n","> References\n","- [https://arxiv.org/pdf/1803.09587.pdf](https://arxiv.org/pdf/1803.09587.pdf)\n","- [https://github.com/mmaher22/iCV-SBR/tree/master/Source Codes/AR%26SR_Python](https://github.com/mmaher22/iCV-SBR/tree/master/Source%20Codes/AR%26SR_Python)"],"metadata":{"id":"kJWaToepAU5h"}},{"cell_type":"code","source":["import numpy as np\n","import pandas as pd\n","from math import log10\n","import collections as col"],"metadata":{"id":"4M2bpBGKAU3U"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class SequentialRules: \n","    '''\n","    SequentialRules(steps = 10, weighting='div', pruning=20.0, session_key='SessionId', item_keys=['ItemId'])\n","        \n","    Parameters\n","    --------\n","    pruning : int\n","        Prune the results per item to a list of the top N co-occurrences. (Default value: 10)\n","    session_key : string\n","        The data frame key for the session identifier. (Default value: SessionId)\n","    item_keys : string\n","        The data frame list of keys for the item identifier as first item in list \n","        and features keys next. (Default value: [ItemID])    \n","    steps : int\n","        Number of steps to walk back from the currently viewed item. (Default value: 10)\n","    weighting : string\n","        Weighting function for the previous items (linear, same, div, log, qudratic). (Default value: div)\n","    pruning : int\n","        Prune the results per item to a list of the top N sequential co-occurrences. (Default value: 20). \n","    '''\n","    \n","    def __init__( self, steps = 10, weighting='div', pruning=20, \n","                 session_key='SessionID', item_keys=['ItemId']):\n","        self.steps = steps\n","        self.pruning = pruning\n","        self.weighting = weighting\n","        self.session_key = session_key\n","        self.item_keys = item_keys\n","        self.items_features = {}\n","        self.predict_for_item_ids = []\n","        self.session = -1\n","        self.session_items = []\n","            \n","    def fit( self, train):\n","        '''\n","        Trains the predictor.\n","        \n","        Parameters\n","        --------\n","        data: pandas.DataFrame\n","            Training data. It contains the transactions of the sessions. \n","            It has one column for session IDs, one for item IDs and many for the\n","            item features if exist.\n","            It must have a header. Column names are arbitrary, but must \n","            correspond to the ones you set during the initialization of the \n","            network (session_key, item_keys).\n","        '''\n","        cur_session = -1\n","        last_items = []\n","        all_rules = []\n","        indices_item = []\n","        for i in self.item_keys:\n","            all_rules.append(dict())\n","            indices_item.append( train.columns.get_loc(i) )\n","            \n","        train.sort_values(self.session_key, inplace=True)\n","        index_session = train.columns.get_loc(self.session_key)\n","        \n","        #Create Dictionary of items and their features\n","        for row in train.itertuples( index=False ):\n","            item_id = row[indices_item[0]]\n","            if not item_id in self.items_features.keys() :\n","                self.items_features[item_id] = []\n","                for i in indices_item:\n","                    self.items_features[item_id].append(row[i])\n","        \n","        for i in range(len(self.item_keys)):\n","            rules = all_rules[i]\n","            index_item = indices_item[i] #which feature of the items to work on\n","            for row in train.itertuples( index=False ):\n","                session_id, item_id = row[index_session], row[index_item]\n","                if session_id != cur_session:\n","                    cur_session = session_id\n","                    last_items = []\n","                else: \n","                    for j in range( 1, self.steps+1 if len(last_items) >= self.steps else len(last_items)+1 ):\n","                        prev_item = last_items[-j]   \n","                        if not prev_item in rules :\n","                            rules[prev_item] = dict()        \n","                        if not item_id in rules[prev_item]:\n","                            rules[prev_item][item_id] = 0\n","                        \n","                        rules[prev_item][item_id] += getattr(self, self.weighting)( j )\n","                        \n","                last_items.append(item_id)\n","                \n","            if self.pruning > 0 :\n","                rules = self.prune( rules )\n","            \n","            all_rules[i] = rules\n","        \n","        self.all_rules = all_rules\n","        self.predict_for_item_ids = list(self.all_rules[0].keys())\n","    \n","    def linear(self, i):\n","        return 1 - (0.1*i) if i <= 100 else 0\n","    \n","    def same(self, i):\n","        return 1\n","    \n","    def div(self, i):\n","        return 1/i\n","    \n","    def log(self, i):\n","        return 1/(log10(i+1.7))\n","    \n","    def quadratic(self, i):\n","        return 1/(i*i)\n","    \n","    def predict_next(self, session_items, k = 20):\n","        '''\n","        Gives predicton scores for a selected set of items on how likely they be the next item in the session.\n","                \n","        Parameters\n","        --------\n","        session_items : List\n","            Items IDs in current session.\n","        k : Integer\n","            How many items to recommend\n","        Returns\n","        --------\n","        out : pandas.Series\n","            Prediction scores for selected items on how likely to be the next item of this session. \n","            Indexed by the item IDs.\n","        \n","        '''\n","        all_len = len(self.predict_for_item_ids)\n","        input_item_id = session_items[-1]\n","        preds = np.zeros( all_len ) \n","             \n","        if input_item_id in self.all_rules[0].keys():\n","            for k_ind in range(all_len):\n","                key = self.predict_for_item_ids[k_ind]\n","                if key in session_items:\n","                    continue\n","                try:\n","                    preds[ k_ind ] += self.all_rules[0][input_item_id][key]\n","                except:\n","                    pass\n","                for i in range(1, len(self.all_rules)):\n","                    input_item_feature = self.items_features[input_item_id][i]\n","                    key_feature = self.items_features[key][i]\n","                    try:\n","                        preds[ k_ind ] += self.all_rules[i][input_item_feature][key_feature]\n","                    except:\n","                        pass\n","        \n","        series = pd.Series(data=preds, index=self.predict_for_item_ids)\n","        series = series / series.max()\n","        \n","        return series.nlargest(k).index.values\n","    \n","    def prune(self, rules): \n","        '''\n","        Gives predicton scores for a selected set of items on how likely they be the next item in the session.\n","        Parameters\n","            --------\n","            rules : dict of dicts\n","                The rules mined from the training data\n","        '''\n","        for k1 in rules:\n","            tmp = rules[k1]\n","            if self.pruning < 1:\n","                keep = len(tmp) - int( len(tmp) * self.pruning )\n","            elif self.pruning >= 1:\n","                keep = self.pruning\n","            counter = col.Counter( tmp )\n","            rules[k1] = dict()\n","            for k2, v in counter.most_common( keep ):\n","                rules[k1][k2] = v\n","        return rules"],"metadata":{"id":"Kd0SJmn9AU1j"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import os\n","import time\n","import argparse\n","import pandas as pd"],"metadata":{"id":"-iV8WJ2RAUzU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["parser = argparse.ArgumentParser()\n","parser.add_argument('--prune', type=int, default=0, help=\"Association Rules Pruning Parameter\")\n","parser.add_argument('--K', type=int, default=20, help=\"K items to be used in Recall@K and MRR@K\")\n","parser.add_argument('--steps', type=int, default=10, help=\"Max Number of steps to walk back from the currently viewed item\")\n","parser.add_argument('--weighting', type=str, default='div', help=\"Weighting function for the previous items (linear, same, div, log, qudratic)\")\n","parser.add_argument('--itemid', default='sid', type=str)\n","parser.add_argument('--sessionid', default='uid', type=str)\n","parser.add_argument('--item_feats', default='', type=str, \n","                    help=\"Names of Columns containing items features separated by #\")\n","parser.add_argument('--valid_data', default='yoochoose_valid.txt', type=str)\n","parser.add_argument('--train_data', default='yoochoose_train.txt', type=str)\n","parser.add_argument('--data_folder', default=data_root, type=str)\n","\n","# Get the arguments\n","args = parser.parse_args([])\n","train_data = os.path.join(args.data_folder, args.train_data)\n","x_train = pd.read_csv(train_data)\n","valid_data = os.path.join(args.data_folder, args.valid_data)\n","x_valid = pd.read_csv(valid_data)\n","x_valid.sort_values(args.sessionid, inplace=True)\n","\n","items_feats = [args.itemid]\n","ffeats = args.item_feats.strip().split(\"#\")\n","if ffeats[0] != '':\n","    items_feats.extend(ffeats)\n","\n","print('Finished Reading Data \\nStart Model Fitting...')\n","# Fitting AR Model\n","t1 = time.time()\n","model = SequentialRules(session_key = args.sessionid, item_keys = items_feats, \n","                        pruning=args.prune, steps=args.steps, weighting=args.weighting)\n","model.fit(x_train)\n","t2 = time.time()\n","print('End Model Fitting with total time =', t2 - t1, '\\n Start Predictions...')\n","\n","# Test Set Evaluation\n","test_size = 0.0\n","hit = 0.0\n","MRR = 0.0\n","cur_length = 0\n","cur_session = -1\n","last_items = []\n","t1 = time.time()\n","index_item = x_valid.columns.get_loc(args.itemid)\n","index_session = x_valid.columns.get_loc(args.sessionid)\n","train_items = model.items_features.keys()\n","counter = 0\n","for row in x_valid.itertuples( index=False ):\n","    counter += 1\n","    if counter % 5000 == 0:\n","        print('Finished Prediction for ', counter, 'items.')\n","    session_id, item_id = row[index_session], row[index_item]\n","    if session_id != cur_session:\n","        cur_session = session_id\n","        last_items = []\n","        cur_length = 0\n","    \n","    if not item_id in last_items and item_id in train_items:\n","        if len(last_items) > cur_length: #make prediction\n","            cur_length += 1\n","            test_size += 1\n","            # Predict the most similar items to items\n","            predictions = model.predict_next(last_items, k = args.K)\n","            #print('preds:', predictions)\n","            # Evaluation\n","            rank = 0\n","            for predicted_item in predictions:\n","                rank += 1\n","                if predicted_item == item_id:\n","                    hit += 1.0\n","                    MRR += 1/rank\n","                    break\n","        \n","        last_items.append(item_id)\n","t2 = time.time()\n","print('Recall: {}'.format(hit / test_size))\n","print ('\\nMRR: {}'.format(MRR / test_size))\n","print('End Model Predictions with total time =', t2 - t1)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vcjwSWY5AcQV","executionInfo":{"status":"ok","timestamp":1641013547897,"user_tz":-330,"elapsed":146710,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}},"outputId":"3c0dfba2-b8dd-41cb-cb02-3e62cfbcf1cb"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Finished Reading Data \n","Start Model Fitting...\n","End Model Fitting with total time = 23.63178014755249 \n"," Start Predictions...\n","Finished Prediction for  5000 items.\n","Recall: 0.44143625192012287\n","\n","MRR: 0.16021305829773633\n","End Model Predictions with total time = 122.83067488670349\n"]}]},{"cell_type":"markdown","source":["## S-Pop\n","\n","Session popularity predictor that gives higher scores to items with higher number of occurrences in the session. Ties are broken up by adding the popularity score of the item.\n","\n","The score is given by $r_{s,i} = supp_{s,i} + \\frac{supp_i}{(1+supp_i)}$.\n","\n","> References\n","- [https://github.com/mmaher22/iCV-SBR/tree/master/Source Codes/S-POP_Python](https://github.com/mmaher22/iCV-SBR/tree/master/Source%20Codes/S-POP_Python)"],"metadata":{"id":"pxzrxEm-BSZt"}},{"cell_type":"code","source":["import numpy as np\n","import pandas as pd"],"metadata":{"id":"FnAGmanYCG6r"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class SessionPop:\n","    '''\n","    SessionPop(top_n=100, item_key='ItemId', support_by_key=None)\n","    Session popularity predictor that gives higher scores to items with higher number of occurrences in the session. \n","    Ties are broken up by adding the popularity score of the item.\n","    The score is given by:\n","    .. math::\n","        r_{s,i} = supp_{s,i} + \\\\frac{supp_i}{(1+supp_i)}\n","    Parameters\n","    --------\n","    top_n : int\n","        Only give back non-zero scores to the top N ranking items. Should be higher or equal than the cut-off of your evaluation. (Default value: 100)\n","    item_key : string\n","        The header of the item IDs in the training data. (Default value: 'ItemId')\n","    '''    \n","    def __init__(self, top_n = 1000, session_key = 'SessionId', item_key = 'ItemId'):\n","        self.top_n = top_n\n","        self.item_key = item_key\n","        self.session_id = session_key\n","        \n","    def fit(self, data):\n","        '''\n","        Trains the predictor.\n","        Parameters\n","        --------\n","        data: pandas.DataFrame\n","            Training data. It contains the transactions of the sessions. \n","            It has one column for session IDs, one for item IDs.\n","        '''\n","        self.items = data[self.item_key].unique()\n","        grp = data.groupby(self.item_key)\n","        self.pop_list = grp.size()\n","        self.pop_list = self.pop_list / (self.pop_list + 1)\n","        self.pop_list.sort_values(ascending=False, inplace=True)\n","        self.pop_list = self.pop_list.head(self.top_n)\n","        self.prev_session_id = -1\n","         \n","    def predict_next(self, last_items, k):\n","        '''\n","        Gives predicton scores for a selected set of items on how likely they be the next item in the session.\n","        Parameters\n","        --------\n","        last_items : list of items clicked in current session\n","        k : number of items to recommend and evaluate based on it\n","        Returns\n","        --------\n","        out : pandas.Series\n","            Prediction scores for selected items on how likely to be the next item of this session. Indexed by the item IDs.\n","        '''\n","        pers = {}\n","        for i in last_items:\n","            pers[i] = pers[i] + 1 if i in pers.keys() else  1\n","        \n","        preds = np.zeros(len(self.items))\n","        mask = np.in1d(self.items, self.pop_list.index)\n","        ser = pd.Series(pers)\n","        preds[mask] = self.pop_list[self.items[mask]]\n","        \n","        mask = np.in1d(self.items, ser.index)\n","        preds[mask] += ser[self.items[mask]]\n","        \n","        series = pd.Series(data=preds, index=self.items)\n","        series = series / series.max()    \n","        return series.nlargest(k).index.values"],"metadata":{"id":"MCqdZrvLCHPb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import os\n","import time\n","import argparse\n","import pandas as pd"],"metadata":{"id":"jENyoADWCJ1C"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["parser = argparse.ArgumentParser()\n","parser.add_argument('--K', type=int, default=20, help=\"K items to be used in Recall@K and MRR@K\")\n","parser.add_argument('--topn', type=int, default=100, help=\"Number of top items to return non zero scores for them (most popular)\")\n","parser.add_argument('--itemid', default='sid', type=str)\n","parser.add_argument('--sessionid', default='uid', type=str)\n","parser.add_argument('--valid_data', default='yoochoose_valid.txt', type=str)\n","parser.add_argument('--train_data', default='yoochoose_train.txt', type=str)\n","parser.add_argument('--data_folder', default=data_root, type=str)\n","\n","# Get the arguments\n","args = parser.parse_args([])\n","train_data = os.path.join(args.data_folder, args.train_data)\n","x_train = pd.read_csv(train_data)\n","valid_data = os.path.join(args.data_folder, args.valid_data)\n","x_valid = pd.read_csv(valid_data)\n","x_valid.sort_values(args.sessionid, inplace=True)\n","\n","print('Finished Reading Data \\nStart Model Fitting...')\n","# Fitting AR Model\n","t1 = time.time()\n","model = SessionPop(top_n = args.topn, session_key = args.sessionid, item_key = args.itemid)\n","model.fit(x_train)\n","t2 = time.time()\n","print('End Model Fitting with total time =', t2 - t1, '\\n Start Predictions...')\n","\n","# Test Set Evaluation\n","test_size = 0.0\n","hit = 0.0\n","MRR = 0.0\n","cur_length = 0\n","cur_session = -1\n","last_items = []\n","t1 = time.time()\n","index_item = x_valid.columns.get_loc(args.itemid)\n","index_session = x_valid.columns.get_loc(args.sessionid)\n","train_items = model.items\n","counter = 0\n","for row in x_valid.itertuples( index=False ):\n","    counter += 1\n","    if counter % 5000 == 0:\n","        print('Finished Prediction for ', counter, 'items.')\n","    session_id, item_id = row[index_session], row[index_item]\n","    if session_id != cur_session:\n","        cur_session = session_id\n","        last_items = []\n","        cur_length = 0\n","    \n","    if item_id in train_items:\n","        if len(last_items) > cur_length: #make prediction\n","            cur_length += 1\n","            test_size += 1\n","            # Predict the most similar items to items\n","            predictions = model.predict_next(last_items, k = args.K)\n","            # Evaluation\n","            rank = 0\n","            for predicted_item in predictions:\n","                rank += 1\n","                if predicted_item == item_id:\n","                    hit += 1.0\n","                    MRR += 1/rank\n","                    break\n","        \n","        last_items.append(item_id)\n","t2 = time.time()\n","print('Recall: {}'.format(hit / test_size))\n","print ('\\nMRR: {}'.format(MRR / test_size))\n","print('End Model Predictions with total time =', t2 - t1)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CuDUFxAzCMOu","executionInfo":{"status":"ok","timestamp":1641013705386,"user_tz":-330,"elapsed":35868,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}},"outputId":"d23b66f7-d48f-43e9-d9c6-33a0d95ae315"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Finished Reading Data \n","Start Model Fitting...\n","End Model Fitting with total time = 0.10341858863830566 \n"," Start Predictions...\n","Finished Prediction for  5000 items.\n","Recall: 0.313485342019544\n","\n","MRR: 0.11998186799961241\n","End Model Predictions with total time = 33.76607871055603\n"]}]},{"cell_type":"markdown","source":["## VSKNN"],"metadata":{"id":"E6JBbdouCWiY"}},{"cell_type":"code","source":["from operator import itemgetter\n","from math import sqrt\n","import time\n","import numpy as np\n","import pandas as pd\n","from math import log10"],"metadata":{"id":"NHpMDGCZCrDF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class VMContextKNN:\n","    '''\n","    VMContextKNN( k, sample_size=1000, similarity='cosine', weighting='div', weighting_score='div_score', session_key = 'SessionId', item_key= 'ItemId')\n","    Parameters\n","    -----------\n","    k : int\n","        Number of neighboring session to calculate the item scores from. (Default value: 200)\n","    sample_size : int\n","        Defines the length of a subset of all training sessions to calculate the nearest neighbors from. (Default value: 2000)\n","    similarity : string\n","        String to define the method for the similarity calculation (jaccard, cosine, binary, tanimoto). (default: cosine)\n","    weighting : string\n","        Decay function to determine the importance/weight of individual actions in the current session (linear, same, div, log, quadratic). (default: div)\n","    weighting_score : string\n","        Decay function to lower the score of candidate items from a neighboring sessions that were selected by less recently clicked items in the current session. (linear, same, div, log, quadratic). (default: div_score)\n","    session_key : string\n","        Header of the session ID column in the input file. (default: 'SessionId')\n","    item_key : string\n","        Header of the item ID column in the input file. (default: 'ItemId')\n","    '''\n","    def __init__( self, k=200, sample_size=0, similarity='cosine', weighting='div', weighting_score='div_score', session_key = 'SessionId', item_key= 'ItemId'):\n","       \n","        self.k = k\n","        self.sample_size = sample_size\n","        self.weighting = weighting\n","        self.weighting_score = weighting_score\n","        self.similarity = similarity\n","        self.session_key = session_key\n","        self.item_key = item_key\n","        \n","        #updated while recommending\n","        self.session = -1\n","        self.session_items = []\n","        self.relevant_sessions = set()\n","\n","        # cache relations once at startup\n","        self.session_item_map = dict() \n","        self.item_session_map = dict()\n","        self.session_time = dict()\n","        self.min_time = -1\n","        \n","        self.sim_time = 0\n","        \n","    def fit(self, train, items=None):\n","        '''\n","        Trains the predictor.\n","        \n","        Parameters\n","        --------\n","        data: pandas.DataFrame\n","            Training data. It contains the transactions of the sessions. It has one column for session IDs, one for item IDs and one for the timestamp of the events (unix timestamps).\n","            It must have a header. Column names are arbitrary, but must correspond to the ones you set during the initialization of the network (session_key, item_key, time_key properties).\n","        '''\n","        self.items_ids = list(train[self.item_key].unique())\n","        train[self.item_key] = train[self.item_key].astype('category')\n","        self.new_old = dict(enumerate(train[self.item_key].cat.categories))\n","        self.old_new = {y:x for x,y in self.new_old.items()}\n","        train[[self.item_key]] = train[[self.item_key]].apply(lambda x: x.cat.codes)\n","        \n","        self.freqs = dict(train[self.item_key].value_counts())\n","        \n","        self.num_items = train[self.item_key].max()\n","        index_session = train.columns.get_loc( self.session_key )\n","        index_item = train.columns.get_loc( self.item_key )\n","        \n","        session = -1\n","        session_items = set()\n","        for row in train.itertuples(index=False):\n","            # cache items of sessions\n","            if row[index_session] != session:\n","                if len(session_items) > 0:\n","                    self.session_item_map.update({session : session_items})\n","                session = row[index_session]\n","                session_items = set()\n","            session_items.add(row[index_item])\n","            \n","            # cache sessions involving an item\n","            map_is = self.item_session_map.get( row[index_item] )\n","            if map_is is None:\n","                map_is = set()\n","                self.item_session_map.update({row[index_item] : map_is})\n","            map_is.add(row[index_session])\n","            \n","        # Add the last tuple    \n","        self.session_item_map.update({session : session_items})\n","        self.predict_for_item_ids = list(range(1, self.num_items+1))\n","        \n","        \n","    def predict_next(self, session_items, k):\n","        '''\n","        Gives predicton scores for a selected set of items on how likely they be the next item in the session.\n","                \n","        Parameters\n","        --------\n","        session_items : List\n","            Items IDs in current session.\n","        k : Integer\n","            How many items to recommend\n","        Returns\n","        --------\n","        out : pandas.Series\n","            Prediction scores for selected items on how likely to be the next item of this session. \n","            Indexed by the item IDs.\n","        '''\n","            \n","        all_len = len(self.predict_for_item_ids)\n","        input_item_id = session_items[-1]\n","        neighbors = self.find_neighbors(input_item_id, session_items)\n","        scores = self.score_items(neighbors, session_items)\n","        \n","        # Create things in the format ..\n","        preds = np.zeros(all_len)\n","        scores_keys = list(scores.keys())\n","        for i in range(all_len):\n","            if i+1 in scores_keys:\n","                preds[i] = scores[i+1]\n","                \n","        series = pd.Series(data = preds, index = self.predict_for_item_ids)\n","        series = series / series.max()\n","        return series.nlargest(k).index.values\n","    \n","    def items_for_session(self, session):\n","        '''\n","        Returns all items in the session\n","        \n","        Parameters\n","        --------\n","        session: Id of a session\n","        \n","        Returns \n","        --------\n","        out : set           \n","        '''\n","        return self.session_item_map.get(session);\n","    \n","    def vec_for_session(self, session):\n","        '''\n","        Returns all items in the session\n","        \n","        Parameters\n","        --------\n","        session: Id of a session\n","        \n","        Returns \n","        --------\n","        out : set           \n","        '''\n","        return self.session_vec_map.get(session);\n","    \n","    def sessions_for_item(self, item_id):\n","        '''\n","        Returns all session for an item\n","        \n","        Parameters\n","        --------\n","        item: Id of the item session\n","        \n","        Returns \n","        --------\n","        out : set           \n","        '''\n","        return self.item_session_map.get( item_id ) if item_id in self.item_session_map else set()\n","        \n","        \n","    def most_recent_sessions( self, sessions, number ):\n","        '''\n","        Find the most recent sessions in the given set\n","        \n","        Parameters\n","        --------\n","        sessions: set of session ids\n","        \n","        Returns \n","        --------\n","        out : set           \n","        '''\n","        sample = set()\n","\n","        tuples = list()\n","        for session in sessions:\n","            time = self.session_time.get( session )\n","            if time is None:\n","                print(' EMPTY TIMESTAMP!! ', session)\n","            tuples.append((session, time))\n","            \n","        tuples = sorted(tuples, key=itemgetter(1), reverse=True)\n","        #print 'sorted list ', sortedList\n","        cnt = 0\n","        for element in tuples:\n","            cnt = cnt + 1\n","            if cnt > number:\n","                break\n","            sample.add( element[0] )\n","        #print 'returning sample of size ', len(sample)\n","        return sample\n","        \n","        \n","    def possible_neighbor_sessions(self, input_item_id):\n","        '''\n","        Find a set of session to later on find neighbors in.\n","        A self.sample_size of 0 uses all sessions in which any item of the current session appears. \n","        \n","        Parameters\n","        --------\n","        sessions: set of session ids\n","        \n","        Returns \n","        --------\n","        out : set           \n","        '''\n","        \n","        self.relevant_sessions = self.relevant_sessions | self.sessions_for_item( input_item_id )\n","               \n","        if self.sample_size == 0: #use all session as possible neighbors\n","            return self.relevant_sessions\n","\n","        else: #sample some sessions\n","            if len(self.relevant_sessions) > self.sample_size:    \n","                return self.relevant_sessions[-self.sample_size:]\n","            else: \n","                return self.relevant_sessions\n","                        \n","    def calc_similarity(self, session_items, sessions):\n","        '''\n","        Calculates the configured similarity for the items in session_items and each session in sessions.\n","        \n","        Parameters\n","        --------\n","        session_items: set of item ids\n","        sessions: list of session ids\n","        \n","        Returns \n","        --------\n","        out : list of tuple (session_id,similarity)           \n","        '''\n","        pos_map = {}\n","        length = len(session_items)\n","        \n","        count = 1\n","        for item in session_items:\n","            if self.weighting is not None: \n","                pos_map[item] = getattr(self, self.weighting)(count, length)\n","                count += 1\n","            else:\n","                pos_map[item] = 1\n","        #print('POS MAP: ', pos_map, session_items)\n","        items = set(session_items)\n","        neighbors = []\n","        for session in sessions: \n","            n_items = self.items_for_session(session)\n","            similarity = self.vec(items, n_items, pos_map)        \n","            if similarity > 0:\n","                neighbors.append((session, similarity))\n","        return neighbors\n","\n","    #-----------------\n","    # Find a set of neighbors, returns a list of tuples (sessionid: similarity) \n","    #-----------------\n","    def find_neighbors( self, input_item_id, session_items):\n","        '''\n","        Finds the k nearest neighbors for the given session_id and the current item input_item_id. \n","        \n","        Parameters\n","        --------\n","        session_items: list of item ids in current session\n","        input_item_id: int\n","        \n","        Returns \n","        --------\n","        out : list of tuple (session_id, similarity)           \n","        '''\n","        #print('SESSION ITEMS1:', session_items)\n","        possible_neighbors = self.possible_neighbor_sessions(input_item_id)\n","        possible_neighbors = self.calc_similarity(session_items, possible_neighbors)\n","        \n","        possible_neighbors = sorted( possible_neighbors, reverse=True, key=lambda x: x[1] )\n","        possible_neighbors = possible_neighbors[:self.k]\n","        \n","        return possible_neighbors\n","    \n","            \n","    def score_items(self, neighbors, current_session):\n","        '''\n","        Compute a set of scores for all items given a set of neighbors.\n","        \n","        Parameters\n","        --------\n","        neighbors: set of session ids\n","        \n","        Returns \n","        --------\n","        out : list of tuple (item, score)           \n","        '''\n","        # now we have the set of relevant items to make predictions\n","        scores = dict()\n","        # iterate over the sessions\n","        for session in neighbors:\n","            # get the items in this session\n","            items = self.items_for_session( session[0] )\n","            step = 1\n","            \n","            for item in reversed( current_session ):\n","                if item in items:\n","                    decay = getattr(self, self.weighting_score)(step)\n","                    break\n","                step += 1\n","                                    \n","            for item in items:\n","                old_score = scores.get( item )\n","                similarity = session[1]\n","                \n","                if old_score is None:\n","                    scores.update({item : ( similarity * decay ) })\n","                else: \n","                    new_score = old_score + ( similarity * decay )\n","                    scores.update({item : new_score})\n","                    \n","        return scores\n","    \n","    \n","    def linear_score(self, i):\n","        return 1 - (0.1*i) if i <= 100 else 0\n","    \n","    def same_score(self, i):\n","        return 1\n","    \n","    def div_score(self, i):\n","        return 1/i\n","    \n","    def log_score(self, i):\n","        return 1/(log10(i+1.7))\n","    \n","    def quadratic_score(self, i):\n","        return 1/(i*i)\n","    \n","    def linear(self, i, length):\n","        return 1 - (0.1*(length-i)) if i <= 10 else 0\n","    \n","    def same(self, i, length):\n","        return 1\n","    \n","    def div(self, i, length):\n","        return i/length\n","    \n","    def log(self, i, length):\n","        return 1/(log10((length-i)+1.7))\n","    \n","    def quadratic(self, i, length):\n","        return (i/length)**2\n","\n","\n","    def jaccard(self, first, second):\n","        '''\n","        Calculates the jaccard index for two sessions\n","        \n","        Parameters\n","        --------\n","        first: Id of a session\n","        second: Id of a session\n","        \n","        Returns \n","        --------\n","        out : float value           \n","        '''\n","        sc = time.clock()\n","        intersection = len(first & second)\n","        union = len(first | second )\n","        res = intersection / union\n","        \n","        self.sim_time += (time.clock() - sc)\n","        \n","        return res \n","    \n","    def cosine(self, first, second):\n","        '''\n","        Calculates the cosine similarity for two sessions\n","        \n","        Parameters\n","        --------\n","        first: Id of a session\n","        second: Id of a session\n","        \n","        Returns \n","        --------\n","        out : float value           \n","        '''\n","        li = len(first&second)\n","        la = len(first)\n","        lb = len(second)\n","        result = li / sqrt(la) * sqrt(lb)\n","\n","        return result\n","    \n","    def tanimoto(self, first, second):\n","        '''\n","        Calculates the cosine tanimoto similarity for two sessions\n","        \n","        Parameters\n","        --------\n","        first: Id of a session\n","        second: Id of a session\n","        \n","        Returns \n","        --------\n","        out : float value           \n","        '''\n","        li = len(first&second)\n","        la = len(first)\n","        lb = len(second)\n","        result = li / ( la + lb -li )\n","\n","        return result\n","    \n","    def binary(self, first, second):\n","        '''\n","        Calculates the ? for 2 sessions\n","        \n","        Parameters\n","        --------\n","        first: Id of a session\n","        second: Id of a session\n","        \n","        Returns \n","        --------\n","        out : float value           \n","        '''\n","        a = len(first&second)\n","        b = len(first)\n","        c = len(second)\n","        \n","        result = (2 * a) / ((2 * a) + b + c)\n","\n","        return result\n","    \n","    def vec(self, first, second, map):\n","        '''\n","        Calculates the ? for 2 sessions\n","        \n","        Parameters\n","        --------\n","        first: Id of a session\n","        second: Id of a session\n","        \n","        Returns \n","        --------\n","        out : float value           \n","        '''\n","        a = first & second\n","        sum = 0\n","        for i in a:\n","            sum += map[i]\n","        \n","        result = sum / len(map)\n","\n","        return result    "],"metadata":{"id":"aTe878vVC2WW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import gc\n","import os\n","import time\n","import argparse\n","import subprocess\n","import numpy as np\n","import pandas as pd"],"metadata":{"id":"1-q4woM5C7fO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["parser = argparse.ArgumentParser()\n","parser.add_argument('--K', type=int, default=20, help=\"K items to be used in Recall@K and MRR@K\")\n","parser.add_argument('--neighbors', type=int, default=200, help=\"K neighbors to be used in KNN\")\n","parser.add_argument('--sample', type=int, default=0, help=\"Max Number of steps to walk back from the currently viewed item\")\n","parser.add_argument('--weight_score', type=str, default='div_score', help=\"Decay function to lower the score of candidate items from a neighboring sessions that were selected by less recently clicked items in the current session. (linear, same, div, log, quadratic)_score\")\n","parser.add_argument('--weighting', type=str, default='div', help=\"Decay function to determine the importance/weight of individual actions in the current session(linear, same, div, log, qudratic)\")\n","parser.add_argument('--similarity', type=str, default='cosine', help=\"String to define the method for the similarity calculation (jaccard, cosine, binary, tanimoto). (default: cosine)\")\n","parser.add_argument('--itemid', default='sid', type=str)\n","parser.add_argument('--sessionid', default='uid', type=str)\n","parser.add_argument('--valid_data', default='yoochoose_valid.txt', type=str)\n","parser.add_argument('--train_data', default='yoochoose_train.txt', type=str)\n","parser.add_argument('--data_folder', default=data_root, type=str)\n","args = parser.parse_args([])\n","\n","# Get the arguments\n","train_data = os.path.join(args.data_folder, args.train_data)\n","x_train = pd.read_csv(train_data)\n","x_train.sort_values(args.sessionid, inplace=True)\n","distinct_train = x_train[args.itemid].nunique()\n","\n","valid_data = os.path.join(args.data_folder, args.valid_data)\n","x_valid = pd.read_csv(valid_data)\n","x_valid.sort_values(args.sessionid, inplace=True)\n","\n","print('Finished Reading Data \\nStart Model Fitting...')\n","# Fitting Model\n","t1 = time.time()\n","model = VMContextKNN(k = args.neighbors, sample_size = args.sample, similarity = args.similarity, \n","\t\t\t\t\t weighting = args.weighting, weighting_score = args.weight_score,\n","\t\t\t\t\t session_key = args.sessionid, item_key = args.itemid)\n","model.fit(x_train)\n","#memory_task.kill()\n","train_time = time.time() - t1\n","print('End Model Fitting\\n Start Predictions...')\n","\n","# Test Set Evaluation\n","test_size = 0.0\n","hit = [0.0]\n","MRR = [0.0]\n","cov = [[]]\n","pop = [[]]\n","Ks = [args.K]\n","cur_length = 0\n","cur_session = -1\n","last_items = []\n","t1 = time.time()\n","index_item = x_valid.columns.get_loc(args.itemid)\n","index_session = x_valid.columns.get_loc(args.sessionid)\n","train_items = model.items_ids\n","counter = 0\n","for row in x_valid.itertuples( index=False ):\n","\tcounter += 1\n","\tif counter % 5000 == 0:\n","\t\tprint('Finished Prediction for ', counter, 'items.')\n","\tsession_id, item_id = row[index_session], row[index_item]\n","\tif session_id != cur_session:\n","\t\tcur_session = session_id\n","\t\tlast_items = []\n","\t\tcur_length = 0\n","\t\n","\tif not item_id in last_items and item_id in train_items:\n","\t\t#print(item_id, item_id in train_items)\n","\t\titem_id = model.old_new[item_id]\n","\t\tif len(last_items) > cur_length: #make prediction\n","\t\t\tcur_length += 1\n","\t\t\ttest_size += 1\n","\t\t\t# Predict the most similar items to items\n","\t\t\tfor k in range(len(Ks)):\n","\t\t\t\tpredictions = model.predict_next(last_items, k = Ks[k])\n","\t\t\t\t# Evaluation\n","\t\t\t\trank = 0\n","\t\t\t\tfor predicted_item in predictions:\n","\t\t\t\t\tif predicted_item not in cov[k]:\n","\t\t\t\t\t\tcov[k].append(predicted_item)\n","\t\t\t\t\tpop[k].append(model.freqs[predicted_item])\n","\t\t\t\t\trank += 1\n","\t\t\t\t\tif predicted_item == item_id:\n","\t\t\t\t\t\thit[k] += 1.0\n","\t\t\t\t\t\tMRR[k] += 1/rank\n","\t\t\t\t\t\tbreak\n","\t\t\n","\t\tlast_items.append(item_id)\n","  \n","#memory_task.kill()\n","hit[:] = [x / test_size for x in hit]\n","MRR[:] = [x / test_size for x in MRR]\n","cov[:] = [len(x) / distinct_train for x in cov]\n","maxi = max(model.freqs.values())\n","pop[:] = [np.mean(x) / maxi for x in pop]\n","test_time = (time.time() - t1)\n","print('Recall:', hit)\n","print ('\\nMRR:', MRR)\n","print ('\\nCoverage:', cov)\n","print ('\\nPopularity:', pop)\n","print ('\\ntrain_time:', train_time)\n","print ('\\ntest_time:', test_time)\n","print('End Model Predictions')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RX4xhYdbC-He","executionInfo":{"status":"ok","timestamp":1641017117647,"user_tz":-330,"elapsed":2828265,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}},"outputId":"a7647ecf-ba8b-4fd5-a1f1-012d3b3f1be6"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Finished Reading Data \n","Start Model Fitting...\n","End Model Fitting\n"," Start Predictions...\n","Finished Prediction for  5000 items.\n","Recall: [0.5305537459283388]\n","\n","MRR: [0.1676372920865109]\n","\n","Coverage: [0.2715548471236053]\n","\n","Popularity: [0.06588592053880978]\n","\n","train_time: 4.415358543395996\n","\n","test_time: 2822.0330970287323\n","End Model Predictions\n"]}]},{"cell_type":"code","source":["!pip install -q watermark\n","%reload_ext watermark\n","%watermark -a \"Sparsh A.\" -m -iv -u -t -d"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"V9iB90SjDKCe","executionInfo":{"status":"ok","timestamp":1641017530493,"user_tz":-330,"elapsed":3722,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}},"outputId":"ac58a675-a8eb-41dc-9a68-9542c70347a7"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Author: Sparsh A.\n","\n","Last updated: 2022-01-01 06:12:09\n","\n","Compiler    : GCC 7.5.0\n","OS          : Linux\n","Release     : 5.4.144+\n","Machine     : x86_64\n","Processor   : x86_64\n","CPU cores   : 2\n","Architecture: 64bit\n","\n","pandas  : 1.1.5\n","numpy   : 1.19.5\n","argparse: 1.1\n","IPython : 5.5.0\n","\n"]}]}]}