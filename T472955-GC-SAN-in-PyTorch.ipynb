{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"T472955 | GC-SAN in PyTorch","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyOSXyTj8YgA6fcLk5j+ogb6"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"KEZ7VF0_qJDP"},"source":["# GC-SAN in PyTorch"]},{"cell_type":"markdown","metadata":{"id":"WfEZ_9zJSTky"},"source":["## Imports"]},{"cell_type":"code","metadata":{"id":"pyLgtYzWgk0H"},"source":["import math\n","\n","import numpy as np\n","import torch\n","from torch import nn\n","from torch.nn import Parameter\n","from torch.nn import functional as F\n","\n","from enum import Enum"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"w8ZpJv0eSV05"},"source":["## Model"]},{"cell_type":"code","metadata":{"id":"uYZROGBRg3o2"},"source":["class SequentialRecommender(AbstractRecommender):\n","    \"\"\"\n","    This is a abstract sequential recommender. All the sequential model should implement This class.\n","    \"\"\"\n","    def __init__(self, config, dataset):\n","        super(SequentialRecommender, self).__init__()\n","\n","        # load dataset info\n","        self.USER_ID = config['USER_ID_FIELD']\n","        self.ITEM_ID = config['ITEM_ID_FIELD']\n","        self.ITEM_SEQ = self.ITEM_ID + config['LIST_SUFFIX']\n","        self.ITEM_SEQ_LEN = config['ITEM_LIST_LENGTH_FIELD']\n","        self.POS_ITEM_ID = self.ITEM_ID\n","        self.NEG_ITEM_ID = config['NEG_PREFIX'] + self.ITEM_ID\n","        self.max_seq_length = config['MAX_ITEM_LIST_LENGTH']\n","        self.n_items = dataset.num(self.ITEM_ID)\n","\n","    def gather_indexes(self, output, gather_index):\n","        \"\"\"Gathers the vectors at the specific positions over a minibatch\"\"\"\n","        gather_index = gather_index.view(-1, 1, 1).expand(-1, -1, output.shape[-1])\n","        output_tensor = output.gather(dim=1, index=gather_index)\n","        return output_tensor.squeeze(1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6dksftU-h_Wc"},"source":["class MultiHeadAttention(nn.Module):\n","    \"\"\"\n","    Multi-head Self-attention layers, a attention score dropout layer is introduced.\n","    Args:\n","        input_tensor (torch.Tensor): the input of the multi-head self-attention layer\n","        attention_mask (torch.Tensor): the attention mask for input tensor\n","    Returns:\n","        hidden_states (torch.Tensor): the output of the multi-head self-attention layer\n","    \"\"\"\n","\n","    def __init__(self, n_heads, hidden_size, hidden_dropout_prob, attn_dropout_prob, layer_norm_eps):\n","        super(MultiHeadAttention, self).__init__()\n","        if hidden_size % n_heads != 0:\n","            raise ValueError(\n","                \"The hidden size (%d) is not a multiple of the number of attention \"\n","                \"heads (%d)\" % (hidden_size, n_heads)\n","            )\n","\n","        self.num_attention_heads = n_heads\n","        self.attention_head_size = int(hidden_size / n_heads)\n","        self.all_head_size = self.num_attention_heads * self.attention_head_size\n","\n","        self.query = nn.Linear(hidden_size, self.all_head_size)\n","        self.key = nn.Linear(hidden_size, self.all_head_size)\n","        self.value = nn.Linear(hidden_size, self.all_head_size)\n","\n","        self.attn_dropout = nn.Dropout(attn_dropout_prob)\n","\n","        self.dense = nn.Linear(hidden_size, hidden_size)\n","        self.LayerNorm = nn.LayerNorm(hidden_size, eps=layer_norm_eps)\n","        self.out_dropout = nn.Dropout(hidden_dropout_prob)\n","\n","    def transpose_for_scores(self, x):\n","        new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)\n","        x = x.view(*new_x_shape)\n","        return x.permute(0, 2, 1, 3)\n","\n","    def forward(self, input_tensor, attention_mask):\n","        mixed_query_layer = self.query(input_tensor)\n","        mixed_key_layer = self.key(input_tensor)\n","        mixed_value_layer = self.value(input_tensor)\n","\n","        query_layer = self.transpose_for_scores(mixed_query_layer)\n","        key_layer = self.transpose_for_scores(mixed_key_layer)\n","        value_layer = self.transpose_for_scores(mixed_value_layer)\n","\n","        # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n","        attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n","\n","        attention_scores = attention_scores / math.sqrt(self.attention_head_size)\n","        # Apply the attention mask is (precomputed for all layers in BertModel forward() function)\n","        # [batch_size heads seq_len seq_len] scores\n","        # [batch_size 1 1 seq_len]\n","        attention_scores = attention_scores + attention_mask\n","\n","        # Normalize the attention scores to probabilities.\n","        attention_probs = nn.Softmax(dim=-1)(attention_scores)\n","        # This is actually dropping out entire tokens to attend to, which might\n","        # seem a bit unusual, but is taken from the original Transformer paper.\n","\n","        attention_probs = self.attn_dropout(attention_probs)\n","        context_layer = torch.matmul(attention_probs, value_layer)\n","        context_layer = context_layer.permute(0, 2, 1, 3).contiguous()\n","        new_context_layer_shape = context_layer.size()[:-2] + (self.all_head_size,)\n","        context_layer = context_layer.view(*new_context_layer_shape)\n","        hidden_states = self.dense(context_layer)\n","        hidden_states = self.out_dropout(hidden_states)\n","        hidden_states = self.LayerNorm(hidden_states + input_tensor)\n","\n","        return hidden_states"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4N4FVR-hh4zh"},"source":["class FeedForward(nn.Module):\n","    \"\"\"\n","    Point-wise feed-forward layer is implemented by two dense layers.\n","    Args:\n","        input_tensor (torch.Tensor): the input of the point-wise feed-forward layer\n","    Returns:\n","        hidden_states (torch.Tensor): the output of the point-wise feed-forward layer\n","    \"\"\"\n","\n","    def __init__(self, hidden_size, inner_size, hidden_dropout_prob, hidden_act, layer_norm_eps):\n","        super(FeedForward, self).__init__()\n","        self.dense_1 = nn.Linear(hidden_size, inner_size)\n","        self.intermediate_act_fn = self.get_hidden_act(hidden_act)\n","\n","        self.dense_2 = nn.Linear(inner_size, hidden_size)\n","        self.LayerNorm = nn.LayerNorm(hidden_size, eps=layer_norm_eps)\n","        self.dropout = nn.Dropout(hidden_dropout_prob)\n","\n","    def get_hidden_act(self, act):\n","        ACT2FN = {\n","            \"gelu\": self.gelu,\n","            \"relu\": fn.relu,\n","            \"swish\": self.swish,\n","            \"tanh\": torch.tanh,\n","            \"sigmoid\": torch.sigmoid,\n","        }\n","        return ACT2FN[act]\n","\n","    def gelu(self, x):\n","        \"\"\"Implementation of the gelu activation function.\n","        For information: OpenAI GPT's gelu is slightly different (and gives slightly different results)::\n","            0.5 * x * (1 + torch.tanh(math.sqrt(2 / math.pi) * (x + 0.044715 * torch.pow(x, 3))))\n","        Also see https://arxiv.org/abs/1606.08415\n","        \"\"\"\n","        return x * 0.5 * (1.0 + torch.erf(x / math.sqrt(2.0)))\n","\n","    def swish(self, x):\n","        return x * torch.sigmoid(x)\n","\n","    def forward(self, input_tensor):\n","        hidden_states = self.dense_1(input_tensor)\n","        hidden_states = self.intermediate_act_fn(hidden_states)\n","\n","        hidden_states = self.dense_2(hidden_states)\n","        hidden_states = self.dropout(hidden_states)\n","        hidden_states = self.LayerNorm(hidden_states + input_tensor)\n","\n","        return hidden_states"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-RQP1Pp6hxCS"},"source":["class TransformerLayer(nn.Module):\n","    \"\"\"\n","    One transformer layer consists of a multi-head self-attention layer and a point-wise feed-forward layer.\n","    Args:\n","        hidden_states (torch.Tensor): the input of the multi-head self-attention sublayer\n","        attention_mask (torch.Tensor): the attention mask for the multi-head self-attention sublayer\n","    Returns:\n","        feedforward_output (torch.Tensor): The output of the point-wise feed-forward sublayer,\n","                                           is the output of the transformer layer.\n","    \"\"\"\n","\n","    def __init__(\n","        self, n_heads, hidden_size, intermediate_size, hidden_dropout_prob, attn_dropout_prob, hidden_act,\n","        layer_norm_eps\n","    ):\n","        super(TransformerLayer, self).__init__()\n","        self.multi_head_attention = MultiHeadAttention(\n","            n_heads, hidden_size, hidden_dropout_prob, attn_dropout_prob, layer_norm_eps\n","        )\n","        self.feed_forward = FeedForward(hidden_size, intermediate_size, hidden_dropout_prob, hidden_act, layer_norm_eps)\n","\n","    def forward(self, hidden_states, attention_mask):\n","        attention_output = self.multi_head_attention(hidden_states, attention_mask)\n","        feedforward_output = self.feed_forward(attention_output)\n","        return feedforward_output"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"iTQZ2HkShrZw"},"source":["class TransformerEncoder(nn.Module):\n","    r\"\"\" One TransformerEncoder consists of several TransformerLayers.\n","        - n_layers(num): num of transformer layers in transformer encoder. Default: 2\n","        - n_heads(num): num of attention heads for multi-head attention layer. Default: 2\n","        - hidden_size(num): the input and output hidden size. Default: 64\n","        - inner_size(num): the dimensionality in feed-forward layer. Default: 256\n","        - hidden_dropout_prob(float): probability of an element to be zeroed. Default: 0.5\n","        - attn_dropout_prob(float): probability of an attention score to be zeroed. Default: 0.5\n","        - hidden_act(str): activation function in feed-forward layer. Default: 'gelu'\n","                      candidates: 'gelu', 'relu', 'swish', 'tanh', 'sigmoid'\n","        - layer_norm_eps(float): a value added to the denominator for numerical stability. Default: 1e-12\n","    \"\"\"\n","\n","    def __init__(\n","        self,\n","        n_layers=2,\n","        n_heads=2,\n","        hidden_size=64,\n","        inner_size=256,\n","        hidden_dropout_prob=0.5,\n","        attn_dropout_prob=0.5,\n","        hidden_act='gelu',\n","        layer_norm_eps=1e-12\n","    ):\n","\n","        super(TransformerEncoder, self).__init__()\n","        layer = TransformerLayer(\n","            n_heads, hidden_size, inner_size, hidden_dropout_prob, attn_dropout_prob, hidden_act, layer_norm_eps\n","        )\n","        self.layer = nn.ModuleList([copy.deepcopy(layer) for _ in range(n_layers)])\n","\n","    def forward(self, hidden_states, attention_mask, output_all_encoded_layers=True):\n","        \"\"\"\n","        Args:\n","            hidden_states (torch.Tensor): the input of the TransformerEncoder\n","            attention_mask (torch.Tensor): the attention mask for the input hidden_states\n","            output_all_encoded_layers (Bool): whether output all transformer layers' output\n","        Returns:\n","            all_encoder_layers (list): if output_all_encoded_layers is True, return a list consists of all transformer\n","            layers' output, otherwise return a list only consists of the output of last transformer layer.\n","        \"\"\"\n","        all_encoder_layers = []\n","        for layer_module in self.layer:\n","            hidden_states = layer_module(hidden_states, attention_mask)\n","            if output_all_encoded_layers:\n","                all_encoder_layers.append(hidden_states)\n","        if not output_all_encoded_layers:\n","            all_encoder_layers.append(hidden_states)\n","        return all_encoder_layers"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"86jlA1NliKMP"},"source":["class BPRLoss(nn.Module):\n","    \"\"\" BPRLoss, based on Bayesian Personalized Ranking\n","    Args:\n","        - gamma(float): Small value to avoid division by zero\n","    Shape:\n","        - Pos_score: (N)\n","        - Neg_score: (N), same shape as the Pos_score\n","        - Output: scalar.\n","    Examples::\n","        >>> loss = BPRLoss()\n","        >>> pos_score = torch.randn(3, requires_grad=True)\n","        >>> neg_score = torch.randn(3, requires_grad=True)\n","        >>> output = loss(pos_score, neg_score)\n","        >>> output.backward()\n","    \"\"\"\n","\n","    def __init__(self, gamma=1e-10):\n","        super(BPRLoss, self).__init__()\n","        self.gamma = gamma\n","\n","    def forward(self, pos_score, neg_score):\n","        loss = -torch.log(self.gamma + torch.sigmoid(pos_score - neg_score)).mean()\n","        return loss\n","\n","\n","class EmbLoss(nn.Module):\n","    \"\"\" EmbLoss, regularization on embeddings\n","    \"\"\"\n","\n","    def __init__(self, norm=2):\n","        super(EmbLoss, self).__init__()\n","        self.norm = norm\n","\n","    def forward(self, *embeddings):\n","        emb_loss = torch.zeros(1).to(embeddings[-1].device)\n","        for embedding in embeddings:\n","            emb_loss += torch.norm(embedding, p=self.norm)\n","        emb_loss /= embeddings[-1].shape[0]\n","        return emb_loss"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Z7_uHNd7iQGd"},"source":["class GNN(nn.Module):\n","    r\"\"\"Graph neural networks are well-suited for session-based recommendation,\n","    because it can automatically extract features of session graphs with considerations of rich node connections.\n","    \"\"\"\n","\n","    def __init__(self, embedding_size, step=1):\n","        super(GNN, self).__init__()\n","        self.step = step\n","        self.embedding_size = embedding_size\n","        self.input_size = embedding_size * 2\n","        self.gate_size = embedding_size * 3\n","        self.w_ih = Parameter(torch.Tensor(self.gate_size, self.input_size))\n","        self.w_hh = Parameter(torch.Tensor(self.gate_size, self.embedding_size))\n","        self.b_ih = Parameter(torch.Tensor(self.gate_size))\n","        self.b_hh = Parameter(torch.Tensor(self.gate_size))\n","\n","        self.linear_edge_in = nn.Linear(self.embedding_size, self.embedding_size, bias=True)\n","        self.linear_edge_out = nn.Linear(self.embedding_size, self.embedding_size, bias=True)\n","\n","        # parameters initialization\n","        self._reset_parameters()\n","\n","    def _reset_parameters(self):\n","        stdv = 1.0 / math.sqrt(self.embedding_size)\n","        for weight in self.parameters():\n","            weight.data.uniform_(-stdv, stdv)\n","\n","    def GNNCell(self, A, hidden):\n","        r\"\"\"Obtain latent vectors of nodes via gated graph neural network.\n","\n","        Args:\n","            A (torch.FloatTensor): The connection matrix,shape of [batch_size, max_session_len, 2 * max_session_len]\n","\n","            hidden (torch.FloatTensor): The item node embedding matrix, shape of\n","                [batch_size, max_session_len, embedding_size]\n","\n","        Returns:\n","            torch.FloatTensor: Latent vectors of nodes,shape of [batch_size, max_session_len, embedding_size]\n","\n","        \"\"\"\n","\n","        input_in = torch.matmul(A[:, :, :A.size(1)], self.linear_edge_in(hidden))\n","        input_out = torch.matmul(A[:, :, A.size(1):2 * A.size(1)], self.linear_edge_out(hidden))\n","        # [batch_size, max_session_len, embedding_size * 2]\n","        inputs = torch.cat([input_in, input_out], 2)\n","\n","        # gi.size equals to gh.size, shape of [batch_size, max_session_len, embedding_size * 3]\n","        gi = F.linear(inputs, self.w_ih, self.b_ih)\n","        gh = F.linear(hidden, self.w_hh, self.b_hh)\n","        # (batch_size, max_session_len, embedding_size)\n","        i_r, i_i, i_n = gi.chunk(3, 2)\n","        h_r, h_i, h_n = gh.chunk(3, 2)\n","        reset_gate = torch.sigmoid(i_r + h_r)\n","        input_gate = torch.sigmoid(i_i + h_i)\n","        new_gate = torch.tanh(i_n + reset_gate * h_n)\n","        hy = (1 - input_gate) * hidden + input_gate * new_gate\n","        return hy\n","\n","    def forward(self, A, hidden):\n","        for i in range(self.step):\n","            hidden = self.GNNCell(A, hidden)\n","        return hidden"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"BHDoJAl7iVZc"},"source":["class GCSAN(SequentialRecommender):\n","    r\"\"\"GCSAN captures rich local dependencies via graph neural network,\n","     and learns long-range dependencies by applying the self-attention mechanism.\n","     \n","    Note:\n","\n","        In the original paper, the attention mechanism in the self-attention layer is a single head,\n","        for the reusability of the project code, we use a unified transformer component.\n","        According to the experimental results, we only applied regularization to embedding.\n","    \"\"\"\n","\n","    def __init__(self, config, dataset):\n","        super(GCSAN, self).__init__(config, dataset)\n","\n","        # load parameters info\n","        self.n_layers = config['n_layers']\n","        self.n_heads = config['n_heads']\n","        self.hidden_size = config['hidden_size']  # same as embedding_size\n","        self.inner_size = config['inner_size']  # the dimensionality in feed-forward layer\n","        self.hidden_dropout_prob = config['hidden_dropout_prob']\n","        self.attn_dropout_prob = config['attn_dropout_prob']\n","        self.hidden_act = config['hidden_act']\n","        self.layer_norm_eps = config['layer_norm_eps']\n","\n","        self.step = config['step']\n","        self.device = config['device']\n","        self.weight = config['weight']\n","        self.reg_weight = config['reg_weight']\n","        self.loss_type = config['loss_type']\n","        self.initializer_range = config['initializer_range']\n","\n","        # define layers and loss\n","        self.item_embedding = nn.Embedding(self.n_items, self.hidden_size, padding_idx=0)\n","        self.gnn = GNN(self.hidden_size, self.step)\n","        self.self_attention = TransformerEncoder(\n","            n_layers=self.n_layers,\n","            n_heads=self.n_heads,\n","            hidden_size=self.hidden_size,\n","            inner_size=self.inner_size,\n","            hidden_dropout_prob=self.hidden_dropout_prob,\n","            attn_dropout_prob=self.attn_dropout_prob,\n","            hidden_act=self.hidden_act,\n","            layer_norm_eps=self.layer_norm_eps\n","        )\n","        self.reg_loss = EmbLoss()\n","        if self.loss_type == 'BPR':\n","            self.loss_fct = BPRLoss()\n","        elif self.loss_type == 'CE':\n","            self.loss_fct = nn.CrossEntropyLoss()\n","        else:\n","            raise NotImplementedError(\"Make sure 'loss_type' in ['BPR', 'CE']!\")\n","\n","        # parameters initialization\n","        self.apply(self._init_weights)\n","\n","    def _init_weights(self, module):\n","        \"\"\" Initialize the weights \"\"\"\n","        if isinstance(module, (nn.Linear, nn.Embedding)):\n","            # Slightly different from the TF version which uses truncated_normal for initialization\n","            # cf https://github.com/pytorch/pytorch/pull/5617\n","            module.weight.data.normal_(mean=0.0, std=self.initializer_range)\n","        elif isinstance(module, nn.LayerNorm):\n","            module.bias.data.zero_()\n","            module.weight.data.fill_(1.0)\n","        if isinstance(module, nn.Linear) and module.bias is not None:\n","            module.bias.data.zero_()\n","\n","    def get_attention_mask(self, item_seq):\n","        \"\"\"Generate left-to-right uni-directional attention mask for multi-head attention.\"\"\"\n","        attention_mask = (item_seq > 0).long()\n","        extended_attention_mask = attention_mask.unsqueeze(1).unsqueeze(2)  # torch.int64\n","        # mask for left-to-right unidirectional\n","        max_len = attention_mask.size(-1)\n","        attn_shape = (1, max_len, max_len)\n","        subsequent_mask = torch.triu(torch.ones(attn_shape), diagonal=1)  # torch.uint8\n","        subsequent_mask = (subsequent_mask == 0).unsqueeze(1)\n","        subsequent_mask = subsequent_mask.long().to(item_seq.device)\n","\n","        extended_attention_mask = extended_attention_mask * subsequent_mask\n","        extended_attention_mask = extended_attention_mask.to(dtype=next(self.parameters()).dtype)  # fp16 compatibility\n","        extended_attention_mask = (1.0 - extended_attention_mask) * -10000.0\n","        return extended_attention_mask\n","\n","\n","    def _get_slice(self, item_seq):\n","        items, n_node, A, alias_inputs = [], [], [], []\n","        max_n_node = item_seq.size(1)\n","        item_seq = item_seq.cpu().numpy()\n","\n","        for u_input in item_seq:\n","            node = np.unique(u_input)\n","            items.append(node.tolist() + (max_n_node - len(node)) * [0])\n","            u_A = np.zeros((max_n_node, max_n_node))\n","            for i in np.arange(len(u_input) - 1):\n","                if u_input[i + 1] == 0:\n","                    break\n","                u = np.where(node == u_input[i])[0][0]\n","                v = np.where(node == u_input[i + 1])[0][0]\n","                u_A[u][v] = 1\n","            u_sum_in = np.sum(u_A, 0)\n","            u_sum_in[np.where(u_sum_in == 0)] = 1\n","            u_A_in = np.divide(u_A, u_sum_in)\n","            u_sum_out = np.sum(u_A, 1)\n","            u_sum_out[np.where(u_sum_out == 0)] = 1\n","            u_A_out = np.divide(u_A.transpose(), u_sum_out)\n","            u_A = np.concatenate([u_A_in, u_A_out]).transpose()\n","            A.append(u_A)\n","\n","            alias_inputs.append([np.where(node == i)[0][0] for i in u_input])\n","        # The relative coordinates of the item node, shape of [batch_size, max_session_len]\n","        alias_inputs = torch.LongTensor(alias_inputs).to(self.device)\n","        # The connecting matrix, shape of [batch_size, max_session_len, 2 * max_session_len]\n","        A = torch.FloatTensor(A).to(self.device)\n","        # The unique item nodes, shape of [batch_size, max_session_len]\n","        items = torch.LongTensor(items).to(self.device)\n","\n","        return alias_inputs, A, items\n","\n","    def forward(self, item_seq, item_seq_len):\n","        assert 0 <= self.weight <= 1\n","        alias_inputs, A, items = self._get_slice(item_seq)\n","        hidden = self.item_embedding(items)\n","        hidden = self.gnn(A, hidden)\n","        alias_inputs = alias_inputs.view(-1, alias_inputs.size(1), 1).expand(-1, -1, self.hidden_size)\n","        seq_hidden = torch.gather(hidden, dim=1, index=alias_inputs)\n","        # fetch the last hidden state of last timestamp\n","        ht = self.gather_indexes(seq_hidden, item_seq_len - 1)\n","        a = seq_hidden\n","        attention_mask = self.get_attention_mask(item_seq)\n","\n","        outputs = self.self_attention(a, attention_mask, output_all_encoded_layers=True)\n","        output = outputs[-1]\n","        at = self.gather_indexes(output, item_seq_len - 1)\n","        seq_output = self.weight * at + (1 - self.weight) * ht\n","        return seq_output\n","\n","\n","    def calculate_loss(self, interaction):\n","        item_seq = interaction[self.ITEM_SEQ]\n","        item_seq_len = interaction[self.ITEM_SEQ_LEN]\n","        seq_output = self.forward(item_seq, item_seq_len)\n","        pos_items = interaction[self.POS_ITEM_ID]\n","        if self.loss_type == 'BPR':\n","            neg_items = interaction[self.NEG_ITEM_ID]\n","            pos_items_emb = self.item_embedding(pos_items)\n","            neg_items_emb = self.item_embedding(neg_items)\n","            pos_score = torch.sum(seq_output * pos_items_emb, dim=-1)  # [B]\n","            neg_score = torch.sum(seq_output * neg_items_emb, dim=-1)  # [B]\n","            loss = self.loss_fct(pos_score, neg_score)\n","        else:  # self.loss_type = 'CE'\n","            test_item_emb = self.item_embedding.weight\n","            logits = torch.matmul(seq_output, test_item_emb.transpose(0, 1))\n","            loss = self.loss_fct(logits, pos_items)\n","\n","        reg_loss = self.reg_loss(self.item_embedding.weight)\n","        total_loss = loss + self.reg_weight * reg_loss\n","        return total_loss\n","\n","\n","    def predict(self, interaction):\n","        item_seq = interaction[self.ITEM_SEQ]\n","        item_seq_len = interaction[self.ITEM_SEQ_LEN]\n","        test_item = interaction[self.ITEM_ID]\n","        seq_output = self.forward(item_seq, item_seq_len)\n","        test_item_emb = self.item_embedding(test_item)\n","        scores = torch.mul(seq_output, test_item_emb).sum(dim=1)  # [B]\n","        return scores\n","\n","\n","    def full_sort_predict(self, interaction):\n","        item_seq = interaction[self.ITEM_SEQ]\n","        item_seq_len = interaction[self.ITEM_SEQ_LEN]\n","        seq_output = self.forward(item_seq, item_seq_len)\n","        test_items_emb = self.item_embedding.weight\n","        scores = torch.matmul(seq_output, test_items_emb.transpose(0, 1))  # [B, n_items]\n","        return scores"],"execution_count":null,"outputs":[]}]}