{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"2022-01-17-movierec.ipynb","provenance":[{"file_id":"https://github.com/recohut/nbs/blob/main/raw/RecSys_01Mar21_RecSys%20Movie_recommender_system_CF_v2_TOFILL.ipynb","timestamp":1644646051443},{"file_id":"https://github.com/couturierc/tutorials/blob/master/recommender_system/Movie_recommender_system_CF_v2_TOFILL.ipynb","timestamp":1617530488634}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"hZzLBRl0iNfE"},"source":["# Movie Recommender System"]},{"cell_type":"markdown","source":["## Collaborative filtering (matrix factorization)\n","\n","You are an online retailer/travel agent/movie review website, and you would like to help the visitors of your website to explore more of your products/destinations/movies. You got data which either describe the different products/destinations/films, or past transactions/trips/views (or preferences) of your visitors (or both!). You decide to leverage that data to provide relevant and meaningful recommendations.\n","\n","This notebook implements a simple collaborative system using  factorization of the user-item matrix."],"metadata":{"id":"g0mwPHs1itx9"}},{"cell_type":"code","metadata":{"id":"J8u8ZXvkhkfY"},"source":["import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","%matplotlib inline"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"CPVCIslerd5e"},"source":["ratings=\"https://github.com/couturierc/tutorials/raw/master/recommender_system/data/ratings.csv\"\n","movies=\"https://github.com/couturierc/tutorials/raw/master/recommender_system/data/movies.csv\"\n","\n","# If data stored locally\n","# ratings=\"./data/ratings.csv\"\n","# movies=\"./data/movies.csv\"\n","\n","df_ratings = pd.read_csv(ratings, sep=',')\n","df_ratings.columns = ['userId', 'itemId', 'rating', 'timestamp']\n","df_movies = pd.read_csv(movies, sep=',')\n","df_movies.columns = ['itemId', 'title', 'genres']"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"IvyAYay5rzcS"},"source":["df_movies.head()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"R4K08KX3sYhr"},"source":["df_ratings.head()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2hUKyFxYdsT5"},"source":["## Quick exploration\n","\n","Hints: use df.describe(), df.column_name.hist(), scatterplot matrix (sns.pairplot(df[column_range])), correlation matrix (sns.heatmap(df.corr()) ), check duplicates, ..."]},{"cell_type":"code","metadata":{"id":"LVqBtDNmJ5vL"},"source":["# Start your exploration -- use as many cells as you need !\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MffuKcE5s8fQ"},"source":["## Obtain the user-item matrice by pivoting df_ratings"]},{"cell_type":"code","metadata":{"id":"qOt3GI3zs2Ts"},"source":["##### FILL HERE (1 line) ######\n","df_user_item = NULL # Use df.pivot, rows ~ userId's, columns ~ itemId's\n","################################\n","\n","# Sort index/rows (userId's) and columns (itemId's)\n","df_user_item.sort_index(axis=0, inplace=True)\n","df_user_item.sort_index(axis=1, inplace=True)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"90Q7L3SQtc1t"},"source":["This matrix has **many** missing values:"]},{"cell_type":"code","metadata":{"id":"P6tkf_s3tgsL"},"source":["df_user_item.head()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"J0EfDXLIRWaG"},"source":["df_user_item.describe()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HXanXrqI4xJ4"},"source":["For instance, rating for userId=1 for movies with itemId 1 to 10:"]},{"cell_type":"code","metadata":{"id":"QLI0gnwT4obE"},"source":["df_user_item.loc[1][:10]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3SM4RU3njy2K"},"source":["# df_user_item.loc[1].dropna().sort_values(ascending=False)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-dtJPkm1knNC"},"source":["Save the movie ids for user 1 for later:"]},{"cell_type":"code","metadata":{"id":"C05fKcNrkmYv"},"source":["item_rated_user_1 = df_user_item.loc[1].dropna().index\n","item_rated_user_1"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"oR-pEwd5thyy"},"source":["We want to find the matrix of rank $k$ which is closest to the original matrix.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"gAUU_b5ma5bA"},"source":["## What not to do: Fill with 0's or mean values, then Singular Value Decomposition (SVD)"]},{"cell_type":"markdown","metadata":{"id":"5ixiAfGIH6VU"},"source":["(Adapted from https://github.com/beckernick/matrix_factorization_recommenders/blob/master/matrix_factorization_recommender.ipynb)\n","\n","Singular Value Decomposition decomposes a matrix $R$ into the best lower rank (i.e. smaller/simpler) approximation of the original matrix $R$. Mathematically, it decomposes R into a two unitary matrices and a diagonal matrix:\n","\n","$$\\begin{equation}\n","R = U\\Sigma V^{T}\n","\\end{equation}$$\n","\n","where: \n","- R is users's ratings matrix, \n","- $U$ is the user \"features\" matrix, it represents how much users \"like\" each feature,\n","- $\\Sigma$ is the diagonal matrix of singular values (essentially weights), \n","- $V^{T}$ is the movie \"features\" matrix, it represents how relevant each feature is to each movie,\n","\n","with $U$ and $V^{T}$ orthogonal."]},{"cell_type":"code","metadata":{"id":"MMVe_feVQQK_"},"source":["df_user_item = df_user_item.fillna(0)\n","df_user_item.head()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Pz16Rlw4tlom"},"source":["R = df_user_item.values"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_R9inUPkH1Hm"},"source":["R"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gypFSYCYHg63"},"source":["Apply SVD to R (e.g. using NumPy or SciPy)"]},{"cell_type":"code","metadata":{"id":"XGSFlWxLHYVE"},"source":["from scipy.sparse.linalg import svds\n","U, sigma, Vt = svds(R, k = 50)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"slRJZ23uIVLt"},"source":["What do $U$, $\\Sigma$, $V^T$ look like?"]},{"cell_type":"code","metadata":{"id":"jfifORX6IIga"},"source":["U"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"nXkKnGWcISzH"},"source":["sigma"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"v0H56AlQIUTM"},"source":["Vt"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"baQzWyVHKQVN"},"source":["Get recommendations:"]},{"cell_type":"code","metadata":{"id":"CyzbchyIKnkW"},"source":["# First make sigma a diagonal matrix:\n","sigma = np.diag(sigma)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"uouELHsfKtOU"},"source":["R_after_svd = np.dot(np.dot(U, sigma), Vt)\n","R_after_svd"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mFID_6eWKskb"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"z6NRarPjJ0DI"},"source":["Drawbacks of this approach: \n","- the missing values (here filled with 0's) is feedback that the user did not give, we should not cannot consider it negative/null rating.\n","- the dense matrix is huge, applying SVD is not scalable."]},{"cell_type":"markdown","metadata":{"id":"Keb06kCFbIPl"},"source":["## Approximate SVD with stochastic gradient descend (SGD)\n","\n","\n","This time, we do **not** fill missing values. \n","\n","We inject $\\Sigma$ into U and V, and try to find P and q such that $\\widehat{R} = P Q^{T}$ is close to  $R$ **for the item-user pairs already rated**.\n"]},{"cell_type":"markdown","metadata":{"id":"tkr8jfzbVS_R"},"source":["A first function to simplify the entries (userId/itemId) : we map the set of "]},{"cell_type":"code","metadata":{"id":"F_HgEkPAQSTG"},"source":["def encode_ids(data):\n","    '''Takes a rating dataframe and return: \n","    - a simplified rating dataframe with ids in range(nb unique id) for users and movies\n","    - 2 mapping disctionaries\n","    \n","    '''\n","\n","    data_encoded = data.copy()\n","    \n","    users = pd.DataFrame(data_encoded.userId.unique(),columns=['userId'])  # df of all unique users\n","    dict_users = users.to_dict()    \n","    inv_dict_users = {v: k for k, v in dict_users['userId'].items()}\n","\n","    items = pd.DataFrame(data_encoded.itemId.unique(),columns=['itemId']) # df of all unique items\n","    dict_items = items.to_dict()    \n","    inv_dict_items = {v: k for k, v in dict_items['itemId'].items()}\n","\n","    data_encoded.userId = data_encoded.userId.map(inv_dict_users)\n","    data_encoded.itemId = data_encoded.itemId.map(inv_dict_items)\n","\n","    return data_encoded, dict_users, dict_items\n","  "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Yt6SYVvAX3Di"},"source":["Here is the procedure we would like to implement in the function SGD():\n","\n","1.   itinialize P and Q to random values\n","\n","2.   for $n_{epochs}$ passes on the data:\n","\n","    *   for all known ratings $r_{ui}$\n","        *   compute the error between the predicted rating $p_u \\cdot q_i$ and the known ratings $r_{ui}$:\n","        $$ err = r_{ui} - p_u \\cdot q_i $$\n","        *   update $p_u$ and $q_i$ with the following rule:\n","        $$ p_u \\leftarrow p_u + \\alpha \\cdot err \\cdot q_i  $$\n","        $$ q_i \\leftarrow q_i + \\alpha \\cdot err \\cdot p_u$$\n","\n","\n","\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"iA0tyBHJ5xyI"},"source":["# Adapted from http://nicolas-hug.com/blog/matrix_facto_4\n","def SGD(data,           # dataframe containing 1 user|item|rating per row\n","        n_factors = 10, # number of factors\n","        alpha = .01,    # number of factors\n","        n_epochs = 3,   # number of iteration of the SGD procedure\n","       ):\n","    '''Learn the vectors P and Q (ie all the weights p_u and q_i) with SGD.\n","    '''\n","\n","    # Encoding userId's and itemId's in data\n","    data, dict_users, dict_items = encode_ids(data)\n","    \n","    ##### FILL HERE (2 lines) ######\n","    n_users = NULL  # number of unique users\n","    n_items = NULL  # number of unique items\n","    ################################\n","    \n","    # Randomly initialize the user and item factors.\n","    p = np.random.normal(0, .1, (n_users, n_factors))\n","    q = np.random.normal(0, .1, (n_items, n_factors))\n","\n","    # Optimization procedure\n","    for epoch in range(n_epochs):\n","        print ('epoch: ', epoch)\n","        # Loop over the rows in data\n","        for index in range(data.shape[0]):\n","            row = data.iloc[[index]]\n","            u = int(row.userId)      # current userId = position in the p vector (thanks to the encoding)\n","            i = int(row.itemId)      # current itemId = position in the q vector\n","            r_ui = float(row.rating) # rating associated to the couple (user u , item i)\n","            \n","            ##### FILL HERE (1 line) ######\n","            err = NULL    # difference between the predicted rating (p_u . q_i) and the known ratings r_ui\n","            ################################\n","            \n","            # Update vectors p_u and q_i\n","            ##### FILL HERE (2 lines) ######\n","            p[u] = NULL  # cf. update rule above \n","            q[i] = NULL\n","            ################################\n","            \n","    return p, q\n","    \n","    \n","def estimate(u, i, p, q):\n","    '''Estimate rating of user u for item i.'''\n","    ##### FILL HERE (1 line) ######\n","    return NULL             #scalar product of p[u] and q[i] /!\\ dimensions\n","    ################################  "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_MYUUm18-id6"},"source":["p, q = SGD(df_ratings)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qJd80gNgNuUR"},"source":["## Get the estimate for all user-item pairs:"]},{"cell_type":"markdown","metadata":{"id":"hj4Pc-FjPJK6"},"source":["Get the user-item matrix filled with predicted ratings:"]},{"cell_type":"code","metadata":{"id":"YRCg3k2IPMSc"},"source":["df_user_item_filled = pd.DataFrame(np.dot(p, q.transpose()))\n","df_user_item_filled.head()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"LLHPMdpyN96R"},"source":["However, it is using the encode ids ; we need to retrieve the association of encoded ids to original ids, and apply it:"]},{"cell_type":"code","metadata":{"id":"cuft25TRN4CY"},"source":["df_ratings_encoded, dict_users, dict_items = encode_ids(df_ratings)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mCidjCrUl2tx"},"source":["df_user_item_filled.rename(columns=(dict_items['itemId']), inplace=True)\n","df_user_item_filled.rename(index=(dict_users['userId']), inplace=True)\n","\n","# Sort index/rows (userId's) and columns (itemId's)\n","df_user_item_filled.sort_index(axis=0, inplace=True)\n","df_user_item_filled.sort_index(axis=1, inplace=True)\n","\n","df_user_item_filled.head()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"AVXIqXAdOPzX"},"source":["Originally available ratings for user 1:"]},{"cell_type":"code","metadata":{"id":"iyka6nXcOPo4"},"source":["df_user_item.loc[1][:10]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Pphixa2wOPeh"},"source":["Estimated ratings after the approximate SVD:"]},{"cell_type":"code","metadata":{"id":"YDczh7x5Q6in"},"source":["df_user_item_filled.loc[1][:10]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Uk8zB0HCmLvk"},"source":["## Give recommendation to a user\n","\n","For instance 10 recommended movies for user 1"]},{"cell_type":"code","metadata":{"id":"G8zxuZ2VmaIs"},"source":["recommendations = list((df_user_item_filled.loc[10]).sort_values(ascending=False)[:10].index)\n","recommendations"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5U7R7lyTuOy_"},"source":["df_movies[df_movies.itemId.isin(recommendations)]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3fhXmfLeuDZo"},"source":["vs the ones that were rated initially:"]},{"cell_type":"code","metadata":{"id":"4ooeCcRnuI8y"},"source":["already_rated = list((df_user_item.loc[10]).sort_values(ascending=False)[:10].index)\n","already_rated"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0SM3mJYwyF1g"},"source":["df_movies[df_movies.itemId.isin(already_rated)]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qKarQdgbm4tw"},"source":["This is all the movies in descending order of predicted rating. Let's remove the ones that where alread rated."]},{"cell_type":"markdown","metadata":{"id":"hkvVcbTALIji"},"source":["\n","\n","\n","---\n","\n","\n","\n","To put this into production, you'd first separate data into a training and validation set and optimize the number of latent factors (n_factors) by minimizing the Root Mean Square Error. \n","It is easier to use a framework that allows to do this, do cross-validation, grid search, etc."]},{"cell_type":"markdown","metadata":{"id":"nMdbrNdLldG9"},"source":["## Gradient Descent SVD using Surprise"]},{"cell_type":"code","metadata":{"id":"4VdMT5PnbIn9"},"source":["!pip install surprise\n","#!pip install scikit-surprise # if the first line does not work"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Ed0lnuff4NOw"},"source":["# from surprise import Reader, Dataset, SVD, evaluate\n","\n","# Following Surprise documentation examples \n","# https://surprise.readthedocs.io/en/stable/getting_started.html\n","\n","from surprise import Reader, Dataset, SVD, evaluate, NormalPredictor\n","from surprise.model_selection import cross_validate\n","from collections import defaultdict\n","\n","# As we're loading a custom dataset, we need to define a reader.\n","reader = Reader(rating_scale=(0.5, 5))\n","\n","# The columns must correspond to user id, item id and ratings (in that order).\n","data = Dataset.load_from_df(df_ratings[['userId', 'itemId', 'rating']], reader)\n","\n","# We'll use the famous SVD algorithm.\n","algo = SVD()\n","\n","# Run 5-fold cross-validation and print results\n","cross_validate(algo, data, measures=['RMSE', 'MAE'], cv=5, verbose=True)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"YyciPjWI4Q94"},"source":["#### Tune algorithm parameters with GridSearchCV\n","\n"]},{"cell_type":"code","metadata":{"id":"tG3nlrAKzLZg"},"source":["from surprise.model_selection import GridSearchCV\n","\n","param_grid = {'n_epochs': [5, 10], 'lr_all': [0.002, 0.005],\n","              'reg_all': [0.4, 0.6]}\n","gs = GridSearchCV(SVD, param_grid, measures=['rmse', 'mae'], cv=3)\n","\n","gs.fit(data)\n","\n","# best RMSE score\n","print(gs.best_score['rmse'])\n","\n","# combination of parameters that gave the best RMSE score\n","print(gs.best_params['rmse'])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"LnfvwVPvzUsw"},"source":["# We can now use the algorithm that yields the best rmse:\n","algo = gs.best_estimator['rmse']\n","trainset = data.build_full_trainset()\n","algo.fit(trainset)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"JVAeYFgTzppL"},"source":["algo.predict(621,1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"li7UhY6fz1oG"},"source":["df_data = data.df\n","df_data = df_data.join(df_movies,how=\"left\", on='itemId',rsuffix='_', lsuffix='')\n","df_data[df_data['userId']==1].sort_values(by = 'rating',ascending=False)[:10]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"CRm97oJVz8wG"},"source":["# From Surprise documentation: https://surprise.readthedocs.io/en/stable/FAQ.html\n","def get_top_n(predictions, n=10):\n","    '''Return the top-N recommendation for each user from a set of predictions.\n","\n","    Args:\n","        predictions(list of Prediction objects): The list of predictions, as\n","            returned by the test method of an algorithm.\n","        n(int): The number of recommendation to output for each user. Default\n","            is 10.\n","\n","    Returns:\n","    A dict where keys are user (raw) ids and values are lists of tuples:\n","        [(raw item id, rating estimation), ...] of size n.\n","    '''\n","\n","    # First map the predictions to each user.\n","    top_n = defaultdict(list)\n","    for uid, iid, true_r, est, _ in predictions:\n","        top_n[uid].append((iid, est))\n","\n","    # Then sort the predictions for each user and retrieve the k highest ones.\n","    for uid, user_ratings in top_n.items():\n","        user_ratings.sort(key=lambda x: x[1], reverse=True)\n","        top_n[uid] = user_ratings[:n]\n","\n","    return top_n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"poADsLk634aR"},"source":["# Predict ratings for all pairs (u, i) that are NOT in the training set.\n","testset = trainset.build_anti_testset()\n","predictions = algo.test(testset)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zn3AViRh19eR"},"source":["top_n = get_top_n(predictions, n=10)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"igRXlPxr4gCH"},"source":["top_n.items()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"U2ElCZzT4EC1"},"source":["# Print the recommended items for all user 1\n","for uid, user_ratings in top_n.items():\n","    print(uid, [iid for (iid, _) in user_ratings])\n","    if uid == 1:\n","        break"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3OVCCW1C4ziF"},"source":["df_movies[df_movies.itemId.isin([318, 750, 1204, 858, 904, 48516, 1221, 912, 1276, 4973])]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"uNVZSfS35PSo"},"source":[""],"execution_count":null,"outputs":[]}]}