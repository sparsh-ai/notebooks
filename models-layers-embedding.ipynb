{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp models.layers.embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embedding Layers\n",
    "> Implementation of NN embedding layers in Pytorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev.showdoc import *\n",
    "from fastcore.nb_imports import *\n",
    "from fastcore.test import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import torch\n",
    "from torch import nn\n",
    "import h5py\n",
    "import os\n",
    "import numpy as np\n",
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#exporti\n",
    "class MaskedAveragePooling(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MaskedAveragePooling, self).__init__()\n",
    "\n",
    "    def forward(self, embedding_matrix):\n",
    "        sum_pooling_matrix = torch.sum(embedding_matrix, dim=1)\n",
    "        non_padding_length = (embedding_matrix != 0).sum(dim=1)\n",
    "        embedding_vec = sum_pooling_matrix / (non_padding_length.float() + 1e-16)\n",
    "        return embedding_vec\n",
    "\n",
    "\n",
    "class MaskedSumPooling(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MaskedSumPooling, self).__init__()\n",
    "\n",
    "    def forward(self, embedding_matrix):\n",
    "        # mask by zeros\n",
    "        return torch.sum(embedding_matrix, dim=1)\n",
    "\n",
    "\n",
    "class KMaxPooling(nn.Module):\n",
    "    def __init__(self, k, dim):\n",
    "        super(KMaxPooling, self).__init__()\n",
    "        self.k = k\n",
    "        self.dim = dim\n",
    "\n",
    "    def forward(self, X):\n",
    "        index = X.topk(self.k, dim=self.dim)[1].sort(dim=self.dim)[0]\n",
    "        output = X.gather(self.dim, index)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class EmbeddingLayer(nn.Module):\n",
    "    def __init__(self, \n",
    "                 feature_map,\n",
    "                 embedding_dim,\n",
    "                 use_pretrain=True,\n",
    "                 required_feature_columns=[],\n",
    "                 not_required_feature_columns=[]):\n",
    "        super(EmbeddingLayer, self).__init__()\n",
    "        self.embedding_layer = EmbeddingDictLayer(feature_map, \n",
    "                                                  embedding_dim,\n",
    "                                                  use_pretrain=use_pretrain,\n",
    "                                                  required_feature_columns=required_feature_columns,\n",
    "                                                  not_required_feature_columns=not_required_feature_columns)\n",
    "\n",
    "    def forward(self, X):\n",
    "        feature_emb_dict = self.embedding_layer(X)\n",
    "        feature_emb = self.embedding_layer.dict2tensor(feature_emb_dict)\n",
    "        return feature_emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class EmbeddingDictLayer(nn.Module):\n",
    "    def __init__(self, \n",
    "                 feature_map, \n",
    "                 embedding_dim, \n",
    "                 use_pretrain=True,\n",
    "                 required_feature_columns=[],\n",
    "                 not_required_feature_columns=[]):\n",
    "        super(EmbeddingDictLayer, self).__init__()\n",
    "        self._feature_map = feature_map\n",
    "        self.required_feature_columns = required_feature_columns\n",
    "        self.not_required_feature_columns = not_required_feature_columns\n",
    "        self.embedding_layer = nn.ModuleDict()\n",
    "        self.sequence_encoder = nn.ModuleDict()\n",
    "        self.embedding_hooks = nn.ModuleDict()\n",
    "        for feature, feature_spec in self._feature_map.feature_specs.items():\n",
    "            if self.is_required(feature):\n",
    "                if (not use_pretrain) and embedding_dim == 1:\n",
    "                    feat_emb_dim = 1 # in case for LR\n",
    "                else:\n",
    "                    feat_emb_dim = feature_spec.get(\"embedding_dim\", embedding_dim)\n",
    "                    if \"pretrained_emb\" in feature_spec:\n",
    "                        self.embedding_hooks[feature] = nn.Linear(feat_emb_dim, embedding_dim, bias=False)\n",
    "\n",
    "                # Set embedding_layer according to share_embedding\n",
    "                if use_pretrain and \"share_embedding\" in feature_spec:\n",
    "                    self.embedding_layer[feature] = self.embedding_layer[feature_spec[\"share_embedding\"]]\n",
    "                    self.set_sequence_encoder(feature, feature_spec.get(\"encoder\", None))\n",
    "                    continue\n",
    "\n",
    "                if feature_spec[\"type\"] == \"numeric\":\n",
    "                    self.embedding_layer[feature] = nn.Linear(1, feat_emb_dim, bias=False)\n",
    "                elif feature_spec[\"type\"] == \"categorical\":\n",
    "                    padding_idx = feature_spec.get(\"padding_idx\", None)\n",
    "                    embedding_matrix = nn.Embedding(feature_spec[\"vocab_size\"], \n",
    "                                                    feat_emb_dim, \n",
    "                                                    padding_idx=padding_idx)\n",
    "                    if use_pretrain and \"pretrained_emb\" in feature_spec:\n",
    "                        embeddings = self.get_pretrained_embedding(feature_map.data_dir, feature, feature_spec)\n",
    "                        embedding_matrix = self.set_pretrained_embedding(embedding_matrix, \n",
    "                                                                         embeddings, \n",
    "                                                                         freeze=feature_spec[\"freeze_emb\"],\n",
    "                                                                         padding_idx=padding_idx)\n",
    "                    self.embedding_layer[feature] = embedding_matrix\n",
    "                elif feature_spec[\"type\"] == \"sequence\":\n",
    "                    padding_idx = feature_spec[\"vocab_size\"] - 1\n",
    "                    embedding_matrix = nn.Embedding(feature_spec[\"vocab_size\"], \n",
    "                                                    feat_emb_dim, \n",
    "                                                    padding_idx=padding_idx)\n",
    "                    if use_pretrain and \"pretrained_emb\" in feature_spec:\n",
    "                        embeddings = self.get_pretrained_embedding(feature_map.data_dir, feature, feature_spec)\n",
    "                        embedding_matrix = self.set_pretrained_embedding(embedding_matrix, \n",
    "                                                                         embeddings, \n",
    "                                                                         freeze=feature_spec[\"freeze_emb\"],\n",
    "                                                                         padding_idx=padding_idx)\n",
    "                    self.embedding_layer[feature] = embedding_matrix\n",
    "                    self.set_sequence_encoder(feature, feature_spec.get(\"encoder\", None))\n",
    "\n",
    "    def is_required(self, feature):\n",
    "        \"\"\" Check whether feature is required for embedding \"\"\"\n",
    "        feature_spec = self._feature_map.feature_specs[feature]\n",
    "        if len(self.required_feature_columns) > 0 and (feature not in self.required_feature_columns):\n",
    "            return False\n",
    "        elif feature in self.not_required_feature_columns:\n",
    "            return False\n",
    "        else:\n",
    "            return True\n",
    "\n",
    "    def set_sequence_encoder(self, feature, encoder):\n",
    "        if encoder is None or encoder in [\"none\", \"null\"]:\n",
    "            self.sequence_encoder.update({feature: None})\n",
    "        elif encoder == \"MaskedAveragePooling\":\n",
    "            self.sequence_encoder.update({feature: MaskedAveragePooling()})\n",
    "        elif encoder == \"MaskedSumPooling\":\n",
    "            self.sequence_encoder.update({feature: MaskedSumPooling()})\n",
    "        else:\n",
    "            raise RuntimeError(\"Sequence encoder={} is not supported.\".format(encoder))\n",
    "\n",
    "    def get_pretrained_embedding(self, data_dir, feature_name, feature_spec):\n",
    "        pretrained_path = os.path.join(data_dir, feature_spec[\"pretrained_emb\"])\n",
    "        with h5py.File(pretrained_path, 'r') as hf:\n",
    "            embeddings = hf[feature_name][:]\n",
    "        return embeddings\n",
    "\n",
    "    def set_pretrained_embedding(self, embedding_matrix, embeddings, freeze=False, padding_idx=None):\n",
    "        if padding_idx is not None:\n",
    "            embeddings[padding_idx] = np.zeros(embeddings.shape[-1])\n",
    "        embeddings = torch.from_numpy(embeddings).float()\n",
    "        embedding_matrix.weight = torch.nn.Parameter(embeddings)\n",
    "        if freeze:\n",
    "            embedding_matrix.weight.requires_grad = False\n",
    "        return embedding_matrix\n",
    "\n",
    "    def dict2tensor(self, embedding_dict, feature_source=None, feature_type=None):\n",
    "        if feature_source is not None:\n",
    "            if not isinstance(feature_source, list):\n",
    "                feature_source = [feature_source]\n",
    "            feature_emb_list = []\n",
    "            for feature, feature_spec in self._feature_map.feature_specs.items():\n",
    "                if feature_spec[\"source\"] in feature_source:\n",
    "                    feature_emb_list.append(embedding_dict[feature])\n",
    "            return torch.stack(feature_emb_list, dim=1)\n",
    "        elif feature_type is not None:\n",
    "            if not isinstance(feature_type, list):\n",
    "                feature_type = [feature_type]\n",
    "            feature_emb_list = []\n",
    "            for feature, feature_spec in self._feature_map.feature_specs.items():\n",
    "                if feature_spec[\"type\"] in feature_type:\n",
    "                    feature_emb_list.append(embedding_dict[feature])\n",
    "            return torch.stack(feature_emb_list, dim=1)\n",
    "        else:\n",
    "            return torch.stack(list(embedding_dict.values()), dim=1)\n",
    "\n",
    "    def forward(self, X):\n",
    "        feature_emb_dict = OrderedDict()\n",
    "        for feature, feature_spec in self._feature_map.feature_specs.items():\n",
    "            if feature in self.embedding_layer:\n",
    "                if feature_spec[\"type\"] == \"numeric\":\n",
    "                    inp = X[:, feature_spec[\"index\"]].float().view(-1, 1)\n",
    "                    embedding_vec = self.embedding_layer[feature](inp)\n",
    "                elif feature_spec[\"type\"] == \"categorical\":\n",
    "                    inp = X[:, feature_spec[\"index\"]].long()\n",
    "                    embedding_vec = self.embedding_layer[feature](inp)\n",
    "                elif feature_spec[\"type\"] == \"sequence\":\n",
    "                    inp = X[:, feature_spec[\"index\"]].long()\n",
    "                    seq_embed_matrix = self.embedding_layer[feature](inp)\n",
    "                    if self.sequence_encoder[feature] is not None:\n",
    "                        embedding_vec = self.sequence_encoder[feature](seq_embed_matrix)\n",
    "                    else:\n",
    "                        embedding_vec = seq_embed_matrix\n",
    "                if feature in self.embedding_hooks:\n",
    "                    embedding_vec = self.embedding_hooks[feature](embedding_vec)\n",
    "                feature_emb_dict[feature] = embedding_vec\n",
    "        return feature_emb_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ItemPosEmbedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class ItemPosEmbedding(nn.Module):\n",
    "    \"\"\"Construct the embeddings from item, position.\n",
    "\n",
    "    References:\n",
    "        1. https://github.com/RecoHut-Stanzas/STOSA/blob/ee14e2eabcc60922eb52cc7d3231df4954d9ff16/modules.py#L102\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 item_size,\n",
    "                 hidden_size,\n",
    "                 max_seq_length,\n",
    "                 hidden_dropout_prob):\n",
    "        super().__init__()\n",
    "\n",
    "        self.item_embeddings = nn.Embedding(item_size, hidden_size, padding_idx=0)\n",
    "        self.position_embeddings = nn.Embedding(max_seq_length, hidden_size)\n",
    "\n",
    "        self.layernorm = nn.LayerNorm(hidden_size, eps=1e-12)\n",
    "        self.dropout = nn.Dropout(hidden_dropout_prob)\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        seq_length = input_ids.size(1)\n",
    "        position_ids = torch.arange(seq_length, dtype=torch.long, device=input_ids.device)\n",
    "        position_ids = position_ids.unsqueeze(0).expand_as(input_ids)\n",
    "        items_embeddings = self.item_embeddings(input_ids)\n",
    "        position_embeddings = self.position_embeddings(position_ids)\n",
    "        embeddings = items_embeddings + position_embeddings\n",
    "        embeddings = self.layernorm(embeddings)\n",
    "        embeddings = self.dropout(embeddings)\n",
    "        return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_size = 10\n",
    "hidden_size = 5\n",
    "max_seq_length = 20\n",
    "hidden_dropout_prob = 0.2\n",
    "layer = ItemPosEmbedding(item_size, hidden_size, max_seq_length, hidden_dropout_prob)\n",
    "\n",
    "torch.manual_seed(0)\n",
    "x = torch.randint(0,5,(item_size, hidden_size))\n",
    "\n",
    "output = torch.round(layer.forward(x).detach()*1e4)/1e4\n",
    "\n",
    "test_eq(output.shape.numel(), 250)\n",
    "test_eq(list(output.shape), [10, 5, 5])\n",
    "test_eq(round(float(output[5][2][2]), 2), 1.41)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **References**\n",
    "> - https://github.com/xue-pai/FuxiCTR/blob/main/fuxictr/pytorch/layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Author: Sparsh A.\n",
      "\n",
      "Last updated: 2022-01-11 12:16:07\n",
      "\n",
      "Compiler    : GCC 7.5.0\n",
      "OS          : Linux\n",
      "Release     : 5.4.144+\n",
      "Machine     : x86_64\n",
      "Processor   : x86_64\n",
      "CPU cores   : 2\n",
      "Architecture: 64bit\n",
      "\n",
      "torch     : 1.10.0+cu111\n",
      "numpy     : 1.19.5\n",
      "h5py      : 3.1.0\n",
      "PIL       : 7.1.2\n",
      "IPython   : 5.5.0\n",
      "matplotlib: 3.2.2\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "%reload_ext watermark\n",
    "%watermark -a \"Sparsh A.\" -m -iv -u -t -d"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
