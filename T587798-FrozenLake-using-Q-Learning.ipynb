{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"T587798 | FrozenLake using Q-Learning","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyPqCsB14gbD0n+pADXWgdrg"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"KVIjdOWtJ0ZS"},"source":["# FrozenLake using Q-Learning"]},{"cell_type":"markdown","metadata":{"id":"-zYDVzsaOAVV"},"source":["## Imports"]},{"cell_type":"code","metadata":{"id":"qjoq2JhWMoQI"},"source":["import numpy as np\n","import gym\n","import random"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"q2Wrkp1HMoq1"},"source":["env = gym.make(\"FrozenLake-v0\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5RH3D53tMuG_"},"source":["## Create the Q-table and initialize it üóÑÔ∏è\n","Now, we'll create our Q-table, to know how much rows (states) and columns (actions) we need, we need to calculate the action_size and the state_size\n","OpenAI Gym provides us a way to do that: env.action_space.n and env.observation_space.n"]},{"cell_type":"code","metadata":{"id":"XXD9WEOzMy5r"},"source":["action_size = env.action_space.n\n","state_size = env.observation_space.n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AW-Ti3swM1Hw","executionInfo":{"status":"ok","timestamp":1634909516972,"user_tz":-330,"elapsed":554,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}},"outputId":"8cd108e3-eafc-4888-bb98-0aa4371b68a3"},"source":["# Create our Q table with state_size rows and action_size columns (64x4)\n","qtable = np.zeros((state_size, action_size))\n","print(qtable)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[[0. 0. 0. 0.]\n"," [0. 0. 0. 0.]\n"," [0. 0. 0. 0.]\n"," [0. 0. 0. 0.]\n"," [0. 0. 0. 0.]\n"," [0. 0. 0. 0.]\n"," [0. 0. 0. 0.]\n"," [0. 0. 0. 0.]\n"," [0. 0. 0. 0.]\n"," [0. 0. 0. 0.]\n"," [0. 0. 0. 0.]\n"," [0. 0. 0. 0.]\n"," [0. 0. 0. 0.]\n"," [0. 0. 0. 0.]\n"," [0. 0. 0. 0.]\n"," [0. 0. 0. 0.]]\n"]}]},{"cell_type":"markdown","metadata":{"id":"4wrFaLS1M29a"},"source":["## Create the hyperparameters ‚öôÔ∏è\n","Here, we'll specify the hyperparameters"]},{"cell_type":"code","metadata":{"id":"ShVBH2toM7mA"},"source":["total_episodes = 20000       # Total episodes\n","learning_rate = 0.7          # Learning rate\n","max_steps = 99               # Max steps per episode\n","gamma = 0.95                 # Discounting rate\n","\n","# Exploration parameters\n","epsilon = 1.0                 # Exploration rate\n","max_epsilon = 1.0             # Exploration probability at start\n","min_epsilon = 0.01            # Minimum exploration probability \n","decay_rate = 0.005            # Exponential decay rate for exploration prob"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"j1m7ADqRM722"},"source":["## The Q learning algorithm üß†\n","Now we implement the Q learning algorithm:"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WN0WDTDrNEgA","executionInfo":{"status":"ok","timestamp":1634909598080,"user_tz":-330,"elapsed":17917,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}},"outputId":"7b02086b-f7c3-4e07-c9ea-7cada07aecd5"},"source":["# List of rewards\n","rewards = []\n","\n","# 2 For life or until learning is stopped\n","for episode in range(total_episodes):\n","    # Reset the environment\n","    state = env.reset()\n","    step = 0\n","    done = False\n","    total_rewards = 0\n","    \n","    for step in range(max_steps):\n","        # 3. Choose an action a in the current world state (s)\n","        ## First we randomize a number\n","        exp_exp_tradeoff = random.uniform(0, 1)\n","        \n","        ## If this number > greater than epsilon --> exploitation (taking the biggest Q value for this state)\n","        if exp_exp_tradeoff > epsilon:\n","            action = np.argmax(qtable[state,:])\n","            #print(exp_exp_tradeoff, \"action\", action)\n","\n","        # Else doing a random choice --> exploration\n","        else:\n","            action = env.action_space.sample()\n","            #print(\"action random\", action)\n","            \n","        \n","        # Take the action (a) and observe the outcome state(s') and reward (r)\n","        new_state, reward, done, info = env.step(action)\n","\n","        # Update Q(s,a):= Q(s,a) + lr [R(s,a) + gamma * max Q(s',a') - Q(s,a)]\n","        # qtable[new_state,:] : all the actions we can take from new state\n","        qtable[state, action] = qtable[state, action] + learning_rate * (reward + gamma * np.max(qtable[new_state, :]) - qtable[state, action])\n","        \n","        total_rewards += reward\n","        \n","        # Our new state is state\n","        state = new_state\n","        \n","        # If done (if we're dead) : finish episode\n","        if done == True: \n","            break\n","        \n","    # Reduce epsilon (because we need less and less exploration)\n","    epsilon = min_epsilon + (max_epsilon - min_epsilon)*np.exp(-decay_rate*episode) \n","    rewards.append(total_rewards)\n","    \n","\n","print (\"Score over time: \" +  str(sum(rewards)/total_episodes))\n","print(qtable)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Score over time: 0.5091\n","[[9.01507329e-02 8.17424478e-02 6.39680351e-02 6.86630922e-02]\n"," [2.62376785e-02 1.09112058e-02 8.89889825e-03 7.77301606e-02]\n"," [2.64740266e-02 1.47344615e-02 2.38915905e-02 2.52383378e-02]\n"," [1.27543068e-03 2.20765613e-03 3.16416215e-03 2.46832992e-02]\n"," [9.79766765e-02 4.04794442e-02 6.38497041e-02 3.75700055e-04]\n"," [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n"," [1.10458406e-03 2.96899943e-02 6.25840861e-04 1.48050755e-04]\n"," [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n"," [4.09218831e-02 4.63296542e-02 4.42937432e-02 1.55235475e-01]\n"," [1.05014686e-02 1.58532972e-01 3.70542973e-03 4.56743994e-02]\n"," [6.42722208e-01 7.37759086e-03 1.53679103e-03 2.50722701e-02]\n"," [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n"," [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n"," [8.74653767e-02 2.29993112e-02 7.76083629e-01 3.36186831e-01]\n"," [1.68609535e-01 9.43051365e-01 3.12274680e-01 3.48405559e-01]\n"," [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]]\n"]}]},{"cell_type":"markdown","metadata":{"id":"xy4odqxoNGhE"},"source":["## Use our Q-table to play FrozenLake ! üëæ\n","After 10 000 episodes, our Q-table can be used as a \"cheatsheet\" to play FrozenLake\"\n","\n","By running this cell you can see our agent playing FrozenLake."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kBi_qh77NJ6a","executionInfo":{"status":"ok","timestamp":1634909627130,"user_tz":-330,"elapsed":633,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}},"outputId":"77a9234c-cab1-484b-9a40-465d046c0c08"},"source":["env.reset()\n","\n","for episode in range(5):\n","    state = env.reset()\n","    step = 0\n","    done = False\n","    print(\"****************************************************\")\n","    print(\"EPISODE \", episode)\n","\n","    for step in range(max_steps):\n","        \n","        # Take the action (index) that have the maximum expected future reward given that state\n","        action = np.argmax(qtable[state,:])\n","        \n","        new_state, reward, done, info = env.step(action)\n","        \n","        if done:\n","            # Here, we decide to only print the last state (to see if our agent is on the goal or fall into an hole)\n","            env.render()\n","            if new_state == 15:\n","                print(\"We reached our Goal üèÜ\")\n","            else:\n","                print(\"We fell into a hole ‚ò†Ô∏è\")\n","            \n","            # We print the number of step it took.\n","            print(\"Number of steps\", step)\n","            \n","            break\n","        state = new_state\n","env.close()"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["****************************************************\n","EPISODE  0\n","  (Down)\n","SFFF\n","FHFH\n","FFFH\n","HFF\u001b[41mG\u001b[0m\n","We reached our Goal üèÜ\n","Number of steps 20\n","****************************************************\n","EPISODE  1\n","  (Down)\n","SFFF\n","FHFH\n","FFFH\n","HFF\u001b[41mG\u001b[0m\n","We reached our Goal üèÜ\n","Number of steps 57\n","****************************************************\n","EPISODE  2\n","  (Down)\n","SFFF\n","F\u001b[41mH\u001b[0mFH\n","FFFH\n","HFFG\n","We fell into a hole ‚ò†Ô∏è\n","Number of steps 87\n","****************************************************\n","EPISODE  3\n","  (Down)\n","SFFF\n","FHFH\n","FFFH\n","HFF\u001b[41mG\u001b[0m\n","We reached our Goal üèÜ\n","Number of steps 16\n","****************************************************\n","EPISODE  4\n","  (Down)\n","SFFF\n","FHFH\n","FFFH\n","HFF\u001b[41mG\u001b[0m\n","We reached our Goal üèÜ\n","Number of steps 38\n"]}]},{"cell_type":"markdown","metadata":{"id":"ZR-yGpEnOVUz"},"source":["## PyTorch version"]},{"cell_type":"code","metadata":{"id":"krU58JoumsLF"},"source":["import gym\n","import collections\n","from torch.utils.tensorboard import SummaryWriter"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"vtzB-mVNmwZC"},"source":["ENV_NAME = \"FrozenLake-v0\"\n","GAMMA = 0.9\n","ALPHA = 0.2\n","TEST_EPISODES = 20"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"PkSo69Vvmu5t"},"source":["class Agent:\n","    def __init__(self):\n","        self.env = gym.make(ENV_NAME)\n","        self.state = self.env.reset()\n","        self.values = collections.defaultdict(float)\n","\n","    def sample_env(self):\n","        action = self.env.action_space.sample()\n","        old_state = self.state\n","        new_state, reward, is_done, _ = self.env.step(action)\n","        self.state = self.env.reset() if is_done else new_state\n","        return old_state, action, reward, new_state\n","\n","    def best_value_and_action(self, state):\n","        best_value, best_action = None, None\n","        for action in range(self.env.action_space.n):\n","            action_value = self.values[(state, action)]\n","            if best_value is None or best_value < action_value:\n","                best_value = action_value\n","                best_action = action\n","        return best_value, best_action\n","\n","    def value_update(self, s, a, r, next_s):\n","        best_v, _ = self.best_value_and_action(next_s)\n","        new_v = r + GAMMA * best_v\n","        old_v = self.values[(s, a)]\n","        self.values[(s, a)] = old_v * (1-ALPHA) + new_v * ALPHA\n","\n","    def play_episode(self, env):\n","        total_reward = 0.0\n","        state = env.reset()\n","        while True:\n","            _, action = self.best_value_and_action(state)\n","            new_state, reward, is_done, _ = env.step(action)\n","            total_reward += reward\n","            if is_done:\n","                break\n","            state = new_state\n","        return total_reward"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2-1z_6bGmtJX","executionInfo":{"status":"ok","timestamp":1634480185460,"user_tz":-330,"elapsed":53454,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}},"outputId":"3d8d2c25-7ec0-4adb-e1e8-f5a617b234c8"},"source":["if __name__ == \"__main__\":\n","    test_env = gym.make(ENV_NAME)\n","    agent = Agent()\n","    writer = SummaryWriter(comment=\"-q-learning\")\n","\n","    iter_no = 0\n","    best_reward = 0.0\n","    while True:\n","        iter_no += 1\n","        s, a, r, next_s = agent.sample_env()\n","        agent.value_update(s, a, r, next_s)\n","\n","        reward = 0.0\n","        for _ in range(TEST_EPISODES):\n","            reward += agent.play_episode(test_env)\n","        reward /= TEST_EPISODES\n","        writer.add_scalar(\"reward\", reward, iter_no)\n","        if reward > best_reward:\n","            print(\"Best reward updated %.3f -> %.3f\" % (\n","                best_reward, reward))\n","            best_reward = reward\n","        if reward > 0.80:\n","            print(\"Solved in %d iterations!\" % iter_no)\n","            break\n","    writer.close()"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Best reward updated 0.000 -> 0.050\n","Best reward updated 0.050 -> 0.150\n","Best reward updated 0.150 -> 0.200\n","Best reward updated 0.200 -> 0.250\n","Best reward updated 0.250 -> 0.300\n","Best reward updated 0.300 -> 0.350\n","Best reward updated 0.350 -> 0.400\n","Best reward updated 0.400 -> 0.450\n","Best reward updated 0.450 -> 0.550\n","Best reward updated 0.550 -> 0.600\n","Best reward updated 0.600 -> 0.650\n","Best reward updated 0.650 -> 0.700\n","Best reward updated 0.700 -> 0.800\n","Best reward updated 0.800 -> 0.850\n","Solved in 5895 iterations!\n"]}]},{"cell_type":"code","metadata":{"id":"8BX2Wq0mm2AI"},"source":["%load_ext tensorboard\n","%tensorboard --logdir runs"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"UrnvYHbuOWlX"},"source":["<p><center><img src='_images/T587798_1.png'></center></p>"]}]}