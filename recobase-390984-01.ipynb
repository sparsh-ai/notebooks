{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"recobase-390984-01.ipynb","provenance":[],"collapsed_sections":[],"mount_file_id":"1Cq3lMHr0G_oP2N0WHbg7OBJb84j86jvh","authorship_tag":"ABX9TyNVf+Z865rWYeLy/xOi4PsA"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"77RmB1QyEvAL","executionInfo":{"status":"ok","timestamp":1631354271762,"user_tz":-330,"elapsed":544,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}},"outputId":"064c3ce4-2f12-4f97-8fe0-ce26e4d65075"},"source":["import os\n","project_name = \"recobase\"; branch = \"US390984\"; account = \"recohut\"\n","project_path = os.path.join('/content', branch)\n","\n","if not os.path.exists(project_path):\n","    !pip install -U -q dvc dvc[gdrive]\n","    !cp -r /content/drive/MyDrive/git_credentials/. ~\n","    !mkdir \"{project_path}\"\n","    %cd \"{project_path}\"\n","    !git init\n","    !git remote add origin https://github.com/\"{account}\"/\"{project_name}\".git\n","    !git pull origin \"{branch}\"\n","    !git checkout -b \"{branch}\"\n","else:\n","    %cd \"{project_path}\""],"execution_count":23,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/US390984\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"psh8202bV5Nu","executionInfo":{"status":"ok","timestamp":1631362140618,"user_tz":-330,"elapsed":505,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}},"outputId":"b67cd190-4745-43c0-b5fb-321114a2c4e1"},"source":["!git status"],"execution_count":51,"outputs":[{"output_type":"stream","name":"stdout","text":["On branch US390984\n","Untracked files:\n","  (use \"git add <file>...\" to include in what will be committed)\n","\n","\t\u001b[31msrc/calculate_mle_probs.py\u001b[m\n","\n","nothing added to commit but untracked files present (use \"git add\" to track)\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"iLdFo-nrEvAR","executionInfo":{"status":"ok","timestamp":1631362145465,"user_tz":-330,"elapsed":1836,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}},"outputId":"81a2e5cd-227d-4549-b55e-b0e1da810a98"},"source":["!git add .\n","!git commit -m 'commit'\n","!git push origin \"{branch}\""],"execution_count":52,"outputs":[{"output_type":"stream","name":"stdout","text":["[US390984 41ed658] commit\n"," 1 file changed, 182 insertions(+)\n"," create mode 100644 src/calculate_mle_probs.py\n","Counting objects: 4, done.\n","Delta compression using up to 2 threads.\n","Compressing objects: 100% (4/4), done.\n","Writing objects: 100% (4/4), 1.90 KiB | 1.90 MiB/s, done.\n","Total 4 (delta 2), reused 0 (delta 0)\n","remote: Resolving deltas: 100% (2/2), completed with 2 local objects.\u001b[K\n","To https://github.com/recohut/recobase.git\n","   659ffa2..41ed658  US390984 -> US390984\n"]}]},{"cell_type":"code","metadata":{"id":"151nHKcbWg5X"},"source":["!dvc status\n","!dvc add\n","!dvc commit\n","!dvc push"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ggSVwBfbE0gb","executionInfo":{"status":"ok","timestamp":1631352917454,"user_tz":-330,"elapsed":522,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}},"outputId":"f781f803-3487-45c8-f989-bf4f2ba8e58d"},"source":["!unzip ./data/redial_dataset.zip"],"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["Archive:  ./data/redial_dataset.zip\n","  inflating: movies_with_mentions.csv  \n","  inflating: test_data.jsonl         \n","  inflating: train_data.jsonl        \n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dthM43-INDBl","executionInfo":{"status":"ok","timestamp":1631354090373,"user_tz":-330,"elapsed":612,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}},"outputId":"3b03722a-660b-4c8a-b6f3-1059504a9790"},"source":["%%writefile /content/US390984/src/preprocess_dialogs.py\n","#!/usr/bin/env python3\n","\n","import pandas as pd\n","import numpy as np\n","import os\n","import simplejson as json\n","\n","\n","class Dataset:\n","    def __init__(self):\n","        self.data = None\n","        self.text_messages_raw = []\n","\n","    def read_input_json_file(self, filepath):\n","        with open(filepath, 'r', encoding='utf-8') as json_file:\n","            self.data = json.load(json_file)\n","\n","    def parse_dialogues(self):\n","        dialogs = self.data['foo']\n","        counter = 0\n","        for key, d in enumerate(dialogs):\n","            messages = dialogs[key]['messages']\n","            seeker_id = dialogs[key]['initiatorWorkerId']\n","            recommender_id = dialogs[key]['respondentWorkerId']\n","            seeker_text = ''\n","            gt_text = ''\n","            counter = counter +1\n","            self.text_messages_raw.append('CONVERSATION:'+ str(counter))\n","            for msgid, msg in enumerate(messages):\n","\n","                senderId = messages[msgid]['senderWorkerId']\n","                if senderId == seeker_id:\n","                    if gt_text:\n","                        self.text_messages_raw.append('GT~' + gt_text)\n","                        gt_text = ''\n","                        seeker_text =  seeker_text +' '+ messages[msgid]['text']\n","                    else:\n","                        seeker_text =  seeker_text +' ' + messages[msgid]['text']\n","\n","                elif senderId == recommender_id:\n","                    if seeker_text:\n","                        self.text_messages_raw.append('SKR~' + seeker_text)\n","                        seeker_text = ''\n","                        gt_text = gt_text+' '  + messages[msgid]['text']\n","                    else:\n","                        gt_text = gt_text +' ' + messages[msgid]['text']\n","\n","            if gt_text:\n","                self.text_messages_raw.append('GT~' + gt_text)\n","            elif seeker_text:\n","                self.text_messages_raw.append('SKR~' + seeker_text)\n","\n","    def write_data(self, filepath):\n","        with open(filepath, 'w', encoding='utf-8') as filehandle:\n","            for line in self.text_messages_raw:\n","                filehandle.write(\"%s\\n\" % line)\n","\n","\n","if __name__ == '__main__':\n","    dataset = Dataset()\n","    dataset.read_input_json_file('data/bronze/dialog_data/unparsed_train_data.txt')\n","    dataset.parse_dialogues()\n","    dataset.write_data('data/silver/dialog_data/training_data_parsed_con.txt')\n","    print('data exported')"],"execution_count":16,"outputs":[{"output_type":"stream","name":"stdout","text":["Overwriting /content/US390984/src/preprocess_dialogs.py\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sqjrnGkpR36k","executionInfo":{"status":"ok","timestamp":1631354278297,"user_tz":-330,"elapsed":1757,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}},"outputId":"570ee18c-a505-4816-fbcf-a98274475367"},"source":["!python /content/US390984/src/preprocess_dialogs.py"],"execution_count":24,"outputs":[{"output_type":"stream","name":"stdout","text":["data exported\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"iLxrLhK8SELD","executionInfo":{"status":"ok","timestamp":1631355079382,"user_tz":-330,"elapsed":3487,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}},"outputId":"a9e775a7-3174-4eac-d7d8-8f08984ca13d"},"source":["!dvc run -n preprocess_dialogs \\\n","          -d src/preprocess_dialogs.py -d data/bronze/dialog_data/unparsed_train_data.txt \\\n","          -o data/silver/dialog_data/parsed_dialogs \\\n","          python src/preprocess_dialogs.py"],"execution_count":26,"outputs":[{"output_type":"stream","name":"stdout","text":["\r!\rIf DVC froze, see `hardlink_lock` in <\u001b[36mhttps://man.dvc.org/config#core\u001b[39m>\r                                                                      \rRunning stage 'preprocess_dialogs':\n","> python src/preprocess_dialogs.py\n","data exported\n","Computing file/dir hashes (only done once)          |0.00 [00:00,      ?md5/s]\n","!\u001b[A\n","          |0.00 [00:00,       ?it/s]\u001b[A\n","Transferring:   0% 0/2 [00:00<?, ?file/s{'info': ''}]\n","c57e1504cb8e59b658b5752be17765.dir:   0% 0/88 [00:00<?, ?it/s]\u001b[A\n","c57e1504cb8e59b658b5752be17765.dir:   0% 0/88 [00:00<?, ?it/s{'info': ''}]\u001b[A\n","Creating 'dvc.yaml'\n","Adding stage 'preprocess_dialogs' in 'dvc.yaml'\n","Generating lock file 'dvc.lock'\n","Updating lock file 'dvc.lock'\n","\n","To track the changes with git, run:\n","\n","\tgit add dvc.yaml dvc.lock data/silver/dialog_data/.gitignore\n","\u001b[0m"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oKq1565oUUHA","executionInfo":{"status":"ok","timestamp":1631359975435,"user_tz":-330,"elapsed":643,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}},"outputId":"523c9984-50c2-484a-f2aa-7eb203579f4d"},"source":["%%writefile ./src/prepare_sentences.py\n","#!/usr/bin/env python3\n","\n","import sys\n","import pandas as pd\n","import numpy as np\n","import re\n","import os\n","from pathlib import Path\n","\n","import nltk\n","from nltk.corpus import stopwords\n","nltk.download('stopwords')\n","from nltk.tokenize import word_tokenize\n","\n","\n","def contraction_handle(filepath='data/bronze/dialog_data/contractions.txt'):\n","    contraction_dict = {}\n","    with open(filepath) as f:\n","        for key_line in f:\n","            (key, val) = key_line.split(':')\n","            contraction_dict[key] = val\n","    return contraction_dict\n","\n","class PrepareSentences:\n","    def __init__(self):\n","        self.contraction_dict = contraction_handle()\n","        self.sentence_data = []\n","\n","    @staticmethod\n","    def seeker_sentences_parser(line):\n","        if line:\n","            p = re.compile(\"SEEKER:(.*)\").search(str(line))\n","            temp_line = p.group(1)\n","            m = re.compile('<s>(.*?)</s>').search(temp_line)\n","            seeker_line = m.group(1)\n","            seeker_line = seeker_line.lower().strip()\n","            return seeker_line\n","\n","    @staticmethod\n","    def gt_sentence_parser(line):\n","        try:\n","            if not line == '\\n':\n","                p = re.compile(\"GROUND TRUTH:(.*)\").search(str(line))\n","                temp_line = p.group(1)\n","                m = re.compile('<s>(.*?)</s>').search(temp_line)\n","                gt_line = m.group(1)\n","                gt_line = gt_line.lower().strip()\n","                # gt_line = re.sub('[^A-Za-z0-9]+', ' ', gt_line)\n","            else:\n","                gt_line = \"\"\n","        except AttributeError as err:\n","                # print('exception accured while parsing ground truth.. \\n')\n","                # print(line)\n","                # print(err)\n","                return gt_line\n","\n","    @staticmethod\n","    def replace_movieIds_withPL(line):\n","        try:\n","            if \"@\" in line:\n","                ids = re.findall(r'@\\S+', line)\n","                for id in ids:\n","                    line = line.replace(id,'movieid')\n","                    #id = re.sub('[^0-9@]+', 'movieid', id)\n","        except:\n","            lines.append(line)\n","            # print('exception occured here')\n","        return line\n","        # print('execution ends here')\n","\n","    @staticmethod\n","    def remove_stopwords(line):\n","        text_tokens = word_tokenize(line)\n","        tokens_without_sw = [word for word in text_tokens if not word in stopwords.words()]\n","        filtered_sentence = (\" \").join(tokens_without_sw)\n","        print(filtered_sentence)\n","        return filtered_sentence\n","\n","    @staticmethod\n","    def convert_contractions(line):\n","        #line = \"What's the best way to ensure this?\"\n","        for word in line.split():\n","            if word.lower() in self.contraction_dict:\n","                line = line.replace(word, contraction_dict[word.lower()])\n","        return line\n","\n","    #read and retrive dialogs from the input file\n","    def read_sentences(self, file_name):\n","        counter =0\n","        previous_line = ''\n","        counter = 0\n","        with open(file_name, 'r', encoding='utf-8') as input:\n","            for line in input:\n","                try:\n","                    #if line.__contains__('~') and line.__contains__('SKR~'):\n","                    if line:\n","                        if line.__contains__('CONVERSATION:'):\n","                            self.sentence_data.append(line.replace('\\n',''))\n","                            continue\n","                        else:\n","                            previous_line = line\n","                            line = self.replace_movieIds_withPL(line)\n","                            line = line.split('~')[1].strip().lower()\n","                            line = self.convert_contractions(line)\n","                            line = re.sub('[^A-Za-z0-9]+', ' ', line)\n","                            line = line.replace('im','i am').strip()\n","                            line = self.remove_stopwords(line)\n","                            if len(line) < 1:\n","                                self.sentence_data.append('**')\n","                            else:\n","                                self.sentence_data.append(line)\n","                    else:\n","                        #print('not found')\n","                        #print(line)\n","                        #print('previous line is ...' +previous_line)\n","                        # print('line issue')\n","                        counter = counter+1\n","                except:\n","                    # print((previous_line))\n","                    # print(line)\n","                    continue\n","\n","    def write_data(self, filepath):\n","        Path(filepath).parent.mkdir(parents=True, exist_ok=True)\n","        with open(filepath, 'w', encoding='utf-8') as filehandle:\n","            for line in self.sentence_data:\n","                filehandle.write(\"%s\\n\" % line)\n","\n","\n","\n","if __name__ == '__main__':\n","    prep = PrepareSentences()\n","    prep.read_sentences('data/silver/dialog_data/parsed_dialogs/training_data_parsed_con.txt')\n","    prep.write_data('data/gold/dialog_data/dialog_sentences/training_data_plsw.txt')\n","    print('Dialogs have been preprocessed successfully.')"],"execution_count":41,"outputs":[{"output_type":"stream","name":"stdout","text":["Overwriting ./src/prepare_sentences.py\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1mLqk2K-V1oj","executionInfo":{"status":"ok","timestamp":1631359978951,"user_tz":-330,"elapsed":1480,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}},"outputId":"340188d2-2d12-495b-8e6f-819b65d2e1c2"},"source":["!python ./src/prepare_sentences.py"],"execution_count":42,"outputs":[{"output_type":"stream","name":"stdout","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","Dialogs have been preprocessed successfully.\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jtQNpr_VWM_4","executionInfo":{"status":"ok","timestamp":1631360103130,"user_tz":-330,"elapsed":4491,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}},"outputId":"e72c161a-d620-4b4d-90bf-b311593206a3"},"source":["!dvc run -n prepare_sentences \\\n","          -d src/prepare_sentences.py -d data/silver/dialog_data/parsed_dialogs/training_data_parsed_con.txt \\\n","          -o data/gold/dialog_data/dialog_sentences \\\n","          python src/prepare_sentences.py"],"execution_count":44,"outputs":[{"output_type":"stream","name":"stdout","text":["\r!\rIf DVC froze, see `hardlink_lock` in <\u001b[36mhttps://man.dvc.org/config#core\u001b[39m>\r                                                                      \rRunning stage 'prepare_sentences':\n","> python src/prepare_sentences.py\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","Dialogs have been preprocessed successfully.\n","Computing file/dir hashes (only done once)          |0.00 [00:00,      ?md5/s]\n","!\u001b[A\n","          |0.00 [00:00,       ?it/s]\u001b[A\n","Transferring:   0% 0/2 [00:00<?, ?file/s{'info': ''}]\n","57d49bb7ac0e5c986bd84850cd5c51.dir:   0% 0/82 [00:00<?, ?it/s]\u001b[A\n","57d49bb7ac0e5c986bd84850cd5c51.dir:   0% 0/82 [00:00<?, ?it/s{'info': ''}]\u001b[A\n","Adding stage 'prepare_sentences' in 'dvc.yaml'\n","Updating lock file 'dvc.lock'\n","\n","To track the changes with git, run:\n","\n","\tgit add data/gold/dialog_data/.gitignore dvc.lock dvc.yaml\n","\u001b[0m"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dintjCEkoySj","executionInfo":{"status":"ok","timestamp":1631360174265,"user_tz":-330,"elapsed":12176,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}},"outputId":"e49225c6-6cbe-461a-89b8-ed3c885a9169"},"source":["!dvc commit && dvc push"],"execution_count":47,"outputs":[{"output_type":"stream","name":"stdout","text":["Transferring:   0% 0/3 [00:00<?, ?file/s{'info': ''}]\n","!\u001b[A\n","  0%|          |41d6dffc9eea567f54426ea7de1853     0.00/? [00:00<?,        ?B/s]\u001b[A\n","41d6dffc9eea567f54426ea7de1853:   0% 0.00/140k [00:00<?, ?B/s{'info': ''}]      \u001b[A\n","  6% 8.00k/140k [00:01<00:17, 7.59kB/s{'info': ''}]                       \u001b[A\n","Transferring:  33% 1/3 [00:02<00:04,  2.08s/file{'info': ''}]\n","!\u001b[A\n","  0%|          |57d49bb7ac0e5c986bd84850cd5c51.dir 0.00/? [00:00<?,        ?B/s]\u001b[A\n","57d49bb7ac0e5c986bd84850cd5c51.dir:   0% 0.00/82.0 [00:00<?, ?B/s{'info': ''}]  \u001b[A\n","100% 82.0/82.0 [00:00<00:00, 82.6B/s{'info': ''}]                             \u001b[A\n","Transferring:  67% 2/3 [00:04<00:02,  2.08s/file{'info': ''}]\n","!\u001b[A\n","  0%|          |0b9d4b57f7a7b896f52b1415070060     0.00/? [00:00<?,        ?B/s]\u001b[A\n","0b9d4b57f7a7b896f52b1415070060:   0% 0.00/2.21k [00:00<?, ?B/s{'info': ''}]     \u001b[A\n","100% 2.21k/2.21k [00:01<00:00, 2.22kB/s{'info': ''}]                       \u001b[A\n","3 files pushed\n","\u001b[0m"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bx48X58JpCcI","executionInfo":{"status":"ok","timestamp":1631361602811,"user_tz":-330,"elapsed":435,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}},"outputId":"a64c86ab-265a-443a-dbcb-e9f74c8b43fd"},"source":["%%writefile ./src/calculate_mle_probs.py\n","\n","from __future__ import division\n","from collections import Counter\n","import math as calc\n","import os\n","\n","\n","class CacululateMLEProbs:\n","    \"\"\"A program which creates n-Gram (1-5) Maximum Likelihood Probabilistic Language Model with Laplace Add-1 smoothing\n","    and stores it in hash-able dictionary form.\n","    n: number of bigrams (supports up to 5)\n","    corpus_file: relative path to the corpus file.\n","    cache: saves computed values if True\n","\"\"\"\n","\n","    def __init__(self, n=1, corpus_file=None, cache=False):\n","        \"\"\"Constructor method which loads the corpus from file and creates ngrams based on imput parameters.\"\"\"\n","        self.PATH = os.path.dirname(os.path.abspath(__file__))\n","        self.ROOT_DIR_PATH = os.path.abspath(os.path.dirname(self.PATH))\n","        self.DATA_path = os.path.join(self.ROOT_DIR_PATH, 'data')\n","        self.DATA_path = os.path.join(self.DATA_path, 'dialog_data', \"\")\n","        self.words = []\n","        self.load_corpus(corpus_file)\n","        self.unigram = self.bigram = self.trigram = self.quadrigram = self.pentigram = None\n","        self.create_unigram(cache)\n","        if n >= 2:\n","            self.create_bigram(cache)\n","        if n >= 3:\n","            self.create_trigram(cache)\n","        if n >= 4:\n","            self.create_quadrigram(cache)\n","        if n >= 5:\n","            self.create_pentigram(cache)\n","        return\n","\n","    def load_corpus(self, file_name):\n","        \"\"\"Method to load external file which contains raw corpus.\"\"\"\n","        print(\"Loading Corpus from data file\")\n","        if file_name is None:\n","            file_name = self.DATA_path+'GT_corpus_tokens.txt'\n","        corpus_file = open(file_name, 'r')\n","        corpus = corpus_file.read()\n","        corpus_file.close()\n","        print(\"Processing Corpus\")\n","        self.words = corpus.split('\\n')\n","\n","    def create_unigram(self, cache):\n","        \"\"\"Method to create Unigram Model for words loaded from corpus.\"\"\"\n","        print(\"Creating Unigram Model\")\n","        unigram_file = None\n","        if cache:\n","            unigram_file = open(self.DATA_path+ 'unigram.data', 'w')\n","        print(\"Calculating Count for Unigram Model\")\n","        unigram = Counter(self.words)\n","        if cache:\n","            unigram_file.write(str(unigram))\n","            unigram_file.close()\n","        self.unigram = unigram\n","\n","    def create_bigram(self, cache):\n","        \"\"\"Method to create Bigram Model for words loaded from corpus.\"\"\"\n","        print(\"Creating Bigram Model\")\n","        words = self.words\n","        biwords = []\n","        for index, item in enumerate(words):\n","            if index == len(words)-1:\n","                break\n","            biwords.append(item+' '+words[index+1])\n","        print(\"Calculating Count for Bigram Model\")\n","        bigram_file = None\n","        if cache:\n","            bigram_file = open(self.DATA_path + 'bigram.data', 'w')\n","        bigram = Counter(biwords)\n","        if cache:\n","            bigram_file.write(str(bigram))\n","            bigram_file.close()\n","        self.bigram = bigram\n","\n","    def create_trigram(self, cache):\n","        \"\"\"Method to create Trigram Model for words loaded from corpus.\"\"\"\n","        print(\"Creating Trigram Model\")\n","        words = self.words\n","        triwords = []\n","        for index, item in enumerate(words):\n","            if index == len(words)-2:\n","                break\n","            triwords.append(item+' '+words[index+1]+' '+words[index+2])\n","        print(\"Calculating Count for Trigram Model\")\n","        if cache:\n","            trigram_file = open('trigram.data', 'w')\n","        trigram = Counter(triwords)\n","        if cache:\n","            trigram_file.write(str(trigram))\n","            trigram_file.close()\n","        self.trigram = trigram\n","\n","    def create_quadrigram(self, cache):\n","        \"\"\"Method to create Quadrigram Model for words loaded from corpus.\"\"\"\n","        print(\"Creating Quadrigram Model\")\n","        words = self.words\n","        quadriwords = []\n","        for index, item in enumerate(words):\n","            if index == len(words)-3:\n","                break\n","            quadriwords.append(item+' '+words[index+1]+' '+words[index+2]+' '+words[index+3])\n","        print(\"Calculating Count for Quadrigram Model\")\n","        if cache:\n","            quadrigram_file = open('fourgram.data', 'w')\n","        quadrigram = Counter(quadriwords)\n","        if cache:\n","            quadrigram_file.write(str(quadrigram))\n","            quadrigram_file.close()\n","        self.quadrigram = quadrigram\n","\n","    def create_pentigram(self, cache):\n","        \"\"\"Method to create Pentigram Model for words loaded from corpus.\"\"\"\n","        print(\"Creating pentigram Model\")\n","        words = self.words\n","        pentiwords = []\n","        for index, item in enumerate(words):\n","            if index == len(words)-4:\n","                break\n","            pentiwords.append(item+' '+words[index+1]+' '+words[index+2]+' '+words[index+3]+' '+words[index+4])\n","        print(\"Calculating Count for pentigram Model\")\n","        if cache:\n","            pentigram_file = open('pentagram.data', 'w')\n","        pentigram = Counter(pentiwords)\n","        if cache:\n","            pentigram_file.write(str(pentigram))\n","            pentigram_file.close()\n","        self.pentigram = pentigram\n","\n","    def probability(self, word, words=\"\", n=1):\n","        \"\"\"Method to calculate the Maximum Likelihood Probability of n-Grams on the basis of various parameters.\"\"\"\n","        if n == 1:\n","            return calc.log((self.unigram[word]+1)/(len(self.words)+len(self.unigram)))\n","        elif n == 2:\n","            return calc.log((self.bigram[words]+1)/(self.unigram[word]+len(self.unigram)))\n","        elif n == 3:\n","            return calc.log((self.trigram[words]+1)/(self.bigram[word]+len(self.unigram)))\n","        elif n == 4:\n","            return calc.log((self.quadrigram[words]+1)/(self.trigram[word]+len(self.unigram)))\n","        elif n == 5:\n","            return calc.log((self.pentigram[words]+1)/(self.quadrigram[word]+len(self.unigram)))\n","\n","    def sentence_probability(self, sentence, n=1):\n","        \"\"\"Method to calculate cumulative n-gram Maximum Likelihood Probability of a phrase or sentence.\"\"\"\n","        words = sentence.lower().split()\n","        P = 0\n","        if n == 1:\n","            for index, item in enumerate(words):\n","                P += self.probability(item)\n","        if n == 2:\n","            for index, item in enumerate(words):\n","                if index >= len(words) - 1:\n","                    break\n","                P += self.probability(item, item+' '+words[index+1], 2)\n","        if n == 3:\n","            for index, item in enumerate(words):\n","                if index >= len(words) - 2:\n","                    break\n","                P += self.probability(item+' '+words[index+1], item+' '+words[index+1]+' '+words[index+2], 3)\n","        if n == 4:\n","            for index, item in enumerate(words):\n","                if index >= len(words) - 3:\n","                    break\n","                P += self.probability(item+' '+words[index+1]+' '+words[index+2], item+' '+words[index+1]+' ' +\n","                                      words[index+2]+' '+words[index+3], 4)\n","        if n == 5:\n","            for index, item in enumerate(words):\n","                if index >= len(words) - 4:\n","                    break\n","                P += self.probability(item+' '+words[index+1]+' '+words[index+2]+' '+words[index+3], item+' ' +\n","                                      words[index+1]+' '+words[index+2]+' '+words[index+3]+' '+words[index+4], 5)\n","\n","        return P\n","\n","\n","if __name__ == '__main__':\n","        ng = CacululateMLEProbs(n=2, corpus_file=None, cache=True)\n","        sentence = 'what kind of movies do you like'\n","        print(ng.sentence_probability(sentence, n=2))"],"execution_count":49,"outputs":[{"output_type":"stream","name":"stdout","text":["Writing ./src/calculate_mle_probs.py\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2LkJuhQfupAs","executionInfo":{"status":"ok","timestamp":1631361616819,"user_tz":-330,"elapsed":438,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}},"outputId":"c5042fb4-9b41-4df7-febf-c15cb57a0e00"},"source":["!python ./src/calculate_mle_probs.py"],"execution_count":50,"outputs":[{"output_type":"stream","name":"stdout","text":["Loading Corpus from data file\n","Traceback (most recent call last):\n","  File \"./src/calculate_mle_probs.py\", line 180, in <module>\n","    ng = CacululateMLEProbs(n=2, corpus_file=None, cache=True)\n","  File \"./src/calculate_mle_probs.py\", line 23, in __init__\n","    self.load_corpus(corpus_file)\n","  File \"./src/calculate_mle_probs.py\", line 41, in load_corpus\n","    corpus_file = open(file_name, 'r')\n","FileNotFoundError: [Errno 2] No such file or directory: '/content/US390984/data/dialog_data/GT_corpus_tokens.txt'\n"]}]}]}