{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"T519611 | DaRE Cross-domain recommender on Amazon Reviews dataset","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyOl4i/ALmezsMj9XUTvBCTo"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"71af347650b942369ec81944117b690d":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_64c03e117862428caa0345c121dc1e65","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_8a558da37a33403882178cb4a3aef6dd","IPY_MODEL_a950962c7bbc482894dac5691960a48c","IPY_MODEL_feb69d523c2a42368eba2e6c862aed1f"]}},"64c03e117862428caa0345c121dc1e65":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"8a558da37a33403882178cb4a3aef6dd":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_cc2b25addf0a4c1a9c8397a7fa0b7a0b","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"100%","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_4f16a579d9ca49e2982612b7a69eb937"}},"a950962c7bbc482894dac5691960a48c":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_f220f3f708ca4520bf8af56ddbfcddae","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"","max":321,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":321,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_99eb423c1fbe4f5a9003b2d42a4bdfb5"}},"feb69d523c2a42368eba2e6c862aed1f":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_ef71f61de89946b09497f919da483edf","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 320/321 [13:43&lt;00:02,  2.54s/it]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_939d579f7b064b798a94a96b73826e63"}},"cc2b25addf0a4c1a9c8397a7fa0b7a0b":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"4f16a579d9ca49e2982612b7a69eb937":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"f220f3f708ca4520bf8af56ddbfcddae":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"99eb423c1fbe4f5a9003b2d42a4bdfb5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"ef71f61de89946b09497f919da483edf":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"939d579f7b064b798a94a96b73826e63":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"4f4509f59ac048e3b4066b5aa5e6aebc":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_bb6ce32472674f07ba801c01c1afd3fb","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_38015bb960f04061a49f4e2aae55b945","IPY_MODEL_13474a1fd3f74bed8efff76fbecf4a94","IPY_MODEL_7aa5e54bcece46cab79637f18a96d05f"]}},"bb6ce32472674f07ba801c01c1afd3fb":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"38015bb960f04061a49f4e2aae55b945":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_1b1826b54e824f4b8155ff7d1b5014a5","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 98%","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_ef8e674873b54bcdae218ac80261824d"}},"13474a1fd3f74bed8efff76fbecf4a94":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_d87c8408a4344c5ca33c9ea8946ff1d7","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"","max":42,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":42,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_10274b50d21047d48c056b4e20420515"}},"7aa5e54bcece46cab79637f18a96d05f":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_7b7da642f2e64aa7b895c56c816dfe92","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 41/42 [00:40&lt;00:01,  1.00s/it]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_6cfedfbd6aa740b3a0ad5b5b69266359"}},"1b1826b54e824f4b8155ff7d1b5014a5":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"ef8e674873b54bcdae218ac80261824d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"d87c8408a4344c5ca33c9ea8946ff1d7":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"10274b50d21047d48c056b4e20420515":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"7b7da642f2e64aa7b895c56c816dfe92":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"6cfedfbd6aa740b3a0ad5b5b69266359":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"278978e363ce4ad39dc9683c3a875358":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_a45ba79505e84f669d3eed41ab3ea6a1","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_4200609081684d48844888c7db2d9980","IPY_MODEL_bb4da70fa81b47e6b9a799c715dc5396","IPY_MODEL_0519e08c1d06482c8dce555b334bf0c4"]}},"a45ba79505e84f669d3eed41ab3ea6a1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"4200609081684d48844888c7db2d9980":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_5c6b77244fb34c94928f306c8b5c177f","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 98%","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_81902f8f148d4e97b30e8ddb4c7790ca"}},"bb4da70fa81b47e6b9a799c715dc5396":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_5454cb78d1504a93bff233ae47f0862e","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"","max":42,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":42,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_23f0caef6a194857ba14d024fa40ccc3"}},"0519e08c1d06482c8dce555b334bf0c4":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_3bfad4f7189c4284a879cecf44bdcb0f","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 41/42 [00:37&lt;00:00,  1.08it/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_62a692be31ac4463aaca39a729e11550"}},"5c6b77244fb34c94928f306c8b5c177f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"81902f8f148d4e97b30e8ddb4c7790ca":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"5454cb78d1504a93bff233ae47f0862e":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"23f0caef6a194857ba14d024fa40ccc3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"3bfad4f7189c4284a879cecf44bdcb0f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"62a692be31ac4463aaca39a729e11550":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"554de49e9f334bbe9e6de81e00aa6bff":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_cb19273aadd04a598ab097ecd71d881a","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_1139d0692c5442ad9e7855a922ff0008","IPY_MODEL_3ffe23dc45cc4a7095f3160b7c02ce12","IPY_MODEL_39dd7912df95465b8922ddab77a4c40b"]}},"cb19273aadd04a598ab097ecd71d881a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"1139d0692c5442ad9e7855a922ff0008":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_243da63d3ff6408792f11ec2bf88a88e","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"100%","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_dd2b38765377478a91f93799d25a6162"}},"3ffe23dc45cc4a7095f3160b7c02ce12":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_de8e36541f424b8aa64682bb0b1b5251","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"","max":321,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":321,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_d9cbd681c8394e89aa2f62c2f9fe18c2"}},"39dd7912df95465b8922ddab77a4c40b":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_72ee96fd766e4400bca24d7a45994bd2","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 320/321 [14:09&lt;00:02,  2.63s/it]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_ee5bd63317644e9dbd4e3132e6c7af4a"}},"243da63d3ff6408792f11ec2bf88a88e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"dd2b38765377478a91f93799d25a6162":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"de8e36541f424b8aa64682bb0b1b5251":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"d9cbd681c8394e89aa2f62c2f9fe18c2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"72ee96fd766e4400bca24d7a45994bd2":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"ee5bd63317644e9dbd4e3132e6c7af4a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"e01a372c721b4ad987379973cba79488":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_d6d1d0235bd8428b867438df43b760ab","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_e1fff4d24bbe4761b096ca4d2ba3118f","IPY_MODEL_d46cd04d4ffb4fc0998bca89036cf901","IPY_MODEL_70acccddbae2473c83e76af5558a9953"]}},"d6d1d0235bd8428b867438df43b760ab":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"e1fff4d24bbe4761b096ca4d2ba3118f":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_f49280fdd2634c88a1096386bc073005","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 98%","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_89ee1a0ff2354807b96f5894009b5329"}},"d46cd04d4ffb4fc0998bca89036cf901":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_4b870844c785414dbba8b6e9518cd015","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"","max":42,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":42,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_c69e47fe6b8c4372ad421f5dd62da39e"}},"70acccddbae2473c83e76af5558a9953":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_73bc130a63f140bfb6a3852b9514352a","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 41/42 [00:41&lt;00:01,  1.03s/it]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_6afe98930b594112b35cb6bd952de3d7"}},"f49280fdd2634c88a1096386bc073005":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"89ee1a0ff2354807b96f5894009b5329":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"4b870844c785414dbba8b6e9518cd015":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"c69e47fe6b8c4372ad421f5dd62da39e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"73bc130a63f140bfb6a3852b9514352a":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"6afe98930b594112b35cb6bd952de3d7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"1efeebb504b742f8b4019823650a32ef":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_c5455de90d1941aeba7116538b183d16","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_11b807095d544a79ade2293208e9ec41","IPY_MODEL_7ba1e4cd7a2745bebb2767accde223f3","IPY_MODEL_f3801bffc9a74e49992fc14ee0e01d51"]}},"c5455de90d1941aeba7116538b183d16":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"11b807095d544a79ade2293208e9ec41":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_e8c5ae46821743588463426f237a492b","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 98%","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_3a3c190733c24867b92c2114d31ed3c4"}},"7ba1e4cd7a2745bebb2767accde223f3":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_ad36ce8ecdaf4f95b28fe7035777cdd4","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"","max":42,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":42,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_8ef14db4028145d59b3311008ffac1dd"}},"f3801bffc9a74e49992fc14ee0e01d51":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_0fe5dcd76f7c4286bd9b827649e27397","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 41/42 [00:37&lt;00:00,  1.06it/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_c1682c045acd4296b48a0a76cd6b5a51"}},"e8c5ae46821743588463426f237a492b":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"3a3c190733c24867b92c2114d31ed3c4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"ad36ce8ecdaf4f95b28fe7035777cdd4":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"8ef14db4028145d59b3311008ffac1dd":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"0fe5dcd76f7c4286bd9b827649e27397":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"c1682c045acd4296b48a0a76cd6b5a51":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"571c9d9e2b5547d1af69e1d4f650d17c":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_a820912ebccc49c18991ef5ef9165373","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_fb8e04f667b649799866b17e2f169889","IPY_MODEL_6cf577f733dc4115b5d7b5f505dbe297","IPY_MODEL_5590435c86664c06ac80757bd2203d75"]}},"a820912ebccc49c18991ef5ef9165373":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"fb8e04f667b649799866b17e2f169889":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_c4ab49b8fb7a4dbb95e1f064e7a0ff11","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"100%","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_02285f8e717142be93115df03ef4c3cd"}},"6cf577f733dc4115b5d7b5f505dbe297":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_ad88f5e7de984426abd2f5cb8e8335ab","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"","max":321,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":321,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_4af93b684f684ca587e85b009301cfec"}},"5590435c86664c06ac80757bd2203d75":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_7a4e6883a10544c9a1041ed8896a3347","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 320/321 [13:45&lt;00:02,  2.61s/it]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_8b352644e4f144378b9d2b28009b6875"}},"c4ab49b8fb7a4dbb95e1f064e7a0ff11":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"02285f8e717142be93115df03ef4c3cd":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"ad88f5e7de984426abd2f5cb8e8335ab":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"4af93b684f684ca587e85b009301cfec":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"7a4e6883a10544c9a1041ed8896a3347":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"8b352644e4f144378b9d2b28009b6875":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"f54b9b78c1cd49afa93c27ee9f5dc91b":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_fa7f688d3ddb41958ac16b30a461d72f","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_4b44d5abe1f54eabbc5255581d147cc4","IPY_MODEL_4663e4aba9334486b9523efa33cd14de","IPY_MODEL_76b579d195c14d958ef4a029fe8e8103"]}},"fa7f688d3ddb41958ac16b30a461d72f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"4b44d5abe1f54eabbc5255581d147cc4":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_0c60a7e3ca7240a1b455d2055b9ff294","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"  5%","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_8e9fbd6a4ec9488cb90b5bef74e6d9e0"}},"4663e4aba9334486b9523efa33cd14de":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_08c8671698ca491789e31ecc4986dc30","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"","max":42,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":2,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_7a07ee3ec0684a74bc791e7ae3a8a993"}},"76b579d195c14d958ef4a029fe8e8103":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_0e991b9cc3f84c06b4ed7401462cd2de","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 2/42 [00:02&lt;00:44,  1.11s/it]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_1088366c3ee54635ad41bed2cbf92289"}},"0c60a7e3ca7240a1b455d2055b9ff294":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"8e9fbd6a4ec9488cb90b5bef74e6d9e0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"08c8671698ca491789e31ecc4986dc30":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"7a07ee3ec0684a74bc791e7ae3a8a993":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"0e991b9cc3f84c06b4ed7401462cd2de":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"1088366c3ee54635ad41bed2cbf92289":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}}},"cells":[{"cell_type":"markdown","metadata":{"id":"jvhe3raF5V0J"},"source":["# DaRE Cross-domain recommender on Amazon Reviews dataset"]},{"cell_type":"markdown","metadata":{"id":"UZL_MAOGcEtj"},"source":["CDR utilizes information from source domains to alleviate the cold-start problem in the target domain. Early studies adopt feature mapping technique that requires overlapped users. For example, RC-DFM applies Stacked Denoising Autoencoder (SDAE) to each domain, where the learned knowledge of the same set of users are transferred from source to target domain. To overcome the restrictive requirement of overlapped users, CDLFM and CATN employ neighbor or similar user-based feature mapping. However, this kind of cross-domain algorithm implicates defects like filtering noises or requiring duplicate users.\n","\n","## Problem Statement\n","\n","Assume two datasets, $𝐷^𝑠$ and $𝐷^𝑡$, be the information from the source and target domains, respectively. Each dataset consists of tuples, $(𝑢,𝑖,𝑦_{𝑢,𝑖}, 𝑟_{𝑢,𝑖})$ which represents an individual review $𝑟_{𝑢,𝑖}$ written by a user 𝑢 for item 𝑖 with a rating $𝑦_{𝑢,𝑖}$. The two datasets take the form of $D^s = (𝑢^s,𝑖^s,𝑦^s_{𝑢,𝑖}, 𝑟^s_{𝑢,𝑖})$ and $D^t = (𝑢^t,𝑖^t,𝑦^t_{𝑢,𝑖}, 𝑟^t_{𝑢,𝑖})$, respectively. The goal of our task is to predict an accurate rating score $y^t_{u,i}$ using $𝐷^𝑠$ and a partial set of $𝐷^t$."]},{"cell_type":"markdown","metadata":{"id":"LIfOZU8vcMrF"},"source":["## Model Architecture"]},{"cell_type":"markdown","metadata":{"id":"ItDbE2R52gDs"},"source":["<p><center><img src='_images/T519611_1.png'></center></p>"]},{"cell_type":"markdown","metadata":{"id":"3CxPWxDHcS19"},"source":["## Training Procedure\n","\n","The training phase starts with review embedding layers followed by three types of feature extractors, ${𝐹𝐸}^𝑠$, ${𝐹𝐸}^c$, and ${𝐹𝐸}^t$, named source, common, and target, for the separation of domain-specific, domain-common knowledge. Integrated with domain discriminator, three FEs are trained independently for the parallel extraction of domain-specific $𝑂^𝑠$, $𝑂^𝑡$ and domain-common knowledge $𝑂^{𝑐,𝑠}$, $𝑂^{𝑐,𝑡}$."]},{"cell_type":"markdown","metadata":{"id":"iXEwWiav2kLq"},"source":["<p><center><img src='_images/T519611_2.png'></center></p>"]},{"cell_type":"markdown","metadata":{"id":"yitawNqtcWwB"},"source":["Then, for each domain, the review encoder generates a single vector $𝐸^𝑠$, $𝐸^𝑡$ with extracted features 𝑂 by aligning them with individual review $𝐼^𝑠$, $𝐼^𝑡$. Finally, the regressor predicts an accurate rating that the user will give on an item. Here, shared parameters across two domains are common FE and a domain discriminator."]},{"cell_type":"markdown","metadata":{"id":"9hNJcUY06EXn"},"source":["## Setup"]},{"cell_type":"markdown","metadata":{"id":"_IRLprNA6FsR"},"source":["### Imports"]},{"cell_type":"code","metadata":{"id":"-DE_6QV66J5t"},"source":["import re\n","import json\n","import numpy as np\n","from string import punctuation\n","from tqdm.notebook import tqdm\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.utils.data import DataLoader\n","from torch.autograd import Function"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"v9gifAgn9x-9"},"source":["import warnings\n","warnings.filterwarnings('ignore') "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"g9nByEAv6I_l"},"source":["### Params"]},{"cell_type":"code","metadata":{"id":"ZzYyF_7T6LZG"},"source":["device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n","batch_size = 32"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"YQXExCxl51QU"},"source":["## Dataset"]},{"cell_type":"markdown","metadata":{"id":"4swld9pM8ZvD"},"source":["### Loading"]},{"cell_type":"code","metadata":{"id":"QQbU1RYS5jlj"},"source":["# !wget -q --show-progress https://anonymous.4open.science/api/repo/DaRE-9CC9/file/DaRE/Musical_Instruments.json\n","# !wget -q --show-progress https://anonymous.4open.science/api/repo/DaRE-9CC9/file/DaRE/Patio_Lawn_and_Garden.json\n","\n","!wget -q --show-progress https://github.com/sparsh-ai/coldstart-recsys/raw/main/data/DaRE/Musical_Instruments.zip\n","!unzip Musical_Instruments.zip\n","\n","!wget -q --show-progress https://github.com/sparsh-ai/coldstart-recsys/raw/main/data/DaRE/Patio_Lawn_and_Garden.zip\n","!unzip Patio_Lawn_and_Garden.zip"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lYVlwb_z5ut5","executionInfo":{"status":"ok","timestamp":1635794798318,"user_tz":-330,"elapsed":13,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}},"outputId":"ab8e536a-5e56-4bd2-9716-5a56e8be609c"},"source":["!head -1 Musical_Instruments.json"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["{\"reviewerID\": \"A2IBPI20UZIR0U\", \"asin\": \"1384719342\", \"reviewerName\": \"cassandra tu \\\"Yeah, well, that's just like, u...\", \"helpful\": [0, 0], \"reviewText\": \"Not much to write about here, but it does exactly what it's supposed to. filters out the pop sounds. now my recordings are much more crisp. it is one of the lowest prices pop filters on amazon so might as well buy it, they honestly work the same despite their pricing,\", \"overall\": 5.0, \"summary\": \"good\", \"unixReviewTime\": 1393545600, \"reviewTime\": \"02 28, 2014\"}\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_FtJHBUE8soB","executionInfo":{"status":"ok","timestamp":1635794804188,"user_tz":-330,"elapsed":5876,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}},"outputId":"479bdb92-f0a1-4f2e-d17c-23d9b3706a14"},"source":["!wget -q --show-progress https://github.com/allenai/spv2/raw/master/model/glove.6B.100d.txt.gz"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["glove.6B.100d.txt.g 100%[===================>] 128.18M   148MB/s    in 0.9s    \n"]}]},{"cell_type":"code","metadata":{"id":"d0b_JFKC8vge"},"source":["!gunzip glove.6B.100d.txt.gz"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1XsN56Eu8czS"},"source":["### Preprocessing"]},{"cell_type":"code","metadata":{"id":"34-E-Gl-8fLk"},"source":["def read_dataset(s_path, t_path):\n","    # Initialization\n","    s_dict, t_dict, w_embed = dict(), dict(), dict()\n","    s_data, t_train, t_valid, t_test = [], [], [], []\n","    len_t_data = 0\n","\n","    print('\\nProcessing Source & Target Data ... \\n')\n","\n","    f = open(s_path, 'r')\n","\n","    # Read source data and generate user & item's review dict\n","    while True:\n","        line = f.readline()\n","        if not line: break\n","\n","        # Convert str to json format\n","        line = json.loads(line)\n","\n","        try:\n","            user, item, review, rating = line['reviewerID'], line['asin'], line['reviewText'], line['overall']\n","\n","            review = review.lower()\n","            review = ''.join([c for c in review if c not in punctuation])\n","\n","        except KeyError:\n","            continue\n","\n","        s_data.append([user, item, rating])\n","\n","        if user in s_dict:\n","            s_dict[user].append([item, review])\n","        else:\n","            s_dict[user] = [[item, review]]\n","\n","        if item in s_dict:\n","            s_dict[item].append([user, review])\n","        else:\n","            s_dict[item] = [[user, review]]\n","    f.close()\n","\n","    # For the separation of train / valid / test data in a target domain\n","    f = open(t_path, 'r')\n","    while True:\n","        len_t_data += 1\n","        line = f.readline()\n","        if not line: break\n","\n","    len_train_data = int(len_t_data * 0.8)\n","    len_t_data = int(len_t_data * 0.2)\n","    f.close()\n","\n","    # Read target domain's data\n","    f = open(t_path, 'r')\n","    while True:\n","        line = f.readline()\n","        if not line: break\n","\n","        line = json.loads(line)\n","\n","        try:\n","            user, item, review, rating = line['reviewerID'], line['asin'], line['reviewText'], line['overall']\n","\n","            review = review.lower()\n","            review = ''.join([c for c in review if c not in punctuation])\n","\n","        except KeyError:\n","            continue\n","\n","        if user in t_dict and item in t_dict and len(t_valid) < len_t_data:\n","            t_valid.append([user, item, rating])\n","        else:\n","            if len(t_train) > len_train_data:\n","                break\n","\n","            t_train.append([user, item, rating])\n","\n","            if user in t_dict:\n","                t_dict[user].append([item, review])\n","            else:\n","                t_dict[user] = [[item, review]]\n","            if item in t_dict:\n","                t_dict[item].append([user, review])\n","            else:\n","                t_dict[item] = [[user, review]]\n","\n","    f.close()\n","\n","    # Split valid / test data\n","    t_test, t_valid = t_valid[int(len_t_data/2):len_t_data], t_valid[0:int(len_t_data/2)]\n","\n","    print('Size of Train / Valid / Test data  : %d / %d / %d' % (len(t_train), len(t_valid), len(t_test)))\n","\n","    # Dictionary for word embedding\n","    f = open('glove.6B.100d.txt')\n","\n","    for line in f:\n","        word_vector = line.split()\n","        word = word_vector[0]\n","        word_vector_arr = np.asarray(word_vector[1:], dtype='float32')\n","        w_embed[word] = word_vector_arr\n","\n","    f.close()\n","\n","    return s_data, s_dict, t_train, t_valid, t_test, t_dict, w_embed"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KbQgd-Eu54a1"},"source":["**Define GRL for common feature extraction**"]},{"cell_type":"code","metadata":{"id":"Sz-LpCDL6wwX"},"source":["class GradientReversalFunction(Function):\n","    @staticmethod\n","    def forward(ctx, x):\n","        ctx.lambda_ = 1\n","        return x.clone()\n","\n","    @staticmethod\n","    def backward(ctx, grads):\n","        lambda_ = 1\n","        lambda_ = grads.new_tensor(lambda_)\n","        dx = -lambda_ * grads\n","        return dx, None"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Zwqh6Hxp6qNZ"},"source":["## Model Definition"]},{"cell_type":"code","metadata":{"id":"eZHvt9CL6o18"},"source":["class DaRE(nn.Module):\n","    def __init__(self):\n","        super(DaRE, self).__init__()\n","        # Num of CNN filter, CNN filter size 5x100\n","        self.filters_num = 100\n","        self.kernel_size = 5\n","        # Word embedding dimension\n","        self.word_dim = 100\n","        # Loss for siamese encoder\n","        self.dist = nn.MSELoss()\n","\n","        self.s_user_feature_extractor = nn.Sequential(\n","            nn.Conv2d(1, self.filters_num, (self.kernel_size, self.word_dim)),\n","            nn.BatchNorm2d(self.filters_num),\n","            nn.Sigmoid(),\n","            nn.MaxPool2d((496, 1)),\n","            nn.Dropout(),\n","        )\n","\n","        self.s_item_feature_extractor = nn.Sequential(\n","            nn.Conv2d(1, self.filters_num, (self.kernel_size, self.word_dim)),\n","            nn.BatchNorm2d(self.filters_num),\n","            nn.Sigmoid(),\n","            nn.MaxPool2d((496, 1)),\n","            nn.Dropout(),\n","        )\n","\n","        self.t_user_feature_extractor = nn.Sequential(\n","            nn.Conv2d(1, self.filters_num, (self.kernel_size, self.word_dim)),\n","            nn.BatchNorm2d(self.filters_num),\n","            nn.Sigmoid(),\n","            nn.MaxPool2d((496, 1)),\n","            nn.Dropout(),\n","        )\n","\n","        self.t_item_feature_extractor = nn.Sequential(\n","            nn.Conv2d(1, self.filters_num, (self.kernel_size, self.word_dim)),\n","            nn.BatchNorm2d(self.filters_num),\n","            nn.Sigmoid(),\n","            nn.MaxPool2d((496, 1)),\n","            nn.Dropout(),\n","        )\n","\n","        self.c_user_feature_extractor = nn.Sequential(\n","            nn.Conv2d(1, self.filters_num, (self.kernel_size, self.word_dim)),\n","            nn.BatchNorm2d(self.filters_num),\n","            nn.Sigmoid(),\n","            nn.MaxPool2d((496, 1)),\n","            nn.Dropout(),\n","        )\n","\n","        self.c_item_feature_extractor = nn.Sequential(\n","            nn.Conv2d(1, self.filters_num, (self.kernel_size, self.word_dim)),\n","            nn.BatchNorm2d(self.filters_num),\n","            nn.Sigmoid(),\n","            nn.MaxPool2d((496, 1)),\n","            nn.Dropout(),\n","        )\n","\n","        self.discriminator = nn.Sequential(\n","            nn.Linear(200, 64),\n","            nn.Sigmoid(),\n","            nn.Linear(64, 1),\n","        )\n","\n","        self.s_encoder = nn.Sequential(\n","            nn.Linear(200, 200)\n","        )\n","\n","        self.s_classifier = nn.Sequential(\n","            nn.Linear(200, 32),\n","            nn.Sigmoid(),\n","            nn.Linear(32, 1),\n","        )\n","\n","        self.t_encoder = nn.Sequential(\n","            nn.Linear(200, 200)\n","        )\n","\n","        self.t_classifier = nn.Sequential(\n","            nn.Linear(200, 32),\n","            nn.Sigmoid(),\n","            nn.Linear(32, 1),\n","        )\n","\n","        self.reset_para()\n","\n","    def reset_para(self):\n","        for cnn in [self.s_user_feature_extractor[0], self.s_item_feature_extractor[0]]:\n","            nn.init.xavier_normal_(cnn.weight)\n","            nn.init.constant_(cnn.bias, 0.1)\n","\n","        for cnn in [self.t_user_feature_extractor[0], self.t_item_feature_extractor[0]]:\n","            nn.init.xavier_normal_(cnn.weight)\n","            nn.init.constant_(cnn.bias, 0.1)\n","\n","        for cnn in [self.c_user_feature_extractor[0], self.c_item_feature_extractor[0]]:\n","            nn.init.xavier_normal_(cnn.weight)\n","            nn.init.constant_(cnn.bias, 0.1)\n","\n","        for fc in [self.s_classifier[0]]:\n","            nn.init.uniform_(fc.weight, -0.1, 0.1)\n","            nn.init.constant_(fc.bias, 0.1)\n","\n","        for fc in [self.t_classifier[0]]:\n","            nn.init.uniform_(fc.weight, -0.1, 0.1)\n","            nn.init.constant_(fc.bias, 0.1)\n","\n","    def forward(self, user, item, ans, label):\n","        # Source individual review FE\n","        s_u_ans_fea = self.s_user_feature_extractor(ans).squeeze(2).squeeze(2)\n","        c_u_ans_fea = self.c_user_feature_extractor(ans).squeeze(2).squeeze(2)\n","        s_u_ans_fea = (s_u_ans_fea + c_u_ans_fea) / 2\n","\n","        s_i_ans_fea = self.s_item_feature_extractor(ans).squeeze(2).squeeze(2)\n","        c_i_ans_fea = self.c_item_feature_extractor(ans).squeeze(2).squeeze(2)\n","        s_i_ans_fea = (s_i_ans_fea + c_i_ans_fea) / 2\n","\n","        s_ans_fea = torch.cat((s_u_ans_fea, s_i_ans_fea), 1).squeeze(1)\n","\n","        # Label of source individual review\n","        s_cls_out = self.s_classifier(s_ans_fea)\n","\n","        # Output is [Source | Target] --> Masking target output for loss calculation\n","        masking = torch.cat([torch.ones(batch_size), torch.zeros(batch_size)]).view(batch_size * 2, -1).to(device)\n","        s_ans_out, s_label = torch.mul(s_cls_out, masking), torch.mul(label, masking)\n","\n","        # Source aggregated reviews FE\n","        s_u_fea = self.s_user_feature_extractor(user).squeeze(2).squeeze(2)\n","        s_i_fea = self.s_item_feature_extractor(item).squeeze(2).squeeze(2)\n","\n","        s_c_u_fea = self.c_user_feature_extractor(user).squeeze(2).squeeze(2)\n","        s_c_i_fea = self.c_item_feature_extractor(item).squeeze(2).squeeze(2)\n","\n","        s_u_fea = (s_u_fea + s_c_u_fea) / 2\n","        s_i_fea = (s_i_fea + s_c_i_fea) / 2\n","\n","        s_fea = torch.cat((s_u_fea, s_i_fea), 1).squeeze(1)\n","\n","        # Passing through encoder for aggregated review embedding\n","        s_fea = self.s_encoder(s_fea)\n","\n","        s_cls_out = self.s_classifier(s_fea)\n","        s_out = torch.mul(s_cls_out, masking)\n","\n","        # Distance between individual review & aggregated review\n","        s_dist = self.dist(torch.mul(s_ans_fea, masking), torch.mul(s_fea, masking))\n","\n","        # Same for target domain\n","        t_u_ans_fea = self.t_user_feature_extractor(ans).squeeze(2).squeeze(2)\n","        c_u_ans_fea = self.c_user_feature_extractor(ans).squeeze(2).squeeze(2)\n","        t_u_ans_fea = (t_u_ans_fea + c_u_ans_fea) / 2\n","\n","        t_i_ans_fea = self.t_item_feature_extractor(ans).squeeze(2).squeeze(2)\n","        c_i_ans_fea = self.c_item_feature_extractor(ans).squeeze(2).squeeze(2)\n","        t_i_ans_fea = (t_i_ans_fea + c_i_ans_fea) / 2\n","\n","        t_ans_fea = torch.cat((t_u_ans_fea, t_i_ans_fea), 1).squeeze(1)\n","\n","        t_cls_out = self.t_classifier(t_ans_fea)\n","\n","        masking = torch.cat([torch.zeros(batch_size), torch.ones(batch_size)]).view(batch_size * 2, -1).to(device)\n","        t_ans_out, t_label = torch.mul(t_cls_out, masking), torch.mul(label, masking)\n","\n","        # Target classification loss\n","        t_u_fea = self.t_user_feature_extractor(user).squeeze(2).squeeze(2)\n","        t_i_fea = self.t_item_feature_extractor(item).squeeze(2).squeeze(2)\n","\n","        t_c_u_fea = self.c_user_feature_extractor(user).squeeze(2).squeeze(2)\n","        t_c_i_fea = self.c_item_feature_extractor(item).squeeze(2).squeeze(2)\n","\n","        t_u_fea = (t_u_fea + t_c_u_fea) / 2\n","        t_i_fea = (t_i_fea + t_c_i_fea) / 2\n","\n","        t_fea = torch.cat((t_u_fea, t_i_fea), 1).squeeze(1)\n","\n","        t_fea = self.t_encoder(t_fea)\n","\n","        t_cls_out = self.t_classifier(t_fea)\n","        t_out = torch.mul(t_cls_out, masking)\n","\n","        t_dist = self.dist(torch.mul(t_ans_fea, masking), torch.mul(t_fea, masking))\n","\n","        # Discriminator label\n","        s_domain_specific = torch.zeros(batch_size).to(device)\n","        t_domain_specific = torch.ones(batch_size).to(device)\n","\n","        # Common source discriminator loss\n","        s_c_d_fea = torch.cat((s_c_u_fea, s_c_i_fea), 1)\n","        s_c_d_fea = GradientReversalFunction.apply(s_c_d_fea)\n","        s_c_d_fea = self.discriminator(s_c_d_fea).squeeze(1)[0:batch_size]\n","        s_c_domain_loss = F.binary_cross_entropy_with_logits(s_c_d_fea, s_domain_specific)\n","\n","        # Common target discriminator loss\n","        t_c_d_fea = torch.cat((t_c_u_fea, t_c_i_fea), 1)\n","        t_c_d_fea = GradientReversalFunction.apply(t_c_d_fea)\n","        t_c_d_fea = self.discriminator(t_c_d_fea).squeeze(1)[batch_size:batch_size * 2]\n","        t_c_domain_loss = F.binary_cross_entropy_with_logits(t_c_d_fea, t_domain_specific)\n","\n","        domain_common_loss = (s_c_domain_loss + t_c_domain_loss) / 2\n","\n","        # Source specific discriminator loss\n","        s_d_fea = torch.cat((s_u_fea, s_i_fea), 1)\n","        s_d_fea = self.discriminator(s_d_fea).squeeze(1)[0:batch_size]\n","\n","        # Target specific discriminator loss\n","        t_d_fea = torch.cat((t_u_fea, t_i_fea), 1)\n","        t_d_fea = self.discriminator(t_d_fea).squeeze(1)[batch_size:batch_size * 2]\n","\n","        s_domain_specific = torch.zeros(batch_size).to(device)\n","        s_domain_loss = F.binary_cross_entropy_with_logits(s_d_fea, s_domain_specific)\n","        t_domain_specific = torch.ones(batch_size).to(device)\n","        t_domain_loss = F.binary_cross_entropy_with_logits(t_d_fea, t_domain_specific)\n","        domain_specific_loss = (s_domain_loss + t_domain_loss) / 2\n","\n","        return s_ans_out, s_out, s_label, s_dist, t_ans_out, t_out, t_label, t_dist, domain_common_loss, domain_specific_loss"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"UVCoOitl6lhA"},"source":["## Clean strings for reviews"]},{"cell_type":"code","metadata":{"id":"WG9z1Sr26h8g"},"source":["def clean_str(string):\n","    string = re.sub(r\"[^A-Za-z0-9]\", \" \", string)\n","    string = re.sub(r\"\\'s\", \" \\'s\", string)\n","    string = re.sub(r\"\\'ve\", \" \\'ve\", string)\n","    string = re.sub(r\"n\\'t\", \" n\\'t\", string)\n","    string = re.sub(r\"\\'re\", \" \\'re\", string)\n","    string = re.sub(r\"\\'d\", \" \\'d\", string)\n","    string = re.sub(r\"\\'ll\", \" \\'ll\", string)\n","    string = re.sub(r\",\", \" , \", string)\n","    string = re.sub(r\"!\", \" ! \", string)\n","    string = re.sub(r\"\\(\", \" \\( \", string)\n","    string = re.sub(r\"\\)\", \" \\) \", string)\n","    string = re.sub(r\"\\?\", \" \\? \", string)\n","    string = re.sub(r\"\\s{2,}\", \" \", string)\n","    string = re.sub(r\"\\s{2,}\", \" \", string)\n","    string = re.sub(r\"sssss \", \" \", string)\n","\n","    return string.strip().lower()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xwe21etL6dy6"},"source":["## Review embedding layer"]},{"cell_type":"code","metadata":{"id":"rFxm4B3I6e_M"},"source":["def pre_processing(s_data, s_dict, t_data, t_dict, w_embed, valid_idx):\n","    # Return embedded vector [user, item, rev_ans, rat]\n","    u_embed, i_embed, ans_embed, label = [], [], [], []\n","    limit = 500\n","\n","    for idx in range(batch_size):\n","        u, i, rat = s_data[0][idx], s_data[1][idx], s_data[2][idx]\n","\n","        u_rev, i_rev, ans_rev = [], [], []\n","\n","        reviews = s_dict[u]\n","        for review in reviews:\n","            if review[0] != i:\n","                review = review[1].split(' ')\n","                for rev in review:\n","                    try:\n","                        rev = clean_str(rev)\n","                        rev = w_embed[rev]\n","                        u_rev.append(rev)\n","                        if len(u_rev) > limit:\n","                            break\n","                    except KeyError:\n","                        continue\n","\n","        reviews = s_dict[i]\n","        for review in reviews:\n","            if review[0] != u:\n","                review = review[1].split(' ')\n","                for rev in review:\n","                    try:\n","                        rev = clean_str(rev)\n","                        rev = w_embed[rev]\n","                        i_rev.append(rev)\n","                        if len(i_rev) > limit:\n","                            break\n","                    except KeyError:\n","                        continue\n","\n","        reviews = s_dict[u]\n","        for review in reviews:\n","            if review[0] == i:\n","                review = review[1].split(' ')\n","                for rev in review:\n","                    try:\n","                        rev = clean_str(rev)\n","                        rev = w_embed[rev]\n","                        ans_rev.append(rev)\n","                        if len(ans_rev) > limit:\n","                            break\n","                    except KeyError:\n","                        continue\n","\n","        if len(u_rev) > limit:\n","            u_rev = u_rev[0:limit]\n","        else:\n","            lis = [0.0] * 100\n","            pend = limit - len(u_rev)\n","            for p in range(pend):\n","                u_rev.append(lis)\n","\n","        if len(i_rev) > limit:\n","            i_rev = i_rev[0:limit]\n","        else:\n","            lis = [0.0] * 100\n","            pend = limit - len(i_rev)\n","            for p in range(pend):\n","                i_rev.append(lis)\n","\n","        if len(ans_rev) > limit:\n","            ans_rev = ans_rev[0:limit]\n","        else:\n","            lis = [0.0] * 100\n","            pend = limit - len(ans_rev)\n","            for p in range(pend):\n","                ans_rev.append(lis)\n","\n","        u_embed.append(u_rev)\n","        i_embed.append(i_rev)\n","        ans_embed.append(ans_rev)\n","        label.append([rat])\n","\n","    if valid_idx:\n","        u_embed = torch.tensor(u_embed, requires_grad=True).view(batch_size, 1, 500, 100).to(device)\n","        i_embed = torch.tensor(i_embed, requires_grad=True).view(batch_size, 1, 500, 100).to(device)\n","        ans_embed = torch.tensor(ans_embed, requires_grad=True).view(batch_size, 1, 500, 100).to(device)\n","        label = torch.FloatTensor(label).to(device)\n","\n","        return u_embed, i_embed, ans_embed, label\n","\n","    for idx in range(batch_size):\n","        u, i, rat = t_data[0][idx], t_data[1][idx], t_data[2][idx]\n","\n","        u_rev, i_rev, ans_rev = [], [], []\n","\n","        reviews = t_dict[u]\n","        for review in reviews:\n","            if review[0] != i:\n","                review = review[1].split(' ')\n","                for rev in review:\n","                    try:\n","                        rev = clean_str(rev)\n","                        rev = w_embed[rev]\n","                        u_rev.append(rev)\n","                        if len(u_rev) > limit:\n","                            break\n","                    except KeyError:\n","                        continue\n","\n","        reviews = t_dict[i]\n","        for review in reviews:\n","            if review[0] != u:\n","                review = review[1].split(' ')\n","                for rev in review:\n","                    try:\n","                        rev = clean_str(rev)\n","                        rev = w_embed[rev]\n","                        i_rev.append(rev)\n","                        if len(i_rev) > limit:\n","                            break\n","                    except KeyError:\n","                        continue\n","\n","        reviews = t_dict[u]\n","        for review in reviews:\n","            if review[0] == i:\n","                review = review[1].split(' ')\n","                for rev in review:\n","                    try:\n","                        rev = clean_str(rev)\n","                        rev = w_embed[rev]\n","                        ans_rev.append(rev)\n","                        if len(ans_rev) > limit:\n","                            break\n","                    except KeyError:\n","                        continue\n","\n","        if len(u_rev) > limit:\n","            u_rev = u_rev[0:limit]\n","        else:\n","            lis = [0.0] * 100\n","            pend = limit - len(u_rev)\n","            for p in range(pend):\n","                u_rev.append(lis)\n","\n","        if len(i_rev) > limit:\n","            i_rev = i_rev[0:limit]\n","        else:\n","            lis = [0.0] * 100\n","            pend = limit - len(i_rev)\n","            for p in range(pend):\n","                i_rev.append(lis)\n","\n","        if len(ans_rev) > limit:\n","            ans_rev = ans_rev[0:limit]\n","        else:\n","            lis = [0.0] * 100\n","            pend = limit - len(ans_rev)\n","            for p in range(pend):\n","                ans_rev.append(lis)\n","\n","        u_embed.append(u_rev)\n","        i_embed.append(i_rev)\n","        ans_embed.append(ans_rev)\n","        label.append([rat])\n","\n","    u_embed = torch.tensor(u_embed, requires_grad=True).view(batch_size * 2, 1, 500, 100).to(device)\n","    i_embed = torch.tensor(i_embed, requires_grad=True).view(batch_size * 2, 1, 500, 100).to(device)\n","    ans_embed = torch.tensor(ans_embed, requires_grad=True).view(batch_size * 2, 1, 500, 100).to(device)\n","    label = torch.FloatTensor(label).to(device)\n","\n","    return u_embed, i_embed, ans_embed, label"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3Lt9LqV06WWs"},"source":["## Training function"]},{"cell_type":"code","metadata":{"id":"gMnwLfpW6U9e"},"source":["def learning(s_data, s_dict, t_data, t_dict, w_embed, save, idx):\n","    # Model\n","    print('Start Training ... \\n')\n","    enc_loss_ratio, domain_loss_ratio = 0.05, 0.1\n","    model = DaRE()\n","    # After 1 epoch, load trained parameters\n","    if idx == 1:\n","        model.load_state_dict(torch.load(save, map_location=device))\n","    model.to(device)\n","    model.train()\n","\n","    criterion = nn.MSELoss()\n","\n","    optim = torch.optim.Adam(model.parameters(), lr=1e-4)\n","\n","    # Make batch\n","    batch_size = 32\n","    s_batch = DataLoader(s_data, batch_size=batch_size, shuffle=True, num_workers=2)\n","    t_batch = DataLoader(t_data, batch_size=batch_size, shuffle=True, num_workers=2)\n","\n","    batch_data, zip_size = zip(s_batch, t_batch), min(len(s_batch), len(t_batch))\n","\n","    for source_x, target_x in tqdm(batch_data, leave=False, total=zip_size):\n","        # Pre processing\n","        if len(source_x[0]) != batch_size or len(target_x[0]) != batch_size:\n","            continue\n","\n","        # Get embedding of user and item reviews\n","        u_embed, i_embed, ans_embed, label = pre_processing(source_x, s_dict, target_x, t_dict, w_embed, 0)\n","\n","        s_ans_out, s_out, s_label, s_dist, t_ans_out, t_out, t_label, t_dist, \\\n","        c_domain_loss, domain_loss = model(u_embed, i_embed, ans_embed, label)\n","\n","        # Loss\n","        s_ans_loss, s_loss = criterion(s_ans_out, s_label) * 2, criterion(s_out, s_label) * 2\n","        t_ans_loss, t_loss = criterion(t_ans_out, t_label) * 2, criterion(t_out, t_label) * 2\n","\n","        # Train\n","        loss_func = (s_loss + t_loss + s_ans_loss + t_ans_loss) / 2 + \\\n","                    (s_dist + t_dist) * enc_loss_ratio + (c_domain_loss + domain_loss) * domain_loss_ratio\n","\n","        optim.zero_grad()\n","        loss_func.backward()\n","        optim.step()\n","\n","        torch.save(model.state_dict(), save)\n","              \n","        print('Prediction Loss / Encoder Loss / Domain Loss: %.2f %.2f %.2f %.2f %.2f %.2f' %\n","              (s_loss, t_loss, s_dist, t_dist, c_domain_loss, domain_loss))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6r-C9tV56ZPK"},"source":["## Validation & Inference function"]},{"cell_type":"code","metadata":{"id":"c00Kd_KW6aIp"},"source":["def valid(v_data, t_data, t_dict, w_embed, save, write_file):\n","    model = DaRE()\n","    model.load_state_dict(torch.load(save, map_location=device))\n","    model.to(device)\n","    model.eval()\n","\n","    criterion = nn.MSELoss()\n","\n","    t_user_feature_extractor = model.t_user_feature_extractor\n","    t_item_feature_extractor = model.t_item_feature_extractor\n","    t_encoder = model.t_encoder\n","    t_clf = model.t_classifier\n","\n","    c_user_feature_extractor = model.c_user_feature_extractor\n","    c_item_feature_extractor = model.c_item_feature_extractor\n","\n","    v_batch = DataLoader(v_data, batch_size=batch_size, shuffle=True, num_workers=2)\n","    v_loss, idx = 0, 0\n","\n","    for v_data in tqdm(v_batch, leave=False):\n","        if len(v_data[0]) != batch_size:\n","            continue\n","        u_embed, i_embed, ans_embed, label = pre_processing(v_data, t_dict, v_data, t_dict, w_embed, 1)\n","\n","        with torch.no_grad():\n","            # Target rating encoder\n","            c_u_fea = c_user_feature_extractor(u_embed).squeeze(2).squeeze(2)\n","            c_i_fea = c_item_feature_extractor(i_embed).squeeze(2).squeeze(2)\n","\n","            t_u_fea = t_user_feature_extractor(u_embed).squeeze(2).squeeze(2)\n","            t_i_fea = t_item_feature_extractor(i_embed).squeeze(2).squeeze(2)\n","\n","            u_fea, i_fea = (c_u_fea + t_u_fea) / 2, (c_i_fea + t_i_fea) / 2\n","\n","            t_fea = t_encoder(torch.cat((u_fea, i_fea), 1).squeeze(1))\n","\n","            t_out = t_clf(t_fea)\n","\n","            v_loss += criterion(t_out, label)\n","        idx += 1\n","    v_loss = v_loss / idx\n","\n","    t_batch = DataLoader(t_data, batch_size=batch_size, shuffle=True, num_workers=2)\n","    t_loss, idx = 0, 0\n","\n","    for t_data in tqdm(t_batch, leave=False):\n","        if len(t_data[0]) != batch_size:\n","            continue\n","        u_embed, i_embed, ans_embed, label = pre_processing(t_data, t_dict, t_data, t_dict, w_embed, 1)\n","\n","        with torch.no_grad():\n","            # Target rating encoder\n","            c_u_fea = c_user_feature_extractor(u_embed).squeeze(2).squeeze(2)\n","            c_i_fea = c_item_feature_extractor(i_embed).squeeze(2).squeeze(2)\n","\n","            t_u_fea = t_user_feature_extractor(u_embed).squeeze(2).squeeze(2)\n","            t_i_fea = t_item_feature_extractor(i_embed).squeeze(2).squeeze(2)\n","\n","            u_fea, i_fea = (c_u_fea + t_u_fea) / 2, (c_i_fea + t_i_fea) / 2\n","\n","            t_fea = t_encoder(torch.cat((u_fea, i_fea), 1).squeeze(1))\n","\n","            t_out = t_clf(t_fea)\n","\n","            t_loss += criterion(t_out, label)\n","        idx += 1\n","\n","    t_loss = t_loss / idx\n","\n","    print('Loss: %.4f %.4f' % (v_loss, t_loss))\n","\n","    w = open(write_file, 'a')\n","    w.write('%.6f %.6f\\n' % (v_loss, t_loss))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["71af347650b942369ec81944117b690d","64c03e117862428caa0345c121dc1e65","8a558da37a33403882178cb4a3aef6dd","a950962c7bbc482894dac5691960a48c","feb69d523c2a42368eba2e6c862aed1f","cc2b25addf0a4c1a9c8397a7fa0b7a0b","4f16a579d9ca49e2982612b7a69eb937","f220f3f708ca4520bf8af56ddbfcddae","99eb423c1fbe4f5a9003b2d42a4bdfb5","ef71f61de89946b09497f919da483edf","939d579f7b064b798a94a96b73826e63","4f4509f59ac048e3b4066b5aa5e6aebc","bb6ce32472674f07ba801c01c1afd3fb","38015bb960f04061a49f4e2aae55b945","13474a1fd3f74bed8efff76fbecf4a94","7aa5e54bcece46cab79637f18a96d05f","1b1826b54e824f4b8155ff7d1b5014a5","ef8e674873b54bcdae218ac80261824d","d87c8408a4344c5ca33c9ea8946ff1d7","10274b50d21047d48c056b4e20420515","7b7da642f2e64aa7b895c56c816dfe92","6cfedfbd6aa740b3a0ad5b5b69266359","278978e363ce4ad39dc9683c3a875358","a45ba79505e84f669d3eed41ab3ea6a1","4200609081684d48844888c7db2d9980","bb4da70fa81b47e6b9a799c715dc5396","0519e08c1d06482c8dce555b334bf0c4","5c6b77244fb34c94928f306c8b5c177f","81902f8f148d4e97b30e8ddb4c7790ca","5454cb78d1504a93bff233ae47f0862e","23f0caef6a194857ba14d024fa40ccc3","3bfad4f7189c4284a879cecf44bdcb0f","62a692be31ac4463aaca39a729e11550","554de49e9f334bbe9e6de81e00aa6bff","cb19273aadd04a598ab097ecd71d881a","1139d0692c5442ad9e7855a922ff0008","3ffe23dc45cc4a7095f3160b7c02ce12","39dd7912df95465b8922ddab77a4c40b","243da63d3ff6408792f11ec2bf88a88e","dd2b38765377478a91f93799d25a6162","de8e36541f424b8aa64682bb0b1b5251","d9cbd681c8394e89aa2f62c2f9fe18c2","72ee96fd766e4400bca24d7a45994bd2","ee5bd63317644e9dbd4e3132e6c7af4a","e01a372c721b4ad987379973cba79488","d6d1d0235bd8428b867438df43b760ab","e1fff4d24bbe4761b096ca4d2ba3118f","d46cd04d4ffb4fc0998bca89036cf901","70acccddbae2473c83e76af5558a9953","f49280fdd2634c88a1096386bc073005","89ee1a0ff2354807b96f5894009b5329","4b870844c785414dbba8b6e9518cd015","c69e47fe6b8c4372ad421f5dd62da39e","73bc130a63f140bfb6a3852b9514352a","6afe98930b594112b35cb6bd952de3d7","1efeebb504b742f8b4019823650a32ef","c5455de90d1941aeba7116538b183d16","11b807095d544a79ade2293208e9ec41","7ba1e4cd7a2745bebb2767accde223f3","f3801bffc9a74e49992fc14ee0e01d51","e8c5ae46821743588463426f237a492b","3a3c190733c24867b92c2114d31ed3c4","ad36ce8ecdaf4f95b28fe7035777cdd4","8ef14db4028145d59b3311008ffac1dd","0fe5dcd76f7c4286bd9b827649e27397","c1682c045acd4296b48a0a76cd6b5a51","571c9d9e2b5547d1af69e1d4f650d17c","a820912ebccc49c18991ef5ef9165373","fb8e04f667b649799866b17e2f169889","6cf577f733dc4115b5d7b5f505dbe297","5590435c86664c06ac80757bd2203d75","c4ab49b8fb7a4dbb95e1f064e7a0ff11","02285f8e717142be93115df03ef4c3cd","ad88f5e7de984426abd2f5cb8e8335ab","4af93b684f684ca587e85b009301cfec","7a4e6883a10544c9a1041ed8896a3347","8b352644e4f144378b9d2b28009b6875","f54b9b78c1cd49afa93c27ee9f5dc91b","fa7f688d3ddb41958ac16b30a461d72f","4b44d5abe1f54eabbc5255581d147cc4","4663e4aba9334486b9523efa33cd14de","76b579d195c14d958ef4a029fe8e8103","0c60a7e3ca7240a1b455d2055b9ff294","8e9fbd6a4ec9488cb90b5bef74e6d9e0","08c8671698ca491789e31ecc4986dc30","7a07ee3ec0684a74bc791e7ae3a8a993","0e991b9cc3f84c06b4ed7401462cd2de","1088366c3ee54635ad41bed2cbf92289"]},"id":"HLWtIMWF8-Yq","outputId":"bebcb283-5e75-4667-f06a-617f09ac4615"},"source":["if __name__ == '__main__':\n","    # Define paths for source & target domain\n","    source_path = './Musical_Instruments.json'\n","    target_path = './Patio_Lawn_and_Garden.json'\n","\n","    iteration = 5\n","\n","    path = source_path[2:-5] + '_plus_' + target_path[2:-5]\n","    print('Source & Target domain: ', path)\n","\n","    save = './' + path + '.pth'\n","    write_file = './Performance_' + path + '.txt'\n","\n","    s_data, s_dict, t_train, t_valid, t_test, t_dict, w_embed = read_dataset(source_path, target_path)\n","\n","    for i in range(iteration):\n","        # After 1 epoch of training -> load trained parameter\n","        if i > 0:\n","            learning(s_data, s_dict, t_train, t_dict, w_embed, save, 1)\n","        # First training\n","        else:\n","            learning(s_data, s_dict, t_train, t_dict, w_embed, save, 0)\n","\n","        # Validation and Test\n","        valid(t_valid, t_test, t_dict, w_embed, save, write_file)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Source & Target domain:  Musical_Instruments_plus_Patio_Lawn_and_Garden\n","\n","Processing Source & Target Data ... \n","\n","Size of Train / Valid / Test data  : 10618 / 1327 / 1327\n","Start Training ... \n","\n"]},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"71af347650b942369ec81944117b690d","version_minor":0,"version_major":2},"text/plain":["  0%|          | 0/321 [00:00<?, ?it/s]"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Prediction Loss / Encoder Loss / Domain Loss: 26.19 19.32 0.90 0.98 0.70 0.69\n","Prediction Loss / Encoder Loss / Domain Loss: 23.41 18.45 0.88 0.96 0.71 0.70\n","Prediction Loss / Encoder Loss / Domain Loss: 22.63 18.95 0.89 0.95 0.70 0.70\n","Prediction Loss / Encoder Loss / Domain Loss: 25.88 19.87 0.85 0.99 0.70 0.70\n","Prediction Loss / Encoder Loss / Domain Loss: 22.18 17.75 0.85 0.94 0.70 0.70\n","Prediction Loss / Encoder Loss / Domain Loss: 23.46 19.79 0.85 0.93 0.69 0.69\n","Prediction Loss / Encoder Loss / Domain Loss: 24.17 16.33 0.83 0.93 0.70 0.70\n","Prediction Loss / Encoder Loss / Domain Loss: 22.42 17.31 0.84 0.93 0.69 0.69\n","Prediction Loss / Encoder Loss / Domain Loss: 23.93 15.30 0.82 0.91 0.69 0.69\n","Prediction Loss / Encoder Loss / Domain Loss: 23.86 18.12 0.80 0.91 0.69 0.69\n","Prediction Loss / Encoder Loss / Domain Loss: 22.18 14.11 0.81 0.89 0.70 0.70\n","Prediction Loss / Encoder Loss / Domain Loss: 21.92 15.48 0.80 0.89 0.69 0.69\n","Prediction Loss / Encoder Loss / Domain Loss: 20.06 16.45 0.81 0.89 0.70 0.70\n","Prediction Loss / Encoder Loss / Domain Loss: 22.45 17.11 0.78 0.90 0.69 0.69\n","Prediction Loss / Encoder Loss / Domain Loss: 20.94 17.29 0.77 0.87 0.69 0.70\n","Prediction Loss / Encoder Loss / Domain Loss: 20.91 16.61 0.79 0.86 0.70 0.69\n","Prediction Loss / Encoder Loss / Domain Loss: 21.18 14.18 0.79 0.87 0.70 0.70\n","Prediction Loss / Encoder Loss / Domain Loss: 19.32 14.04 0.77 0.88 0.70 0.70\n","Prediction Loss / Encoder Loss / Domain Loss: 20.00 13.79 0.76 0.85 0.69 0.69\n","Prediction Loss / Encoder Loss / Domain Loss: 19.25 14.63 0.77 0.84 0.69 0.70\n","Prediction Loss / Encoder Loss / Domain Loss: 20.34 16.40 0.75 0.84 0.69 0.69\n","Prediction Loss / Encoder Loss / Domain Loss: 21.31 15.37 0.78 0.85 0.69 0.69\n","Prediction Loss / Encoder Loss / Domain Loss: 17.34 14.22 0.77 0.85 0.70 0.70\n","Prediction Loss / Encoder Loss / Domain Loss: 18.06 15.67 0.78 0.85 0.69 0.69\n","Prediction Loss / Encoder Loss / Domain Loss: 19.77 14.69 0.75 0.86 0.69 0.69\n","Prediction Loss / Encoder Loss / Domain Loss: 19.45 12.66 0.75 0.84 0.70 0.70\n","Prediction Loss / Encoder Loss / Domain Loss: 20.59 13.38 0.75 0.84 0.69 0.69\n","Prediction Loss / Encoder Loss / Domain Loss: 17.46 12.78 0.74 0.85 0.69 0.69\n","Prediction Loss / Encoder Loss / Domain Loss: 18.28 14.84 0.76 0.85 0.69 0.70\n","Prediction Loss / Encoder Loss / Domain Loss: 17.07 13.31 0.78 0.84 0.70 0.70\n","Prediction Loss / Encoder Loss / Domain Loss: 19.17 13.59 0.77 0.85 0.69 0.69\n","Prediction Loss / Encoder Loss / Domain Loss: 17.55 13.56 0.77 0.84 0.70 0.69\n","Prediction Loss / Encoder Loss / Domain Loss: 17.99 13.02 0.79 0.87 0.70 0.69\n","Prediction Loss / Encoder Loss / Domain Loss: 17.46 13.56 0.79 0.87 0.70 0.70\n","Prediction Loss / Encoder Loss / Domain Loss: 17.96 13.23 0.78 0.85 0.69 0.69\n","Prediction Loss / Encoder Loss / Domain Loss: 16.65 12.41 0.78 0.85 0.69 0.69\n","Prediction Loss / Encoder Loss / Domain Loss: 18.07 12.43 0.78 0.89 0.69 0.69\n","Prediction Loss / Encoder Loss / Domain Loss: 18.07 12.09 0.79 0.86 0.69 0.69\n","Prediction Loss / Encoder Loss / Domain Loss: 17.30 12.74 0.79 0.88 0.69 0.69\n","Prediction Loss / Encoder Loss / Domain Loss: 17.66 12.91 0.77 0.88 0.69 0.69\n","Prediction Loss / Encoder Loss / Domain Loss: 16.05 12.10 0.81 0.86 0.69 0.69\n","Prediction Loss / Encoder Loss / Domain Loss: 17.10 11.36 0.84 0.90 0.69 0.69\n","Prediction Loss / Encoder Loss / Domain Loss: 14.91 12.35 0.82 0.89 0.69 0.69\n","Prediction Loss / Encoder Loss / Domain Loss: 15.89 10.59 0.80 0.91 0.69 0.70\n","Prediction Loss / Encoder Loss / Domain Loss: 18.01 11.06 0.83 0.88 0.70 0.70\n","Prediction Loss / Encoder Loss / Domain Loss: 15.35 11.33 0.83 0.87 0.70 0.69\n","Prediction Loss / Encoder Loss / Domain Loss: 15.54 11.90 0.83 0.91 0.70 0.69\n","Prediction Loss / Encoder Loss / Domain Loss: 15.28 10.60 0.83 0.88 0.69 0.69\n","Prediction Loss / Encoder Loss / Domain Loss: 16.92 9.99 0.82 0.92 0.69 0.69\n","Prediction Loss / Encoder Loss / Domain Loss: 15.32 9.83 0.84 0.95 0.70 0.69\n","Prediction Loss / Encoder Loss / Domain Loss: 14.64 10.82 0.84 0.91 0.69 0.69\n","Prediction Loss / Encoder Loss / Domain Loss: 15.72 10.95 0.87 0.92 0.70 0.70\n","Prediction Loss / Encoder Loss / Domain Loss: 16.45 10.12 0.87 0.93 0.70 0.70\n","Prediction Loss / Encoder Loss / Domain Loss: 15.14 10.28 0.86 0.91 0.69 0.69\n","Prediction Loss / Encoder Loss / Domain Loss: 14.65 12.22 0.86 0.92 0.69 0.69\n","Prediction Loss / Encoder Loss / Domain Loss: 14.75 10.60 0.88 0.91 0.70 0.70\n","Prediction Loss / Encoder Loss / Domain Loss: 15.21 9.65 0.86 0.94 0.70 0.69\n","Prediction Loss / Encoder Loss / Domain Loss: 16.14 9.03 0.87 0.96 0.69 0.70\n","Prediction Loss / Encoder Loss / Domain Loss: 13.92 10.14 0.89 0.95 0.69 0.69\n","Prediction Loss / Encoder Loss / Domain Loss: 15.52 10.68 0.88 0.94 0.69 0.69\n","Prediction Loss / Encoder Loss / Domain Loss: 15.40 10.07 0.89 0.95 0.69 0.69\n","Prediction Loss / Encoder Loss / Domain Loss: 15.26 9.76 0.88 0.95 0.69 0.69\n","Prediction Loss / Encoder Loss / Domain Loss: 14.70 10.39 0.91 0.98 0.70 0.70\n","Prediction Loss / Encoder Loss / Domain Loss: 13.39 9.28 0.91 0.97 0.69 0.69\n","Prediction Loss / Encoder Loss / Domain Loss: 14.68 9.87 0.91 0.95 0.69 0.69\n","Prediction Loss / Encoder Loss / Domain Loss: 15.63 8.26 0.91 0.99 0.69 0.69\n","Prediction Loss / Encoder Loss / Domain Loss: 13.14 9.65 0.95 0.98 0.69 0.69\n","Prediction Loss / Encoder Loss / Domain Loss: 14.14 9.24 0.94 1.01 0.69 0.69\n","Prediction Loss / Encoder Loss / Domain Loss: 15.22 11.08 0.92 0.98 0.70 0.69\n","Prediction Loss / Encoder Loss / Domain Loss: 14.95 9.93 0.94 1.00 0.69 0.69\n","Prediction Loss / Encoder Loss / Domain Loss: 12.20 11.14 0.96 1.00 0.69 0.69\n","Prediction Loss / Encoder Loss / Domain Loss: 13.19 9.12 0.95 1.01 0.70 0.70\n","Prediction Loss / Encoder Loss / Domain Loss: 14.07 10.03 0.96 1.01 0.69 0.69\n","Prediction Loss / Encoder Loss / Domain Loss: 13.93 8.18 0.95 1.00 0.69 0.69\n","Prediction Loss / Encoder Loss / Domain Loss: 13.81 8.93 0.94 1.04 0.70 0.69\n","Prediction Loss / Encoder Loss / Domain Loss: 13.96 9.35 0.99 1.06 0.68 0.69\n","Prediction Loss / Encoder Loss / Domain Loss: 14.31 9.75 0.96 1.04 0.69 0.69\n","Prediction Loss / Encoder Loss / Domain Loss: 12.85 9.77 0.97 1.06 0.69 0.69\n","Prediction Loss / Encoder Loss / Domain Loss: 14.61 8.85 0.99 1.06 0.69 0.69\n","Prediction Loss / Encoder Loss / Domain Loss: 13.84 8.53 0.97 1.05 0.69 0.69\n","Prediction Loss / Encoder Loss / Domain Loss: 12.51 8.09 0.97 1.05 0.69 0.69\n","Prediction Loss / Encoder Loss / Domain Loss: 12.41 7.21 1.00 1.09 0.69 0.69\n","Prediction Loss / Encoder Loss / Domain Loss: 12.55 8.58 1.00 1.07 0.69 0.69\n","Prediction Loss / Encoder Loss / Domain Loss: 14.12 8.21 0.98 1.05 0.69 0.69\n","Prediction Loss / Encoder Loss / Domain Loss: 14.25 9.96 0.98 1.05 0.70 0.69\n","Prediction Loss / Encoder Loss / Domain Loss: 14.02 7.32 0.97 1.10 0.70 0.69\n","Prediction Loss / Encoder Loss / Domain Loss: 13.33 8.29 1.00 1.05 0.69 0.69\n","Prediction Loss / Encoder Loss / Domain Loss: 13.68 8.76 0.99 1.08 0.69 0.69\n","Prediction Loss / Encoder Loss / Domain Loss: 13.31 9.12 1.01 1.08 0.69 0.69\n","Prediction Loss / Encoder Loss / Domain Loss: 13.78 8.95 1.01 1.10 0.69 0.69\n","Prediction Loss / Encoder Loss / Domain Loss: 13.91 8.41 1.01 1.09 0.70 0.70\n","Prediction Loss / Encoder Loss / Domain Loss: 13.76 9.18 1.03 1.16 0.70 0.69\n","Prediction Loss / Encoder Loss / Domain Loss: 13.69 8.34 1.02 1.11 0.69 0.70\n","Prediction Loss / Encoder Loss / Domain Loss: 13.44 8.99 0.99 1.12 0.70 0.69\n","Prediction Loss / Encoder Loss / Domain Loss: 13.39 8.73 1.02 1.11 0.70 0.70\n","Prediction Loss / Encoder Loss / Domain Loss: 13.28 7.76 1.07 1.12 0.69 0.69\n","Prediction Loss / Encoder Loss / Domain Loss: 13.99 6.93 1.07 1.13 0.70 0.69\n","Prediction Loss / Encoder Loss / Domain Loss: 13.43 7.38 1.04 1.15 0.69 0.69\n","Prediction Loss / Encoder Loss / Domain Loss: 13.18 9.46 1.07 1.10 0.69 0.69\n","Prediction Loss / Encoder Loss / Domain Loss: 13.20 8.52 1.05 1.13 0.69 0.69\n","Prediction Loss / Encoder Loss / Domain Loss: 12.88 6.39 1.06 1.14 0.69 0.69\n","Prediction Loss / Encoder Loss / Domain Loss: 14.31 7.68 1.08 1.14 0.69 0.69\n","Prediction Loss / Encoder Loss / Domain Loss: 14.47 8.75 1.07 1.16 0.69 0.69\n","Prediction Loss / Encoder Loss / Domain Loss: 11.21 8.59 1.07 1.15 0.70 0.69\n","Prediction Loss / Encoder Loss / Domain Loss: 11.93 9.08 1.06 1.19 0.69 0.69\n","Prediction Loss / Encoder Loss / Domain Loss: 13.13 7.63 1.07 1.17 0.69 0.69\n","Prediction Loss / Encoder Loss / Domain Loss: 13.51 9.24 1.08 1.19 0.69 0.69\n","Prediction Loss / Encoder Loss / Domain Loss: 12.68 7.18 1.08 1.18 0.69 0.69\n","Prediction Loss / Encoder Loss / Domain Loss: 13.35 8.94 1.10 1.18 0.69 0.69\n","Prediction Loss / Encoder Loss / Domain Loss: 12.38 8.38 1.08 1.21 0.69 0.69\n","Prediction Loss / Encoder Loss / Domain Loss: 12.87 7.75 1.08 1.21 0.69 0.69\n","Prediction Loss / Encoder Loss / Domain Loss: 12.16 6.84 1.12 1.20 0.69 0.69\n","Prediction Loss / Encoder Loss / Domain Loss: 12.21 7.08 1.09 1.19 0.69 0.69\n","Prediction Loss / Encoder Loss / Domain Loss: 13.27 8.08 1.10 1.19 0.70 0.69\n","Prediction Loss / Encoder Loss / Domain Loss: 11.31 7.00 1.11 1.21 0.69 0.69\n","Prediction Loss / Encoder Loss / Domain Loss: 13.23 8.35 1.10 1.18 0.70 0.69\n","Prediction Loss / Encoder Loss / Domain Loss: 13.55 6.46 1.11 1.22 0.70 0.70\n","Prediction Loss / Encoder Loss / Domain Loss: 11.66 7.30 1.10 1.23 0.69 0.69\n","Prediction Loss / Encoder Loss / Domain Loss: 12.86 8.17 1.11 1.21 0.69 0.69\n","Prediction Loss / Encoder Loss / Domain Loss: 12.58 5.96 1.13 1.18 0.70 0.69\n","Prediction Loss / Encoder Loss / Domain Loss: 11.10 7.73 1.12 1.23 0.69 0.69\n","Prediction Loss / Encoder Loss / Domain Loss: 14.18 9.23 1.10 1.21 0.70 0.69\n","Prediction Loss / Encoder Loss / Domain Loss: 13.19 8.29 1.13 1.22 0.70 0.69\n","Prediction Loss / Encoder Loss / Domain Loss: 12.69 7.63 1.14 1.23 0.69 0.69\n","Prediction Loss / Encoder Loss / Domain Loss: 12.60 8.57 1.13 1.23 0.69 0.69\n","Prediction Loss / Encoder Loss / Domain Loss: 12.07 7.60 1.13 1.24 0.70 0.69\n","Prediction Loss / Encoder Loss / Domain Loss: 10.73 7.31 1.16 1.21 0.69 0.69\n","Prediction Loss / Encoder Loss / Domain Loss: 12.91 8.51 1.13 1.24 0.69 0.69\n","Prediction Loss / Encoder Loss / Domain Loss: 12.46 8.23 1.14 1.22 0.69 0.69\n","Prediction Loss / Encoder Loss / Domain Loss: 12.07 6.78 1.12 1.25 0.69 0.69\n","Prediction Loss / Encoder Loss / Domain Loss: 12.73 8.29 1.15 1.24 0.69 0.69\n","Prediction Loss / Encoder Loss / Domain Loss: 13.34 7.24 1.17 1.22 0.69 0.69\n","Prediction Loss / Encoder Loss / Domain Loss: 12.37 8.07 1.14 1.26 0.69 0.69\n","Prediction Loss / Encoder Loss / Domain Loss: 12.09 6.89 1.12 1.23 0.69 0.69\n","Prediction Loss / Encoder Loss / Domain Loss: 12.16 8.47 1.18 1.25 0.69 0.69\n","Prediction Loss / Encoder Loss / Domain Loss: 11.48 8.46 1.15 1.27 0.69 0.69\n","Prediction Loss / Encoder Loss / Domain Loss: 11.47 7.77 1.17 1.25 0.69 0.69\n","Prediction Loss / Encoder Loss / Domain Loss: 10.66 7.79 1.20 1.22 0.69 0.69\n","Prediction Loss / Encoder Loss / Domain Loss: 10.29 7.02 1.17 1.28 0.69 0.69\n","Prediction Loss / Encoder Loss / Domain Loss: 11.37 7.11 1.17 1.29 0.68 0.68\n","Prediction Loss / Encoder Loss / Domain Loss: 12.16 7.55 1.18 1.26 0.70 0.69\n","Prediction Loss / Encoder Loss / Domain Loss: 13.37 7.61 1.18 1.29 0.69 0.69\n","Prediction Loss / Encoder Loss / Domain Loss: 11.58 6.96 1.18 1.27 0.69 0.69\n","Prediction Loss / Encoder Loss / Domain Loss: 12.63 7.14 1.16 1.30 0.69 0.69\n","Prediction Loss / Encoder Loss / Domain Loss: 12.39 6.91 1.19 1.27 0.69 0.69\n","Prediction Loss / Encoder Loss / Domain Loss: 11.76 7.83 1.17 1.27 0.68 0.69\n","Prediction Loss / Encoder Loss / Domain Loss: 11.52 8.41 1.22 1.32 0.70 0.69\n","Prediction Loss / Encoder Loss / Domain Loss: 11.61 7.55 1.19 1.30 0.68 0.68\n","Prediction Loss / Encoder Loss / Domain Loss: 12.28 7.66 1.22 1.26 0.69 0.68\n","Prediction Loss / Encoder Loss / Domain Loss: 12.36 7.39 1.18 1.28 0.68 0.69\n","Prediction Loss / Encoder Loss / Domain Loss: 10.15 7.98 1.23 1.26 0.70 0.69\n","Prediction Loss / Encoder Loss / Domain Loss: 12.15 7.28 1.16 1.28 0.69 0.69\n","Prediction Loss / Encoder Loss / Domain Loss: 10.39 8.09 1.16 1.30 0.69 0.69\n","Prediction Loss / Encoder Loss / Domain Loss: 11.96 7.63 1.17 1.28 0.69 0.69\n","Prediction Loss / Encoder Loss / Domain Loss: 13.01 6.18 1.20 1.32 0.70 0.69\n","Prediction Loss / Encoder Loss / Domain Loss: 11.85 6.98 1.18 1.33 0.70 0.69\n","Prediction Loss / Encoder Loss / Domain Loss: 12.40 6.63 1.20 1.32 0.70 0.69\n","Prediction Loss / Encoder Loss / Domain Loss: 10.95 7.56 1.20 1.27 0.69 0.69\n","Prediction Loss / Encoder Loss / Domain Loss: 12.09 7.95 1.21 1.27 0.70 0.69\n","Prediction Loss / Encoder Loss / Domain Loss: 12.20 7.31 1.24 1.28 0.69 0.68\n","Prediction Loss / Encoder Loss / Domain Loss: 12.69 7.65 1.22 1.28 0.70 0.69\n","Prediction Loss / Encoder Loss / Domain Loss: 11.98 6.39 1.21 1.35 0.68 0.68\n","Prediction Loss / Encoder Loss / Domain Loss: 12.04 6.63 1.19 1.32 0.69 0.68\n","Prediction Loss / Encoder Loss / Domain Loss: 10.78 5.65 1.23 1.31 0.69 0.69\n","Prediction Loss / Encoder Loss / Domain Loss: 11.71 8.12 1.23 1.30 0.69 0.69\n","Prediction Loss / Encoder Loss / Domain Loss: 12.32 7.39 1.22 1.25 0.68 0.68\n","Prediction Loss / Encoder Loss / Domain Loss: 12.64 7.33 1.21 1.31 0.69 0.69\n","Prediction Loss / Encoder Loss / Domain Loss: 9.79 6.05 1.25 1.31 0.69 0.69\n","Prediction Loss / Encoder Loss / Domain Loss: 11.17 7.73 1.23 1.32 0.68 0.69\n","Prediction Loss / Encoder Loss / Domain Loss: 11.48 5.87 1.24 1.31 0.69 0.69\n","Prediction Loss / Encoder Loss / Domain Loss: 11.85 6.21 1.22 1.28 0.69 0.68\n","Prediction Loss / Encoder Loss / Domain Loss: 10.73 8.40 1.28 1.28 0.69 0.69\n","Prediction Loss / Encoder Loss / Domain Loss: 9.91 6.48 1.22 1.30 0.68 0.68\n","Prediction Loss / Encoder Loss / Domain Loss: 11.75 7.16 1.24 1.30 0.69 0.69\n","Prediction Loss / Encoder Loss / Domain Loss: 11.51 8.69 1.23 1.30 0.69 0.69\n","Prediction Loss / Encoder Loss / Domain Loss: 10.70 7.21 1.22 1.32 0.69 0.69\n","Prediction Loss / Encoder Loss / Domain Loss: 11.76 7.78 1.22 1.28 0.69 0.68\n","Prediction Loss / Encoder Loss / Domain Loss: 10.22 6.73 1.27 1.32 0.69 0.69\n","Prediction Loss / Encoder Loss / Domain Loss: 10.62 7.58 1.23 1.31 0.69 0.68\n","Prediction Loss / Encoder Loss / Domain Loss: 11.85 7.73 1.23 1.37 0.68 0.68\n","Prediction Loss / Encoder Loss / Domain Loss: 11.01 7.00 1.21 1.31 0.69 0.68\n","Prediction Loss / Encoder Loss / Domain Loss: 10.96 7.59 1.22 1.36 0.69 0.69\n","Prediction Loss / Encoder Loss / Domain Loss: 11.68 6.91 1.25 1.31 0.69 0.69\n","Prediction Loss / Encoder Loss / Domain Loss: 11.66 7.10 1.28 1.32 0.70 0.69\n","Prediction Loss / Encoder Loss / Domain Loss: 11.97 8.53 1.25 1.29 0.69 0.69\n","Prediction Loss / Encoder Loss / Domain Loss: 10.23 6.47 1.23 1.28 0.69 0.69\n","Prediction Loss / Encoder Loss / Domain Loss: 11.36 6.55 1.23 1.32 0.70 0.69\n","Prediction Loss / Encoder Loss / Domain Loss: 11.64 5.51 1.26 1.32 0.69 0.68\n","Prediction Loss / Encoder Loss / Domain Loss: 11.78 7.56 1.23 1.32 0.69 0.68\n","Prediction Loss / Encoder Loss / Domain Loss: 11.36 7.91 1.24 1.35 0.69 0.69\n","Prediction Loss / Encoder Loss / Domain Loss: 11.34 7.03 1.23 1.36 0.69 0.69\n","Prediction Loss / Encoder Loss / Domain Loss: 11.74 6.10 1.22 1.29 0.69 0.69\n","Prediction Loss / Encoder Loss / Domain Loss: 11.94 6.76 1.25 1.35 0.69 0.68\n","Prediction Loss / Encoder Loss / Domain Loss: 10.95 6.80 1.24 1.37 0.69 0.68\n","Prediction Loss / Encoder Loss / Domain Loss: 11.40 6.67 1.28 1.34 0.70 0.69\n","Prediction Loss / Encoder Loss / Domain Loss: 11.36 6.86 1.29 1.37 0.68 0.68\n","Prediction Loss / Encoder Loss / Domain Loss: 11.83 8.11 1.22 1.35 0.69 0.68\n","Prediction Loss / Encoder Loss / Domain Loss: 11.86 6.86 1.22 1.33 0.70 0.69\n","Prediction Loss / Encoder Loss / Domain Loss: 9.19 6.73 1.29 1.34 0.69 0.68\n","Prediction Loss / Encoder Loss / Domain Loss: 12.75 6.73 1.24 1.36 0.69 0.68\n","Prediction Loss / Encoder Loss / Domain Loss: 9.98 5.54 1.24 1.35 0.69 0.69\n","Prediction Loss / Encoder Loss / Domain Loss: 10.82 5.64 1.27 1.37 0.69 0.68\n","Prediction Loss / Encoder Loss / Domain Loss: 10.81 6.69 1.26 1.34 0.69 0.69\n","Prediction Loss / Encoder Loss / Domain Loss: 11.72 7.37 1.28 1.35 0.69 0.69\n","Prediction Loss / Encoder Loss / Domain Loss: 11.13 6.80 1.26 1.34 0.69 0.69\n","Prediction Loss / Encoder Loss / Domain Loss: 9.35 7.04 1.29 1.33 0.68 0.68\n","Prediction Loss / Encoder Loss / Domain Loss: 11.49 5.77 1.24 1.40 0.68 0.68\n","Prediction Loss / Encoder Loss / Domain Loss: 10.85 6.91 1.27 1.37 0.69 0.68\n","Prediction Loss / Encoder Loss / Domain Loss: 11.73 6.50 1.25 1.43 0.68 0.68\n","Prediction Loss / Encoder Loss / Domain Loss: 10.98 6.87 1.22 1.36 0.69 0.68\n","Prediction Loss / Encoder Loss / Domain Loss: 9.77 5.95 1.22 1.35 0.70 0.69\n","Prediction Loss / Encoder Loss / Domain Loss: 11.23 6.74 1.27 1.39 0.69 0.68\n","Prediction Loss / Encoder Loss / Domain Loss: 10.89 7.48 1.27 1.33 0.69 0.68\n","Prediction Loss / Encoder Loss / Domain Loss: 10.80 6.19 1.23 1.38 0.70 0.69\n","Prediction Loss / Encoder Loss / Domain Loss: 10.87 6.55 1.27 1.40 0.69 0.68\n","Prediction Loss / Encoder Loss / Domain Loss: 10.82 6.87 1.25 1.32 0.69 0.68\n","Prediction Loss / Encoder Loss / Domain Loss: 12.05 5.98 1.23 1.38 0.69 0.68\n","Prediction Loss / Encoder Loss / Domain Loss: 10.67 6.29 1.29 1.40 0.69 0.69\n","Prediction Loss / Encoder Loss / Domain Loss: 10.11 6.42 1.25 1.39 0.69 0.69\n","Prediction Loss / Encoder Loss / Domain Loss: 10.27 7.94 1.28 1.37 0.69 0.68\n","Prediction Loss / Encoder Loss / Domain Loss: 10.67 6.47 1.30 1.37 0.69 0.69\n","Prediction Loss / Encoder Loss / Domain Loss: 9.93 6.26 1.26 1.33 0.68 0.68\n","Prediction Loss / Encoder Loss / Domain Loss: 11.08 6.09 1.32 1.38 0.68 0.68\n","Prediction Loss / Encoder Loss / Domain Loss: 10.81 7.77 1.24 1.33 0.70 0.69\n","Prediction Loss / Encoder Loss / Domain Loss: 10.56 6.49 1.27 1.37 0.69 0.69\n","Prediction Loss / Encoder Loss / Domain Loss: 10.37 7.45 1.31 1.36 0.68 0.68\n","Prediction Loss / Encoder Loss / Domain Loss: 11.50 7.36 1.32 1.35 0.69 0.68\n","Prediction Loss / Encoder Loss / Domain Loss: 10.33 6.24 1.26 1.35 0.69 0.68\n","Prediction Loss / Encoder Loss / Domain Loss: 11.60 5.58 1.28 1.37 0.69 0.69\n","Prediction Loss / Encoder Loss / Domain Loss: 12.14 5.59 1.28 1.32 0.69 0.68\n","Prediction Loss / Encoder Loss / Domain Loss: 10.84 6.83 1.27 1.40 0.68 0.68\n","Prediction Loss / Encoder Loss / Domain Loss: 11.99 5.66 1.23 1.33 0.69 0.68\n","Prediction Loss / Encoder Loss / Domain Loss: 12.05 6.58 1.24 1.35 0.68 0.67\n","Prediction Loss / Encoder Loss / Domain Loss: 11.88 5.98 1.26 1.35 0.69 0.68\n","Prediction Loss / Encoder Loss / Domain Loss: 9.20 6.33 1.28 1.37 0.69 0.68\n","Prediction Loss / Encoder Loss / Domain Loss: 10.12 6.80 1.30 1.36 0.69 0.68\n","Prediction Loss / Encoder Loss / Domain Loss: 10.81 6.14 1.31 1.37 0.69 0.69\n","Prediction Loss / Encoder Loss / Domain Loss: 11.07 5.55 1.32 1.39 0.69 0.68\n","Prediction Loss / Encoder Loss / Domain Loss: 11.49 6.92 1.27 1.38 0.70 0.69\n","Prediction Loss / Encoder Loss / Domain Loss: 11.27 6.42 1.28 1.34 0.69 0.68\n","Prediction Loss / Encoder Loss / Domain Loss: 11.51 7.41 1.28 1.39 0.69 0.68\n","Prediction Loss / Encoder Loss / Domain Loss: 11.24 5.71 1.27 1.38 0.69 0.68\n","Prediction Loss / Encoder Loss / Domain Loss: 9.85 7.26 1.22 1.34 0.69 0.68\n","Prediction Loss / Encoder Loss / Domain Loss: 11.83 5.88 1.30 1.40 0.70 0.69\n","Prediction Loss / Encoder Loss / Domain Loss: 10.92 6.57 1.27 1.36 0.69 0.69\n","Prediction Loss / Encoder Loss / Domain Loss: 10.80 6.15 1.30 1.35 0.70 0.69\n","Prediction Loss / Encoder Loss / Domain Loss: 11.19 6.58 1.27 1.33 0.68 0.68\n","Prediction Loss / Encoder Loss / Domain Loss: 11.65 6.82 1.25 1.33 0.69 0.68\n","Prediction Loss / Encoder Loss / Domain Loss: 10.51 5.66 1.29 1.31 0.69 0.68\n","Prediction Loss / Encoder Loss / Domain Loss: 10.01 6.35 1.29 1.34 0.69 0.69\n","Prediction Loss / Encoder Loss / Domain Loss: 10.69 6.55 1.25 1.35 0.69 0.68\n","Prediction Loss / Encoder Loss / Domain Loss: 10.95 6.89 1.27 1.36 0.69 0.68\n","Prediction Loss / Encoder Loss / Domain Loss: 10.80 6.76 1.28 1.35 0.69 0.68\n","Prediction Loss / Encoder Loss / Domain Loss: 10.05 6.21 1.26 1.33 0.69 0.68\n","Prediction Loss / Encoder Loss / Domain Loss: 10.17 5.48 1.22 1.38 0.69 0.68\n","Prediction Loss / Encoder Loss / Domain Loss: 10.88 5.13 1.31 1.36 0.69 0.68\n","Prediction Loss / Encoder Loss / Domain Loss: 9.66 6.92 1.25 1.36 0.68 0.68\n","Prediction Loss / Encoder Loss / Domain Loss: 9.41 6.17 1.27 1.35 0.69 0.68\n","Prediction Loss / Encoder Loss / Domain Loss: 10.92 6.23 1.27 1.38 0.69 0.68\n","Prediction Loss / Encoder Loss / Domain Loss: 9.74 5.07 1.25 1.37 0.68 0.68\n","Prediction Loss / Encoder Loss / Domain Loss: 10.69 6.84 1.33 1.33 0.70 0.68\n","Prediction Loss / Encoder Loss / Domain Loss: 9.82 5.64 1.32 1.38 0.69 0.68\n","Prediction Loss / Encoder Loss / Domain Loss: 9.99 6.63 1.25 1.37 0.69 0.68\n","Prediction Loss / Encoder Loss / Domain Loss: 11.38 7.27 1.31 1.36 0.69 0.68\n","Prediction Loss / Encoder Loss / Domain Loss: 9.46 6.19 1.29 1.35 0.69 0.68\n","Prediction Loss / Encoder Loss / Domain Loss: 10.24 6.53 1.26 1.35 0.68 0.68\n","Prediction Loss / Encoder Loss / Domain Loss: 10.63 7.06 1.28 1.32 0.68 0.68\n","Prediction Loss / Encoder Loss / Domain Loss: 10.29 6.36 1.26 1.37 0.69 0.68\n","Prediction Loss / Encoder Loss / Domain Loss: 10.05 6.48 1.29 1.36 0.69 0.68\n","Prediction Loss / Encoder Loss / Domain Loss: 8.74 5.69 1.30 1.42 0.68 0.68\n","Prediction Loss / Encoder Loss / Domain Loss: 10.90 5.53 1.26 1.43 0.68 0.68\n","Prediction Loss / Encoder Loss / Domain Loss: 11.08 5.91 1.27 1.37 0.68 0.68\n","Prediction Loss / Encoder Loss / Domain Loss: 9.70 5.73 1.25 1.37 0.68 0.68\n","Prediction Loss / Encoder Loss / Domain Loss: 10.07 6.99 1.28 1.32 0.69 0.68\n","Prediction Loss / Encoder Loss / Domain Loss: 9.74 5.71 1.28 1.39 0.69 0.68\n","Prediction Loss / Encoder Loss / Domain Loss: 10.04 5.18 1.28 1.38 0.69 0.68\n","Prediction Loss / Encoder Loss / Domain Loss: 10.56 6.41 1.31 1.38 0.69 0.69\n","Prediction Loss / Encoder Loss / Domain Loss: 9.90 5.96 1.24 1.35 0.69 0.68\n","Prediction Loss / Encoder Loss / Domain Loss: 9.18 5.65 1.25 1.40 0.69 0.68\n","Prediction Loss / Encoder Loss / Domain Loss: 10.13 6.59 1.28 1.31 0.69 0.68\n","Prediction Loss / Encoder Loss / Domain Loss: 9.74 6.55 1.24 1.36 0.68 0.67\n","Prediction Loss / Encoder Loss / Domain Loss: 9.98 5.92 1.30 1.41 0.68 0.67\n","Prediction Loss / Encoder Loss / Domain Loss: 9.18 4.12 1.29 1.39 0.70 0.68\n","Prediction Loss / Encoder Loss / Domain Loss: 10.29 5.88 1.26 1.34 0.69 0.68\n","Prediction Loss / Encoder Loss / Domain Loss: 11.29 5.65 1.27 1.35 0.68 0.67\n","Prediction Loss / Encoder Loss / Domain Loss: 9.43 6.38 1.26 1.33 0.68 0.67\n","Prediction Loss / Encoder Loss / Domain Loss: 11.15 6.08 1.26 1.39 0.68 0.68\n","Prediction Loss / Encoder Loss / Domain Loss: 10.50 6.08 1.28 1.35 0.69 0.69\n","Prediction Loss / Encoder Loss / Domain Loss: 9.84 6.83 1.28 1.33 0.69 0.68\n","Prediction Loss / Encoder Loss / Domain Loss: 10.16 5.50 1.27 1.37 0.68 0.67\n","Prediction Loss / Encoder Loss / Domain Loss: 10.46 6.68 1.25 1.38 0.69 0.68\n","Prediction Loss / Encoder Loss / Domain Loss: 10.52 5.54 1.24 1.36 0.67 0.67\n","Prediction Loss / Encoder Loss / Domain Loss: 10.64 8.08 1.25 1.32 0.68 0.67\n","Prediction Loss / Encoder Loss / Domain Loss: 10.94 6.50 1.25 1.35 0.68 0.67\n","Prediction Loss / Encoder Loss / Domain Loss: 10.80 5.36 1.25 1.31 0.70 0.69\n","Prediction Loss / Encoder Loss / Domain Loss: 9.89 5.48 1.30 1.35 0.68 0.68\n","Prediction Loss / Encoder Loss / Domain Loss: 10.65 6.07 1.28 1.36 0.68 0.67\n","Prediction Loss / Encoder Loss / Domain Loss: 10.31 6.31 1.29 1.40 0.69 0.68\n","Prediction Loss / Encoder Loss / Domain Loss: 9.99 6.38 1.23 1.34 0.68 0.67\n","Prediction Loss / Encoder Loss / Domain Loss: 9.85 5.38 1.24 1.37 0.68 0.68\n","Prediction Loss / Encoder Loss / Domain Loss: 11.49 5.48 1.25 1.34 0.68 0.67\n","Prediction Loss / Encoder Loss / Domain Loss: 10.33 6.00 1.30 1.38 0.68 0.67\n","Prediction Loss / Encoder Loss / Domain Loss: 9.68 6.67 1.29 1.29 0.69 0.68\n","Prediction Loss / Encoder Loss / Domain Loss: 10.05 5.82 1.23 1.40 0.68 0.67\n","Prediction Loss / Encoder Loss / Domain Loss: 10.29 5.33 1.28 1.32 0.69 0.68\n","Prediction Loss / Encoder Loss / Domain Loss: 9.21 4.91 1.24 1.35 0.69 0.68\n","Prediction Loss / Encoder Loss / Domain Loss: 10.65 6.88 1.20 1.34 0.70 0.69\n","Prediction Loss / Encoder Loss / Domain Loss: 10.26 6.71 1.27 1.35 0.69 0.68\n","Prediction Loss / Encoder Loss / Domain Loss: 10.12 6.42 1.27 1.33 0.69 0.68\n","Prediction Loss / Encoder Loss / Domain Loss: 10.75 7.17 1.24 1.33 0.68 0.67\n","Prediction Loss / Encoder Loss / Domain Loss: 9.78 6.50 1.29 1.35 0.68 0.67\n","Prediction Loss / Encoder Loss / Domain Loss: 9.58 5.03 1.27 1.30 0.68 0.68\n","Prediction Loss / Encoder Loss / Domain Loss: 10.08 5.81 1.24 1.32 0.68 0.68\n","Prediction Loss / Encoder Loss / Domain Loss: 10.06 4.73 1.28 1.39 0.68 0.68\n","Prediction Loss / Encoder Loss / Domain Loss: 9.49 4.73 1.22 1.39 0.69 0.68\n","Prediction Loss / Encoder Loss / Domain Loss: 9.86 5.78 1.23 1.33 0.69 0.68\n","Prediction Loss / Encoder Loss / Domain Loss: 10.47 5.21 1.27 1.35 0.68 0.67\n","Prediction Loss / Encoder Loss / Domain Loss: 9.89 6.58 1.27 1.36 0.69 0.68\n","Prediction Loss / Encoder Loss / Domain Loss: 10.01 6.22 1.24 1.37 0.68 0.68\n","Prediction Loss / Encoder Loss / Domain Loss: 9.99 6.33 1.31 1.41 0.68 0.66\n"]},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"4f4509f59ac048e3b4066b5aa5e6aebc","version_minor":0,"version_major":2},"text/plain":["  0%|          | 0/42 [00:00<?, ?it/s]"]},"metadata":{}},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"278978e363ce4ad39dc9683c3a875358","version_minor":0,"version_major":2},"text/plain":["  0%|          | 0/42 [00:00<?, ?it/s]"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Loss: 6.0532 6.0182\n","Start Training ... \n","\n"]},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"554de49e9f334bbe9e6de81e00aa6bff","version_minor":0,"version_major":2},"text/plain":["  0%|          | 0/321 [00:00<?, ?it/s]"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Prediction Loss / Encoder Loss / Domain Loss: 9.92 4.63 1.27 1.35 0.69 0.68\n","Prediction Loss / Encoder Loss / Domain Loss: 10.16 5.18 1.26 1.33 0.69 0.68\n","Prediction Loss / Encoder Loss / Domain Loss: 9.64 5.82 1.25 1.33 0.69 0.68\n","Prediction Loss / Encoder Loss / Domain Loss: 8.70 7.07 1.26 1.35 0.69 0.68\n","Prediction Loss / Encoder Loss / Domain Loss: 8.87 5.80 1.27 1.27 0.69 0.68\n","Prediction Loss / Encoder Loss / Domain Loss: 11.17 5.20 1.27 1.31 0.69 0.67\n","Prediction Loss / Encoder Loss / Domain Loss: 9.91 6.11 1.23 1.29 0.69 0.68\n","Prediction Loss / Encoder Loss / Domain Loss: 9.28 6.36 1.22 1.33 0.68 0.67\n","Prediction Loss / Encoder Loss / Domain Loss: 7.65 5.89 1.22 1.28 0.68 0.67\n","Prediction Loss / Encoder Loss / Domain Loss: 7.82 6.34 1.15 1.28 0.68 0.68\n","Prediction Loss / Encoder Loss / Domain Loss: 8.93 4.26 1.21 1.25 0.69 0.67\n","Prediction Loss / Encoder Loss / Domain Loss: 8.86 5.24 1.19 1.31 0.68 0.67\n","Prediction Loss / Encoder Loss / Domain Loss: 9.16 5.43 1.17 1.25 0.68 0.67\n","Prediction Loss / Encoder Loss / Domain Loss: 9.09 5.34 1.15 1.26 0.69 0.68\n","Prediction Loss / Encoder Loss / Domain Loss: 9.14 5.20 1.15 1.22 0.68 0.67\n","Prediction Loss / Encoder Loss / Domain Loss: 8.26 6.09 1.16 1.26 0.67 0.67\n","Prediction Loss / Encoder Loss / Domain Loss: 8.19 5.78 1.14 1.19 0.68 0.68\n","Prediction Loss / Encoder Loss / Domain Loss: 9.42 4.51 1.12 1.22 0.67 0.67\n","Prediction Loss / Encoder Loss / Domain Loss: 9.47 4.67 1.12 1.16 0.69 0.67\n","Prediction Loss / Encoder Loss / Domain Loss: 9.15 6.94 1.08 1.21 0.68 0.67\n","Prediction Loss / Encoder Loss / Domain Loss: 8.59 6.58 1.07 1.08 0.68 0.67\n","Prediction Loss / Encoder Loss / Domain Loss: 9.26 5.97 1.04 1.11 0.68 0.67\n","Prediction Loss / Encoder Loss / Domain Loss: 10.17 5.90 1.04 1.10 0.69 0.67\n","Prediction Loss / Encoder Loss / Domain Loss: 10.10 6.19 1.05 1.09 0.68 0.68\n","Prediction Loss / Encoder Loss / Domain Loss: 8.98 6.01 1.00 1.09 0.68 0.67\n","Prediction Loss / Encoder Loss / Domain Loss: 10.26 4.66 1.00 1.04 0.70 0.68\n","Prediction Loss / Encoder Loss / Domain Loss: 7.67 6.74 1.01 1.08 0.68 0.67\n","Prediction Loss / Encoder Loss / Domain Loss: 9.81 5.43 1.01 1.08 0.68 0.67\n","Prediction Loss / Encoder Loss / Domain Loss: 10.47 5.23 0.94 1.06 0.66 0.66\n","Prediction Loss / Encoder Loss / Domain Loss: 9.06 5.67 0.96 0.99 0.69 0.68\n","Prediction Loss / Encoder Loss / Domain Loss: 9.96 5.88 0.93 1.03 0.68 0.67\n","Prediction Loss / Encoder Loss / Domain Loss: 9.70 5.62 0.92 0.95 0.69 0.68\n","Prediction Loss / Encoder Loss / Domain Loss: 10.42 6.35 0.92 0.99 0.68 0.67\n","Prediction Loss / Encoder Loss / Domain Loss: 8.95 6.23 0.92 0.98 0.68 0.67\n","Prediction Loss / Encoder Loss / Domain Loss: 9.86 5.17 0.90 0.99 0.70 0.68\n","Prediction Loss / Encoder Loss / Domain Loss: 9.42 4.73 0.91 0.98 0.68 0.67\n","Prediction Loss / Encoder Loss / Domain Loss: 9.34 5.61 0.87 0.93 0.68 0.67\n","Prediction Loss / Encoder Loss / Domain Loss: 9.88 6.44 0.89 0.92 0.68 0.67\n","Prediction Loss / Encoder Loss / Domain Loss: 9.32 4.90 0.86 0.94 0.69 0.67\n","Prediction Loss / Encoder Loss / Domain Loss: 8.27 5.45 0.83 0.92 0.68 0.67\n","Prediction Loss / Encoder Loss / Domain Loss: 9.85 6.09 0.81 0.88 0.69 0.67\n","Prediction Loss / Encoder Loss / Domain Loss: 8.51 6.59 0.85 0.86 0.68 0.67\n","Prediction Loss / Encoder Loss / Domain Loss: 8.80 5.74 0.81 0.86 0.68 0.67\n","Prediction Loss / Encoder Loss / Domain Loss: 8.60 6.34 0.81 0.87 0.69 0.68\n","Prediction Loss / Encoder Loss / Domain Loss: 8.78 5.66 0.81 0.83 0.69 0.67\n","Prediction Loss / Encoder Loss / Domain Loss: 9.57 6.82 0.76 0.84 0.69 0.68\n","Prediction Loss / Encoder Loss / Domain Loss: 8.89 6.03 0.78 0.84 0.68 0.66\n","Prediction Loss / Encoder Loss / Domain Loss: 8.21 4.44 0.78 0.84 0.69 0.67\n","Prediction Loss / Encoder Loss / Domain Loss: 9.78 6.12 0.77 0.82 0.69 0.67\n","Prediction Loss / Encoder Loss / Domain Loss: 9.53 5.30 0.75 0.80 0.67 0.66\n","Prediction Loss / Encoder Loss / Domain Loss: 9.87 5.98 0.72 0.80 0.69 0.67\n","Prediction Loss / Encoder Loss / Domain Loss: 9.03 5.28 0.77 0.79 0.69 0.67\n","Prediction Loss / Encoder Loss / Domain Loss: 9.25 6.12 0.73 0.77 0.69 0.67\n","Prediction Loss / Encoder Loss / Domain Loss: 8.28 4.88 0.76 0.79 0.68 0.67\n","Prediction Loss / Encoder Loss / Domain Loss: 8.94 5.48 0.72 0.77 0.69 0.67\n","Prediction Loss / Encoder Loss / Domain Loss: 9.05 5.33 0.71 0.77 0.68 0.67\n","Prediction Loss / Encoder Loss / Domain Loss: 9.51 5.69 0.70 0.75 0.69 0.68\n","Prediction Loss / Encoder Loss / Domain Loss: 8.50 5.75 0.72 0.74 0.68 0.67\n","Prediction Loss / Encoder Loss / Domain Loss: 9.24 6.00 0.69 0.74 0.68 0.66\n","Prediction Loss / Encoder Loss / Domain Loss: 9.60 5.37 0.68 0.72 0.68 0.66\n","Prediction Loss / Encoder Loss / Domain Loss: 9.40 6.01 0.66 0.69 0.69 0.67\n","Prediction Loss / Encoder Loss / Domain Loss: 8.37 6.85 0.69 0.71 0.68 0.66\n","Prediction Loss / Encoder Loss / Domain Loss: 9.39 6.63 0.65 0.70 0.69 0.66\n","Prediction Loss / Encoder Loss / Domain Loss: 9.56 6.13 0.65 0.70 0.69 0.68\n","Prediction Loss / Encoder Loss / Domain Loss: 8.95 5.62 0.66 0.70 0.69 0.67\n","Prediction Loss / Encoder Loss / Domain Loss: 9.23 4.53 0.66 0.70 0.69 0.67\n","Prediction Loss / Encoder Loss / Domain Loss: 7.99 6.17 0.66 0.67 0.68 0.66\n","Prediction Loss / Encoder Loss / Domain Loss: 9.76 5.78 0.64 0.67 0.68 0.66\n","Prediction Loss / Encoder Loss / Domain Loss: 7.55 5.16 0.63 0.68 0.70 0.68\n","Prediction Loss / Encoder Loss / Domain Loss: 8.11 6.31 0.66 0.66 0.69 0.68\n","Prediction Loss / Encoder Loss / Domain Loss: 8.65 4.78 0.60 0.66 0.68 0.66\n","Prediction Loss / Encoder Loss / Domain Loss: 9.54 5.18 0.62 0.63 0.67 0.66\n","Prediction Loss / Encoder Loss / Domain Loss: 8.51 6.67 0.60 0.64 0.67 0.67\n","Prediction Loss / Encoder Loss / Domain Loss: 10.34 5.75 0.63 0.65 0.69 0.67\n","Prediction Loss / Encoder Loss / Domain Loss: 9.31 6.23 0.59 0.63 0.68 0.66\n","Prediction Loss / Encoder Loss / Domain Loss: 9.49 5.76 0.59 0.64 0.68 0.66\n","Prediction Loss / Encoder Loss / Domain Loss: 9.42 4.60 0.58 0.61 0.70 0.67\n","Prediction Loss / Encoder Loss / Domain Loss: 9.53 4.85 0.59 0.60 0.68 0.66\n","Prediction Loss / Encoder Loss / Domain Loss: 8.47 5.42 0.57 0.63 0.68 0.66\n","Prediction Loss / Encoder Loss / Domain Loss: 9.10 5.15 0.59 0.60 0.70 0.67\n","Prediction Loss / Encoder Loss / Domain Loss: 9.90 4.95 0.56 0.60 0.70 0.67\n","Prediction Loss / Encoder Loss / Domain Loss: 9.02 5.13 0.58 0.58 0.68 0.67\n","Prediction Loss / Encoder Loss / Domain Loss: 9.31 5.81 0.56 0.60 0.66 0.65\n","Prediction Loss / Encoder Loss / Domain Loss: 7.89 5.60 0.57 0.60 0.67 0.66\n","Prediction Loss / Encoder Loss / Domain Loss: 8.35 4.38 0.56 0.59 0.70 0.68\n","Prediction Loss / Encoder Loss / Domain Loss: 9.57 4.77 0.57 0.61 0.67 0.66\n","Prediction Loss / Encoder Loss / Domain Loss: 9.96 4.91 0.55 0.57 0.67 0.66\n","Prediction Loss / Encoder Loss / Domain Loss: 9.20 4.95 0.55 0.57 0.68 0.67\n","Prediction Loss / Encoder Loss / Domain Loss: 8.42 5.44 0.54 0.58 0.68 0.66\n","Prediction Loss / Encoder Loss / Domain Loss: 9.41 6.07 0.54 0.55 0.68 0.67\n","Prediction Loss / Encoder Loss / Domain Loss: 9.33 5.45 0.56 0.55 0.70 0.67\n","Prediction Loss / Encoder Loss / Domain Loss: 9.15 4.55 0.54 0.55 0.69 0.66\n","Prediction Loss / Encoder Loss / Domain Loss: 8.40 4.69 0.53 0.54 0.69 0.67\n","Prediction Loss / Encoder Loss / Domain Loss: 8.95 4.89 0.52 0.53 0.69 0.66\n","Prediction Loss / Encoder Loss / Domain Loss: 7.97 5.79 0.52 0.53 0.69 0.68\n","Prediction Loss / Encoder Loss / Domain Loss: 8.69 5.54 0.52 0.54 0.70 0.67\n","Prediction Loss / Encoder Loss / Domain Loss: 8.76 4.91 0.51 0.54 0.67 0.65\n","Prediction Loss / Encoder Loss / Domain Loss: 8.81 5.44 0.52 0.54 0.69 0.67\n","Prediction Loss / Encoder Loss / Domain Loss: 7.93 4.63 0.53 0.54 0.68 0.67\n","Prediction Loss / Encoder Loss / Domain Loss: 7.79 6.12 0.52 0.51 0.69 0.66\n","Prediction Loss / Encoder Loss / Domain Loss: 8.59 4.58 0.52 0.52 0.68 0.66\n","Prediction Loss / Encoder Loss / Domain Loss: 8.59 5.10 0.52 0.52 0.68 0.66\n","Prediction Loss / Encoder Loss / Domain Loss: 9.44 4.85 0.51 0.51 0.68 0.66\n","Prediction Loss / Encoder Loss / Domain Loss: 7.94 5.36 0.50 0.52 0.67 0.66\n","Prediction Loss / Encoder Loss / Domain Loss: 10.24 5.31 0.49 0.53 0.69 0.67\n","Prediction Loss / Encoder Loss / Domain Loss: 9.25 4.37 0.50 0.50 0.68 0.67\n","Prediction Loss / Encoder Loss / Domain Loss: 8.21 4.97 0.48 0.50 0.70 0.67\n","Prediction Loss / Encoder Loss / Domain Loss: 8.89 4.77 0.47 0.52 0.69 0.67\n","Prediction Loss / Encoder Loss / Domain Loss: 8.76 5.18 0.48 0.51 0.69 0.67\n","Prediction Loss / Encoder Loss / Domain Loss: 9.33 4.51 0.48 0.50 0.67 0.65\n","Prediction Loss / Encoder Loss / Domain Loss: 8.99 5.54 0.48 0.50 0.67 0.66\n","Prediction Loss / Encoder Loss / Domain Loss: 8.18 4.96 0.47 0.50 0.69 0.67\n","Prediction Loss / Encoder Loss / Domain Loss: 8.04 5.21 0.48 0.49 0.68 0.66\n","Prediction Loss / Encoder Loss / Domain Loss: 8.30 5.17 0.48 0.49 0.68 0.66\n","Prediction Loss / Encoder Loss / Domain Loss: 8.88 4.69 0.46 0.50 0.69 0.67\n","Prediction Loss / Encoder Loss / Domain Loss: 9.54 4.83 0.47 0.48 0.68 0.66\n","Prediction Loss / Encoder Loss / Domain Loss: 8.51 5.38 0.48 0.49 0.67 0.65\n","Prediction Loss / Encoder Loss / Domain Loss: 8.95 4.45 0.46 0.48 0.68 0.66\n","Prediction Loss / Encoder Loss / Domain Loss: 8.97 4.51 0.44 0.47 0.69 0.67\n","Prediction Loss / Encoder Loss / Domain Loss: 8.82 4.53 0.47 0.48 0.68 0.67\n","Prediction Loss / Encoder Loss / Domain Loss: 8.75 4.99 0.46 0.48 0.69 0.67\n","Prediction Loss / Encoder Loss / Domain Loss: 8.10 4.45 0.46 0.46 0.68 0.66\n","Prediction Loss / Encoder Loss / Domain Loss: 8.41 4.63 0.45 0.47 0.69 0.67\n","Prediction Loss / Encoder Loss / Domain Loss: 8.73 4.32 0.47 0.45 0.69 0.67\n","Prediction Loss / Encoder Loss / Domain Loss: 8.20 5.99 0.45 0.46 0.68 0.66\n","Prediction Loss / Encoder Loss / Domain Loss: 8.09 5.23 0.46 0.46 0.66 0.65\n","Prediction Loss / Encoder Loss / Domain Loss: 8.88 4.48 0.46 0.46 0.69 0.66\n","Prediction Loss / Encoder Loss / Domain Loss: 9.15 5.31 0.46 0.44 0.68 0.66\n","Prediction Loss / Encoder Loss / Domain Loss: 9.64 5.92 0.44 0.45 0.67 0.66\n","Prediction Loss / Encoder Loss / Domain Loss: 7.88 4.96 0.46 0.46 0.69 0.66\n","Prediction Loss / Encoder Loss / Domain Loss: 9.45 4.61 0.45 0.46 0.68 0.66\n","Prediction Loss / Encoder Loss / Domain Loss: 7.85 5.31 0.44 0.44 0.67 0.66\n","Prediction Loss / Encoder Loss / Domain Loss: 8.91 4.48 0.47 0.45 0.68 0.66\n","Prediction Loss / Encoder Loss / Domain Loss: 8.89 4.81 0.43 0.44 0.69 0.66\n","Prediction Loss / Encoder Loss / Domain Loss: 8.64 4.09 0.43 0.43 0.70 0.67\n","Prediction Loss / Encoder Loss / Domain Loss: 8.11 5.13 0.44 0.45 0.68 0.66\n","Prediction Loss / Encoder Loss / Domain Loss: 7.24 4.84 0.45 0.44 0.69 0.67\n","Prediction Loss / Encoder Loss / Domain Loss: 8.35 5.15 0.42 0.44 0.69 0.66\n","Prediction Loss / Encoder Loss / Domain Loss: 8.73 3.46 0.45 0.45 0.68 0.65\n","Prediction Loss / Encoder Loss / Domain Loss: 9.01 4.88 0.42 0.45 0.68 0.66\n","Prediction Loss / Encoder Loss / Domain Loss: 7.63 5.08 0.44 0.43 0.70 0.66\n","Prediction Loss / Encoder Loss / Domain Loss: 7.95 5.17 0.43 0.43 0.70 0.66\n","Prediction Loss / Encoder Loss / Domain Loss: 9.17 5.37 0.45 0.44 0.70 0.66\n","Prediction Loss / Encoder Loss / Domain Loss: 8.98 3.78 0.44 0.42 0.69 0.67\n","Prediction Loss / Encoder Loss / Domain Loss: 9.19 3.81 0.44 0.42 0.70 0.67\n","Prediction Loss / Encoder Loss / Domain Loss: 8.18 4.28 0.43 0.42 0.69 0.67\n","Prediction Loss / Encoder Loss / Domain Loss: 8.38 4.76 0.43 0.43 0.68 0.65\n","Prediction Loss / Encoder Loss / Domain Loss: 7.57 5.12 0.43 0.43 0.68 0.65\n","Prediction Loss / Encoder Loss / Domain Loss: 9.50 5.07 0.42 0.44 0.69 0.66\n","Prediction Loss / Encoder Loss / Domain Loss: 8.63 5.05 0.43 0.42 0.72 0.68\n","Prediction Loss / Encoder Loss / Domain Loss: 8.61 5.34 0.41 0.44 0.69 0.65\n","Prediction Loss / Encoder Loss / Domain Loss: 7.38 4.63 0.42 0.42 0.69 0.66\n","Prediction Loss / Encoder Loss / Domain Loss: 7.23 4.97 0.41 0.42 0.69 0.67\n","Prediction Loss / Encoder Loss / Domain Loss: 7.86 4.45 0.42 0.42 0.69 0.66\n","Prediction Loss / Encoder Loss / Domain Loss: 8.33 5.09 0.40 0.43 0.69 0.67\n","Prediction Loss / Encoder Loss / Domain Loss: 8.76 4.66 0.41 0.41 0.70 0.67\n","Prediction Loss / Encoder Loss / Domain Loss: 8.30 5.14 0.41 0.41 0.71 0.68\n","Prediction Loss / Encoder Loss / Domain Loss: 7.96 4.54 0.40 0.41 0.68 0.65\n","Prediction Loss / Encoder Loss / Domain Loss: 8.62 5.00 0.42 0.40 0.68 0.65\n","Prediction Loss / Encoder Loss / Domain Loss: 6.93 4.56 0.40 0.42 0.69 0.66\n","Prediction Loss / Encoder Loss / Domain Loss: 8.08 4.39 0.42 0.43 0.68 0.65\n","Prediction Loss / Encoder Loss / Domain Loss: 8.24 4.52 0.40 0.41 0.68 0.66\n","Prediction Loss / Encoder Loss / Domain Loss: 7.59 5.05 0.40 0.41 0.68 0.65\n","Prediction Loss / Encoder Loss / Domain Loss: 8.57 4.60 0.42 0.41 0.67 0.65\n","Prediction Loss / Encoder Loss / Domain Loss: 7.18 5.88 0.40 0.39 0.70 0.66\n","Prediction Loss / Encoder Loss / Domain Loss: 8.37 5.56 0.41 0.40 0.67 0.65\n","Prediction Loss / Encoder Loss / Domain Loss: 8.87 3.93 0.39 0.40 0.69 0.65\n","Prediction Loss / Encoder Loss / Domain Loss: 8.65 3.68 0.41 0.39 0.69 0.65\n","Prediction Loss / Encoder Loss / Domain Loss: 8.51 4.10 0.40 0.40 0.68 0.65\n","Prediction Loss / Encoder Loss / Domain Loss: 7.39 3.65 0.41 0.40 0.69 0.66\n","Prediction Loss / Encoder Loss / Domain Loss: 8.01 4.99 0.40 0.39 0.67 0.65\n","Prediction Loss / Encoder Loss / Domain Loss: 8.53 4.73 0.41 0.40 0.68 0.66\n","Prediction Loss / Encoder Loss / Domain Loss: 8.08 4.32 0.40 0.40 0.67 0.64\n","Prediction Loss / Encoder Loss / Domain Loss: 8.95 5.64 0.39 0.38 0.69 0.66\n","Prediction Loss / Encoder Loss / Domain Loss: 7.88 4.91 0.40 0.40 0.67 0.65\n","Prediction Loss / Encoder Loss / Domain Loss: 7.08 4.73 0.40 0.39 0.69 0.66\n","Prediction Loss / Encoder Loss / Domain Loss: 9.09 4.53 0.40 0.39 0.69 0.66\n","Prediction Loss / Encoder Loss / Domain Loss: 6.96 5.19 0.39 0.40 0.68 0.65\n","Prediction Loss / Encoder Loss / Domain Loss: 8.33 3.36 0.40 0.40 0.67 0.65\n","Prediction Loss / Encoder Loss / Domain Loss: 8.79 3.07 0.40 0.39 0.68 0.65\n","Prediction Loss / Encoder Loss / Domain Loss: 8.36 4.01 0.40 0.39 0.71 0.66\n","Prediction Loss / Encoder Loss / Domain Loss: 7.99 4.62 0.39 0.40 0.70 0.66\n","Prediction Loss / Encoder Loss / Domain Loss: 7.54 4.86 0.38 0.39 0.70 0.66\n","Prediction Loss / Encoder Loss / Domain Loss: 8.52 4.56 0.40 0.39 0.70 0.66\n","Prediction Loss / Encoder Loss / Domain Loss: 7.26 4.67 0.39 0.38 0.71 0.67\n","Prediction Loss / Encoder Loss / Domain Loss: 7.97 5.09 0.41 0.39 0.69 0.66\n","Prediction Loss / Encoder Loss / Domain Loss: 7.95 4.42 0.39 0.38 0.68 0.65\n","Prediction Loss / Encoder Loss / Domain Loss: 6.82 4.27 0.41 0.39 0.71 0.66\n","Prediction Loss / Encoder Loss / Domain Loss: 6.12 4.85 0.39 0.39 0.68 0.64\n","Prediction Loss / Encoder Loss / Domain Loss: 7.82 4.67 0.40 0.39 0.68 0.65\n","Prediction Loss / Encoder Loss / Domain Loss: 8.56 4.25 0.40 0.38 0.68 0.65\n","Prediction Loss / Encoder Loss / Domain Loss: 7.23 5.13 0.39 0.38 0.69 0.65\n","Prediction Loss / Encoder Loss / Domain Loss: 7.96 3.71 0.41 0.38 0.67 0.64\n","Prediction Loss / Encoder Loss / Domain Loss: 6.94 4.53 0.39 0.39 0.69 0.66\n","Prediction Loss / Encoder Loss / Domain Loss: 8.36 4.21 0.39 0.39 0.71 0.67\n","Prediction Loss / Encoder Loss / Domain Loss: 7.89 4.56 0.39 0.38 0.70 0.66\n","Prediction Loss / Encoder Loss / Domain Loss: 7.90 4.19 0.38 0.39 0.70 0.67\n","Prediction Loss / Encoder Loss / Domain Loss: 7.98 4.32 0.39 0.38 0.70 0.66\n","Prediction Loss / Encoder Loss / Domain Loss: 8.15 4.53 0.40 0.39 0.68 0.65\n","Prediction Loss / Encoder Loss / Domain Loss: 7.96 4.16 0.39 0.39 0.69 0.66\n","Prediction Loss / Encoder Loss / Domain Loss: 7.57 4.21 0.39 0.39 0.68 0.64\n","Prediction Loss / Encoder Loss / Domain Loss: 7.71 4.73 0.39 0.40 0.68 0.65\n","Prediction Loss / Encoder Loss / Domain Loss: 7.33 4.52 0.37 0.38 0.69 0.66\n","Prediction Loss / Encoder Loss / Domain Loss: 8.11 4.32 0.38 0.38 0.70 0.65\n","Prediction Loss / Encoder Loss / Domain Loss: 8.12 3.94 0.37 0.38 0.67 0.65\n","Prediction Loss / Encoder Loss / Domain Loss: 8.18 4.87 0.38 0.37 0.68 0.65\n","Prediction Loss / Encoder Loss / Domain Loss: 8.00 3.55 0.38 0.38 0.68 0.65\n","Prediction Loss / Encoder Loss / Domain Loss: 7.33 4.51 0.37 0.37 0.70 0.66\n","Prediction Loss / Encoder Loss / Domain Loss: 8.58 4.90 0.38 0.37 0.69 0.66\n","Prediction Loss / Encoder Loss / Domain Loss: 7.70 4.97 0.38 0.39 0.69 0.64\n","Prediction Loss / Encoder Loss / Domain Loss: 7.59 4.57 0.37 0.37 0.69 0.65\n","Prediction Loss / Encoder Loss / Domain Loss: 7.13 4.37 0.37 0.38 0.68 0.66\n","Prediction Loss / Encoder Loss / Domain Loss: 7.68 4.46 0.39 0.37 0.69 0.66\n","Prediction Loss / Encoder Loss / Domain Loss: 6.89 4.78 0.37 0.38 0.68 0.65\n","Prediction Loss / Encoder Loss / Domain Loss: 8.28 5.49 0.39 0.36 0.68 0.64\n","Prediction Loss / Encoder Loss / Domain Loss: 7.30 4.99 0.38 0.37 0.69 0.66\n","Prediction Loss / Encoder Loss / Domain Loss: 7.69 4.63 0.37 0.37 0.67 0.64\n","Prediction Loss / Encoder Loss / Domain Loss: 8.24 4.80 0.37 0.37 0.68 0.65\n","Prediction Loss / Encoder Loss / Domain Loss: 8.23 4.06 0.38 0.37 0.68 0.64\n","Prediction Loss / Encoder Loss / Domain Loss: 8.11 4.53 0.38 0.37 0.68 0.65\n","Prediction Loss / Encoder Loss / Domain Loss: 8.70 5.19 0.37 0.37 0.69 0.65\n","Prediction Loss / Encoder Loss / Domain Loss: 6.80 4.56 0.38 0.38 0.69 0.65\n","Prediction Loss / Encoder Loss / Domain Loss: 7.88 4.33 0.36 0.35 0.69 0.65\n","Prediction Loss / Encoder Loss / Domain Loss: 7.69 4.01 0.38 0.39 0.67 0.63\n","Prediction Loss / Encoder Loss / Domain Loss: 7.47 5.28 0.39 0.37 0.67 0.64\n","Prediction Loss / Encoder Loss / Domain Loss: 7.18 4.18 0.38 0.38 0.69 0.65\n","Prediction Loss / Encoder Loss / Domain Loss: 7.96 4.32 0.37 0.39 0.68 0.64\n","Prediction Loss / Encoder Loss / Domain Loss: 8.35 4.06 0.37 0.38 0.69 0.66\n","Prediction Loss / Encoder Loss / Domain Loss: 8.07 4.45 0.38 0.38 0.67 0.64\n","Prediction Loss / Encoder Loss / Domain Loss: 8.88 4.29 0.36 0.39 0.68 0.64\n","Prediction Loss / Encoder Loss / Domain Loss: 7.03 4.62 0.38 0.37 0.70 0.66\n","Prediction Loss / Encoder Loss / Domain Loss: 7.78 4.18 0.36 0.37 0.71 0.67\n","Prediction Loss / Encoder Loss / Domain Loss: 7.71 3.93 0.37 0.39 0.68 0.65\n","Prediction Loss / Encoder Loss / Domain Loss: 7.08 4.75 0.39 0.38 0.70 0.65\n","Prediction Loss / Encoder Loss / Domain Loss: 7.26 4.25 0.38 0.38 0.69 0.65\n","Prediction Loss / Encoder Loss / Domain Loss: 7.64 4.26 0.37 0.36 0.69 0.65\n","Prediction Loss / Encoder Loss / Domain Loss: 7.77 5.54 0.37 0.37 0.69 0.66\n","Prediction Loss / Encoder Loss / Domain Loss: 7.42 3.65 0.37 0.36 0.68 0.65\n","Prediction Loss / Encoder Loss / Domain Loss: 6.46 4.96 0.39 0.35 0.70 0.66\n","Prediction Loss / Encoder Loss / Domain Loss: 8.56 4.18 0.38 0.36 0.69 0.65\n","Prediction Loss / Encoder Loss / Domain Loss: 7.95 3.58 0.37 0.37 0.69 0.64\n","Prediction Loss / Encoder Loss / Domain Loss: 7.55 4.91 0.37 0.36 0.70 0.65\n","Prediction Loss / Encoder Loss / Domain Loss: 7.01 2.85 0.37 0.36 0.70 0.65\n","Prediction Loss / Encoder Loss / Domain Loss: 8.32 4.74 0.38 0.36 0.69 0.64\n","Prediction Loss / Encoder Loss / Domain Loss: 7.07 4.15 0.36 0.35 0.69 0.66\n","Prediction Loss / Encoder Loss / Domain Loss: 7.43 4.14 0.38 0.36 0.68 0.65\n","Prediction Loss / Encoder Loss / Domain Loss: 7.55 4.41 0.37 0.37 0.70 0.64\n","Prediction Loss / Encoder Loss / Domain Loss: 8.00 4.87 0.36 0.36 0.68 0.64\n","Prediction Loss / Encoder Loss / Domain Loss: 8.09 4.07 0.38 0.38 0.69 0.65\n","Prediction Loss / Encoder Loss / Domain Loss: 7.23 4.27 0.35 0.37 0.69 0.64\n","Prediction Loss / Encoder Loss / Domain Loss: 6.82 4.05 0.36 0.35 0.68 0.64\n","Prediction Loss / Encoder Loss / Domain Loss: 7.36 3.87 0.37 0.37 0.71 0.65\n","Prediction Loss / Encoder Loss / Domain Loss: 7.96 3.70 0.36 0.36 0.68 0.64\n","Prediction Loss / Encoder Loss / Domain Loss: 7.55 4.55 0.35 0.35 0.70 0.66\n","Prediction Loss / Encoder Loss / Domain Loss: 7.12 3.82 0.37 0.34 0.68 0.65\n","Prediction Loss / Encoder Loss / Domain Loss: 7.49 3.31 0.36 0.35 0.69 0.66\n","Prediction Loss / Encoder Loss / Domain Loss: 7.71 4.67 0.35 0.37 0.70 0.65\n","Prediction Loss / Encoder Loss / Domain Loss: 7.36 4.35 0.35 0.35 0.70 0.66\n","Prediction Loss / Encoder Loss / Domain Loss: 6.76 4.53 0.36 0.36 0.70 0.65\n","Prediction Loss / Encoder Loss / Domain Loss: 8.11 4.21 0.37 0.37 0.69 0.65\n","Prediction Loss / Encoder Loss / Domain Loss: 7.62 4.00 0.37 0.35 0.70 0.65\n","Prediction Loss / Encoder Loss / Domain Loss: 7.58 3.48 0.37 0.36 0.69 0.65\n","Prediction Loss / Encoder Loss / Domain Loss: 8.35 3.81 0.37 0.35 0.70 0.65\n","Prediction Loss / Encoder Loss / Domain Loss: 7.84 4.13 0.37 0.35 0.70 0.64\n","Prediction Loss / Encoder Loss / Domain Loss: 7.20 3.45 0.36 0.35 0.72 0.66\n","Prediction Loss / Encoder Loss / Domain Loss: 8.32 4.16 0.36 0.36 0.70 0.65\n","Prediction Loss / Encoder Loss / Domain Loss: 7.38 4.28 0.36 0.36 0.70 0.65\n","Prediction Loss / Encoder Loss / Domain Loss: 6.96 4.48 0.36 0.36 0.71 0.66\n","Prediction Loss / Encoder Loss / Domain Loss: 8.38 4.14 0.36 0.36 0.69 0.64\n","Prediction Loss / Encoder Loss / Domain Loss: 7.05 4.72 0.37 0.35 0.68 0.64\n","Prediction Loss / Encoder Loss / Domain Loss: 6.14 4.08 0.36 0.37 0.67 0.63\n","Prediction Loss / Encoder Loss / Domain Loss: 6.69 3.59 0.36 0.35 0.68 0.63\n","Prediction Loss / Encoder Loss / Domain Loss: 7.74 4.07 0.36 0.35 0.69 0.66\n","Prediction Loss / Encoder Loss / Domain Loss: 7.60 3.66 0.37 0.36 0.68 0.64\n","Prediction Loss / Encoder Loss / Domain Loss: 6.95 3.92 0.36 0.35 0.69 0.65\n","Prediction Loss / Encoder Loss / Domain Loss: 7.27 3.15 0.37 0.37 0.70 0.65\n","Prediction Loss / Encoder Loss / Domain Loss: 7.67 4.16 0.36 0.36 0.70 0.64\n","Prediction Loss / Encoder Loss / Domain Loss: 6.93 3.71 0.36 0.38 0.68 0.65\n","Prediction Loss / Encoder Loss / Domain Loss: 8.10 4.45 0.36 0.37 0.68 0.65\n","Prediction Loss / Encoder Loss / Domain Loss: 7.36 3.74 0.37 0.38 0.70 0.64\n","Prediction Loss / Encoder Loss / Domain Loss: 7.61 4.56 0.36 0.40 0.68 0.63\n","Prediction Loss / Encoder Loss / Domain Loss: 6.80 4.11 0.36 0.39 0.71 0.65\n","Prediction Loss / Encoder Loss / Domain Loss: 8.09 3.72 0.36 0.40 0.69 0.65\n","Prediction Loss / Encoder Loss / Domain Loss: 6.85 4.21 0.36 0.40 0.71 0.66\n","Prediction Loss / Encoder Loss / Domain Loss: 7.04 3.89 0.37 0.44 0.71 0.65\n","Prediction Loss / Encoder Loss / Domain Loss: 7.18 3.38 0.36 0.42 0.69 0.63\n","Prediction Loss / Encoder Loss / Domain Loss: 7.40 3.63 0.36 0.42 0.70 0.65\n","Prediction Loss / Encoder Loss / Domain Loss: 7.86 3.37 0.35 0.46 0.67 0.64\n","Prediction Loss / Encoder Loss / Domain Loss: 7.03 4.49 0.36 0.44 0.69 0.64\n","Prediction Loss / Encoder Loss / Domain Loss: 6.80 4.23 0.35 0.46 0.67 0.63\n","Prediction Loss / Encoder Loss / Domain Loss: 5.97 4.23 0.36 0.45 0.67 0.63\n","Prediction Loss / Encoder Loss / Domain Loss: 7.19 3.66 0.36 0.45 0.70 0.65\n","Prediction Loss / Encoder Loss / Domain Loss: 7.00 3.72 0.36 0.45 0.71 0.66\n","Prediction Loss / Encoder Loss / Domain Loss: 6.55 3.28 0.35 0.46 0.69 0.65\n","Prediction Loss / Encoder Loss / Domain Loss: 7.17 3.89 0.36 0.45 0.71 0.66\n","Prediction Loss / Encoder Loss / Domain Loss: 7.29 3.64 0.34 0.46 0.69 0.64\n","Prediction Loss / Encoder Loss / Domain Loss: 7.03 3.51 0.35 0.46 0.67 0.63\n","Prediction Loss / Encoder Loss / Domain Loss: 6.78 4.18 0.34 0.45 0.68 0.64\n","Prediction Loss / Encoder Loss / Domain Loss: 6.70 4.23 0.34 0.45 0.71 0.65\n","Prediction Loss / Encoder Loss / Domain Loss: 5.79 4.11 0.35 0.44 0.71 0.65\n","Prediction Loss / Encoder Loss / Domain Loss: 7.69 4.04 0.36 0.44 0.69 0.64\n","Prediction Loss / Encoder Loss / Domain Loss: 7.03 3.91 0.35 0.44 0.70 0.66\n","Prediction Loss / Encoder Loss / Domain Loss: 6.67 3.29 0.36 0.44 0.68 0.64\n","Prediction Loss / Encoder Loss / Domain Loss: 8.07 3.35 0.36 0.43 0.67 0.62\n","Prediction Loss / Encoder Loss / Domain Loss: 7.10 3.23 0.36 0.45 0.69 0.63\n","Prediction Loss / Encoder Loss / Domain Loss: 7.09 3.82 0.35 0.43 0.68 0.64\n","Prediction Loss / Encoder Loss / Domain Loss: 7.49 3.57 0.37 0.42 0.70 0.64\n","Prediction Loss / Encoder Loss / Domain Loss: 7.70 4.00 0.36 0.42 0.67 0.64\n","Prediction Loss / Encoder Loss / Domain Loss: 6.54 3.75 0.35 0.42 0.69 0.65\n","Prediction Loss / Encoder Loss / Domain Loss: 7.24 3.80 0.35 0.40 0.68 0.63\n","Prediction Loss / Encoder Loss / Domain Loss: 7.36 4.28 0.35 0.40 0.71 0.65\n","Prediction Loss / Encoder Loss / Domain Loss: 7.35 3.44 0.35 0.40 0.70 0.65\n","Prediction Loss / Encoder Loss / Domain Loss: 6.51 4.16 0.35 0.40 0.68 0.65\n","Prediction Loss / Encoder Loss / Domain Loss: 7.12 3.53 0.35 0.40 0.70 0.65\n","Prediction Loss / Encoder Loss / Domain Loss: 7.39 3.46 0.35 0.39 0.71 0.65\n","Prediction Loss / Encoder Loss / Domain Loss: 7.63 4.24 0.37 0.39 0.70 0.64\n","Prediction Loss / Encoder Loss / Domain Loss: 6.94 3.35 0.36 0.39 0.69 0.64\n","Prediction Loss / Encoder Loss / Domain Loss: 6.87 4.68 0.37 0.39 0.68 0.65\n","Prediction Loss / Encoder Loss / Domain Loss: 7.36 4.30 0.36 0.39 0.68 0.63\n","Prediction Loss / Encoder Loss / Domain Loss: 7.66 4.10 0.35 0.38 0.71 0.63\n"]},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"e01a372c721b4ad987379973cba79488","version_minor":0,"version_major":2},"text/plain":["  0%|          | 0/42 [00:00<?, ?it/s]"]},"metadata":{}},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"1efeebb504b742f8b4019823650a32ef","version_minor":0,"version_major":2},"text/plain":["  0%|          | 0/42 [00:00<?, ?it/s]"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Loss: 3.9323 3.9282\n","Start Training ... \n","\n"]},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"571c9d9e2b5547d1af69e1d4f650d17c","version_minor":0,"version_major":2},"text/plain":["  0%|          | 0/321 [00:00<?, ?it/s]"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Prediction Loss / Encoder Loss / Domain Loss: 6.04 2.99 0.36 0.38 0.70 0.64\n","Prediction Loss / Encoder Loss / Domain Loss: 7.33 3.03 0.35 0.38 0.68 0.62\n","Prediction Loss / Encoder Loss / Domain Loss: 6.74 3.27 0.34 0.38 0.70 0.65\n","Prediction Loss / Encoder Loss / Domain Loss: 6.86 3.32 0.35 0.37 0.68 0.64\n","Prediction Loss / Encoder Loss / Domain Loss: 6.29 4.19 0.34 0.37 0.69 0.64\n","Prediction Loss / Encoder Loss / Domain Loss: 6.57 4.05 0.36 0.37 0.68 0.63\n","Prediction Loss / Encoder Loss / Domain Loss: 7.53 3.91 0.34 0.37 0.70 0.63\n","Prediction Loss / Encoder Loss / Domain Loss: 7.46 3.65 0.36 0.35 0.71 0.65\n","Prediction Loss / Encoder Loss / Domain Loss: 6.20 3.46 0.35 0.35 0.69 0.64\n","Prediction Loss / Encoder Loss / Domain Loss: 7.01 3.39 0.35 0.37 0.72 0.65\n","Prediction Loss / Encoder Loss / Domain Loss: 6.53 3.06 0.35 0.36 0.71 0.65\n","Prediction Loss / Encoder Loss / Domain Loss: 6.94 3.20 0.34 0.35 0.69 0.63\n","Prediction Loss / Encoder Loss / Domain Loss: 7.15 3.83 0.34 0.36 0.72 0.65\n","Prediction Loss / Encoder Loss / Domain Loss: 6.03 4.10 0.35 0.35 0.70 0.65\n","Prediction Loss / Encoder Loss / Domain Loss: 6.74 4.01 0.35 0.35 0.68 0.63\n","Prediction Loss / Encoder Loss / Domain Loss: 7.38 3.74 0.35 0.34 0.72 0.66\n","Prediction Loss / Encoder Loss / Domain Loss: 5.71 3.75 0.35 0.35 0.69 0.64\n","Prediction Loss / Encoder Loss / Domain Loss: 6.03 3.43 0.36 0.35 0.70 0.64\n","Prediction Loss / Encoder Loss / Domain Loss: 7.22 3.70 0.34 0.37 0.69 0.64\n","Prediction Loss / Encoder Loss / Domain Loss: 6.78 3.35 0.34 0.35 0.70 0.65\n","Prediction Loss / Encoder Loss / Domain Loss: 6.74 3.54 0.34 0.36 0.68 0.62\n","Prediction Loss / Encoder Loss / Domain Loss: 7.22 3.45 0.33 0.34 0.68 0.64\n","Prediction Loss / Encoder Loss / Domain Loss: 6.76 3.07 0.33 0.34 0.68 0.63\n","Prediction Loss / Encoder Loss / Domain Loss: 6.72 3.66 0.32 0.35 0.71 0.63\n","Prediction Loss / Encoder Loss / Domain Loss: 7.13 4.21 0.34 0.35 0.69 0.63\n","Prediction Loss / Encoder Loss / Domain Loss: 6.24 3.61 0.33 0.35 0.68 0.62\n","Prediction Loss / Encoder Loss / Domain Loss: 5.15 3.44 0.35 0.34 0.69 0.63\n","Prediction Loss / Encoder Loss / Domain Loss: 7.01 4.08 0.33 0.34 0.68 0.64\n","Prediction Loss / Encoder Loss / Domain Loss: 6.95 3.08 0.34 0.34 0.68 0.63\n","Prediction Loss / Encoder Loss / Domain Loss: 7.47 4.35 0.34 0.33 0.69 0.64\n","Prediction Loss / Encoder Loss / Domain Loss: 7.13 2.99 0.33 0.34 0.71 0.64\n","Prediction Loss / Encoder Loss / Domain Loss: 6.25 3.40 0.34 0.34 0.73 0.64\n","Prediction Loss / Encoder Loss / Domain Loss: 7.30 3.33 0.33 0.34 0.71 0.64\n","Prediction Loss / Encoder Loss / Domain Loss: 6.51 3.87 0.34 0.34 0.68 0.65\n","Prediction Loss / Encoder Loss / Domain Loss: 6.90 3.54 0.33 0.34 0.71 0.65\n","Prediction Loss / Encoder Loss / Domain Loss: 6.87 3.20 0.33 0.33 0.68 0.63\n","Prediction Loss / Encoder Loss / Domain Loss: 5.96 3.69 0.33 0.32 0.72 0.65\n","Prediction Loss / Encoder Loss / Domain Loss: 7.00 3.88 0.32 0.33 0.69 0.63\n","Prediction Loss / Encoder Loss / Domain Loss: 5.67 3.61 0.33 0.34 0.70 0.64\n","Prediction Loss / Encoder Loss / Domain Loss: 6.69 3.71 0.32 0.33 0.69 0.63\n","Prediction Loss / Encoder Loss / Domain Loss: 6.31 3.43 0.33 0.33 0.70 0.63\n","Prediction Loss / Encoder Loss / Domain Loss: 6.42 3.51 0.32 0.33 0.70 0.65\n","Prediction Loss / Encoder Loss / Domain Loss: 7.43 3.70 0.34 0.34 0.70 0.64\n","Prediction Loss / Encoder Loss / Domain Loss: 5.69 3.17 0.33 0.34 0.69 0.62\n","Prediction Loss / Encoder Loss / Domain Loss: 6.51 4.09 0.33 0.33 0.74 0.64\n","Prediction Loss / Encoder Loss / Domain Loss: 6.10 3.85 0.32 0.32 0.71 0.64\n","Prediction Loss / Encoder Loss / Domain Loss: 6.31 3.64 0.33 0.34 0.71 0.64\n","Prediction Loss / Encoder Loss / Domain Loss: 6.19 3.80 0.33 0.33 0.70 0.64\n","Prediction Loss / Encoder Loss / Domain Loss: 6.19 3.29 0.32 0.33 0.71 0.64\n","Prediction Loss / Encoder Loss / Domain Loss: 6.93 3.32 0.33 0.32 0.71 0.65\n","Prediction Loss / Encoder Loss / Domain Loss: 6.61 4.03 0.32 0.33 0.73 0.66\n","Prediction Loss / Encoder Loss / Domain Loss: 7.06 4.06 0.33 0.33 0.73 0.66\n","Prediction Loss / Encoder Loss / Domain Loss: 6.88 3.66 0.32 0.33 0.71 0.63\n","Prediction Loss / Encoder Loss / Domain Loss: 7.41 3.25 0.32 0.32 0.72 0.64\n","Prediction Loss / Encoder Loss / Domain Loss: 7.04 3.22 0.32 0.32 0.71 0.63\n","Prediction Loss / Encoder Loss / Domain Loss: 6.38 2.94 0.32 0.32 0.68 0.63\n","Prediction Loss / Encoder Loss / Domain Loss: 6.46 4.51 0.32 0.32 0.71 0.63\n","Prediction Loss / Encoder Loss / Domain Loss: 6.37 3.09 0.31 0.32 0.70 0.65\n","Prediction Loss / Encoder Loss / Domain Loss: 7.21 4.29 0.31 0.33 0.69 0.63\n","Prediction Loss / Encoder Loss / Domain Loss: 6.44 3.43 0.33 0.31 0.71 0.64\n","Prediction Loss / Encoder Loss / Domain Loss: 6.97 2.93 0.32 0.32 0.71 0.64\n","Prediction Loss / Encoder Loss / Domain Loss: 6.44 3.85 0.33 0.31 0.69 0.62\n","Prediction Loss / Encoder Loss / Domain Loss: 5.83 3.64 0.33 0.32 0.72 0.63\n","Prediction Loss / Encoder Loss / Domain Loss: 6.57 3.17 0.31 0.31 0.72 0.64\n","Prediction Loss / Encoder Loss / Domain Loss: 5.56 3.45 0.31 0.32 0.68 0.61\n","Prediction Loss / Encoder Loss / Domain Loss: 6.28 3.23 0.32 0.32 0.70 0.62\n","Prediction Loss / Encoder Loss / Domain Loss: 6.10 3.82 0.31 0.31 0.71 0.63\n","Prediction Loss / Encoder Loss / Domain Loss: 6.21 2.82 0.32 0.31 0.71 0.63\n","Prediction Loss / Encoder Loss / Domain Loss: 7.08 2.97 0.31 0.31 0.72 0.63\n","Prediction Loss / Encoder Loss / Domain Loss: 6.21 3.62 0.32 0.32 0.72 0.64\n","Prediction Loss / Encoder Loss / Domain Loss: 5.39 2.86 0.32 0.31 0.72 0.63\n","Prediction Loss / Encoder Loss / Domain Loss: 5.76 3.96 0.32 0.32 0.71 0.64\n","Prediction Loss / Encoder Loss / Domain Loss: 5.55 3.02 0.31 0.31 0.70 0.63\n","Prediction Loss / Encoder Loss / Domain Loss: 5.96 3.53 0.31 0.31 0.73 0.63\n","Prediction Loss / Encoder Loss / Domain Loss: 6.06 3.43 0.31 0.31 0.70 0.63\n","Prediction Loss / Encoder Loss / Domain Loss: 6.54 3.30 0.31 0.30 0.72 0.64\n","Prediction Loss / Encoder Loss / Domain Loss: 5.57 2.82 0.31 0.32 0.71 0.64\n","Prediction Loss / Encoder Loss / Domain Loss: 6.08 3.33 0.32 0.30 0.70 0.63\n","Prediction Loss / Encoder Loss / Domain Loss: 6.24 3.86 0.31 0.31 0.72 0.64\n","Prediction Loss / Encoder Loss / Domain Loss: 6.71 3.32 0.32 0.30 0.74 0.63\n","Prediction Loss / Encoder Loss / Domain Loss: 6.43 4.00 0.31 0.31 0.72 0.64\n","Prediction Loss / Encoder Loss / Domain Loss: 6.86 3.62 0.30 0.31 0.72 0.63\n","Prediction Loss / Encoder Loss / Domain Loss: 5.63 3.75 0.31 0.31 0.72 0.64\n","Prediction Loss / Encoder Loss / Domain Loss: 6.93 3.67 0.32 0.31 0.71 0.62\n","Prediction Loss / Encoder Loss / Domain Loss: 6.11 3.29 0.31 0.30 0.71 0.63\n","Prediction Loss / Encoder Loss / Domain Loss: 5.87 3.80 0.31 0.30 0.74 0.63\n","Prediction Loss / Encoder Loss / Domain Loss: 6.37 3.75 0.32 0.30 0.71 0.65\n","Prediction Loss / Encoder Loss / Domain Loss: 5.96 3.20 0.31 0.31 0.71 0.63\n","Prediction Loss / Encoder Loss / Domain Loss: 6.60 3.43 0.30 0.30 0.72 0.63\n","Prediction Loss / Encoder Loss / Domain Loss: 6.76 2.93 0.31 0.30 0.72 0.63\n","Prediction Loss / Encoder Loss / Domain Loss: 5.77 3.57 0.31 0.31 0.72 0.64\n","Prediction Loss / Encoder Loss / Domain Loss: 6.08 3.47 0.32 0.31 0.70 0.63\n","Prediction Loss / Encoder Loss / Domain Loss: 5.49 2.85 0.31 0.29 0.75 0.65\n","Prediction Loss / Encoder Loss / Domain Loss: 6.91 3.33 0.32 0.31 0.69 0.62\n","Prediction Loss / Encoder Loss / Domain Loss: 6.29 2.99 0.31 0.31 0.73 0.63\n","Prediction Loss / Encoder Loss / Domain Loss: 6.15 3.72 0.30 0.31 0.70 0.62\n","Prediction Loss / Encoder Loss / Domain Loss: 5.86 3.59 0.31 0.30 0.70 0.62\n","Prediction Loss / Encoder Loss / Domain Loss: 6.16 2.78 0.31 0.30 0.73 0.64\n","Prediction Loss / Encoder Loss / Domain Loss: 6.13 2.83 0.30 0.31 0.70 0.61\n","Prediction Loss / Encoder Loss / Domain Loss: 5.52 2.66 0.31 0.30 0.71 0.63\n","Prediction Loss / Encoder Loss / Domain Loss: 5.28 3.06 0.30 0.31 0.71 0.63\n","Prediction Loss / Encoder Loss / Domain Loss: 5.78 3.01 0.31 0.30 0.72 0.64\n","Prediction Loss / Encoder Loss / Domain Loss: 6.63 3.63 0.30 0.30 0.73 0.64\n","Prediction Loss / Encoder Loss / Domain Loss: 6.33 2.93 0.31 0.29 0.67 0.61\n","Prediction Loss / Encoder Loss / Domain Loss: 6.00 3.27 0.31 0.29 0.71 0.61\n","Prediction Loss / Encoder Loss / Domain Loss: 6.13 3.05 0.31 0.30 0.70 0.62\n","Prediction Loss / Encoder Loss / Domain Loss: 6.03 3.73 0.30 0.29 0.70 0.62\n","Prediction Loss / Encoder Loss / Domain Loss: 5.92 2.82 0.30 0.30 0.73 0.63\n","Prediction Loss / Encoder Loss / Domain Loss: 6.12 3.22 0.30 0.30 0.73 0.65\n","Prediction Loss / Encoder Loss / Domain Loss: 5.80 3.09 0.31 0.30 0.71 0.63\n","Prediction Loss / Encoder Loss / Domain Loss: 6.12 3.41 0.30 0.29 0.73 0.65\n","Prediction Loss / Encoder Loss / Domain Loss: 6.00 3.08 0.30 0.30 0.70 0.63\n","Prediction Loss / Encoder Loss / Domain Loss: 6.67 3.14 0.30 0.29 0.69 0.62\n","Prediction Loss / Encoder Loss / Domain Loss: 5.93 3.33 0.30 0.30 0.74 0.64\n","Prediction Loss / Encoder Loss / Domain Loss: 6.16 3.23 0.30 0.29 0.71 0.63\n","Prediction Loss / Encoder Loss / Domain Loss: 5.86 3.54 0.31 0.29 0.71 0.63\n","Prediction Loss / Encoder Loss / Domain Loss: 7.03 3.33 0.29 0.29 0.73 0.64\n","Prediction Loss / Encoder Loss / Domain Loss: 6.35 3.40 0.29 0.30 0.73 0.64\n","Prediction Loss / Encoder Loss / Domain Loss: 6.68 3.60 0.30 0.30 0.73 0.64\n","Prediction Loss / Encoder Loss / Domain Loss: 6.43 3.44 0.30 0.29 0.71 0.63\n","Prediction Loss / Encoder Loss / Domain Loss: 6.26 3.01 0.30 0.29 0.70 0.63\n","Prediction Loss / Encoder Loss / Domain Loss: 5.96 2.71 0.30 0.29 0.72 0.63\n","Prediction Loss / Encoder Loss / Domain Loss: 5.75 3.58 0.30 0.29 0.70 0.61\n","Prediction Loss / Encoder Loss / Domain Loss: 5.98 3.32 0.30 0.29 0.71 0.63\n","Prediction Loss / Encoder Loss / Domain Loss: 6.46 2.98 0.30 0.29 0.74 0.64\n","Prediction Loss / Encoder Loss / Domain Loss: 5.84 2.86 0.31 0.29 0.69 0.62\n","Prediction Loss / Encoder Loss / Domain Loss: 5.91 3.03 0.31 0.29 0.73 0.63\n","Prediction Loss / Encoder Loss / Domain Loss: 5.77 3.09 0.31 0.30 0.74 0.64\n","Prediction Loss / Encoder Loss / Domain Loss: 6.38 3.29 0.30 0.30 0.71 0.62\n","Prediction Loss / Encoder Loss / Domain Loss: 5.00 2.67 0.30 0.29 0.73 0.63\n","Prediction Loss / Encoder Loss / Domain Loss: 6.32 3.37 0.30 0.28 0.74 0.64\n","Prediction Loss / Encoder Loss / Domain Loss: 5.71 3.21 0.30 0.29 0.71 0.62\n","Prediction Loss / Encoder Loss / Domain Loss: 6.71 3.10 0.31 0.28 0.73 0.64\n","Prediction Loss / Encoder Loss / Domain Loss: 5.90 3.18 0.30 0.29 0.72 0.63\n","Prediction Loss / Encoder Loss / Domain Loss: 5.92 3.42 0.29 0.29 0.75 0.62\n","Prediction Loss / Encoder Loss / Domain Loss: 6.17 2.87 0.29 0.30 0.71 0.62\n","Prediction Loss / Encoder Loss / Domain Loss: 6.11 3.09 0.29 0.28 0.73 0.63\n","Prediction Loss / Encoder Loss / Domain Loss: 5.75 3.12 0.30 0.29 0.72 0.61\n","Prediction Loss / Encoder Loss / Domain Loss: 5.24 2.53 0.30 0.27 0.73 0.63\n","Prediction Loss / Encoder Loss / Domain Loss: 5.95 3.47 0.31 0.28 0.71 0.62\n","Prediction Loss / Encoder Loss / Domain Loss: 5.50 2.82 0.30 0.28 0.70 0.62\n","Prediction Loss / Encoder Loss / Domain Loss: 6.71 3.02 0.29 0.28 0.75 0.64\n","Prediction Loss / Encoder Loss / Domain Loss: 5.60 2.94 0.29 0.28 0.73 0.63\n","Prediction Loss / Encoder Loss / Domain Loss: 6.03 2.93 0.29 0.28 0.73 0.63\n","Prediction Loss / Encoder Loss / Domain Loss: 5.94 3.23 0.30 0.28 0.72 0.63\n","Prediction Loss / Encoder Loss / Domain Loss: 5.89 3.27 0.29 0.28 0.75 0.63\n","Prediction Loss / Encoder Loss / Domain Loss: 4.77 3.45 0.30 0.28 0.73 0.63\n","Prediction Loss / Encoder Loss / Domain Loss: 5.42 2.85 0.30 0.29 0.73 0.63\n","Prediction Loss / Encoder Loss / Domain Loss: 4.75 3.83 0.29 0.28 0.75 0.63\n","Prediction Loss / Encoder Loss / Domain Loss: 5.30 3.25 0.29 0.28 0.76 0.64\n","Prediction Loss / Encoder Loss / Domain Loss: 5.52 3.30 0.29 0.28 0.69 0.61\n","Prediction Loss / Encoder Loss / Domain Loss: 5.45 2.87 0.29 0.28 0.75 0.62\n","Prediction Loss / Encoder Loss / Domain Loss: 5.90 2.83 0.28 0.28 0.70 0.61\n","Prediction Loss / Encoder Loss / Domain Loss: 4.97 3.18 0.29 0.28 0.74 0.65\n","Prediction Loss / Encoder Loss / Domain Loss: 6.06 3.41 0.29 0.28 0.69 0.62\n","Prediction Loss / Encoder Loss / Domain Loss: 5.21 2.34 0.30 0.28 0.76 0.64\n","Prediction Loss / Encoder Loss / Domain Loss: 4.91 3.12 0.31 0.29 0.70 0.62\n","Prediction Loss / Encoder Loss / Domain Loss: 5.16 2.97 0.29 0.28 0.71 0.62\n","Prediction Loss / Encoder Loss / Domain Loss: 6.08 2.20 0.29 0.28 0.73 0.63\n","Prediction Loss / Encoder Loss / Domain Loss: 5.92 3.19 0.30 0.29 0.73 0.63\n","Prediction Loss / Encoder Loss / Domain Loss: 6.19 2.88 0.29 0.29 0.75 0.64\n","Prediction Loss / Encoder Loss / Domain Loss: 5.98 3.01 0.29 0.27 0.71 0.61\n","Prediction Loss / Encoder Loss / Domain Loss: 5.73 2.85 0.29 0.28 0.73 0.63\n","Prediction Loss / Encoder Loss / Domain Loss: 5.10 2.46 0.28 0.27 0.72 0.62\n","Prediction Loss / Encoder Loss / Domain Loss: 5.80 2.86 0.29 0.29 0.73 0.63\n","Prediction Loss / Encoder Loss / Domain Loss: 5.48 2.88 0.29 0.28 0.72 0.62\n","Prediction Loss / Encoder Loss / Domain Loss: 5.76 2.58 0.29 0.28 0.72 0.62\n","Prediction Loss / Encoder Loss / Domain Loss: 5.94 3.11 0.29 0.29 0.72 0.63\n","Prediction Loss / Encoder Loss / Domain Loss: 6.08 2.97 0.29 0.28 0.70 0.62\n","Prediction Loss / Encoder Loss / Domain Loss: 4.93 3.33 0.30 0.28 0.75 0.64\n","Prediction Loss / Encoder Loss / Domain Loss: 5.55 2.65 0.30 0.28 0.72 0.61\n","Prediction Loss / Encoder Loss / Domain Loss: 5.70 2.71 0.29 0.28 0.74 0.63\n","Prediction Loss / Encoder Loss / Domain Loss: 5.27 2.73 0.29 0.28 0.75 0.63\n","Prediction Loss / Encoder Loss / Domain Loss: 5.63 2.99 0.30 0.26 0.75 0.64\n","Prediction Loss / Encoder Loss / Domain Loss: 5.70 2.68 0.29 0.26 0.73 0.62\n","Prediction Loss / Encoder Loss / Domain Loss: 5.20 2.86 0.29 0.28 0.74 0.64\n","Prediction Loss / Encoder Loss / Domain Loss: 5.79 3.67 0.29 0.27 0.71 0.62\n","Prediction Loss / Encoder Loss / Domain Loss: 5.07 2.51 0.29 0.28 0.75 0.64\n","Prediction Loss / Encoder Loss / Domain Loss: 5.70 2.59 0.29 0.26 0.76 0.63\n","Prediction Loss / Encoder Loss / Domain Loss: 6.05 2.91 0.28 0.27 0.78 0.65\n","Prediction Loss / Encoder Loss / Domain Loss: 4.87 2.66 0.28 0.27 0.72 0.62\n","Prediction Loss / Encoder Loss / Domain Loss: 5.28 2.69 0.29 0.28 0.75 0.64\n","Prediction Loss / Encoder Loss / Domain Loss: 5.45 2.86 0.29 0.28 0.74 0.63\n","Prediction Loss / Encoder Loss / Domain Loss: 4.78 3.09 0.28 0.29 0.73 0.62\n","Prediction Loss / Encoder Loss / Domain Loss: 5.52 2.35 0.28 0.27 0.73 0.62\n","Prediction Loss / Encoder Loss / Domain Loss: 5.76 3.11 0.29 0.27 0.74 0.63\n","Prediction Loss / Encoder Loss / Domain Loss: 5.78 3.26 0.28 0.28 0.72 0.63\n","Prediction Loss / Encoder Loss / Domain Loss: 5.15 2.47 0.28 0.26 0.74 0.63\n","Prediction Loss / Encoder Loss / Domain Loss: 5.39 3.13 0.29 0.26 0.73 0.63\n","Prediction Loss / Encoder Loss / Domain Loss: 5.58 2.81 0.29 0.27 0.75 0.64\n","Prediction Loss / Encoder Loss / Domain Loss: 5.62 3.06 0.29 0.27 0.70 0.61\n","Prediction Loss / Encoder Loss / Domain Loss: 4.83 2.52 0.28 0.27 0.74 0.63\n","Prediction Loss / Encoder Loss / Domain Loss: 5.62 1.85 0.28 0.27 0.72 0.62\n","Prediction Loss / Encoder Loss / Domain Loss: 6.30 2.81 0.28 0.27 0.70 0.61\n","Prediction Loss / Encoder Loss / Domain Loss: 5.72 2.84 0.29 0.27 0.75 0.64\n","Prediction Loss / Encoder Loss / Domain Loss: 4.81 2.91 0.28 0.26 0.75 0.62\n","Prediction Loss / Encoder Loss / Domain Loss: 5.48 2.89 0.29 0.27 0.74 0.62\n","Prediction Loss / Encoder Loss / Domain Loss: 5.66 2.99 0.28 0.27 0.76 0.63\n","Prediction Loss / Encoder Loss / Domain Loss: 5.36 2.86 0.29 0.27 0.71 0.60\n","Prediction Loss / Encoder Loss / Domain Loss: 5.69 3.15 0.28 0.26 0.74 0.63\n","Prediction Loss / Encoder Loss / Domain Loss: 5.24 2.58 0.28 0.27 0.75 0.63\n","Prediction Loss / Encoder Loss / Domain Loss: 5.15 2.29 0.29 0.28 0.75 0.63\n","Prediction Loss / Encoder Loss / Domain Loss: 5.41 2.82 0.29 0.27 0.75 0.65\n","Prediction Loss / Encoder Loss / Domain Loss: 5.16 2.78 0.28 0.27 0.72 0.61\n","Prediction Loss / Encoder Loss / Domain Loss: 5.30 2.73 0.28 0.27 0.74 0.62\n","Prediction Loss / Encoder Loss / Domain Loss: 5.87 2.96 0.28 0.28 0.75 0.62\n","Prediction Loss / Encoder Loss / Domain Loss: 4.89 2.59 0.28 0.27 0.76 0.64\n","Prediction Loss / Encoder Loss / Domain Loss: 5.16 2.34 0.28 0.28 0.75 0.64\n","Prediction Loss / Encoder Loss / Domain Loss: 5.04 2.87 0.29 0.28 0.77 0.64\n","Prediction Loss / Encoder Loss / Domain Loss: 5.45 2.93 0.27 0.29 0.71 0.62\n","Prediction Loss / Encoder Loss / Domain Loss: 5.63 3.07 0.28 0.30 0.78 0.66\n","Prediction Loss / Encoder Loss / Domain Loss: 5.69 2.51 0.28 0.30 0.76 0.63\n","Prediction Loss / Encoder Loss / Domain Loss: 5.60 2.93 0.27 0.31 0.74 0.63\n","Prediction Loss / Encoder Loss / Domain Loss: 5.75 2.95 0.28 0.33 0.75 0.63\n","Prediction Loss / Encoder Loss / Domain Loss: 5.60 2.60 0.28 0.34 0.74 0.63\n","Prediction Loss / Encoder Loss / Domain Loss: 5.15 2.62 0.28 0.35 0.72 0.61\n","Prediction Loss / Encoder Loss / Domain Loss: 5.92 2.71 0.28 0.36 0.73 0.62\n","Prediction Loss / Encoder Loss / Domain Loss: 5.32 3.15 0.28 0.37 0.73 0.63\n","Prediction Loss / Encoder Loss / Domain Loss: 5.71 2.73 0.28 0.38 0.72 0.62\n","Prediction Loss / Encoder Loss / Domain Loss: 5.42 2.64 0.28 0.39 0.75 0.64\n","Prediction Loss / Encoder Loss / Domain Loss: 4.93 2.44 0.28 0.39 0.75 0.63\n","Prediction Loss / Encoder Loss / Domain Loss: 4.89 2.40 0.28 0.39 0.76 0.62\n","Prediction Loss / Encoder Loss / Domain Loss: 5.03 2.90 0.28 0.41 0.73 0.61\n","Prediction Loss / Encoder Loss / Domain Loss: 5.50 2.20 0.28 0.38 0.77 0.63\n","Prediction Loss / Encoder Loss / Domain Loss: 5.12 2.07 0.28 0.40 0.72 0.62\n","Prediction Loss / Encoder Loss / Domain Loss: 5.58 2.77 0.28 0.39 0.76 0.63\n","Prediction Loss / Encoder Loss / Domain Loss: 5.44 2.50 0.27 0.39 0.74 0.63\n","Prediction Loss / Encoder Loss / Domain Loss: 4.91 2.36 0.27 0.38 0.75 0.61\n","Prediction Loss / Encoder Loss / Domain Loss: 4.94 2.80 0.27 0.38 0.73 0.62\n","Prediction Loss / Encoder Loss / Domain Loss: 4.96 2.79 0.29 0.38 0.73 0.63\n","Prediction Loss / Encoder Loss / Domain Loss: 5.42 2.87 0.27 0.38 0.77 0.63\n","Prediction Loss / Encoder Loss / Domain Loss: 5.11 2.26 0.27 0.37 0.74 0.63\n","Prediction Loss / Encoder Loss / Domain Loss: 5.53 2.80 0.27 0.35 0.74 0.63\n","Prediction Loss / Encoder Loss / Domain Loss: 5.19 2.67 0.28 0.34 0.73 0.62\n","Prediction Loss / Encoder Loss / Domain Loss: 5.22 3.01 0.28 0.34 0.75 0.63\n","Prediction Loss / Encoder Loss / Domain Loss: 5.18 2.75 0.28 0.34 0.74 0.63\n","Prediction Loss / Encoder Loss / Domain Loss: 5.31 2.47 0.28 0.33 0.75 0.63\n","Prediction Loss / Encoder Loss / Domain Loss: 5.04 3.15 0.27 0.32 0.78 0.65\n","Prediction Loss / Encoder Loss / Domain Loss: 5.45 3.09 0.27 0.32 0.75 0.65\n","Prediction Loss / Encoder Loss / Domain Loss: 4.85 2.92 0.27 0.32 0.74 0.62\n","Prediction Loss / Encoder Loss / Domain Loss: 5.61 2.22 0.28 0.31 0.79 0.66\n","Prediction Loss / Encoder Loss / Domain Loss: 4.10 2.25 0.27 0.32 0.74 0.62\n","Prediction Loss / Encoder Loss / Domain Loss: 5.41 1.97 0.27 0.31 0.79 0.66\n","Prediction Loss / Encoder Loss / Domain Loss: 5.46 2.43 0.27 0.30 0.76 0.64\n","Prediction Loss / Encoder Loss / Domain Loss: 4.44 2.06 0.28 0.30 0.72 0.63\n","Prediction Loss / Encoder Loss / Domain Loss: 5.18 2.43 0.27 0.29 0.76 0.62\n","Prediction Loss / Encoder Loss / Domain Loss: 4.66 2.17 0.28 0.28 0.74 0.63\n","Prediction Loss / Encoder Loss / Domain Loss: 5.44 2.14 0.27 0.29 0.76 0.64\n","Prediction Loss / Encoder Loss / Domain Loss: 5.42 2.63 0.27 0.30 0.76 0.63\n","Prediction Loss / Encoder Loss / Domain Loss: 4.76 2.53 0.27 0.28 0.77 0.64\n","Prediction Loss / Encoder Loss / Domain Loss: 4.36 2.72 0.27 0.28 0.74 0.62\n","Prediction Loss / Encoder Loss / Domain Loss: 4.62 3.08 0.27 0.28 0.77 0.64\n","Prediction Loss / Encoder Loss / Domain Loss: 5.13 2.74 0.28 0.28 0.76 0.65\n","Prediction Loss / Encoder Loss / Domain Loss: 5.65 2.19 0.28 0.28 0.76 0.65\n","Prediction Loss / Encoder Loss / Domain Loss: 5.84 2.37 0.27 0.28 0.76 0.64\n","Prediction Loss / Encoder Loss / Domain Loss: 4.52 2.94 0.27 0.28 0.77 0.66\n","Prediction Loss / Encoder Loss / Domain Loss: 4.78 2.41 0.27 0.29 0.78 0.63\n","Prediction Loss / Encoder Loss / Domain Loss: 4.58 2.22 0.27 0.29 0.74 0.63\n","Prediction Loss / Encoder Loss / Domain Loss: 5.10 2.38 0.26 0.28 0.73 0.63\n","Prediction Loss / Encoder Loss / Domain Loss: 4.45 2.16 0.27 0.29 0.76 0.63\n","Prediction Loss / Encoder Loss / Domain Loss: 5.67 2.51 0.27 0.29 0.78 0.66\n","Prediction Loss / Encoder Loss / Domain Loss: 4.61 2.39 0.27 0.27 0.76 0.64\n","Prediction Loss / Encoder Loss / Domain Loss: 4.75 2.42 0.26 0.29 0.78 0.64\n","Prediction Loss / Encoder Loss / Domain Loss: 4.55 2.81 0.27 0.28 0.77 0.62\n","Prediction Loss / Encoder Loss / Domain Loss: 4.54 2.99 0.27 0.29 0.75 0.61\n","Prediction Loss / Encoder Loss / Domain Loss: 4.85 2.80 0.27 0.28 0.72 0.63\n","Prediction Loss / Encoder Loss / Domain Loss: 4.64 2.64 0.26 0.29 0.77 0.64\n","Prediction Loss / Encoder Loss / Domain Loss: 4.64 2.74 0.26 0.28 0.75 0.64\n","Prediction Loss / Encoder Loss / Domain Loss: 5.22 2.34 0.26 0.27 0.75 0.63\n","Prediction Loss / Encoder Loss / Domain Loss: 4.77 2.63 0.26 0.28 0.74 0.64\n","Prediction Loss / Encoder Loss / Domain Loss: 4.63 2.52 0.26 0.29 0.74 0.63\n","Prediction Loss / Encoder Loss / Domain Loss: 4.44 2.63 0.27 0.28 0.74 0.62\n","Prediction Loss / Encoder Loss / Domain Loss: 4.30 2.97 0.27 0.28 0.74 0.63\n","Prediction Loss / Encoder Loss / Domain Loss: 4.61 2.20 0.27 0.28 0.75 0.64\n","Prediction Loss / Encoder Loss / Domain Loss: 4.21 2.31 0.26 0.28 0.79 0.65\n","Prediction Loss / Encoder Loss / Domain Loss: 4.86 2.29 0.26 0.28 0.75 0.64\n","Prediction Loss / Encoder Loss / Domain Loss: 4.72 2.48 0.26 0.29 0.74 0.63\n","Prediction Loss / Encoder Loss / Domain Loss: 4.72 1.89 0.26 0.27 0.76 0.64\n","Prediction Loss / Encoder Loss / Domain Loss: 4.46 2.33 0.26 0.27 0.73 0.62\n","Prediction Loss / Encoder Loss / Domain Loss: 4.32 1.96 0.26 0.27 0.75 0.64\n","Prediction Loss / Encoder Loss / Domain Loss: 4.32 2.33 0.26 0.26 0.77 0.66\n","Prediction Loss / Encoder Loss / Domain Loss: 4.89 2.24 0.25 0.27 0.76 0.65\n","Prediction Loss / Encoder Loss / Domain Loss: 5.13 2.23 0.26 0.26 0.79 0.65\n","Prediction Loss / Encoder Loss / Domain Loss: 4.17 1.79 0.26 0.27 0.77 0.64\n","Prediction Loss / Encoder Loss / Domain Loss: 4.81 2.43 0.27 0.26 0.74 0.65\n","Prediction Loss / Encoder Loss / Domain Loss: 4.36 2.65 0.27 0.26 0.77 0.64\n","Prediction Loss / Encoder Loss / Domain Loss: 5.11 2.68 0.26 0.27 0.77 0.64\n","Prediction Loss / Encoder Loss / Domain Loss: 4.54 2.59 0.26 0.25 0.75 0.63\n","Prediction Loss / Encoder Loss / Domain Loss: 5.41 2.44 0.26 0.26 0.77 0.65\n","Prediction Loss / Encoder Loss / Domain Loss: 4.15 2.22 0.26 0.26 0.78 0.65\n","Prediction Loss / Encoder Loss / Domain Loss: 4.20 2.21 0.26 0.27 0.76 0.66\n","Prediction Loss / Encoder Loss / Domain Loss: 4.64 2.46 0.26 0.27 0.76 0.64\n","Prediction Loss / Encoder Loss / Domain Loss: 4.01 2.83 0.26 0.25 0.76 0.63\n","Prediction Loss / Encoder Loss / Domain Loss: 4.81 2.29 0.26 0.25 0.74 0.63\n","Prediction Loss / Encoder Loss / Domain Loss: 4.56 2.68 0.26 0.26 0.77 0.63\n","Prediction Loss / Encoder Loss / Domain Loss: 4.74 2.47 0.26 0.26 0.77 0.66\n","Prediction Loss / Encoder Loss / Domain Loss: 4.86 2.53 0.26 0.26 0.76 0.63\n","Prediction Loss / Encoder Loss / Domain Loss: 4.85 1.84 0.25 0.26 0.79 0.64\n","Prediction Loss / Encoder Loss / Domain Loss: 4.66 2.24 0.26 0.26 0.76 0.63\n","Prediction Loss / Encoder Loss / Domain Loss: 4.78 2.35 0.26 0.25 0.76 0.65\n","Prediction Loss / Encoder Loss / Domain Loss: 4.71 2.46 0.26 0.25 0.78 0.64\n","Prediction Loss / Encoder Loss / Domain Loss: 4.58 2.22 0.26 0.25 0.76 0.64\n","Prediction Loss / Encoder Loss / Domain Loss: 4.83 2.60 0.26 0.26 0.75 0.63\n","Prediction Loss / Encoder Loss / Domain Loss: 4.51 2.07 0.26 0.24 0.76 0.64\n","Prediction Loss / Encoder Loss / Domain Loss: 4.00 2.49 0.25 0.25 0.73 0.62\n","Prediction Loss / Encoder Loss / Domain Loss: 4.86 2.06 0.25 0.26 0.78 0.65\n","Prediction Loss / Encoder Loss / Domain Loss: 4.42 3.01 0.26 0.25 0.73 0.62\n","Prediction Loss / Encoder Loss / Domain Loss: 5.59 2.08 0.26 0.26 0.75 0.62\n","Prediction Loss / Encoder Loss / Domain Loss: 4.61 2.16 0.26 0.25 0.77 0.64\n","Prediction Loss / Encoder Loss / Domain Loss: 5.09 2.40 0.26 0.25 0.80 0.66\n","Prediction Loss / Encoder Loss / Domain Loss: 5.33 2.28 0.26 0.26 0.77 0.64\n","Prediction Loss / Encoder Loss / Domain Loss: 4.27 2.53 0.26 0.25 0.77 0.65\n","Prediction Loss / Encoder Loss / Domain Loss: 4.95 2.63 0.26 0.25 0.77 0.64\n","Prediction Loss / Encoder Loss / Domain Loss: 4.14 2.37 0.26 0.25 0.77 0.63\n","Prediction Loss / Encoder Loss / Domain Loss: 4.93 2.31 0.26 0.26 0.79 0.65\n","Prediction Loss / Encoder Loss / Domain Loss: 4.86 1.72 0.26 0.26 0.79 0.65\n","Prediction Loss / Encoder Loss / Domain Loss: 4.49 2.11 0.25 0.25 0.76 0.64\n","Prediction Loss / Encoder Loss / Domain Loss: 4.42 2.07 0.26 0.26 0.76 0.65\n","Prediction Loss / Encoder Loss / Domain Loss: 4.30 2.33 0.27 0.26 0.76 0.64\n","Prediction Loss / Encoder Loss / Domain Loss: 5.02 2.37 0.26 0.26 0.77 0.65\n"]},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"f54b9b78c1cd49afa93c27ee9f5dc91b","version_minor":0,"version_major":2},"text/plain":["  0%|          | 0/42 [00:00<?, ?it/s]"]},"metadata":{}}]},{"cell_type":"markdown","metadata":{"id":"Qdf0K8U0ccnr"},"source":["## Extra Notes"]},{"cell_type":"markdown","metadata":{"id":"QN10zXQhcdxN"},"source":["### Inference Process"]},{"cell_type":"markdown","metadata":{"id":"LPRaVnTr2pRY"},"source":["<p><center><img src='_images/T519611_3.png'></center></p>"]},{"cell_type":"markdown","metadata":{"id":"hKLpv5VCcgtM"},"source":["### Domain-Aware Feature Extraction Example"]},{"cell_type":"markdown","metadata":{"id":"1GDyHYD3cuZP"},"source":["Following is the example of domain-aware feature extraction from a real-world benchmark dataset Amazon."]},{"cell_type":"markdown","metadata":{"id":"cqTLXp_I2rCs"},"source":["<p><center><img src='_images/T519611_4.png'></center></p>\n","\n","<p><center><img src='_images/T519611_5.png'></center></p>"]},{"cell_type":"markdown","metadata":{"id":"gEPw_zBdcwIP"},"source":["We assume two phases: training and inference, with two different domains: Musical Instruments and Toys & Games for cross-domain recommendation scenario. The scenario assumes a training phase with source (upper) and target (lower) domain. The difference is that a common FE (red-box) is shared across domains, while the source and target FEs (green and blue boxes) are domain-specific networks. **The objective is predicting a rating that a user 𝐴 gives on item 2**. Excluding individual review, user 𝐴′𝑠 review on item 2, the source and common extractors distillate latent of user and item respectively. Specifically, for user 𝐴 in 𝑀𝑢𝑠𝑖𝑐𝑎𝑙 𝐼𝑛𝑠𝑡𝑟𝑢𝑚𝑒𝑛𝑡𝑠, a source FE captures domain-specific knowledge that she makes much of sound quality, while common FE extracts domain-common information like beautiful, and nice price. The analysis for item 2 follows the same mechanism. To summarize, DaRE model not only considers domain-shareable knowledge with common FE but also reflects domain-specific information through the source and target FE."]},{"cell_type":"markdown","metadata":{"id":"eLSOoBHEcgnm"},"source":["### Review Encoder Example"]},{"cell_type":"markdown","metadata":{"id":"Ilf85u992u8N"},"source":["<p><center><img src='_images/T519611_6.png'></center></p>\n","\n","<p><center><img src='_images/T519611_7.png'></center></p>"]},{"cell_type":"markdown","metadata":{"id":"NTrmae_6cgiL"},"source":["For the training of a review encoder, we utilize individual review that user 𝐴 has written on item 2 (blue box) as another label. Taking the above figure as an example, the review encoder (purple box) takes four types of inputs which are extracted from the source and common FEs. Then, the encoder generates a single output, which contains mixed information of user 𝐴 and item 2. Here, the encoder is trained to infer an individual review, negative feedback of user 𝐴 who takes sound quality into account. Likewise, another encoder in a target domain can be trained in a same manner. With user and item’s previous reviews, the encoder assumes a real feedback that user will leave after purchasing an item."]},{"cell_type":"markdown","metadata":{"id":"gSfOe-Gocyhv"},"source":["**END**"]}]}