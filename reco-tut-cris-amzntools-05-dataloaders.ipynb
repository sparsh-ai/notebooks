{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"reco-tut-cris-amzntools-05-dataloaders.ipynb","provenance":[{"file_id":"1B_1DW5SyWDUxcIKlQbH4Nk0SX9GxCdOa","timestamp":1628062033021},{"file_id":"1Cc6BUED4BJN3vr-LQcOs07O6N_AJdJPz","timestamp":1628058942997},{"file_id":"1vdLDyovpu0iLdHkDfg4D_X1BIUK2mN_6","timestamp":1628018416069}],"collapsed_sections":[],"mount_file_id":"1atgiSIU7NMl43DmBaRqQ1TnfsC4_b_bJ","authorship_tag":"ABX9TyN0fWs/eLpfkjkPaefk1zXO"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"id":"Dg8frDmMWhHA","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1628062221981,"user_tz":-330,"elapsed":5887,"user":{"displayName":"Sparsh Agarwal","photoUrl":"","userId":"13037694610922482904"}},"outputId":"8c9441a2-08bc-4abf-be66-3522b82afd9f"},"source":["import os\n","project_name = \"reco-tut-cris\"; branch = \"main\"; account = \"sparsh-ai\"\n","project_path = os.path.join('/content', project_name)\n","\n","if not os.path.exists(project_path):\n","    !cp /content/drive/MyDrive/mykeys.py /content\n","    import mykeys\n","    !rm /content/mykeys.py\n","    path = \"/content/\" + project_name; \n","    !mkdir \"{path}\"\n","    %cd \"{path}\"\n","    import sys; sys.path.append(path)\n","    !git config --global user.email \"recotut@recohut.com\"\n","    !git config --global user.name  \"reco-tut\"\n","    !git init\n","    !git remote add origin https://\"{mykeys.git_token}\":x-oauth-basic@github.com/\"{account}\"/\"{project_name}\".git\n","    !git pull origin \"{branch}\"\n","    !git checkout main\n","else:\n","    %cd \"{project_path}\""],"execution_count":null,"outputs":[{"output_type":"stream","text":["/content/reco-tut-cris\n","Initialized empty Git repository in /content/reco-tut-cris/.git/\n","remote: Enumerating objects: 36, done.\u001b[K\n","remote: Counting objects: 100% (36/36), done.\u001b[K\n","remote: Compressing objects: 100% (27/27), done.\u001b[K\n","remote: Total 36 (delta 7), reused 31 (delta 3), pack-reused 0\u001b[K\n","Unpacking objects: 100% (36/36), done.\n","From https://github.com/sparsh-ai/reco-tut-cris\n"," * branch            main       -> FETCH_HEAD\n"," * [new branch]      main       -> origin/main\n","Branch 'main' set up to track remote branch 'main' from 'origin'.\n","Switched to a new branch 'main'\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Hwg3020-GL4U"},"source":["### Dataloader for Interest modeling"]},{"cell_type":"code","metadata":{"id":"J_bq26aXEhBE"},"source":["import os\n","import csv\n","import pdb\n","import time\n","import pickle\n","import numpy as np\n","from datetime import datetime\n","from dateutil.relativedelta import relativedelta\n","from sklearn.preprocessing import LabelEncoder\n","from torch.utils import data\n","from torch.utils.data.dataloader import default_collate"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"157QNwLiEy_k"},"source":["def toymd(time):\n","    return datetime.utcfromtimestamp(time)#.strftime('%Y-%m-%d')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2nguaw-hFJdh"},"source":["class Dataset(data.Dataset):\n","\n","    def __init__(self, data):\n","        st = time.time()\n","        \n","        self.iids, self.labels, self.timediffs = [], [], []\n","        self.most_oldtime = None\n","        \n","        for row in data:\n","            self.iids.append(row[0])\n","            self.labels.append(row[1])\n","            self.timediffs.append(row[2:])\n","            \n","        self.iids = np.array(self.iids)\n","        self.timediffs = np.array(self.timediffs).astype(int)\n","        self.labels = (np.array(self.labels) == 'True').astype(int) \n","        \n","        print('Data building time : %.1fs' % (time.time()-st))\n","        \n","    def __getitem__(self, index):\n","        return self.iids[index], self.timediffs[index], self.labels[index]\n","    \n","    def __len__(self):\n","        \"\"\"Returns the total number of user-item pairs.\"\"\"\n","        return len(self.timediffs)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"uiad4e3BFN_0"},"source":["def build_loader(eachdata, batch_size, shuffle=True, num_workers=0):\n","    \n","    def my_collate(batch):\n","        batch = [i for i in filter(lambda x:x is not None, batch)]\n","        return default_collate(batch)\n","    \n","    \"\"\"Builds and returns Dataloader.\"\"\"\n","    dataset = Dataset(eachdata)\n","    \n","    data_loader = data.DataLoader(dataset=dataset,\n","        batch_size=batch_size,\n","        shuffle=shuffle,\n","        num_workers=num_workers,\n","        collate_fn=my_collate)\n","\n","    return data_loader  "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2R_iv7xsE3Yi"},"source":["def build_data_directly(dpath, period, binsize):\n","    def toymd(time):\n","        return datetime.utcfromtimestamp(time)\n","    \n","    def build_data(true_items, item_feature):\n","        output = []\n","        for i in item_feature:\n","            feature = item_feature[i]\n","            instance = [i] + [bool(i in true_items)] + list(feature) # [iid, label, features]\n","            output.append(instance)    \n","        return np.array(output)\n","    \n","    def get_item_feature(data):\n","        times = data[:,-1].astype(float).astype(int)\n","        mintime, maxtime = toymd(min(times)), toymd(max(times))\n","\n","        # Binning training time (D_f) with fixed-sized bins\n","        timedelta = relativedelta(weeks=binsize)\n","        bins = np.array([mintime + timedelta*i for i in range(1000) # quick implementation\n","                         if mintime + timedelta*i < maxtime + timedelta*0])\n","\n","        # Build features from data\n","        idict = {}\n","        for u, i, r, t in data:\n","            if i not in idict: idict[i] = []\n","            idict[i].append(toymd(int(float(t))))\n","\n","        # Build features for each item\n","        item_feature = {}\n","        for i in idict:\n","            times = np.array(idict[i])\n","\n","            # Transform times into frequency bins\n","            binned_times = []\n","            for t in times:\n","                binidx = np.where(bins <= t)[0][-1]\n","                each_binfeature = np.zeros(len(bins))\n","                each_binfeature[binidx] = 1\n","                binned_times.append(each_binfeature)\n","            binned_times = np.array(binned_times).sum(axis=0).astype(int)\n","\n","            item_feature[i] = binned_times\n","            \n","        return item_feature\n","\n","    rawtrn = np.array([l for l in csv.reader(open(dpath+'train.csv'))])\n","    rawvld = np.array([l for l in csv.reader(open(dpath+'valid.csv'))])\n","    rawtst = np.array([l for l in csv.reader(open(dpath+'test.csv'))])\n","    \n","    times_trn = rawtrn[:,-1].astype(int)\n","    \n","    # Split data by period (unit: week)\n","    # [trn_start - trnfront - vld_start - tst_start - tst_end]\n","    trnfront_time = times_trn.max() - 60 * 60 * 24 * 7 * period \n","    trnfront_idx = np.where(times_trn < trnfront_time)[0][-1]\n","    trn_start_time = int(float(times_trn[0])) # -1 denotes the time index\n","    trnfront_start_time = int(float(rawtrn[trnfront_idx][-1]))\n","    vld_start_time = int(float(rawvld[0][-1]))\n","    tst_start_time = int(float(rawtst[0][-1]))\n","    tst_end_time = int(float(rawtst[-1][-1]))\n","    \n","    print('\\nðŸ“‹ Data loaded from: {}\\n'.format(dpath))\n","\n","    print('Trn start time:\\t{}'.format(toymd(trn_start_time)))\n","    print('Trn front time:\\t{}'.format(toymd(trnfront_start_time)))\n","    print('Vld start time:\\t{}'.format(toymd(vld_start_time)))\n","    print('Tst start time:\\t{}'.format(toymd(tst_start_time)))\n","    print('Tst end time:\\t{}'.format(toymd(tst_end_time)))\n","    \n","    trn_4feature = rawtrn[:trnfront_idx]\n","    feature_trn = get_item_feature(trn_4feature) # features for training\n","    feature_eval = get_item_feature(rawtrn) # features for evaluation (to get ISS for training RS)\n","    \n","    trn_4label = rawtrn[trnfront_idx:] # D_b\n","    \n","    trndata = build_data(set(trn_4label[:,1]), feature_trn)\n","    vlddata = build_data(set(rawvld[:,1]), feature_eval)\n","    tstdata = build_data(set(rawtst[:,1]), feature_eval)\n","    \n","    return trndata, vlddata, tstdata"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Ifd1oGXyFnzl"},"source":["class DataLoader:\n","    def __init__(self, opt):\n","        self.dpath = opt.dataset_path + '/'\n","        self.batch_size = opt.batch_size\n","        \n","        trndata, vlddata, tstdata = build_data_directly(self.dpath, opt.period, opt.binsize)        \n","        \n","        self.trn_loader = build_loader(trndata, opt.batch_size, shuffle=True)\n","        self.vld_loader = build_loader(vlddata, opt.batch_size, shuffle=False)\n","        self.tst_loader = build_loader(tstdata, opt.batch_size, shuffle=False)\n","        \n","        print((\"train/val/test/ divided by batch size {:d}/{:d}/{:d}\".format(len(self.trn_loader), len(self.vld_loader),len(self.tst_loader))))\n","        print(\"==================================================================================\")\n","            \n","    def get_loaders(self):\n","        return self.trn_loader, self.vld_loader, self.tst_loader\n","    \n","    def get_embedding(self):\n","        return self.input_embedding"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MOI9KSIIHgqp"},"source":["### Unit testing"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kb_GXlmSH1Jl","executionInfo":{"status":"ok","timestamp":1628063380215,"user_tz":-330,"elapsed":628,"user":{"displayName":"Sparsh Agarwal","photoUrl":"","userId":"13037694610922482904"}},"outputId":"d1ae4a8f-30d3-492e-b4c2-d2a212303bb1"},"source":["import argparse\n","\n","parser = argparse.ArgumentParser()\n","parser.add_argument('--dataset', default='amazon_tools', type=str)    \n","parser.add_argument('--period', default=16, type=float)\n","parser.add_argument('--binsize', default=8, type=int)\n","parser.add_argument('--learning_rate', default=1e-2, type=float)\n","parser.add_argument('--l2reg', default=1e-4, type=float)\n","parser.add_argument('--num_epoch', default=100, type=int)\n","parser.add_argument('--batch_size', default=128, type=int)    \n","parser.add_argument('--hidden_dim', default=64, type=int)    \n","parser.add_argument('--pos_weight', default=1e-2, type=float)   \n","parser.add_argument('--gpu', default=3, type=int)       \n","\n","opt = parser.parse_args(args={})\n","dataset_path = './data/silver/{}'.format(opt.dataset)    \n","\n","opt.dataset_path = dataset_path\n","opt"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Namespace(batch_size=128, binsize=8, dataset='amazon_tools', dataset_path='./data/silver/amazon_tools', gpu=3, hidden_dim=64, l2reg=0.0001, learning_rate=0.01, num_epoch=100, period=16, pos_weight=0.01)"]},"metadata":{"tags":[]},"execution_count":22}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"50Qr_uYHHgJX","executionInfo":{"status":"ok","timestamp":1628063464490,"user_tz":-330,"elapsed":9043,"user":{"displayName":"Sparsh Agarwal","photoUrl":"","userId":"13037694610922482904"}},"outputId":"6b2fe302-685c-4a44-f2b2-3e6fa1dde97a"},"source":["from collections import Counter\n","\n","data_loader = DataLoader(opt)\n","trn_loader, vld_loader, tst_loader = data_loader.get_loaders()\n","\n","trnlen = trn_loader.dataset.timediffs.shape[1]        \n","\n","print('TRN labels: {}'.format(Counter(trn_loader.dataset.labels)))\n","print('VLD labels: {}'.format(Counter(vld_loader.dataset.labels)))\n","print('TST labels: {}'.format(Counter(tst_loader.dataset.labels)))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["\n","ðŸ“‹ Data loaded from: ./data/silver/amazon_tools/\n","\n","Trn start time:\t1999-11-08 00:00:00\n","Trn front time:\t2014-01-30 00:00:00\n","Vld start time:\t2014-05-23 00:00:00\n","Tst start time:\t2014-06-23 00:00:00\n","Tst end time:\t2014-07-22 00:00:00\n","Data building time : 0.4s\n","Data building time : 0.4s\n","Data building time : 0.4s\n","train/val/test/ divided by batch size 79/80/80\n","==================================================================================\n","TRN labels: Counter({1: 6158, 0: 3891})\n","VLD labels: Counter({0: 7945, 1: 2232})\n","TST labels: Counter({0: 8092, 1: 2085})\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Co36ZXYgGGIw"},"source":["### Dataloader for recommendation modeling"]},{"cell_type":"code","metadata":{"id":"RiDKoWXsGqQK"},"source":["import os\n","import pdb\n","import time\n","import torch\n","import pickle\n","import random\n","import numpy as np\n","import pandas as pd\n","from torch.utils import data\n","from torch.utils.data.dataloader import default_collate\n","\n","random.seed(2020)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"dQQDpx8YGTj5"},"source":["class ML_Dataset(data.Dataset):\n","    \n","    def build_consumption_history(self, uir):\n","        # Build a dictionary for user: items consumed by the user\n","        uir = uir.astype(int)\n","        uidict = {}\n","        allitems = set()\n","        for u, i, _ in uir:\n","            if u not in uidict: uidict[u] = set()\n","            uidict[u].add(i)\n","            allitems.add(i)\n","            \n","        self.ui_cand_dict = {}    \n","        for u in uidict:\n","            self.ui_cand_dict[u] = np.array(list(allitems - uidict[u]))\n","        \n","        return uidict, allitems\n","        \n","    def __init__(self, path, trn_numneg):\n","        dpath = '/'.join(path.split('/')[:-1])\n","        if dpath[-1] != '/': dpath += '/'\n","        dtype = path.split('/')[-1].split('.')[0]\n","        \n","        st = time.time()        \n","        \n","        if dtype == 'train': self.numneg = trn_numneg\n","        self.uir = np.load(path)\n","\n","        if dtype == 'train':             \n","            self.uir[:,-1] = 1 # Mark explicit feedback as implicit feedback\n","\n","            self.first = self.uir[:,0].astype(int)\n","            self.second = self.uir[:,1].astype(int)\n","            self.third = np.zeros(self.uir.shape[0]) # This will be replaced in 'train_collate'\n","            \n","            self.numuser = len(set(self.uir[:,0].astype(int)))\n","            self.numitem = len(set(self.uir[:,1].astype(int)))\n","            \n","            self.uidict, self.allitems = self.build_consumption_history(self.uir)\n","            \n","        elif dtype == 'valid' or dtype == 'test':             \n","            # Build validation data for ranking evaluation\n","            newuir = []\n","            for row in self.uir:\n","                user = row[0]\n","                true_item = row[1]\n","                newuir.append([user, true_item, 1]) # a true consumption\n","                for item in row[2:]: newuir.append([user, item, 0]) # negative candidates\n","            self.uir = np.array(newuir) # User, Item, Rating\n","        \n","            self.first, self.second, self.third = self.uir[:,0], self.uir[:,1], self.uir[:,2]\n","        \n","        \n","        print('Data building time : %.1fs' % (time.time()-st))\n","\n","    def __getitem__(self, index):\n","        # Training: [user, positive, negative]\n","        # Testing: [user, canidate item, label] \n","        return self.first[index], self.second[index], self.third[index]\n","    \n","    def __len__(self):\n","        \"\"\"Returns the total number of user-item pairs.\"\"\"\n","        return len(self.first)\n","    \n","    \n","    def train_collate(self, batch):\n","        # Input: [user, postive item, dummy]\n","        # Output: [user, positive item, negative item]\n","        batch = [i for i in filter(lambda x:x is not None, batch)]\n","        \n","        # Negative sampling for each batch\n","        outputs = []\n","        for u, pi, dummy in batch:\n","            rand_idx = np.random.randint(len(self.ui_cand_dict[u]), size=self.numneg)\n","            neg_items = self.ui_cand_dict[u][rand_idx]\n","            \n","            for ni in neg_items: \n","                outputs.append([u, pi, ni])\n","            \n","        return default_collate(outputs)      "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4vypXyYKGmDH"},"source":["def test_collate(batch):\n","    batch = [i for i in filter(lambda x:x is not None, batch)]\n","    return default_collate(batch)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"lKsWcrQpGkdH"},"source":["def get_each_loader(data_path, batch_size, trn_negnum, shuffle=True, num_workers=0):\n","    \"\"\"Builds and returns Dataloader.\"\"\"\n","    \n","    dataset = ML_Dataset(data_path, trn_negnum)\n","    \n","    if data_path.endswith('train.npy') == True:\n","        collate = dataset.train_collate\n","    else:\n","        collate = test_collate\n","\n","    data_loader = data.DataLoader(dataset=dataset,\n","        batch_size=batch_size,\n","        shuffle=shuffle,\n","        num_workers=num_workers,\n","        collate_fn=collate)\n","\n","    return data_loader"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9PME29ToGdMJ"},"source":["class DataLoader:\n","    def __init__(self, opt):\n","        self.dpath = opt.dataset_path + '/'\n","        self.batch_size = opt.batch_size\n","        self.trn_numneg = opt.numneg\n","        \n","        self.trn_loader, self.vld_loader, self.tst_loader = self.get_loaders_for_metric_learning(self.trn_numneg)\n","    \n","        print((\"train/val/test/ divided by batch size {:d}/{:d}/{:d}\".format(len(self.trn_loader), len(self.vld_loader),len(self.tst_loader))))\n","        print(\"=\" * 80)\n","        \n","    def get_loaders_for_metric_learning(self, trn_numneg):\n","        print(\"\\nðŸ“‹ Loading data...\\n\")\n","        trn_loader = get_each_loader(self.dpath+'train.npy', self.batch_size, trn_numneg, shuffle=True)\n","        print('\\tTraining data loaded')\n","        \n","        vld_loader = get_each_loader(self.dpath+'valid.npy', self.batch_size, trn_numneg, shuffle=False)\n","        print('\\tValidation data loaded')\n","        \n","        tst_loader = get_each_loader(self.dpath+'test.npy', self.batch_size, trn_numneg, shuffle=False)\n","        print('\\tTest data loaded')\n","        \n","        return trn_loader, vld_loader, tst_loader\n","    \n","    def get_loaders(self):\n","        return self.trn_loader, self.vld_loader, self.tst_loader\n","    \n","    def get_embedding(self):\n","        return self.input_embedding"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"AEAkK5qGM8wF"},"source":["### Unit testing"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6-JxFH-aM8wW","executionInfo":{"status":"ok","timestamp":1628064656757,"user_tz":-330,"elapsed":579,"user":{"displayName":"Sparsh Agarwal","photoUrl":"","userId":"13037694610922482904"}},"outputId":"2c0e8d79-362d-44f0-9163-eaf1babc2934"},"source":["import argparse\n","\n","parser = argparse.ArgumentParser()\n","parser.add_argument('--dataset', default='amazon_tools', type=str)    \n","parser.add_argument('--batch_size', default=4096, type=int)      \n","parser.add_argument('--numneg', default=10, type=int)\n","\n","opt = parser.parse_args(args={})\n","dataset_path = './data/gold/{}'.format(opt.dataset)    \n","\n","opt.dataset_path = dataset_path\n","opt"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Namespace(batch_size=4096, dataset='amazon_tools', dataset_path='./data/gold/amazon_tools', learning_rate=0.01, num_epoch=50, numneg=10)"]},"metadata":{"tags":[]},"execution_count":40}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"e7a3BiecM8wY","executionInfo":{"status":"ok","timestamp":1628064752679,"user_tz":-330,"elapsed":23655,"user":{"displayName":"Sparsh Agarwal","photoUrl":"","userId":"13037694610922482904"}},"outputId":"2359660a-6929-4990-a950-99aceb64792f"},"source":["data_loader = DataLoader(opt)\n","\n","trn_loader, vld_loader, tst_loader = data_loader.get_loaders()\n","\n","opt.numuser = trn_loader.dataset.numuser\n","opt.numitem = trn_loader.dataset.numitem"],"execution_count":null,"outputs":[{"output_type":"stream","text":["\n","ðŸ“‹ Loading data...\n","\n","Data building time : 21.9s\n","\tTraining data loaded\n","Data building time : 0.8s\n","\tValidation data loaded\n","Data building time : 0.7s\n","\tTest data loaded\n","train/val/test/ divided by batch size 31/85/73\n","================================================================================\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"uWHShmo2OOa7"},"source":["## Exporting the methods"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nBdOOntLOQ8v","executionInfo":{"status":"ok","timestamp":1628064889575,"user_tz":-330,"elapsed":683,"user":{"displayName":"Sparsh Agarwal","photoUrl":"","userId":"13037694610922482904"}},"outputId":"3b7c5032-6bd9-401d-a34e-bdbc74db67f8"},"source":["%%writefile ./code/dataloader_interest.py\n","import os\n","import csv\n","import pdb\n","import time\n","import pickle\n","import numpy as np\n","from datetime import datetime\n","from dateutil.relativedelta import relativedelta\n","from sklearn.preprocessing import LabelEncoder\n","from torch.utils import data\n","from torch.utils.data.dataloader import default_collate\n","\n","\n","def toymd(time):\n","    return datetime.utcfromtimestamp(time)#.strftime('%Y-%m-%d')\n","\n","\n","class Dataset(data.Dataset):\n","\n","    def __init__(self, data):\n","        st = time.time()\n","        \n","        self.iids, self.labels, self.timediffs = [], [], []\n","        self.most_oldtime = None\n","        \n","        for row in data:\n","            self.iids.append(row[0])\n","            self.labels.append(row[1])\n","            self.timediffs.append(row[2:])\n","            \n","        self.iids = np.array(self.iids)\n","        self.timediffs = np.array(self.timediffs).astype(int)\n","        self.labels = (np.array(self.labels) == 'True').astype(int) \n","        \n","        print('Data building time : %.1fs' % (time.time()-st))\n","        \n","    def __getitem__(self, index):\n","        return self.iids[index], self.timediffs[index], self.labels[index]\n","    \n","    def __len__(self):\n","        \"\"\"Returns the total number of user-item pairs.\"\"\"\n","        return len(self.timediffs)\n","\n","\n","def build_loader(eachdata, batch_size, shuffle=True, num_workers=0):\n","    \n","    def my_collate(batch):\n","        batch = [i for i in filter(lambda x:x is not None, batch)]\n","        return default_collate(batch)\n","    \n","    \"\"\"Builds and returns Dataloader.\"\"\"\n","    dataset = Dataset(eachdata)\n","    \n","    data_loader = data.DataLoader(dataset=dataset,\n","        batch_size=batch_size,\n","        shuffle=shuffle,\n","        num_workers=num_workers,\n","        collate_fn=my_collate)\n","\n","    return data_loader  \n","\n","\n","def build_data_directly(dpath, period, binsize):\n","    def toymd(time):\n","        return datetime.utcfromtimestamp(time)\n","    \n","    def build_data(true_items, item_feature):\n","        output = []\n","        for i in item_feature:\n","            feature = item_feature[i]\n","            instance = [i] + [bool(i in true_items)] + list(feature) # [iid, label, features]\n","            output.append(instance)    \n","        return np.array(output)\n","    \n","    def get_item_feature(data):\n","        times = data[:,-1].astype(float).astype(int)\n","        mintime, maxtime = toymd(min(times)), toymd(max(times))\n","\n","        # Binning training time (D_f) with fixed-sized bins\n","        timedelta = relativedelta(weeks=binsize)\n","        bins = np.array([mintime + timedelta*i for i in range(1000) # quick implementation\n","                         if mintime + timedelta*i < maxtime + timedelta*0])\n","\n","        # Build features from data\n","        idict = {}\n","        for u, i, r, t in data:\n","            if i not in idict: idict[i] = []\n","            idict[i].append(toymd(int(float(t))))\n","\n","        # Build features for each item\n","        item_feature = {}\n","        for i in idict:\n","            times = np.array(idict[i])\n","\n","            # Transform times into frequency bins\n","            binned_times = []\n","            for t in times:\n","                binidx = np.where(bins <= t)[0][-1]\n","                each_binfeature = np.zeros(len(bins))\n","                each_binfeature[binidx] = 1\n","                binned_times.append(each_binfeature)\n","            binned_times = np.array(binned_times).sum(axis=0).astype(int)\n","\n","            item_feature[i] = binned_times\n","            \n","        return item_feature\n","\n","    rawtrn = np.array([l for l in csv.reader(open(dpath+'train.csv'))])\n","    rawvld = np.array([l for l in csv.reader(open(dpath+'valid.csv'))])\n","    rawtst = np.array([l for l in csv.reader(open(dpath+'test.csv'))])\n","    \n","    times_trn = rawtrn[:,-1].astype(int)\n","    \n","    # Split data by period (unit: week)\n","    # [trn_start - trnfront - vld_start - tst_start - tst_end]\n","    trnfront_time = times_trn.max() - 60 * 60 * 24 * 7 * period \n","    trnfront_idx = np.where(times_trn < trnfront_time)[0][-1]\n","    trn_start_time = int(float(times_trn[0])) # -1 denotes the time index\n","    trnfront_start_time = int(float(rawtrn[trnfront_idx][-1]))\n","    vld_start_time = int(float(rawvld[0][-1]))\n","    tst_start_time = int(float(rawtst[0][-1]))\n","    tst_end_time = int(float(rawtst[-1][-1]))\n","    \n","    print('\\nðŸ“‹ Data loaded from: {}\\n'.format(dpath))\n","\n","    print('Trn start time:\\t{}'.format(toymd(trn_start_time)))\n","    print('Trn front time:\\t{}'.format(toymd(trnfront_start_time)))\n","    print('Vld start time:\\t{}'.format(toymd(vld_start_time)))\n","    print('Tst start time:\\t{}'.format(toymd(tst_start_time)))\n","    print('Tst end time:\\t{}'.format(toymd(tst_end_time)))\n","    \n","    trn_4feature = rawtrn[:trnfront_idx]\n","    feature_trn = get_item_feature(trn_4feature) # features for training\n","    feature_eval = get_item_feature(rawtrn) # features for evaluation (to get ISS for training RS)\n","    \n","    trn_4label = rawtrn[trnfront_idx:] # D_b\n","    \n","    trndata = build_data(set(trn_4label[:,1]), feature_trn)\n","    vlddata = build_data(set(rawvld[:,1]), feature_eval)\n","    tstdata = build_data(set(rawtst[:,1]), feature_eval)\n","    \n","    return trndata, vlddata, tstdata\n","\n","\n","class DataLoader:\n","    def __init__(self, opt):\n","        self.dpath = opt.dataset_path + '/'\n","        self.batch_size = opt.batch_size\n","        \n","        trndata, vlddata, tstdata = build_data_directly(self.dpath, opt.period, opt.binsize)        \n","        \n","        self.trn_loader = build_loader(trndata, opt.batch_size, shuffle=True)\n","        self.vld_loader = build_loader(vlddata, opt.batch_size, shuffle=False)\n","        self.tst_loader = build_loader(tstdata, opt.batch_size, shuffle=False)\n","        \n","        print((\"train/val/test/ divided by batch size {:d}/{:d}/{:d}\".format(len(self.trn_loader), len(self.vld_loader),len(self.tst_loader))))\n","        print(\"==================================================================================\")\n","            \n","    def get_loaders(self):\n","        return self.trn_loader, self.vld_loader, self.tst_loader\n","    \n","    def get_embedding(self):\n","        return self.input_embedding"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Writing ./code/dataloader_interest.py\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OvWExDTSOstr","executionInfo":{"status":"ok","timestamp":1628064979872,"user_tz":-330,"elapsed":10,"user":{"displayName":"Sparsh Agarwal","photoUrl":"","userId":"13037694610922482904"}},"outputId":"b0a60847-522a-4350-b24a-40a42c3e2f92"},"source":["%%writefile ./code/dataloader_recommendation.py\n","import os\n","import pdb\n","import time\n","import torch\n","import pickle\n","import random\n","import numpy as np\n","import pandas as pd\n","from torch.utils import data\n","from torch.utils.data.dataloader import default_collate\n","\n","random.seed(2020)\n","\n","\n","class ML_Dataset(data.Dataset):\n","    \n","    def build_consumption_history(self, uir):\n","        # Build a dictionary for user: items consumed by the user\n","        uir = uir.astype(int)\n","        uidict = {}\n","        allitems = set()\n","        for u, i, _ in uir:\n","            if u not in uidict: uidict[u] = set()\n","            uidict[u].add(i)\n","            allitems.add(i)\n","            \n","        self.ui_cand_dict = {}    \n","        for u in uidict:\n","            self.ui_cand_dict[u] = np.array(list(allitems - uidict[u]))\n","        \n","        return uidict, allitems\n","        \n","    def __init__(self, path, trn_numneg):\n","        dpath = '/'.join(path.split('/')[:-1])\n","        if dpath[-1] != '/': dpath += '/'\n","        dtype = path.split('/')[-1].split('.')[0]\n","        \n","        st = time.time()        \n","        \n","        if dtype == 'train': self.numneg = trn_numneg\n","        self.uir = np.load(path)\n","\n","        if dtype == 'train':             \n","            self.uir[:,-1] = 1 # Mark explicit feedback as implicit feedback\n","\n","            self.first = self.uir[:,0].astype(int)\n","            self.second = self.uir[:,1].astype(int)\n","            self.third = np.zeros(self.uir.shape[0]) # This will be replaced in 'train_collate'\n","            \n","            self.numuser = len(set(self.uir[:,0].astype(int)))\n","            self.numitem = len(set(self.uir[:,1].astype(int)))\n","            \n","            self.uidict, self.allitems = self.build_consumption_history(self.uir)\n","            \n","        elif dtype == 'valid' or dtype == 'test':             \n","            # Build validation data for ranking evaluation\n","            newuir = []\n","            for row in self.uir:\n","                user = row[0]\n","                true_item = row[1]\n","                newuir.append([user, true_item, 1]) # a true consumption\n","                for item in row[2:]: newuir.append([user, item, 0]) # negative candidates\n","            self.uir = np.array(newuir) # User, Item, Rating\n","        \n","            self.first, self.second, self.third = self.uir[:,0], self.uir[:,1], self.uir[:,2]\n","        \n","        \n","        print('Data building time : %.1fs' % (time.time()-st))\n","\n","    def __getitem__(self, index):\n","        # Training: [user, positive, negative]\n","        # Testing: [user, canidate item, label] \n","        return self.first[index], self.second[index], self.third[index]\n","    \n","    def __len__(self):\n","        \"\"\"Returns the total number of user-item pairs.\"\"\"\n","        return len(self.first)\n","    \n","    \n","    def train_collate(self, batch):\n","        # Input: [user, postive item, dummy]\n","        # Output: [user, positive item, negative item]\n","        batch = [i for i in filter(lambda x:x is not None, batch)]\n","        \n","        # Negative sampling for each batch\n","        outputs = []\n","        for u, pi, dummy in batch:\n","            rand_idx = np.random.randint(len(self.ui_cand_dict[u]), size=self.numneg)\n","            neg_items = self.ui_cand_dict[u][rand_idx]\n","            \n","            for ni in neg_items: \n","                outputs.append([u, pi, ni])\n","            \n","        return default_collate(outputs)      \n","\n","\n","def test_collate(batch):\n","    batch = [i for i in filter(lambda x:x is not None, batch)]\n","    return default_collate(batch)\n","\n","\n","def get_each_loader(data_path, batch_size, trn_negnum, shuffle=True, num_workers=0):\n","    \"\"\"Builds and returns Dataloader.\"\"\"\n","    \n","    dataset = ML_Dataset(data_path, trn_negnum)\n","    \n","    if data_path.endswith('train.npy') == True:\n","        collate = dataset.train_collate\n","    else:\n","        collate = test_collate\n","\n","    data_loader = data.DataLoader(dataset=dataset,\n","        batch_size=batch_size,\n","        shuffle=shuffle,\n","        num_workers=num_workers,\n","        collate_fn=collate)\n","\n","    return data_loader\n","\n","\n","class DataLoader:\n","    def __init__(self, opt):\n","        self.dpath = opt.dataset_path + '/'\n","        self.batch_size = opt.batch_size\n","        self.trn_numneg = opt.numneg\n","        \n","        self.trn_loader, self.vld_loader, self.tst_loader = self.get_loaders_for_metric_learning(self.trn_numneg)\n","    \n","        print((\"train/val/test/ divided by batch size {:d}/{:d}/{:d}\".format(len(self.trn_loader), len(self.vld_loader),len(self.tst_loader))))\n","        print(\"=\" * 80)\n","        \n","    def get_loaders_for_metric_learning(self, trn_numneg):\n","        print(\"\\nðŸ“‹ Loading data...\\n\")\n","        trn_loader = get_each_loader(self.dpath+'train.npy', self.batch_size, trn_numneg, shuffle=True)\n","        print('\\tTraining data loaded')\n","        \n","        vld_loader = get_each_loader(self.dpath+'valid.npy', self.batch_size, trn_numneg, shuffle=False)\n","        print('\\tValidation data loaded')\n","        \n","        tst_loader = get_each_loader(self.dpath+'test.npy', self.batch_size, trn_numneg, shuffle=False)\n","        print('\\tTest data loaded')\n","        \n","        return trn_loader, vld_loader, tst_loader\n","    \n","    def get_loaders(self):\n","        return self.trn_loader, self.vld_loader, self.tst_loader\n","    \n","    def get_embedding(self):\n","        return self.input_embedding"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Writing ./code/dataloader_recommendation.py\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cZQxNJELPJAE","executionInfo":{"status":"ok","timestamp":1628065019801,"user_tz":-330,"elapsed":618,"user":{"displayName":"Sparsh Agarwal","photoUrl":"","userId":"13037694610922482904"}},"outputId":"476edcb5-999c-471b-a3e9-180c69757818"},"source":["!git status"],"execution_count":null,"outputs":[{"output_type":"stream","text":["On branch main\n","Your branch is up to date with 'origin/main'.\n","\n","Untracked files:\n","  (use \"git add <file>...\" to include in what will be committed)\n","\n","\t\u001b[31mcode/dataloader_interest.py\u001b[m\n","\t\u001b[31mcode/dataloader_recommendation.py\u001b[m\n","\n","nothing added to commit but untracked files present (use \"git add\" to track)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ojGU36JOPJjD","executionInfo":{"status":"ok","timestamp":1628065057555,"user_tz":-330,"elapsed":1412,"user":{"displayName":"Sparsh Agarwal","photoUrl":"","userId":"13037694610922482904"}},"outputId":"6d45f82a-91a9-44ab-999f-5f33e6769298"},"source":["!git add . && git commit -m 'ADD code dataloaders for interest modeling and recommendations' && git push origin main"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[main e7a3c51] ADD code dataloaders for interest modeling and recommendations\n"," 2 files changed, 312 insertions(+)\n"," create mode 100644 code/dataloader_interest.py\n"," create mode 100644 code/dataloader_recommendation.py\n","Counting objects: 5, done.\n","Delta compression using up to 2 threads.\n","Compressing objects: 100% (5/5), done.\n","Writing objects: 100% (5/5), 3.80 KiB | 3.80 MiB/s, done.\n","Total 5 (delta 1), reused 0 (delta 0)\n","remote: Resolving deltas: 100% (1/1), completed with 1 local object.\u001b[K\n","To https://github.com/sparsh-ai/reco-tut-cris.git\n","   aa75f54..e7a3c51  main -> main\n"],"name":"stdout"}]}]}