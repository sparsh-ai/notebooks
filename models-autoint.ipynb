{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp models.autoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AutoInt\n",
    "> A pytorch implementation of AutoInt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev.showdoc import *\n",
    "from fastcore.nb_imports import *\n",
    "from fastcore.test import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "from recohut.models.layers.embedding import EmbeddingLayer\n",
    "from recohut.models.layers.common import MLP_Layer, LR_Layer\n",
    "from recohut.models.layers.attention import MultiHeadSelfAttention\n",
    "\n",
    "from recohut.models.bases.ctr import CTRModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class AutoInt(CTRModel):\n",
    "    def __init__(self, \n",
    "                 feature_map, \n",
    "                 model_id=\"AutoInt\",\n",
    "                 task=\"binary_classification\",\n",
    "                 learning_rate=1e-3, \n",
    "                 embedding_initializer=\"torch.nn.init.normal_(std=1e-4)\",\n",
    "                 embedding_dim=10, \n",
    "                 dnn_hidden_units=[64, 64, 64], \n",
    "                 dnn_activations=\"ReLU\", \n",
    "                 attention_layers=2,\n",
    "                 num_heads=1,\n",
    "                 attention_dim=8,\n",
    "                 net_dropout=0, \n",
    "                 batch_norm=False,\n",
    "                 layer_norm=False,\n",
    "                 use_scale=False,\n",
    "                 use_wide=False,\n",
    "                 use_residual=True,\n",
    "                 **kwargs):\n",
    "        super(AutoInt, self).__init__(feature_map, \n",
    "                                           model_id=model_id,\n",
    "                                           **kwargs)\n",
    "        self.embedding_layer = EmbeddingLayer(feature_map, embedding_dim)\n",
    "        self.lr_layer = LR_Layer(feature_map, output_activation=None, use_bias=False) \\\n",
    "                        if use_wide else None\n",
    "        self.dnn = MLP_Layer(input_dim=embedding_dim * feature_map.num_fields,\n",
    "                             output_dim=1, \n",
    "                             hidden_units=dnn_hidden_units,\n",
    "                             hidden_activations=dnn_activations,\n",
    "                             output_activation=None, \n",
    "                             dropout_rates=net_dropout, \n",
    "                             batch_norm=batch_norm, \n",
    "                             use_bias=True) \\\n",
    "                   if dnn_hidden_units else None # in case no DNN used\n",
    "        self.self_attention = nn.Sequential(\n",
    "            *[MultiHeadSelfAttention(embedding_dim if i == 0 else num_heads * attention_dim,\n",
    "                                    attention_dim=attention_dim, \n",
    "                                    num_heads=num_heads, \n",
    "                                    dropout_rate=net_dropout, \n",
    "                                    use_residual=use_residual, \n",
    "                                    use_scale=use_scale, \n",
    "                                    layer_norm=layer_norm,\n",
    "                                    align_to=\"output\") \n",
    "             for i in range(attention_layers)])\n",
    "        self.fc = nn.Linear(feature_map.num_fields * attention_dim * num_heads, 1)\n",
    "        self.output_activation = self.get_final_activation(task)\n",
    "        self.init_weights(embedding_initializer=embedding_initializer)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        feature_emb = self.embedding_layer(inputs)\n",
    "        attention_out = self.self_attention(feature_emb)\n",
    "        attention_out = torch.flatten(attention_out, start_dim=1)\n",
    "        y_pred = self.fc(attention_out)\n",
    "        if self.dnn is not None:\n",
    "            y_pred += self.dnn(feature_emb.flatten(start_dim=1))\n",
    "        if self.lr_layer is not None:\n",
    "            y_pred += self.lr_layer(X)\n",
    "        if self.output_activation is not None:\n",
    "            y_pred = self.output_activation(y_pred)\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'model_id': 'AutoInt',\n",
    "              'data_dir': '/content/data',\n",
    "              'model_root': './checkpoints/',\n",
    "              'learning_rate': 1e-3,\n",
    "              'optimizer': 'adamw',\n",
    "              'task': 'binary_classification',\n",
    "              'loss': 'binary_crossentropy',\n",
    "              'metrics': ['logloss', 'AUC'],\n",
    "              'embedding_dim': 10,\n",
    "              'dnn_hidden_units': [400, 400],\n",
    "              'dnn_activations': 'relu',\n",
    "              'net_dropout': 0,\n",
    "              'num_heads': 2,\n",
    "              'attention_layers': 3,\n",
    "              'attention_dim': 40,\n",
    "              'use_residual': True,\n",
    "              'batch_norm': False,\n",
    "              'layer_norm': False,\n",
    "              'use_scale': False,\n",
    "              'use_wide': False,\n",
    "              'batch_size': 64,\n",
    "              'epochs': 3,\n",
    "              'shuffle': True,\n",
    "              'seed': 2019,\n",
    "              'use_hdf5': True,\n",
    "              'workers': 1,\n",
    "              'verbose': 0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoInt(ds.dataset.feature_map, **params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "\n",
      "  | Name              | Type           | Params\n",
      "-----------------------------------------------------\n",
      "0 | embedding_layer   | EmbeddingLayer | 4.8 K \n",
      "1 | dnn               | MLP_Layer      | 217 K \n",
      "2 | self_attention    | Sequential     | 41.6 K\n",
      "3 | fc                | Linear         | 1.1 K \n",
      "4 | output_activation | Sigmoid        | 0     \n",
      "-----------------------------------------------------\n",
      "264 K     Trainable params\n",
      "0         Non-trainable params\n",
      "264 K     Total params\n",
      "1.059     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4c2144a82de47aab2d2139e026451c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f04011affc4f49a395a1200a7ca3aed0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7664a5cf645948e6b2ffbc03ea941b4e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{'Test Metrics': {'AUC': tensor(1.), 'logloss': tensor(0.1913)}}\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'Test Metrics': {'AUC': tensor(1.), 'logloss': tensor(0.1913)}}]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pl_trainer(model, ds, max_epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from recohut.models.layers.common import FeaturesEmbedding, FeaturesLinear, MultiLayerPerceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class AutoInt_v2(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    A pytorch implementation of AutoInt.\n",
    "    Reference:\n",
    "        W Song, et al. AutoInt: Automatic Feature Interaction Learning via Self-Attentive Neural Networks, 2018.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, field_dims, embed_dim, atten_embed_dim, num_heads, num_layers, mlp_dims, dropouts, has_residual=True):\n",
    "        super().__init__()\n",
    "        self.num_fields = len(field_dims)\n",
    "        self.linear = FeaturesLinear(field_dims)\n",
    "        self.embedding = FeaturesEmbedding(field_dims, embed_dim)\n",
    "        self.atten_embedding = torch.nn.Linear(embed_dim, atten_embed_dim)\n",
    "        self.embed_output_dim = len(field_dims) * embed_dim\n",
    "        self.atten_output_dim = len(field_dims) * atten_embed_dim\n",
    "        self.has_residual = has_residual\n",
    "        self.mlp = MultiLayerPerceptron(self.embed_output_dim, mlp_dims, dropouts[1])\n",
    "        self.self_attns = torch.nn.ModuleList([\n",
    "            torch.nn.MultiheadAttention(atten_embed_dim, num_heads, dropout=dropouts[0]) for _ in range(num_layers)\n",
    "        ])\n",
    "        self.attn_fc = torch.nn.Linear(self.atten_output_dim, 1)\n",
    "        if self.has_residual:\n",
    "            self.V_res_embedding = torch.nn.Linear(embed_dim, atten_embed_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        :param x: Long tensor of size ``(batch_size, num_fields)``\n",
    "        \"\"\"\n",
    "        embed_x = self.embedding(x)\n",
    "        atten_x = self.atten_embedding(embed_x)\n",
    "        cross_term = atten_x.transpose(0, 1)\n",
    "        for self_attn in self.self_attns:\n",
    "            cross_term, _ = self_attn(cross_term, cross_term, cross_term)\n",
    "        cross_term = cross_term.transpose(0, 1)\n",
    "        if self.has_residual:\n",
    "            V_res = self.V_res_embedding(embed_x)\n",
    "            cross_term += V_res\n",
    "        cross_term = F.relu(cross_term).contiguous().view(-1, self.atten_output_dim)\n",
    "        x = self.linear(x) + self.attn_fc(cross_term) + self.mlp(embed_x.view(-1, self.embed_output_dim))\n",
    "        return torch.sigmoid(x.squeeze(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **References:-**\n",
    "- W Song, et al. AutoInt: Automatic Feature Interaction Learning via Self-Attentive Neural Networks, 2018. https://arxiv.org/abs/1810.11921.\n",
    "- https://github.com/rixwew/pytorch-fm/blob/master/torchfm/model/afi.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "%reload_ext watermark\n",
    "%watermark -a \"Sparsh A.\" -m -iv -u -t -d -p recohut"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
