{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"2022-01-18-bst-deetctr.ipynb","provenance":[{"file_id":"https://github.com/recohut/nbs/blob/main/raw/T025247%20%7C%20BST%20in%20Deepctr.ipynb","timestamp":1644650341239}],"collapsed_sections":[],"toc_visible":true,"authorship_tag":"ABX9TyMu4RoQgbVeGHhnBQCMuV7t"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"8VP4UHlM8CG5"},"source":["# BST in Deepctr"]},{"cell_type":"markdown","metadata":{"id":"yeTnOiBIJU48"},"source":["## Installation"]},{"cell_type":"code","metadata":{"id":"mcwCEbdn5Mej"},"source":["!pip install -q deepctr"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"g9Bagen2JWiB"},"source":["## API run"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jkwBtowr6c6I","executionInfo":{"status":"ok","timestamp":1633948670717,"user_tz":-330,"elapsed":9017,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}},"outputId":"c20a2eaa-2200-434b-979c-5593d8747d4c"},"source":["import numpy as np\n","\n","from deepctr.models import BST\n","from deepctr.feature_column import SparseFeat, VarLenSparseFeat, DenseFeat, get_feature_names\n","\n","\n","def get_xy_fd():\n","    feature_columns = [SparseFeat('user', 3, embedding_dim=10), SparseFeat(\n","        'gender', 2, embedding_dim=4), SparseFeat('item_id', 3 + 1, embedding_dim=8),\n","                       SparseFeat('cate_id', 2 + 1, embedding_dim=4), DenseFeat('pay_score', 1)]\n","    feature_columns += [\n","        VarLenSparseFeat(SparseFeat('hist_item_id', vocabulary_size=3 + 1, embedding_dim=8, embedding_name='item_id'),\n","                         maxlen=4, length_name=\"seq_length\"),\n","        VarLenSparseFeat(SparseFeat('hist_cate_id', 2 + 1, embedding_dim=4, embedding_name='cate_id'), maxlen=4,\n","                         length_name=\"seq_length\")]\n","    # Notice: History behavior sequence feature name must start with \"hist_\".\n","    behavior_feature_list = [\"item_id\", \"cate_id\"]\n","    uid = np.array([0, 1, 2])\n","    ugender = np.array([0, 1, 0])\n","    iid = np.array([1, 2, 3])  # 0 is mask value\n","    cate_id = np.array([1, 2, 2])  # 0 is mask value\n","    pay_score = np.array([0.1, 0.2, 0.3])\n","\n","    hist_iid = np.array([[1, 2, 3, 0], [3, 2, 1, 0], [1, 2, 0, 0]])\n","    hist_cate_id = np.array([[1, 2, 2, 0], [2, 2, 1, 0], [1, 2, 0, 0]])\n","    seq_length = np.array([3, 3, 2])  # the actual length of the behavior sequence\n","\n","    feature_dict = {'user': uid, 'gender': ugender, 'item_id': iid, 'cate_id': cate_id,\n","                    'hist_item_id': hist_iid, 'hist_cate_id': hist_cate_id,\n","                    'pay_score': pay_score, 'seq_length': seq_length}\n","    x = {name: feature_dict[name] for name in get_feature_names(feature_columns)}\n","    y = np.array([1, 0, 1])\n","    return x, y, feature_columns, behavior_feature_list\n","\n","\n","if __name__ == \"__main__\":\n","    x, y, feature_columns, behavior_feature_list = get_xy_fd()\n","    model = BST(feature_columns, behavior_feature_list,att_head_num=4)\n","    model.compile('adam', 'binary_crossentropy',\n","                  metrics=['binary_crossentropy'])\n","    history = model.fit(x, y, verbose=1, epochs=10, validation_split=0.5)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/10\n","1/1 [==============================] - 4s 4s/step - loss: 0.5943 - binary_crossentropy: 0.5943 - val_loss: 0.7111 - val_binary_crossentropy: 0.7111\n","Epoch 2/10\n","1/1 [==============================] - 0s 36ms/step - loss: 0.4350 - binary_crossentropy: 0.4350 - val_loss: 0.7956 - val_binary_crossentropy: 0.7956\n","Epoch 3/10\n","1/1 [==============================] - 0s 48ms/step - loss: 0.3241 - binary_crossentropy: 0.3241 - val_loss: 0.8739 - val_binary_crossentropy: 0.8739\n","Epoch 4/10\n","1/1 [==============================] - 0s 41ms/step - loss: 0.2472 - binary_crossentropy: 0.2472 - val_loss: 0.9696 - val_binary_crossentropy: 0.9696\n","Epoch 5/10\n","1/1 [==============================] - 0s 36ms/step - loss: 0.1866 - binary_crossentropy: 0.1866 - val_loss: 1.0888 - val_binary_crossentropy: 1.0888\n","Epoch 6/10\n","1/1 [==============================] - 0s 35ms/step - loss: 0.1364 - binary_crossentropy: 0.1364 - val_loss: 1.2330 - val_binary_crossentropy: 1.2330\n","Epoch 7/10\n","1/1 [==============================] - 0s 37ms/step - loss: 0.0963 - binary_crossentropy: 0.0963 - val_loss: 1.3976 - val_binary_crossentropy: 1.3976\n","Epoch 8/10\n","1/1 [==============================] - 0s 56ms/step - loss: 0.0661 - binary_crossentropy: 0.0661 - val_loss: 1.5767 - val_binary_crossentropy: 1.5767\n","Epoch 9/10\n","1/1 [==============================] - 0s 39ms/step - loss: 0.0445 - binary_crossentropy: 0.0445 - val_loss: 1.7664 - val_binary_crossentropy: 1.7664\n","Epoch 10/10\n","1/1 [==============================] - 0s 36ms/step - loss: 0.0296 - binary_crossentropy: 0.0296 - val_loss: 1.9621 - val_binary_crossentropy: 1.9621\n"]}]},{"cell_type":"markdown","metadata":{"id":"LxbTlPCwJNn0"},"source":["## Model definition"]},{"cell_type":"code","metadata":{"id":"_qDCeT8h6c1S"},"source":["def BST(dnn_feature_columns, history_feature_list, transformer_num=1, att_head_num=8,\n","        use_bn=False, dnn_hidden_units=(256, 128, 64), dnn_activation='relu', l2_reg_dnn=0,\n","        l2_reg_embedding=1e-6, dnn_dropout=0.0, seed=1024, task='binary'):\n","    \"\"\"Instantiates the BST architecture.\n","     :param dnn_feature_columns: An iterable containing all the features used by deep part of the model.\n","     :param history_feature_list: list, to indicate sequence sparse field.\n","     :param transformer_num: int, the number of transformer layer.\n","     :param att_head_num: int, the number of heads in multi-head self attention.\n","     :param use_bn: bool. Whether use BatchNormalization before activation or not in deep net\n","     :param dnn_hidden_units: list,list of positive integer or empty list, the layer number and units in each layer of DNN\n","     :param dnn_activation: Activation function to use in DNN\n","     :param l2_reg_dnn: float. L2 regularizer strength applied to DNN\n","     :param l2_reg_embedding: float. L2 regularizer strength applied to embedding vector\n","     :param dnn_dropout: float in [0,1), the probability we will drop out a given DNN coordinate.\n","     :param seed: integer ,to use as random seed.\n","     :param task: str, ``\"binary\"`` for  binary logloss or ``\"regression\"`` for regression loss\n","     :return: A Keras model instance.\n","     \"\"\"\n","\n","    features = build_input_features(dnn_feature_columns)\n","    inputs_list = list(features.values())\n","\n","    user_behavior_length = features[\"seq_length\"]\n","\n","    sparse_feature_columns = list(\n","        filter(lambda x: isinstance(x, SparseFeat), dnn_feature_columns)) if dnn_feature_columns else []\n","    dense_feature_columns = list(\n","        filter(lambda x: isinstance(x, DenseFeat), dnn_feature_columns)) if dnn_feature_columns else []\n","    varlen_sparse_feature_columns = list(\n","        filter(lambda x: isinstance(x, VarLenSparseFeat), dnn_feature_columns)) if dnn_feature_columns else []\n","\n","    history_feature_columns = []\n","    sparse_varlen_feature_columns = []\n","    history_fc_names = list(map(lambda x: \"hist_\" + x, history_feature_list))\n","\n","    for fc in varlen_sparse_feature_columns:\n","        feature_name = fc.name\n","        if feature_name in history_fc_names:\n","            history_feature_columns.append(fc)\n","        else:\n","            sparse_varlen_feature_columns.append(fc)\n","\n","    embedding_dict = create_embedding_matrix(dnn_feature_columns, l2_reg_embedding, seed, prefix=\"\",\n","                                             seq_mask_zero=True)\n","\n","    query_emb_list = embedding_lookup(embedding_dict, features, sparse_feature_columns,\n","                                      return_feat_list=history_feature_list, to_list=True)\n","    hist_emb_list = embedding_lookup(embedding_dict, features, history_feature_columns,\n","                                     return_feat_list=history_fc_names, to_list=True)\n","    dnn_input_emb_list = embedding_lookup(embedding_dict, features, sparse_feature_columns,\n","                                          mask_feat_list=history_feature_list, to_list=True)\n","    dense_value_list = get_dense_input(features, dense_feature_columns)\n","    sequence_embed_dict = varlen_embedding_lookup(embedding_dict, features, sparse_varlen_feature_columns)\n","    sequence_embed_list = get_varlen_pooling_list(sequence_embed_dict, features, sparse_varlen_feature_columns,\n","                                                  to_list=True)\n","\n","    dnn_input_emb_list += sequence_embed_list\n","    query_emb = concat_func(query_emb_list)\n","    deep_input_emb = concat_func(dnn_input_emb_list)\n","    hist_emb = concat_func(hist_emb_list)\n","\n","    transformer_output = hist_emb\n","    for _ in range(transformer_num):\n","        att_embedding_size = transformer_output.get_shape().as_list()[-1] // att_head_num\n","        transformer_layer = Transformer(att_embedding_size=att_embedding_size, head_num=att_head_num,\n","                                        dropout_rate=dnn_dropout, use_positional_encoding=True, use_res=True,\n","                                        use_feed_forward=True, use_layer_norm=True, blinding=False, seed=seed,\n","                                        supports_masking=False, output_type=None)\n","        transformer_output = transformer_layer([transformer_output, transformer_output,\n","                                                user_behavior_length, user_behavior_length])\n","\n","    attn_output = AttentionSequencePoolingLayer(att_hidden_units=(64, 16), weight_normalization=True,\n","                                                supports_masking=False)([query_emb, transformer_output,\n","                                                                         user_behavior_length])\n","    deep_input_emb = concat_func([deep_input_emb, attn_output], axis=-1)\n","    deep_input_emb = Flatten()(deep_input_emb)\n","\n","    dnn_input = combined_dnn_input([deep_input_emb], dense_value_list)\n","    output = DNN(dnn_hidden_units, dnn_activation, l2_reg_dnn, dnn_dropout, use_bn, seed=seed)(dnn_input)\n","    final_logit = Dense(1, use_bias=False, kernel_initializer=tf.keras.initializers.glorot_normal(seed))(output)\n","    output = PredictionLayer(task)(final_logit)\n","\n","    model = tf.keras.models.Model(inputs=inputs_list, outputs=output)\n","\n","    return model"],"execution_count":null,"outputs":[]}]}