{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "5.MatrixFactorization.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5MzSv49iMF1_"
      },
      "source": [
        "# CF Part 5 - Matrix factorization method\n",
        "> Collaborative Filtering on MovieLens Latest-small Part 5 - Finding recommendations using model based  matrix factorization method\n",
        "\n",
        "- toc: true\n",
        "- badges: true\n",
        "- comments: true\n",
        "- categories: [movie, collaborative]\n",
        "- image:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mGoHyoipMF2B"
      },
      "source": [
        "<b>User-based</b> and <b>Item-based</b> collaborative Filtering recommender systems suffer from <i>data sparsity</i> and <i>scalability</i> for online recommendations. <b>Matrix Factorization</b> helps to address these drawbacks of memory-based collaborative filtering by reducing the dimension of the rating matrix $R$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kSvTMCyVMF2C"
      },
      "source": [
        "The movielen lasted small dataset has 100k ratings of $m=610$ users on $n=9724$ items. The rating matrix in then a $m\\times n$ matrix (i.e $R\\in \\mathbb{R}^{m\\times n}$). The fact that users usually interact with less than $1\\%$ of items leads the rating matrix $R$ to be highly sparse. For example, the degree of sparsity of the movielen lasted small dataset is \n",
        "\n",
        "\\begin{equation}\n",
        "sparsity = 100 - \\frac{\\text{total # ratings}}{m \\times n} = 100 - \\frac{100000}{610\\times 9724} = 98,3\\%\n",
        "\\end{equation}\n",
        "\n",
        "This means that in this dataset, a user has interacted with less than $2\\%$ of items. To reduce the dimension of the rating matrix $R$, Matrix Factorization (MF) mappes both users and items to a joint latent factor space of dimensionality $k$ such that user-item interactions are modeled as inner products in that space <a href='https://ieeexplore.ieee.org/document/5197422'>(Yehuda Koren et al., 2009)</a>. MF then decomposes $R$ in two matrices as follows :\n",
        "\n",
        "\\begin{equation}\n",
        "R = Q^\\top P\n",
        "\\end{equation}\n",
        "\n",
        "Where $P \\in \\mathbb{R}^{m\\times k}$ represents latent factors of users and $Q \\in \\mathbb{R}^{n\\times k}$ is the latent factors of items. Each line of $P$, say $p_u \\in \\mathbb{R}^k$ denotes the taste of user $u$ and each $q_i \\in \\mathbb{R}^k$ the features of item $i$. The dot product between $p_u$ and $q_i$ will be the rating prediction of user $u$ on item $i$ :\n",
        "\n",
        "\\begin{equation}\n",
        "\\hat{r}_{u,i} = q_{i}^{\\top} p_u.\n",
        "\\end{equation}\n",
        "\n",
        "Figure 1 presents an example of decomposition of $R$ into two matrices $P$ and $Q$.\n",
        "\n",
        "![MF](https://raw.githubusercontent.com/nzhinusoftcm/review-on-collaborative-filtering/master/recsys/img/MF.png)\n",
        "\n",
        "\n",
        "To learn the latent factors $p_u$ and $q_i$, the system minimizes the regularized squared error on the set of known ratings. The cost function $J$ is defined as follows : \n",
        "\n",
        "\\begin{equation}\n",
        "J = \\frac{1}{2}\\sum_{(u,i)\\in \\kappa} (r_{ui} - q_{i}^{\\top} p_u)^2 + \\lambda(||p_u||^2 + ||q_i||^2)\n",
        "\\end{equation}\n",
        "\n",
        "where $\\kappa$ is the set of $(u,i)$ pairs for which $r_{u,i}$ is known (the training set), and $\\lambda$ is the regularizer parameter."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "azVL_VTEMF2C"
      },
      "source": [
        "## Learning Algorithms\n",
        "\n",
        "As described in <a href='https://ieeexplore.ieee.org/document/5197422'>(Yehuda Koren et al., 2009)</a>, to minimize the cost function $J$, the matrix factorization algorithm predicts $\\hat{r}_{u,i}$ for each given training case (existing $r_{u,i}$), and computes the associated error defined by the Mean Absolute Error (MAE) as :\n",
        "\n",
        "\\begin{equation}\n",
        "e_{u,i} = |r_{ui} - q_{i}^{\\top} p_u|.\n",
        "\\end{equation}\n",
        "\n",
        "<b>Note</b> : The overall error $E$ is defined as :\n",
        "\n",
        "\\begin{equation}\n",
        "E = \\frac{1}{M}\\sum_{(u,i)\\in\\kappa} e_{u,i}\n",
        "\\end{equation}\n",
        "\n",
        "Where $M$ is the number of example. The update rules for parameters $p_u$ and $q_i$ are defined as follows :\n",
        "\n",
        "\\begin{equation}\n",
        "q_i \\leftarrow q_i - \\alpha\\frac{\\partial}{\\partial q_i}J_{u,i}, \n",
        "\\end{equation}\n",
        "\n",
        "\\begin{equation}\n",
        "p_u \\leftarrow p_u - \\alpha\\frac{\\partial}{\\partial p_u}J_{u,i}\n",
        "\\end{equation}\n",
        "\n",
        "where $\\alpha$ is the learning rate and $\\frac{\\partial}{\\partial p_u}J_{u,i}$ is the partial derivative of the cost function $J$ according to $p_u$. It computes the extent to which $p_u$ contributes to the total error.\n",
        "\n",
        "### How to compute $\\frac{\\partial}{\\partial q_i}J_{u,i}$ ?\n",
        "\n",
        "\\begin{align}\n",
        "\\frac{\\partial}{\\partial q_i}J_{u,i} & = & \\frac{1}{2}\\frac{\\partial}{\\partial q_i} \\begin{bmatrix}(r_{ui} - q_{i}^{\\top} p_u)^2 + \\lambda(||p_u||^2 + ||q_i||^2)\\end{bmatrix} \\\\\n",
        "& = & -(r_{u,i}-q_{i}^{\\top} p_u)\\cdot p_u + \\lambda \\cdot q_i  \\\\\n",
        "& = & -e_{u,i}\\cdot p_u+\\lambda \\cdot q_i\n",
        "\\end{align}\n",
        "\n",
        "The update rules are then given by : \n",
        "\n",
        "\\begin{equation}\n",
        "q_i \\leftarrow q_i + \\alpha\\cdot (e_{u,i}\\cdot p_u-\\lambda \\cdot q_i), \n",
        "\\end{equation}\n",
        "\n",
        "\\begin{equation}\n",
        "p_u \\leftarrow p_u + \\alpha\\cdot (e_{u,i}\\cdot q_i-\\lambda \\cdot p_u)\n",
        "\\end{equation}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "04D2zA48MF2D"
      },
      "source": [
        "## Matrix Factorization : algorithm\n",
        "<ol>\n",
        "    <li>Initialize $P$ and $Q$ with random values\n",
        "    <li>For each training example $(u,i)\\in\\kappa$ with the corresponding rating $r_{u,i}$ :\n",
        "        <ul>\n",
        "            <li>compute $\\hat{r}_{u,i}$ as $\\hat{r}_{u,i} = q_{i}^{\\top} p_u$\n",
        "            <li>compute the error : $e_{u,i} = |r_{ui} - \\hat{r}_{u,i}|$\n",
        "            <li>update $p_u$ and $q_i$:\n",
        "                <ul>\n",
        "                    <li>$p_u \\leftarrow p_u + \\alpha\\cdot (e_{u,i}\\cdot q_i-\\lambda \\cdot p_u)$\n",
        "                    <li>$q_i \\leftarrow q_i + \\alpha\\cdot (e_{u,i}\\cdot p_u-\\lambda \\cdot q_i)$\n",
        "                </ul>\n",
        "        </ul>\n",
        "    <li> Repeat step 2 until the optimal parameters are reached.\n",
        "</ol>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qAr3PdnAgCzT"
      },
      "source": [
        "### Download useful files"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HQEezEwQgCzb"
      },
      "source": [
        "import os\n",
        "if not (os.path.exists(\"recsys.zip\") or os.path.exists(\"recsys\")):\n",
        "    !wget https://github.com/nzhinusoftcm/review-on-collaborative-filtering/raw/master/recsys.zip    \n",
        "    !unzip recsys.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KzGsFLCJgCzo"
      },
      "source": [
        "### Import requirements\n",
        "```\n",
        "matplotlib==3.2.2\n",
        "numpy==1.19.2\n",
        "pandas==1.0.5\n",
        "python==3.7\n",
        "scikit-learn==0.24.1\n",
        "scikit-surprise==1.1.1\n",
        "scipy==1.6.2\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x_ohlWURMF2F"
      },
      "source": [
        "from recsys.preprocessing import mean_ratings\n",
        "from recsys.preprocessing import normalized_ratings\n",
        "from recsys.preprocessing import ids_encoder\n",
        "from recsys.preprocessing import train_test_split\n",
        "from recsys.preprocessing import rating_matrix\n",
        "from recsys.preprocessing import get_examples\n",
        "from recsys.preprocessing import scale_ratings\n",
        "\n",
        "from recsys.datasets import ml100k\n",
        "from recsys.datasets import ml1m\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "import os"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JE1e_7yvgCzp"
      },
      "source": [
        "## Model definition"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GAYonOOzgCzq"
      },
      "source": [
        "class MatrixFactorization:\n",
        "    \n",
        "    def __init__(self, m, n, k=10, alpha=0.001, lamb=0.01):\n",
        "        \"\"\"\n",
        "        Initialization of the model        \n",
        "        : param\n",
        "            - m : number of users\n",
        "            - n : number of items\n",
        "            - k : length of latent factor, both for users and items. 50 by default\n",
        "            - alpha : learning rate. 0.001 by default\n",
        "            - lamb : regularizer parameter. 0.02 by default\n",
        "        \"\"\"\n",
        "        np.random.seed(32)\n",
        "        \n",
        "        # initialize the latent factor matrices P and Q (of shapes (m,k) and (n,k) respectively) that will be learnt\n",
        "        self.k = k\n",
        "        self.P = np.random.normal(size=(m, k))\n",
        "        self.Q = np.random.normal(size=(n, k))\n",
        "        \n",
        "        # hyperparameter initialization\n",
        "        self.alpha = alpha\n",
        "        self.lamb = lamb\n",
        "        \n",
        "        # training history\n",
        "        self.history = {\n",
        "            \"epochs\":[],\n",
        "            \"loss\":[],\n",
        "            \"val_loss\":[],\n",
        "            \"lr\":[]\n",
        "        }\n",
        "    \n",
        "    def print_training_parameters(self):\n",
        "        print('Training Matrix Factorization Model ...')\n",
        "        print(f'k={self.k} \\t alpha={self.alpha} \\t lambda={self.lamb}')\n",
        "    \n",
        "    def update_rule(self, u, i, error):\n",
        "        self.P[u] = self.P[u] + self.alpha * (error * self.Q[i] - self.lamb * self.P[u])\n",
        "        self.Q[i] = self.Q[i] + self.alpha * (error * self.P[u] - self.lamb * self.Q[i])\n",
        "        \n",
        "    def mae(self,  x_train, y_train):\n",
        "        \"\"\"\n",
        "        returns the Mean Absolute Error\n",
        "        \"\"\"\n",
        "        # number of training exemples\n",
        "        M = x_train.shape[0]\n",
        "        error = 0\n",
        "        for pair, r in zip(x_train, y_train):\n",
        "            u, i = pair\n",
        "            error += abs(r - np.dot(self.P[u], self.Q[i]))\n",
        "        return error/M\n",
        "    \n",
        "    def print_training_progress(self, epoch, epochs, error, val_error, steps=5):\n",
        "        if epoch == 1 or epoch % steps == 0 :\n",
        "                print(\"epoch {}/{} - loss : {} - val_loss : {}\".format(epoch, epochs, round(error,3), round(val_error,3)))\n",
        "                \n",
        "    def learning_rate_schedule(self, epoch, target_epochs = 20):\n",
        "        if (epoch >= target_epochs) and (epoch % target_epochs == 0):\n",
        "                factor = epoch // target_epochs\n",
        "                self.alpha = self.alpha * (1 / (factor * 20))\n",
        "                print(\"\\nLearning Rate : {}\\n\".format(self.alpha))\n",
        "    \n",
        "    def fit(self, x_train, y_train, validation_data, epochs=1000):\n",
        "        \"\"\"\n",
        "        Train latent factors P and Q according to the training set\n",
        "        \n",
        "        :param\n",
        "            - x_train : training pairs (u,i) for which rating r_ui is known\n",
        "            - y_train : set of ratings r_ui for all training pairs (u,i)\n",
        "            - validation_data : tuple (x_test, y_test)\n",
        "            - epochs : number of time to loop over the entire training set. \n",
        "            1000 epochs by default\n",
        "            \n",
        "        Note that u and i are encoded values of userid and itemid\n",
        "        \"\"\"\n",
        "        self.print_training_parameters()\n",
        "        \n",
        "        # validation data\n",
        "        x_test, y_test = validation_data\n",
        "        \n",
        "        # loop over the number of epochs\n",
        "        for epoch in range(1, epochs+1):\n",
        "            \n",
        "            # for each pair (u,i) and the corresponding rating r\n",
        "            for pair, r in zip(x_train, y_train):\n",
        "                \n",
        "                # get encoded values of userid and itemid from pair\n",
        "                u,i = pair\n",
        "                \n",
        "                # compute the predicted rating r_hat\n",
        "                r_hat = np.dot(self.P[u], self.Q[i])\n",
        "                \n",
        "                # compute the prediction error\n",
        "                e = abs(r - r_hat)\n",
        "                \n",
        "                # update rules\n",
        "                self.update_rule(u, i, e)\n",
        "                \n",
        "            # training and validation error  after this epochs\n",
        "            error = self.mae(x_train, y_train)\n",
        "            val_error = self.mae(x_test, y_test)\n",
        "            \n",
        "            # update history\n",
        "            self.history['epochs'].append(epoch)\n",
        "            self.history['loss'].append(error)\n",
        "            self.history['val_loss'].append(val_error)\n",
        "            \n",
        "            # update history\n",
        "            self.update_history(epoch, error, val_error)\n",
        "            \n",
        "            # print training progress after each steps epochs\n",
        "            self.print_training_progress(epoch, epochs, error, val_error, steps=1)\n",
        "              \n",
        "            # leaning rate scheduler : redure the learning rate as we go deeper in the number of epochs\n",
        "            # self.learning_rate_schedule(epoch)\n",
        "        \n",
        "        return self.history\n",
        "    \n",
        "    def update_history(self, epoch, error, val_error):\n",
        "        self.history['epochs'].append(epoch)\n",
        "        self.history['loss'].append(error)\n",
        "        self.history['val_loss'].append(val_error)\n",
        "        self.history['lr'].append(self.alpha)\n",
        "    \n",
        "    def evaluate(self, x_test, y_test):\n",
        "        \"\"\"\n",
        "        compute the global error on the test set        \n",
        "        :param x_test : test pairs (u,i) for which rating r_ui is known\n",
        "        :param y_test : set of ratings r_ui for all test pairs (u,i)\n",
        "        \"\"\"\n",
        "        error = self.mae(x_test, y_test)\n",
        "        print(f\"validation error : {round(error,3)}\")\n",
        "        \n",
        "        return error\n",
        "      \n",
        "    def predict(self, userid, itemid):\n",
        "        \"\"\"\n",
        "        Make rating prediction for a user on an item\n",
        "        :param userid\n",
        "        :param itemid\n",
        "        :return r : predicted rating\n",
        "        \"\"\"\n",
        "        # encode user and item ids to be able to access their latent factors in\n",
        "        # matrices P and Q\n",
        "        u = uencoder.transform([userid])[0]\n",
        "        i = iencoder.transform([itemid])[0]\n",
        "\n",
        "        # rating prediction using encoded ids. Dot product between P_u and Q_i\n",
        "        r = np.dot(self.P[u], self.Q[i])\n",
        "        return r\n",
        "\n",
        "    def recommend(self, userid, N=30):\n",
        "        \"\"\"\n",
        "        make to N recommendations for a given user\n",
        "\n",
        "        :return(top_items,preds) : top N items with the highest predictions \n",
        "        with their corresponding predictions\n",
        "        \"\"\"\n",
        "        # encode the userid\n",
        "        u = uencoder.transform([userid])[0]\n",
        "\n",
        "        # predictions for users userid on all product\n",
        "        predictions = np.dot(self.P[u], self.Q.T)\n",
        "\n",
        "        # get the indices of the top N predictions\n",
        "        top_idx = np.flip(np.argsort(predictions))[:N]\n",
        "\n",
        "        # decode indices to get their corresponding itemids\n",
        "        top_items = iencoder.inverse_transform(top_idx)\n",
        "\n",
        "        # take corresponding predictions for top N indices\n",
        "        preds = predictions[top_idx]\n",
        "\n",
        "        return top_items, preds        "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "anzaSi8ggCzv"
      },
      "source": [
        "epochs = 10"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "grOTFFETgCzz"
      },
      "source": [
        "## MovieLens 100k"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pPChuAJ3lT1a"
      },
      "source": [
        "### Evaluation on raw ratings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lJ94EgebgCzz"
      },
      "source": [
        "# load the ml100k dataset\n",
        "ratings, movies = ml100k.load()\n",
        "\n",
        "ratings, uencoder, iencoder = ids_encoder(ratings)\n",
        "\n",
        "m = ratings.userid.nunique()   # total number of users\n",
        "n = ratings.itemid.nunique()   # total number of items\n",
        "\n",
        "# get examples as tuples of userids and itemids and labels from normalize ratings\n",
        "raw_examples, raw_labels = get_examples(ratings)\n",
        "\n",
        "# train test split\n",
        "(x_train, x_test), (y_train, y_test) = train_test_split(examples=raw_examples, labels=raw_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VRp3MossgCz0",
        "outputId": "2047dd8f-b8d9-4f3a-c87d-f5bfec6fc239"
      },
      "source": [
        "# create the model\n",
        "MF = MatrixFactorization(m, n, k=10, alpha=0.01, lamb=1.5)\n",
        "\n",
        "# fit the model on the training set\n",
        "history = MF.fit(x_train, y_train, epochs=epochs, validation_data=(x_test, y_test))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training Matrix Factorization Model ...\n",
            "k=10 \t alpha=0.01 \t lambda=1.5\n",
            "epoch 1/10 - loss : 2.734 - val_loss : 2.779\n",
            "epoch 2/10 - loss : 1.764 - val_loss : 1.794\n",
            "epoch 3/10 - loss : 1.592 - val_loss : 1.614\n",
            "epoch 4/10 - loss : 1.538 - val_loss : 1.556\n",
            "epoch 5/10 - loss : 1.515 - val_loss : 1.531\n",
            "epoch 6/10 - loss : 1.503 - val_loss : 1.517\n",
            "epoch 7/10 - loss : 1.496 - val_loss : 1.509\n",
            "epoch 8/10 - loss : 1.491 - val_loss : 1.504\n",
            "epoch 9/10 - loss : 1.488 - val_loss : 1.5\n",
            "epoch 10/10 - loss : 1.486 - val_loss : 1.497\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9CbdCn3EgCz5",
        "outputId": "0926a075-12cd-473e-a85d-cb125b74bd93"
      },
      "source": [
        "MF.evaluate(x_test, y_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "validation error : 1.497\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.4973507972141993"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WfxfW68agCz7"
      },
      "source": [
        "### Evaluation on normalized ratings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zuCxWPZvgCz8"
      },
      "source": [
        "# load data\n",
        "ratings, movies = ml100k.load()\n",
        "\n",
        "ratings, uencoder, iencoder = ids_encoder(ratings)\n",
        "\n",
        "m = ratings['userid'].nunique()   # total number of users\n",
        "n = ratings['itemid'].nunique()   # total number of items\n",
        "\n",
        "# normalize ratings by substracting means\n",
        "normalized_column_name = \"norm_rating\"\n",
        "ratings = normalized_ratings(ratings, norm_column=normalized_column_name)\n",
        "\n",
        "# get examples as tuples of userids and itemids and labels from normalize ratings\n",
        "raw_examples, raw_labels = get_examples(ratings, labels_column=normalized_column_name)\n",
        "\n",
        "# train test split\n",
        "(x_train, x_test), (y_train, y_test) = train_test_split(examples=raw_examples, labels=raw_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oHjJks4PgCz8",
        "outputId": "706bae70-c9c1-4bd9-c2b3-9c3dbe35878a"
      },
      "source": [
        "# create the model\n",
        "MF = MatrixFactorization(m, n, k=10, alpha=0.01, lamb=1.5)\n",
        "\n",
        "# fit the model on the training set\n",
        "history = MF.fit(x_train, y_train, epochs=epochs, validation_data=(x_test, y_test))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training Matrix Factorization Model ...\n",
            "k=10 \t alpha=0.01 \t lambda=1.5\n",
            "epoch 1/10 - loss : 0.851 - val_loss : 0.847\n",
            "epoch 2/10 - loss : 0.831 - val_loss : 0.831\n",
            "epoch 3/10 - loss : 0.828 - val_loss : 0.829\n",
            "epoch 4/10 - loss : 0.827 - val_loss : 0.828\n",
            "epoch 5/10 - loss : 0.827 - val_loss : 0.828\n",
            "epoch 6/10 - loss : 0.826 - val_loss : 0.828\n",
            "epoch 7/10 - loss : 0.826 - val_loss : 0.828\n",
            "epoch 8/10 - loss : 0.826 - val_loss : 0.828\n",
            "epoch 9/10 - loss : 0.826 - val_loss : 0.828\n",
            "epoch 10/10 - loss : 0.826 - val_loss : 0.828\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lxH4jNI3gCz9",
        "outputId": "aafdeb67-5d76-4f26-d38c-2fa2d731e4a4"
      },
      "source": [
        "MF.evaluate(x_test, y_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "validation error : 0.828\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8276982643684648"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5YW54SougCz-"
      },
      "source": [
        "## MovieLens 1M"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ROsCpy1KlWaw"
      },
      "source": [
        "### Evaluation on raw data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PPWsOqMpgCz-"
      },
      "source": [
        "# load the ml1m dataset\n",
        "ratings, movies = ml1m.load()\n",
        "\n",
        "ratings, uencoder, iencoder = ids_encoder(ratings)\n",
        "\n",
        "m = ratings.userid.nunique()   # total number of users\n",
        "n = ratings.itemid.nunique()   # total number of items\n",
        "\n",
        "# get examples as tuples of userids and itemids and labels from normalize ratings\n",
        "raw_examples, raw_labels = get_examples(ratings)\n",
        "\n",
        "# train test split\n",
        "(x_train, x_test), (y_train, y_test) = train_test_split(examples=raw_examples, labels=raw_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8raSMMJZgCz_",
        "outputId": "815f2e0f-d980-456f-dbd5-66f0791898b9"
      },
      "source": [
        "# create the model\n",
        "MF = MatrixFactorization(m, n, k=10, alpha=0.01, lamb=1.5)\n",
        "\n",
        "# fit the model on the training set\n",
        "history = MF.fit(x_train, y_train, epochs=epochs, validation_data=(x_test, y_test))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training Matrix Factorization Model ...\n",
            "k=10 \t alpha=0.01 \t lambda=1.5\n",
            "epoch 1/10 - loss : 1.713 - val_loss : 1.718\n",
            "epoch 2/10 - loss : 1.523 - val_loss : 1.526\n",
            "epoch 3/10 - loss : 1.496 - val_loss : 1.498\n",
            "epoch 4/10 - loss : 1.489 - val_loss : 1.489\n",
            "epoch 5/10 - loss : 1.485 - val_loss : 1.486\n",
            "epoch 6/10 - loss : 1.484 - val_loss : 1.484\n",
            "epoch 7/10 - loss : 1.483 - val_loss : 1.483\n",
            "epoch 8/10 - loss : 1.483 - val_loss : 1.483\n",
            "epoch 9/10 - loss : 1.482 - val_loss : 1.482\n",
            "epoch 10/10 - loss : 1.482 - val_loss : 1.482\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1u17P3ESgC0A",
        "outputId": "926b9008-f2f4-4f78-c344-89b8416de178"
      },
      "source": [
        "MF.evaluate(x_test, y_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "validation error : 1.482\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.4820034560467208"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "apQplsBJgC0A"
      },
      "source": [
        "### Evaluation on normalized ratings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q62I-C8ggC0B"
      },
      "source": [
        "# load data\n",
        "ratings, movies = ml1m.load()\n",
        "\n",
        "ratings, uencoder, iencoder = ids_encoder(ratings)\n",
        "\n",
        "m = ratings['userid'].nunique()   # total number of users\n",
        "n = ratings['itemid'].nunique()   # total number of items\n",
        "\n",
        "# normalize ratings by substracting means\n",
        "normalized_column_name = \"norm_rating\"\n",
        "ratings = normalized_ratings(ratings, norm_column=normalized_column_name)\n",
        "\n",
        "# get examples as tuples of userids and itemids and labels from normalize ratings\n",
        "raw_examples, raw_labels = get_examples(ratings, labels_column=normalized_column_name)\n",
        "\n",
        "# train test split\n",
        "(x_train, x_test), (y_train, y_test) = train_test_split(examples=raw_examples, labels=raw_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OP9_mHRigC0D",
        "outputId": "8456210d-e289-4d79-c19a-ff7cc873ae54"
      },
      "source": [
        "# create the model\n",
        "MF = MatrixFactorization(m, n, k=10, alpha=0.01, lamb=1.5)\n",
        "\n",
        "# fit the model on the training set\n",
        "history = MF.fit(x_train, y_train, epochs=epochs, validation_data=(x_test, y_test))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training Matrix Factorization Model ...\n",
            "k=10 \t alpha=0.01 \t lambda=1.5\n",
            "epoch 1/10 - loss : 0.826 - val_loss : 0.827\n",
            "epoch 2/10 - loss : 0.824 - val_loss : 0.825\n",
            "epoch 3/10 - loss : 0.823 - val_loss : 0.825\n",
            "epoch 4/10 - loss : 0.823 - val_loss : 0.825\n",
            "epoch 5/10 - loss : 0.823 - val_loss : 0.825\n",
            "epoch 6/10 - loss : 0.823 - val_loss : 0.825\n",
            "epoch 7/10 - loss : 0.823 - val_loss : 0.825\n",
            "epoch 8/10 - loss : 0.823 - val_loss : 0.825\n",
            "epoch 9/10 - loss : 0.823 - val_loss : 0.825\n",
            "epoch 10/10 - loss : 0.823 - val_loss : 0.825\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QH8pxjwsgC0E",
        "outputId": "c33fbb65-9380-43a7-d2d1-ea0eb1e9f7b0"
      },
      "source": [
        "MF.evaluate(x_test, y_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "validation error : 0.825\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8250208634455388"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DF8hqi1ygC0E"
      },
      "source": [
        "## Predictions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_hzclXP0RL8V"
      },
      "source": [
        "Now that the latent factors $P$ and $Q$, we can use them to make predictions and recommendations. Let's call the ```predict``` function of the ```Matrix Factorization``` class to make prediction for a given.\n",
        "\n",
        "rating prediction for user 1 on item 1 for which the truth rating $r=5.0$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4rAUY-AHgC0F",
        "outputId": "49e343d0-264f-4ca0-99b1-be9e97a5da13"
      },
      "source": [
        "ratings.userid = uencoder.inverse_transform(ratings.userid.to_list())\n",
        "ratings.itemid = uencoder.inverse_transform(ratings.itemid.to_list())\n",
        "ratings.head(5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>userid</th>\n",
              "      <th>itemid</th>\n",
              "      <th>rating</th>\n",
              "      <th>rating_mean</th>\n",
              "      <th>norm_rating</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>4.188679</td>\n",
              "      <td>0.811321</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>48</td>\n",
              "      <td>5</td>\n",
              "      <td>4.188679</td>\n",
              "      <td>0.811321</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>145</td>\n",
              "      <td>5</td>\n",
              "      <td>4.188679</td>\n",
              "      <td>0.811321</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>254</td>\n",
              "      <td>4</td>\n",
              "      <td>4.188679</td>\n",
              "      <td>-0.188679</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>514</td>\n",
              "      <td>5</td>\n",
              "      <td>4.188679</td>\n",
              "      <td>0.811321</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   userid  itemid  rating  rating_mean  norm_rating\n",
              "0       1       1       5     4.188679     0.811321\n",
              "1       1      48       5     4.188679     0.811321\n",
              "2       1     145       5     4.188679     0.811321\n",
              "3       1     254       4     4.188679    -0.188679\n",
              "4       1     514       5     4.188679     0.811321"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "WjQN_N1WRKSm",
        "outputId": "0003d90b-3c19-4af8-c77f-5206a60d5be5"
      },
      "source": [
        "4.188679 + MF.predict(userid=1, itemid=1) # add the mean because we have used the normalised ratings for training"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4.188679163563357"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YYncVXWvebEZ"
      },
      "source": [
        "## Summary\n",
        "\n",
        "This is the link to the MatrixFactorization class : [MatrixFactorization.py](https://github.com/nzhinusoftcm/review-on-collaborative-filtering/blob/master/recsys/models/MatrixFactorization.py)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "erPyco4tgC0H"
      },
      "source": [
        "## Non-negative Matrix Factorization\n",
        "\n",
        "With the Matrix Factorization model, $P$ and $Q$ latent factors are non interpretable since they contain arbitrary negative or positive values. This make the Matrix Factorization model to be non explainable.\n",
        "\n",
        "The **Non-negative Matrix Factorization (NMF)** algorithm is a variant of Matrix Factorization which generate explainable latent factors for $P$ and $Q$ by constraining their values in the range [0,1]. This allow a probability interpretation of these latent factors, hense the explainability.\n",
        "\n",
        "Click [here](https://github.com/nzhinusoftcm/review-on-collaborative-filtering/blob/master/6.NonNegativeMatrixFactorization.ipynb) to go to the NMF notebook."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B-jjxDhex5Gs"
      },
      "source": [
        "## Reference\n",
        "\n",
        "1. Yehuda Koren et al. (2009). <a href='https://ieeexplore.ieee.org/document/5197422'>Matrix Factorization Techniques for Recommender Systems</a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jV3Jj17ox_UR"
      },
      "source": [
        "## Author\n",
        "\n",
        "[Carmel WENGA](https://www.linkedin.com/in/carmel-wenga-871876178/), <br>\n",
        "PhD student at Université de la Polynésie Française, <br> \n",
        "Applied Machine Learning Research Engineer, <br>\n",
        "[ShoppingList](https://shoppinglist.cm), NzhinuSoft."
      ]
    }
  ]
}