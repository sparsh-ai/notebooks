{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"2022-01-20-lessr.ipynb","provenance":[{"file_id":"https://github.com/recohut/nbs/blob/main/raw/T072666%20%7C%20LESSR%20Session-based%20Recommendations%20on%20Sample%20data%20in%20PyTorch.ipynb","timestamp":1644654292217}],"collapsed_sections":[],"authorship_tag":"ABX9TyNq8peSmgmqAIFZ8ocLuy+S"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"f2oOCVn5Hq2k"},"source":["# LESSR Session-based Recommendations on Sample data in PyTorch"]},{"cell_type":"markdown","metadata":{"id":"Ha4vDCQEHYnL"},"source":["## Setup"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"131W9rq_QwwD","executionInfo":{"status":"ok","timestamp":1638014333875,"user_tz":-330,"elapsed":647,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}},"outputId":"d111077c-a689-4b2d-f0c1-c54e1d979899"},"source":["import torch\n","torch.__version__"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'1.10.0+cu111'"]},"metadata":{},"execution_count":9}]},{"cell_type":"code","metadata":{"id":"bxJibf5NKaZ0"},"source":["!pip install dgl-cu111"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5WWtr35DK48_"},"source":["class Args:\n","    dataset_dir = '.'\n","    embedding_dim = 32\n","    num_layers = 3\n","    feat_drop = 0.2\n","    lr = 1e-3\n","    batch_size = 512\n","    epochs = 30\n","    weight_decay = 1e-4\n","    Ks = '10,20' # the values of K in evaluation metrics, separated by commas\n","    patience = 2\n","    num_workers = 2\n","    valid_split = None\n","    log_interval = 100"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"FHkHxp1VKmtS"},"source":["## Dataset"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_6R5Toa6KOEi","executionInfo":{"status":"ok","timestamp":1638014176142,"user_tz":-330,"elapsed":1496,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}},"outputId":"8ad293d6-d60c-425b-b8c1-2378a937cd44"},"source":["!wget -q --show-progress https://github.com/sparsh-ai/stanza/raw/S969796/datasets/sample/num_items.txt\n","!wget -q --show-progress https://github.com/sparsh-ai/stanza/raw/S969796/datasets/sample/train.txt\n","!wget -q --show-progress https://github.com/sparsh-ai/stanza/raw/S969796/datasets/sample/test.txt"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["num_items.txt       100%[===================>]       4  --.-KB/s    in 0s      \n","train.txt           100%[===================>] 172.87K  --.-KB/s    in 0.01s   \n","test.txt            100%[===================>]  22.89K  --.-KB/s    in 0s      \n"]}]},{"cell_type":"markdown","metadata":{"id":"3SDfoUOvKnyN"},"source":["### Collate"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HgDdcSIHKdjz","executionInfo":{"status":"ok","timestamp":1638014376657,"user_tz":-330,"elapsed":717,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}},"outputId":"a4eac9c3-dee3-4202-a831-da9bafbb34cc"},"source":["from collections import Counter\n","import numpy as np\n","import torch as th\n","import dgl\n","\n","\n","def label_last(g, last_nid):\n","    is_last = th.zeros(g.number_of_nodes(), dtype=th.int32)\n","    is_last[last_nid] = 1\n","    g.ndata['last'] = is_last\n","    return g\n","\n","\n","def seq_to_eop_multigraph(seq):\n","    items = np.unique(seq)\n","    iid2nid = {iid: i for i, iid in enumerate(items)}\n","    num_nodes = len(items)\n","\n","    if len(seq) > 1:\n","        seq_nid = [iid2nid[iid] for iid in seq]\n","        src = seq_nid[:-1]\n","        dst = seq_nid[1:]\n","    else:\n","        src = th.LongTensor([])\n","        dst = th.LongTensor([])\n","    g = dgl.graph((src, dst), num_nodes=num_nodes)\n","    g.ndata['iid'] = th.from_numpy(items)\n","    label_last(g, iid2nid[seq[-1]])\n","    return g\n","\n","\n","def seq_to_shortcut_graph(seq):\n","    items = np.unique(seq)\n","    iid2nid = {iid: i for i, iid in enumerate(items)}\n","    num_nodes = len(items)\n","\n","    seq_nid = [iid2nid[iid] for iid in seq]\n","    counter = Counter(\n","        [(seq_nid[i], seq_nid[j]) for i in range(len(seq)) for j in range(i, len(seq))]\n","    )\n","    edges = counter.keys()\n","    src, dst = zip(*edges)\n","\n","    g = dgl.graph((src, dst), num_nodes=num_nodes)\n","    return g\n","\n","\n","def collate_fn_factory(*seq_to_graph_fns):\n","    def collate_fn(samples):\n","        seqs, labels = zip(*samples)\n","        inputs = []\n","        for seq_to_graph in seq_to_graph_fns:\n","            graphs = list(map(seq_to_graph, seqs))\n","            bg = dgl.batch(graphs)\n","            inputs.append(bg)\n","        labels = th.LongTensor(labels)\n","        return inputs, labels\n","\n","    return collate_fn"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Using backend: pytorch\n"]}]},{"cell_type":"markdown","metadata":{"id":"lWg59bP_Kv-1"},"source":["### Augment"]},{"cell_type":"code","metadata":{"id":"TXYtRBkpMUhE"},"source":["import itertools\n","import numpy as np\n","import pandas as pd\n","\n","\n","def create_index(sessions):\n","    lens = np.fromiter(map(len, sessions), dtype=np.long)\n","    session_idx = np.repeat(np.arange(len(sessions)), lens - 1)\n","    label_idx = map(lambda l: range(1, l), lens)\n","    label_idx = itertools.chain.from_iterable(label_idx)\n","    label_idx = np.fromiter(label_idx, dtype=np.long)\n","    idx = np.column_stack((session_idx, label_idx))\n","    return idx\n","\n","\n","def read_sessions(filepath):\n","    sessions = pd.read_csv(filepath, sep='\\t', header=None, squeeze=True)\n","    sessions = sessions.apply(lambda x: list(map(int, x.split(',')))).values\n","    return sessions\n","\n","\n","def read_dataset(dataset_dir):\n","    train_sessions = read_sessions(dataset_dir / 'train.txt')\n","    test_sessions = read_sessions(dataset_dir / 'test.txt')\n","    with open(dataset_dir / 'num_items.txt', 'r') as f:\n","        num_items = int(f.readline())\n","    return train_sessions, test_sessions, num_items\n","\n","\n","class AugmentedDataset:\n","    def __init__(self, sessions, sort_by_length=True):\n","        self.sessions = sessions\n","        index = create_index(sessions)  # columns: sessionId, labelIndex\n","        if sort_by_length:\n","            # sort by labelIndex in descending order\n","            ind = np.argsort(index[:, 1])[::-1]\n","            index = index[ind]\n","        self.index = index\n","\n","    def __getitem__(self, idx):\n","        sid, lidx = self.index[idx]\n","        seq = self.sessions[sid][:lidx]\n","        label = self.sessions[sid][lidx]\n","        return seq, label\n","\n","    def __len__(self):\n","        return len(self.index)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"874vbgVdMWL6"},"source":["## Utils"]},{"cell_type":"code","metadata":{"id":"5LXZkAU2Mbpj"},"source":["import time\n","from collections import defaultdict\n","\n","import torch as th\n","from torch import nn, optim\n","\n","\n","# ignore weight decay for parameters in bias, batch norm and activation\n","def fix_weight_decay(model):\n","    decay = []\n","    no_decay = []\n","    for name, param in model.named_parameters():\n","        if not param.requires_grad:\n","            continue\n","        if any(map(lambda x: x in name, ['bias', 'batch_norm', 'activation'])):\n","            no_decay.append(param)\n","        else:\n","            decay.append(param)\n","    params = [{'params': decay}, {'params': no_decay, 'weight_decay': 0}]\n","    return params\n","\n","\n","def prepare_batch(batch, device):\n","    inputs, labels = batch\n","    inputs_gpu = [x.to(device) for x in inputs]\n","    labels_gpu = labels.to(device)\n","    return inputs_gpu, labels_gpu\n","\n","\n","def evaluate(model, data_loader, device, Ks=[20]):\n","    model.eval()\n","    num_samples = 0\n","    max_K = max(Ks)\n","    results = defaultdict(float)\n","    with th.no_grad():\n","        for batch in data_loader:\n","            inputs, labels = prepare_batch(batch, device)\n","            logits = model(*inputs)\n","            batch_size = logits.size(0)\n","            num_samples += batch_size\n","            topk = th.topk(logits, k=max_K, sorted=True)[1]\n","            labels = labels.unsqueeze(-1)\n","            for K in Ks:\n","                hit_ranks = th.where(topk[:, :K] == labels)[1] + 1\n","                hit_ranks = hit_ranks.float().cpu()\n","                results[f'HR@{K}'] += hit_ranks.numel()\n","                results[f'MRR@{K}'] += hit_ranks.reciprocal().sum().item()\n","                results[f'NDCG@{K}'] += th.log2(1 + hit_ranks).reciprocal().sum().item()\n","    for metric in results:\n","        results[metric] /= num_samples\n","    return results\n","\n","\n","def print_results(results, epochs=None):\n","    print('Metric\\t' + '\\t'.join(results.keys()))\n","    print(\n","        'Value\\t' +\n","        '\\t'.join([f'{round(val * 100, 2):.2f}' for val in results.values()])\n","    )\n","    if epochs is not None:\n","        print('Epoch\\t' + '\\t'.join([str(epochs[metric]) for metric in results]))\n","\n","\n","class TrainRunner:\n","    def __init__(\n","        self,\n","        model,\n","        train_loader,\n","        test_loader,\n","        device,\n","        lr=1e-3,\n","        weight_decay=0,\n","        patience=3,\n","        Ks=[20],\n","    ):\n","        self.model = model\n","        if weight_decay > 0:\n","            params = fix_weight_decay(model)\n","        else:\n","            params = model.parameters()\n","        self.optimizer = optim.AdamW(params, lr=lr, weight_decay=weight_decay)\n","        self.train_loader = train_loader\n","        self.test_loader = test_loader\n","        self.device = device\n","        self.epoch = 0\n","        self.batch = 0\n","        self.patience = patience\n","        self.Ks = Ks\n","\n","    def train(self, epochs, log_interval=100):\n","        max_results = defaultdict(float)\n","        max_epochs = defaultdict(int)\n","        bad_counter = 0\n","        t = time.time()\n","        mean_loss = 0\n","        for epoch in range(epochs):\n","            self.model.train()\n","            for batch in self.train_loader:\n","                inputs, labels = prepare_batch(batch, self.device)\n","                self.optimizer.zero_grad()\n","                logits = self.model(*inputs)\n","                loss = nn.functional.cross_entropy(logits, labels)\n","                loss.backward()\n","                self.optimizer.step()\n","                mean_loss += loss.item() / log_interval\n","                if self.batch > 0 and self.batch % log_interval == 0:\n","                    print(\n","                        f'Batch {self.batch}: Loss = {mean_loss:.4f}, Time Elapsed = {time.time() - t:.2f}s'\n","                    )\n","                    t = time.time()\n","                    mean_loss = 0\n","                self.batch += 1\n","\n","            curr_results = evaluate(\n","                self.model, self.test_loader, self.device, Ks=self.Ks\n","            )\n","\n","            print(f'\\nEpoch {self.epoch}:')\n","            print_results(curr_results)\n","\n","            any_better_result = False\n","            for metric in curr_results:\n","                if curr_results[metric] > max_results[metric]:\n","                    max_results[metric] = curr_results[metric]\n","                    max_epochs[metric] = self.epoch\n","                    any_better_result = True\n","\n","            if any_better_result:\n","                bad_counter = 0\n","            else:\n","                bad_counter += 1\n","                if bad_counter == self.patience:\n","                    break\n","\n","            self.epoch += 1\n","        print('\\nBest results')\n","        print_results(max_results, max_epochs)\n","        return max_results"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"FVTTbG64MtwT"},"source":["## Model"]},{"cell_type":"code","metadata":{"id":"F_AJMToXMtts"},"source":["import torch as th\n","from torch import nn\n","import dgl\n","import dgl.ops as F\n","import dgl.function as fn\n","\n","\n","class EOPA(nn.Module):\n","    def __init__(\n","        self, input_dim, output_dim, batch_norm=True, feat_drop=0.0, activation=None\n","    ):\n","        super().__init__()\n","        self.batch_norm = nn.BatchNorm1d(input_dim) if batch_norm else None\n","        self.feat_drop = nn.Dropout(feat_drop)\n","        self.gru = nn.GRU(input_dim, input_dim, batch_first=True)\n","        self.fc_self = nn.Linear(input_dim, output_dim, bias=False)\n","        self.fc_neigh = nn.Linear(input_dim, output_dim, bias=False)\n","        self.activation = activation\n","\n","    def reducer(self, nodes):\n","        m = nodes.mailbox['m']  # (num_nodes, deg, d)\n","        # m[i]: the messages passed to the i-th node with in-degree equal to 'deg'\n","        # the order of messages follows the order of incoming edges\n","        # since the edges are sorted by occurrence time when the EOP multigraph is built\n","        # the messages are in the order required by EOPA\n","        _, hn = self.gru(m)  # hn: (1, num_nodes, d)\n","        return {'neigh': hn.squeeze(0)}\n","\n","    def forward(self, mg, feat):\n","        with mg.local_scope():\n","            if self.batch_norm is not None:\n","                feat = self.batch_norm(feat)\n","            mg.ndata['ft'] = self.feat_drop(feat)\n","            if mg.number_of_edges() > 0:\n","                mg.update_all(fn.copy_u('ft', 'm'), self.reducer)\n","                neigh = mg.ndata['neigh']\n","                rst = self.fc_self(feat) + self.fc_neigh(neigh)\n","            else:\n","                rst = self.fc_self(feat)\n","            if self.activation is not None:\n","                rst = self.activation(rst)\n","            return rst\n","\n","\n","class SGAT(nn.Module):\n","    def __init__(\n","        self,\n","        input_dim,\n","        hidden_dim,\n","        output_dim,\n","        batch_norm=True,\n","        feat_drop=0.0,\n","        activation=None,\n","    ):\n","        super().__init__()\n","        self.batch_norm = nn.BatchNorm1d(input_dim) if batch_norm else None\n","        self.feat_drop = nn.Dropout(feat_drop)\n","        self.fc_q = nn.Linear(input_dim, hidden_dim, bias=True)\n","        self.fc_k = nn.Linear(input_dim, hidden_dim, bias=False)\n","        self.fc_v = nn.Linear(input_dim, output_dim, bias=False)\n","        self.fc_e = nn.Linear(hidden_dim, 1, bias=False)\n","        self.activation = activation\n","\n","    def forward(self, sg, feat):\n","        if self.batch_norm is not None:\n","            feat = self.batch_norm(feat)\n","        feat = self.feat_drop(feat)\n","        q = self.fc_q(feat)\n","        k = self.fc_k(feat)\n","        v = self.fc_v(feat)\n","        e = F.u_add_v(sg, q, k)\n","        e = self.fc_e(th.sigmoid(e))\n","        a = F.edge_softmax(sg, e)\n","        rst = F.u_mul_e_sum(sg, v, a)\n","        if self.activation is not None:\n","            rst = self.activation(rst)\n","        return rst\n","\n","\n","class AttnReadout(nn.Module):\n","    def __init__(\n","        self,\n","        input_dim,\n","        hidden_dim,\n","        output_dim,\n","        batch_norm=True,\n","        feat_drop=0.0,\n","        activation=None,\n","    ):\n","        super().__init__()\n","        self.batch_norm = nn.BatchNorm1d(input_dim) if batch_norm else None\n","        self.feat_drop = nn.Dropout(feat_drop)\n","        self.fc_u = nn.Linear(input_dim, hidden_dim, bias=False)\n","        self.fc_v = nn.Linear(input_dim, hidden_dim, bias=True)\n","        self.fc_e = nn.Linear(hidden_dim, 1, bias=False)\n","        self.fc_out = (\n","            nn.Linear(input_dim, output_dim, bias=False)\n","            if output_dim != input_dim else None\n","        )\n","        self.activation = activation\n","\n","    def forward(self, g, feat, last_nodes):\n","        if self.batch_norm is not None:\n","            feat = self.batch_norm(feat)\n","        feat = self.feat_drop(feat)\n","        feat_u = self.fc_u(feat)\n","        feat_v = self.fc_v(feat[last_nodes])\n","        feat_v = dgl.broadcast_nodes(g, feat_v)\n","        e = self.fc_e(th.sigmoid(feat_u + feat_v))\n","        alpha = F.segment.segment_softmax(g.batch_num_nodes(), e)\n","        feat_norm = feat * alpha\n","        rst = F.segment.segment_reduce(g.batch_num_nodes(), feat_norm, 'sum')\n","        if self.fc_out is not None:\n","            rst = self.fc_out(rst)\n","        if self.activation is not None:\n","            rst = self.activation(rst)\n","        return rst\n","\n","\n","class LESSR(nn.Module):\n","    def __init__(\n","        self, num_items, embedding_dim, num_layers, batch_norm=True, feat_drop=0.0\n","    ):\n","        super().__init__()\n","        self.embedding = nn.Embedding(num_items, embedding_dim, max_norm=1)\n","        self.indices = nn.Parameter(\n","            th.arange(num_items, dtype=th.long), requires_grad=False\n","        )\n","        self.num_layers = num_layers\n","        self.layers = nn.ModuleList()\n","        input_dim = embedding_dim\n","        for i in range(num_layers):\n","            if i % 2 == 0:\n","                layer = EOPA(\n","                    input_dim,\n","                    embedding_dim,\n","                    batch_norm=batch_norm,\n","                    feat_drop=feat_drop,\n","                    activation=nn.PReLU(embedding_dim),\n","                )\n","            else:\n","                layer = SGAT(\n","                    input_dim,\n","                    embedding_dim,\n","                    embedding_dim,\n","                    batch_norm=batch_norm,\n","                    feat_drop=feat_drop,\n","                    activation=nn.PReLU(embedding_dim),\n","                )\n","            input_dim += embedding_dim\n","            self.layers.append(layer)\n","        self.readout = AttnReadout(\n","            input_dim,\n","            embedding_dim,\n","            embedding_dim,\n","            batch_norm=batch_norm,\n","            feat_drop=feat_drop,\n","            activation=nn.PReLU(embedding_dim),\n","        )\n","        input_dim += embedding_dim\n","        self.batch_norm = nn.BatchNorm1d(input_dim) if batch_norm else None\n","        self.feat_drop = nn.Dropout(feat_drop)\n","        self.fc_sr = nn.Linear(input_dim, embedding_dim, bias=False)\n","\n","    def forward(self, mg, sg=None):\n","        iid = mg.ndata['iid']\n","        feat = self.embedding(iid)\n","        for i, layer in enumerate(self.layers):\n","            if i % 2 == 0:\n","                out = layer(mg, feat)\n","            else:\n","                out = layer(sg, feat)\n","            feat = th.cat([out, feat], dim=1)\n","        last_nodes = mg.filter_nodes(lambda nodes: nodes.data['last'] == 1)\n","        sr_g = self.readout(mg, feat, last_nodes)\n","        sr_l = feat[last_nodes]\n","        sr = th.cat([sr_l, sr_g], dim=1)\n","        if self.batch_norm is not None:\n","            sr = self.batch_norm(sr)\n","        sr = self.fc_sr(self.feat_drop(sr))\n","        logits = sr @ self.embedding(self.indices).t()\n","        return logits"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Udy2JJZtMbnP"},"source":["## Main"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"krAmCUY7M7Jc","executionInfo":{"status":"ok","timestamp":1638014955678,"user_tz":-330,"elapsed":577644,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}},"outputId":"4bfa0dc4-2a72-413d-f91e-350737bcb252"},"source":["from pathlib import Path\n","import torch as th\n","from torch.utils.data import DataLoader\n","\n","\n","args = Args()\n","\n","\n","dataset_dir = Path(args.dataset_dir)\n","args.Ks = [int(K) for K in args.Ks.split(',')]\n","print('reading dataset')\n","train_sessions, test_sessions, num_items = read_dataset(dataset_dir)\n","\n","\n","if args.valid_split is not None:\n","    num_valid = int(len(train_sessions) * args.valid_split)\n","    test_sessions = train_sessions[-num_valid:]\n","    train_sessions = train_sessions[:-num_valid]\n","\n","\n","train_set = AugmentedDataset(train_sessions)\n","test_set = AugmentedDataset(test_sessions)\n","\n","\n","if args.num_layers > 1:\n","    collate_fn = collate_fn_factory(seq_to_eop_multigraph, seq_to_shortcut_graph)\n","else:\n","    collate_fn = collate_fn_factory(seq_to_eop_multigraph)\n","\n","\n","train_loader = DataLoader(\n","    train_set,\n","    batch_size=args.batch_size,\n","    shuffle=True,\n","    drop_last=True,\n","    num_workers=args.num_workers,\n","    collate_fn=collate_fn,\n",")\n","\n","test_loader = DataLoader(\n","    test_set,\n","    batch_size=args.batch_size,\n","    shuffle=False,\n","    num_workers=args.num_workers,\n","    collate_fn=collate_fn,\n",")\n","\n","\n","model = LESSR(num_items, args.embedding_dim, args.num_layers, feat_drop=args.feat_drop)\n","device = th.device('cuda' if th.cuda.is_available() else 'cpu')\n","model = model.to(device)\n","print(model)\n","\n","\n","runner = TrainRunner(\n","    model,\n","    train_loader,\n","    test_loader,\n","    device=device,\n","    lr=args.lr,\n","    weight_decay=args.weight_decay,\n","    patience=args.patience,\n","    Ks=args.Ks,\n",")\n","\n","\n","print('start training')\n","runner.train(args.epochs, args.log_interval)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["reading dataset\n","LESSR(\n","  (embedding): Embedding(3429, 32, max_norm=1)\n","  (layers): ModuleList(\n","    (0): EOPA(\n","      (batch_norm): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (feat_drop): Dropout(p=0.2, inplace=False)\n","      (gru): GRU(32, 32, batch_first=True)\n","      (fc_self): Linear(in_features=32, out_features=32, bias=False)\n","      (fc_neigh): Linear(in_features=32, out_features=32, bias=False)\n","      (activation): PReLU(num_parameters=32)\n","    )\n","    (1): SGAT(\n","      (batch_norm): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (feat_drop): Dropout(p=0.2, inplace=False)\n","      (fc_q): Linear(in_features=64, out_features=32, bias=True)\n","      (fc_k): Linear(in_features=64, out_features=32, bias=False)\n","      (fc_v): Linear(in_features=64, out_features=32, bias=False)\n","      (fc_e): Linear(in_features=32, out_features=1, bias=False)\n","      (activation): PReLU(num_parameters=32)\n","    )\n","    (2): EOPA(\n","      (batch_norm): BatchNorm1d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (feat_drop): Dropout(p=0.2, inplace=False)\n","      (gru): GRU(96, 96, batch_first=True)\n","      (fc_self): Linear(in_features=96, out_features=32, bias=False)\n","      (fc_neigh): Linear(in_features=96, out_features=32, bias=False)\n","      (activation): PReLU(num_parameters=32)\n","    )\n","  )\n","  (readout): AttnReadout(\n","    (batch_norm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (feat_drop): Dropout(p=0.2, inplace=False)\n","    (fc_u): Linear(in_features=128, out_features=32, bias=False)\n","    (fc_v): Linear(in_features=128, out_features=32, bias=True)\n","    (fc_e): Linear(in_features=32, out_features=1, bias=False)\n","    (fc_out): Linear(in_features=128, out_features=32, bias=False)\n","    (activation): PReLU(num_parameters=32)\n","  )\n","  (batch_norm): BatchNorm1d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","  (feat_drop): Dropout(p=0.2, inplace=False)\n","  (fc_sr): Linear(in_features=160, out_features=32, bias=False)\n",")\n","start training\n","\n","Epoch 0:\n","Metric\tHR@10\tMRR@10\tNDCG@10\tHR@20\tMRR@20\tNDCG@20\n","Value\t18.90\t14.54\t15.58\t20.79\t14.67\t16.05\n","Batch 100: Loss = 7.7529, Time Elapsed = 38.81s\n","\n","Epoch 1:\n","Metric\tHR@10\tMRR@10\tNDCG@10\tHR@20\tMRR@20\tNDCG@20\n","Value\t23.18\t18.23\t19.43\t25.22\t18.37\t19.95\n","\n","Epoch 2:\n","Metric\tHR@10\tMRR@10\tNDCG@10\tHR@20\tMRR@20\tNDCG@20\n","Value\t27.36\t20.51\t22.17\t29.28\t20.65\t22.66\n","Batch 200: Loss = 6.6354, Time Elapsed = 41.06s\n","\n","Epoch 3:\n","Metric\tHR@10\tMRR@10\tNDCG@10\tHR@20\tMRR@20\tNDCG@20\n","Value\t30.62\t22.00\t24.11\t33.01\t22.16\t24.71\n","\n","Epoch 4:\n","Metric\tHR@10\tMRR@10\tNDCG@10\tHR@20\tMRR@20\tNDCG@20\n","Value\t34.04\t23.61\t26.15\t36.85\t23.81\t26.87\n","Batch 300: Loss = 5.7573, Time Elapsed = 40.80s\n","\n","Epoch 5:\n","Metric\tHR@10\tMRR@10\tNDCG@10\tHR@20\tMRR@20\tNDCG@20\n","Value\t37.66\t24.79\t27.92\t41.61\t25.06\t28.91\n","\n","Epoch 6:\n","Metric\tHR@10\tMRR@10\tNDCG@10\tHR@20\tMRR@20\tNDCG@20\n","Value\t41.16\t26.51\t30.06\t45.56\t26.81\t31.16\n","Batch 400: Loss = 5.0562, Time Elapsed = 41.00s\n","\n","Epoch 7:\n","Metric\tHR@10\tMRR@10\tNDCG@10\tHR@20\tMRR@20\tNDCG@20\n","Value\t43.58\t27.66\t31.51\t48.65\t28.01\t32.80\n","Batch 500: Loss = 4.4297, Time Elapsed = 38.31s\n","\n","Epoch 8:\n","Metric\tHR@10\tMRR@10\tNDCG@10\tHR@20\tMRR@20\tNDCG@20\n","Value\t45.45\t28.36\t32.49\t51.27\t28.77\t33.98\n","\n","Epoch 9:\n","Metric\tHR@10\tMRR@10\tNDCG@10\tHR@20\tMRR@20\tNDCG@20\n","Value\t47.09\t28.83\t33.24\t53.05\t29.24\t34.75\n","Batch 600: Loss = 3.9828, Time Elapsed = 40.68s\n","\n","Epoch 10:\n","Metric\tHR@10\tMRR@10\tNDCG@10\tHR@20\tMRR@20\tNDCG@20\n","Value\t47.98\t29.44\t33.91\t53.66\t29.84\t35.36\n","\n","Epoch 11:\n","Metric\tHR@10\tMRR@10\tNDCG@10\tHR@20\tMRR@20\tNDCG@20\n","Value\t48.82\t30.07\t34.58\t55.08\t30.50\t36.17\n","Batch 700: Loss = 3.6589, Time Elapsed = 40.59s\n","\n","Epoch 12:\n","Metric\tHR@10\tMRR@10\tNDCG@10\tHR@20\tMRR@20\tNDCG@20\n","Value\t49.15\t30.41\t34.91\t56.25\t30.90\t36.71\n","\n","Epoch 13:\n","Metric\tHR@10\tMRR@10\tNDCG@10\tHR@20\tMRR@20\tNDCG@20\n","Value\t49.90\t30.91\t35.47\t56.36\t31.36\t37.11\n","Batch 800: Loss = 3.4623, Time Elapsed = 43.01s\n","\n","Epoch 14:\n","Metric\tHR@10\tMRR@10\tNDCG@10\tHR@20\tMRR@20\tNDCG@20\n","Value\t50.40\t31.16\t35.78\t56.67\t31.61\t37.38\n","\n","Epoch 15:\n","Metric\tHR@10\tMRR@10\tNDCG@10\tHR@20\tMRR@20\tNDCG@20\n","Value\t50.49\t31.22\t35.84\t57.33\t31.71\t37.58\n","Batch 900: Loss = 3.3167, Time Elapsed = 43.44s\n","\n","Epoch 16:\n","Metric\tHR@10\tMRR@10\tNDCG@10\tHR@20\tMRR@20\tNDCG@20\n","Value\t50.74\t31.21\t35.89\t57.83\t31.70\t37.69\n","Batch 1000: Loss = 3.1853, Time Elapsed = 40.49s\n","\n","Epoch 17:\n","Metric\tHR@10\tMRR@10\tNDCG@10\tHR@20\tMRR@20\tNDCG@20\n","Value\t51.10\t31.54\t36.23\t58.11\t32.04\t38.02\n","\n","Epoch 18:\n","Metric\tHR@10\tMRR@10\tNDCG@10\tHR@20\tMRR@20\tNDCG@20\n","Value\t51.13\t31.56\t36.25\t58.34\t32.08\t38.09\n","Batch 1100: Loss = 3.0797, Time Elapsed = 42.92s\n","\n","Epoch 19:\n","Metric\tHR@10\tMRR@10\tNDCG@10\tHR@20\tMRR@20\tNDCG@20\n","Value\t51.21\t31.78\t36.43\t57.70\t32.23\t38.07\n","\n","Epoch 20:\n","Metric\tHR@10\tMRR@10\tNDCG@10\tHR@20\tMRR@20\tNDCG@20\n","Value\t51.16\t31.45\t36.17\t57.97\t31.93\t37.90\n","Batch 1200: Loss = 2.9932, Time Elapsed = 43.07s\n","\n","Epoch 21:\n","Metric\tHR@10\tMRR@10\tNDCG@10\tHR@20\tMRR@20\tNDCG@20\n","Value\t51.18\t31.71\t36.38\t57.97\t32.19\t38.11\n","\n","Epoch 22:\n","Metric\tHR@10\tMRR@10\tNDCG@10\tHR@20\tMRR@20\tNDCG@20\n","Value\t51.43\t31.62\t36.36\t58.28\t32.09\t38.09\n","Batch 1300: Loss = 2.9322, Time Elapsed = 40.85s\n","\n","Epoch 23:\n","Metric\tHR@10\tMRR@10\tNDCG@10\tHR@20\tMRR@20\tNDCG@20\n","Value\t50.79\t31.37\t36.03\t58.03\t31.88\t37.87\n","\n","Epoch 24:\n","Metric\tHR@10\tMRR@10\tNDCG@10\tHR@20\tMRR@20\tNDCG@20\n","Value\t50.65\t31.49\t36.09\t58.31\t32.03\t38.03\n","\n","Best results\n","Metric\tHR@10\tMRR@10\tNDCG@10\tHR@20\tMRR@20\tNDCG@20\n","Value\t51.43\t31.78\t36.43\t58.34\t32.23\t38.11\n","Epoch\t22\t19\t19\t18\t19\t21\n"]},{"output_type":"execute_result","data":{"text/plain":["defaultdict(float,\n","            {'HR@10': 0.5143334261063178,\n","             'HR@20': 0.58335652657946,\n","             'MRR@10': 0.3178392481478748,\n","             'MRR@20': 0.3223134756353683,\n","             'NDCG@10': 0.36433385989807326,\n","             'NDCG@20': 0.38109836749568676})"]},"metadata":{},"execution_count":6}]},{"cell_type":"markdown","metadata":{"id":"68pdexKKQNuS"},"source":["---"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"C10k8bo_QNuT","executionInfo":{"status":"ok","timestamp":1638014967203,"user_tz":-330,"elapsed":5705,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}},"outputId":"a4afb425-6bf4-4edf-860d-20fdec83b7c2"},"source":["!apt-get -qq install tree\n","!rm -r sample_data"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Selecting previously unselected package tree.\n","(Reading database ... 155222 files and directories currently installed.)\n","Preparing to unpack .../tree_1.7.0-5_amd64.deb ...\n","Unpacking tree (1.7.0-5) ...\n","Setting up tree (1.7.0-5) ...\n","Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TZRs_d-2QNuU","executionInfo":{"status":"ok","timestamp":1638014967204,"user_tz":-330,"elapsed":20,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}},"outputId":"eac15d60-3331-4c84-959a-bd98e353f3a2"},"source":["!tree -h --du ."],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":[".\n","├── [   4]  num_items.txt\n","├── [ 23K]  test.txt\n","└── [173K]  train.txt\n","\n"," 200K used in 0 directories, 3 files\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1CfCN3IZQNuU","executionInfo":{"status":"ok","timestamp":1638014977422,"user_tz":-330,"elapsed":3650,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}},"outputId":"16693ead-fdbf-49dd-d62a-37e9126bb4dd"},"source":["!pip install -q watermark\n","%reload_ext watermark\n","%watermark -a \"Sparsh A.\" -m -iv -u -t -d"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Author: Sparsh A.\n","\n","Last updated: 2021-11-27 12:09:36\n","\n","Compiler    : GCC 7.5.0\n","OS          : Linux\n","Release     : 5.4.104+\n","Machine     : x86_64\n","Processor   : x86_64\n","CPU cores   : 2\n","Architecture: 64bit\n","\n","numpy  : 1.19.5\n","pandas : 1.1.5\n","torch  : 1.10.0+cu111\n","dgl    : 0.6.1\n","IPython: 5.5.0\n","\n"]}]},{"cell_type":"markdown","metadata":{"id":"yspsGY6iQNuV"},"source":["---"]},{"cell_type":"markdown","metadata":{"id":"1RSSPS8eQNuV"},"source":["**END**"]}]}