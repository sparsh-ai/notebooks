{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"T602245 | BST in PyTorch","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyNfOqgI0u4O1WUso88NzsjG"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"CuUYKeCrxb-N"},"source":["# BST in PyTorch\n","\n","> BST Model Implementation in PyTorch. Main purpose is to get familier with BST model, so only code is available upto trainer module. Inference and dataset runs will be added in future possibly."]},{"cell_type":"markdown","metadata":{"id":"ayhIfDiPxPFY"},"source":["### Imports"]},{"cell_type":"code","metadata":{"id":"FhS247l1SymE"},"source":["import random\n","import numpy as np\n","import time\n","\n","import torch\n","from torch import nn"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ngbBIZikxNUx"},"source":["### Params"]},{"cell_type":"code","metadata":{"id":"N21knoI8SZTG"},"source":["%%writefile config_sample.py\n","config = {'item_embed': {\n","    'num_embeddings': 500,\n","    'embedding_dim': 32,\n","    'sparse': False,\n","    'padding_idx': -1,\n","},\n","    'trans': {\n","        'input_size': 32,\n","        'hidden_size': 16,\n","        'n_layers': 2,\n","        'n_heads': 4,\n","        'max_len': 5,\n","    },\n","    'context_features': [\n","            {'num_embeddings': 6, 'embedding_dim': 10, 'sparse': False, 'padding_idx': -1},\n","            {'num_embeddings': 4, 'embedding_dim': 10, 'sparse': False, 'padding_idx': -1},\n","\n","        ],\n","\n","    'cuda': False,\n","    'max_seq_len': 6,\n","}"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-dqEg3nXxQY8"},"source":["## Utils"]},{"cell_type":"code","metadata":{"id":"A7gp8uLUSpDn"},"source":["def pad(seq, max_seq_len, pad_with=0):\n","    seq_len = len(seq)\n","    return [pad_with]*(max_seq_len - seq_len) + seq\n","\n","\n","def batch_fn(user_seq, context_features, batch_size, max_seq_len, shuffle=True):\n","    if shuffle:\n","        data = list(zip(user_seq, context_features))\n","        random.shuffle(data)\n","        user_seq, context_features = zip(*data)\n","    context_features = np.array(context_features).T\n","    for start_idx in range(0, len(user_seq) - batch_size + 1, batch_size):\n","        batch = user_seq[start_idx:start_idx + batch_size]\n","        context_batch = context_features[..., start_idx:start_idx + batch_size].tolist()\n","        batch = [seq[-max_seq_len:] for seq in batch]\n","        user_seq_batch = []\n","        for seq in batch:\n","            pseq = pad(seq, max_seq_len)\n","            user_seq_batch += [pseq]\n","        yield user_seq_batch, context_batch"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"VuehNyDYSz2d"},"source":["class GradientClipping:\n","    def __init__(self, clip_value):\n","        self.epoch_grads = []\n","        self.total_grads = []\n","        self.clip = clip_value\n","\n","    def track_grads(self, x, grad_input, grad_output):\n","        self.epoch_grads.append(grad_input[0].norm().cpu().data.numpy())\n","\n","    def register_hook(self, encoder):\n","        encoder.register_backward_hook(self.track_grads)\n","\n","    def gradient_mean(self):\n","        return np.mean(self.epoch_grads)\n","\n","    def gradient_std(self):\n","        return np.std(self.epoch_grads)\n","\n","    def reset_gradients(self):\n","        self.total_grads.append(self.epoch_grads)\n","        self.epoch_grads = []\n","\n","    def update_clip_value(self):\n","        self.clip = self.gradient_mean() + self.gradient_std()\n","\n","    def update_clip_value_total(self):\n","        grads = [y for x in self.total_grads.append(self.epoch_grads) for y in x]\n","        self.clip = np.mean(grads)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mJs3EOt0xVLu"},"source":["## Model"]},{"cell_type":"code","metadata":{"id":"xyH62_g6Szy5"},"source":["class FF(nn.Module):\n","    \"\"\"\n","    Feed-forward in a transformer layer.\n","    \"\"\"\n","    def __init__(self, input_size, hidden_size):\n","        super().__init__()\n","        self.lin_1 = nn.Linear(input_size, hidden_size)\n","        self.lin_2 = nn.Linear(hidden_size, input_size)\n","        self.relu = nn.ReLU()\n","\n","    def forward(self, x):\n","        output = self.lin_2(self.relu(self.lin_1(x)))\n","        return output\n","\n","\n","class MultiHeadAttention(nn.Module):\n","    \"\"\"\n","    Multi-head Attention block in a transformer layer.\n","    \"\"\"\n","    def __init__(self, att_dim, n_heads):\n","        super().__init__()\n","        # Check for compatible  #Attention Heads\n","        self.n_heads = n_heads\n","        # Check compatibility for input size and #attention heads.\n","        assert att_dim % self.n_heads == 0\n","        self.att_size = int(att_dim / n_heads)\n","\n","        # Query, Key, Value\n","        self._query = nn.Linear(att_dim, att_dim, bias=False)\n","        self._key = nn.Linear(att_dim, att_dim, bias=False)\n","        self._value = nn.Linear(att_dim, att_dim, bias=False)\n","\n","        # Attention Block\n","        self.dense = nn.Linear(att_dim, att_dim, bias=False)\n","        self.activation = nn.Softmax(dim=-1)\n","        self.dropout = nn.Dropout(0.1)\n","\n","    def forward(self, q, k, v, mask=None):\n","        scale_factor = torch.sqrt(torch.FloatTensor([self.n_heads])).item()\n","        batch_size = q.size(0)\n","\n","        # To Multiple Attention Heads\n","        _query = self._query(q).view(batch_size, -1, self.n_heads, self.att_size).transpose(1, 2)\n","        _key = self._key(k).view(batch_size, -1, self.n_heads, self.att_size).transpose(1, 2)\n","        _value = self._value(v).view(batch_size, -1, self.n_heads, self.att_size).transpose(1, 2)\n","\n","        # Scaled dot-product Attention score\n","        score = torch.matmul(_query, _key.transpose(-2, -1)) / scale_factor\n","        # Mask applied.\n","        if mask is not None:\n","            mask = mask.unsqueeze(1)\n","            score = score.masked_fill(mask == 0, -1e9)\n","        # Softmax on Score\n","        score = self.activation(score)\n","        z = torch.matmul(self.dropout(score), _value)\n","\n","        # To fully-connected layer\n","        z = z.transpose(1, 2).reshape(batch_size, -1, self.att_size * self.n_heads)\n","        return self.dense(z)\n","\n","\n","class EncoderCell(nn.Module):\n","    \"\"\"\n","    Encoder Cell contains MultiHeadAttention > Add & LayerNorm1 >\n","    Feed Forward > Add & LayerNorm2\n","    \"\"\"\n","    def __init__(self, input_size, hidden_size, n_heads):\n","        super().__init__()\n","        # Attention Block\n","        self.mh_attention = MultiHeadAttention(input_size, n_heads)\n","        self.lnorm_1 = nn.LayerNorm(input_size)\n","        # Feed forward block\n","        self.ff = FF(input_size, hidden_size)\n","        self.lnorm_2 = nn.LayerNorm(input_size)\n","        # Dropout\n","        self.dropout = nn.Dropout(0.1)\n","\n","    def forward(self, x, mask=None):\n","        attention_out = self.mh_attention(x, x, x, mask)\n","        attention_out = self.lnorm_1(self.dropout(attention_out) + x)\n","\n","        ff_attention = self.ff(attention_out)\n","        return self.lnorm_2(self.dropout(ff_attention) + attention_out)\n","\n","\n","class Encoder(nn.Module):\n","    \"\"\"\n","    Encoder Block with n stacked encoder cells.\n","    \"\"\"\n","    def __init__(self, input_size, hidden_size, n_layers, n_heads):\n","        super().__init__()\n","        # Stack of encoder-cells n_layers high\n","        self.stack = nn.ModuleList()\n","        # Building encoder stack\n","        for layer in range(n_layers):\n","            self.stack.append(EncoderCell(input_size, hidden_size, n_heads))\n","        # Dropout layer\n","        self.dropout = nn.Dropout(0.1)\n","\n","    def forward(self, x, mask=None):\n","        for cell in self.stack:\n","            x = cell(self.dropout(x), mask)\n","        return x"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Dy2RRdd3TFoY"},"source":["class BSTransformer(nn.Module):\n","    \"\"\"\n","    Behaviour Sequence Transformer with dynamic context embeddings\n","    and sinusoidal pos-encoding.\n","    \"\"\"\n","    def __init__(self, config):\n","        super().__init__()\n","        self.config = config\n","        self.item_embed = nn.Embedding(num_embeddings=config['item_embed']['num_embeddings'],\n","                                       embedding_dim=config['item_embed']['embedding_dim'],\n","                                       sparse=config['item_embed']['sparse'],\n","                                       padding_idx=config['item_embed']['padding_idx'])\n","\n","        self.pos_embedding = self.pos_embedding_sinusoidal(config['max_seq_len'], \n","                                                           config['item_embed']['embedding_dim'],\n","                                                           config['cuda'])\n","        self.context_embeddings = nn.ModuleList([nn.Embedding(num_embeddings=feat['num_embeddings'],\n","                                                              embedding_dim=feat['embedding_dim'],\n","                                                              sparse=feat['sparse'],\n","                                                              padding_idx=feat['padding_idx'])\n","                                                 for feat in config['context_features']])\n","\n","        self.encoder = Encoder(input_size=config['trans']['input_size'],\n","                               hidden_size=config['trans']['hidden_size'],\n","                               n_layers=config['trans']['n_layers'],\n","                               n_heads=config['trans']['n_heads'])\n","\n","        mlp_input_size = config['trans']['input_size'] + sum(\n","            [feat['embedding_dim'] for feat in config['context_features']])\n","\n","        self.mlp = nn.Sequential(nn.Linear(mlp_input_size, 1024),\n","                                 nn.LeakyReLU(),\n","                                 nn.Linear(1024, config['item_embed']['num_embeddings'])\n","                                 )\n","\n","        for param in self.parameters():\n","            if param.dim() > 1 and config['init_method'] == 'xavier':\n","                torch.nn.init.xavier_uniform_(param)\n","            if param.dim() > 1 and config['init_method'] == 'kaiming':\n","                torch.nn.init.kaiming_uniform_(param)\n","        print(f\"Parameters initialised using {config['init_method']} initialisation!\")\n","\n","    def forward(self, x, context):\n","        targets = x[..., -1:].long()\n","        enc_mask = self.get_mask(x)\n","        item_embed = self.item_embed(x.long()) * np.sqrt(self.config['item_embed']['embedding_dim'])\n","        agg_encoding = torch.mean(self.encoder(item_embed + self.pos_embedding[:x.size(1), :], mask=enc_mask), dim=1)\n","        context_embs = torch.tensor([]).to(x.device)\n","        for emb, feat in zip(self.context_embeddings, context):\n","            context_embs = torch.cat([context_embs, emb(feat)], dim=1)\n","        output = self.mlp(torch.cat([agg_encoding, context_embs], dim=1))\n","        return output, targets\n","\n","    def get_mask(self, x):\n","        seq_len = x.size(1)\n","        mask = (x != 0).unsqueeze(1).byte()\n","        triu = (np.triu(np.ones([1, seq_len, seq_len]), k=1) == 0).astype('uint8')\n","        if self.config['cuda']:\n","            dtype = torch.cuda.ByteTensor\n","        else:\n","            dtype = torch.ByteTensor\n","        return dtype(triu) & dtype(mask)\n","\n","    @staticmethod\n","    def pos_embedding_sinusoidal(max_seq_len, embedding_dim, is_cuda):\n","        half_dim = embedding_dim // 2\n","        emb = torch.log(torch.tensor(10000)) / (half_dim - 1)\n","        emb = torch.exp(torch.arange(half_dim, dtype=torch.float) * -emb)\n","        emb = torch.arange(max_seq_len, dtype=torch.float).unsqueeze(\n","            1\n","        ) * emb.unsqueeze(0)\n","        emb = torch.stack((torch.sin(emb), torch.cos(emb)), dim=0).view(\n","            max_seq_len, -1).t().contiguous().view(max_seq_len, -1)\n","        if embedding_dim % 2 == 1:\n","            emb = torch.cat([emb, torch.zeros(max_seq_len, 1)], dim=1)\n","        if is_cuda:\n","            return emb.cuda()\n","        return emb"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"iCHbf9v-xWnp"},"source":["## Trainer"]},{"cell_type":"code","metadata":{"id":"uVepRYk5SzxL"},"source":["class Trainer:\n","    def __init__(self, config, loss_fn, batch_fn, device, grad_clipping=True):\n","        self.config = config\n","        self.bst = self.init_bst_encoder()\n","        self.optimizer = torch.optim.AdamW(self.bst.parameters(), lr=config['lr'])\n","        self.loss_fn = loss_fn\n","        self.batch_fn = batch_fn\n","        self.training_start = None\n","        self.device = device\n","        self.train_loss = 0\n","        self.best_loss = np.inf\n","        self.batch_num = 0\n","        self.epoch_num = 0\n","        self.scheduler = None\n","        try:\n","            if grad_clipping:\n","                self.clipper = GradientClipping(config['clip_value'])\n","                self.clipper.register_hook(self.bst)\n","        except KeyError:\n","            print(\"Gradient Clipping not available! Pass clip value in config!\")\n","\n","    def epoch(self, user_seq, context_features, batch_size, max_seq_len):\n","        self.training_start = time.time()\n","        self.bst.train()\n","        self.train_loss = 0\n","\n","        # Iterate through batch.\n","        for user_seq_batch, context_batch in self.batch_fn(user_seq, context_features, batch_size, max_seq_len):\n","            pred, target = self.bst(torch.tensor(user_seq_batch).to(self.device),\n","                                    torch.tensor(context_batch).to(self.device))\n","            loss = self.loss_fn(pred.view(-1, pred.size(-1)), target.view(-1))\n","\n","            self.optimizer.zero_grad()\n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_(self.bst.parameters(),\n","                                           self.clipper.clip)\n","            self.optimizer.step()\n","            self.train_loss += loss.data\n","            self.batch_num += 1\n","\n","            self.scheduler.step()  # set_to_none=True\n","        self.train_loss = self.train_loss.cpu().data.numpy() / self.batch_num\n","\n","        # Log\n","        print(f'Loss after {self.batch_num * batch_size} sequences: '\n","              f'{self.train_loss}'\n","              f'\\nTraining time: {time.time() - self.training_start}')\n","\n","        # Save best weights\n","        if self.train_loss < self.best_loss:\n","            self.save_state('best', save_grads=False)\n","            self.best_loss = self.train_loss\n","\n","    def init_bst_encoder(self):\n","        # Init Behaviour Seq Transformer model.\n","        bst = BSTransformer(self.config)\n","        bst = bst.cuda() if self.config['cuda'] else bst\n","        return bst\n","\n","    def save_state(self, path, save_grads=False):\n","        # Save state to path.\n","        torch.save(self.bst.state_dict(), path)\n","        if save_grads:\n","            np.save(f'{path}_grads', self.clipper.total_grads)\n","\n","    def set_lr_scheduler(self, milestones, gamma, last_epoch):\n","        self.scheduler = torch.optim.lr_scheduler.MultiStepLR(self.optimizer, milestones=milestones,\n","                                                              gamma=gamma, last_epoch=last_epoch)"],"execution_count":null,"outputs":[]}]}