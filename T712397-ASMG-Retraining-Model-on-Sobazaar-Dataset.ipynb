{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"T712397 | ASMG Retraining Model on Sobazaar Dataset","provenance":[{"file_id":"1M5g9Rtk1HxbkmNdQrdKwv0ZQ6XgtbJRO","timestamp":1636708149757},{"file_id":"1RRiC55q03vJg8vr-Fr9MbDVr8VHTN8XZ","timestamp":1636618766183}],"collapsed_sections":[],"mount_file_id":"1_gqT0nzqsrSzfRcutMwIMbipZIXu3aKO","authorship_tag":"ABX9TyPVLNu+PrfqZET66CqdWima"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"83c9f2d6bc6a42109250d55a39b4b3e7":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_6af9c2d14cd344408ebeb165aecd5812","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_14a3c2eab70348fd8336ae11dbbfcffd","IPY_MODEL_1f7c997368b34aedb5ecafc1ad6b1121","IPY_MODEL_712fb93cec584db29a18f17f48ce85dc"]}},"6af9c2d14cd344408ebeb165aecd5812":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"14a3c2eab70348fd8336ae11dbbfcffd":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_6149942fe6ad4e0ea119545de5a8d242","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"100%","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_f4349bb599ab413c9c087714eb7ba11f"}},"1f7c997368b34aedb5ecafc1ad6b1121":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_7b90eb42b83641b4890847c92356a8f6","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":842660,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":842660,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_8d3fb8b1ab5742ec983d41c822319729"}},"712fb93cec584db29a18f17f48ce85dc":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_27e8cfe72e7a4a93b1f2e1f705a551a5","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 842660/842660 [35:09&lt;00:00, 407.13it/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_74ce21ef75514a30a12822ebc5165352"}},"6149942fe6ad4e0ea119545de5a8d242":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"f4349bb599ab413c9c087714eb7ba11f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"7b90eb42b83641b4890847c92356a8f6":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"8d3fb8b1ab5742ec983d41c822319729":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"27e8cfe72e7a4a93b1f2e1f705a551a5":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"74ce21ef75514a30a12822ebc5165352":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"30c470ccdef34a2999ab74aad3503ba7":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_7ef1f03aed684fdbbc6602ec174cd2cd","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_45c6daf562174f79ad42975093c84210","IPY_MODEL_d3fc9033cc8f45c9a9bc114a6c590fd9","IPY_MODEL_53e9c4c383124944a89e833d7f0a56df"]}},"7ef1f03aed684fdbbc6602ec174cd2cd":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"45c6daf562174f79ad42975093c84210":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_e69ef6e531314efe9a9ee6dc58e5998b","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"100%","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_813103a929a84c78aa67e72419563cb7"}},"d3fc9033cc8f45c9a9bc114a6c590fd9":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_0fa28afc080a4e98a0f5ccd7d083e3b3","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":842660,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":842660,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_60fe4254494e4bc589c971c5e287ec88"}},"53e9c4c383124944a89e833d7f0a56df":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_bb7538a3fc0045c28b51edc7f7cbbc00","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 842660/842660 [00:03&lt;00:00, 266169.09it/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_d30f88908fe54f779659b41bd1e5ba6f"}},"e69ef6e531314efe9a9ee6dc58e5998b":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"813103a929a84c78aa67e72419563cb7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"0fa28afc080a4e98a0f5ccd7d083e3b3":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"60fe4254494e4bc589c971c5e287ec88":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"bb7538a3fc0045c28b51edc7f7cbbc00":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"d30f88908fe54f779659b41bd1e5ba6f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"1QG8dU-gzPyr"},"source":["# Multiple Retraining Models on Sobazaar dataset"]},{"cell_type":"markdown","metadata":{"id":"CxiWmRiFzT2X"},"source":["## Setup"]},{"cell_type":"markdown","metadata":{"id":"zVtJ4JTGH353"},"source":["### Git"]},{"cell_type":"code","metadata":{"id":"Z3qjPp055tXf","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1636712556084,"user_tz":-330,"elapsed":435,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}},"outputId":"3826fafa-4a43-46b1-a858-598e8f1f17b0"},"source":["import os\n","project_name = \"incremental-learning\"; branch = \"T712397\"; account = \"sparsh-ai\"\n","project_path = os.path.join('/content', branch)\n","\n","if not os.path.exists(project_path):\n","    !cp -r /content/drive/MyDrive/git_credentials/. ~\n","    !mkdir \"{project_path}\"\n","    %cd \"{project_path}\"\n","    !git init\n","    !git remote add origin https://github.com/\"{account}\"/\"{project_name}\".git\n","    !git pull origin \"{branch}\"\n","    !git checkout -b \"{branch}\"\n","else:\n","    %cd \"{project_path}\""],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/T712397\n"]}]},{"cell_type":"code","metadata":{"id":"xoKGydGDIwSX","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1636712915127,"user_tz":-330,"elapsed":663,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}},"outputId":"91e95090-2b95-406a-97e3-39a7d68a2370"},"source":["%cd /content"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/content\n"]}]},{"cell_type":"code","metadata":{"id":"7lAKlgUD5tXi"},"source":["!cd /content/T967215 && git add .\n","!cd /content/T967215 && git commit -m 'commit'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"IqqZ6Do-uswE"},"source":["!cd /content/T967215 && git pull --rebase origin \"{branch}\"\n","!cd /content/T967215 && git push origin \"{branch}\""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"evKxFrICIpy_"},"source":["# !mv /content/ckpts .\n","# !mv /content/soba_4mth_2014_1neg_30seq_1.parquet.snappy ."],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BXJY8c9d4Xi5"},"source":["### Installations"]},{"cell_type":"markdown","metadata":{"id":"BK-ZCkf00xZt"},"source":["### Dataset"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5eJSFty70xW-","executionInfo":{"status":"ok","timestamp":1636712565775,"user_tz":-330,"elapsed":2386,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}},"outputId":"7fe0950a-cadf-4223-a49a-04cb5d24fd6e"},"source":["!wget -q --show-progress https://github.com/RecoHut-Datasets/sobazaar/raw/main/Data/Sobazaar-hashID.csv.gz\n","!wget -q --show-progress https://github.com/sparsh-ai/incremental-learning/raw/T644011/soba_4mth_2014_1neg_30seq_1.parquet.snappy"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Sobazaar-hashID.csv 100%[===================>]  17.11M  --.-KB/s    in 0.1s    \n","soba_4mth_2014_1neg 100%[===================>]  35.27M   147MB/s    in 0.2s    \n"]}]},{"cell_type":"markdown","metadata":{"id":"GB_yDppW3_Yt"},"source":["### Imports"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3lFs8AyO1IWc","executionInfo":{"status":"ok","timestamp":1636712916259,"user_tz":-330,"elapsed":585,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}},"outputId":"bb30dc31-4ef4-4d77-9cd5-451ea37aedf6"},"source":["%tensorflow_version 1.x"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["TensorFlow 1.x selected.\n"]}]},{"cell_type":"code","metadata":{"id":"vrEmNkAAsQlM"},"source":["import numpy as np\n","from tqdm.notebook import tqdm\n","import sys\n","import os\n","import logging\n","import pandas as pd\n","from os import path as osp\n","from pathlib import Path\n","import random\n","import datetime\n","import time\n","import glob\n","\n","import bz2\n","import pickle\n","import _pickle as cPickle\n","\n","import tensorflow as tf"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NyxCtlrJ3_Ta"},"source":["### Params"]},{"cell_type":"code","metadata":{"id":"MXBwnUCD3_RD"},"source":["class Args:\n","    path_bronze = '/content'\n","    path_silver = '/content'\n","    iu_ckpts_path = '/content/ckpts/IU_train11-23_test24-30_1epoch_0.001'\n","    \n","args = Args()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"K5cAMUaO2H8W"},"source":["random.seed(1234)\n","np.random.seed(1234)\n","tf.set_random_seed(123)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Q40X4lHf4JHw"},"source":["### Logger"]},{"cell_type":"code","metadata":{"id":"cibwpV5L4JFb"},"source":["logging.basicConfig(stream=sys.stdout,\n","                    level = logging.DEBUG,\n","                    format='%(asctime)s [%(levelname)s] : %(message)s',\n","                    datefmt='%d-%b-%y %H:%M:%S')\n","\n","logger = logging.getLogger('Logger')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2M0-cN2ZzWE-"},"source":["## Modules"]},{"cell_type":"markdown","metadata":{"id":"qY9Y0q2sz1MS"},"source":["### Utils"]},{"cell_type":"code","metadata":{"id":"tH7lmOJbAOIf"},"source":["def save_pickle(data, title):\n"," with bz2.BZ2File(title + '.pbz2', 'w') as f: \n","    cPickle.dump(data, f)\n","\n","def load_pickle(path):\n","    data = bz2.BZ2File(path+'.pbz2', 'rb')\n","    data = cPickle.load(data)\n","    return data"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"yfAz0LdAsU7S"},"source":["def restore_ckpts():\n","    # using shell commands for now, will be changed to python later\n","    !mkdir /content/temp_ckpts\n","    !git clone -b T644011 https://github.com/sparsh-ai/incremental-learning.git /content/temp_ckpts\n","    !mv /content/temp_ckpts/ckpts /content"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5lHX1pHU7fvN"},"source":["class BatchLoader:\n","    \"\"\"\n","    batch data loader by batch size\n","    return: [[users], [items], np.array(item_seqs_matrix), [seq_lens], [labels]] in batch iterator\n","    \"\"\"\n","\n","    def __init__(self, data_df, batch_size):\n","\n","        self.data_df = data_df.reset_index(drop=True)  # df ['userId', 'itemId', 'label']\n","        self.data_df['index'] = self.data_df.index\n","        self.data_df['batch'] = self.data_df['index'].apply(lambda x: int(x / batch_size) + 1)\n","        self.num_batches = self.data_df['batch'].max()\n","\n","    def get_batch(self, batch_id):\n","\n","        batch = self.data_df[self.data_df['batch'] == batch_id]\n","        users = batch['userId'].tolist()\n","        items = batch['itemId'].tolist()\n","        labels = batch['label'].tolist()\n","        seq_lens = batch['itemSeq'].apply(len).tolist()\n","\n","        item_seqs_matrix = np.zeros([len(batch), 30], np.int32)\n","\n","        i = 0\n","        for itemSeq in batch['itemSeq'].tolist():\n","            for j in range(len(itemSeq)):\n","                item_seqs_matrix[i][j] = itemSeq[j]  # convert list of itemSeq into a matrix with zero padding\n","            i += 1\n","\n","        return [users, items, item_seqs_matrix, seq_lens, labels]\n","\n","\n","def cal_roc_auc(scores, labels):\n","\n","    arr = sorted(zip(scores, labels), key=lambda d: d[0], reverse=True)\n","    pos, neg = 0., 0.\n","    for record in arr:\n","        if record[1] == 1.:\n","            pos += 1\n","        else:\n","            neg += 1\n","\n","    if pos == 0 or neg == 0:\n","        return None\n","\n","    fp, tp = 0., 0.\n","    xy_arr = []\n","    for record in arr:\n","        if record[1] == 1.:\n","            tp += 1\n","        else:\n","            fp += 1\n","        xy_arr.append([fp/neg, tp/pos])\n","\n","    auc = 0.\n","    prev_x = 0.\n","    prev_y = 0.\n","    for x, y in xy_arr:\n","        auc += ((x - prev_x) * (y + prev_y) / 2.)\n","        prev_x = x\n","        prev_y = y\n","    return auc\n","\n","\n","def cal_roc_gauc(users, scores, labels):\n","    # weighted sum of individual auc\n","    df = pd.DataFrame({'user': users,\n","                       'score': scores,\n","                       'label': labels})\n","\n","    df_gb = df.groupby('user').agg(lambda x: x.tolist())\n","\n","    auc_ls = []  # collect auc for all users\n","    user_imp_ls = []\n","\n","    for row in df_gb.itertuples():\n","        auc = cal_roc_auc(row.score, row.label)\n","        if auc is None:\n","            pass\n","        else:\n","            auc_ls.append(auc)\n","            user_imp = len(row.label)\n","            user_imp_ls.append(user_imp)\n","\n","    total_imp = sum(user_imp_ls)\n","    weighted_auc_ls = [auc * user_imp / total_imp for auc, user_imp in zip(auc_ls, user_imp_ls)]\n","\n","    return sum(weighted_auc_ls)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fPgH5yepipnW"},"source":["class BatchLoader2:\n","    \"\"\"\n","    batch data loader by number of batches\n","    return: [[users], [items], np.array(item_seqs_matrix), [seq_lens], [labels]] in batch iterator\n","    \"\"\"\n","\n","    def __init__(self, data_df, num_batches):\n","\n","        self.data_df = data_df.reset_index(drop=True)  # df ['userId', 'itemId', 'label']\n","        self.batch_size = -int(-len(data_df) / num_batches)\n","        self.data_df['index'] = self.data_df.index\n","        self.data_df['batch'] = self.data_df['index'].apply(lambda x: int(x / self.batch_size) + 1)\n","        self.num_batches = self.data_df['batch'].max()\n","\n","    def get_batch(self, batch_id):\n","\n","        batch = self.data_df[self.data_df['batch'] == batch_id]\n","        users = batch['userId'].tolist()\n","        items = batch['itemId'].tolist()\n","        labels = batch['label'].tolist()\n","        seq_lens = batch['itemSeq'].apply(len).tolist()\n","\n","        item_seqs_matrix = np.zeros([len(batch), 30], np.int32)\n","\n","        i = 0\n","        for itemSeq in batch['itemSeq'].tolist():\n","            for j in range(len(itemSeq)):\n","                item_seqs_matrix[i][j] = itemSeq[j]  # convert list of itemSeq into a matrix with zero padding\n","            i += 1\n","\n","        return [users, items, item_seqs_matrix, seq_lens, labels]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"oVSMgDeANiLH"},"source":["class BatchLoaderYsoft:\n","    \"\"\"\n","    batch data loader by batch size with y_soft\n","    return: [[users], [items], np.array(item_seqs_matrix), [seq_lens], [labels], [labels_soft]] in batch iterator\n","    \"\"\"\n","\n","    def __init__(self, data_df, batch_size):\n","\n","        self.data_df = data_df.reset_index(drop=True)  # df ['userId', 'itemId', 'label']\n","        self.data_df['index'] = self.data_df.index\n","        self.data_df['batch'] = self.data_df['index'].apply(lambda x: int(x / batch_size) + 1)\n","        self.num_batches = self.data_df['batch'].max()\n","\n","    def get_batch(self, batch_id):\n","\n","        batch = self.data_df[self.data_df['batch'] == batch_id]\n","        users = batch['userId'].tolist()\n","        items = batch['itemId'].tolist()\n","        labels = batch['label'].tolist()\n","        labels_soft = batch['label_soft'].tolist()\n","        seq_lens = batch['itemSeq'].apply(len).tolist()\n","\n","        item_seqs_matrix = np.zeros([len(batch), 30], np.int32)\n","\n","        i = 0\n","        for itemSeq in batch['itemSeq'].tolist():\n","            for j in range(len(itemSeq)):\n","                item_seqs_matrix[i][j] = itemSeq[j]  # convert list of itemSeq into a matrix with zero padding\n","            i += 1\n","\n","        return [users, items, item_seqs_matrix, seq_lens, labels, labels_soft]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"SvNkS4mP7T7m"},"source":["def average_pooling(emb, seq_len):\n","    mask = tf.sequence_mask(seq_len, tf.shape(emb)[-2], dtype=tf.float32)  # [B, T]\n","    mask = tf.expand_dims(mask, -1)  # [B, T, 1]\n","    emb *= mask  # [B, T, H]\n","    sum_pool = tf.reduce_sum(emb, -2)  # [B, H]\n","    avg_pool = tf.div(sum_pool, tf.expand_dims(tf.cast(seq_len, tf.float32), -1) + 1e-8)  # [B, H]\n","    return avg_pool"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"WQjMHSPwCTgU"},"source":["def search_ckpt(search_alias, mode='last'):\n","    ckpt_ls = glob.glob(search_alias)\n","\n","    if mode == 'best logloss':\n","        metrics_ls = [float(ckpt.split('.ckpt')[0].split('_')[-1].split('TestLOGLOSS')[-1]) for ckpt in ckpt_ls]  # logloss\n","        selected_metrics_pos_ls = [i for i, x in enumerate(metrics_ls) if x == min(metrics_ls)]  # find all positions of the selected ckpts\n","    elif mode == 'best auc':\n","        metrics_ls = [float(ckpt.split('.ckpt')[0].split('_')[-2].split('TestAUC')[-1]) for ckpt in ckpt_ls]  # auc\n","        selected_metrics_pos_ls = [i for i, x in enumerate(metrics_ls) if x == max(metrics_ls)]  # find all positions of the selected ckpts\n","    else:  # mode == 'last'\n","        metrics_ls = [float(ckpt.split('.ckpt')[0].split('_')[-3].split('Epoch')[-1]) for ckpt in ckpt_ls]  # epoch no.\n","        selected_metrics_pos_ls = [i for i, x in enumerate(metrics_ls) if x == max(metrics_ls)]  # find all positions of the selected ckpts\n","    ckpt = ckpt_ls[max(selected_metrics_pos_ls)]  # get the full path of the last selected ckpt\n","\n","    ckpt = ckpt.split('.ckpt')[0]  # get the path name before .ckpt\n","    ckpt = ckpt + '.ckpt'  # get the path with .ckpt\n","    return ckpt"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"RASF-mMAHU4u"},"source":["def parquet_to_csv(path):\n","    save_path = path.split('.parquet')[0]+'.csv'\n","    pd.read_parquet(path).to_csv(save_path)\n","    logger.info('csv file saved at {}'.format(save_path))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"PguTj6gN2oj8"},"source":["### Dataset"]},{"cell_type":"code","metadata":{"id":"XQbXoMDO26pN"},"source":["def _gen_neg(num_items, pos_ls, num_neg):\n","    neg_ls = []\n","    for n in range(num_neg):  # generate num_neg\n","        neg = pos_ls[0]\n","        while neg in pos_ls:\n","            neg = random.randint(0, num_items - 1)\n","        neg_ls.append(neg)\n","    return neg_ls"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jnzORRSw2p_v"},"source":["def preprocess_sobazaar():\n","    # convert csv into pandas dataframe\n","    data_path = osp.join(args.path_bronze,'Sobazaar-hashID.csv.gz')\n","    save_path = osp.join(args.path_silver,'soba_4mth_2014_1neg_30seq_1.csv')\n","\n","    if not osp.exists(save_path):\n","        df = pd.read_csv(data_path)\n","        \n","        # preprocess\n","        df['date'] = df['Timestamp'].apply(lambda x: int(''.join(c for c in x.split('T')[0] if c.isdigit())))  # extract date and convert to int\n","        df['timestamp'] = df['Timestamp'].apply(lambda x: int(datetime.datetime.strptime(x.split('.')[0], '%Y-%m-%dT%H:%M:%S').timestamp()))\n","        df = df.drop(['Action', 'Timestamp'], axis=1)  # drop useless\n","        df.columns = ['itemId', 'userId', 'date', 'timestamp']  # rename\n","        df = df[['userId', 'itemId', 'date', 'timestamp']]  # switch columns\n","\n","        # remap id\n","        user_id = sorted(df['userId'].unique().tolist())  # sort column\n","        user_map = dict(zip(user_id, range(len(user_id))))  # create map, key is original id, value is mapped id starting from 0\n","        df['userId'] = df['userId'].map(lambda x: user_map[x])  # map key to value in df\n","\n","        item_id = sorted(df['itemId'].unique().tolist())  # sort column\n","        item_map = dict(zip(item_id, range(len(item_id))))  # create map, key is original id, value is mapped id starting from 0\n","        df['itemId'] = df['itemId'].map(lambda x: item_map[x])  # map key to value in df\n","\n","        logger.info('dataframe head - {}'.format(df.head(20)))\n","        logger.info('num_users: {}'.format(len(user_map)))  # 17126\n","        logger.info('num_items: {}'.format(len(item_map)))  # 24785\n","        logger.info('num_records: {}'.format(len(df)))  # 842660\n","\n","        # collect user history\n","        df_gb = df.groupby(['userId'])\n","        neg_lss = []\n","        num_neg = 1\n","        item_seqs = []\n","        max_len = 30\n","        count = 0\n","        for row in tqdm(df.itertuples(), total=df.shape[0]):\n","            user_df = df_gb.get_group(row.userId)\n","            user_history_df = user_df[user_df['timestamp'] <= row.timestamp].sort_values(['timestamp'], ascending=False).reset_index(drop=True)\n","            userHist = user_history_df['itemId'].unique().tolist()\n","            neg_lss.append(_gen_neg(len(item_map), userHist, num_neg))\n","\n","            user_history_df = user_history_df[user_history_df['timestamp'] < row.timestamp].sort_values(['timestamp'], ascending=False).reset_index(drop=True)\n","            item_seq_ls = user_history_df['itemId'][:max_len].tolist()\n","            itemSeq = '#'.join(str(i) for i in item_seq_ls)\n","            item_seqs.append(itemSeq)\n","\n","            count += 1\n","            if count % 100000 == 0:\n","                logger.info('done row {}'.format(count))\n","\n","        df['neg_itemId_ls'] = neg_lss\n","        df['itemSeq'] = item_seqs\n","\n","        users, itemseqs, items, labels, dates, timestamps = [], [], [], [], [], []\n","        for row in tqdm(df.itertuples(), total=df.shape[0]):\n","            users.append(row.userId)\n","            itemseqs.append(row.itemSeq)\n","            items.append(row.itemId)\n","            labels.append(1)  # positive samples have label 1\n","            dates.append(row.date)\n","            timestamps.append(row.timestamp)\n","            for j in range(num_neg):\n","                users.append(row.userId)\n","                itemseqs.append(row.itemSeq)\n","                items.append(row.neg_itemId_ls[j])\n","                labels.append(0)  # negative samples have label 0\n","                dates.append(row.date)\n","                timestamps.append(row.timestamp)\n","\n","        df = pd.DataFrame({'userId': users,\n","                        'itemSeq': itemseqs,\n","                        'itemId': items,\n","                        'label': labels,\n","                        'date': dates,\n","                        'timestamp': timestamps})\n","\n","        logger.info('dataframe head - {}'.format(df.head(20)))\n","        logger.info(len(df))  # 1685320\n","\n","        # save csv and pickle\n","        # ['userId', 'itemSeq', 'itemId', 'label', 'date', 'timestamp']\n","        df.to_csv(save_path, index=False)\n","        logger.info('processed data saved at {}'.format(save_path))\n","    else:\n","        logger.info('File already exists at {}'.format(save_path))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"x5we-S657T-E"},"source":["### Pretraining"]},{"cell_type":"code","metadata":{"id":"nxC7vY0i-DtE"},"source":["class EmbMLPnocate(object):\n","    \"\"\"\n","        Embedding&MLP base model without item category\n","    \"\"\"\n","    def __init__(self, hyperparams, train_config=None):\n","\n","        self.train_config = train_config\n","\n","        # create placeholder\n","        self.u = tf.placeholder(tf.int32, [None])  # [B]\n","        self.i = tf.placeholder(tf.int32, [None])  # [B]\n","        self.hist_i = tf.placeholder(tf.int32, [None, None])  # [B, T]\n","        self.hist_len = tf.placeholder(tf.int32, [None])  # [B]\n","        self.y = tf.placeholder(tf.float32, [None])  # [B]\n","        self.base_lr = tf.placeholder(tf.float32, [], name='base_lr')  # scalar\n","\n","        # -- create emb begin -------\n","        user_emb_w = tf.get_variable(\"user_emb_w\", [hyperparams['num_users'], hyperparams['user_embed_dim']])\n","        item_emb_w = tf.get_variable(\"item_emb_w\", [hyperparams['num_items'], hyperparams['item_embed_dim']])\n","        # -- create emb end -------\n","\n","        # -- create mlp begin ---\n","        concat_dim = hyperparams['user_embed_dim'] + hyperparams['item_embed_dim'] * 2\n","        with tf.variable_scope('fcn1'):\n","            fcn1_kernel = tf.get_variable(name='kernel', shape=[concat_dim, hyperparams['layers'][1]])\n","            fcn1_bias = tf.get_variable(name='bias', shape=[hyperparams['layers'][1]])\n","        with tf.variable_scope('fcn2'):\n","            fcn2_kernel = tf.get_variable(name='kernel', shape=[hyperparams['layers'][1], hyperparams['layers'][2]])\n","            fcn2_bias = tf.get_variable(name='bias', shape=[hyperparams['layers'][2]])\n","        with tf.variable_scope('fcn3'):\n","            fcn3_kernel = tf.get_variable(name='kernel', shape=[hyperparams['layers'][2], 1])\n","            fcn3_bias = tf.get_variable(name='bias', shape=[1])\n","        # -- create mlp end ---\n","\n","        # -- emb begin -------\n","        u_emb = tf.nn.embedding_lookup(user_emb_w, self.u)  # [B, H]\n","        i_emb = tf.nn.embedding_lookup(item_emb_w, self.i)  # [B, H]\n","        h_emb = tf.nn.embedding_lookup(item_emb_w, self.hist_i)  # [B, T, H]\n","        u_hist = average_pooling(h_emb, self.hist_len)  # [B, H]\n","        # -- emb end -------\n","\n","        # -- mlp begin -------\n","        fcn = tf.concat([u_emb, u_hist, i_emb], axis=-1)  # [B, H x 3]\n","        fcn_layer_1 = tf.nn.relu(tf.matmul(fcn, fcn1_kernel) + fcn1_bias)  # [B, l1]\n","        fcn_layer_2 = tf.nn.relu(tf.matmul(fcn_layer_1, fcn2_kernel) + fcn2_bias)  # [B, l2]\n","        fcn_layer_3 = tf.matmul(fcn_layer_2, fcn3_kernel) + fcn3_bias  # [B, 1]\n","        # -- mlp end -------\n","\n","        logits = tf.reshape(fcn_layer_3, [-1])  # [B]\n","        self.scores = tf.sigmoid(logits)  # [B]\n","\n","        # return same dimension as input tensors, let x = logits, z = labels, z * -log(sigmoid(x)) + (1 - z) * -log(1 - sigmoid(x))\n","        self.losses = tf.nn.sigmoid_cross_entropy_with_logits(logits=logits, labels=self.y)\n","        self.loss = tf.reduce_mean(self.losses)\n","\n","        # base_optimizer\n","        if train_config['base_optimizer'] == 'adam':\n","            base_optimizer = tf.train.AdamOptimizer(learning_rate=self.base_lr)\n","        elif train_config['base_optimizer'] == 'rmsprop':\n","            base_optimizer = tf.train.RMSPropOptimizer(learning_rate=self.base_lr)\n","        else:\n","            base_optimizer = tf.train.GradientDescentOptimizer(learning_rate=self.base_lr)\n","\n","        trainable_params = tf.trainable_variables()\n","\n","        # update base model\n","        update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n","        with tf.control_dependencies(update_ops):\n","            base_grads = tf.gradients(self.loss, trainable_params)  # return a list of gradients (A list of `sum(dy/dx)` for each x in `xs`)\n","            base_grads_tuples = zip(base_grads, trainable_params)\n","            self.train_base_op = base_optimizer.apply_gradients(base_grads_tuples)\n","\n","    def train_base(self, sess, batch):\n","        loss, _ = sess.run([self.loss, self.train_base_op], feed_dict={\n","            self.u: batch[0],\n","            self.i: batch[1],\n","            self.hist_i: batch[2],\n","            self.hist_len: batch[3],\n","            self.y: batch[4],\n","            self.base_lr: self.train_config['base_lr'],\n","        })\n","        return loss\n","\n","    def inference(self, sess, batch):\n","        scores, losses = sess.run([self.scores, self.losses], feed_dict={\n","            self.u: batch[0],\n","            self.i: batch[1],\n","            self.hist_i: batch[2],\n","            self.hist_len: batch[3],\n","            self.y: batch[4],\n","        })\n","        return scores, losses"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"rzeAyivB-DrL"},"source":["class Engine(object):\n","    \"\"\"\n","    Training epoch and test\n","    \"\"\"\n","\n","    def __init__(self, sess, model):\n","\n","        self.sess = sess\n","        self.model = model\n","\n","    def base_train_an_epoch(self, epoch_id, cur_set, train_config):\n","\n","        train_start_time = time.time()\n","\n","        if train_config['shuffle']:\n","            cur_set = cur_set.sample(frac=1)\n","\n","        cur_batch_loader = BatchLoader(cur_set, train_config['base_bs'])\n","\n","        base_loss_cur_sum = 0\n","\n","        for i in range(1, cur_batch_loader.num_batches + 1):\n","\n","            cur_batch = cur_batch_loader.get_batch(batch_id=i)\n","\n","            base_loss_cur = self.model.train_base(self.sess, cur_batch)  # sess.run\n","\n","            if (i - 1) % 100 == 0:\n","                print('[Epoch {} Batch {}] base_loss_cur {:.4f}, time elapsed {}'.format(epoch_id,\n","                                                                                         i,\n","                                                                                         base_loss_cur,\n","                                                                                         time.strftime('%H:%M:%S',\n","                                                                                                       time.gmtime(\n","                                                                                                           time.time() - train_start_time))))\n","\n","            base_loss_cur_sum += base_loss_cur\n","\n","        # epoch done, compute average loss\n","        base_loss_cur_avg = base_loss_cur_sum / cur_batch_loader.num_batches\n","\n","        return base_loss_cur_avg\n","\n","    def test(self, test_set, train_config):\n","\n","        test_batch_loader = BatchLoader(test_set, train_config['base_bs'])\n","\n","        scores, losses, labels = [], [], []\n","        for i in range(1, test_batch_loader.num_batches + 1):\n","            test_batch = test_batch_loader.get_batch(batch_id=i)\n","            batch_scores, batch_losses = self.model.inference(self.sess, test_batch)  # sees.run\n","            scores.extend(batch_scores.tolist())\n","            losses.extend(batch_losses.tolist())\n","            labels.extend(test_batch[4])\n","\n","        test_auc = cal_roc_auc(scores, labels)\n","        test_logloss = sum(losses) / len(losses)\n","\n","        return test_auc, test_logloss"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YECKrjGc-Dow"},"source":["def pretrain_sobazaar():\n","    # load data to df\n","    start_time = time.time()\n","\n","    load_path = osp.join(args.path_silver,'soba_4mth_2014_1neg_30seq_1.csv')\n","    data_df = pd.read_csv(load_path)\n","\n","    data_df['itemSeq'] = data_df['itemSeq'].fillna('')  # empty seq are NaN\n","    data_df['itemSeq'] = data_df['itemSeq'].apply(lambda x: [int(item) for item in x.split('#') if item != ''])\n","\n","    logger.info('Done loading data! time elapsed: {}'.format(time.strftime('%H:%M:%S', time.gmtime(time.time() - start_time))))\n","\n","    num_users = data_df['userId'].max() + 1\n","    num_items = data_df['itemId'].max() + 1\n","\n","    train_config = {'method': 'pretrain',\n","                    'dir_name': 'pretrain_train1-10_test11_10epoch',  # edit train test period range, number of epochs\n","                    'start_date': 20140901,  # overall train start date\n","                    'end_date': 20141231,  # overall train end date\n","                    'num_periods': 31,  # number of periods divided into\n","                    'train_start_period': 1,\n","                    'train_end_period': 10,\n","                    'test_period': 11,\n","                    'train_set_size': None,\n","                    'test_set_size': None,\n","\n","                    'base_optimizer': 'adam',  # base model optimizer: adam, rmsprop, sgd\n","                    'base_lr': None,  # base model learning rate\n","                    'base_bs': 256,  # base model batch size\n","                    'base_num_epochs': 10,  # base model number of epochs\n","                    'shuffle': True,  # whether to shuffle the dataset for each epoch\n","                    }\n","\n","    EmbMLPnocate_hyperparams = {'num_users': num_users,\n","                                'num_items': num_items,\n","                                'user_embed_dim': 8,\n","                                'item_embed_dim': 8,\n","                                'layers': [24, 12, 6, 1]  # input dim is user_embed_dim + item_embed_dim x 2\n","                                }\n","\n","    # sort train data into periods based on num_periods\n","    data_df = data_df[(data_df['date'] >= train_config['start_date']) & (data_df['date'] <= train_config['end_date'])]\n","    data_df = data_df.sort_values(['timestamp']).reset_index(drop=True)\n","    records_per_period = int(len(data_df) / train_config['num_periods'])\n","    data_df['index'] = data_df.index\n","    data_df['period'] = data_df['index'].apply(lambda x: int(x / records_per_period) + 1)\n","    data_df = data_df[data_df.period != train_config['num_periods'] + 1]  # delete last extra period\n","    period_df = data_df.groupby('period')['date'].agg(['count', 'min', 'max'])\n","    data_df = data_df.drop(['index', 'date', 'timestamp'], axis=1)\n","\n","    # build base model computation graph\n","    base_model = EmbMLPnocate(EmbMLPnocate_hyperparams, train_config=train_config)\n","\n","    # create session\n","    sess = tf.Session()\n","\n","    # create saver\n","    saver = tf.train.Saver(max_to_keep=80)\n","\n","    orig_dir_name = train_config['dir_name']\n","\n","    for base_lr in [1e-3]:\n","\n","        print('')\n","        print('base_lr', base_lr)\n","\n","        train_config['base_lr'] = base_lr\n","\n","        train_config['dir_name'] = orig_dir_name + '_' + str(base_lr)\n","        print('dir_name: ', train_config['dir_name'])\n","\n","        # create current and next set\n","        train_set = data_df[(data_df['period'] >= train_config['train_start_period']) &\n","                            (data_df['period'] <= train_config['train_end_period'])]\n","        test_set = data_df[data_df['period'] == train_config['test_period']]\n","        train_config['train_set_size'] = len(train_set)\n","        train_config['test_set_size'] = len(test_set)\n","        print('train set size', len(train_set), 'test set size', len(test_set))\n","\n","        # checkpoints directory\n","        checkpoints_dir = os.path.join('ckpts', train_config['dir_name'])\n","        if not os.path.exists(checkpoints_dir):\n","            os.makedirs(checkpoints_dir)\n","\n","        # write train_config to text file\n","        with open(os.path.join(checkpoints_dir, 'config.txt'), mode='w') as f:\n","            f.write('train_config: ' + str(train_config) + '\\n')\n","            f.write('\\n')\n","            f.write('EmbMLPnocate_hyperparams: ' + str(EmbMLPnocate_hyperparams) + '\\n')\n","\n","        sess.run([tf.global_variables_initializer(), tf.local_variables_initializer()])\n","\n","        # create an engine instance\n","        engine = Engine(sess, base_model)\n","\n","        train_start_time = time.time()\n","\n","        for epoch_id in range(1, train_config['base_num_epochs'] + 1):\n","\n","            print('Training Base Model Epoch {} Start!'.format(epoch_id))\n","\n","            base_loss_cur_avg = engine.base_train_an_epoch(epoch_id, train_set, train_config)\n","            print('Epoch {} Done! time elapsed: {}, base_loss_cur_avg {:.4f}'.format(\n","                epoch_id,\n","                time.strftime('%H:%M:%S', time.gmtime(time.time() - train_start_time)),\n","                base_loss_cur_avg))\n","\n","            test_auc, test_logloss = engine.test(test_set, train_config)\n","            print('test_auc {:.4f}, test_logloss {:.4f}'.format(\n","                test_auc,\n","                test_logloss))\n","            print('time elapsed {}'.format(time.strftime('%H:%M:%S', time.gmtime(time.time() - train_start_time))))\n","\n","            print('')\n","\n","            # save checkpoint\n","            checkpoint_alias = 'Epoch{}_TestAUC{:.4f}_TestLOGLOSS{:.4f}.ckpt'.format(\n","                epoch_id,\n","                test_auc,\n","                test_logloss)\n","            checkpoint_path = os.path.join(checkpoints_dir, checkpoint_alias)\n","            saver.save(sess, checkpoint_path)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hr7X8V0h-DmW"},"source":["### Incremental Update"]},{"cell_type":"code","metadata":{"id":"l41GttlhB0Ry"},"source":["def iu_sobazaar():\n","    # load data to df\n","    start_time = time.time()\n","\n","    load_path = osp.join(args.path_silver,'soba_4mth_2014_1neg_30seq_1.csv')\n","    data_df = pd.read_csv(load_path)\n","\n","    data_df['itemSeq'] = data_df['itemSeq'].fillna('')  # empty seq are NaN\n","    data_df['itemSeq'] = data_df['itemSeq'].apply(lambda x: [int(item) for item in x.split('#') if item != ''])\n","\n","    logger.info('Done loading data! time elapsed: {}'.format(time.strftime('%H:%M:%S', time.gmtime(time.time() - start_time))))\n","\n","    num_users = data_df['userId'].max() + 1\n","    num_items = data_df['itemId'].max() + 1\n","\n","    train_config = {'method': 'IU_by_period',\n","                    'dir_name': 'IU_train11-23_test24-30_1epoch',  # edit train test period, number of epochs\n","                    'pretrain_model': 'pretrain_train1-10_test11_10epoch_0.001',\n","                    'start_date': 20140901,  # overall train start date\n","                    'end_date': 20141231,  # overall train end date\n","                    'num_periods': 31,  # number of periods divided into\n","                    'train_start_period': 11,\n","                    'test_start_period': 24,\n","                    'cur_period': None,  # current incremental period\n","                    'next_period': None,  # next incremental period\n","                    'cur_set_size': None,  # current incremental dataset size\n","                    'next_set_size': None,  # next incremental dataset size\n","                    'period_alias': None,  # individual period directory alias to save ckpts\n","                    'restored_ckpt_mode': 'best auc',  # mode to search the checkpoint to restore, 'best auc', 'best gauc', 'last'\n","                    'restored_ckpt': None,  # configure in the for loop\n","\n","                    'base_optimizer': 'adam',  # base model optimizer: adam, rmsprop, sgd\n","                    'base_lr': None,  # base model learning rate\n","                    'base_bs': 256,  # base model batch size\n","                    'base_num_epochs': 1,  # base model number of epochs\n","                    'shuffle': True,  # whether to shuffle the dataset for each epoch\n","                    }\n","\n","    EmbMLPnocate_hyperparams = {'num_users': num_users,\n","                                'num_items': num_items,\n","                                'user_embed_dim': 8,\n","                                'item_embed_dim': 8,\n","                                'layers': [24, 12, 6, 1]  # input dim is user_embed_dim + item_embed_dim x 2\n","                                }\n","\n","    # sort train data into periods based on num_periods\n","    data_df = data_df[(data_df['date'] >= train_config['start_date']) & (data_df['date'] <= train_config['end_date'])]\n","    data_df = data_df.sort_values(['timestamp']).reset_index(drop=True)\n","    records_per_period = int(len(data_df) / train_config['num_periods'])\n","    data_df['index'] = data_df.index\n","    data_df['period'] = data_df['index'].apply(lambda x: int(x / records_per_period) + 1)\n","    data_df = data_df[data_df.period != train_config['num_periods'] + 1]  # delete last extra period\n","    period_df = data_df.groupby('period')['date'].agg(['count', 'min', 'max'])\n","    data_df = data_df.drop(['index', 'date', 'timestamp'], axis=1)\n","\n","    orig_dir_name = train_config['dir_name']\n","\n","    for base_lr in [1e-3]:\n","\n","        print('')\n","        print('base_lr', base_lr)\n","\n","        train_config['base_lr'] = base_lr\n","\n","        train_config['dir_name'] = orig_dir_name + '_' + str(base_lr)\n","        print('dir_name: ', train_config['dir_name'])\n","\n","        test_aucs = []\n","        test_loglosses = []\n","\n","        for i in range(train_config['train_start_period'], train_config['num_periods']):\n","\n","            # configure cur_period, next_period\n","            train_config['cur_period'] = i\n","            train_config['next_period'] = i + 1\n","            print('')\n","            print('current period: {}, next period: {}'.format(\n","                train_config['cur_period'],\n","                train_config['next_period']))\n","            print('')\n","\n","            # create current and next set\n","            cur_set = data_df[data_df['period'] == train_config['cur_period']]\n","            next_set = data_df[data_df['period'] == train_config['next_period']]\n","            train_config['cur_set_size'] = len(cur_set)\n","            train_config['next_set_size'] = len(next_set)\n","            print('current set size', len(cur_set), 'next set size', len(next_set))\n","\n","            train_config['period_alias'] = 'period' + str(i)\n","\n","            # checkpoints directory\n","            ckpts_dir = os.path.join('ckpts', train_config['dir_name'], train_config['period_alias'])\n","            if not os.path.exists(ckpts_dir):\n","                os.makedirs(ckpts_dir)\n","\n","            if i == train_config['train_start_period']:\n","                search_alias = os.path.join('ckpts', train_config['pretrain_model'], 'Epoch*')\n","                train_config['restored_ckpt'] = search_ckpt(search_alias, mode=train_config['restored_ckpt_mode'])\n","            else:\n","                prev_period_alias = 'period' + str(i - 1)\n","                search_alias = os.path.join('ckpts', train_config['dir_name'], prev_period_alias, 'Epoch*')\n","                train_config['restored_ckpt'] = search_ckpt(search_alias, mode=train_config['restored_ckpt_mode'])\n","            print('restored checkpoint: {}'.format(train_config['restored_ckpt']))\n","\n","            # write train_config to text file\n","            with open(os.path.join(ckpts_dir, 'config.txt'), mode='w') as f:\n","                f.write('train_config: ' + str(train_config) + '\\n')\n","                f.write('\\n')\n","                f.write('EmbMLPnocate_hyperparams: ' + str(EmbMLPnocate_hyperparams) + '\\n')\n","\n","            # build base model computation graph\n","            tf.reset_default_graph()\n","            base_model = EmbMLPnocate(EmbMLPnocate_hyperparams, train_config=train_config)\n","\n","            # create session\n","            with tf.Session() as sess:\n","                \n","                saver = tf.train.Saver()\n","                saver.restore(sess, train_config['restored_ckpt'])\n","                # create an engine instance with base_model\n","                engine = Engine(sess, base_model)\n","                train_start_time = time.time()\n","                max_auc = 0\n","                best_logloss = 0\n","\n","                for epoch_id in range(1, train_config['base_num_epochs'] + 1):\n","                    print('Training Base Model Epoch {} Start!'.format(epoch_id))\n","                    base_loss_cur_avg = engine.base_train_an_epoch(epoch_id, cur_set, train_config)\n","                    print('Epoch {} Done! time elapsed: {}, base_loss_cur_avg {:.4f}'.format(\n","                        epoch_id,\n","                        time.strftime('%H:%M:%S', time.gmtime(time.time() - train_start_time)),\n","                        base_loss_cur_avg))\n","                    cur_auc, cur_logloss = engine.test(cur_set, train_config)\n","                    next_auc, next_logloss = engine.test(next_set, train_config)\n","                    print('cur_auc {:.4f}, cur_logloss {:.4f}, next_auc {:.4f}, next_logloss {:.4f}'.format(\n","                        cur_auc,\n","                        cur_logloss,\n","                        next_auc,\n","                        next_logloss))\n","                    print('time elapsed {}'.format(time.strftime('%H:%M:%S', time.gmtime(time.time() - train_start_time))))\n","                    print('')\n","                    # save checkpoint\n","                    checkpoint_alias = 'Epoch{}_TestAUC{:.4f}_TestLOGLOSS{:.4f}.ckpt'.format(\n","                        epoch_id,\n","                        next_auc,\n","                        next_logloss)\n","                    checkpoint_path = os.path.join(ckpts_dir, checkpoint_alias)\n","                    saver.save(sess, checkpoint_path)\n","                    if next_auc > max_auc:\n","                        max_auc = next_auc\n","                        best_logloss = next_logloss\n","\n","                if i >= train_config['test_start_period']:\n","                    test_aucs.append(max_auc)\n","                    test_loglosses.append(best_logloss)\n","\n","            if i >= train_config['test_start_period']:\n","                average_auc = sum(test_aucs) / len(test_aucs)\n","                average_logloss = sum(test_loglosses) / len(test_loglosses)\n","                print('test aucs', test_aucs)\n","                print('average auc', average_auc)\n","                print('')\n","                print('test loglosses', test_loglosses)\n","                print('average logloss', average_logloss)\n","\n","                # write metrics to text file\n","                with open(os.path.join(ckpts_dir, 'test_metrics.txt'), mode='w') as f:\n","                    f.write('test_aucs: ' + str(test_aucs) + '\\n')\n","                    f.write('average_auc: ' + str(average_auc) + '\\n')\n","                    f.write('test_loglosses: ' + str(test_loglosses) + '\\n')\n","                    f.write('average_logloss: ' + str(average_logloss) + '\\n')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"V41KTzWZhjM-"},"source":["### ASMG-GRU Single"]},{"cell_type":"code","metadata":{"id":"4ozxcTAzhjKi"},"source":["def gru_parallel(name, prev_h, cur_x, param_dim, hidden_dim):\n","    \"\"\"\n","    perform one step of gru with tensor parallelization\n","    :param name: parameter name\n","    :param prev_h: [N, param_dim, 1, hidden_dim]\n","    :param cur_x: [N, param_dim, 1, 1]\n","    :param param_dim: dimension of (flattened) parameter\n","    :param hidden_dim: dimension of gru hidden state\n","    :return: cur_h, cur_y\n","    \"\"\"\n","    with tf.variable_scope('gru_' + name, reuse=tf.AUTO_REUSE):\n","        N = tf.shape(prev_h)[0]\n","\n","        # gate params\n","        u_z = tf.tile(tf.expand_dims(tf.get_variable(name='u_z', shape=[param_dim, 1, hidden_dim]), 0), [N, 1, 1, 1])\n","        w_z = tf.tile(tf.expand_dims(tf.get_variable(name='w_z', shape=[param_dim, hidden_dim, hidden_dim]), 0), [N, 1, 1, 1])\n","        z = tf.sigmoid(tf.matmul(cur_x, u_z) + tf.matmul(prev_h, w_z))  # [N, param_dim, 1, hidden_dim]\n","\n","        u_r = tf.tile(tf.expand_dims(tf.get_variable(name='u_r', shape=[param_dim, 1, hidden_dim]), 0), [N, 1, 1, 1])\n","        w_r = tf.tile(tf.expand_dims(tf.get_variable(name='w_r', shape=[param_dim, hidden_dim, hidden_dim]), 0), [N, 1, 1, 1])\n","        r = tf.sigmoid(tf.matmul(cur_x, u_r) + tf.matmul(prev_h, w_r))  # [N, param_dim, 1, hidden_dim]\n","\n","        u_g = tf.tile(tf.expand_dims(tf.get_variable(name='u_g', shape=[param_dim, 1, hidden_dim]), 0), [N, 1, 1, 1])\n","        w_g = tf.tile(tf.expand_dims(tf.get_variable(name='w_g', shape=[param_dim, hidden_dim, hidden_dim]), 0), [N, 1, 1, 1])\n","        cur_h_tilde = tf.tanh(tf.matmul(cur_x, u_g) + tf.matmul(prev_h * r, w_g))  # [N, param_dim, 1, hidden_dim]\n","\n","        cur_h = (1 - z) * prev_h + z * cur_h_tilde  # [N, param_dim, 1, hidden_dim]\n","\n","        # params to generate cur_y\n","        w_hy = tf.tile(tf.expand_dims(tf.get_variable(name='w_hy', shape=[param_dim, hidden_dim, 1]), 0), [N, 1, 1, 1])\n","        b_y = tf.tile(tf.expand_dims(tf.get_variable(name='b_y', shape=[param_dim, 1, 1]), 0), [N, 1, 1, 1])\n","\n","        # cur_y = tf.tanh(tf.matmul(cur_h, w_hy) + b_y)  # [N, param_dim, 1, 1]\n","        cur_y = tf.matmul(cur_h, w_hy) + b_y  # [N, param_dim, 1, 1]\n","\n","    return cur_h, cur_y\n","\n","\n","def lstm_parallel(name, prev_h, prev_c, cur_x, param_dim, hidden_dim):\n","    \"\"\"\n","    perform one step of lstm with tensor parallelization\n","    :param name: parameter name\n","    :param prev_h: [N, param_dim, 1, hidden_dim]\n","    :param prev_c: [N, param_dim, 1, hidden_dim]\n","    :param cur_x: [N, param_dim, 1, 1]\n","    :param param_dim: dimension of (flattened) parameter\n","    :param hidden_dim: dimension of lstm hidden state\n","    :return: cur_h, cur_c, cur_y\n","    \"\"\"\n","    with tf.variable_scope('lstm_' + name, reuse=tf.AUTO_REUSE):\n","        N = tf.shape(prev_h)[0]\n","\n","        # gate params\n","        u_i = tf.tile(tf.expand_dims(tf.get_variable(name='u_i', shape=[param_dim, 1, hidden_dim]), 0), [N, 1, 1, 1])\n","        w_i = tf.tile(tf.expand_dims(tf.get_variable(name='w_i', shape=[param_dim, hidden_dim, hidden_dim]), 0), [N, 1, 1, 1])\n","        i = tf.sigmoid(tf.matmul(cur_x, u_i) + tf.matmul(prev_h, w_i))  # [N, param_dim, 1, hidden_dim]\n","\n","        u_f = tf.tile(tf.expand_dims(tf.get_variable(name='u_f', shape=[param_dim, 1, hidden_dim]), 0), [N, 1, 1, 1])\n","        w_f = tf.tile(tf.expand_dims(tf.get_variable(name='w_f', shape=[param_dim, hidden_dim, hidden_dim]), 0), [N, 1, 1, 1])\n","        f = tf.sigmoid(tf.matmul(cur_x, u_f) + tf.matmul(prev_h, w_f))  # [N, param_dim, 1, hidden_dim]\n","\n","        u_o = tf.tile(tf.expand_dims(tf.get_variable(name='u_o', shape=[param_dim, 1, hidden_dim]), 0), [N, 1, 1, 1])\n","        w_o = tf.tile(tf.expand_dims(tf.get_variable(name='w_o', shape=[param_dim, hidden_dim, hidden_dim]), 0), [N, 1, 1, 1])\n","        o = tf.sigmoid(tf.matmul(cur_x, u_o) + tf.matmul(prev_h, w_o))  # [N, param_dim, 1, hidden_dim]\n","\n","        u_g = tf.tile(tf.expand_dims(tf.get_variable(name='u_g', shape=[param_dim, 1, hidden_dim]), 0), [N, 1, 1, 1])\n","        w_g = tf.tile(tf.expand_dims(tf.get_variable(name='w_g', shape=[param_dim, hidden_dim, hidden_dim]), 0), [N, 1, 1, 1])\n","        cur_c_tilde = tf.tanh(tf.matmul(cur_x, u_g) + tf.matmul(prev_h, w_g))  # [N, param_dim, 1, hidden_dim]\n","\n","        cur_c = tf.sigmoid(f * prev_c + i * cur_c_tilde)  # [N, param_dim, 1, hidden_dim]\n","        cur_h = tf.tanh(cur_c) * o  # [N, param_dim, 1, hidden_dim]\n","\n","        # params to generate cur_y\n","        w_hy = tf.tile(tf.expand_dims(tf.get_variable(name='w_hy', shape=[param_dim, hidden_dim, 1]), 0), [N, 1, 1, 1])\n","        b_y = tf.tile(tf.expand_dims(tf.get_variable(name='b_y', shape=[param_dim, 1, 1]), 0), [N, 1, 1, 1])\n","\n","        # cur_y = tf.tanh(tf.matmul(cur_h, w_hy) + b_y)  # [N, param_dim, 1, 1]\n","        cur_y = tf.matmul(cur_h, w_hy) + b_y  # [N, param_dim, 1, 1]\n","\n","    return cur_h, cur_c, cur_y\n","\n","\n","def vanilla_parallel(name, prev_h, cur_x, param_dim, hidden_dim):\n","    \"\"\"\n","    perform one step of vanilla rnn with tensor parallelization\n","    :param name: parameter name\n","    :param prev_h: [N, param_dim, 1, hidden_dim]\n","    :param cur_x: [N, param_dim, 1, 1]\n","    :param param_dim: dimension of (flattened) parameter\n","    :param hidden_dim: dimension of vanilla rnn hidden state\n","    :return: cur_h, cur_y\n","    \"\"\"\n","    with tf.variable_scope('vanilla_' + name, reuse=tf.AUTO_REUSE):\n","        N = tf.shape(prev_h)[0]\n","\n","        # params to generate cur_h\n","        w_hh = tf.tile(tf.expand_dims(tf.get_variable(name='w_hh', shape=[param_dim, hidden_dim, hidden_dim]), 0), [N, 1, 1, 1])\n","        w_xh = tf.tile(tf.expand_dims(tf.get_variable(name='w_xh', shape=[param_dim, 1, hidden_dim]), 0), [N, 1, 1, 1])\n","        b_h = tf.tile(tf.expand_dims(tf.get_variable(name='w_xh', shape=[param_dim, 1, hidden_dim]), 0), [N, 1, 1, 1])\n","\n","        cur_h = tf.tanh(tf.matmul(prev_h, w_hh) + tf.matmul(cur_x, w_xh) + b_h)  # [N, param_dim, 1, hidden_dim]\n","\n","        # params to generate cur_y\n","        w_hy = tf.tile(tf.expand_dims(tf.get_variable(name='w_hy', shape=[param_dim, hidden_dim, 1]), 0), [N, 1, 1, 1])\n","        b_y = tf.tile(tf.expand_dims(tf.get_variable(name='b_y', shape=[param_dim, 1, 1]), 0), [N, 1, 1, 1])\n","\n","        # cur_y = tf.tanh(tf.matmul(cur_h, w_hy) + b_y)  # [N, param_dim, 1, 1]\n","        cur_y = tf.matmul(cur_h, w_hy) + b_y  # [N, param_dim, 1, 1]\n","\n","    return cur_h, cur_y\n","\n","\n","rnn_dict = {'vanilla': vanilla_parallel,\n","            'gru': gru_parallel,\n","            'lstm': lstm_parallel}\n","\n","\n","def rnn_combine(name, param_ls, param_shape, seq_length, rnn_type, init_h, hidden_dim):\n","    \"\"\"\n","    perform rnn on sequence of parameters and output the final hs and cur_y\n","    :param name: param name\n","    :param param_ls: param list of seq_length\n","    :param param_shape: param shape [d1, d2, ...]\n","    :param seq_length: input sequence length\n","    :param rnn_type: type of rnn cell\n","    :param init_h: initial h [d1, d2, ..., hidden_dim]\n","    :param hidden_dim: rnn hidden dimensions\n","    :return: final hs [d1, d2, ..., hidden_dim, seq_length], final cur_y [d1, d2, ...]\n","    \"\"\"\n","    with tf.variable_scope('rnn_combine'):\n","        if 'fcn' in name:\n","            # N =1, param_dim = flat_dim\n","            if 'bias' in name:\n","                param_dim = param_shape[0]  # d1\n","            else:  # 'kernel'\n","                param_dim = param_shape[0] * param_shape[1]  # d1 x d2\n","            flat_param_ls = tf.reshape(param_ls, [-1, seq_length])  # [param_dim, seq_length]\n","            prev_h = tf.expand_dims(tf.expand_dims(tf.reshape(init_h, [-1, hidden_dim]), 1), 0)  # [1, param_dim, 1, hidden_dim]\n","            h_ls = []\n","            for i in range(seq_length):\n","                cur_x = flat_param_ls[:, i]  # [param_dim]\n","                cur_x = tf.expand_dims(tf.expand_dims(tf.expand_dims(cur_x, -1), -1), 0)  # [1, param_dim, 1, 1]\n","                cur_h, cur_y = rnn_dict[rnn_type](name, prev_h, cur_x, param_dim, hidden_dim)  # [1, param_dim, 1, hidden_dim], [1, param_dim, 1, 1]\n","                prev_h = cur_h\n","                h_ls.append(tf.reshape(cur_h, [param_dim, hidden_dim]))  # [param_dim, hidden_dim]\n","            hs = tf.stack(h_ls, -1)  # [param_dim, hidden_dim, seq_length]\n","            hs = tf.reshape(hs, param_shape + [hidden_dim, seq_length])  # [d1, d2, hidden_dim, seq_length] / [d1, hidden_dim, seq_length]\n","            cur_y = tf.reshape(cur_y, param_shape)  # [d1, d2] / [d1]\n","        else:  # 'emb' in name\n","            # N = num, param_dim = embed_dim\n","            param_dim = param_shape[1]\n","            prev_h = tf.expand_dims(init_h, 2)  # [N, param_dim, 1, hidden_dim]\n","            h_ls = []\n","            for i in range(seq_length):\n","                cur_x = param_ls[:, :, i]  # [N, param_dim]\n","                cur_x = tf.expand_dims(tf.expand_dims(cur_x, -1), -1)  # [N, param_dim, 1, 1]\n","                cur_h, cur_y = rnn_dict[rnn_type](name, prev_h, cur_x, param_dim, hidden_dim)  # [N, param_dim, 1, hidden_dim], [N, param_dim, 1, 1]\n","                prev_h = cur_h\n","                h_ls.append(tf.reshape(cur_h, param_shape + [hidden_dim]))  # [N, param_dim, hidden_dim]\n","            hs = tf.stack(h_ls, -1)  # [num, embed_dim, hidden_dim, seq_length]\n","            cur_y = tf.reshape(cur_y, param_shape)  # [num, embed_dim]\n","    return hs, cur_y\n","\n","\n","class ASMGrnnSingle(object):\n","\n","    def __init__(self, hyperparams, emb_ls_dict, mlp_ls_dict, init_h_dict, train_config=None):\n","\n","        self.train_config = train_config\n","\n","        # create placeholder\n","        self.u = tf.placeholder(tf.int32, [None])  # [B]\n","        self.i = tf.placeholder(tf.int32, [None])  # [B]\n","        self.hist_i = tf.placeholder(tf.int32, [None, None])  # [B, T]\n","        self.hist_len = tf.placeholder(tf.int32, [None])  # [B]\n","        self.y = tf.placeholder(tf.float32, [None])  # [B]\n","        self.meta_lr = tf.placeholder(tf.float32, [], name='meta_lr')  # scalar\n","\n","        # -- create emb_w_ls begin ----\n","        user_emb_w_ls = tf.convert_to_tensor(emb_ls_dict['user_emb_w'], tf.float32)  # [num_users, user_embed_dim, seq_length]\n","        item_emb_w_ls = tf.convert_to_tensor(emb_ls_dict['item_emb_w'], tf.float32)  # [num_items, item_embed_dim, seq_length]\n","        # -- create emb_w_ls end ----\n","\n","        # -- create mlp_ls begin ----\n","        fcn1_kernel_ls = tf.convert_to_tensor(mlp_ls_dict['fcn1/kernel'], tf.float32)  # [concat_dim, l1, seq_length]\n","        fcn1_bias_ls = tf.convert_to_tensor(mlp_ls_dict['fcn1/bias'], tf.float32)  # [l1, seq_length]\n","        fcn2_kernel_ls = tf.convert_to_tensor(mlp_ls_dict['fcn2/kernel'], tf.float32)  # [l1, l2, seq_length]\n","        fcn2_bias_ls = tf.convert_to_tensor(mlp_ls_dict['fcn2/bias'], tf.float32)  # [l2, seq_length]\n","        fcn3_kernel_ls = tf.convert_to_tensor(mlp_ls_dict['fcn3/kernel'], tf.float32)  # [l2, 1, seq_length]\n","        fcn3_bias_ls = tf.convert_to_tensor(mlp_ls_dict['fcn3/bias'], tf.float32)  # [1, seq_length]\n","        # -- create mlp_ls end ----\n","\n","        # -- generate emb_w begin ----\n","        with tf.variable_scope('meta_emb'):\n","            user_emb_w_init_h = tf.convert_to_tensor(init_h_dict['user_emb_w'], tf.float32)  # [num_users, user_embed_dim, emb_hidden_dim]\n","            self.user_emb_w_hs, user_emb_w = rnn_combine(name='user_emb_w',\n","                                                         param_ls=user_emb_w_ls,\n","                                                         param_shape=[hyperparams['num_users'], hyperparams['user_embed_dim']],\n","                                                         seq_length=train_config['seq_length'],\n","                                                         rnn_type=train_config['rnn_type'],\n","                                                         init_h=user_emb_w_init_h,\n","                                                         hidden_dim=train_config['emb_hidden_dim'])\n","            item_emb_w_init_h = tf.convert_to_tensor(init_h_dict['item_emb_w'], tf.float32)  # [num_items, item_embed_dim, emb_hidden_dim]\n","            self.item_emb_w_hs, item_emb_w = rnn_combine(name='item_emb_w',\n","                                                         param_ls=item_emb_w_ls,\n","                                                         param_shape=[hyperparams['num_items'], hyperparams['item_embed_dim']],\n","                                                         seq_length=train_config['seq_length'],\n","                                                         rnn_type=train_config['rnn_type'],\n","                                                         init_h=item_emb_w_init_h,\n","                                                         hidden_dim=train_config['emb_hidden_dim'])\n","        # -- generate emb_w end ----\n","\n","        # -- generate mlp begin ----\n","        with tf.variable_scope('meta_mlp'):\n","            concat_dim = hyperparams['user_embed_dim'] + hyperparams['item_embed_dim'] * 2\n","            fcn1_kernel_init_h = tf.convert_to_tensor(init_h_dict['fcn1_kernel'], tf.float32)  # [concat_dim, l1, mlp_hidden_dim]\n","            self.fcn1_kernel_hs, fcn1_kernel = rnn_combine(name='fcn1_kernel',\n","                                                           param_ls=fcn1_kernel_ls,\n","                                                           param_shape=[concat_dim, hyperparams['layers'][1]],\n","                                                           seq_length=train_config['seq_length'],\n","                                                           rnn_type=train_config['rnn_type'],\n","                                                           init_h=fcn1_kernel_init_h,\n","                                                           hidden_dim=train_config['mlp_hidden_dim'])\n","            fcn1_bias_init_h = tf.convert_to_tensor(init_h_dict['fcn1_bias'], tf.float32)  # [l1, mlp_hidden_dim]\n","            self.fcn1_bias_hs, fcn1_bias = rnn_combine(name='fcn1_bias',\n","                                                       param_ls=fcn1_bias_ls,\n","                                                       param_shape=[hyperparams['layers'][1]],\n","                                                       seq_length=train_config['seq_length'],\n","                                                       rnn_type=train_config['rnn_type'],\n","                                                       init_h=fcn1_bias_init_h,\n","                                                       hidden_dim=train_config['mlp_hidden_dim'])\n","            fcn2_kernel_init_h = tf.convert_to_tensor(init_h_dict['fcn2_kernel'], tf.float32)  # [l1, l2, mlp_hidden_dim]\n","            self.fcn2_kernel_hs, fcn2_kernel = rnn_combine(name='fcn2_kernel',\n","                                                           param_ls=fcn2_kernel_ls,\n","                                                           param_shape=[hyperparams['layers'][1], hyperparams['layers'][2]],\n","                                                           seq_length=train_config['seq_length'],\n","                                                           rnn_type=train_config['rnn_type'],\n","                                                           init_h=fcn2_kernel_init_h,\n","                                                           hidden_dim=train_config['mlp_hidden_dim'])\n","            fcn2_bias_init_h = tf.convert_to_tensor(init_h_dict['fcn2_bias'], tf.float32)  # [l2, mlp_hidden_dim]\n","            self.fcn2_bias_hs, fcn2_bias = rnn_combine(name='fcn2_bias',\n","                                                       param_ls=fcn2_bias_ls,\n","                                                       param_shape=[hyperparams['layers'][2]],\n","                                                       seq_length=train_config['seq_length'],\n","                                                       rnn_type=train_config['rnn_type'],\n","                                                       init_h=fcn2_bias_init_h,\n","                                                       hidden_dim=train_config['mlp_hidden_dim'])\n","            fcn3_kernel_init_h = tf.convert_to_tensor(init_h_dict['fcn3_kernel'], tf.float32)  # [l2, 1, mlp_hidden_dim]\n","            self.fcn3_kernel_hs, fcn3_kernel = rnn_combine(name='fcn3_kernel',\n","                                                           param_ls=fcn3_kernel_ls,\n","                                                           param_shape=[hyperparams['layers'][2], 1],\n","                                                           seq_length=train_config['seq_length'],\n","                                                           rnn_type=train_config['rnn_type'],\n","                                                           init_h=fcn3_kernel_init_h,\n","                                                           hidden_dim=train_config['mlp_hidden_dim'])\n","            fcn3_bias_init_h = tf.convert_to_tensor(init_h_dict['fcn3_bias'], tf.float32)  # [1, mlp_hidden_dim]\n","            self.fcn3_bias_hs, fcn3_bias = rnn_combine(name='fcn3_bias',\n","                                                       param_ls=fcn3_bias_ls,\n","                                                       param_shape=[1],\n","                                                       seq_length=train_config['seq_length'],\n","                                                       rnn_type=train_config['rnn_type'],\n","                                                       init_h=fcn3_bias_init_h,\n","                                                       hidden_dim=train_config['mlp_hidden_dim'])\n","        # -- generate mlp end ----\n","\n","        # -- emb begin -------\n","        u_emb = tf.nn.embedding_lookup(user_emb_w, self.u)  # [B, H]\n","        i_emb = tf.nn.embedding_lookup(item_emb_w, self.i)  # [B, H]\n","        h_emb = tf.nn.embedding_lookup(item_emb_w, self.hist_i)  # [B, T, H]\n","        u_hist = average_pooling(h_emb, self.hist_len)  # [B, H]\n","        # -- emb end -------\n","\n","        # -- mlp begin -------\n","        fcn = tf.concat([u_emb, u_hist, i_emb], axis=-1)  # [B, H x 3]\n","        fcn_layer_1 = tf.nn.relu(tf.matmul(fcn, fcn1_kernel) + fcn1_bias)  # [B, l1]\n","        fcn_layer_2 = tf.nn.relu(tf.matmul(fcn_layer_1, fcn2_kernel) + fcn2_bias)  # [B, l2]\n","        fcn_layer_3 = tf.matmul(fcn_layer_2, fcn3_kernel) + fcn3_bias  # [B, 1]\n","        # -- mlp end -------\n","\n","        logits = tf.reshape(fcn_layer_3, [-1])  # [B]\n","        self.scores = tf.sigmoid(logits)  # [B]\n","\n","        # return same dimension as input tensors, let x = logits, z = labels, z * -log(sigmoid(x)) + (1 - z) * -log(1 - sigmoid(x))\n","        self.losses = tf.nn.sigmoid_cross_entropy_with_logits(logits=logits, labels=self.y)\n","        self.loss = tf.reduce_mean(self.losses)\n","\n","        # meta_optimizer\n","        if train_config['meta_optimizer'] == 'adam':\n","            meta_optimizer = tf.train.AdamOptimizer(learning_rate=self.meta_lr)\n","        elif train_config['meta_optimizer'] == 'rmsprop':\n","            meta_optimizer = tf.train.RMSPropOptimizer(learning_rate=self.meta_lr)\n","        else:\n","            meta_optimizer = tf.train.GradientDescentOptimizer(learning_rate=self.meta_lr)\n","\n","        trainable_params = tf.trainable_variables()\n","\n","        # update meta generator\n","        update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n","        with tf.control_dependencies(update_ops):\n","            meta_grads = tf.gradients(self.loss, trainable_params)\n","            meta_grads_tuples = zip(meta_grads, trainable_params)\n","            with tf.variable_scope('meta_opt'):\n","                self.train_meta_op = meta_optimizer.apply_gradients(meta_grads_tuples)\n","\n","    def get_h_dict(self, sess):\n","        user_emb_w_hs, item_emb_w_hs, \\\n","        fcn1_kernel_hs, fcn1_bias_hs, fcn2_kernel_hs, fcn2_bias_hs, \\\n","        fcn3_kernel_hs, fcn3_bias_hs = sess.run([\n","            self.user_emb_w_hs, self.item_emb_w_hs,\n","            self.fcn1_kernel_hs, self.fcn1_bias_hs, self.fcn2_kernel_hs, self.fcn2_bias_hs,\n","            self.fcn3_kernel_hs, self.fcn3_bias_hs])\n","        h_dict = {'user_emb_w': user_emb_w_hs,\n","                  'item_emb_w': item_emb_w_hs,\n","                  'fcn1_kernel': fcn1_kernel_hs,\n","                  'fcn1_bias': fcn1_bias_hs,\n","                  'fcn2_kernel': fcn2_kernel_hs,\n","                  'fcn2_bias': fcn2_bias_hs,\n","                  'fcn3_kernel': fcn3_kernel_hs,\n","                  'fcn3_bias': fcn3_bias_hs}\n","        return h_dict\n","\n","    def train_meta(self, sess, batch):\n","        loss, _, = sess.run([self.loss, self.train_meta_op], feed_dict={\n","            self.u: batch[0],\n","            self.i: batch[1],\n","            self.hist_i: batch[2],\n","            self.hist_len: batch[3],\n","            self.y: batch[4],\n","            self.meta_lr: self.train_config['meta_lr'],\n","        })\n","        return loss\n","\n","    def inference(self, sess, batch):\n","        scores, losses = sess.run([self.scores, self.losses], feed_dict={\n","            self.u: batch[0],\n","            self.i: batch[1],\n","            self.hist_i: batch[2],\n","            self.hist_len: batch[3],\n","            self.y: batch[4],\n","        })\n","        return scores, losses"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"uo-MDWvgiy0q"},"source":["class ASMGEngine(object):\n","    \"\"\"\n","    Training epoch and test\n","    \"\"\"\n","\n","    def __init__(self, sess, model):\n","\n","        self.sess = sess\n","        self.model = model\n","\n","    def meta_train_an_epoch(self, epoch_id, next_set, train_config):\n","\n","        train_start_time = time.time()\n","\n","        if train_config['shuffle']:\n","            next_set = next_set.sample(frac=1)\n","\n","        next_batch_loader = BatchLoader(next_set, train_config['meta_bs'])\n","\n","        meta_loss_next_sum = 0\n","\n","        for i in range(1, next_batch_loader.num_batches + 1):\n","\n","            next_batch = next_batch_loader.get_batch(batch_id=i)\n","\n","            meta_loss_next = self.model.train_meta(self.sess, next_batch)  # sess.run\n","\n","            if (i - 1) % 100 == 0:\n","                print('[Epoch {} Batch {}] meta_loss_next {:.4f}, time elapsed {}'.format(epoch_id,\n","                                                                                          i,\n","                                                                                          meta_loss_next,\n","                                                                                          time.strftime('%H:%M:%S', time.gmtime(time.time() - train_start_time))))\n","\n","                # test the performance of output serving model at last period (can comment out if not needed)\n","                next_auc, next_logloss = self.test(next_set, train_config)\n","                print('next_auc {:.4f}, next_logloss {:.4f}'.format(\n","                    next_auc,\n","                    next_logloss))\n","                print('time elapsed {}'.format(time.strftime('%H:%M:%S', time.gmtime(time.time() - train_start_time))))\n","\n","                print('')\n","\n","            meta_loss_next_sum += meta_loss_next\n","\n","        # epoch done, compute average loss\n","        meta_loss_next_avg = meta_loss_next_sum / next_batch_loader.num_batches\n","\n","        return meta_loss_next_avg\n","\n","    def test(self, test_set, train_config):\n","\n","        test_batch_loader = BatchLoader(test_set, train_config['meta_bs'])\n","\n","        scores, losses, labels = [], [], []\n","        for i in range(1, test_batch_loader.num_batches + 1):\n","            test_batch = test_batch_loader.get_batch(batch_id=i)\n","            batch_scores, batch_losses = self.model.inference(self.sess, test_batch)  # sees.run\n","            scores.extend(batch_scores.tolist())\n","            losses.extend(batch_losses.tolist())\n","            labels.extend(test_batch[4])\n","\n","        test_auc = cal_roc_auc(scores, labels)\n","        test_logloss = sum(losses) / len(losses)\n","\n","        return test_auc, test_logloss"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"LoPiNDhho2zl","executionInfo":{"status":"ok","timestamp":1636714265199,"user_tz":-330,"elapsed":1237,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}}},"source":["def asmg_single_sobazaar():\n","    # load data to df\n","    start_time = time.time()\n","\n","    load_path = osp.join(args.path_silver,'soba_4mth_2014_1neg_30seq_1.csv')\n","    data_df = pd.read_csv(load_path)\n","\n","    data_df['itemSeq'] = data_df['itemSeq'].fillna('')  # empty seq are NaN\n","    data_df['itemSeq'] = data_df['itemSeq'].apply(lambda x: [int(item) for item in x.split('#') if item != ''])\n","\n","    logger.info('Done loading data! time elapsed: {}'.format(time.strftime('%H:%M:%S', time.gmtime(time.time() - start_time))))\n","\n","    num_users = data_df['userId'].max() + 1\n","    num_items = data_df['itemId'].max() + 1\n","\n","    train_config = {'method': 'ASMGgru_single_by_period',\n","                    'dir_name': 'ASMGgru_single_train11-23_test24-30_4emb_4mlp_1epoch',  # edit train test period, rnn hidden size for emb and mlp, number of epochs\n","                    'niu_dir_name': 'NIU_train11-23_test24-30_1epoch_0.001',  # input model sequence directory\n","                    'start_date': 20140901,  # overall train start date\n","                    'end_date': 20141231,  # overall train end date\n","                    'num_periods': 31,  # number of periods divided into\n","                    'train_start_period': 11,\n","                    'test_start_period': 24,\n","                    'cur_period': None,  # current incremental period\n","                    'next_period': None,  # next incremental period\n","                    'next_set_size': None,  # next incremental dataset size\n","                    'period_alias': None,  # individual period directory alias to save ckpts\n","                    'restored_ckpt_mode': 'best auc',  # mode to search the ckpt to restore: 'best auc', 'best logloss', 'last'\n","                    'restored_ckpt': None,  # restored meta generator checkpoint\n","\n","                    'seq_length': None,   # length of input model sequence\n","                    'rnn_type': 'gru',  # type of rnn cell: vanilla, gru\n","                    'emb_hidden_dim': 4,  # rnn hidden size for embedding layers parameters\n","                    'mlp_hidden_dim': 4,  # rnn hidden size for MLP layers parameters\n","                    'test_stop_train': False,  # whether to stop updating meta generator during test periods\n","\n","                    'meta_optimizer': 'adam',  # meta generator optimizer: adam, rmsprop, sgd\n","                    'meta_lr': None,  # meta generator learning rate\n","                    'meta_bs': 256,  # meta generator batch size\n","                    'meta_num_epochs': 1,  # meta generator number of epochs\n","                    'shuffle': True,  # whether to shuffle the dataset for each epoch\n","                    }\n","\n","    EmbMLPnocate_hyperparams = {'num_users': num_users,\n","                                'num_items': num_items,\n","                                'user_embed_dim': 8,\n","                                'item_embed_dim': 8,\n","                                'layers': [24, 12, 6, 1]  # input dim is user_embed_dim + item_embed_dim x 2\n","                                }\n","\n","    def collect_params():\n","        \"\"\"\n","        collect list of parameters for input model sequence\n","        :return: emb_ls_dict, mlp_ls_dict\n","        \"\"\"\n","\n","        collect_params_start_time = time.time()\n","\n","        emb_ls = ['user_emb_w', 'item_emb_w']\n","        mlp_ls = ['fcn1/kernel', 'fcn2/kernel', 'fcn3/kernel', 'fcn3/bias', 'fcn1/bias', 'fcn2/bias']\n","\n","        # collect input model sequence from niu_dir\n","        emb_dict_ls = []\n","        mlp_dict_ls = []\n","        for prev_num in reversed(range(train_config['seq_length'])):\n","            period_alias = 'period' + str(i - prev_num)\n","            # alias = os.path.join('./IU/ckpts', train_config['niu_dir_name'], period_alias, 'Epoch*')\n","            alias = os.path.join(args.iu_ckpts_path, 'Epoch*')\n","            restored_ckpt = search_ckpt(alias, mode=train_config['restored_ckpt_mode'])\n","            print('restored model {}: {}'.format(i - prev_num, restored_ckpt))\n","            emb_dict = {name: tf.train.load_checkpoint(restored_ckpt).get_tensor(name)\n","                        for name, _ in tf.train.list_variables(restored_ckpt) if name in emb_ls}\n","            mlp_dict = {name: tf.train.load_checkpoint(restored_ckpt).get_tensor(name)\n","                        for name, _ in tf.train.list_variables(restored_ckpt) if name in mlp_ls}\n","            emb_dict_ls.append(emb_dict)\n","            mlp_dict_ls.append(mlp_dict)\n","\n","        # concat sequence for different parameters on the last axis\n","        emb_ls_dict_ = {}\n","        for k in emb_dict_ls[0].keys():\n","            for emb_dict in emb_dict_ls:\n","                if k not in emb_ls_dict_.keys():\n","                    emb_ls_dict_[k] = np.expand_dims(emb_dict[k], axis=-1)\n","                else:\n","                    emb_ls_dict_[k] = np.concatenate((emb_ls_dict_[k], np.expand_dims(emb_dict[k], axis=-1)), axis=-1)\n","\n","        mlp_ls_dict_ = {}\n","        for k in mlp_dict_ls[0].keys():\n","            for mlp_dict in mlp_dict_ls:\n","                if k not in mlp_ls_dict_.keys():\n","                    mlp_ls_dict_[k] = np.expand_dims(mlp_dict[k], axis=-1)\n","                else:\n","                    mlp_ls_dict_[k] = np.concatenate((mlp_ls_dict_[k], np.expand_dims(mlp_dict[k], axis=-1)), axis=-1)\n","\n","        # check that the shapes are correct\n","        for k in emb_ls_dict_.keys():\n","            print(k, np.shape(emb_ls_dict_[k]))\n","        for k in mlp_ls_dict_.keys():\n","            print(k, np.shape(mlp_ls_dict_[k]))\n","\n","        print('collect params time elapsed: {}'.format(\n","            time.strftime('%H:%M:%S', time.gmtime(time.time() - collect_params_start_time))))\n","\n","        return emb_ls_dict_, mlp_ls_dict_\n","\n","\n","    def collect_init_h():\n","        \"\"\"\n","        collect previously trained initial hidden state\n","        :return: init_h_dict\n","        \"\"\"\n","        collect_init_h_start_time = time.time()\n","        path = os.path.join('ckpts', train_config['dir_name'], prev_period_alias, 'h_dict.pkl')\n","        with open(path, mode='r') as fh:\n","            h_dict = pickle.load(fh)\n","        init_h_dict_ = {}\n","        for k in h_dict.keys():\n","            init_h_dict_[k] = h_dict[k][..., 0]\n","        print('collect init h time elapsed: {}'.format(\n","            time.strftime('%H:%M:%S', time.gmtime(time.time() - collect_init_h_start_time))))\n","\n","        return init_h_dict_\n","\n","    # sort train data into periods based on num_periods\n","    data_df = data_df[(data_df['date'] >= train_config['start_date']) & (data_df['date'] <= train_config['end_date'])]\n","    data_df = data_df.sort_values(['timestamp']).reset_index(drop=True)\n","    records_per_period = int(len(data_df) / train_config['num_periods'])\n","    data_df['index'] = data_df.index\n","    data_df['period'] = data_df['index'].apply(lambda x: int(x / records_per_period) + 1)\n","    data_df = data_df[data_df.period != train_config['num_periods'] + 1]  # delete last extra period\n","    period_df = data_df.groupby('period')['date'].agg(['count', 'min', 'max'])\n","    data_df = data_df.drop(['index', 'date', 'timestamp'], axis=1)\n","\n","    orig_dir_name = train_config['dir_name']\n","\n","    for seq_length in [3]:\n","\n","        for meta_lr in [1e-2]:\n","\n","            print('')\n","            print('seq_length', seq_length, 'meta_lr', meta_lr)\n","\n","            train_config['seq_length'] = seq_length\n","            train_config['meta_lr'] = meta_lr\n","\n","            train_config['dir_name'] = orig_dir_name + '_' + str(seq_length) + '_' + str(meta_lr)\n","            print('dir_name: ', train_config['dir_name'])\n","\n","            test_aucs = []\n","            test_loglosses = []\n","\n","            start_period = train_config['train_start_period'] + seq_length - 1\n","\n","            for i in range(start_period, train_config['num_periods']):\n","\n","                # configure cur_period, next_period\n","                train_config['cur_period'] = i\n","                train_config['next_period'] = i + 1\n","                print('')\n","                print('current period: {}, next period: {}'.format(\n","                    train_config['cur_period'],\n","                    train_config['next_period']))\n","                print('')\n","\n","                # create next set\n","                next_set = data_df[data_df['period'] == train_config['next_period']]\n","                train_config['next_set_size'] = len(next_set)\n","                print('next set size', len(next_set))\n","\n","                train_config['period_alias'] = 'period' + str(i)\n","\n","                # checkpoints directory\n","                ckpts_dir = os.path.join('ckpts', train_config['dir_name'], train_config['period_alias'])\n","                if not os.path.exists(ckpts_dir):\n","                    os.makedirs(ckpts_dir)\n","\n","                if i == start_period:\n","                    train_config['restored_ckpt'] = None\n","                else:\n","                    prev_period_alias = 'period' + str(i - 1)\n","                    search_alias = os.path.join('ckpts', train_config['dir_name'], prev_period_alias, 'Epoch*')\n","                    train_config['restored_ckpt'] = search_ckpt(search_alias, mode=train_config['restored_ckpt_mode'])\n","                print('restored checkpoint: {}'.format(train_config['restored_ckpt']))\n","\n","                # write train_config to text file\n","                with open(os.path.join(ckpts_dir, 'config.txt'), mode='w') as f:\n","                    f.write('train_config: ' + str(train_config) + '\\n')\n","                    f.write('\\n')\n","                    f.write('EmbMLPnocate_hyperparams: ' + str(EmbMLPnocate_hyperparams) + '\\n')\n","                    f.write('\\n')\n","                    f.write('period_df:' + '\\n')\n","                    f.write(str(period_df))\n","\n","                # collect list of parameters for input model sequence\n","                emb_ls_dict, mlp_ls_dict = collect_params()\n","\n","                # collect previously trained initial hidden state\n","                if i == start_period:\n","                    init_h_dict = {'user_emb_w': np.zeros((EmbMLPnocate_hyperparams['num_users'],\n","                                                        EmbMLPnocate_hyperparams['user_embed_dim'],\n","                                                        train_config['emb_hidden_dim'])),\n","                                'item_emb_w': np.zeros((EmbMLPnocate_hyperparams['num_items'],\n","                                                        EmbMLPnocate_hyperparams['item_embed_dim'],\n","                                                        train_config['emb_hidden_dim'])),\n","                                'fcn1_kernel': np.zeros((EmbMLPnocate_hyperparams['layers'][0],\n","                                                            EmbMLPnocate_hyperparams['layers'][1],\n","                                                            train_config['mlp_hidden_dim'])),\n","                                'fcn1_bias': np.zeros((EmbMLPnocate_hyperparams['layers'][1],\n","                                                        train_config['mlp_hidden_dim'])),\n","                                'fcn2_kernel': np.zeros((EmbMLPnocate_hyperparams['layers'][1],\n","                                                            EmbMLPnocate_hyperparams['layers'][2],\n","                                                            train_config['mlp_hidden_dim'])),\n","                                'fcn2_bias': np.zeros((EmbMLPnocate_hyperparams['layers'][2],\n","                                                        train_config['mlp_hidden_dim'])),\n","                                'fcn3_kernel': np.zeros((EmbMLPnocate_hyperparams['layers'][2],\n","                                                            EmbMLPnocate_hyperparams['layers'][3],\n","                                                            train_config['mlp_hidden_dim'])),\n","                                'fcn3_bias': np.zeros((EmbMLPnocate_hyperparams['layers'][3],\n","                                                        train_config['mlp_hidden_dim']))}\n","                else:\n","                    init_h_dict = collect_init_h()\n","\n","                # build asmg model computation graph\n","                tf.reset_default_graph()\n","                asmg_model = ASMGrnnSingle(EmbMLPnocate_hyperparams, emb_ls_dict, mlp_ls_dict, init_h_dict, train_config=train_config)\n","\n","                # create session\n","                with tf.Session() as sess:\n","\n","                    # restore meta generator\n","                    if i == start_period:\n","                        # print([var.name for var in tf.global_variables()])  # check graph variables\n","                        sess.run([tf.global_variables_initializer(), tf.local_variables_initializer()])\n","                    else:\n","                        restorer = tf.train.Saver()\n","                        restorer.restore(sess, train_config['restored_ckpt'])\n","                    saver = tf.train.Saver()\n","\n","                    # test and then train meta generator\n","                    # create an engine instance with asmg_model\n","                    engine = ASMGEngine(sess, asmg_model)\n","\n","                    test_start_time = time.time()\n","                    print('Testing Meta Generator Start!')\n","                    next_auc, next_logloss = engine.test(next_set, train_config)\n","                    print('Done! time elapsed: {}, next_auc {:.4f}, next_logloss {:.4f}'.format(\n","                        time.strftime('%H:%M:%S', time.gmtime(time.time() - test_start_time)),\n","                        next_auc,\n","                        next_logloss))\n","                    print('')\n","\n","                    if i >= train_config['test_start_period']:\n","                        test_aucs.append(next_auc)\n","                        test_loglosses.append(next_logloss)\n","\n","                    if i < train_config['test_start_period'] or not train_config['test_stop_train']:\n","\n","                        train_start_time = time.time()\n","\n","                        for epoch_id in range(1, train_config['meta_num_epochs'] + 1):\n","\n","                            print('Training Meta Generator Epoch {} Start!'.format(epoch_id))\n","\n","                            meta_loss_next_avg = engine.meta_train_an_epoch(epoch_id, next_set, train_config)\n","                            print('Epoch {} Done! time elapsed: {}, meta_loss_next_avg {:.4f}'.format(\n","                                epoch_id,\n","                                time.strftime('%H:%M:%S', time.gmtime(time.time() - train_start_time)),\n","                                meta_loss_next_avg\n","                            ))\n","\n","                            next_auc, next_logloss = engine.test(next_set, train_config)\n","                            print('next_auc {:.4f}, next_logloss {:.4f}'.format(\n","                                next_auc,\n","                                next_logloss))\n","                            print('time elapsed {}'.format(time.strftime('%H:%M:%S', time.gmtime(time.time() - train_start_time))))\n","\n","                            print('')\n","\n","                            # save checkpoint\n","                            checkpoint_alias = 'Epoch{}_TestAUC{:.4f}_TestLOGLOSS{:.4f}.ckpt'.format(\n","                                epoch_id,\n","                                next_auc,\n","                                next_logloss)\n","                            checkpoint_path = os.path.join(ckpts_dir, checkpoint_alias)\n","                            saver.save(sess, checkpoint_path)\n","\n","                        # save h_dict\n","                        h_dict_ = asmg_model.get_h_dict(sess)\n","                        with open(os.path.join(ckpts_dir, 'h_dict.pkl'), mode='w') as fh:\n","                            pickle.dump(h_dict_, fh)\n","\n","                    else:\n","                        # save checkpoint\n","                        checkpoint_alias = 'EpochNA_TestAUC{:.4f}_TestLOGLOSS{:.4f}.ckpt'.format(\n","                            next_auc,\n","                            next_logloss)\n","                        checkpoint_path = os.path.join(ckpts_dir, checkpoint_alias)\n","                        saver.save(sess, checkpoint_path)\n","\n","                        # save h_dict\n","                        h_dict_ = asmg_model.get_h_dict(sess)\n","                        with open(os.path.join(ckpts_dir, 'h_dict.pkl'), mode='w') as fh:\n","                            pickle.dump(h_dict_, fh)\n","\n","                if i >= train_config['test_start_period']:\n","                    average_auc = sum(test_aucs) / len(test_aucs)\n","                    average_logloss = sum(test_loglosses) / len(test_loglosses)\n","                    print('test aucs', test_aucs)\n","                    print('average auc', average_auc)\n","                    print('')\n","                    print('test loglosses', test_loglosses)\n","                    print('average logloss', average_logloss)\n","\n","                    # write metrics to text file\n","                    with open(os.path.join(ckpts_dir, 'test_metrics.txt'), mode='w') as f:\n","                        f.write('test_aucs: ' + str(test_aucs) + '\\n')\n","                        f.write('average_auc: ' + str(average_auc) + '\\n')\n","                        f.write('test_loglosses: ' + str(test_loglosses) + '\\n')\n","                        f.write('average_logloss: ' + str(average_logloss) + '\\n')"],"execution_count":31,"outputs":[]},{"cell_type":"code","metadata":{"id":"pmtxt7p-xzYM","executionInfo":{"status":"ok","timestamp":1636714477992,"user_tz":-330,"elapsed":1231,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}}},"source":["def asmg_single_sobazaar():\n","    # load data to df\n","    start_time = time.time()\n","\n","    load_path = osp.join(args.path_silver,'soba_4mth_2014_1neg_30seq_1.csv')\n","    data_df = pd.read_csv(load_path)\n","\n","    data_df['itemSeq'] = data_df['itemSeq'].fillna('')  # empty seq are NaN\n","    data_df['itemSeq'] = data_df['itemSeq'].apply(lambda x: [int(item) for item in x.split('#') if item != ''])\n","\n","    logger.info('Done loading data! time elapsed: {}'.format(time.strftime('%H:%M:%S', time.gmtime(time.time() - start_time))))\n","\n","    num_users = data_df['userId'].max() + 1\n","    num_items = data_df['itemId'].max() + 1\n","\n","    train_config = {'method': 'ASMGgru_single_by_period',\n","                    'dir_name': 'ASMGgru_single_train11-23_test24-30_4emb_4mlp_1epoch',  # edit train test period, rnn hidden size for emb and mlp, number of epochs\n","                    'niu_dir_name': 'NIU_train11-23_test24-30_1epoch_0.001',  # input model sequence directory\n","                    'start_date': 20140901,  # overall train start date\n","                    'end_date': 20141231,  # overall train end date\n","                    'num_periods': 31,  # number of periods divided into\n","                    'train_start_period': 11,\n","                    'test_start_period': 24,\n","                    'cur_period': None,  # current incremental period\n","                    'next_period': None,  # next incremental period\n","                    'next_set_size': None,  # next incremental dataset size\n","                    'period_alias': None,  # individual period directory alias to save ckpts\n","                    'restored_ckpt_mode': 'best auc',  # mode to search the ckpt to restore: 'best auc', 'best logloss', 'last'\n","                    'restored_ckpt': None,  # restored meta generator checkpoint\n","\n","                    'seq_length': None,   # length of input model sequence\n","                    'rnn_type': 'gru',  # type of rnn cell: vanilla, gru\n","                    'emb_hidden_dim': 4,  # rnn hidden size for embedding layers parameters\n","                    'mlp_hidden_dim': 4,  # rnn hidden size for MLP layers parameters\n","                    'test_stop_train': False,  # whether to stop updating meta generator during test periods\n","\n","                    'meta_optimizer': 'adam',  # meta generator optimizer: adam, rmsprop, sgd\n","                    'meta_lr': None,  # meta generator learning rate\n","                    'meta_bs': 256,  # meta generator batch size\n","                    'meta_num_epochs': 1,  # meta generator number of epochs\n","                    'shuffle': True,  # whether to shuffle the dataset for each epoch\n","                    }\n","\n","    EmbMLPnocate_hyperparams = {'num_users': num_users,\n","                                'num_items': num_items,\n","                                'user_embed_dim': 8,\n","                                'item_embed_dim': 8,\n","                                'layers': [24, 12, 6, 1]  # input dim is user_embed_dim + item_embed_dim x 2\n","                                }\n","\n","    def collect_params():\n","        \"\"\"\n","        collect list of parameters for input model sequence\n","        :return: emb_ls_dict, mlp_ls_dict\n","        \"\"\"\n","\n","        collect_params_start_time = time.time()\n","\n","        emb_ls = ['user_emb_w', 'item_emb_w']\n","        mlp_ls = ['fcn1/kernel', 'fcn2/kernel', 'fcn3/kernel', 'fcn3/bias', 'fcn1/bias', 'fcn2/bias']\n","\n","        # collect input model sequence from niu_dir\n","        emb_dict_ls = []\n","        mlp_dict_ls = []\n","        for prev_num in reversed(range(train_config['seq_length'])):\n","            period_alias = 'period' + str(i - prev_num)\n","            # alias = os.path.join('./IU/ckpts', train_config['niu_dir_name'], period_alias, 'Epoch*')\n","            alias = os.path.join(args.iu_ckpts_path, 'Epoch*')\n","            restored_ckpt = search_ckpt(alias, mode=train_config['restored_ckpt_mode'])\n","            print('restored model {}: {}'.format(i - prev_num, restored_ckpt))\n","            emb_dict = {name: tf.train.load_checkpoint(restored_ckpt).get_tensor(name)\n","                        for name, _ in tf.train.list_variables(restored_ckpt) if name in emb_ls}\n","            mlp_dict = {name: tf.train.load_checkpoint(restored_ckpt).get_tensor(name)\n","                        for name, _ in tf.train.list_variables(restored_ckpt) if name in mlp_ls}\n","            emb_dict_ls.append(emb_dict)\n","            mlp_dict_ls.append(mlp_dict)\n","\n","        # concat sequence for different parameters on the last axis\n","        emb_ls_dict_ = {}\n","        for k in emb_dict_ls[0].keys():\n","            for emb_dict in emb_dict_ls:\n","                if k not in emb_ls_dict_.keys():\n","                    emb_ls_dict_[k] = np.expand_dims(emb_dict[k], axis=-1)\n","                else:\n","                    emb_ls_dict_[k] = np.concatenate((emb_ls_dict_[k], np.expand_dims(emb_dict[k], axis=-1)), axis=-1)\n","\n","        mlp_ls_dict_ = {}\n","        for k in mlp_dict_ls[0].keys():\n","            for mlp_dict in mlp_dict_ls:\n","                if k not in mlp_ls_dict_.keys():\n","                    mlp_ls_dict_[k] = np.expand_dims(mlp_dict[k], axis=-1)\n","                else:\n","                    mlp_ls_dict_[k] = np.concatenate((mlp_ls_dict_[k], np.expand_dims(mlp_dict[k], axis=-1)), axis=-1)\n","\n","        # check that the shapes are correct\n","        for k in emb_ls_dict_.keys():\n","            print(k, np.shape(emb_ls_dict_[k]))\n","        for k in mlp_ls_dict_.keys():\n","            print(k, np.shape(mlp_ls_dict_[k]))\n","\n","        print('collect params time elapsed: {}'.format(\n","            time.strftime('%H:%M:%S', time.gmtime(time.time() - collect_params_start_time))))\n","\n","        return emb_ls_dict_, mlp_ls_dict_\n","\n","\n","    def collect_init_h():\n","        \"\"\"\n","        collect previously trained initial hidden state\n","        :return: init_h_dict\n","        \"\"\"\n","        collect_init_h_start_time = time.time()\n","        path = os.path.join('ckpts', train_config['dir_name'], prev_period_alias, 'h_dict.pkl')\n","        with open(path, mode='r') as fh:\n","            h_dict = pickle.load(fh)\n","        init_h_dict_ = {}\n","        for k in h_dict.keys():\n","            init_h_dict_[k] = h_dict[k][..., 0]\n","        print('collect init h time elapsed: {}'.format(\n","            time.strftime('%H:%M:%S', time.gmtime(time.time() - collect_init_h_start_time))))\n","\n","        return init_h_dict_\n","\n","    # sort train data into periods based on num_periods\n","    data_df = data_df[(data_df['date'] >= train_config['start_date']) & (data_df['date'] <= train_config['end_date'])]\n","    data_df = data_df.sort_values(['timestamp']).reset_index(drop=True)\n","    records_per_period = int(len(data_df) / train_config['num_periods'])\n","    data_df['index'] = data_df.index\n","    data_df['period'] = data_df['index'].apply(lambda x: int(x / records_per_period) + 1)\n","    data_df = data_df[data_df.period != train_config['num_periods'] + 1]  # delete last extra period\n","    period_df = data_df.groupby('period')['date'].agg(['count', 'min', 'max'])\n","    data_df = data_df.drop(['index', 'date', 'timestamp'], axis=1)\n","\n","    orig_dir_name = train_config['dir_name']\n","\n","    for seq_length in [3]:\n","\n","        for meta_lr in [1e-2]:\n","\n","            print('')\n","            print('seq_length', seq_length, 'meta_lr', meta_lr)\n","\n","            train_config['seq_length'] = seq_length\n","            train_config['meta_lr'] = meta_lr\n","\n","            train_config['dir_name'] = orig_dir_name + '_' + str(seq_length) + '_' + str(meta_lr)\n","            print('dir_name: ', train_config['dir_name'])\n","\n","            test_aucs = []\n","            test_loglosses = []\n","\n","            start_period = train_config['train_start_period'] + seq_length - 1\n","\n","            for i in range(start_period, train_config['num_periods']):\n","\n","                # configure cur_period, next_period\n","                train_config['cur_period'] = i\n","                train_config['next_period'] = i + 1\n","                print('')\n","                print('current period: {}, next period: {}'.format(\n","                    train_config['cur_period'],\n","                    train_config['next_period']))\n","                print('')\n","\n","                # create next set\n","                next_set = data_df[data_df['period'] == train_config['next_period']]\n","                train_config['next_set_size'] = len(next_set)\n","                print('next set size', len(next_set))\n","\n","                train_config['period_alias'] = 'period' + str(i)\n","\n","                # checkpoints directory\n","                ckpts_dir = os.path.join('ckpts', train_config['dir_name'], train_config['period_alias'])\n","                if not os.path.exists(ckpts_dir):\n","                    os.makedirs(ckpts_dir)\n","\n","                if i == start_period:\n","                    train_config['restored_ckpt'] = None\n","                else:\n","                    prev_period_alias = 'period' + str(i - 1)\n","                    search_alias = os.path.join('ckpts', train_config['dir_name'], prev_period_alias, 'Epoch*')\n","                    train_config['restored_ckpt'] = search_ckpt(search_alias, mode=train_config['restored_ckpt_mode'])\n","                print('restored checkpoint: {}'.format(train_config['restored_ckpt']))\n","\n","                # write train_config to text file\n","                try:\n","                    with open(os.path.join(ckpts_dir, 'config.txt'), mode='w') as f:\n","                        f.write('train_config: ' + str(train_config) + '\\n')\n","                except:\n","                    with open(os.path.join(ckpts_dir, 'config.txt'), mode='wb') as f:\n","                        f.write('train_config: ' + str(train_config) + '\\n')\n","                    f.write('\\n')\n","                    f.write('EmbMLPnocate_hyperparams: ' + str(EmbMLPnocate_hyperparams) + '\\n')\n","                    f.write('\\n')\n","                    f.write('period_df:' + '\\n')\n","                    f.write(str(period_df))\n","\n","                # collect list of parameters for input model sequence\n","                emb_ls_dict, mlp_ls_dict = collect_params()\n","\n","                # collect previously trained initial hidden state\n","                if i == start_period:\n","                    init_h_dict = {'user_emb_w': np.zeros((EmbMLPnocate_hyperparams['num_users'],\n","                                                        EmbMLPnocate_hyperparams['user_embed_dim'],\n","                                                        train_config['emb_hidden_dim'])),\n","                                'item_emb_w': np.zeros((EmbMLPnocate_hyperparams['num_items'],\n","                                                        EmbMLPnocate_hyperparams['item_embed_dim'],\n","                                                        train_config['emb_hidden_dim'])),\n","                                'fcn1_kernel': np.zeros((EmbMLPnocate_hyperparams['layers'][0],\n","                                                            EmbMLPnocate_hyperparams['layers'][1],\n","                                                            train_config['mlp_hidden_dim'])),\n","                                'fcn1_bias': np.zeros((EmbMLPnocate_hyperparams['layers'][1],\n","                                                        train_config['mlp_hidden_dim'])),\n","                                'fcn2_kernel': np.zeros((EmbMLPnocate_hyperparams['layers'][1],\n","                                                            EmbMLPnocate_hyperparams['layers'][2],\n","                                                            train_config['mlp_hidden_dim'])),\n","                                'fcn2_bias': np.zeros((EmbMLPnocate_hyperparams['layers'][2],\n","                                                        train_config['mlp_hidden_dim'])),\n","                                'fcn3_kernel': np.zeros((EmbMLPnocate_hyperparams['layers'][2],\n","                                                            EmbMLPnocate_hyperparams['layers'][3],\n","                                                            train_config['mlp_hidden_dim'])),\n","                                'fcn3_bias': np.zeros((EmbMLPnocate_hyperparams['layers'][3],\n","                                                        train_config['mlp_hidden_dim']))}\n","                else:\n","                    init_h_dict = collect_init_h()\n","\n","                # build asmg model computation graph\n","                tf.reset_default_graph()\n","                asmg_model = ASMGrnnSingle(EmbMLPnocate_hyperparams, emb_ls_dict, mlp_ls_dict, init_h_dict, train_config=train_config)\n","\n","                # create session\n","                with tf.Session() as sess:\n","\n","                    # restore meta generator\n","                    if i == start_period:\n","                        # print([var.name for var in tf.global_variables()])  # check graph variables\n","                        sess.run([tf.global_variables_initializer(), tf.local_variables_initializer()])\n","                    else:\n","                        restorer = tf.train.Saver()\n","                        restorer.restore(sess, train_config['restored_ckpt'])\n","                    saver = tf.train.Saver()\n","\n","                    # test and then train meta generator\n","                    # create an engine instance with asmg_model\n","                    engine = ASMGEngine(sess, asmg_model)\n","\n","                    test_start_time = time.time()\n","                    print('Testing Meta Generator Start!')\n","                    next_auc, next_logloss = engine.test(next_set, train_config)\n","                    print('Done! time elapsed: {}, next_auc {:.4f}, next_logloss {:.4f}'.format(\n","                        time.strftime('%H:%M:%S', time.gmtime(time.time() - test_start_time)),\n","                        next_auc,\n","                        next_logloss))\n","                    print('')\n","\n","                    if i >= train_config['test_start_period']:\n","                        test_aucs.append(next_auc)\n","                        test_loglosses.append(next_logloss)\n","\n","                    if i < train_config['test_start_period'] or not train_config['test_stop_train']:\n","\n","                        train_start_time = time.time()\n","\n","                        for epoch_id in range(1, train_config['meta_num_epochs'] + 1):\n","\n","                            print('Training Meta Generator Epoch {} Start!'.format(epoch_id))\n","\n","                            meta_loss_next_avg = engine.meta_train_an_epoch(epoch_id, next_set, train_config)\n","                            print('Epoch {} Done! time elapsed: {}, meta_loss_next_avg {:.4f}'.format(\n","                                epoch_id,\n","                                time.strftime('%H:%M:%S', time.gmtime(time.time() - train_start_time)),\n","                                meta_loss_next_avg\n","                            ))\n","\n","                            next_auc, next_logloss = engine.test(next_set, train_config)\n","                            print('next_auc {:.4f}, next_logloss {:.4f}'.format(\n","                                next_auc,\n","                                next_logloss))\n","                            print('time elapsed {}'.format(time.strftime('%H:%M:%S', time.gmtime(time.time() - train_start_time))))\n","\n","                            print('')\n","\n","                            # save checkpoint\n","                            checkpoint_alias = 'Epoch{}_TestAUC{:.4f}_TestLOGLOSS{:.4f}.ckpt'.format(\n","                                epoch_id,\n","                                next_auc,\n","                                next_logloss)\n","                            checkpoint_path = os.path.join(ckpts_dir, checkpoint_alias)\n","                            saver.save(sess, checkpoint_path)\n","\n","                        # save h_dict\n","                        h_dict_ = asmg_model.get_h_dict(sess)\n","                        try:\n","                            with open(os.path.join(ckpts_dir, 'h_dict.pkl'), mode='w') as fh:\n","                                pickle.dump(h_dict_, fh)\n","                        except:\n","                            with open(os.path.join(ckpts_dir, 'h_dict.pkl'), mode='wb') as fh:\n","                                pickle.dump(h_dict_, fh)\n","\n","                    else:\n","                        # save checkpoint\n","                        checkpoint_alias = 'EpochNA_TestAUC{:.4f}_TestLOGLOSS{:.4f}.ckpt'.format(\n","                            next_auc,\n","                            next_logloss)\n","                        checkpoint_path = os.path.join(ckpts_dir, checkpoint_alias)\n","                        saver.save(sess, checkpoint_path)\n","\n","                        # save h_dict\n","                        h_dict_ = asmg_model.get_h_dict(sess)\n","                        try:\n","                            with open(os.path.join(ckpts_dir, 'h_dict.pkl'), mode='w') as fh:\n","                                pickle.dump(h_dict_, fh)\n","                        except:\n","                            with open(os.path.join(ckpts_dir, 'h_dict.pkl'), mode='wb') as fh:\n","                                pickle.dump(h_dict_, fh)\n","\n","                if i >= train_config['test_start_period']:\n","                    average_auc = sum(test_aucs) / len(test_aucs)\n","                    average_logloss = sum(test_loglosses) / len(test_loglosses)\n","                    print('test aucs', test_aucs)\n","                    print('average auc', average_auc)\n","                    print('')\n","                    print('test loglosses', test_loglosses)\n","                    print('average logloss', average_logloss)\n","\n","                    # write metrics to text file\n","                    try:\n","                        with open(os.path.join(ckpts_dir, 'test_metrics.txt'), mode='w') as f:\n","                            f.write('test_aucs: ' + str(test_aucs) + '\\n')\n","                    except:\n","                        with open(os.path.join(ckpts_dir, 'test_metrics.txt'), mode='wb') as f:\n","                            f.write('test_aucs: ' + str(test_aucs) + '\\n')\n","                        f.write('average_auc: ' + str(average_auc) + '\\n')\n","                        f.write('test_loglosses: ' + str(test_loglosses) + '\\n')\n","                        f.write('average_logloss: ' + str(average_logloss) + '\\n')"],"execution_count":33,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"P0nc1kyjzX9T"},"source":["## Jobs"]},{"cell_type":"code","metadata":{"id":"2y8mdDjds6dr","colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["83c9f2d6bc6a42109250d55a39b4b3e7","6af9c2d14cd344408ebeb165aecd5812","14a3c2eab70348fd8336ae11dbbfcffd","1f7c997368b34aedb5ecafc1ad6b1121","712fb93cec584db29a18f17f48ce85dc","6149942fe6ad4e0ea119545de5a8d242","f4349bb599ab413c9c087714eb7ba11f","7b90eb42b83641b4890847c92356a8f6","8d3fb8b1ab5742ec983d41c822319729","27e8cfe72e7a4a93b1f2e1f705a551a5","74ce21ef75514a30a12822ebc5165352","30c470ccdef34a2999ab74aad3503ba7","7ef1f03aed684fdbbc6602ec174cd2cd","45c6daf562174f79ad42975093c84210","d3fc9033cc8f45c9a9bc114a6c590fd9","53e9c4c383124944a89e833d7f0a56df","e69ef6e531314efe9a9ee6dc58e5998b","813103a929a84c78aa67e72419563cb7","0fa28afc080a4e98a0f5ccd7d083e3b3","60fe4254494e4bc589c971c5e287ec88","bb7538a3fc0045c28b51edc7f7cbbc00","d30f88908fe54f779659b41bd1e5ba6f"]},"collapsed":true,"executionInfo":{"status":"ok","timestamp":1636550822729,"user_tz":-330,"elapsed":2141725,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}},"outputId":"af2586b2-0874-4753-f5f5-7da803ca06aa"},"source":["# logger.info('JOB START: PREPROCESS_DATASET')\n","# preprocess_sobazaar()\n","# logger.info('JOB END: PREPROCESS_DATASET')"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["10-Nov-21 12:51:22 [INFO] : JOB START: PREPROCESS_DATASET\n","10-Nov-21 12:51:37 [INFO] : dataframe head -     userId  itemId      date   timestamp\n","0     3192   14808  20140901  1409596736\n","1     3192   14808  20140901  1409596739\n","2     3192   18402  20140901  1409596746\n","3    13749    4020  20140901  1409596746\n","4     9380    5935  20140901  1409596753\n","5     3192   18402  20140901  1409596756\n","6    11381    1603  20140901  1409596756\n","7    11381   20984  20140901  1409596762\n","8     6149    6092  20140901  1409596766\n","9    11381    1204  20140901  1409596772\n","10   11381    1204  20140901  1409596774\n","11    9380    4576  20140901  1409596774\n","12   11381    1204  20140901  1409596774\n","13    3192   11779  20140901  1409596789\n","14   10241    9656  20140901  1409596795\n","15    9380   18238  20140901  1409596813\n","16   10241   16933  20140901  1409596816\n","17   13749    3756  20140901  1409596820\n","18   13749    3756  20140901  1409596822\n","19    6149   20603  20140901  1409596825\n","10-Nov-21 12:51:37 [INFO] : num_users: 17126\n","10-Nov-21 12:51:37 [INFO] : num_items: 24785\n","10-Nov-21 12:51:37 [INFO] : num_records: 842660\n"]},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"83c9f2d6bc6a42109250d55a39b4b3e7","version_minor":0,"version_major":2},"text/plain":["  0%|          | 0/842660 [00:00<?, ?it/s]"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["10-Nov-21 12:51:49 [INFO] : NumExpr defaulting to 2 threads.\n","10-Nov-21 12:55:52 [INFO] : done row 100000\n","10-Nov-21 12:59:54 [INFO] : done row 200000\n","10-Nov-21 13:03:55 [INFO] : done row 300000\n","10-Nov-21 13:07:59 [INFO] : done row 400000\n","10-Nov-21 13:12:17 [INFO] : done row 500000\n","10-Nov-21 13:16:34 [INFO] : done row 600000\n","10-Nov-21 13:20:47 [INFO] : done row 700000\n","10-Nov-21 13:25:00 [INFO] : done row 800000\n"]},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"30c470ccdef34a2999ab74aad3503ba7","version_minor":0,"version_major":2},"text/plain":["  0%|          | 0/842660 [00:00<?, ?it/s]"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["10-Nov-21 13:26:54 [INFO] : dataframe head -     userId  ...   timestamp\n","0     3192  ...  1409596736\n","1     3192  ...  1409596736\n","2     3192  ...  1409596739\n","3     3192  ...  1409596739\n","4     3192  ...  1409596746\n","5     3192  ...  1409596746\n","6    13749  ...  1409596746\n","7    13749  ...  1409596746\n","8     9380  ...  1409596753\n","9     9380  ...  1409596753\n","10    3192  ...  1409596756\n","11    3192  ...  1409596756\n","12   11381  ...  1409596756\n","13   11381  ...  1409596756\n","14   11381  ...  1409596762\n","15   11381  ...  1409596762\n","16    6149  ...  1409596766\n","17    6149  ...  1409596766\n","18   11381  ...  1409596772\n","19   11381  ...  1409596772\n","\n","[20 rows x 6 columns]\n","10-Nov-21 13:26:54 [INFO] : 1685320\n","10-Nov-21 13:27:03 [INFO] : processed data saved at /content/soba_4mth_2014_1neg_30seq_1.csv\n","10-Nov-21 13:27:03 [INFO] : JOB END: PREPROCESS_DATASET\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0A3Bwm7DHyc4","executionInfo":{"status":"ok","timestamp":1636712651227,"user_tz":-330,"elapsed":11086,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}},"outputId":"ae419519-fb17-4e8a-8bf6-e014401676a7"},"source":["logger.info('JOB START: CONVERT_PARQUET_TO_CSV')\n","parquet_to_csv('/content/soba_4mth_2014_1neg_30seq_1.parquet.snappy')\n","logger.info('JOB END: CONVERT_PARQUET_TO_CSV')"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["12-Nov-21 10:24:01 [INFO] : JOB START: CONVERT_PARQUET_TO_CSV\n","12-Nov-21 10:24:12 [INFO] : csv file saved at /content/soba_4mth_2014_1neg_30seq_1.csv\n","12-Nov-21 10:24:12 [INFO] : JOB END: CONVERT_PARQUET_TO_CSV\n"]}]},{"cell_type":"code","metadata":{"id":"3ig3tPpB2Fx-","colab":{"base_uri":"https://localhost:8080/","height":612},"collapsed":true,"executionInfo":{"status":"error","timestamp":1636712794823,"user_tz":-330,"elapsed":23129,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}},"outputId":"e3bfd77a-6b02-4624-9c15-98e1fe50684a"},"source":["# logger.info('JOB START: EMBEDMLP_PRETRAINING')\n","# pretrain_sobazaar()\n","# logger.info('JOB END: EMBEDMLP_PRETRAINING')"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["12-Nov-21 10:26:13 [INFO] : JOB START: EMBEDMLP_PRETRAINING\n","12-Nov-21 10:26:34 [INFO] : Done loading data! time elapsed: 00:00:21\n"]},{"output_type":"error","ename":"ValueError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-35-9bbdcb519ea3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'JOB START: EMBEDMLP_PRETRAINING'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mpretrain_sobazaar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'JOB END: EMBEDMLP_PRETRAINING'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-30-b67006733af5>\u001b[0m in \u001b[0;36mpretrain_sobazaar\u001b[0;34m()\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0;31m# build base model computation graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m     \u001b[0mbase_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEmbMLPnocate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEmbMLPnocate_hyperparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_config\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[0;31m# create session\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-19-9fcd500a66d5>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, hyperparams, train_config)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0;31m# -- create emb begin -------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0muser_emb_w\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_variable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"user_emb_w\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mhyperparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'num_users'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhyperparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'user_embed_dim'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m         \u001b[0mitem_emb_w\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_variable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"item_emb_w\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mhyperparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'num_items'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhyperparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'item_embed_dim'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0;31m# -- create emb end -------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tensorflow-1.15.2/python3.7/tensorflow_core/python/ops/variable_scope.py\u001b[0m in \u001b[0;36mget_variable\u001b[0;34m(name, shape, dtype, initializer, regularizer, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter, constraint, synchronization, aggregation)\u001b[0m\n\u001b[1;32m   1498\u001b[0m       \u001b[0mconstraint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconstraint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1499\u001b[0m       \u001b[0msynchronization\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msynchronization\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1500\u001b[0;31m       aggregation=aggregation)\n\u001b[0m\u001b[1;32m   1501\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1502\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tensorflow-1.15.2/python3.7/tensorflow_core/python/ops/variable_scope.py\u001b[0m in \u001b[0;36mget_variable\u001b[0;34m(self, var_store, name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter, constraint, synchronization, aggregation)\u001b[0m\n\u001b[1;32m   1241\u001b[0m           \u001b[0mconstraint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconstraint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1242\u001b[0m           \u001b[0msynchronization\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msynchronization\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1243\u001b[0;31m           aggregation=aggregation)\n\u001b[0m\u001b[1;32m   1244\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1245\u001b[0m   def _get_partitioned_variable(self,\n","\u001b[0;32m/tensorflow-1.15.2/python3.7/tensorflow_core/python/ops/variable_scope.py\u001b[0m in \u001b[0;36mget_variable\u001b[0;34m(self, name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter, constraint, synchronization, aggregation)\u001b[0m\n\u001b[1;32m    565\u001b[0m           \u001b[0mconstraint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconstraint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    566\u001b[0m           \u001b[0msynchronization\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msynchronization\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 567\u001b[0;31m           aggregation=aggregation)\n\u001b[0m\u001b[1;32m    568\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    569\u001b[0m   def _get_partitioned_variable(self,\n","\u001b[0;32m/tensorflow-1.15.2/python3.7/tensorflow_core/python/ops/variable_scope.py\u001b[0m in \u001b[0;36m_true_getter\u001b[0;34m(name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, constraint, synchronization, aggregation)\u001b[0m\n\u001b[1;32m    517\u001b[0m           \u001b[0mconstraint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconstraint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    518\u001b[0m           \u001b[0msynchronization\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msynchronization\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 519\u001b[0;31m           aggregation=aggregation)\n\u001b[0m\u001b[1;32m    520\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    521\u001b[0m     synchronization, aggregation, trainable = (\n","\u001b[0;32m/tensorflow-1.15.2/python3.7/tensorflow_core/python/ops/variable_scope.py\u001b[0m in \u001b[0;36m_get_single_variable\u001b[0;34m(self, name, shape, dtype, initializer, regularizer, partition_info, reuse, trainable, collections, caching_device, validate_shape, use_resource, constraint, synchronization, aggregation)\u001b[0m\n\u001b[1;32m    866\u001b[0m         \u001b[0mtb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtb\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;34m\"tensorflow/python\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    867\u001b[0m         raise ValueError(\"%s Originally defined at:\\n\\n%s\" %\n\u001b[0;32m--> 868\u001b[0;31m                          (err_msg, \"\".join(traceback.format_list(tb))))\n\u001b[0m\u001b[1;32m    869\u001b[0m       \u001b[0mfound_var\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_vars\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    870\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_compatible_with\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfound_var\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: Variable user_emb_w already exists, disallowed. Did you mean to set reuse=True or reuse=tf.AUTO_REUSE in VarScope? Originally defined at:\n\n  File \"/tensorflow-1.15.2/python3.7/tensorflow_core/python/framework/ops.py\", line 1748, in __init__\n    self._traceback = tf_stack.extract_stack()\n  File \"/tensorflow-1.15.2/python3.7/tensorflow_core/python/framework/ops.py\", line 3426, in _create_op_internal\n    op_def=op_def)\n  File \"/tensorflow-1.15.2/python3.7/tensorflow_core/python/framework/ops.py\", line 3357, in create_op\n    attrs, op_def, compute_device)\n  File \"/tensorflow-1.15.2/python3.7/tensorflow_core/python/util/deprecation.py\", line 507, in new_func\n    return func(*args, **kwargs)\n  File \"/tensorflow-1.15.2/python3.7/tensorflow_core/python/framework/op_def_library.py\", line 794, in _apply_op_helper\n    op_def=op_def)\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":544},"collapsed":true,"id":"Rsb9dm2Zh3RU","executionInfo":{"status":"error","timestamp":1636712675040,"user_tz":-330,"elapsed":23820,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}},"outputId":"050066fc-1a77-4730-bae9-623e5e1cf605"},"source":["# logger.info('JOB START: INCREMENTAL_UPDATING')\n","# iu_sobazaar()\n","# logger.info('JOB END: INCREMENTAL_UPDATING')"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["12-Nov-21 10:24:12 [INFO] : JOB START: INCREMENTAL_UPDATING\n","12-Nov-21 10:24:32 [INFO] : Done loading data! time elapsed: 00:00:20\n","12-Nov-21 10:24:32 [INFO] : NumExpr defaulting to 2 threads.\n","\n","base_lr 0.001\n","dir_name:  IU_train11-23_test24-30_1epoch_0.001\n","\n","current period: 11, next period: 12\n","\n","current set size 54365 next set size 54365\n"]},{"output_type":"error","ename":"ValueError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-26-3b4dbaee002c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'JOB START: INCREMENTAL_UPDATING'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0miu_sobazaar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'JOB END: INCREMENTAL_UPDATING'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-22-6b4b7c356dda>\u001b[0m in \u001b[0;36miu_sobazaar\u001b[0;34m()\u001b[0m\n\u001b[1;32m     96\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mtrain_config\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'train_start_period'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m                 \u001b[0msearch_alias\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ckpts'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_config\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'pretrain_model'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Epoch*'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m                 \u001b[0mtrain_config\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'restored_ckpt'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msearch_ckpt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msearch_alias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_config\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'restored_ckpt_mode'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m                 \u001b[0mprev_period_alias\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'period'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-15-334c113158f7>\u001b[0m in \u001b[0;36msearch_ckpt\u001b[0;34m(search_alias, mode)\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mmetrics_ls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mckpt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'.ckpt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'_'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Epoch'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mckpt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mckpt_ls\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# epoch no.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mselected_metrics_pos_ls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmetrics_ls\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmetrics_ls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# find all positions of the selected ckpts\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0mckpt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mckpt_ls\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mselected_metrics_pos_ls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# get the full path of the last selected ckpt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mckpt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mckpt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'.ckpt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# get the path name before .ckpt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: max() arg is an empty sequence"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RilXyID6s9n_","executionInfo":{"status":"ok","timestamp":1636713139139,"user_tz":-330,"elapsed":22193,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}},"outputId":"f71d8cf8-d1d5-437d-9689-d1f06790703e"},"source":["logger.info('JOB START: RESTORE_CHECKPOINTS')\n","restore_ckpts()\n","logger.info('JOB END: RESTORE_CHECKPOINTS')"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["12-Nov-21 10:31:59 [INFO] : JOB START: RESTORE_CHECKPOINTS\n","Cloning into '/content/temp_ckpts'...\n","remote: Enumerating objects: 1165, done.\u001b[K\n","remote: Counting objects: 100% (1165/1165), done.\u001b[K\n","remote: Compressing objects: 100% (755/755), done.\u001b[K\n","remote: Total 1165 (delta 447), reused 1103 (delta 398), pack-reused 0\u001b[K\n","Receiving objects: 100% (1165/1165), 445.77 MiB | 28.68 MiB/s, done.\n","Resolving deltas: 100% (447/447), done.\n","12-Nov-21 10:32:20 [INFO] : JOB END: RESTORE_CHECKPOINTS\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"e3qVWT030c87","outputId":"a055e666-64ce-4350-8424-33140b1115c3"},"source":["logger.info('JOB START: ASMG_SINGLE_STRATEGY_MODEL')\n","args.iu_ckpts_path='/content/ckpts/IU_train11-23_test24-30_1epoch_0.001/period30'\n","asmg_single_sobazaar()\n","logger.info('JOB END: ASMG_SINGLE_STRATEGY_MODEL')"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["12-Nov-21 10:54:42 [INFO] : JOB START: INCREMENTAL_UPDATING\n","12-Nov-21 10:55:02 [INFO] : Done loading data! time elapsed: 00:00:20\n","\n","seq_length 3 meta_lr 0.01\n","dir_name:  ASMGgru_single_train11-23_test24-30_4emb_4mlp_1epoch_3_0.01\n","\n","current period: 13, next period: 14\n","\n","next set size 54365\n","restored checkpoint: None\n","restored model 11: /content/ckpts/IU_train11-23_test24-30_1epoch_0.001/period30/Epoch1_TestAUC0.8046_TestLOGLOSS0.5446.ckpt\n","restored model 12: /content/ckpts/IU_train11-23_test24-30_1epoch_0.001/period30/Epoch1_TestAUC0.8046_TestLOGLOSS0.5446.ckpt\n","restored model 13: /content/ckpts/IU_train11-23_test24-30_1epoch_0.001/period30/Epoch1_TestAUC0.8046_TestLOGLOSS0.5446.ckpt\n","item_emb_w (24785, 8, 3)\n","user_emb_w (17126, 8, 3)\n","fcn1/bias (12, 3)\n","fcn1/kernel (24, 12, 3)\n","fcn2/bias (6, 3)\n","fcn2/kernel (12, 6, 3)\n","fcn3/bias (1, 3)\n","fcn3/kernel (6, 1, 3)\n","collect params time elapsed: 00:00:00\n","Testing Meta Generator Start!\n","Done! time elapsed: 00:01:06, next_auc 0.2111, next_logloss 1.2430\n","\n","Training Meta Generator Epoch 1 Start!\n","[Epoch 1 Batch 1] meta_loss_next 1.1444, time elapsed 00:00:03\n","next_auc 0.3338, next_logloss 1.1756\n","time elapsed 00:01:04\n","\n","[Epoch 1 Batch 101] meta_loss_next 0.3190, time elapsed 00:02:34\n","next_auc 0.9368, next_logloss 0.2894\n","time elapsed 00:03:35\n","\n","[Epoch 1 Batch 201] meta_loss_next 0.2657, time elapsed 00:05:04\n","next_auc 0.9476, next_logloss 0.2746\n","time elapsed 00:06:05\n","\n","Epoch 1 Done! time elapsed: 00:06:15, meta_loss_next_avg 0.4005\n"]}]}]}