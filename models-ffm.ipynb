{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp models.ffm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FFM\n",
    "> A pytorch implementation of Field-aware Factorization Machine.\n",
    "\n",
    "Despite effectiveness, FM can be hindered by its modelling of all feature interactions with the same weight, as not all feature interactions are equally useful and predictive. For example, the interactions with useless features may even introduce noises and adversely degrade the performance.\n",
    "\n",
    "|  | For each | Learn |\n",
    "| --- | --- | --- |\n",
    "| Linear | feature | a weight |\n",
    "| Poly | feature pair | a weight |\n",
    "| FM | feature | a latent vector |\n",
    "| FFM | feature | multiple latent vectors |\n",
    "\n",
    "Field-aware factorization machine (FFM) is an extension to FM. It was originally introduced in [2]. The advantage of FFM over FM is that it uses different factorized latent factors for different groups of features. The \"group\" is called \"field\" in the context of FFM. Putting features into fields resolves the issue that the latent factors shared by features that intuitively represent different categories of information may not well generalize the correlation.\n",
    "\n",
    "FFM addresses this issue by splitting the original latent space into smaller latent spaces specific to the fields of the features.\n",
    "\n",
    "$$\\phi(\\pmb{w}, \\pmb{x}) = w_0 + \\sum\\limits_{i=1}^n w_i x_i + \\sum\\limits_{i=1}^n \\sum\\limits_{j=i + 1}^n \\langle \\mathbf{v}_{i, f_{2}} \\cdot \\mathbf{v}_{j, f_{1}} \\rangle x_i x_j$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev.showdoc import *\n",
    "from fastcore.nb_imports import *\n",
    "from fastcore.test import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "from recohut.models.layers.embedding import EmbeddingLayer\n",
    "from recohut.models.layers.common import LR_Layer\n",
    "\n",
    "from recohut.models.bases.ctr import CTRModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class FFM(CTRModel):\n",
    "    def __init__(self, \n",
    "                 feature_map, \n",
    "                 model_id=\"FFM\",\n",
    "                 task=\"binary_classification\",\n",
    "                 learning_rate=1e-3, \n",
    "                 embedding_initializer=\"torch.nn.init.normal_(std=1e-4)\",\n",
    "                 embedding_dim=2, \n",
    "                 regularizer=None,\n",
    "                 **kwargs):\n",
    "        super(FFM, self).__init__(feature_map, \n",
    "                                           model_id=model_id,\n",
    "                                           **kwargs)\n",
    "        self.num_fields = feature_map.num_fields\n",
    "        self.lr_layer = LR_Layer(feature_map, output_activation=None, use_bias=True)\n",
    "        self.embedding_layers = nn.ModuleList([EmbeddingLayer(feature_map, embedding_dim) \n",
    "                                               for x in range(self.num_fields - 1)]) # (F - 1) x F x bs x dim\n",
    "        self.output_activation = self.get_final_activation(task)\n",
    "        self.init_weights(embedding_initializer=embedding_initializer)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        lr_out = self.lr_layer(inputs)\n",
    "        field_wise_emb_list = [each_layer(inputs) for each_layer in self.embedding_layers] # (F - 1) list of bs x F x d\n",
    "        ffm_out = self.ffm_interaction(field_wise_emb_list)\n",
    "        y_pred = lr_out + ffm_out\n",
    "        if self.output_activation is not None:\n",
    "            y_pred = self.output_activation(y_pred)\n",
    "        return y_pred\n",
    "\n",
    "    def ffm_interaction(self, field_wise_emb_list):\n",
    "        dot = 0\n",
    "        for i in range(self.num_fields - 1):\n",
    "            for j in range(i + 1, self.num_fields):\n",
    "                v_ij = field_wise_emb_list[j - 1][:, i, :]\n",
    "                v_ji = field_wise_emb_list[i][:, j, :]\n",
    "                dot += torch.sum(v_ij * v_ji, dim=1, keepdim=True)\n",
    "        return dot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'model_id': 'FFM',\n",
    "              'data_dir': '/content/data',\n",
    "              'model_root': './checkpoints/',\n",
    "              'learning_rate': 1e-3,\n",
    "              'optimizer': 'adamw',\n",
    "              'task': 'binary_classification',\n",
    "              'loss': 'binary_crossentropy',\n",
    "              'metrics': ['logloss', 'AUC'],\n",
    "              'embedding_dim': 2,\n",
    "              'regularizer': 0,\n",
    "              'batch_size': 64,\n",
    "              'epochs': 3,\n",
    "              'shuffle': True,\n",
    "              'seed': 2019,\n",
    "              'use_hdf5': True,\n",
    "              'workers': 1,\n",
    "              'verbose': 0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FFM(ds.dataset.feature_map, **params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "\n",
      "  | Name              | Type       | Params\n",
      "-------------------------------------------------\n",
      "0 | lr_layer          | LR_Layer   | 477   \n",
      "1 | embedding_layers  | ModuleList | 12.4 K\n",
      "2 | output_activation | Sigmoid    | 0     \n",
      "-------------------------------------------------\n",
      "12.9 K    Trainable params\n",
      "0         Non-trainable params\n",
      "12.9 K    Total params\n",
      "0.051     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f98bad8085be4abe8283aacff18d647e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "115f389ead0b4226944fd69754a0d8ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58268430deca44d1bb6992ea7321dd9e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{'Test Metrics': {'AUC': tensor(1.), 'logloss': tensor(0.5025)}}\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'Test Metrics': {'AUC': tensor(1.), 'logloss': tensor(0.5025)}}]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pl_trainer(model, ds, max_epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from recohut.models.layers.common import FeaturesLinear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class FieldAwareFactorizationMachine(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, field_dims, embed_dim):\n",
    "        super().__init__()\n",
    "        self.num_fields = len(field_dims)\n",
    "        self.embeddings = torch.nn.ModuleList([\n",
    "            torch.nn.Embedding(sum(field_dims), embed_dim) for _ in range(self.num_fields)\n",
    "        ])\n",
    "        self.offsets = np.array((0, *np.cumsum(field_dims)[:-1]), dtype=np.long)\n",
    "        for embedding in self.embeddings:\n",
    "            torch.nn.init.xavier_uniform_(embedding.weight.data)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        :param x: Long tensor of size ``(batch_size, num_fields)``\n",
    "        \"\"\"\n",
    "        x = x + x.new_tensor(self.offsets).unsqueeze(0)\n",
    "        xs = [self.embeddings[i](x) for i in range(self.num_fields)]\n",
    "        ix = list()\n",
    "        for i in range(self.num_fields - 1):\n",
    "            for j in range(i + 1, self.num_fields):\n",
    "                ix.append(xs[j][:, i] * xs[i][:, j])\n",
    "        ix = torch.stack(ix, dim=1)\n",
    "        return ix\n",
    "\n",
    "class FFM_v2(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    A pytorch implementation of Field-aware Factorization Machine.\n",
    "    Reference:\n",
    "        Y Juan, et al. Field-aware Factorization Machines for CTR Prediction, 2015.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, field_dims, embed_dim):\n",
    "        super().__init__()\n",
    "        self.linear = FeaturesLinear(field_dims)\n",
    "        self.ffm = FieldAwareFactorizationMachine(field_dims, embed_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        :param x: Long tensor of size ``(batch_size, num_fields)``\n",
    "        \"\"\"\n",
    "        ffm_term = torch.sum(torch.sum(self.ffm(x), dim=1), dim=1, keepdim=True)\n",
    "        x = self.linear(x) + ffm_term\n",
    "        return torch.sigmoid(x.squeeze(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **References:-**\n",
    "- Y Juan, et al. Field-aware Factorization Machines for CTR Prediction, 2015.\n",
    "- https://github.com/rixwew/pytorch-fm/blob/master/torchfm/model/ffm.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "%reload_ext watermark\n",
    "%watermark -a \"Sparsh A.\" -m -iv -u -t -d -p recohut"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
