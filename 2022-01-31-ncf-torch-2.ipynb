{"cells":[{"cell_type":"markdown","metadata":{},"source":["# NCF Torch"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5873,"status":"ok","timestamp":1617988994519,"user":{"displayName":"Sparsh Agarwal","photoUrl":"","userId":"13037694610922482904"},"user_tz":-330},"id":"13rPS11Ll8G8","outputId":"9d198f9c-5663-4725-ec4e-1f15dfee0eb4"},"outputs":[],"source":["!pip install -q tensorboardX"]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":2107,"status":"ok","timestamp":1617988994520,"user":{"displayName":"Sparsh Agarwal","photoUrl":"","userId":"13037694610922482904"},"user_tz":-330},"id":"svq5fNg8VwS6"},"outputs":[],"source":["import torch\n","import random\n","import numpy as np\n","import pandas as pd\n","from copy import deepcopy\n","from torch.utils.data import DataLoader, Dataset\n","import math\n","from torch.autograd import Variable\n","from tensorboardX import SummaryWriter"]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":2137,"status":"ok","timestamp":1617988999765,"user":{"displayName":"Sparsh Agarwal","photoUrl":"","userId":"13037694610922482904"},"user_tz":-330},"id":"VAX0_lHoVzZN"},"outputs":[],"source":["random.seed(0)"]},{"cell_type":"code","execution_count":6,"metadata":{"executionInfo":{"elapsed":2944,"status":"ok","timestamp":1617989000781,"user":{"displayName":"Sparsh Agarwal","photoUrl":"","userId":"13037694610922482904"},"user_tz":-330},"id":"gO37gFMyV2Tx"},"outputs":[],"source":["class UserItemRatingDataset(Dataset):\n","    \"\"\"Wrapper, convert <user, item, rating> Tensor into Pytorch Dataset\"\"\"\n","    def __init__(self, user_tensor, item_tensor, target_tensor):\n","        \"\"\"\n","        args:\n","\n","            target_tensor: torch.Tensor, the corresponding rating for <user, item> pair\n","        \"\"\"\n","        self.user_tensor = user_tensor\n","        self.item_tensor = item_tensor\n","        self.target_tensor = target_tensor\n","\n","    def __getitem__(self, index):\n","        return self.user_tensor[index], self.item_tensor[index], self.target_tensor[index]\n","\n","    def __len__(self):\n","        return self.user_tensor.size(0)"]},{"cell_type":"code","execution_count":7,"metadata":{"executionInfo":{"elapsed":2818,"status":"ok","timestamp":1617989000783,"user":{"displayName":"Sparsh Agarwal","photoUrl":"","userId":"13037694610922482904"},"user_tz":-330},"id":"v6mqy7Pm5jyu"},"outputs":[],"source":["class SampleGenerator(object):\n","    \"\"\"Construct dataset for NCF\"\"\"\n","\n","    def __init__(self, ratings):\n","        \"\"\"\n","        args:\n","            ratings: pd.DataFrame, which contains 4 columns = ['userId', 'itemId', 'rating', 'timestamp']\n","        \"\"\"\n","        assert 'userId' in ratings.columns\n","        assert 'itemId' in ratings.columns\n","        assert 'rating' in ratings.columns\n","\n","        self.ratings = ratings\n","        # explicit feedback using _normalize and implicit using _binarize\n","        # self.preprocess_ratings = self._normalize(ratings)\n","        self.preprocess_ratings = self._binarize(ratings)\n","        self.user_pool = set(self.ratings['userId'].unique())\n","        self.item_pool = set(self.ratings['itemId'].unique())\n","        # create negative item samples for NCF learning\n","        self.negatives = self._sample_negative(ratings)\n","        self.train_ratings, self.test_ratings = self._split_loo(self.preprocess_ratings)\n","\n","    def _normalize(self, ratings):\n","        \"\"\"normalize into [0, 1] from [0, max_rating], explicit feedback\"\"\"\n","        ratings = deepcopy(ratings)\n","        max_rating = ratings.rating.max()\n","        ratings['rating'] = ratings.rating * 1.0 / max_rating\n","        return ratings\n","    \n","    def _binarize(self, ratings):\n","        \"\"\"binarize into 0 or 1, imlicit feedback\"\"\"\n","        ratings = deepcopy(ratings)\n","        ratings['rating'][ratings['rating'] > 0] = 1.0\n","        return ratings\n","\n","    def _split_loo(self, ratings):\n","        \"\"\"leave one out train/test split \"\"\"\n","        ratings['rank_latest'] = ratings.groupby(['userId'])['timestamp'].rank(method='first', ascending=False)\n","        test = ratings[ratings['rank_latest'] == 1]\n","        train = ratings[ratings['rank_latest'] > 1]\n","        assert train['userId'].nunique() == test['userId'].nunique()\n","        return train[['userId', 'itemId', 'rating']], test[['userId', 'itemId', 'rating']]\n","\n","    def _sample_negative(self, ratings):\n","        \"\"\"return all negative items & 100 sampled negative items\"\"\"\n","        interact_status = ratings.groupby('userId')['itemId'].apply(set).reset_index().rename(\n","            columns={'itemId': 'interacted_items'})\n","        interact_status['negative_items'] = interact_status['interacted_items'].apply(lambda x: self.item_pool - x)\n","        interact_status['negative_samples'] = interact_status['negative_items'].apply(lambda x: random.sample(x, 99))\n","        return interact_status[['userId', 'negative_items', 'negative_samples']]\n","\n","    def instance_a_train_loader(self, num_negatives, batch_size):\n","        \"\"\"instance train loader for one training epoch\"\"\"\n","        users, items, ratings = [], [], []\n","        train_ratings = pd.merge(self.train_ratings, self.negatives[['userId', 'negative_items']], on='userId')\n","        train_ratings['negatives'] = train_ratings['negative_items'].apply(lambda x: random.sample(x, num_negatives))\n","        for row in train_ratings.itertuples():\n","            users.append(int(row.userId))\n","            items.append(int(row.itemId))\n","            ratings.append(float(row.rating))\n","            for i in range(num_negatives):\n","                users.append(int(row.userId))\n","                items.append(int(row.negatives[i]))\n","                ratings.append(float(0))  # negative samples get 0 rating\n","        dataset = UserItemRatingDataset(user_tensor=torch.LongTensor(users),\n","                                        item_tensor=torch.LongTensor(items),\n","                                        target_tensor=torch.FloatTensor(ratings))\n","        return DataLoader(dataset, batch_size=batch_size, shuffle=True)\n","\n","    @property\n","    def evaluate_data(self):\n","        \"\"\"create evaluate data\"\"\"\n","        test_ratings = pd.merge(self.test_ratings, self.negatives[['userId', 'negative_samples']], on='userId')\n","        test_users, test_items, negative_users, negative_items = [], [], [], []\n","        for row in test_ratings.itertuples():\n","            test_users.append(int(row.userId))\n","            test_items.append(int(row.itemId))\n","            for i in range(len(row.negative_samples)):\n","                negative_users.append(int(row.userId))\n","                negative_items.append(int(row.negative_samples[i]))\n","        return [torch.LongTensor(test_users), torch.LongTensor(test_items), torch.LongTensor(negative_users),\n","                torch.LongTensor(negative_items)]"]},{"cell_type":"code","execution_count":8,"metadata":{"executionInfo":{"elapsed":2665,"status":"ok","timestamp":1617989000784,"user":{"displayName":"Sparsh Agarwal","photoUrl":"","userId":"13037694610922482904"},"user_tz":-330},"id":"_t9_tCwGV5Sn"},"outputs":[],"source":["# Checkpoints\n","def save_checkpoint(model, model_dir):\n","    torch.save(model.state_dict(), model_dir)\n","\n","\n","def resume_checkpoint(model, model_dir, device_id):\n","    state_dict = torch.load(model_dir,\n","                            map_location=lambda storage, loc: storage.cuda(device=device_id))  # ensure all storage are on gpu\n","    model.load_state_dict(state_dict)\n","\n","\n","# Hyper params\n","def use_cuda(enabled, device_id=0):\n","    if enabled:\n","        assert torch.cuda.is_available(), 'CUDA is not available'\n","        torch.cuda.set_device(device_id)\n","\n","\n","def use_optimizer(network, params):\n","    if params['optimizer'] == 'sgd':\n","        optimizer = torch.optim.SGD(network.parameters(),\n","                                    lr=params['sgd_lr'],\n","                                    momentum=params['sgd_momentum'],\n","                                    weight_decay=params['l2_regularization'])\n","    elif params['optimizer'] == 'adam':\n","        optimizer = torch.optim.Adam(network.parameters(), \n","                                                          lr=params['adam_lr'],\n","                                                          weight_decay=params['l2_regularization'])\n","    elif params['optimizer'] == 'rmsprop':\n","        optimizer = torch.optim.RMSprop(network.parameters(),\n","                                        lr=params['rmsprop_lr'],\n","                                        alpha=params['rmsprop_alpha'],\n","                                        momentum=params['rmsprop_momentum'])\n","    return optimizer"]},{"cell_type":"code","execution_count":9,"metadata":{"executionInfo":{"elapsed":2547,"status":"ok","timestamp":1617989000785,"user":{"displayName":"Sparsh Agarwal","photoUrl":"","userId":"13037694610922482904"},"user_tz":-330},"id":"8EfH0iMugWFG"},"outputs":[],"source":["class MetronAtK(object):\n","    def __init__(self, top_k):\n","        self._top_k = top_k\n","        self._subjects = None  # Subjects which we ran evaluation on\n","\n","    @property\n","    def top_k(self):\n","        return self._top_k\n","\n","    @top_k.setter\n","    def top_k(self, top_k):\n","        self._top_k = top_k\n","\n","    @property\n","    def subjects(self):\n","        return self._subjects\n","\n","    @subjects.setter\n","    def subjects(self, subjects):\n","        \"\"\"\n","        args:\n","            subjects: list, [test_users, test_items, test_scores, negative users, negative items, negative scores]\n","        \"\"\"\n","        assert isinstance(subjects, list)\n","        test_users, test_items, test_scores = subjects[0], subjects[1], subjects[2]\n","        neg_users, neg_items, neg_scores = subjects[3], subjects[4], subjects[5]\n","        # the golden set\n","        test = pd.DataFrame({'user': test_users,\n","                             'test_item': test_items,\n","                             'test_score': test_scores})\n","        # the full set\n","        full = pd.DataFrame({'user': neg_users + test_users,\n","                            'item': neg_items + test_items,\n","                            'score': neg_scores + test_scores})\n","        full = pd.merge(full, test, on=['user'], how='left')\n","        # rank the items according to the scores for each user\n","        full['rank'] = full.groupby('user')['score'].rank(method='first', ascending=False)\n","        full.sort_values(['user', 'rank'], inplace=True)\n","        self._subjects = full\n","\n","    def cal_hit_ratio(self):\n","        \"\"\"Hit Ratio @ top_K\"\"\"\n","        full, top_k = self._subjects, self._top_k\n","        top_k = full[full['rank']<=top_k]\n","        test_in_top_k =top_k[top_k['test_item'] == top_k['item']]  # golden items hit in the top_K items\n","        return len(test_in_top_k) * 1.0 / full['user'].nunique()\n","\n","    def cal_ndcg(self):\n","        full, top_k = self._subjects, self._top_k\n","        top_k = full[full['rank']<=top_k]\n","        test_in_top_k =top_k[top_k['test_item'] == top_k['item']]\n","        test_in_top_k['ndcg'] = test_in_top_k['rank'].apply(lambda x: math.log(2) / math.log(1 + x)) # the rank starts from 1\n","        return test_in_top_k['ndcg'].sum() * 1.0 / full['user'].nunique()"]},{"cell_type":"code","execution_count":10,"metadata":{"executionInfo":{"elapsed":2939,"status":"ok","timestamp":1617989001351,"user":{"displayName":"Sparsh Agarwal","photoUrl":"","userId":"13037694610922482904"},"user_tz":-330},"id":"_hfO6fiWg_Q1"},"outputs":[],"source":["class Engine(object):\n","    \"\"\"Meta Engine for training & evaluating NCF model\n","\n","    Note: Subclass should implement self.model !\n","    \"\"\"\n","\n","    def __init__(self, config):\n","        self.config = config  # model configuration\n","        self._metron = MetronAtK(top_k=10)\n","        self._writer = SummaryWriter(log_dir='runs/{}'.format(config['alias']))  # tensorboard writer\n","        self._writer.add_text('config', str(config), 0)\n","        self.opt = use_optimizer(self.model, config)\n","        # explicit feedback\n","        # self.crit = torch.nn.MSELoss()\n","        # implicit feedback\n","        self.crit = torch.nn.BCELoss()\n","\n","    def train_single_batch(self, users, items, ratings):\n","        assert hasattr(self, 'model'), 'Please specify the exact model !'\n","        if self.config['use_cuda'] is True:\n","            users, items, ratings = users.cuda(), items.cuda(), ratings.cuda()\n","        self.opt.zero_grad()\n","        ratings_pred = self.model(users, items)\n","        loss = self.crit(ratings_pred.view(-1), ratings)\n","        loss.backward()\n","        self.opt.step()\n","        loss = loss.item()\n","        return loss\n","\n","    def train_an_epoch(self, train_loader, epoch_id):\n","        assert hasattr(self, 'model'), 'Please specify the exact model !'\n","        self.model.train()\n","        total_loss = 0\n","        for batch_id, batch in enumerate(train_loader):\n","            assert isinstance(batch[0], torch.LongTensor)\n","            user, item, rating = batch[0], batch[1], batch[2]\n","            rating = rating.float()\n","            loss = self.train_single_batch(user, item, rating)\n","            print('[Training Epoch {}] Batch {}, Loss {}'.format(epoch_id, batch_id, loss))\n","            total_loss += loss\n","        self._writer.add_scalar('model/loss', total_loss, epoch_id)\n","\n","    def evaluate(self, evaluate_data, epoch_id):\n","        assert hasattr(self, 'model'), 'Please specify the exact model !'\n","        self.model.eval()\n","        with torch.no_grad():\n","            test_users, test_items = evaluate_data[0], evaluate_data[1]\n","            negative_users, negative_items = evaluate_data[2], evaluate_data[3]\n","            if self.config['use_cuda'] is True:\n","                test_users = test_users.cuda()\n","                test_items = test_items.cuda()\n","                negative_users = negative_users.cuda()\n","                negative_items = negative_items.cuda()\n","            test_scores = self.model(test_users, test_items)\n","            negative_scores = self.model(negative_users, negative_items)\n","            if self.config['use_cuda'] is True:\n","                test_users = test_users.cpu()\n","                test_items = test_items.cpu()\n","                test_scores = test_scores.cpu()\n","                negative_users = negative_users.cpu()\n","                negative_items = negative_items.cpu()\n","                negative_scores = negative_scores.cpu()\n","            self._metron.subjects = [test_users.data.view(-1).tolist(),\n","                                 test_items.data.view(-1).tolist(),\n","                                 test_scores.data.view(-1).tolist(),\n","                                 negative_users.data.view(-1).tolist(),\n","                                 negative_items.data.view(-1).tolist(),\n","                                 negative_scores.data.view(-1).tolist()]\n","        hit_ratio, ndcg = self._metron.cal_hit_ratio(), self._metron.cal_ndcg()\n","        self._writer.add_scalar('performance/HR', hit_ratio, epoch_id)\n","        self._writer.add_scalar('performance/NDCG', ndcg, epoch_id)\n","        print('[Evluating Epoch {}] HR = {:.4f}, NDCG = {:.4f}'.format(epoch_id, hit_ratio, ndcg))\n","        return hit_ratio, ndcg\n","\n","    def save(self, alias, epoch_id, hit_ratio, ndcg):\n","        assert hasattr(self, 'model'), 'Please specify the exact model !'\n","        model_dir = self.config['model_dir'].format(alias, epoch_id, hit_ratio, ndcg)\n","        save_checkpoint(self.model, model_dir)"]},{"cell_type":"code","execution_count":11,"metadata":{"executionInfo":{"elapsed":2846,"status":"ok","timestamp":1617989001352,"user":{"displayName":"Sparsh Agarwal","photoUrl":"","userId":"13037694610922482904"},"user_tz":-330},"id":"ZxUoFDa8i1Da"},"outputs":[],"source":["class GMF(torch.nn.Module):\n","    def __init__(self, config):\n","        super(GMF, self).__init__()\n","        self.num_users = config['num_users']\n","        self.num_items = config['num_items']\n","        self.latent_dim = config['latent_dim']\n","\n","        self.embedding_user = torch.nn.Embedding(num_embeddings=self.num_users, embedding_dim=self.latent_dim)\n","        self.embedding_item = torch.nn.Embedding(num_embeddings=self.num_items, embedding_dim=self.latent_dim)\n","\n","        self.affine_output = torch.nn.Linear(in_features=self.latent_dim, out_features=1)\n","        self.logistic = torch.nn.Sigmoid()\n","\n","    def forward(self, user_indices, item_indices):\n","        user_embedding = self.embedding_user(user_indices)\n","        item_embedding = self.embedding_item(item_indices)\n","        element_product = torch.mul(user_embedding, item_embedding)\n","        logits = self.affine_output(element_product)\n","        rating = self.logistic(logits)\n","        return rating\n","\n","    def init_weight(self):\n","        pass\n","\n","\n","class GMFEngine(Engine):\n","    \"\"\"Engine for training & evaluating GMF model\"\"\"\n","    def __init__(self, config):\n","        self.model = GMF(config)\n","        if config['use_cuda'] is True:\n","            use_cuda(True, config['device_id'])\n","            self.model.cuda()\n","        super(GMFEngine, self).__init__(config)"]},{"cell_type":"code","execution_count":12,"metadata":{"executionInfo":{"elapsed":2750,"status":"ok","timestamp":1617989001353,"user":{"displayName":"Sparsh Agarwal","photoUrl":"","userId":"13037694610922482904"},"user_tz":-330},"id":"Q94YEnLSkcdl"},"outputs":[],"source":["class MLP(torch.nn.Module):\n","    def __init__(self, config):\n","        super(MLP, self).__init__()\n","        self.config = config\n","        self.num_users = config['num_users']\n","        self.num_items = config['num_items']\n","        self.latent_dim = config['latent_dim']\n","\n","        self.embedding_user = torch.nn.Embedding(num_embeddings=self.num_users, embedding_dim=self.latent_dim)\n","        self.embedding_item = torch.nn.Embedding(num_embeddings=self.num_items, embedding_dim=self.latent_dim)\n","\n","        self.fc_layers = torch.nn.ModuleList()\n","        for idx, (in_size, out_size) in enumerate(zip(config['layers'][:-1], config['layers'][1:])):\n","            self.fc_layers.append(torch.nn.Linear(in_size, out_size))\n","\n","        self.affine_output = torch.nn.Linear(in_features=config['layers'][-1], out_features=1)\n","        self.logistic = torch.nn.Sigmoid()\n","\n","    def forward(self, user_indices, item_indices):\n","        user_embedding = self.embedding_user(user_indices)\n","        item_embedding = self.embedding_item(item_indices)\n","        vector = torch.cat([user_embedding, item_embedding], dim=-1)  # the concat latent vector\n","        for idx, _ in enumerate(range(len(self.fc_layers))):\n","            vector = self.fc_layers[idx](vector)\n","            vector = torch.nn.ReLU()(vector)\n","            # vector = torch.nn.BatchNorm1d()(vector)\n","            # vector = torch.nn.Dropout(p=0.5)(vector)\n","        logits = self.affine_output(vector)\n","        rating = self.logistic(logits)\n","        return rating\n","\n","    def init_weight(self):\n","        pass\n","\n","    def load_pretrain_weights(self):\n","        \"\"\"Loading weights from trained GMF model\"\"\"\n","        config = self.config\n","        gmf_model = GMF(config)\n","        if config['use_cuda'] is True:\n","            gmf_model.cuda()\n","        resume_checkpoint(gmf_model, model_dir=config['pretrain_mf'], device_id=config['device_id'])\n","        self.embedding_user.weight.data = gmf_model.embedding_user.weight.data\n","        self.embedding_item.weight.data = gmf_model.embedding_item.weight.data\n","\n","\n","class MLPEngine(Engine):\n","    \"\"\"Engine for training & evaluating GMF model\"\"\"\n","    def __init__(self, config):\n","        self.model = MLP(config)\n","        if config['use_cuda'] is True:\n","            use_cuda(True, config['device_id'])\n","            self.model.cuda()\n","        super(MLPEngine, self).__init__(config)\n","        print(self.model)\n","\n","        if config['pretrain']:\n","            self.model.load_pretrain_weights()"]},{"cell_type":"code","execution_count":13,"metadata":{"executionInfo":{"elapsed":2552,"status":"ok","timestamp":1617989001354,"user":{"displayName":"Sparsh Agarwal","photoUrl":"","userId":"13037694610922482904"},"user_tz":-330},"id":"N__jSZ_4mqk2"},"outputs":[],"source":["class NeuMF(torch.nn.Module):\n","    def __init__(self, config):\n","        super(NeuMF, self).__init__()\n","        self.config = config\n","        self.num_users = config['num_users']\n","        self.num_items = config['num_items']\n","        self.latent_dim_mf = config['latent_dim_mf']\n","        self.latent_dim_mlp = config['latent_dim_mlp']\n","\n","        self.embedding_user_mlp = torch.nn.Embedding(num_embeddings=self.num_users, embedding_dim=self.latent_dim_mlp)\n","        self.embedding_item_mlp = torch.nn.Embedding(num_embeddings=self.num_items, embedding_dim=self.latent_dim_mlp)\n","        self.embedding_user_mf = torch.nn.Embedding(num_embeddings=self.num_users, embedding_dim=self.latent_dim_mf)\n","        self.embedding_item_mf = torch.nn.Embedding(num_embeddings=self.num_items, embedding_dim=self.latent_dim_mf)\n","\n","        self.fc_layers = torch.nn.ModuleList()\n","        for idx, (in_size, out_size) in enumerate(zip(config['layers'][:-1], config['layers'][1:])):\n","            self.fc_layers.append(torch.nn.Linear(in_size, out_size))\n","\n","        self.affine_output = torch.nn.Linear(in_features=config['layers'][-1] + config['latent_dim_mf'], out_features=1)\n","        self.logistic = torch.nn.Sigmoid()\n","\n","    def forward(self, user_indices, item_indices):\n","        user_embedding_mlp = self.embedding_user_mlp(user_indices)\n","        item_embedding_mlp = self.embedding_item_mlp(item_indices)\n","        user_embedding_mf = self.embedding_user_mf(user_indices)\n","        item_embedding_mf = self.embedding_item_mf(item_indices)\n","\n","        mlp_vector = torch.cat([user_embedding_mlp, item_embedding_mlp], dim=-1)  # the concat latent vector\n","        mf_vector =torch.mul(user_embedding_mf, item_embedding_mf)\n","\n","        for idx, _ in enumerate(range(len(self.fc_layers))):\n","            mlp_vector = self.fc_layers[idx](mlp_vector)\n","            mlp_vector = torch.nn.ReLU()(mlp_vector)\n","\n","        vector = torch.cat([mlp_vector, mf_vector], dim=-1)\n","        logits = self.affine_output(vector)\n","        rating = self.logistic(logits)\n","        return rating\n","\n","    def init_weight(self):\n","        pass\n","\n","    def load_pretrain_weights(self):\n","        \"\"\"Loading weights from trained MLP model & GMF model\"\"\"\n","        config = self.config\n","        config['latent_dim'] = config['latent_dim_mlp']\n","        mlp_model = MLP(config)\n","        if config['use_cuda'] is True:\n","            mlp_model.cuda()\n","        resume_checkpoint(mlp_model, model_dir=config['pretrain_mlp'], device_id=config['device_id'])\n","\n","        self.embedding_user_mlp.weight.data = mlp_model.embedding_user.weight.data\n","        self.embedding_item_mlp.weight.data = mlp_model.embedding_item.weight.data\n","        for idx in range(len(self.fc_layers)):\n","            self.fc_layers[idx].weight.data = mlp_model.fc_layers[idx].weight.data\n","\n","        config['latent_dim'] = config['latent_dim_mf']\n","        gmf_model = GMF(config)\n","        if config['use_cuda'] is True:\n","            gmf_model.cuda()\n","        resume_checkpoint(gmf_model, model_dir=config['pretrain_mf'], device_id=config['device_id'])\n","        self.embedding_user_mf.weight.data = gmf_model.embedding_user.weight.data\n","        self.embedding_item_mf.weight.data = gmf_model.embedding_item.weight.data\n","\n","        self.affine_output.weight.data = 0.5 * torch.cat([mlp_model.affine_output.weight.data, gmf_model.affine_output.weight.data], dim=-1)\n","        self.affine_output.bias.data = 0.5 * (mlp_model.affine_output.bias.data + gmf_model.affine_output.bias.data)\n","\n","\n","class NeuMFEngine(Engine):\n","    \"\"\"Engine for training & evaluating GMF model\"\"\"\n","    def __init__(self, config):\n","        self.model = NeuMF(config)\n","        if config['use_cuda'] is True:\n","            use_cuda(True, config['device_id'])\n","            self.model.cuda()\n","        super(NeuMFEngine, self).__init__(config)\n","        print(self.model)\n","\n","        if config['pretrain']:\n","            self.model.load_pretrain_weights()"]},{"cell_type":"code","execution_count":15,"metadata":{"executionInfo":{"elapsed":1736,"status":"ok","timestamp":1617989245184,"user":{"displayName":"Sparsh Agarwal","photoUrl":"","userId":"13037694610922482904"},"user_tz":-330},"id":"5VtKKYSCrGYn"},"outputs":[],"source":["!mkdir -p checkpoints"]},{"cell_type":"code","execution_count":16,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":92308,"status":"ok","timestamp":1617990029185,"user":{"displayName":"Sparsh Agarwal","photoUrl":"","userId":"13037694610922482904"},"user_tz":-330},"id":"_pn9e23RnDgJ","outputId":"d8d12fc5-8580-4a09-98c7-82092abed7cb"},"outputs":[{"name":"stdout","output_type":"stream","text":["File ‘ratings.dat’ already there; not retrieving.\n","\n","Range of userId is [0, 6039]\n","Range of itemId is [0, 3705]\n","Epoch 0 starts !\n","--------------------------------------------------------------------------------\n","[Training Epoch 0] Batch 0, Loss 0.7820725440979004\n","[Training Epoch 0] Batch 1, Loss 0.7801235914230347\n","[Training Epoch 0] Batch 2, Loss 0.7833565473556519\n","[Training Epoch 0] Batch 3, Loss 0.7763739824295044\n","[Training Epoch 0] Batch 4, Loss 0.7832895517349243\n","[Training Epoch 0] Batch 5, Loss 0.7710307836532593\n","[Training Epoch 0] Batch 6, Loss 0.7725749015808105\n","[Training Epoch 0] Batch 7, Loss 0.7801134586334229\n","[Training Epoch 0] Batch 8, Loss 0.7858853340148926\n","[Training Epoch 0] Batch 9, Loss 0.7892516851425171\n","[Training Epoch 0] Batch 10, Loss 0.7754082679748535\n","[Training Epoch 0] Batch 11, Loss 0.7685412168502808\n","[Training Epoch 0] Batch 12, Loss 0.7681798934936523\n","[Training Epoch 0] Batch 13, Loss 0.7866722345352173\n","[Training Epoch 0] Batch 14, Loss 0.7767859101295471\n","[Training Epoch 0] Batch 15, Loss 0.7830716967582703\n","[Training Epoch 0] Batch 16, Loss 0.7664467692375183\n","[Training Epoch 0] Batch 17, Loss 0.7784453630447388\n","[Training Epoch 0] Batch 18, Loss 0.7745177745819092\n","[Training Epoch 0] Batch 19, Loss 0.793806791305542\n","[Training Epoch 0] Batch 20, Loss 0.7884412407875061\n","[Training Epoch 0] Batch 21, Loss 0.7803287506103516\n","[Training Epoch 0] Batch 22, Loss 0.7707473039627075\n","[Training Epoch 0] Batch 23, Loss 0.781633734703064\n","[Training Epoch 0] Batch 24, Loss 0.7632657289505005\n","[Training Epoch 0] Batch 25, Loss 0.7679644227027893\n","[Training Epoch 0] Batch 26, Loss 0.7647939920425415\n","[Training Epoch 0] Batch 27, Loss 0.7556871175765991\n","[Training Epoch 0] Batch 28, Loss 0.769565224647522\n","[Training Epoch 0] Batch 29, Loss 0.7715408802032471\n","[Training Epoch 0] Batch 30, Loss 0.7667243480682373\n","[Training Epoch 0] Batch 31, Loss 0.7744553089141846\n","[Training Epoch 0] Batch 32, Loss 0.7625815272331238\n","[Training Epoch 0] Batch 33, Loss 0.7707279920578003\n","[Training Epoch 0] Batch 34, Loss 0.7673296928405762\n","[Training Epoch 0] Batch 35, Loss 0.7646277546882629\n","[Training Epoch 0] Batch 36, Loss 0.7613212466239929\n","[Training Epoch 0] Batch 37, Loss 0.7688143849372864\n","[Training Epoch 0] Batch 38, Loss 0.7614284753799438\n","[Training Epoch 0] Batch 39, Loss 0.7677364349365234\n","[Training Epoch 0] Batch 40, Loss 0.7626262903213501\n","[Training Epoch 0] Batch 41, Loss 0.7632488012313843\n","[Training Epoch 0] Batch 42, Loss 0.7555193901062012\n","[Training Epoch 0] Batch 43, Loss 0.7633686065673828\n","[Training Epoch 0] Batch 44, Loss 0.7663373947143555\n","[Training Epoch 0] Batch 45, Loss 0.7569084763526917\n","[Training Epoch 0] Batch 46, Loss 0.7551908493041992\n","[Training Epoch 0] Batch 47, Loss 0.7625195980072021\n","[Training Epoch 0] Batch 48, Loss 0.7463064193725586\n","[Training Epoch 0] Batch 49, Loss 0.7585875391960144\n","[Training Epoch 0] Batch 50, Loss 0.758040726184845\n","[Training Epoch 0] Batch 51, Loss 0.7569712400436401\n","[Training Epoch 0] Batch 52, Loss 0.7487424612045288\n","[Training Epoch 0] Batch 53, Loss 0.7593638300895691\n","[Training Epoch 0] Batch 54, Loss 0.7579134106636047\n","[Training Epoch 0] Batch 55, Loss 0.7511026859283447\n","[Training Epoch 0] Batch 56, Loss 0.7527116537094116\n","[Training Epoch 0] Batch 57, Loss 0.7493395805358887\n","[Training Epoch 0] Batch 58, Loss 0.7468554973602295\n","[Training Epoch 0] Batch 59, Loss 0.7478497624397278\n","[Training Epoch 0] Batch 60, Loss 0.745278000831604\n","[Training Epoch 0] Batch 61, Loss 0.7496569156646729\n","[Training Epoch 0] Batch 62, Loss 0.7487744688987732\n","[Training Epoch 0] Batch 63, Loss 0.7461574077606201\n","[Training Epoch 0] Batch 64, Loss 0.7534419298171997\n","[Training Epoch 0] Batch 65, Loss 0.7509372234344482\n","[Training Epoch 0] Batch 66, Loss 0.7458736896514893\n","[Training Epoch 0] Batch 67, Loss 0.748882532119751\n","[Training Epoch 0] Batch 68, Loss 0.7462542057037354\n","[Training Epoch 0] Batch 69, Loss 0.7328560948371887\n","[Training Epoch 0] Batch 70, Loss 0.7474038600921631\n","[Training Epoch 0] Batch 71, Loss 0.7421505451202393\n","[Training Epoch 0] Batch 72, Loss 0.7454535961151123\n","[Training Epoch 0] Batch 73, Loss 0.7465763092041016\n","[Training Epoch 0] Batch 74, Loss 0.7475524544715881\n","[Training Epoch 0] Batch 75, Loss 0.7431748509407043\n","[Training Epoch 0] Batch 76, Loss 0.7417237758636475\n","[Training Epoch 0] Batch 77, Loss 0.7462348937988281\n","[Training Epoch 0] Batch 78, Loss 0.7397794723510742\n","[Training Epoch 0] Batch 79, Loss 0.7492613792419434\n","[Training Epoch 0] Batch 80, Loss 0.7548602819442749\n","[Training Epoch 0] Batch 81, Loss 0.7396311163902283\n","[Training Epoch 0] Batch 82, Loss 0.7360535860061646\n","[Training Epoch 0] Batch 83, Loss 0.736152708530426\n","[Training Epoch 0] Batch 84, Loss 0.7409294843673706\n","[Training Epoch 0] Batch 85, Loss 0.7434346675872803\n","[Training Epoch 0] Batch 86, Loss 0.7415341138839722\n","[Training Epoch 0] Batch 87, Loss 0.7392820119857788\n","[Training Epoch 0] Batch 88, Loss 0.7397104501724243\n","[Training Epoch 0] Batch 89, Loss 0.7379254102706909\n","[Training Epoch 0] Batch 90, Loss 0.7468702793121338\n","[Training Epoch 0] Batch 91, Loss 0.7373098134994507\n","[Training Epoch 0] Batch 92, Loss 0.7389183640480042\n","[Training Epoch 0] Batch 93, Loss 0.7405123710632324\n","[Training Epoch 0] Batch 94, Loss 0.7352478504180908\n","[Training Epoch 0] Batch 95, Loss 0.7369357347488403\n","[Training Epoch 0] Batch 96, Loss 0.7371131181716919\n","[Training Epoch 0] Batch 97, Loss 0.7359224557876587\n","[Training Epoch 0] Batch 98, Loss 0.737521231174469\n","[Training Epoch 0] Batch 99, Loss 0.7340514659881592\n","[Training Epoch 0] Batch 100, Loss 0.7303071618080139\n","[Training Epoch 0] Batch 101, Loss 0.7364560961723328\n","[Training Epoch 0] Batch 102, Loss 0.7313967943191528\n","[Training Epoch 0] Batch 103, Loss 0.7312257885932922\n","[Training Epoch 0] Batch 104, Loss 0.729649543762207\n","[Training Epoch 0] Batch 105, Loss 0.7372740507125854\n","[Training Epoch 0] Batch 106, Loss 0.7278473973274231\n","[Training Epoch 0] Batch 107, Loss 0.7328073978424072\n","[Training Epoch 0] Batch 108, Loss 0.7266845107078552\n","[Training Epoch 0] Batch 109, Loss 0.7330362796783447\n","[Training Epoch 0] Batch 110, Loss 0.7307650446891785\n","[Training Epoch 0] Batch 111, Loss 0.7253074645996094\n","[Training Epoch 0] Batch 112, Loss 0.7352719306945801\n","[Training Epoch 0] Batch 113, Loss 0.7288184762001038\n","[Training Epoch 0] Batch 114, Loss 0.7282110452651978\n","[Training Epoch 0] Batch 115, Loss 0.7251459360122681\n","[Training Epoch 0] Batch 116, Loss 0.7235174179077148\n","[Training Epoch 0] Batch 117, Loss 0.7293139696121216\n","[Training Epoch 0] Batch 118, Loss 0.7262305021286011\n","[Training Epoch 0] Batch 119, Loss 0.7234488129615784\n","[Training Epoch 0] Batch 120, Loss 0.7305394411087036\n","[Training Epoch 0] Batch 121, Loss 0.7291164398193359\n","[Training Epoch 0] Batch 122, Loss 0.7293859720230103\n","[Training Epoch 0] Batch 123, Loss 0.7283470630645752\n","[Training Epoch 0] Batch 124, Loss 0.721196174621582\n","[Training Epoch 0] Batch 125, Loss 0.7243689894676208\n","[Training Epoch 0] Batch 126, Loss 0.7219177484512329\n","[Training Epoch 0] Batch 127, Loss 0.7231299877166748\n","[Training Epoch 0] Batch 128, Loss 0.7199928760528564\n","[Training Epoch 0] Batch 129, Loss 0.7251061201095581\n","[Training Epoch 0] Batch 130, Loss 0.7249228954315186\n","[Training Epoch 0] Batch 131, Loss 0.7209109663963318\n","[Training Epoch 0] Batch 132, Loss 0.7240991592407227\n","[Training Epoch 0] Batch 133, Loss 0.7178799510002136\n","[Training Epoch 0] Batch 134, Loss 0.7198516726493835\n","[Training Epoch 0] Batch 135, Loss 0.7231320738792419\n","[Training Epoch 0] Batch 136, Loss 0.7174463868141174\n","[Training Epoch 0] Batch 137, Loss 0.7172288298606873\n","[Training Epoch 0] Batch 138, Loss 0.7190156579017639\n","[Training Epoch 0] Batch 139, Loss 0.7185490727424622\n","[Training Epoch 0] Batch 140, Loss 0.72205650806427\n","[Training Epoch 0] Batch 141, Loss 0.724839448928833\n","[Training Epoch 0] Batch 142, Loss 0.7185966968536377\n","[Training Epoch 0] Batch 143, Loss 0.7252165079116821\n","[Training Epoch 0] Batch 144, Loss 0.7187219858169556\n","[Training Epoch 0] Batch 145, Loss 0.7240186929702759\n","[Training Epoch 0] Batch 146, Loss 0.7201987504959106\n","[Training Epoch 0] Batch 147, Loss 0.7189422845840454\n","[Training Epoch 0] Batch 148, Loss 0.7196288704872131\n","[Training Epoch 0] Batch 149, Loss 0.7163575887680054\n","[Training Epoch 0] Batch 150, Loss 0.7158665657043457\n","[Training Epoch 0] Batch 151, Loss 0.7170429229736328\n","[Training Epoch 0] Batch 152, Loss 0.7174328565597534\n","[Training Epoch 0] Batch 153, Loss 0.7140889167785645\n","[Training Epoch 0] Batch 154, Loss 0.7166547775268555\n","[Training Epoch 0] Batch 155, Loss 0.7162032127380371\n","[Training Epoch 0] Batch 156, Loss 0.7181082963943481\n","[Training Epoch 0] Batch 157, Loss 0.7159580588340759\n","[Training Epoch 0] Batch 158, Loss 0.7149037718772888\n","[Training Epoch 0] Batch 159, Loss 0.7131848335266113\n","[Training Epoch 0] Batch 160, Loss 0.7146643996238708\n","[Training Epoch 0] Batch 161, Loss 0.7162221670150757\n","[Training Epoch 0] Batch 162, Loss 0.71087646484375\n","[Training Epoch 0] Batch 163, Loss 0.7144927978515625\n","[Training Epoch 0] Batch 164, Loss 0.7135331630706787\n","[Training Epoch 0] Batch 165, Loss 0.7122827768325806\n","[Training Epoch 0] Batch 166, Loss 0.7117569446563721\n","[Training Epoch 0] Batch 167, Loss 0.71326744556427\n","[Training Epoch 0] Batch 168, Loss 0.7118279933929443\n","[Training Epoch 0] Batch 169, Loss 0.7115854620933533\n","[Training Epoch 0] Batch 170, Loss 0.7103301882743835\n","[Training Epoch 0] Batch 171, Loss 0.7118736505508423\n","[Training Epoch 0] Batch 172, Loss 0.7095385789871216\n","[Training Epoch 0] Batch 173, Loss 0.7086914777755737\n","[Training Epoch 0] Batch 174, Loss 0.7121635675430298\n","[Training Epoch 0] Batch 175, Loss 0.7106388211250305\n","[Training Epoch 0] Batch 176, Loss 0.7087235450744629\n","[Training Epoch 0] Batch 177, Loss 0.7094274759292603\n","[Training Epoch 0] Batch 178, Loss 0.7109763622283936\n","[Training Epoch 0] Batch 179, Loss 0.7081362009048462\n","[Training Epoch 0] Batch 180, Loss 0.7087811231613159\n","[Training Epoch 0] Batch 181, Loss 0.7083709239959717\n","[Training Epoch 0] Batch 182, Loss 0.7084951400756836\n","[Training Epoch 0] Batch 183, Loss 0.7061761021614075\n","[Training Epoch 0] Batch 184, Loss 0.7087318897247314\n","[Training Epoch 0] Batch 185, Loss 0.7045555710792542\n","[Training Epoch 0] Batch 186, Loss 0.7043237686157227\n","[Training Epoch 0] Batch 187, Loss 0.7060081958770752\n","[Training Epoch 0] Batch 188, Loss 0.705295741558075\n","[Training Epoch 0] Batch 189, Loss 0.7039101123809814\n","[Training Epoch 0] Batch 190, Loss 0.7042839527130127\n","[Training Epoch 0] Batch 191, Loss 0.7021024227142334\n","[Training Epoch 0] Batch 192, Loss 0.7067700624465942\n","[Training Epoch 0] Batch 193, Loss 0.701485812664032\n","[Training Epoch 0] Batch 194, Loss 0.7039996981620789\n","[Training Epoch 0] Batch 195, Loss 0.7021064758300781\n","[Training Epoch 0] Batch 196, Loss 0.7015414237976074\n","[Training Epoch 0] Batch 197, Loss 0.7015737891197205\n","[Training Epoch 0] Batch 198, Loss 0.7027503252029419\n","[Training Epoch 0] Batch 199, Loss 0.7013992071151733\n","[Training Epoch 0] Batch 200, Loss 0.7021641731262207\n","[Training Epoch 0] Batch 201, Loss 0.7035635709762573\n","[Training Epoch 0] Batch 202, Loss 0.7011741995811462\n","[Training Epoch 0] Batch 203, Loss 0.699711799621582\n","[Training Epoch 0] Batch 204, Loss 0.6997228264808655\n","[Training Epoch 0] Batch 205, Loss 0.6993556618690491\n","[Training Epoch 0] Batch 206, Loss 0.7002267241477966\n","[Training Epoch 0] Batch 207, Loss 0.7002354860305786\n","[Training Epoch 0] Batch 208, Loss 0.6999051570892334\n","[Training Epoch 0] Batch 209, Loss 0.6981825232505798\n","[Training Epoch 0] Batch 210, Loss 0.6968241333961487\n","[Training Epoch 0] Batch 211, Loss 0.6996990442276001\n","[Training Epoch 0] Batch 212, Loss 0.6967669725418091\n","[Training Epoch 0] Batch 213, Loss 0.6984751224517822\n","[Training Epoch 0] Batch 214, Loss 0.6982529759407043\n","[Training Epoch 0] Batch 215, Loss 0.6967566013336182\n","[Training Epoch 0] Batch 216, Loss 0.6982439160346985\n","[Training Epoch 0] Batch 217, Loss 0.6962951421737671\n","[Training Epoch 0] Batch 218, Loss 0.6958463191986084\n","[Training Epoch 0] Batch 219, Loss 0.6966784000396729\n","[Training Epoch 0] Batch 220, Loss 0.6965802311897278\n","[Training Epoch 0] Batch 221, Loss 0.6971892714500427\n","[Training Epoch 0] Batch 222, Loss 0.695527195930481\n","[Training Epoch 0] Batch 223, Loss 0.6947551369667053\n","[Training Epoch 0] Batch 224, Loss 0.6949939131736755\n","[Training Epoch 0] Batch 225, Loss 0.6947290301322937\n","[Training Epoch 0] Batch 226, Loss 0.6933663487434387\n","[Training Epoch 0] Batch 227, Loss 0.6921170949935913\n","[Training Epoch 0] Batch 228, Loss 0.6924375295639038\n","[Training Epoch 0] Batch 229, Loss 0.6921007633209229\n","[Training Epoch 0] Batch 230, Loss 0.6938132047653198\n","[Training Epoch 0] Batch 231, Loss 0.692778468132019\n","[Training Epoch 0] Batch 232, Loss 0.6931279897689819\n","[Training Epoch 0] Batch 233, Loss 0.692034125328064\n","[Training Epoch 0] Batch 234, Loss 0.6927344799041748\n","[Training Epoch 0] Batch 235, Loss 0.6917877197265625\n","[Training Epoch 0] Batch 236, Loss 0.6898478269577026\n","[Training Epoch 0] Batch 237, Loss 0.6893405914306641\n","[Training Epoch 0] Batch 238, Loss 0.6897203326225281\n","[Training Epoch 0] Batch 239, Loss 0.6892766952514648\n","[Training Epoch 0] Batch 240, Loss 0.6892414093017578\n","[Training Epoch 0] Batch 241, Loss 0.6905151605606079\n","[Training Epoch 0] Batch 242, Loss 0.6891929507255554\n","[Training Epoch 0] Batch 243, Loss 0.6895002126693726\n","[Training Epoch 0] Batch 244, Loss 0.6876645684242249\n","[Training Epoch 0] Batch 245, Loss 0.6884257197380066\n","[Training Epoch 0] Batch 246, Loss 0.688202977180481\n","[Training Epoch 0] Batch 247, Loss 0.6889207363128662\n","[Training Epoch 0] Batch 248, Loss 0.6867491006851196\n","[Training Epoch 0] Batch 249, Loss 0.6878668069839478\n","[Training Epoch 0] Batch 250, Loss 0.6881092190742493\n","[Training Epoch 0] Batch 251, Loss 0.6884773969650269\n","[Training Epoch 0] Batch 252, Loss 0.6876832246780396\n","[Training Epoch 0] Batch 253, Loss 0.6873005628585815\n","[Training Epoch 0] Batch 254, Loss 0.6867295503616333\n","[Training Epoch 0] Batch 255, Loss 0.6861740946769714\n","[Training Epoch 0] Batch 256, Loss 0.6869173049926758\n","[Training Epoch 0] Batch 257, Loss 0.6845183372497559\n","[Training Epoch 0] Batch 258, Loss 0.6856150031089783\n","[Training Epoch 0] Batch 259, Loss 0.686116635799408\n","[Training Epoch 0] Batch 260, Loss 0.6844030618667603\n","[Training Epoch 0] Batch 261, Loss 0.6850083470344543\n","[Training Epoch 0] Batch 262, Loss 0.684369683265686\n","[Training Epoch 0] Batch 263, Loss 0.6843138933181763\n","[Training Epoch 0] Batch 264, Loss 0.6842422485351562\n","[Training Epoch 0] Batch 265, Loss 0.6829847097396851\n","[Training Epoch 0] Batch 266, Loss 0.682415246963501\n","[Training Epoch 0] Batch 267, Loss 0.6833672523498535\n","[Training Epoch 0] Batch 268, Loss 0.6826022863388062\n","[Training Epoch 0] Batch 269, Loss 0.6818374991416931\n","[Training Epoch 0] Batch 270, Loss 0.6829250454902649\n","[Training Epoch 0] Batch 271, Loss 0.6809756755828857\n","[Training Epoch 0] Batch 272, Loss 0.6808493733406067\n","[Training Epoch 0] Batch 273, Loss 0.6809386014938354\n","[Training Epoch 0] Batch 274, Loss 0.6806083917617798\n","[Training Epoch 0] Batch 275, Loss 0.6808993220329285\n","[Training Epoch 0] Batch 276, Loss 0.6811888217926025\n","[Training Epoch 0] Batch 277, Loss 0.6805115342140198\n","[Training Epoch 0] Batch 278, Loss 0.6794942021369934\n","[Training Epoch 0] Batch 279, Loss 0.6783459186553955\n","[Training Epoch 0] Batch 280, Loss 0.6789799332618713\n","[Training Epoch 0] Batch 281, Loss 0.6783338189125061\n","[Training Epoch 0] Batch 282, Loss 0.678110659122467\n","[Training Epoch 0] Batch 283, Loss 0.6793696284294128\n","[Training Epoch 0] Batch 284, Loss 0.6767696142196655\n","[Training Epoch 0] Batch 285, Loss 0.6785302758216858\n","[Training Epoch 0] Batch 286, Loss 0.6780386567115784\n","[Training Epoch 0] Batch 287, Loss 0.6777468919754028\n","[Training Epoch 0] Batch 288, Loss 0.6776249408721924\n","[Training Epoch 0] Batch 289, Loss 0.6769004464149475\n","[Training Epoch 0] Batch 290, Loss 0.6759090423583984\n","[Training Epoch 0] Batch 291, Loss 0.6776376962661743\n","[Training Epoch 0] Batch 292, Loss 0.6760852336883545\n","[Training Epoch 0] Batch 293, Loss 0.6754712462425232\n","[Training Epoch 0] Batch 294, Loss 0.675958514213562\n","[Training Epoch 0] Batch 295, Loss 0.6754787564277649\n","[Training Epoch 0] Batch 296, Loss 0.6740235686302185\n","[Training Epoch 0] Batch 297, Loss 0.6735603213310242\n","[Training Epoch 0] Batch 298, Loss 0.6755490303039551\n","[Training Epoch 0] Batch 299, Loss 0.6746512055397034\n","[Training Epoch 0] Batch 300, Loss 0.6735931634902954\n","[Training Epoch 0] Batch 301, Loss 0.6727004051208496\n","[Training Epoch 0] Batch 302, Loss 0.6735554933547974\n","[Training Epoch 0] Batch 303, Loss 0.6735745668411255\n","[Training Epoch 0] Batch 304, Loss 0.6721144914627075\n","[Training Epoch 0] Batch 305, Loss 0.6732749938964844\n","[Training Epoch 0] Batch 306, Loss 0.6721527576446533\n","[Training Epoch 0] Batch 307, Loss 0.6707273721694946\n","[Training Epoch 0] Batch 308, Loss 0.6711208820343018\n","[Training Epoch 0] Batch 309, Loss 0.6727410554885864\n","[Training Epoch 0] Batch 310, Loss 0.6703528165817261\n","[Training Epoch 0] Batch 311, Loss 0.6713889837265015\n","[Training Epoch 0] Batch 312, Loss 0.6714650392532349\n","[Training Epoch 0] Batch 313, Loss 0.670183539390564\n","[Training Epoch 0] Batch 314, Loss 0.669899582862854\n","[Training Epoch 0] Batch 315, Loss 0.6709340810775757\n","[Training Epoch 0] Batch 316, Loss 0.6712799668312073\n","[Training Epoch 0] Batch 317, Loss 0.670261025428772\n","[Training Epoch 0] Batch 318, Loss 0.6703001260757446\n","[Training Epoch 0] Batch 319, Loss 0.6704297065734863\n","[Training Epoch 0] Batch 320, Loss 0.6686212420463562\n","[Training Epoch 0] Batch 321, Loss 0.667858362197876\n","[Training Epoch 0] Batch 322, Loss 0.6696265935897827\n","[Training Epoch 0] Batch 323, Loss 0.6672818064689636\n","[Training Epoch 0] Batch 324, Loss 0.6655932068824768\n","[Training Epoch 0] Batch 325, Loss 0.6667059063911438\n","[Training Epoch 0] Batch 326, Loss 0.6667439937591553\n","[Training Epoch 0] Batch 327, Loss 0.6691604852676392\n","[Training Epoch 0] Batch 328, Loss 0.6670846939086914\n","[Training Epoch 0] Batch 329, Loss 0.6667022705078125\n","[Training Epoch 0] Batch 330, Loss 0.6688900589942932\n","[Training Epoch 0] Batch 331, Loss 0.6663087010383606\n","[Training Epoch 0] Batch 332, Loss 0.6649641990661621\n","[Training Epoch 0] Batch 333, Loss 0.6654085516929626\n","[Training Epoch 0] Batch 334, Loss 0.6639727354049683\n","[Training Epoch 0] Batch 335, Loss 0.665486216545105\n","[Training Epoch 0] Batch 336, Loss 0.6651740670204163\n","[Training Epoch 0] Batch 337, Loss 0.6651568412780762\n","[Training Epoch 0] Batch 338, Loss 0.6638452410697937\n","[Training Epoch 0] Batch 339, Loss 0.6630867719650269\n","[Training Epoch 0] Batch 340, Loss 0.6627964973449707\n","[Training Epoch 0] Batch 341, Loss 0.6654331684112549\n","[Training Epoch 0] Batch 342, Loss 0.6644588708877563\n","[Training Epoch 0] Batch 343, Loss 0.6630797386169434\n","[Training Epoch 0] Batch 344, Loss 0.6624427437782288\n","[Training Epoch 0] Batch 345, Loss 0.6618108749389648\n","[Training Epoch 0] Batch 346, Loss 0.6631646752357483\n","[Training Epoch 0] Batch 347, Loss 0.6630418300628662\n","[Training Epoch 0] Batch 348, Loss 0.662560224533081\n","[Training Epoch 0] Batch 349, Loss 0.66282057762146\n","[Training Epoch 0] Batch 350, Loss 0.6630052328109741\n","[Training Epoch 0] Batch 351, Loss 0.6601094603538513\n","[Training Epoch 0] Batch 352, Loss 0.6633445024490356\n","[Training Epoch 0] Batch 353, Loss 0.6604483723640442\n","[Training Epoch 0] Batch 354, Loss 0.6606355905532837\n","[Training Epoch 0] Batch 355, Loss 0.658967137336731\n","[Training Epoch 0] Batch 356, Loss 0.660087525844574\n","[Training Epoch 0] Batch 357, Loss 0.6615819931030273\n","[Training Epoch 0] Batch 358, Loss 0.6603172421455383\n","[Training Epoch 0] Batch 359, Loss 0.6605057716369629\n","[Training Epoch 0] Batch 360, Loss 0.6590383052825928\n","[Training Epoch 0] Batch 361, Loss 0.6586245894432068\n","[Training Epoch 0] Batch 362, Loss 0.6591341495513916\n","[Training Epoch 0] Batch 363, Loss 0.6589682102203369\n","[Training Epoch 0] Batch 364, Loss 0.6591904163360596\n","[Training Epoch 0] Batch 365, Loss 0.6592251062393188\n","[Training Epoch 0] Batch 366, Loss 0.6572302579879761\n","[Training Epoch 0] Batch 367, Loss 0.6582565307617188\n","[Training Epoch 0] Batch 368, Loss 0.6550171971321106\n","[Training Epoch 0] Batch 369, Loss 0.6573398113250732\n","[Training Epoch 0] Batch 370, Loss 0.6555336713790894\n","[Training Epoch 0] Batch 371, Loss 0.6558370590209961\n","[Training Epoch 0] Batch 372, Loss 0.6554877758026123\n","[Training Epoch 0] Batch 373, Loss 0.6572887897491455\n","[Training Epoch 0] Batch 374, Loss 0.6550641059875488\n","[Training Epoch 0] Batch 375, Loss 0.659328818321228\n","[Training Epoch 0] Batch 376, Loss 0.6544853448867798\n","[Training Epoch 0] Batch 377, Loss 0.6541033983230591\n","[Training Epoch 0] Batch 378, Loss 0.6568498611450195\n","[Training Epoch 0] Batch 379, Loss 0.6528763771057129\n","[Training Epoch 0] Batch 380, Loss 0.6565674543380737\n","[Training Epoch 0] Batch 381, Loss 0.6518687009811401\n","[Training Epoch 0] Batch 382, Loss 0.6571800112724304\n","[Training Epoch 0] Batch 383, Loss 0.6510932445526123\n","[Training Epoch 0] Batch 384, Loss 0.6552563905715942\n","[Training Epoch 0] Batch 385, Loss 0.6522481441497803\n","[Training Epoch 0] Batch 386, Loss 0.6521551609039307\n","[Training Epoch 0] Batch 387, Loss 0.6509639620780945\n","[Training Epoch 0] Batch 388, Loss 0.6527904272079468\n","[Training Epoch 0] Batch 389, Loss 0.6528375148773193\n","[Training Epoch 0] Batch 390, Loss 0.6515482664108276\n","[Training Epoch 0] Batch 391, Loss 0.65057373046875\n","[Training Epoch 0] Batch 392, Loss 0.6516902446746826\n","[Training Epoch 0] Batch 393, Loss 0.6502333879470825\n","[Training Epoch 0] Batch 394, Loss 0.6472923755645752\n","[Training Epoch 0] Batch 395, Loss 0.6521779298782349\n","[Training Epoch 0] Batch 396, Loss 0.6491211652755737\n","[Training Epoch 0] Batch 397, Loss 0.6501068472862244\n","[Training Epoch 0] Batch 398, Loss 0.6494946479797363\n","[Training Epoch 0] Batch 399, Loss 0.6502031683921814\n","[Training Epoch 0] Batch 400, Loss 0.6525653004646301\n","[Training Epoch 0] Batch 401, Loss 0.6521388292312622\n","[Training Epoch 0] Batch 402, Loss 0.6500527858734131\n","[Training Epoch 0] Batch 403, Loss 0.6485954523086548\n","[Training Epoch 0] Batch 404, Loss 0.6482588052749634\n","[Training Epoch 0] Batch 405, Loss 0.6483040452003479\n","[Training Epoch 0] Batch 406, Loss 0.6485877633094788\n","[Training Epoch 0] Batch 407, Loss 0.6487621068954468\n","[Training Epoch 0] Batch 408, Loss 0.6495193243026733\n","[Training Epoch 0] Batch 409, Loss 0.6523183584213257\n","[Training Epoch 0] Batch 410, Loss 0.644804835319519\n","[Training Epoch 0] Batch 411, Loss 0.6474321484565735\n","[Training Epoch 0] Batch 412, Loss 0.6484413146972656\n","[Training Epoch 0] Batch 413, Loss 0.6426239609718323\n","[Training Epoch 0] Batch 414, Loss 0.6471755504608154\n","[Training Epoch 0] Batch 415, Loss 0.6430168151855469\n","[Training Epoch 0] Batch 416, Loss 0.6451923251152039\n","[Training Epoch 0] Batch 417, Loss 0.6483351588249207\n","[Training Epoch 0] Batch 418, Loss 0.6425739526748657\n","[Training Epoch 0] Batch 419, Loss 0.642060399055481\n","[Training Epoch 0] Batch 420, Loss 0.6435455679893494\n","[Training Epoch 0] Batch 421, Loss 0.6424459218978882\n","[Training Epoch 0] Batch 422, Loss 0.6445239782333374\n","[Training Epoch 0] Batch 423, Loss 0.6428071856498718\n","[Training Epoch 0] Batch 424, Loss 0.6486608982086182\n","[Training Epoch 0] Batch 425, Loss 0.6431602835655212\n","[Training Epoch 0] Batch 426, Loss 0.6418687105178833\n","[Training Epoch 0] Batch 427, Loss 0.6432204246520996\n","[Training Epoch 0] Batch 428, Loss 0.646110475063324\n","[Training Epoch 0] Batch 429, Loss 0.6438566446304321\n","[Training Epoch 0] Batch 430, Loss 0.6400516033172607\n","[Training Epoch 0] Batch 431, Loss 0.646539032459259\n","[Training Epoch 0] Batch 432, Loss 0.6418991684913635\n","[Training Epoch 0] Batch 433, Loss 0.6422979831695557\n","[Training Epoch 0] Batch 434, Loss 0.6435818672180176\n","[Training Epoch 0] Batch 435, Loss 0.6379272937774658\n","[Training Epoch 0] Batch 436, Loss 0.6375292539596558\n","[Training Epoch 0] Batch 437, Loss 0.64030921459198\n","[Training Epoch 0] Batch 438, Loss 0.6420761346817017\n","[Training Epoch 0] Batch 439, Loss 0.6404401063919067\n","[Training Epoch 0] Batch 440, Loss 0.6452668309211731\n","[Training Epoch 0] Batch 441, Loss 0.640873372554779\n","[Training Epoch 0] Batch 442, Loss 0.6467334032058716\n","[Training Epoch 0] Batch 443, Loss 0.6401036381721497\n","[Training Epoch 0] Batch 444, Loss 0.6408856511116028\n","[Training Epoch 0] Batch 445, Loss 0.6373745799064636\n","[Training Epoch 0] Batch 446, Loss 0.6397526264190674\n","[Training Epoch 0] Batch 447, Loss 0.6382862329483032\n","[Training Epoch 0] Batch 448, Loss 0.6363245248794556\n","[Training Epoch 0] Batch 449, Loss 0.6357687711715698\n","[Training Epoch 0] Batch 450, Loss 0.6410539150238037\n","[Training Epoch 0] Batch 451, Loss 0.640248715877533\n","[Training Epoch 0] Batch 452, Loss 0.6390129327774048\n","[Training Epoch 0] Batch 453, Loss 0.6407322287559509\n","[Training Epoch 0] Batch 454, Loss 0.6427029371261597\n","[Training Epoch 0] Batch 455, Loss 0.6348474025726318\n","[Training Epoch 0] Batch 456, Loss 0.6405782699584961\n","[Training Epoch 0] Batch 457, Loss 0.6387318968772888\n","[Training Epoch 0] Batch 458, Loss 0.6362653970718384\n","[Training Epoch 0] Batch 459, Loss 0.6404318809509277\n","[Training Epoch 0] Batch 460, Loss 0.636372983455658\n","[Training Epoch 0] Batch 461, Loss 0.638856053352356\n","[Training Epoch 0] Batch 462, Loss 0.6370555758476257\n","[Training Epoch 0] Batch 463, Loss 0.6384602189064026\n","[Training Epoch 0] Batch 464, Loss 0.6348901391029358\n","[Training Epoch 0] Batch 465, Loss 0.6406090259552002\n","[Training Epoch 0] Batch 466, Loss 0.6387317180633545\n","[Training Epoch 0] Batch 467, Loss 0.6317804455757141\n","[Training Epoch 0] Batch 468, Loss 0.6364600658416748\n","[Training Epoch 0] Batch 469, Loss 0.6339826583862305\n","[Training Epoch 0] Batch 470, Loss 0.6372512578964233\n","[Training Epoch 0] Batch 471, Loss 0.6317808628082275\n","[Training Epoch 0] Batch 472, Loss 0.6337012052536011\n","[Training Epoch 0] Batch 473, Loss 0.6363639235496521\n","[Training Epoch 0] Batch 474, Loss 0.6308691501617432\n","[Training Epoch 0] Batch 475, Loss 0.6376085877418518\n","[Training Epoch 0] Batch 476, Loss 0.6339211463928223\n","[Training Epoch 0] Batch 477, Loss 0.6345524787902832\n","[Training Epoch 0] Batch 478, Loss 0.6358300447463989\n","[Training Epoch 0] Batch 479, Loss 0.6312972903251648\n","[Training Epoch 0] Batch 480, Loss 0.6319855451583862\n","[Training Epoch 0] Batch 481, Loss 0.6320330500602722\n","[Training Epoch 0] Batch 482, Loss 0.6309294700622559\n","[Training Epoch 0] Batch 483, Loss 0.6345767378807068\n","[Training Epoch 0] Batch 484, Loss 0.6298102140426636\n","[Training Epoch 0] Batch 485, Loss 0.6315289735794067\n","[Training Epoch 0] Batch 486, Loss 0.6360362768173218\n","[Training Epoch 0] Batch 487, Loss 0.6263887882232666\n","[Training Epoch 0] Batch 488, Loss 0.6304672956466675\n","[Training Epoch 0] Batch 489, Loss 0.6245652437210083\n","[Training Epoch 0] Batch 490, Loss 0.6310176253318787\n","[Training Epoch 0] Batch 491, Loss 0.6332436800003052\n","[Training Epoch 0] Batch 492, Loss 0.6277411580085754\n","[Training Epoch 0] Batch 493, Loss 0.6302550435066223\n","[Training Epoch 0] Batch 494, Loss 0.6279600858688354\n","[Training Epoch 0] Batch 495, Loss 0.6295704245567322\n","[Training Epoch 0] Batch 496, Loss 0.6377050280570984\n","[Training Epoch 0] Batch 497, Loss 0.6261491179466248\n","[Training Epoch 0] Batch 498, Loss 0.6279288530349731\n","[Training Epoch 0] Batch 499, Loss 0.6289325952529907\n","[Training Epoch 0] Batch 500, Loss 0.632448673248291\n","[Training Epoch 0] Batch 501, Loss 0.6270727515220642\n","[Training Epoch 0] Batch 502, Loss 0.6225403547286987\n","[Training Epoch 0] Batch 503, Loss 0.6292740702629089\n","[Training Epoch 0] Batch 504, Loss 0.6285043954849243\n","[Training Epoch 0] Batch 505, Loss 0.6295124292373657\n","[Training Epoch 0] Batch 506, Loss 0.6273475885391235\n","[Training Epoch 0] Batch 507, Loss 0.6258373856544495\n","[Training Epoch 0] Batch 508, Loss 0.625301718711853\n","[Training Epoch 0] Batch 509, Loss 0.6379843950271606\n","[Training Epoch 0] Batch 510, Loss 0.6221345663070679\n","[Training Epoch 0] Batch 511, Loss 0.6288132071495056\n","[Training Epoch 0] Batch 512, Loss 0.6272232532501221\n","[Training Epoch 0] Batch 513, Loss 0.6312178373336792\n","[Training Epoch 0] Batch 514, Loss 0.6295806765556335\n","[Training Epoch 0] Batch 515, Loss 0.6196193695068359\n","[Training Epoch 0] Batch 516, Loss 0.617096483707428\n","[Training Epoch 0] Batch 517, Loss 0.6210095882415771\n","[Training Epoch 0] Batch 518, Loss 0.6248964667320251\n","[Training Epoch 0] Batch 519, Loss 0.6270058155059814\n","[Training Epoch 0] Batch 520, Loss 0.6246629357337952\n","[Training Epoch 0] Batch 521, Loss 0.6239669322967529\n","[Training Epoch 0] Batch 522, Loss 0.6217362880706787\n","[Training Epoch 0] Batch 523, Loss 0.6236752867698669\n","[Training Epoch 0] Batch 524, Loss 0.6311158537864685\n","[Training Epoch 0] Batch 525, Loss 0.6254180073738098\n","[Training Epoch 0] Batch 526, Loss 0.6317521929740906\n","[Training Epoch 0] Batch 527, Loss 0.6283869743347168\n","[Training Epoch 0] Batch 528, Loss 0.6255150437355042\n","[Training Epoch 0] Batch 529, Loss 0.6208481788635254\n","[Training Epoch 0] Batch 530, Loss 0.6204435229301453\n","[Training Epoch 0] Batch 531, Loss 0.6192665696144104\n","[Training Epoch 0] Batch 532, Loss 0.6215030550956726\n","[Training Epoch 0] Batch 533, Loss 0.6199039220809937\n","[Training Epoch 0] Batch 534, Loss 0.6230467557907104\n","[Training Epoch 0] Batch 535, Loss 0.6261683702468872\n","[Training Epoch 0] Batch 536, Loss 0.6197288036346436\n","[Training Epoch 0] Batch 537, Loss 0.6226037740707397\n","[Training Epoch 0] Batch 538, Loss 0.6198643445968628\n","[Training Epoch 0] Batch 539, Loss 0.6230267286300659\n","[Training Epoch 0] Batch 540, Loss 0.6196640729904175\n","[Training Epoch 0] Batch 541, Loss 0.6211122274398804\n","[Training Epoch 0] Batch 542, Loss 0.614505410194397\n","[Training Epoch 0] Batch 543, Loss 0.6172103881835938\n","[Training Epoch 0] Batch 544, Loss 0.6246614456176758\n","[Training Epoch 0] Batch 545, Loss 0.6212902069091797\n","[Training Epoch 0] Batch 546, Loss 0.6197768449783325\n","[Training Epoch 0] Batch 547, Loss 0.6165547370910645\n","[Training Epoch 0] Batch 548, Loss 0.6163098812103271\n","[Training Epoch 0] Batch 549, Loss 0.6161338090896606\n","[Training Epoch 0] Batch 550, Loss 0.6178374290466309\n","[Training Epoch 0] Batch 551, Loss 0.619313657283783\n","[Training Epoch 0] Batch 552, Loss 0.6160526871681213\n","[Training Epoch 0] Batch 553, Loss 0.6186507344245911\n","[Training Epoch 0] Batch 554, Loss 0.618229866027832\n","[Training Epoch 0] Batch 555, Loss 0.6222330331802368\n","[Training Epoch 0] Batch 556, Loss 0.6167548298835754\n","[Training Epoch 0] Batch 557, Loss 0.6186749339103699\n","[Training Epoch 0] Batch 558, Loss 0.6190630793571472\n","[Training Epoch 0] Batch 559, Loss 0.6165522336959839\n","[Training Epoch 0] Batch 560, Loss 0.6220460534095764\n","[Training Epoch 0] Batch 561, Loss 0.6174700260162354\n","[Training Epoch 0] Batch 562, Loss 0.6130325794219971\n","[Training Epoch 0] Batch 563, Loss 0.6152044534683228\n","[Training Epoch 0] Batch 564, Loss 0.6160386800765991\n","[Training Epoch 0] Batch 565, Loss 0.6161446571350098\n","[Training Epoch 0] Batch 566, Loss 0.6233452558517456\n","[Training Epoch 0] Batch 567, Loss 0.6087666749954224\n","[Training Epoch 0] Batch 568, Loss 0.620861828327179\n","[Training Epoch 0] Batch 569, Loss 0.613828182220459\n","[Training Epoch 0] Batch 570, Loss 0.613735020160675\n","[Training Epoch 0] Batch 571, Loss 0.6177235841751099\n","[Training Epoch 0] Batch 572, Loss 0.6147751808166504\n","[Training Epoch 0] Batch 573, Loss 0.6134991645812988\n","[Training Epoch 0] Batch 574, Loss 0.6145246028900146\n","[Training Epoch 0] Batch 575, Loss 0.6164448261260986\n","[Training Epoch 0] Batch 576, Loss 0.6100533604621887\n","[Training Epoch 0] Batch 577, Loss 0.6165896058082581\n","[Training Epoch 0] Batch 578, Loss 0.6171481013298035\n","[Training Epoch 0] Batch 579, Loss 0.6117149591445923\n","[Training Epoch 0] Batch 580, Loss 0.6173195838928223\n","[Training Epoch 0] Batch 581, Loss 0.6119203567504883\n","[Training Epoch 0] Batch 582, Loss 0.6145608425140381\n","[Training Epoch 0] Batch 583, Loss 0.6107970476150513\n","[Training Epoch 0] Batch 584, Loss 0.6142367720603943\n","[Training Epoch 0] Batch 585, Loss 0.6163412928581238\n","[Training Epoch 0] Batch 586, Loss 0.6073647141456604\n","[Training Epoch 0] Batch 587, Loss 0.6097385883331299\n","[Training Epoch 0] Batch 588, Loss 0.6088647842407227\n","[Training Epoch 0] Batch 589, Loss 0.6103870868682861\n","[Training Epoch 0] Batch 590, Loss 0.6098860502243042\n","[Training Epoch 0] Batch 591, Loss 0.6120933294296265\n","[Training Epoch 0] Batch 592, Loss 0.6097381114959717\n","[Training Epoch 0] Batch 593, Loss 0.6132047176361084\n","[Training Epoch 0] Batch 594, Loss 0.6131211519241333\n","[Training Epoch 0] Batch 595, Loss 0.60703444480896\n","[Training Epoch 0] Batch 596, Loss 0.6112174987792969\n","[Training Epoch 0] Batch 597, Loss 0.6109971404075623\n","[Training Epoch 0] Batch 598, Loss 0.6095898151397705\n","[Training Epoch 0] Batch 599, Loss 0.6111900806427002\n","[Training Epoch 0] Batch 600, Loss 0.6082428693771362\n","[Training Epoch 0] Batch 601, Loss 0.604279100894928\n","[Training Epoch 0] Batch 602, Loss 0.6049992442131042\n","[Training Epoch 0] Batch 603, Loss 0.6106922030448914\n","[Training Epoch 0] Batch 604, Loss 0.61040860414505\n","[Training Epoch 0] Batch 605, Loss 0.6142433881759644\n","[Training Epoch 0] Batch 606, Loss 0.6120513677597046\n","[Training Epoch 0] Batch 607, Loss 0.615972638130188\n","[Training Epoch 0] Batch 608, Loss 0.610456109046936\n","[Training Epoch 0] Batch 609, Loss 0.6062358617782593\n","[Training Epoch 0] Batch 610, Loss 0.6117580533027649\n","[Training Epoch 0] Batch 611, Loss 0.6074163913726807\n","[Training Epoch 0] Batch 612, Loss 0.605256974697113\n","[Training Epoch 0] Batch 613, Loss 0.6068722009658813\n","[Training Epoch 0] Batch 614, Loss 0.599716067314148\n","[Training Epoch 0] Batch 615, Loss 0.611332356929779\n","[Training Epoch 0] Batch 616, Loss 0.6125392317771912\n","[Training Epoch 0] Batch 617, Loss 0.6053388118743896\n","[Training Epoch 0] Batch 618, Loss 0.6062043905258179\n","[Training Epoch 0] Batch 619, Loss 0.6117437481880188\n","[Training Epoch 0] Batch 620, Loss 0.6004226207733154\n","[Training Epoch 0] Batch 621, Loss 0.6107460260391235\n","[Training Epoch 0] Batch 622, Loss 0.6071987748146057\n","[Training Epoch 0] Batch 623, Loss 0.5979271531105042\n","[Training Epoch 0] Batch 624, Loss 0.6057095527648926\n","[Training Epoch 0] Batch 625, Loss 0.6015520095825195\n","[Training Epoch 0] Batch 626, Loss 0.6040550470352173\n","[Training Epoch 0] Batch 627, Loss 0.6054190397262573\n","[Training Epoch 0] Batch 628, Loss 0.6068512797355652\n","[Training Epoch 0] Batch 629, Loss 0.602924108505249\n","[Training Epoch 0] Batch 630, Loss 0.6068342924118042\n","[Training Epoch 0] Batch 631, Loss 0.6059330701828003\n","[Training Epoch 0] Batch 632, Loss 0.6113942265510559\n","[Training Epoch 0] Batch 633, Loss 0.6017375588417053\n","[Training Epoch 0] Batch 634, Loss 0.5984897017478943\n","[Training Epoch 0] Batch 635, Loss 0.5915548801422119\n","[Training Epoch 0] Batch 636, Loss 0.6050238609313965\n","[Training Epoch 0] Batch 637, Loss 0.60274338722229\n","[Training Epoch 0] Batch 638, Loss 0.604219377040863\n","[Training Epoch 0] Batch 639, Loss 0.6050323247909546\n","[Training Epoch 0] Batch 640, Loss 0.605111300945282\n","[Training Epoch 0] Batch 641, Loss 0.603338360786438\n","[Training Epoch 0] Batch 642, Loss 0.610819935798645\n","[Training Epoch 0] Batch 643, Loss 0.6058947443962097\n","[Training Epoch 0] Batch 644, Loss 0.6042861938476562\n","[Training Epoch 0] Batch 645, Loss 0.5930482149124146\n","[Training Epoch 0] Batch 646, Loss 0.6013715863227844\n","[Training Epoch 0] Batch 647, Loss 0.6024938225746155\n","[Training Epoch 0] Batch 648, Loss 0.6000779867172241\n","[Training Epoch 0] Batch 649, Loss 0.5977323055267334\n","[Training Epoch 0] Batch 650, Loss 0.6010705232620239\n","[Training Epoch 0] Batch 651, Loss 0.5987609028816223\n","[Training Epoch 0] Batch 652, Loss 0.5994555354118347\n","[Training Epoch 0] Batch 653, Loss 0.6024700403213501\n","[Training Epoch 0] Batch 654, Loss 0.6013911962509155\n","[Training Epoch 0] Batch 655, Loss 0.6004725694656372\n","[Training Epoch 0] Batch 656, Loss 0.610040545463562\n","[Training Epoch 0] Batch 657, Loss 0.6019457578659058\n","[Training Epoch 0] Batch 658, Loss 0.6003910303115845\n","[Training Epoch 0] Batch 659, Loss 0.6060943603515625\n","[Training Epoch 0] Batch 660, Loss 0.6006273031234741\n","[Training Epoch 0] Batch 661, Loss 0.6012139320373535\n","[Training Epoch 0] Batch 662, Loss 0.5938689708709717\n","[Training Epoch 0] Batch 663, Loss 0.5973043441772461\n","[Training Epoch 0] Batch 664, Loss 0.6024243831634521\n","[Training Epoch 0] Batch 665, Loss 0.5897219777107239\n","[Training Epoch 0] Batch 666, Loss 0.5998218059539795\n","[Training Epoch 0] Batch 667, Loss 0.5986260175704956\n","[Training Epoch 0] Batch 668, Loss 0.5989888906478882\n","[Training Epoch 0] Batch 669, Loss 0.5985120534896851\n","[Training Epoch 0] Batch 670, Loss 0.5984644889831543\n","[Training Epoch 0] Batch 671, Loss 0.5923698544502258\n","[Training Epoch 0] Batch 672, Loss 0.601608157157898\n","[Training Epoch 0] Batch 673, Loss 0.5984100103378296\n","[Training Epoch 0] Batch 674, Loss 0.5940259695053101\n","[Training Epoch 0] Batch 675, Loss 0.6021506190299988\n","[Training Epoch 0] Batch 676, Loss 0.5960726737976074\n","[Training Epoch 0] Batch 677, Loss 0.6059435606002808\n","[Training Epoch 0] Batch 678, Loss 0.5974462628364563\n","[Training Epoch 0] Batch 679, Loss 0.5908343195915222\n","[Training Epoch 0] Batch 680, Loss 0.5897982716560364\n","[Training Epoch 0] Batch 681, Loss 0.5907941460609436\n","[Training Epoch 0] Batch 682, Loss 0.5956278443336487\n","[Training Epoch 0] Batch 683, Loss 0.588520884513855\n","[Training Epoch 0] Batch 684, Loss 0.6016067266464233\n","[Training Epoch 0] Batch 685, Loss 0.5962203741073608\n","[Training Epoch 0] Batch 686, Loss 0.6028835773468018\n","[Training Epoch 0] Batch 687, Loss 0.6023082137107849\n","[Training Epoch 0] Batch 688, Loss 0.5971762537956238\n","[Training Epoch 0] Batch 689, Loss 0.594008207321167\n","[Training Epoch 0] Batch 690, Loss 0.6009489297866821\n","[Training Epoch 0] Batch 691, Loss 0.5914708971977234\n","[Training Epoch 0] Batch 692, Loss 0.5944311618804932\n","[Training Epoch 0] Batch 693, Loss 0.5953851938247681\n","[Training Epoch 0] Batch 694, Loss 0.5915859937667847\n","[Training Epoch 0] Batch 695, Loss 0.5924785137176514\n","[Training Epoch 0] Batch 696, Loss 0.5946094393730164\n","[Training Epoch 0] Batch 697, Loss 0.6004119515419006\n","[Training Epoch 0] Batch 698, Loss 0.5872219204902649\n","[Training Epoch 0] Batch 699, Loss 0.5934187173843384\n","[Training Epoch 0] Batch 700, Loss 0.5991886854171753\n","[Training Epoch 0] Batch 701, Loss 0.5972145795822144\n","[Training Epoch 0] Batch 702, Loss 0.5988128781318665\n","[Training Epoch 0] Batch 703, Loss 0.5964728593826294\n","[Training Epoch 0] Batch 704, Loss 0.5919257998466492\n","[Training Epoch 0] Batch 705, Loss 0.5910627245903015\n","[Training Epoch 0] Batch 706, Loss 0.5915768146514893\n","[Training Epoch 0] Batch 707, Loss 0.5957614183425903\n","[Training Epoch 0] Batch 708, Loss 0.5995975732803345\n","[Training Epoch 0] Batch 709, Loss 0.5908575654029846\n","[Training Epoch 0] Batch 710, Loss 0.5892144441604614\n","[Training Epoch 0] Batch 711, Loss 0.5901078581809998\n","[Training Epoch 0] Batch 712, Loss 0.5990539789199829\n","[Training Epoch 0] Batch 713, Loss 0.5915777087211609\n","[Training Epoch 0] Batch 714, Loss 0.5844454765319824\n","[Training Epoch 0] Batch 715, Loss 0.591728687286377\n","[Training Epoch 0] Batch 716, Loss 0.5967198610305786\n","[Training Epoch 0] Batch 717, Loss 0.5867701768875122\n","[Training Epoch 0] Batch 718, Loss 0.594115138053894\n","[Training Epoch 0] Batch 719, Loss 0.5890704989433289\n","[Training Epoch 0] Batch 720, Loss 0.588057816028595\n","[Training Epoch 0] Batch 721, Loss 0.5875556468963623\n","[Training Epoch 0] Batch 722, Loss 0.5913100838661194\n","[Training Epoch 0] Batch 723, Loss 0.6010260581970215\n","[Training Epoch 0] Batch 724, Loss 0.585492730140686\n","[Training Epoch 0] Batch 725, Loss 0.5826704502105713\n","[Training Epoch 0] Batch 726, Loss 0.584351658821106\n","[Training Epoch 0] Batch 727, Loss 0.5881929993629456\n","[Training Epoch 0] Batch 728, Loss 0.5859880447387695\n","[Training Epoch 0] Batch 729, Loss 0.5964980721473694\n","[Training Epoch 0] Batch 730, Loss 0.5869682431221008\n","[Training Epoch 0] Batch 731, Loss 0.5870370268821716\n","[Training Epoch 0] Batch 732, Loss 0.5881514549255371\n","[Training Epoch 0] Batch 733, Loss 0.5810871720314026\n","[Training Epoch 0] Batch 734, Loss 0.5864192247390747\n","[Training Epoch 0] Batch 735, Loss 0.58906090259552\n","[Training Epoch 0] Batch 736, Loss 0.5902440547943115\n","[Training Epoch 0] Batch 737, Loss 0.5856694579124451\n","[Training Epoch 0] Batch 738, Loss 0.5896449089050293\n","[Training Epoch 0] Batch 739, Loss 0.5889729261398315\n","[Training Epoch 0] Batch 740, Loss 0.5843274593353271\n","[Training Epoch 0] Batch 741, Loss 0.5799494385719299\n","[Training Epoch 0] Batch 742, Loss 0.5922273397445679\n","[Training Epoch 0] Batch 743, Loss 0.5859153270721436\n","[Training Epoch 0] Batch 744, Loss 0.5966047644615173\n","[Training Epoch 0] Batch 745, Loss 0.592742919921875\n","[Training Epoch 0] Batch 746, Loss 0.5778920650482178\n","[Training Epoch 0] Batch 747, Loss 0.5861338376998901\n","[Training Epoch 0] Batch 748, Loss 0.5871500968933105\n","[Training Epoch 0] Batch 749, Loss 0.582406759262085\n","[Training Epoch 0] Batch 750, Loss 0.584915041923523\n","[Training Epoch 0] Batch 751, Loss 0.5796412229537964\n","[Training Epoch 0] Batch 752, Loss 0.5833561420440674\n","[Training Epoch 0] Batch 753, Loss 0.5843896269798279\n","[Training Epoch 0] Batch 754, Loss 0.5909067392349243\n","[Training Epoch 0] Batch 755, Loss 0.58223557472229\n","[Training Epoch 0] Batch 756, Loss 0.5873701572418213\n","[Training Epoch 0] Batch 757, Loss 0.583846390247345\n","[Training Epoch 0] Batch 758, Loss 0.5849869251251221\n","[Training Epoch 0] Batch 759, Loss 0.5851267576217651\n","[Training Epoch 0] Batch 760, Loss 0.5917630791664124\n","[Training Epoch 0] Batch 761, Loss 0.5940011739730835\n","[Training Epoch 0] Batch 762, Loss 0.5787373781204224\n","[Training Epoch 0] Batch 763, Loss 0.5781291723251343\n","[Training Epoch 0] Batch 764, Loss 0.5765824317932129\n","[Training Epoch 0] Batch 765, Loss 0.5751456022262573\n","[Training Epoch 0] Batch 766, Loss 0.585827112197876\n","[Training Epoch 0] Batch 767, Loss 0.5904268026351929\n","[Training Epoch 0] Batch 768, Loss 0.5855376720428467\n","[Training Epoch 0] Batch 769, Loss 0.5750941634178162\n","[Training Epoch 0] Batch 770, Loss 0.5827363729476929\n","[Training Epoch 0] Batch 771, Loss 0.588680624961853\n","[Training Epoch 0] Batch 772, Loss 0.5818431377410889\n","[Training Epoch 0] Batch 773, Loss 0.5852281451225281\n","[Training Epoch 0] Batch 774, Loss 0.5809266567230225\n","[Training Epoch 0] Batch 775, Loss 0.5924453735351562\n","[Training Epoch 0] Batch 776, Loss 0.5833451747894287\n","[Training Epoch 0] Batch 777, Loss 0.5822194814682007\n","[Training Epoch 0] Batch 778, Loss 0.5796972513198853\n","[Training Epoch 0] Batch 779, Loss 0.5823646783828735\n","[Training Epoch 0] Batch 780, Loss 0.5842252969741821\n","[Training Epoch 0] Batch 781, Loss 0.5839000940322876\n","[Training Epoch 0] Batch 782, Loss 0.5869547128677368\n","[Training Epoch 0] Batch 783, Loss 0.5771214365959167\n","[Training Epoch 0] Batch 784, Loss 0.5775211453437805\n","[Training Epoch 0] Batch 785, Loss 0.5865534543991089\n","[Training Epoch 0] Batch 786, Loss 0.5810074210166931\n","[Training Epoch 0] Batch 787, Loss 0.5778119564056396\n","[Training Epoch 0] Batch 788, Loss 0.5728309154510498\n","[Training Epoch 0] Batch 789, Loss 0.5781658291816711\n","[Training Epoch 0] Batch 790, Loss 0.5877173542976379\n","[Training Epoch 0] Batch 791, Loss 0.5837187767028809\n","[Training Epoch 0] Batch 792, Loss 0.5749292373657227\n","[Training Epoch 0] Batch 793, Loss 0.5780874490737915\n","[Training Epoch 0] Batch 794, Loss 0.5773665904998779\n","[Training Epoch 0] Batch 795, Loss 0.5736020803451538\n","[Training Epoch 0] Batch 796, Loss 0.5890539884567261\n","[Training Epoch 0] Batch 797, Loss 0.5755523443222046\n","[Training Epoch 0] Batch 798, Loss 0.5886109471321106\n","[Training Epoch 0] Batch 799, Loss 0.5707840919494629\n","[Training Epoch 0] Batch 800, Loss 0.5814304947853088\n","[Training Epoch 0] Batch 801, Loss 0.5749856233596802\n","[Training Epoch 0] Batch 802, Loss 0.5874011516571045\n","[Training Epoch 0] Batch 803, Loss 0.5903398990631104\n","[Training Epoch 0] Batch 804, Loss 0.5834611654281616\n","[Training Epoch 0] Batch 805, Loss 0.5738734006881714\n","[Training Epoch 0] Batch 806, Loss 0.5835872888565063\n","[Training Epoch 0] Batch 807, Loss 0.576948881149292\n","[Training Epoch 0] Batch 808, Loss 0.5740603804588318\n","[Training Epoch 0] Batch 809, Loss 0.5768761038780212\n","[Training Epoch 0] Batch 810, Loss 0.5775209069252014\n","[Training Epoch 0] Batch 811, Loss 0.5721027255058289\n","[Training Epoch 0] Batch 812, Loss 0.5700763463973999\n","[Training Epoch 0] Batch 813, Loss 0.5798091292381287\n","[Training Epoch 0] Batch 814, Loss 0.5713098645210266\n","[Training Epoch 0] Batch 815, Loss 0.575706958770752\n","[Training Epoch 0] Batch 816, Loss 0.5777687430381775\n","[Training Epoch 0] Batch 817, Loss 0.5814110040664673\n","[Training Epoch 0] Batch 818, Loss 0.5643247365951538\n","[Training Epoch 0] Batch 819, Loss 0.575583279132843\n","[Training Epoch 0] Batch 820, Loss 0.5700209140777588\n","[Training Epoch 0] Batch 821, Loss 0.5673493146896362\n","[Training Epoch 0] Batch 822, Loss 0.5709245204925537\n","[Training Epoch 0] Batch 823, Loss 0.5730481147766113\n","[Training Epoch 0] Batch 824, Loss 0.5761255025863647\n","[Training Epoch 0] Batch 825, Loss 0.5691114664077759\n","[Training Epoch 0] Batch 826, Loss 0.570189356803894\n","[Training Epoch 0] Batch 827, Loss 0.5738838911056519\n","[Training Epoch 0] Batch 828, Loss 0.5734437108039856\n","[Training Epoch 0] Batch 829, Loss 0.5750905275344849\n","[Training Epoch 0] Batch 830, Loss 0.5766080617904663\n","[Training Epoch 0] Batch 831, Loss 0.5725374221801758\n","[Training Epoch 0] Batch 832, Loss 0.5757569074630737\n","[Training Epoch 0] Batch 833, Loss 0.5563336610794067\n","[Training Epoch 0] Batch 834, Loss 0.5784620046615601\n","[Training Epoch 0] Batch 835, Loss 0.567696213722229\n","[Training Epoch 0] Batch 836, Loss 0.5779403448104858\n","[Training Epoch 0] Batch 837, Loss 0.5682389736175537\n","[Training Epoch 0] Batch 838, Loss 0.5750170946121216\n","[Training Epoch 0] Batch 839, Loss 0.5729594826698303\n","[Training Epoch 0] Batch 840, Loss 0.5732188820838928\n","[Training Epoch 0] Batch 841, Loss 0.568426251411438\n","[Training Epoch 0] Batch 842, Loss 0.5643553137779236\n","[Training Epoch 0] Batch 843, Loss 0.5738070011138916\n","[Training Epoch 0] Batch 844, Loss 0.5744161605834961\n","[Training Epoch 0] Batch 845, Loss 0.5698634386062622\n","[Training Epoch 0] Batch 846, Loss 0.5808975696563721\n","[Training Epoch 0] Batch 847, Loss 0.5754492282867432\n","[Training Epoch 0] Batch 848, Loss 0.5814987421035767\n","[Training Epoch 0] Batch 849, Loss 0.5680602788925171\n","[Training Epoch 0] Batch 850, Loss 0.5743426084518433\n","[Training Epoch 0] Batch 851, Loss 0.5800654888153076\n","[Training Epoch 0] Batch 852, Loss 0.5686476230621338\n","[Training Epoch 0] Batch 853, Loss 0.5791418552398682\n","[Training Epoch 0] Batch 854, Loss 0.5837469696998596\n","[Training Epoch 0] Batch 855, Loss 0.572940468788147\n","[Training Epoch 0] Batch 856, Loss 0.5722161531448364\n","[Training Epoch 0] Batch 857, Loss 0.5806338787078857\n","[Training Epoch 0] Batch 858, Loss 0.5680392384529114\n","[Training Epoch 0] Batch 859, Loss 0.5726584196090698\n","[Training Epoch 0] Batch 860, Loss 0.573376476764679\n","[Training Epoch 0] Batch 861, Loss 0.5686616897583008\n","[Training Epoch 0] Batch 862, Loss 0.5776619911193848\n","[Training Epoch 0] Batch 863, Loss 0.5645287036895752\n","[Training Epoch 0] Batch 864, Loss 0.5761045217514038\n","[Training Epoch 0] Batch 865, Loss 0.5758532285690308\n","[Training Epoch 0] Batch 866, Loss 0.5611178278923035\n","[Training Epoch 0] Batch 867, Loss 0.5716046094894409\n","[Training Epoch 0] Batch 868, Loss 0.5684630870819092\n","[Training Epoch 0] Batch 869, Loss 0.5683140754699707\n","[Training Epoch 0] Batch 870, Loss 0.5771247148513794\n","[Training Epoch 0] Batch 871, Loss 0.577392578125\n","[Training Epoch 0] Batch 872, Loss 0.5800639390945435\n","[Training Epoch 0] Batch 873, Loss 0.5627488493919373\n","[Training Epoch 0] Batch 874, Loss 0.5799529552459717\n","[Training Epoch 0] Batch 875, Loss 0.5719213485717773\n","[Training Epoch 0] Batch 876, Loss 0.5760388970375061\n","[Training Epoch 0] Batch 877, Loss 0.5758045315742493\n","[Training Epoch 0] Batch 878, Loss 0.5699695348739624\n","[Training Epoch 0] Batch 879, Loss 0.5725725889205933\n","[Training Epoch 0] Batch 880, Loss 0.5630794763565063\n","[Training Epoch 0] Batch 881, Loss 0.5615217685699463\n","[Training Epoch 0] Batch 882, Loss 0.5761657357215881\n","[Training Epoch 0] Batch 883, Loss 0.5755332112312317\n","[Training Epoch 0] Batch 884, Loss 0.5602909922599792\n","[Training Epoch 0] Batch 885, Loss 0.5712571144104004\n","[Training Epoch 0] Batch 886, Loss 0.5625807046890259\n","[Training Epoch 0] Batch 887, Loss 0.5606764554977417\n","[Training Epoch 0] Batch 888, Loss 0.571272611618042\n","[Training Epoch 0] Batch 889, Loss 0.556265652179718\n","[Training Epoch 0] Batch 890, Loss 0.5644428133964539\n","[Training Epoch 0] Batch 891, Loss 0.5638077259063721\n","[Training Epoch 0] Batch 892, Loss 0.5596360564231873\n","[Training Epoch 0] Batch 893, Loss 0.5692810416221619\n","[Training Epoch 0] Batch 894, Loss 0.5627371072769165\n","[Training Epoch 0] Batch 895, Loss 0.5779942870140076\n","[Training Epoch 0] Batch 896, Loss 0.5663650035858154\n","[Training Epoch 0] Batch 897, Loss 0.5661362409591675\n","[Training Epoch 0] Batch 898, Loss 0.5666108727455139\n","[Training Epoch 0] Batch 899, Loss 0.5596487522125244\n","[Training Epoch 0] Batch 900, Loss 0.5698167085647583\n","[Training Epoch 0] Batch 901, Loss 0.5701863765716553\n","[Training Epoch 0] Batch 902, Loss 0.5739365220069885\n","[Training Epoch 0] Batch 903, Loss 0.5651691555976868\n","[Training Epoch 0] Batch 904, Loss 0.559154212474823\n","[Training Epoch 0] Batch 905, Loss 0.5710744261741638\n","[Training Epoch 0] Batch 906, Loss 0.5662153363227844\n","[Training Epoch 0] Batch 907, Loss 0.5646371841430664\n","[Training Epoch 0] Batch 908, Loss 0.5827087163925171\n","[Training Epoch 0] Batch 909, Loss 0.5695458054542542\n","[Training Epoch 0] Batch 910, Loss 0.5741989016532898\n","[Training Epoch 0] Batch 911, Loss 0.5786682367324829\n","[Training Epoch 0] Batch 912, Loss 0.5742342472076416\n","[Training Epoch 0] Batch 913, Loss 0.558022141456604\n","[Training Epoch 0] Batch 914, Loss 0.5679051876068115\n","[Training Epoch 0] Batch 915, Loss 0.5646016001701355\n","[Training Epoch 0] Batch 916, Loss 0.5598235726356506\n","[Training Epoch 0] Batch 917, Loss 0.5796481370925903\n","[Training Epoch 0] Batch 918, Loss 0.5755905508995056\n","[Training Epoch 0] Batch 919, Loss 0.5631676316261292\n","[Training Epoch 0] Batch 920, Loss 0.5713643431663513\n","[Training Epoch 0] Batch 921, Loss 0.561348557472229\n","[Training Epoch 0] Batch 922, Loss 0.5803351402282715\n","[Training Epoch 0] Batch 923, Loss 0.5659650564193726\n","[Training Epoch 0] Batch 924, Loss 0.571935772895813\n","[Training Epoch 0] Batch 925, Loss 0.5722322463989258\n","[Training Epoch 0] Batch 926, Loss 0.5640206933021545\n","[Training Epoch 0] Batch 927, Loss 0.5578473806381226\n","[Training Epoch 0] Batch 928, Loss 0.5507718324661255\n","[Training Epoch 0] Batch 929, Loss 0.5619311928749084\n","[Training Epoch 0] Batch 930, Loss 0.5615725517272949\n","[Training Epoch 0] Batch 931, Loss 0.5624441504478455\n","[Training Epoch 0] Batch 932, Loss 0.559431254863739\n","[Training Epoch 0] Batch 933, Loss 0.5537596940994263\n","[Training Epoch 0] Batch 934, Loss 0.5604705810546875\n","[Training Epoch 0] Batch 935, Loss 0.5655401945114136\n","[Training Epoch 0] Batch 936, Loss 0.5626980066299438\n","[Training Epoch 0] Batch 937, Loss 0.5656270980834961\n","[Training Epoch 0] Batch 938, Loss 0.5638433694839478\n","[Training Epoch 0] Batch 939, Loss 0.5726033449172974\n","[Training Epoch 0] Batch 940, Loss 0.5474284887313843\n","[Training Epoch 0] Batch 941, Loss 0.5558123588562012\n","[Training Epoch 0] Batch 942, Loss 0.5520525574684143\n","[Training Epoch 0] Batch 943, Loss 0.5562766790390015\n","[Training Epoch 0] Batch 944, Loss 0.5638977289199829\n","[Training Epoch 0] Batch 945, Loss 0.5682557821273804\n","[Training Epoch 0] Batch 946, Loss 0.5631507039070129\n","[Training Epoch 0] Batch 947, Loss 0.5750753283500671\n","[Training Epoch 0] Batch 948, Loss 0.5598369836807251\n","[Training Epoch 0] Batch 949, Loss 0.5596900582313538\n","[Training Epoch 0] Batch 950, Loss 0.5500907301902771\n","[Training Epoch 0] Batch 951, Loss 0.5709686279296875\n","[Training Epoch 0] Batch 952, Loss 0.5620587468147278\n","[Training Epoch 0] Batch 953, Loss 0.5594868063926697\n","[Training Epoch 0] Batch 954, Loss 0.5652065277099609\n","[Training Epoch 0] Batch 955, Loss 0.567118763923645\n","[Training Epoch 0] Batch 956, Loss 0.5548195838928223\n","[Training Epoch 0] Batch 957, Loss 0.5765730738639832\n","[Training Epoch 0] Batch 958, Loss 0.5571777820587158\n","[Training Epoch 0] Batch 959, Loss 0.5639283657073975\n","[Training Epoch 0] Batch 960, Loss 0.5622912645339966\n","[Training Epoch 0] Batch 961, Loss 0.5671342611312866\n","[Training Epoch 0] Batch 962, Loss 0.5736227035522461\n","[Training Epoch 0] Batch 963, Loss 0.5698362588882446\n","[Training Epoch 0] Batch 964, Loss 0.5549293756484985\n","[Training Epoch 0] Batch 965, Loss 0.5637486577033997\n","[Training Epoch 0] Batch 966, Loss 0.561638593673706\n","[Training Epoch 0] Batch 967, Loss 0.5483500361442566\n","[Training Epoch 0] Batch 968, Loss 0.5582271814346313\n","[Training Epoch 0] Batch 969, Loss 0.5611351728439331\n","[Training Epoch 0] Batch 970, Loss 0.5531086921691895\n","[Training Epoch 0] Batch 971, Loss 0.5552178621292114\n","[Training Epoch 0] Batch 972, Loss 0.5566033720970154\n","[Training Epoch 0] Batch 973, Loss 0.5739067792892456\n","[Training Epoch 0] Batch 974, Loss 0.5627228617668152\n","[Training Epoch 0] Batch 975, Loss 0.5531995296478271\n","[Training Epoch 0] Batch 976, Loss 0.5506608486175537\n","[Training Epoch 0] Batch 977, Loss 0.5604766607284546\n","[Training Epoch 0] Batch 978, Loss 0.5701658129692078\n","[Training Epoch 0] Batch 979, Loss 0.5647096633911133\n","[Training Epoch 0] Batch 980, Loss 0.5562925338745117\n","[Training Epoch 0] Batch 981, Loss 0.5580631494522095\n","[Training Epoch 0] Batch 982, Loss 0.5628177523612976\n","[Training Epoch 0] Batch 983, Loss 0.5545220375061035\n","[Training Epoch 0] Batch 984, Loss 0.5562087297439575\n","[Training Epoch 0] Batch 985, Loss 0.5623594522476196\n","[Training Epoch 0] Batch 986, Loss 0.5630509853363037\n","[Training Epoch 0] Batch 987, Loss 0.567305326461792\n","[Training Epoch 0] Batch 988, Loss 0.5462974309921265\n","[Training Epoch 0] Batch 989, Loss 0.5565109252929688\n","[Training Epoch 0] Batch 990, Loss 0.5487269759178162\n","[Training Epoch 0] Batch 991, Loss 0.5504583120346069\n","[Training Epoch 0] Batch 992, Loss 0.5679702162742615\n","[Training Epoch 0] Batch 993, Loss 0.5592969655990601\n","[Training Epoch 0] Batch 994, Loss 0.5524365901947021\n","[Training Epoch 0] Batch 995, Loss 0.5563112497329712\n","[Training Epoch 0] Batch 996, Loss 0.5521829128265381\n","[Training Epoch 0] Batch 997, Loss 0.5507509708404541\n","[Training Epoch 0] Batch 998, Loss 0.5604431629180908\n","[Training Epoch 0] Batch 999, Loss 0.5527125000953674\n","[Training Epoch 0] Batch 1000, Loss 0.5502692461013794\n","[Training Epoch 0] Batch 1001, Loss 0.5561777353286743\n","[Training Epoch 0] Batch 1002, Loss 0.5486183166503906\n","[Training Epoch 0] Batch 1003, Loss 0.5555840730667114\n","[Training Epoch 0] Batch 1004, Loss 0.5605943202972412\n","[Training Epoch 0] Batch 1005, Loss 0.5655623078346252\n","[Training Epoch 0] Batch 1006, Loss 0.5592716932296753\n","[Training Epoch 0] Batch 1007, Loss 0.5524712800979614\n","[Training Epoch 0] Batch 1008, Loss 0.5602200031280518\n","[Training Epoch 0] Batch 1009, Loss 0.5542003512382507\n","[Training Epoch 0] Batch 1010, Loss 0.555419921875\n","[Training Epoch 0] Batch 1011, Loss 0.5500342845916748\n","[Training Epoch 0] Batch 1012, Loss 0.5381201505661011\n","[Training Epoch 0] Batch 1013, Loss 0.5495911836624146\n","[Training Epoch 0] Batch 1014, Loss 0.554401159286499\n","[Training Epoch 0] Batch 1015, Loss 0.5513412952423096\n","[Training Epoch 0] Batch 1016, Loss 0.5578163862228394\n","[Training Epoch 0] Batch 1017, Loss 0.5636652708053589\n","[Training Epoch 0] Batch 1018, Loss 0.5514859557151794\n","[Training Epoch 0] Batch 1019, Loss 0.547143816947937\n","[Training Epoch 0] Batch 1020, Loss 0.5560873746871948\n","[Training Epoch 0] Batch 1021, Loss 0.548410177230835\n","[Training Epoch 0] Batch 1022, Loss 0.5609162449836731\n","[Training Epoch 0] Batch 1023, Loss 0.5418268442153931\n","[Training Epoch 0] Batch 1024, Loss 0.550365149974823\n","[Training Epoch 0] Batch 1025, Loss 0.5545007586479187\n","[Training Epoch 0] Batch 1026, Loss 0.54029381275177\n","[Training Epoch 0] Batch 1027, Loss 0.5500795245170593\n","[Training Epoch 0] Batch 1028, Loss 0.5461840629577637\n","[Training Epoch 0] Batch 1029, Loss 0.5406537055969238\n","[Training Epoch 0] Batch 1030, Loss 0.5540364980697632\n","[Training Epoch 0] Batch 1031, Loss 0.5360828638076782\n","[Training Epoch 0] Batch 1032, Loss 0.5386242866516113\n","[Training Epoch 0] Batch 1033, Loss 0.5437834858894348\n","[Training Epoch 0] Batch 1034, Loss 0.5482164621353149\n","[Training Epoch 0] Batch 1035, Loss 0.5381330251693726\n","[Training Epoch 0] Batch 1036, Loss 0.5508718490600586\n","[Training Epoch 0] Batch 1037, Loss 0.548818051815033\n","[Training Epoch 0] Batch 1038, Loss 0.5562113523483276\n","[Training Epoch 0] Batch 1039, Loss 0.5497945547103882\n","[Training Epoch 0] Batch 1040, Loss 0.5556064248085022\n","[Training Epoch 0] Batch 1041, Loss 0.5410268306732178\n","[Training Epoch 0] Batch 1042, Loss 0.5678814053535461\n","[Training Epoch 0] Batch 1043, Loss 0.5681965947151184\n","[Training Epoch 0] Batch 1044, Loss 0.5458750128746033\n","[Training Epoch 0] Batch 1045, Loss 0.5468742847442627\n","[Training Epoch 0] Batch 1046, Loss 0.568133533000946\n","[Training Epoch 0] Batch 1047, Loss 0.5523825883865356\n","[Training Epoch 0] Batch 1048, Loss 0.5587015748023987\n","[Training Epoch 0] Batch 1049, Loss 0.5620874166488647\n","[Training Epoch 0] Batch 1050, Loss 0.5509330034255981\n","[Training Epoch 0] Batch 1051, Loss 0.5565279126167297\n","[Training Epoch 0] Batch 1052, Loss 0.5482144951820374\n","[Training Epoch 0] Batch 1053, Loss 0.5562369227409363\n","[Training Epoch 0] Batch 1054, Loss 0.541715681552887\n","[Training Epoch 0] Batch 1055, Loss 0.5452487468719482\n","[Training Epoch 0] Batch 1056, Loss 0.5622120499610901\n","[Training Epoch 0] Batch 1057, Loss 0.5468397736549377\n","[Training Epoch 0] Batch 1058, Loss 0.5664926171302795\n","[Training Epoch 0] Batch 1059, Loss 0.5633599758148193\n","[Training Epoch 0] Batch 1060, Loss 0.553508996963501\n","[Training Epoch 0] Batch 1061, Loss 0.5473427772521973\n","[Training Epoch 0] Batch 1062, Loss 0.5376132726669312\n","[Training Epoch 0] Batch 1063, Loss 0.5641129612922668\n","[Training Epoch 0] Batch 1064, Loss 0.546261191368103\n","[Training Epoch 0] Batch 1065, Loss 0.5576674938201904\n","[Training Epoch 0] Batch 1066, Loss 0.5527486801147461\n","[Training Epoch 0] Batch 1067, Loss 0.5622032284736633\n","[Training Epoch 0] Batch 1068, Loss 0.544167697429657\n","[Training Epoch 0] Batch 1069, Loss 0.5413712859153748\n","[Training Epoch 0] Batch 1070, Loss 0.561917245388031\n","[Training Epoch 0] Batch 1071, Loss 0.5534714460372925\n","[Training Epoch 0] Batch 1072, Loss 0.565351128578186\n","[Training Epoch 0] Batch 1073, Loss 0.5519204139709473\n","[Training Epoch 0] Batch 1074, Loss 0.5469115972518921\n","[Training Epoch 0] Batch 1075, Loss 0.5486135482788086\n","[Training Epoch 0] Batch 1076, Loss 0.540805995464325\n","[Training Epoch 0] Batch 1077, Loss 0.5444961786270142\n","[Training Epoch 0] Batch 1078, Loss 0.547773540019989\n","[Training Epoch 0] Batch 1079, Loss 0.5526281595230103\n","[Training Epoch 0] Batch 1080, Loss 0.5327162146568298\n","[Training Epoch 0] Batch 1081, Loss 0.5654786229133606\n","[Training Epoch 0] Batch 1082, Loss 0.5527712106704712\n","[Training Epoch 0] Batch 1083, Loss 0.544004499912262\n","[Training Epoch 0] Batch 1084, Loss 0.5556158423423767\n","[Training Epoch 0] Batch 1085, Loss 0.5517284870147705\n","[Training Epoch 0] Batch 1086, Loss 0.5510815382003784\n","[Training Epoch 0] Batch 1087, Loss 0.5367036461830139\n","[Training Epoch 0] Batch 1088, Loss 0.5422371625900269\n","[Training Epoch 0] Batch 1089, Loss 0.5350545644760132\n","[Training Epoch 0] Batch 1090, Loss 0.5466688275337219\n","[Training Epoch 0] Batch 1091, Loss 0.5499366521835327\n","[Training Epoch 0] Batch 1092, Loss 0.5504765510559082\n","[Training Epoch 0] Batch 1093, Loss 0.5489534139633179\n","[Training Epoch 0] Batch 1094, Loss 0.5579995512962341\n","[Training Epoch 0] Batch 1095, Loss 0.5508667826652527\n","[Training Epoch 0] Batch 1096, Loss 0.5519795417785645\n","[Training Epoch 0] Batch 1097, Loss 0.5407924652099609\n","[Training Epoch 0] Batch 1098, Loss 0.5538597106933594\n","[Training Epoch 0] Batch 1099, Loss 0.5569926500320435\n","[Training Epoch 0] Batch 1100, Loss 0.5397785902023315\n","[Training Epoch 0] Batch 1101, Loss 0.5292661190032959\n","[Training Epoch 0] Batch 1102, Loss 0.5473949313163757\n","[Training Epoch 0] Batch 1103, Loss 0.5336743593215942\n","[Training Epoch 0] Batch 1104, Loss 0.5473537445068359\n","[Training Epoch 0] Batch 1105, Loss 0.5361297726631165\n","[Training Epoch 0] Batch 1106, Loss 0.5418640375137329\n","[Training Epoch 0] Batch 1107, Loss 0.5418877005577087\n","[Training Epoch 0] Batch 1108, Loss 0.5456830263137817\n","[Training Epoch 0] Batch 1109, Loss 0.5437474250793457\n","[Training Epoch 0] Batch 1110, Loss 0.5429600477218628\n","[Training Epoch 0] Batch 1111, Loss 0.5382043719291687\n","[Training Epoch 0] Batch 1112, Loss 0.5488119721412659\n","[Training Epoch 0] Batch 1113, Loss 0.5465381145477295\n","[Training Epoch 0] Batch 1114, Loss 0.5505182147026062\n","[Training Epoch 0] Batch 1115, Loss 0.535111665725708\n","[Training Epoch 0] Batch 1116, Loss 0.5410692691802979\n","[Training Epoch 0] Batch 1117, Loss 0.5308400392532349\n","[Training Epoch 0] Batch 1118, Loss 0.5448819398880005\n","[Training Epoch 0] Batch 1119, Loss 0.5494535565376282\n","[Training Epoch 0] Batch 1120, Loss 0.5547930002212524\n","[Training Epoch 0] Batch 1121, Loss 0.5518988370895386\n","[Training Epoch 0] Batch 1122, Loss 0.5251820087432861\n","[Training Epoch 0] Batch 1123, Loss 0.5524746179580688\n","[Training Epoch 0] Batch 1124, Loss 0.5456861257553101\n","[Training Epoch 0] Batch 1125, Loss 0.5469132661819458\n","[Training Epoch 0] Batch 1126, Loss 0.5517162680625916\n","[Training Epoch 0] Batch 1127, Loss 0.5466763973236084\n","[Training Epoch 0] Batch 1128, Loss 0.5501133799552917\n","[Training Epoch 0] Batch 1129, Loss 0.5183343291282654\n","[Training Epoch 0] Batch 1130, Loss 0.5445810556411743\n","[Training Epoch 0] Batch 1131, Loss 0.5410329103469849\n","[Training Epoch 0] Batch 1132, Loss 0.5472248792648315\n","[Training Epoch 0] Batch 1133, Loss 0.5354387760162354\n","[Training Epoch 0] Batch 1134, Loss 0.5495419502258301\n","[Training Epoch 0] Batch 1135, Loss 0.5610524415969849\n","[Training Epoch 0] Batch 1136, Loss 0.5413268804550171\n","[Training Epoch 0] Batch 1137, Loss 0.5465906262397766\n","[Training Epoch 0] Batch 1138, Loss 0.5398175716400146\n","[Training Epoch 0] Batch 1139, Loss 0.5553555488586426\n","[Training Epoch 0] Batch 1140, Loss 0.5456809401512146\n","[Training Epoch 0] Batch 1141, Loss 0.5318149328231812\n","[Training Epoch 0] Batch 1142, Loss 0.5375152826309204\n","[Training Epoch 0] Batch 1143, Loss 0.5497472286224365\n","[Training Epoch 0] Batch 1144, Loss 0.5365939140319824\n","[Training Epoch 0] Batch 1145, Loss 0.5521990656852722\n","[Training Epoch 0] Batch 1146, Loss 0.5413165092468262\n","[Training Epoch 0] Batch 1147, Loss 0.5487504005432129\n","[Training Epoch 0] Batch 1148, Loss 0.5217808485031128\n","[Training Epoch 0] Batch 1149, Loss 0.5416425466537476\n","[Training Epoch 0] Batch 1150, Loss 0.5478461980819702\n","[Training Epoch 0] Batch 1151, Loss 0.5278196334838867\n","[Training Epoch 0] Batch 1152, Loss 0.559930682182312\n","[Training Epoch 0] Batch 1153, Loss 0.5487930774688721\n","[Training Epoch 0] Batch 1154, Loss 0.5404454469680786\n","[Training Epoch 0] Batch 1155, Loss 0.5335936546325684\n","[Training Epoch 0] Batch 1156, Loss 0.546549379825592\n","[Training Epoch 0] Batch 1157, Loss 0.5360767841339111\n","[Training Epoch 0] Batch 1158, Loss 0.5486302375793457\n","[Training Epoch 0] Batch 1159, Loss 0.5462515950202942\n","[Training Epoch 0] Batch 1160, Loss 0.5351508855819702\n","[Training Epoch 0] Batch 1161, Loss 0.5435301065444946\n","[Training Epoch 0] Batch 1162, Loss 0.5434178113937378\n","[Training Epoch 0] Batch 1163, Loss 0.5286746025085449\n","[Training Epoch 0] Batch 1164, Loss 0.5411291122436523\n","[Training Epoch 0] Batch 1165, Loss 0.5321049690246582\n","[Training Epoch 0] Batch 1166, Loss 0.544448971748352\n","[Training Epoch 0] Batch 1167, Loss 0.5457513332366943\n","[Training Epoch 0] Batch 1168, Loss 0.533035159111023\n","[Training Epoch 0] Batch 1169, Loss 0.5498044490814209\n","[Training Epoch 0] Batch 1170, Loss 0.5322558283805847\n","[Training Epoch 0] Batch 1171, Loss 0.5439574718475342\n","[Training Epoch 0] Batch 1172, Loss 0.5412006378173828\n","[Training Epoch 0] Batch 1173, Loss 0.5355316400527954\n","[Training Epoch 0] Batch 1174, Loss 0.5347048044204712\n","[Training Epoch 0] Batch 1175, Loss 0.5317835807800293\n","[Training Epoch 0] Batch 1176, Loss 0.5427608489990234\n","[Training Epoch 0] Batch 1177, Loss 0.5487000942230225\n","[Training Epoch 0] Batch 1178, Loss 0.5436578392982483\n","[Training Epoch 0] Batch 1179, Loss 0.5390883088111877\n","[Training Epoch 0] Batch 1180, Loss 0.5419491529464722\n","[Training Epoch 0] Batch 1181, Loss 0.5431413054466248\n","[Training Epoch 0] Batch 1182, Loss 0.5362575054168701\n","[Training Epoch 0] Batch 1183, Loss 0.5465853214263916\n","[Training Epoch 0] Batch 1184, Loss 0.5359249711036682\n","[Training Epoch 0] Batch 1185, Loss 0.5345395803451538\n","[Training Epoch 0] Batch 1186, Loss 0.5501984357833862\n","[Training Epoch 0] Batch 1187, Loss 0.5421703457832336\n","[Training Epoch 0] Batch 1188, Loss 0.5421640276908875\n","[Training Epoch 0] Batch 1189, Loss 0.5431978702545166\n","[Training Epoch 0] Batch 1190, Loss 0.5420694351196289\n","[Training Epoch 0] Batch 1191, Loss 0.5460432767868042\n","[Training Epoch 0] Batch 1192, Loss 0.5417888164520264\n","[Training Epoch 0] Batch 1193, Loss 0.5572957396507263\n","[Training Epoch 0] Batch 1194, Loss 0.5331323742866516\n","[Training Epoch 0] Batch 1195, Loss 0.5579676032066345\n","[Training Epoch 0] Batch 1196, Loss 0.5293576121330261\n","[Training Epoch 0] Batch 1197, Loss 0.5399338603019714\n","[Training Epoch 0] Batch 1198, Loss 0.5333842039108276\n","[Training Epoch 0] Batch 1199, Loss 0.5428174138069153\n","[Training Epoch 0] Batch 1200, Loss 0.5362883806228638\n","[Training Epoch 0] Batch 1201, Loss 0.5419824123382568\n","[Training Epoch 0] Batch 1202, Loss 0.5359007120132446\n","[Training Epoch 0] Batch 1203, Loss 0.5397740006446838\n","[Training Epoch 0] Batch 1204, Loss 0.5185313820838928\n","[Training Epoch 0] Batch 1205, Loss 0.5379878878593445\n","[Training Epoch 0] Batch 1206, Loss 0.5350756645202637\n","[Training Epoch 0] Batch 1207, Loss 0.5326340198516846\n","[Training Epoch 0] Batch 1208, Loss 0.5520792603492737\n","[Training Epoch 0] Batch 1209, Loss 0.5357905626296997\n","[Training Epoch 0] Batch 1210, Loss 0.5420010685920715\n","[Training Epoch 0] Batch 1211, Loss 0.5452678203582764\n","[Training Epoch 0] Batch 1212, Loss 0.5288718938827515\n","[Training Epoch 0] Batch 1213, Loss 0.518336296081543\n","[Training Epoch 0] Batch 1214, Loss 0.5339478254318237\n","[Training Epoch 0] Batch 1215, Loss 0.5338118672370911\n","[Training Epoch 0] Batch 1216, Loss 0.5301851034164429\n","[Training Epoch 0] Batch 1217, Loss 0.5422981977462769\n","[Training Epoch 0] Batch 1218, Loss 0.5409284234046936\n","[Training Epoch 0] Batch 1219, Loss 0.5647631883621216\n","[Training Epoch 0] Batch 1220, Loss 0.5332436561584473\n","[Training Epoch 0] Batch 1221, Loss 0.5543514490127563\n","[Training Epoch 0] Batch 1222, Loss 0.5444630980491638\n","[Training Epoch 0] Batch 1223, Loss 0.5275131464004517\n","[Training Epoch 0] Batch 1224, Loss 0.5431129932403564\n","[Training Epoch 0] Batch 1225, Loss 0.5364334583282471\n","[Training Epoch 0] Batch 1226, Loss 0.536540150642395\n","[Training Epoch 0] Batch 1227, Loss 0.551244854927063\n","[Training Epoch 0] Batch 1228, Loss 0.5239589214324951\n","[Training Epoch 0] Batch 1229, Loss 0.5281476974487305\n","[Training Epoch 0] Batch 1230, Loss 0.5434690713882446\n","[Training Epoch 0] Batch 1231, Loss 0.5413347482681274\n","[Training Epoch 0] Batch 1232, Loss 0.54360032081604\n","[Training Epoch 0] Batch 1233, Loss 0.5330613851547241\n","[Training Epoch 0] Batch 1234, Loss 0.536594033241272\n","[Training Epoch 0] Batch 1235, Loss 0.5503497123718262\n","[Training Epoch 0] Batch 1236, Loss 0.5302457809448242\n","[Training Epoch 0] Batch 1237, Loss 0.5271975994110107\n","[Training Epoch 0] Batch 1238, Loss 0.5287522077560425\n","[Training Epoch 0] Batch 1239, Loss 0.5252763628959656\n","[Training Epoch 0] Batch 1240, Loss 0.526512622833252\n","[Training Epoch 0] Batch 1241, Loss 0.5512584447860718\n","[Training Epoch 0] Batch 1242, Loss 0.5277352333068848\n","[Training Epoch 0] Batch 1243, Loss 0.5050860643386841\n","[Training Epoch 0] Batch 1244, Loss 0.5529384613037109\n","[Training Epoch 0] Batch 1245, Loss 0.5309842824935913\n","[Training Epoch 0] Batch 1246, Loss 0.5401079058647156\n","[Training Epoch 0] Batch 1247, Loss 0.5365439653396606\n","[Training Epoch 0] Batch 1248, Loss 0.5558886528015137\n","[Training Epoch 0] Batch 1249, Loss 0.5505496859550476\n","[Training Epoch 0] Batch 1250, Loss 0.5302048921585083\n","[Training Epoch 0] Batch 1251, Loss 0.5264238715171814\n","[Training Epoch 0] Batch 1252, Loss 0.5613951683044434\n","[Training Epoch 0] Batch 1253, Loss 0.5440149307250977\n","[Training Epoch 0] Batch 1254, Loss 0.5256187915802002\n","[Training Epoch 0] Batch 1255, Loss 0.5280085206031799\n","[Training Epoch 0] Batch 1256, Loss 0.5408316850662231\n","[Training Epoch 0] Batch 1257, Loss 0.5345956087112427\n","[Training Epoch 0] Batch 1258, Loss 0.5387015342712402\n","[Training Epoch 0] Batch 1259, Loss 0.526354193687439\n","[Training Epoch 0] Batch 1260, Loss 0.5307329893112183\n","[Training Epoch 0] Batch 1261, Loss 0.5419217348098755\n","[Training Epoch 0] Batch 1262, Loss 0.5065085887908936\n","[Training Epoch 0] Batch 1263, Loss 0.5298570990562439\n","[Training Epoch 0] Batch 1264, Loss 0.5323789119720459\n","[Training Epoch 0] Batch 1265, Loss 0.549553394317627\n","[Training Epoch 0] Batch 1266, Loss 0.5293743014335632\n","[Training Epoch 0] Batch 1267, Loss 0.5394672155380249\n","[Training Epoch 0] Batch 1268, Loss 0.5494861006736755\n","[Training Epoch 0] Batch 1269, Loss 0.54937344789505\n","[Training Epoch 0] Batch 1270, Loss 0.5441463589668274\n","[Training Epoch 0] Batch 1271, Loss 0.5174555778503418\n","[Training Epoch 0] Batch 1272, Loss 0.5288693308830261\n","[Training Epoch 0] Batch 1273, Loss 0.5317662954330444\n","[Training Epoch 0] Batch 1274, Loss 0.526922881603241\n","[Training Epoch 0] Batch 1275, Loss 0.5386044383049011\n","[Training Epoch 0] Batch 1276, Loss 0.5382890701293945\n","[Training Epoch 0] Batch 1277, Loss 0.5244543552398682\n","[Training Epoch 0] Batch 1278, Loss 0.5238468647003174\n","[Training Epoch 0] Batch 1279, Loss 0.5299078226089478\n","[Training Epoch 0] Batch 1280, Loss 0.5528068542480469\n","[Training Epoch 0] Batch 1281, Loss 0.5253095626831055\n","[Training Epoch 0] Batch 1282, Loss 0.5398784875869751\n","[Training Epoch 0] Batch 1283, Loss 0.5301558971405029\n","[Training Epoch 0] Batch 1284, Loss 0.522108793258667\n","[Training Epoch 0] Batch 1285, Loss 0.539617657661438\n","[Training Epoch 0] Batch 1286, Loss 0.516414225101471\n","[Training Epoch 0] Batch 1287, Loss 0.524978518486023\n","[Training Epoch 0] Batch 1288, Loss 0.5342077016830444\n","[Training Epoch 0] Batch 1289, Loss 0.5567912459373474\n","[Training Epoch 0] Batch 1290, Loss 0.5267682075500488\n","[Training Epoch 0] Batch 1291, Loss 0.5251871347427368\n","[Training Epoch 0] Batch 1292, Loss 0.525996744632721\n","[Training Epoch 0] Batch 1293, Loss 0.5336017608642578\n","[Training Epoch 0] Batch 1294, Loss 0.5242506265640259\n","[Training Epoch 0] Batch 1295, Loss 0.5277117490768433\n","[Training Epoch 0] Batch 1296, Loss 0.5334745049476624\n","[Training Epoch 0] Batch 1297, Loss 0.5249436497688293\n","[Training Epoch 0] Batch 1298, Loss 0.5334024429321289\n","[Training Epoch 0] Batch 1299, Loss 0.5368817448616028\n","[Training Epoch 0] Batch 1300, Loss 0.533135175704956\n","[Training Epoch 0] Batch 1301, Loss 0.5324833393096924\n","[Training Epoch 0] Batch 1302, Loss 0.5354834198951721\n","[Training Epoch 0] Batch 1303, Loss 0.5329194068908691\n","[Training Epoch 0] Batch 1304, Loss 0.5389293432235718\n","[Training Epoch 0] Batch 1305, Loss 0.5437809824943542\n","[Training Epoch 0] Batch 1306, Loss 0.5220396518707275\n","[Training Epoch 0] Batch 1307, Loss 0.5264313817024231\n","[Training Epoch 0] Batch 1308, Loss 0.5388277769088745\n","[Training Epoch 0] Batch 1309, Loss 0.5530090928077698\n","[Training Epoch 0] Batch 1310, Loss 0.5305831432342529\n","[Training Epoch 0] Batch 1311, Loss 0.533258855342865\n","[Training Epoch 0] Batch 1312, Loss 0.5431894659996033\n","[Training Epoch 0] Batch 1313, Loss 0.5243271589279175\n","[Training Epoch 0] Batch 1314, Loss 0.5240669250488281\n","[Training Epoch 0] Batch 1315, Loss 0.5269209146499634\n","[Training Epoch 0] Batch 1316, Loss 0.5336766242980957\n","[Training Epoch 0] Batch 1317, Loss 0.5335855484008789\n","[Training Epoch 0] Batch 1318, Loss 0.5257977247238159\n","[Training Epoch 0] Batch 1319, Loss 0.5177983641624451\n","[Training Epoch 0] Batch 1320, Loss 0.5228573083877563\n","[Training Epoch 0] Batch 1321, Loss 0.5291121006011963\n","[Training Epoch 0] Batch 1322, Loss 0.5186543464660645\n","[Training Epoch 0] Batch 1323, Loss 0.5422742962837219\n","[Training Epoch 0] Batch 1324, Loss 0.5166860818862915\n","[Training Epoch 0] Batch 1325, Loss 0.5251268148422241\n","[Training Epoch 0] Batch 1326, Loss 0.5275857448577881\n","[Training Epoch 0] Batch 1327, Loss 0.533994197845459\n","[Training Epoch 0] Batch 1328, Loss 0.5401713848114014\n","[Training Epoch 0] Batch 1329, Loss 0.538489043712616\n","[Training Epoch 0] Batch 1330, Loss 0.5234576463699341\n","[Training Epoch 0] Batch 1331, Loss 0.5304186344146729\n","[Training Epoch 0] Batch 1332, Loss 0.535851001739502\n","[Training Epoch 0] Batch 1333, Loss 0.5218253135681152\n","[Training Epoch 0] Batch 1334, Loss 0.5406861901283264\n","[Training Epoch 0] Batch 1335, Loss 0.5232111215591431\n","[Training Epoch 0] Batch 1336, Loss 0.5350574851036072\n","[Training Epoch 0] Batch 1337, Loss 0.5216494798660278\n","[Training Epoch 0] Batch 1338, Loss 0.5489766597747803\n","[Training Epoch 0] Batch 1339, Loss 0.5346592664718628\n","[Training Epoch 0] Batch 1340, Loss 0.5068203210830688\n","[Training Epoch 0] Batch 1341, Loss 0.5202271342277527\n","[Training Epoch 0] Batch 1342, Loss 0.5322638750076294\n","[Training Epoch 0] Batch 1343, Loss 0.5417307615280151\n","[Training Epoch 0] Batch 1344, Loss 0.5430805087089539\n","[Training Epoch 0] Batch 1345, Loss 0.5272702574729919\n","[Training Epoch 0] Batch 1346, Loss 0.5382760167121887\n","[Training Epoch 0] Batch 1347, Loss 0.5223658084869385\n","[Training Epoch 0] Batch 1348, Loss 0.5293651223182678\n","[Training Epoch 0] Batch 1349, Loss 0.523800253868103\n","[Training Epoch 0] Batch 1350, Loss 0.5446101427078247\n","[Training Epoch 0] Batch 1351, Loss 0.5247437953948975\n","[Training Epoch 0] Batch 1352, Loss 0.5323337316513062\n","[Training Epoch 0] Batch 1353, Loss 0.5347818732261658\n","[Training Epoch 0] Batch 1354, Loss 0.5381078720092773\n","[Training Epoch 0] Batch 1355, Loss 0.523402214050293\n","[Training Epoch 0] Batch 1356, Loss 0.5354597568511963\n","[Training Epoch 0] Batch 1357, Loss 0.5292486548423767\n","[Training Epoch 0] Batch 1358, Loss 0.5313206911087036\n","[Training Epoch 0] Batch 1359, Loss 0.5272042751312256\n","[Training Epoch 0] Batch 1360, Loss 0.5634253025054932\n","[Training Epoch 0] Batch 1361, Loss 0.5248292088508606\n","[Training Epoch 0] Batch 1362, Loss 0.5467427968978882\n","[Training Epoch 0] Batch 1363, Loss 0.5335158109664917\n","[Training Epoch 0] Batch 1364, Loss 0.5294544696807861\n","[Training Epoch 0] Batch 1365, Loss 0.5236301422119141\n","[Training Epoch 0] Batch 1366, Loss 0.5332287549972534\n","[Training Epoch 0] Batch 1367, Loss 0.5219107866287231\n","[Training Epoch 0] Batch 1368, Loss 0.5281325578689575\n","[Training Epoch 0] Batch 1369, Loss 0.5194638967514038\n","[Training Epoch 0] Batch 1370, Loss 0.5475661754608154\n","[Training Epoch 0] Batch 1371, Loss 0.535698413848877\n","[Training Epoch 0] Batch 1372, Loss 0.5160565376281738\n","[Training Epoch 0] Batch 1373, Loss 0.5126823782920837\n","[Training Epoch 0] Batch 1374, Loss 0.5323129892349243\n","[Training Epoch 0] Batch 1375, Loss 0.5261298418045044\n","[Training Epoch 0] Batch 1376, Loss 0.5328975319862366\n","[Training Epoch 0] Batch 1377, Loss 0.5161460638046265\n","[Training Epoch 0] Batch 1378, Loss 0.5417034029960632\n","[Training Epoch 0] Batch 1379, Loss 0.5424248576164246\n","[Training Epoch 0] Batch 1380, Loss 0.5294538140296936\n","[Training Epoch 0] Batch 1381, Loss 0.5479759573936462\n","[Training Epoch 0] Batch 1382, Loss 0.5405573844909668\n","[Training Epoch 0] Batch 1383, Loss 0.5278053879737854\n","[Training Epoch 0] Batch 1384, Loss 0.5358597040176392\n","[Training Epoch 0] Batch 1385, Loss 0.5375377535820007\n","[Training Epoch 0] Batch 1386, Loss 0.5333067178726196\n","[Training Epoch 0] Batch 1387, Loss 0.5232535600662231\n","[Training Epoch 0] Batch 1388, Loss 0.5331470966339111\n","[Training Epoch 0] Batch 1389, Loss 0.533453106880188\n","[Training Epoch 0] Batch 1390, Loss 0.5373050570487976\n","[Training Epoch 0] Batch 1391, Loss 0.5302960872650146\n","[Training Epoch 0] Batch 1392, Loss 0.5278486013412476\n","[Training Epoch 0] Batch 1393, Loss 0.534127950668335\n","[Training Epoch 0] Batch 1394, Loss 0.5270756483078003\n","[Training Epoch 0] Batch 1395, Loss 0.4941219985485077\n","[Training Epoch 0] Batch 1396, Loss 0.5162549018859863\n","[Training Epoch 0] Batch 1397, Loss 0.5178561806678772\n","[Training Epoch 0] Batch 1398, Loss 0.5201837420463562\n","[Training Epoch 0] Batch 1399, Loss 0.5326752066612244\n","[Training Epoch 0] Batch 1400, Loss 0.5208497047424316\n","[Training Epoch 0] Batch 1401, Loss 0.536368727684021\n","[Training Epoch 0] Batch 1402, Loss 0.5293251872062683\n","[Training Epoch 0] Batch 1403, Loss 0.52988600730896\n","[Training Epoch 0] Batch 1404, Loss 0.5090974569320679\n","[Training Epoch 0] Batch 1405, Loss 0.5267554521560669\n","[Training Epoch 0] Batch 1406, Loss 0.523111879825592\n","[Training Epoch 0] Batch 1407, Loss 0.535315752029419\n","[Training Epoch 0] Batch 1408, Loss 0.5131001472473145\n","[Training Epoch 0] Batch 1409, Loss 0.525915265083313\n","[Training Epoch 0] Batch 1410, Loss 0.5428134202957153\n","[Training Epoch 0] Batch 1411, Loss 0.5149303674697876\n","[Training Epoch 0] Batch 1412, Loss 0.5251414179801941\n","[Training Epoch 0] Batch 1413, Loss 0.5268629193305969\n","[Training Epoch 0] Batch 1414, Loss 0.5208560824394226\n","[Training Epoch 0] Batch 1415, Loss 0.5185444355010986\n","[Training Epoch 0] Batch 1416, Loss 0.5242577791213989\n","[Training Epoch 0] Batch 1417, Loss 0.517313539981842\n","[Training Epoch 0] Batch 1418, Loss 0.5218769311904907\n","[Training Epoch 0] Batch 1419, Loss 0.5127010941505432\n","[Training Epoch 0] Batch 1420, Loss 0.5345093607902527\n","[Training Epoch 0] Batch 1421, Loss 0.5213624835014343\n","[Training Epoch 0] Batch 1422, Loss 0.5171913504600525\n","[Training Epoch 0] Batch 1423, Loss 0.5364975929260254\n","[Training Epoch 0] Batch 1424, Loss 0.5197904109954834\n","[Training Epoch 0] Batch 1425, Loss 0.5153087973594666\n","[Training Epoch 0] Batch 1426, Loss 0.5334935188293457\n","[Training Epoch 0] Batch 1427, Loss 0.5171051621437073\n","[Training Epoch 0] Batch 1428, Loss 0.5235514640808105\n","[Training Epoch 0] Batch 1429, Loss 0.5183367133140564\n","[Training Epoch 0] Batch 1430, Loss 0.5232553482055664\n","[Training Epoch 0] Batch 1431, Loss 0.5141288638114929\n","[Training Epoch 0] Batch 1432, Loss 0.5259467363357544\n","[Training Epoch 0] Batch 1433, Loss 0.5282074213027954\n","[Training Epoch 0] Batch 1434, Loss 0.5127588510513306\n","[Training Epoch 0] Batch 1435, Loss 0.5226737856864929\n","[Training Epoch 0] Batch 1436, Loss 0.5289112329483032\n","[Training Epoch 0] Batch 1437, Loss 0.5403189659118652\n","[Training Epoch 0] Batch 1438, Loss 0.5192658305168152\n","[Training Epoch 0] Batch 1439, Loss 0.5171785354614258\n","[Training Epoch 0] Batch 1440, Loss 0.5134623646736145\n","[Training Epoch 0] Batch 1441, Loss 0.5369715690612793\n","[Training Epoch 0] Batch 1442, Loss 0.5278707146644592\n","[Training Epoch 0] Batch 1443, Loss 0.5255401134490967\n","[Training Epoch 0] Batch 1444, Loss 0.5110096335411072\n","[Training Epoch 0] Batch 1445, Loss 0.5200119018554688\n","[Training Epoch 0] Batch 1446, Loss 0.5317389369010925\n","[Training Epoch 0] Batch 1447, Loss 0.5259442329406738\n","[Training Epoch 0] Batch 1448, Loss 0.5273810625076294\n","[Training Epoch 0] Batch 1449, Loss 0.5157437324523926\n","[Training Epoch 0] Batch 1450, Loss 0.5101699829101562\n","[Training Epoch 0] Batch 1451, Loss 0.4996911287307739\n","[Training Epoch 0] Batch 1452, Loss 0.5264420509338379\n","[Training Epoch 0] Batch 1453, Loss 0.5194421410560608\n","[Training Epoch 0] Batch 1454, Loss 0.5251275300979614\n","[Training Epoch 0] Batch 1455, Loss 0.544607937335968\n","[Training Epoch 0] Batch 1456, Loss 0.5172775983810425\n","[Training Epoch 0] Batch 1457, Loss 0.5228416323661804\n","[Training Epoch 0] Batch 1458, Loss 0.5136159658432007\n","[Training Epoch 0] Batch 1459, Loss 0.523744523525238\n","[Training Epoch 0] Batch 1460, Loss 0.5180650949478149\n","[Training Epoch 0] Batch 1461, Loss 0.5206961631774902\n","[Training Epoch 0] Batch 1462, Loss 0.516006350517273\n","[Training Epoch 0] Batch 1463, Loss 0.5152381658554077\n","[Training Epoch 0] Batch 1464, Loss 0.5134342312812805\n","[Training Epoch 0] Batch 1465, Loss 0.50990229845047\n","[Training Epoch 0] Batch 1466, Loss 0.5087313652038574\n","[Training Epoch 0] Batch 1467, Loss 0.5237630605697632\n","[Training Epoch 0] Batch 1468, Loss 0.5171772241592407\n","[Training Epoch 0] Batch 1469, Loss 0.5314640998840332\n","[Training Epoch 0] Batch 1470, Loss 0.5378209352493286\n","[Training Epoch 0] Batch 1471, Loss 0.5265992879867554\n","[Training Epoch 0] Batch 1472, Loss 0.515544056892395\n","[Training Epoch 0] Batch 1473, Loss 0.5229758024215698\n","[Training Epoch 0] Batch 1474, Loss 0.5241966247558594\n","[Training Epoch 0] Batch 1475, Loss 0.5284973978996277\n","[Training Epoch 0] Batch 1476, Loss 0.5386959910392761\n","[Training Epoch 0] Batch 1477, Loss 0.5298595428466797\n","[Training Epoch 0] Batch 1478, Loss 0.5122092962265015\n","[Training Epoch 0] Batch 1479, Loss 0.5241205096244812\n","[Training Epoch 0] Batch 1480, Loss 0.5269330739974976\n","[Training Epoch 0] Batch 1481, Loss 0.5201812982559204\n","[Training Epoch 0] Batch 1482, Loss 0.5262526869773865\n","[Training Epoch 0] Batch 1483, Loss 0.5295625925064087\n","[Training Epoch 0] Batch 1484, Loss 0.5154041647911072\n","[Training Epoch 0] Batch 1485, Loss 0.5267701148986816\n","[Training Epoch 0] Batch 1486, Loss 0.4974927306175232\n","[Training Epoch 0] Batch 1487, Loss 0.5287883877754211\n","[Training Epoch 0] Batch 1488, Loss 0.5058022141456604\n","[Training Epoch 0] Batch 1489, Loss 0.5255500078201294\n","[Training Epoch 0] Batch 1490, Loss 0.5145694613456726\n","[Training Epoch 0] Batch 1491, Loss 0.5164275169372559\n","[Training Epoch 0] Batch 1492, Loss 0.5230914950370789\n","[Training Epoch 0] Batch 1493, Loss 0.5302441120147705\n","[Training Epoch 0] Batch 1494, Loss 0.5352293848991394\n","[Training Epoch 0] Batch 1495, Loss 0.5222067832946777\n","[Training Epoch 0] Batch 1496, Loss 0.5021555423736572\n","[Training Epoch 0] Batch 1497, Loss 0.5184284448623657\n","[Training Epoch 0] Batch 1498, Loss 0.5450150966644287\n","[Training Epoch 0] Batch 1499, Loss 0.5178219676017761\n","[Training Epoch 0] Batch 1500, Loss 0.5244007110595703\n","[Training Epoch 0] Batch 1501, Loss 0.5139119029045105\n","[Training Epoch 0] Batch 1502, Loss 0.5139420032501221\n","[Training Epoch 0] Batch 1503, Loss 0.5077612400054932\n","[Training Epoch 0] Batch 1504, Loss 0.4980602264404297\n","[Training Epoch 0] Batch 1505, Loss 0.5172523260116577\n","[Training Epoch 0] Batch 1506, Loss 0.4857148826122284\n","[Training Epoch 0] Batch 1507, Loss 0.5055664777755737\n","[Training Epoch 0] Batch 1508, Loss 0.48980069160461426\n","[Training Epoch 0] Batch 1509, Loss 0.5361707210540771\n","[Training Epoch 0] Batch 1510, Loss 0.5179526209831238\n","[Training Epoch 0] Batch 1511, Loss 0.512252151966095\n","[Training Epoch 0] Batch 1512, Loss 0.522962212562561\n","[Training Epoch 0] Batch 1513, Loss 0.5211659669876099\n","[Training Epoch 0] Batch 1514, Loss 0.5177726149559021\n","[Training Epoch 0] Batch 1515, Loss 0.5386727452278137\n","[Training Epoch 0] Batch 1516, Loss 0.5334615707397461\n","[Training Epoch 0] Batch 1517, Loss 0.5237277746200562\n","[Training Epoch 0] Batch 1518, Loss 0.5033198595046997\n","[Training Epoch 0] Batch 1519, Loss 0.5172897577285767\n","[Training Epoch 0] Batch 1520, Loss 0.5155376195907593\n","[Training Epoch 0] Batch 1521, Loss 0.5050860047340393\n","[Training Epoch 0] Batch 1522, Loss 0.509201169013977\n","[Training Epoch 0] Batch 1523, Loss 0.5174779891967773\n","[Training Epoch 0] Batch 1524, Loss 0.50813227891922\n","[Training Epoch 0] Batch 1525, Loss 0.5355271697044373\n","[Training Epoch 0] Batch 1526, Loss 0.5382530689239502\n","[Training Epoch 0] Batch 1527, Loss 0.5256270170211792\n","[Training Epoch 0] Batch 1528, Loss 0.5213608145713806\n","[Training Epoch 0] Batch 1529, Loss 0.5228919982910156\n","[Training Epoch 0] Batch 1530, Loss 0.5309575796127319\n","[Training Epoch 0] Batch 1531, Loss 0.5259861946105957\n","[Training Epoch 0] Batch 1532, Loss 0.5223431587219238\n","[Training Epoch 0] Batch 1533, Loss 0.5211275815963745\n","[Training Epoch 0] Batch 1534, Loss 0.5220075249671936\n","[Training Epoch 0] Batch 1535, Loss 0.5312201976776123\n","[Training Epoch 0] Batch 1536, Loss 0.5002212524414062\n","[Training Epoch 0] Batch 1537, Loss 0.5433865785598755\n","[Training Epoch 0] Batch 1538, Loss 0.5308878421783447\n","[Training Epoch 0] Batch 1539, Loss 0.5023074150085449\n","[Training Epoch 0] Batch 1540, Loss 0.5021504759788513\n","[Training Epoch 0] Batch 1541, Loss 0.49741047620773315\n","[Training Epoch 0] Batch 1542, Loss 0.5153257846832275\n","[Training Epoch 0] Batch 1543, Loss 0.5283201336860657\n","[Training Epoch 0] Batch 1544, Loss 0.5297781229019165\n","[Training Epoch 0] Batch 1545, Loss 0.5076098442077637\n","[Training Epoch 0] Batch 1546, Loss 0.5331124067306519\n","[Training Epoch 0] Batch 1547, Loss 0.5216850638389587\n","[Training Epoch 0] Batch 1548, Loss 0.5128331184387207\n","[Training Epoch 0] Batch 1549, Loss 0.5295062065124512\n","[Training Epoch 0] Batch 1550, Loss 0.5238820314407349\n","[Training Epoch 0] Batch 1551, Loss 0.5229323506355286\n","[Training Epoch 0] Batch 1552, Loss 0.5258049368858337\n","[Training Epoch 0] Batch 1553, Loss 0.5245804786682129\n","[Training Epoch 0] Batch 1554, Loss 0.5150490999221802\n","[Training Epoch 0] Batch 1555, Loss 0.5381748676300049\n","[Training Epoch 0] Batch 1556, Loss 0.5273988246917725\n","[Training Epoch 0] Batch 1557, Loss 0.5291645526885986\n","[Training Epoch 0] Batch 1558, Loss 0.510066032409668\n","[Training Epoch 0] Batch 1559, Loss 0.5189951658248901\n","[Training Epoch 0] Batch 1560, Loss 0.5131544470787048\n","[Training Epoch 0] Batch 1561, Loss 0.5322089195251465\n","[Training Epoch 0] Batch 1562, Loss 0.5263590812683105\n","[Training Epoch 0] Batch 1563, Loss 0.5228807926177979\n","[Training Epoch 0] Batch 1564, Loss 0.5092893242835999\n","[Training Epoch 0] Batch 1565, Loss 0.5099961757659912\n","[Training Epoch 0] Batch 1566, Loss 0.5074643492698669\n","[Training Epoch 0] Batch 1567, Loss 0.5216777920722961\n","[Training Epoch 0] Batch 1568, Loss 0.5192551016807556\n","[Training Epoch 0] Batch 1569, Loss 0.5225774049758911\n","[Training Epoch 0] Batch 1570, Loss 0.5118580460548401\n","[Training Epoch 0] Batch 1571, Loss 0.5220054388046265\n","[Training Epoch 0] Batch 1572, Loss 0.5238232612609863\n","[Training Epoch 0] Batch 1573, Loss 0.502562940120697\n","[Training Epoch 0] Batch 1574, Loss 0.5378271341323853\n","[Training Epoch 0] Batch 1575, Loss 0.5354239344596863\n","[Training Epoch 0] Batch 1576, Loss 0.5281702280044556\n","[Training Epoch 0] Batch 1577, Loss 0.5370322465896606\n","[Training Epoch 0] Batch 1578, Loss 0.5037532448768616\n","[Training Epoch 0] Batch 1579, Loss 0.504616379737854\n","[Training Epoch 0] Batch 1580, Loss 0.5314176082611084\n","[Training Epoch 0] Batch 1581, Loss 0.5180288553237915\n","[Training Epoch 0] Batch 1582, Loss 0.5400975942611694\n","[Training Epoch 0] Batch 1583, Loss 0.5336992144584656\n","[Training Epoch 0] Batch 1584, Loss 0.5152754783630371\n","[Training Epoch 0] Batch 1585, Loss 0.5127487182617188\n","[Training Epoch 0] Batch 1586, Loss 0.5223959684371948\n","[Training Epoch 0] Batch 1587, Loss 0.5175768733024597\n","[Training Epoch 0] Batch 1588, Loss 0.5035098195075989\n","[Training Epoch 0] Batch 1589, Loss 0.5043460130691528\n","[Training Epoch 0] Batch 1590, Loss 0.5302839279174805\n","[Training Epoch 0] Batch 1591, Loss 0.5123498439788818\n","[Training Epoch 0] Batch 1592, Loss 0.5190085768699646\n","[Training Epoch 0] Batch 1593, Loss 0.5269039869308472\n","[Training Epoch 0] Batch 1594, Loss 0.5141717195510864\n","[Training Epoch 0] Batch 1595, Loss 0.5086582899093628\n","[Training Epoch 0] Batch 1596, Loss 0.5344653725624084\n","[Training Epoch 0] Batch 1597, Loss 0.5186520218849182\n","[Training Epoch 0] Batch 1598, Loss 0.5079193115234375\n","[Training Epoch 0] Batch 1599, Loss 0.5254116058349609\n","[Training Epoch 0] Batch 1600, Loss 0.5092583894729614\n","[Training Epoch 0] Batch 1601, Loss 0.5091774463653564\n","[Training Epoch 0] Batch 1602, Loss 0.5155944228172302\n","[Training Epoch 0] Batch 1603, Loss 0.5132673382759094\n","[Training Epoch 0] Batch 1604, Loss 0.5255597829818726\n","[Training Epoch 0] Batch 1605, Loss 0.530128538608551\n","[Training Epoch 0] Batch 1606, Loss 0.5230728387832642\n","[Training Epoch 0] Batch 1607, Loss 0.5364328026771545\n","[Training Epoch 0] Batch 1608, Loss 0.5129175782203674\n","[Training Epoch 0] Batch 1609, Loss 0.5051752328872681\n","[Training Epoch 0] Batch 1610, Loss 0.5160102844238281\n","[Training Epoch 0] Batch 1611, Loss 0.5212197303771973\n","[Training Epoch 0] Batch 1612, Loss 0.5103707909584045\n","[Training Epoch 0] Batch 1613, Loss 0.491540789604187\n","[Training Epoch 0] Batch 1614, Loss 0.509716272354126\n","[Training Epoch 0] Batch 1615, Loss 0.535051167011261\n","[Training Epoch 0] Batch 1616, Loss 0.5192890167236328\n","[Training Epoch 0] Batch 1617, Loss 0.5263610482215881\n","[Training Epoch 0] Batch 1618, Loss 0.5131002068519592\n","[Training Epoch 0] Batch 1619, Loss 0.5211405754089355\n","[Training Epoch 0] Batch 1620, Loss 0.5235190987586975\n","[Training Epoch 0] Batch 1621, Loss 0.516123354434967\n","[Training Epoch 0] Batch 1622, Loss 0.5264575481414795\n","[Training Epoch 0] Batch 1623, Loss 0.5260984301567078\n","[Training Epoch 0] Batch 1624, Loss 0.4908178746700287\n","[Training Epoch 0] Batch 1625, Loss 0.5114223957061768\n","[Training Epoch 0] Batch 1626, Loss 0.5067181587219238\n","[Training Epoch 0] Batch 1627, Loss 0.522388219833374\n","[Training Epoch 0] Batch 1628, Loss 0.5084112286567688\n","[Training Epoch 0] Batch 1629, Loss 0.5009825229644775\n","[Training Epoch 0] Batch 1630, Loss 0.5166863203048706\n","[Training Epoch 0] Batch 1631, Loss 0.5213818550109863\n","[Training Epoch 0] Batch 1632, Loss 0.5212880373001099\n","[Training Epoch 0] Batch 1633, Loss 0.5095428228378296\n","[Training Epoch 0] Batch 1634, Loss 0.49883508682250977\n","[Training Epoch 0] Batch 1635, Loss 0.5193896889686584\n","[Training Epoch 0] Batch 1636, Loss 0.5124399662017822\n","[Training Epoch 0] Batch 1637, Loss 0.5153713822364807\n","[Training Epoch 0] Batch 1638, Loss 0.5164517164230347\n","[Training Epoch 0] Batch 1639, Loss 0.5247045755386353\n","[Training Epoch 0] Batch 1640, Loss 0.48507922887802124\n","[Training Epoch 0] Batch 1641, Loss 0.5332776308059692\n","[Training Epoch 0] Batch 1642, Loss 0.4944431483745575\n","[Training Epoch 0] Batch 1643, Loss 0.49204686284065247\n","[Training Epoch 0] Batch 1644, Loss 0.4911440908908844\n","[Training Epoch 0] Batch 1645, Loss 0.511648416519165\n","[Training Epoch 0] Batch 1646, Loss 0.5196588039398193\n","[Training Epoch 0] Batch 1647, Loss 0.5219320058822632\n","[Training Epoch 0] Batch 1648, Loss 0.5141257047653198\n","[Training Epoch 0] Batch 1649, Loss 0.5000337958335876\n","[Training Epoch 0] Batch 1650, Loss 0.49888861179351807\n","[Training Epoch 0] Batch 1651, Loss 0.5135449171066284\n","[Training Epoch 0] Batch 1652, Loss 0.5080534815788269\n","[Training Epoch 0] Batch 1653, Loss 0.505355954170227\n","[Training Epoch 0] Batch 1654, Loss 0.533744215965271\n","[Training Epoch 0] Batch 1655, Loss 0.5216284394264221\n","[Training Epoch 0] Batch 1656, Loss 0.4844631254673004\n","[Training Epoch 0] Batch 1657, Loss 0.5083170533180237\n","[Training Epoch 0] Batch 1658, Loss 0.5272878408432007\n","[Training Epoch 0] Batch 1659, Loss 0.5432592630386353\n","[Training Epoch 0] Batch 1660, Loss 0.5089858174324036\n","[Training Epoch 0] Batch 1661, Loss 0.5250191688537598\n","[Training Epoch 0] Batch 1662, Loss 0.5089409351348877\n","[Training Epoch 0] Batch 1663, Loss 0.5092135667800903\n","[Training Epoch 0] Batch 1664, Loss 0.5191960334777832\n","[Training Epoch 0] Batch 1665, Loss 0.5229859948158264\n","[Training Epoch 0] Batch 1666, Loss 0.49705931544303894\n","[Training Epoch 0] Batch 1667, Loss 0.5144482851028442\n","[Training Epoch 0] Batch 1668, Loss 0.5088719129562378\n","[Training Epoch 0] Batch 1669, Loss 0.5277968049049377\n","[Training Epoch 0] Batch 1670, Loss 0.5193339586257935\n","[Training Epoch 0] Batch 1671, Loss 0.5132249593734741\n","[Training Epoch 0] Batch 1672, Loss 0.5286300182342529\n","[Training Epoch 0] Batch 1673, Loss 0.5003052949905396\n","[Training Epoch 0] Batch 1674, Loss 0.5198773145675659\n","[Training Epoch 0] Batch 1675, Loss 0.51514732837677\n","[Training Epoch 0] Batch 1676, Loss 0.5093469023704529\n","[Training Epoch 0] Batch 1677, Loss 0.534173309803009\n","[Training Epoch 0] Batch 1678, Loss 0.5053201913833618\n","[Training Epoch 0] Batch 1679, Loss 0.5197337865829468\n","[Training Epoch 0] Batch 1680, Loss 0.4909394383430481\n","[Training Epoch 0] Batch 1681, Loss 0.5272707939147949\n","[Training Epoch 0] Batch 1682, Loss 0.5111969113349915\n","[Training Epoch 0] Batch 1683, Loss 0.5262733697891235\n","[Training Epoch 0] Batch 1684, Loss 0.5242133140563965\n","[Training Epoch 0] Batch 1685, Loss 0.5185010433197021\n","[Training Epoch 0] Batch 1686, Loss 0.5167168378829956\n","[Training Epoch 0] Batch 1687, Loss 0.5071758031845093\n","[Training Epoch 0] Batch 1688, Loss 0.5175703167915344\n","[Training Epoch 0] Batch 1689, Loss 0.5186967849731445\n","[Training Epoch 0] Batch 1690, Loss 0.5212689638137817\n","[Training Epoch 0] Batch 1691, Loss 0.5247992277145386\n","[Training Epoch 0] Batch 1692, Loss 0.5087707042694092\n","[Training Epoch 0] Batch 1693, Loss 0.515464186668396\n","[Training Epoch 0] Batch 1694, Loss 0.501912534236908\n","[Training Epoch 0] Batch 1695, Loss 0.5032510757446289\n","[Training Epoch 0] Batch 1696, Loss 0.5114762783050537\n","[Training Epoch 0] Batch 1697, Loss 0.5255162715911865\n","[Training Epoch 0] Batch 1698, Loss 0.5070210695266724\n","[Training Epoch 0] Batch 1699, Loss 0.5153270959854126\n","[Training Epoch 0] Batch 1700, Loss 0.5318227410316467\n","[Training Epoch 0] Batch 1701, Loss 0.4912056028842926\n","[Training Epoch 0] Batch 1702, Loss 0.4948156476020813\n","[Training Epoch 0] Batch 1703, Loss 0.5191259384155273\n","[Training Epoch 0] Batch 1704, Loss 0.5268559455871582\n","[Training Epoch 0] Batch 1705, Loss 0.5081127285957336\n","[Training Epoch 0] Batch 1706, Loss 0.5013343095779419\n","[Training Epoch 0] Batch 1707, Loss 0.5120083093643188\n","[Training Epoch 0] Batch 1708, Loss 0.5205832123756409\n","[Training Epoch 0] Batch 1709, Loss 0.4985106289386749\n","[Training Epoch 0] Batch 1710, Loss 0.5018426179885864\n","[Training Epoch 0] Batch 1711, Loss 0.5463041067123413\n","[Training Epoch 0] Batch 1712, Loss 0.5170755982398987\n","[Training Epoch 0] Batch 1713, Loss 0.5234405994415283\n","[Training Epoch 0] Batch 1714, Loss 0.5073120594024658\n","[Training Epoch 0] Batch 1715, Loss 0.5109466314315796\n","[Training Epoch 0] Batch 1716, Loss 0.51490318775177\n","[Training Epoch 0] Batch 1717, Loss 0.4944908320903778\n","[Training Epoch 0] Batch 1718, Loss 0.5148199200630188\n","[Training Epoch 0] Batch 1719, Loss 0.5185712575912476\n","[Training Epoch 0] Batch 1720, Loss 0.4943442940711975\n","[Training Epoch 0] Batch 1721, Loss 0.5126917958259583\n","[Training Epoch 0] Batch 1722, Loss 0.5224775671958923\n","[Training Epoch 0] Batch 1723, Loss 0.5039471387863159\n","[Training Epoch 0] Batch 1724, Loss 0.520397961139679\n","[Training Epoch 0] Batch 1725, Loss 0.5115281939506531\n","[Training Epoch 0] Batch 1726, Loss 0.49901634454727173\n","[Training Epoch 0] Batch 1727, Loss 0.5106426477432251\n","[Training Epoch 0] Batch 1728, Loss 0.5281418561935425\n","[Training Epoch 0] Batch 1729, Loss 0.4891743063926697\n","[Training Epoch 0] Batch 1730, Loss 0.502872109413147\n","[Training Epoch 0] Batch 1731, Loss 0.5222338438034058\n","[Training Epoch 0] Batch 1732, Loss 0.5133699178695679\n","[Training Epoch 0] Batch 1733, Loss 0.5096752643585205\n","[Training Epoch 0] Batch 1734, Loss 0.517123818397522\n","[Training Epoch 0] Batch 1735, Loss 0.5169678926467896\n","[Training Epoch 0] Batch 1736, Loss 0.5103248357772827\n","[Training Epoch 0] Batch 1737, Loss 0.5132452249526978\n","[Training Epoch 0] Batch 1738, Loss 0.5133654475212097\n","[Training Epoch 0] Batch 1739, Loss 0.524729311466217\n","[Training Epoch 0] Batch 1740, Loss 0.5300564169883728\n","[Training Epoch 0] Batch 1741, Loss 0.5011933445930481\n","[Training Epoch 0] Batch 1742, Loss 0.5013899803161621\n","[Training Epoch 0] Batch 1743, Loss 0.5179823637008667\n","[Training Epoch 0] Batch 1744, Loss 0.5195460319519043\n","[Training Epoch 0] Batch 1745, Loss 0.4904581308364868\n","[Training Epoch 0] Batch 1746, Loss 0.5132022500038147\n","[Training Epoch 0] Batch 1747, Loss 0.521181583404541\n","[Training Epoch 0] Batch 1748, Loss 0.5101228952407837\n","[Training Epoch 0] Batch 1749, Loss 0.5003327131271362\n","[Training Epoch 0] Batch 1750, Loss 0.5157417058944702\n","[Training Epoch 0] Batch 1751, Loss 0.4933474659919739\n","[Training Epoch 0] Batch 1752, Loss 0.5159253478050232\n","[Training Epoch 0] Batch 1753, Loss 0.5030046701431274\n","[Training Epoch 0] Batch 1754, Loss 0.5107190012931824\n","[Training Epoch 0] Batch 1755, Loss 0.4975816607475281\n","[Training Epoch 0] Batch 1756, Loss 0.5283254384994507\n","[Training Epoch 0] Batch 1757, Loss 0.503578245639801\n","[Training Epoch 0] Batch 1758, Loss 0.5164516568183899\n","[Training Epoch 0] Batch 1759, Loss 0.5197137594223022\n","[Training Epoch 0] Batch 1760, Loss 0.5036728382110596\n","[Training Epoch 0] Batch 1761, Loss 0.5251738429069519\n","[Training Epoch 0] Batch 1762, Loss 0.5157708525657654\n","[Training Epoch 0] Batch 1763, Loss 0.4886767268180847\n","[Training Epoch 0] Batch 1764, Loss 0.4975734353065491\n","[Training Epoch 0] Batch 1765, Loss 0.508275032043457\n","[Training Epoch 0] Batch 1766, Loss 0.5254918336868286\n","[Training Epoch 0] Batch 1767, Loss 0.5271044373512268\n","[Training Epoch 0] Batch 1768, Loss 0.48461493849754333\n","[Training Epoch 0] Batch 1769, Loss 0.511460542678833\n","[Training Epoch 0] Batch 1770, Loss 0.4941187798976898\n","[Training Epoch 0] Batch 1771, Loss 0.49248629808425903\n","[Training Epoch 0] Batch 1772, Loss 0.5351530313491821\n","[Training Epoch 0] Batch 1773, Loss 0.5072827935218811\n","[Training Epoch 0] Batch 1774, Loss 0.5001547932624817\n","[Training Epoch 0] Batch 1775, Loss 0.5180687308311462\n","[Training Epoch 0] Batch 1776, Loss 0.4921427071094513\n","[Training Epoch 0] Batch 1777, Loss 0.5112102627754211\n","[Training Epoch 0] Batch 1778, Loss 0.5229437351226807\n","[Training Epoch 0] Batch 1779, Loss 0.487465500831604\n","[Training Epoch 0] Batch 1780, Loss 0.49083784222602844\n","[Training Epoch 0] Batch 1781, Loss 0.5179887413978577\n","[Training Epoch 0] Batch 1782, Loss 0.5170789361000061\n","[Training Epoch 0] Batch 1783, Loss 0.5000355243682861\n","[Training Epoch 0] Batch 1784, Loss 0.5088601112365723\n","[Training Epoch 0] Batch 1785, Loss 0.5167593359947205\n","[Training Epoch 0] Batch 1786, Loss 0.5209596157073975\n","[Training Epoch 0] Batch 1787, Loss 0.5159096717834473\n","[Training Epoch 0] Batch 1788, Loss 0.5075902938842773\n","[Training Epoch 0] Batch 1789, Loss 0.5116173624992371\n","[Training Epoch 0] Batch 1790, Loss 0.5328331589698792\n","[Training Epoch 0] Batch 1791, Loss 0.5121902823448181\n","[Training Epoch 0] Batch 1792, Loss 0.5096461176872253\n","[Training Epoch 0] Batch 1793, Loss 0.5201228857040405\n","[Training Epoch 0] Batch 1794, Loss 0.5107206106185913\n","[Training Epoch 0] Batch 1795, Loss 0.508618175983429\n","[Training Epoch 0] Batch 1796, Loss 0.5105053186416626\n","[Training Epoch 0] Batch 1797, Loss 0.5188019275665283\n","[Training Epoch 0] Batch 1798, Loss 0.5236173868179321\n","[Training Epoch 0] Batch 1799, Loss 0.4984613358974457\n","[Training Epoch 0] Batch 1800, Loss 0.5288112163543701\n","[Training Epoch 0] Batch 1801, Loss 0.5215414762496948\n","[Training Epoch 0] Batch 1802, Loss 0.5255183577537537\n","[Training Epoch 0] Batch 1803, Loss 0.5143812894821167\n","[Training Epoch 0] Batch 1804, Loss 0.5053179264068604\n","[Training Epoch 0] Batch 1805, Loss 0.5014253854751587\n","[Training Epoch 0] Batch 1806, Loss 0.50428307056427\n","[Training Epoch 0] Batch 1807, Loss 0.5066663026809692\n","[Training Epoch 0] Batch 1808, Loss 0.4883788526058197\n","[Training Epoch 0] Batch 1809, Loss 0.5205012559890747\n","[Training Epoch 0] Batch 1810, Loss 0.512418270111084\n","[Training Epoch 0] Batch 1811, Loss 0.5102065801620483\n","[Training Epoch 0] Batch 1812, Loss 0.5171046257019043\n","[Training Epoch 0] Batch 1813, Loss 0.5203906297683716\n","[Training Epoch 0] Batch 1814, Loss 0.5294370651245117\n","[Training Epoch 0] Batch 1815, Loss 0.516223669052124\n","[Training Epoch 0] Batch 1816, Loss 0.4938986897468567\n","[Training Epoch 0] Batch 1817, Loss 0.533542275428772\n","[Training Epoch 0] Batch 1818, Loss 0.4888874888420105\n","[Training Epoch 0] Batch 1819, Loss 0.5252747535705566\n","[Training Epoch 0] Batch 1820, Loss 0.5180369019508362\n","[Training Epoch 0] Batch 1821, Loss 0.5172137022018433\n","[Training Epoch 0] Batch 1822, Loss 0.5100254416465759\n","[Training Epoch 0] Batch 1823, Loss 0.5088273286819458\n","[Training Epoch 0] Batch 1824, Loss 0.5159796476364136\n","[Training Epoch 0] Batch 1825, Loss 0.5017354488372803\n","[Training Epoch 0] Batch 1826, Loss 0.49984654784202576\n","[Training Epoch 0] Batch 1827, Loss 0.48046839237213135\n","[Training Epoch 0] Batch 1828, Loss 0.5097631216049194\n","[Training Epoch 0] Batch 1829, Loss 0.5169786810874939\n","[Training Epoch 0] Batch 1830, Loss 0.49550923705101013\n","[Training Epoch 0] Batch 1831, Loss 0.5117238759994507\n","[Training Epoch 0] Batch 1832, Loss 0.5188697576522827\n","[Training Epoch 0] Batch 1833, Loss 0.4963892102241516\n","[Training Epoch 0] Batch 1834, Loss 0.5208865404129028\n","[Training Epoch 0] Batch 1835, Loss 0.5056136250495911\n","[Training Epoch 0] Batch 1836, Loss 0.5055334568023682\n","[Training Epoch 0] Batch 1837, Loss 0.517764687538147\n","[Training Epoch 0] Batch 1838, Loss 0.528902530670166\n","[Training Epoch 0] Batch 1839, Loss 0.5095627307891846\n","[Training Epoch 0] Batch 1840, Loss 0.5135542154312134\n","[Training Epoch 0] Batch 1841, Loss 0.5135654211044312\n","[Training Epoch 0] Batch 1842, Loss 0.5134987831115723\n","[Training Epoch 0] Batch 1843, Loss 0.5207405090332031\n","[Training Epoch 0] Batch 1844, Loss 0.5166451930999756\n","[Training Epoch 0] Batch 1845, Loss 0.5012944936752319\n","[Training Epoch 0] Batch 1846, Loss 0.4990478754043579\n","[Training Epoch 0] Batch 1847, Loss 0.5032423734664917\n","[Training Epoch 0] Batch 1848, Loss 0.5083075165748596\n","[Training Epoch 0] Batch 1849, Loss 0.5010936856269836\n","[Training Epoch 0] Batch 1850, Loss 0.49286502599716187\n","[Training Epoch 0] Batch 1851, Loss 0.5061160326004028\n","[Training Epoch 0] Batch 1852, Loss 0.5030684471130371\n","[Training Epoch 0] Batch 1853, Loss 0.5205971002578735\n","[Training Epoch 0] Batch 1854, Loss 0.5102319121360779\n","[Training Epoch 0] Batch 1855, Loss 0.5113064050674438\n","[Training Epoch 0] Batch 1856, Loss 0.4926162660121918\n","[Training Epoch 0] Batch 1857, Loss 0.5203088521957397\n","[Training Epoch 0] Batch 1858, Loss 0.5090818405151367\n","[Training Epoch 0] Batch 1859, Loss 0.5339415073394775\n","[Training Epoch 0] Batch 1860, Loss 0.5286986827850342\n","[Training Epoch 0] Batch 1861, Loss 0.5036867260932922\n","[Training Epoch 0] Batch 1862, Loss 0.4915684163570404\n","[Training Epoch 0] Batch 1863, Loss 0.5210539698600769\n","[Training Epoch 0] Batch 1864, Loss 0.5377650260925293\n","[Training Epoch 0] Batch 1865, Loss 0.5355432033538818\n","[Training Epoch 0] Batch 1866, Loss 0.511051595211029\n","[Training Epoch 0] Batch 1867, Loss 0.5189944505691528\n","[Training Epoch 0] Batch 1868, Loss 0.5232475996017456\n","[Training Epoch 0] Batch 1869, Loss 0.5067681074142456\n","[Training Epoch 0] Batch 1870, Loss 0.5115303993225098\n","[Training Epoch 0] Batch 1871, Loss 0.5087249279022217\n","[Training Epoch 0] Batch 1872, Loss 0.5109564065933228\n","[Training Epoch 0] Batch 1873, Loss 0.5068351030349731\n","[Training Epoch 0] Batch 1874, Loss 0.49945199489593506\n","[Training Epoch 0] Batch 1875, Loss 0.5046135783195496\n","[Training Epoch 0] Batch 1876, Loss 0.4899877905845642\n","[Training Epoch 0] Batch 1877, Loss 0.5218818187713623\n","[Training Epoch 0] Batch 1878, Loss 0.47766798734664917\n","[Training Epoch 0] Batch 1879, Loss 0.5005225539207458\n","[Training Epoch 0] Batch 1880, Loss 0.5148615837097168\n","[Training Epoch 0] Batch 1881, Loss 0.4908214211463928\n","[Training Epoch 0] Batch 1882, Loss 0.5129322409629822\n","[Training Epoch 0] Batch 1883, Loss 0.5168079137802124\n","[Training Epoch 0] Batch 1884, Loss 0.5048031210899353\n","[Training Epoch 0] Batch 1885, Loss 0.5231010913848877\n","[Training Epoch 0] Batch 1886, Loss 0.5199716687202454\n","[Training Epoch 0] Batch 1887, Loss 0.5168012976646423\n","[Training Epoch 0] Batch 1888, Loss 0.5187264680862427\n","[Training Epoch 0] Batch 1889, Loss 0.5013473629951477\n","[Training Epoch 0] Batch 1890, Loss 0.509597659111023\n","[Training Epoch 0] Batch 1891, Loss 0.5112926959991455\n","[Training Epoch 0] Batch 1892, Loss 0.5000424981117249\n","[Training Epoch 0] Batch 1893, Loss 0.5215926170349121\n","[Training Epoch 0] Batch 1894, Loss 0.5147541761398315\n","[Training Epoch 0] Batch 1895, Loss 0.5232046842575073\n","[Training Epoch 0] Batch 1896, Loss 0.49774378538131714\n","[Training Epoch 0] Batch 1897, Loss 0.5020632743835449\n","[Training Epoch 0] Batch 1898, Loss 0.5163959264755249\n","[Training Epoch 0] Batch 1899, Loss 0.516510546207428\n","[Training Epoch 0] Batch 1900, Loss 0.5056490302085876\n","[Training Epoch 0] Batch 1901, Loss 0.5083314180374146\n","[Training Epoch 0] Batch 1902, Loss 0.5127190351486206\n","[Training Epoch 0] Batch 1903, Loss 0.5364945530891418\n","[Training Epoch 0] Batch 1904, Loss 0.5071488618850708\n","[Training Epoch 0] Batch 1905, Loss 0.5204614400863647\n","[Training Epoch 0] Batch 1906, Loss 0.5175427198410034\n","[Training Epoch 0] Batch 1907, Loss 0.5145055651664734\n","[Training Epoch 0] Batch 1908, Loss 0.5065929293632507\n","[Training Epoch 0] Batch 1909, Loss 0.5038086175918579\n","[Training Epoch 0] Batch 1910, Loss 0.5490894317626953\n","[Training Epoch 0] Batch 1911, Loss 0.5165792107582092\n","[Training Epoch 0] Batch 1912, Loss 0.5016642808914185\n","[Training Epoch 0] Batch 1913, Loss 0.5101364850997925\n","[Training Epoch 0] Batch 1914, Loss 0.5323066711425781\n","[Training Epoch 0] Batch 1915, Loss 0.5174814462661743\n","[Training Epoch 0] Batch 1916, Loss 0.507975697517395\n","[Training Epoch 0] Batch 1917, Loss 0.5225286483764648\n","[Training Epoch 0] Batch 1918, Loss 0.5111103057861328\n","[Training Epoch 0] Batch 1919, Loss 0.499396950006485\n","[Training Epoch 0] Batch 1920, Loss 0.4973323345184326\n","[Training Epoch 0] Batch 1921, Loss 0.5076552629470825\n","[Training Epoch 0] Batch 1922, Loss 0.5299807786941528\n","[Training Epoch 0] Batch 1923, Loss 0.5002342462539673\n","[Training Epoch 0] Batch 1924, Loss 0.5276939272880554\n","[Training Epoch 0] Batch 1925, Loss 0.5181492567062378\n","[Training Epoch 0] Batch 1926, Loss 0.5180131196975708\n","[Training Epoch 0] Batch 1927, Loss 0.502528190612793\n","[Training Epoch 0] Batch 1928, Loss 0.4790599048137665\n","[Training Epoch 0] Batch 1929, Loss 0.5127884149551392\n","[Training Epoch 0] Batch 1930, Loss 0.5098089575767517\n","[Training Epoch 0] Batch 1931, Loss 0.5117267370223999\n","[Training Epoch 0] Batch 1932, Loss 0.5012878179550171\n","[Training Epoch 0] Batch 1933, Loss 0.5141515731811523\n","[Training Epoch 0] Batch 1934, Loss 0.492730975151062\n","[Training Epoch 0] Batch 1935, Loss 0.4939157962799072\n","[Training Epoch 0] Batch 1936, Loss 0.5055371522903442\n","[Training Epoch 0] Batch 1937, Loss 0.5180479288101196\n","[Training Epoch 0] Batch 1938, Loss 0.5065294504165649\n","[Training Epoch 0] Batch 1939, Loss 0.530593991279602\n","[Training Epoch 0] Batch 1940, Loss 0.4841119050979614\n","[Training Epoch 0] Batch 1941, Loss 0.4934554100036621\n","[Training Epoch 0] Batch 1942, Loss 0.5275757312774658\n","[Training Epoch 0] Batch 1943, Loss 0.5277836918830872\n","[Training Epoch 0] Batch 1944, Loss 0.5125425457954407\n","[Training Epoch 0] Batch 1945, Loss 0.49787241220474243\n","[Training Epoch 0] Batch 1946, Loss 0.5023548603057861\n","[Training Epoch 0] Batch 1947, Loss 0.5221046805381775\n","[Training Epoch 0] Batch 1948, Loss 0.4943591356277466\n","[Training Epoch 0] Batch 1949, Loss 0.5028805136680603\n","[Training Epoch 0] Batch 1950, Loss 0.5051372647285461\n","[Training Epoch 0] Batch 1951, Loss 0.488127738237381\n","[Training Epoch 0] Batch 1952, Loss 0.5000064373016357\n","[Training Epoch 0] Batch 1953, Loss 0.526164174079895\n","[Training Epoch 0] Batch 1954, Loss 0.47630971670150757\n","[Training Epoch 0] Batch 1955, Loss 0.47857922315597534\n","[Training Epoch 0] Batch 1956, Loss 0.5122069120407104\n","[Training Epoch 0] Batch 1957, Loss 0.4932146370410919\n","[Training Epoch 0] Batch 1958, Loss 0.5155704617500305\n","[Training Epoch 0] Batch 1959, Loss 0.5307595729827881\n","[Training Epoch 0] Batch 1960, Loss 0.5006190538406372\n","[Training Epoch 0] Batch 1961, Loss 0.5343829393386841\n","[Training Epoch 0] Batch 1962, Loss 0.4930524528026581\n","[Training Epoch 0] Batch 1963, Loss 0.4995279312133789\n","[Training Epoch 0] Batch 1964, Loss 0.5115095973014832\n","[Training Epoch 0] Batch 1965, Loss 0.4993903934955597\n","[Training Epoch 0] Batch 1966, Loss 0.5270567536354065\n","[Training Epoch 0] Batch 1967, Loss 0.5057744979858398\n","[Training Epoch 0] Batch 1968, Loss 0.5006700754165649\n","[Training Epoch 0] Batch 1969, Loss 0.5092147588729858\n","[Training Epoch 0] Batch 1970, Loss 0.4899557828903198\n","[Training Epoch 0] Batch 1971, Loss 0.5122710466384888\n","[Training Epoch 0] Batch 1972, Loss 0.5047281980514526\n","[Training Epoch 0] Batch 1973, Loss 0.5057603716850281\n","[Training Epoch 0] Batch 1974, Loss 0.49612534046173096\n","[Training Epoch 0] Batch 1975, Loss 0.49372410774230957\n","[Training Epoch 0] Batch 1976, Loss 0.5066083669662476\n","[Training Epoch 0] Batch 1977, Loss 0.51197350025177\n","[Training Epoch 0] Batch 1978, Loss 0.4991058111190796\n","[Training Epoch 0] Batch 1979, Loss 0.48938342928886414\n","[Training Epoch 0] Batch 1980, Loss 0.5078588724136353\n","[Training Epoch 0] Batch 1981, Loss 0.4581066966056824\n","[Training Epoch 0] Batch 1982, Loss 0.508729100227356\n","[Training Epoch 0] Batch 1983, Loss 0.5173460245132446\n","[Training Epoch 0] Batch 1984, Loss 0.5023407936096191\n","[Training Epoch 0] Batch 1985, Loss 0.5183807611465454\n","[Training Epoch 0] Batch 1986, Loss 0.4975302815437317\n","[Training Epoch 0] Batch 1987, Loss 0.516295313835144\n","[Training Epoch 0] Batch 1988, Loss 0.5196861624717712\n","[Training Epoch 0] Batch 1989, Loss 0.520500898361206\n","[Training Epoch 0] Batch 1990, Loss 0.4979288578033447\n","[Training Epoch 0] Batch 1991, Loss 0.5013572573661804\n","[Training Epoch 0] Batch 1992, Loss 0.4900975525379181\n","[Training Epoch 0] Batch 1993, Loss 0.5033712387084961\n","[Training Epoch 0] Batch 1994, Loss 0.49778738617897034\n","[Training Epoch 0] Batch 1995, Loss 0.5321880578994751\n","[Training Epoch 0] Batch 1996, Loss 0.521672248840332\n","[Training Epoch 0] Batch 1997, Loss 0.5002301931381226\n","[Training Epoch 0] Batch 1998, Loss 0.4803053140640259\n","[Training Epoch 0] Batch 1999, Loss 0.5051660537719727\n","[Training Epoch 0] Batch 2000, Loss 0.5004993677139282\n","[Training Epoch 0] Batch 2001, Loss 0.49881279468536377\n","[Training Epoch 0] Batch 2002, Loss 0.5041608810424805\n","[Training Epoch 0] Batch 2003, Loss 0.4879550039768219\n","[Training Epoch 0] Batch 2004, Loss 0.5097345113754272\n","[Training Epoch 0] Batch 2005, Loss 0.4941614866256714\n","[Training Epoch 0] Batch 2006, Loss 0.513798713684082\n","[Training Epoch 0] Batch 2007, Loss 0.5081295967102051\n","[Training Epoch 0] Batch 2008, Loss 0.507318377494812\n","[Training Epoch 0] Batch 2009, Loss 0.49941056966781616\n","[Training Epoch 0] Batch 2010, Loss 0.4972899854183197\n","[Training Epoch 0] Batch 2011, Loss 0.5267534255981445\n","[Training Epoch 0] Batch 2012, Loss 0.5013776421546936\n","[Training Epoch 0] Batch 2013, Loss 0.5136351585388184\n","[Training Epoch 0] Batch 2014, Loss 0.4976629912853241\n","[Training Epoch 0] Batch 2015, Loss 0.5210303068161011\n","[Training Epoch 0] Batch 2016, Loss 0.4860948920249939\n","[Training Epoch 0] Batch 2017, Loss 0.512988805770874\n","[Training Epoch 0] Batch 2018, Loss 0.5160358548164368\n","[Training Epoch 0] Batch 2019, Loss 0.5047589540481567\n","[Training Epoch 0] Batch 2020, Loss 0.49431100487709045\n","[Training Epoch 0] Batch 2021, Loss 0.4951951503753662\n","[Training Epoch 0] Batch 2022, Loss 0.5017433166503906\n","[Training Epoch 0] Batch 2023, Loss 0.49167531728744507\n","[Training Epoch 0] Batch 2024, Loss 0.5146287679672241\n","[Training Epoch 0] Batch 2025, Loss 0.4860537648200989\n","[Training Epoch 0] Batch 2026, Loss 0.5247371196746826\n","[Training Epoch 0] Batch 2027, Loss 0.5190159678459167\n","[Training Epoch 0] Batch 2028, Loss 0.5103204250335693\n","[Training Epoch 0] Batch 2029, Loss 0.5023422241210938\n","[Training Epoch 0] Batch 2030, Loss 0.4903768301010132\n","[Training Epoch 0] Batch 2031, Loss 0.5058362483978271\n","[Training Epoch 0] Batch 2032, Loss 0.517856240272522\n","[Training Epoch 0] Batch 2033, Loss 0.4819595217704773\n","[Training Epoch 0] Batch 2034, Loss 0.4927870035171509\n","[Training Epoch 0] Batch 2035, Loss 0.5080986022949219\n","[Training Epoch 0] Batch 2036, Loss 0.5077887773513794\n","[Training Epoch 0] Batch 2037, Loss 0.5353659987449646\n","[Training Epoch 0] Batch 2038, Loss 0.5077869892120361\n","[Training Epoch 0] Batch 2039, Loss 0.5111865401268005\n","[Training Epoch 0] Batch 2040, Loss 0.5120615363121033\n","[Training Epoch 0] Batch 2041, Loss 0.5163999795913696\n","[Training Epoch 0] Batch 2042, Loss 0.5099708437919617\n","[Training Epoch 0] Batch 2043, Loss 0.5000065565109253\n","[Training Epoch 0] Batch 2044, Loss 0.47391441464424133\n","[Training Epoch 0] Batch 2045, Loss 0.5174216628074646\n","[Training Epoch 0] Batch 2046, Loss 0.48935335874557495\n","[Training Epoch 0] Batch 2047, Loss 0.4999522268772125\n","[Training Epoch 0] Batch 2048, Loss 0.5121042132377625\n","[Training Epoch 0] Batch 2049, Loss 0.5253140330314636\n","[Training Epoch 0] Batch 2050, Loss 0.5207386016845703\n","[Training Epoch 0] Batch 2051, Loss 0.518524169921875\n","[Training Epoch 0] Batch 2052, Loss 0.5205225348472595\n","[Training Epoch 0] Batch 2053, Loss 0.5020685195922852\n","[Training Epoch 0] Batch 2054, Loss 0.5370943546295166\n","[Training Epoch 0] Batch 2055, Loss 0.5064421892166138\n","[Training Epoch 0] Batch 2056, Loss 0.49558115005493164\n","[Training Epoch 0] Batch 2057, Loss 0.5066535472869873\n","[Training Epoch 0] Batch 2058, Loss 0.5029661655426025\n","[Training Epoch 0] Batch 2059, Loss 0.5360156893730164\n","[Training Epoch 0] Batch 2060, Loss 0.5143364667892456\n","[Training Epoch 0] Batch 2061, Loss 0.5050360560417175\n","[Training Epoch 0] Batch 2062, Loss 0.48354360461235046\n","[Training Epoch 0] Batch 2063, Loss 0.4898347556591034\n","[Training Epoch 0] Batch 2064, Loss 0.5238022804260254\n","[Training Epoch 0] Batch 2065, Loss 0.5097580552101135\n","[Training Epoch 0] Batch 2066, Loss 0.4832345247268677\n","[Training Epoch 0] Batch 2067, Loss 0.4929782748222351\n","[Training Epoch 0] Batch 2068, Loss 0.5182412266731262\n","[Training Epoch 0] Batch 2069, Loss 0.47214820981025696\n","[Training Epoch 0] Batch 2070, Loss 0.49848535656929016\n","[Training Epoch 0] Batch 2071, Loss 0.5061535239219666\n","[Training Epoch 0] Batch 2072, Loss 0.5160454511642456\n","[Training Epoch 0] Batch 2073, Loss 0.4983164668083191\n","[Training Epoch 0] Batch 2074, Loss 0.5139211416244507\n","[Training Epoch 0] Batch 2075, Loss 0.4972091317176819\n","[Training Epoch 0] Batch 2076, Loss 0.5005630850791931\n","[Training Epoch 0] Batch 2077, Loss 0.5016072988510132\n","[Training Epoch 0] Batch 2078, Loss 0.5313805937767029\n","[Training Epoch 0] Batch 2079, Loss 0.5137701034545898\n","[Training Epoch 0] Batch 2080, Loss 0.4941817820072174\n","[Training Epoch 0] Batch 2081, Loss 0.5043861865997314\n","[Training Epoch 0] Batch 2082, Loss 0.479508638381958\n","[Training Epoch 0] Batch 2083, Loss 0.4975755214691162\n","[Training Epoch 0] Batch 2084, Loss 0.5169128179550171\n","[Training Epoch 0] Batch 2085, Loss 0.5169717073440552\n","[Training Epoch 0] Batch 2086, Loss 0.5239028334617615\n","[Training Epoch 0] Batch 2087, Loss 0.5039850473403931\n","[Training Epoch 0] Batch 2088, Loss 0.5238738059997559\n","[Training Epoch 0] Batch 2089, Loss 0.5015695095062256\n","[Training Epoch 0] Batch 2090, Loss 0.5205081701278687\n","[Training Epoch 0] Batch 2091, Loss 0.48960158228874207\n","[Training Epoch 0] Batch 2092, Loss 0.5137826800346375\n","[Training Epoch 0] Batch 2093, Loss 0.4984302520751953\n","[Training Epoch 0] Batch 2094, Loss 0.5328503847122192\n","[Training Epoch 0] Batch 2095, Loss 0.5302001237869263\n","[Training Epoch 0] Batch 2096, Loss 0.5234693288803101\n","[Training Epoch 0] Batch 2097, Loss 0.5171664953231812\n","[Training Epoch 0] Batch 2098, Loss 0.4882224202156067\n","[Training Epoch 0] Batch 2099, Loss 0.4860842525959015\n","[Training Epoch 0] Batch 2100, Loss 0.5042438507080078\n","[Training Epoch 0] Batch 2101, Loss 0.5037912726402283\n","[Training Epoch 0] Batch 2102, Loss 0.5058104991912842\n","[Training Epoch 0] Batch 2103, Loss 0.5114306211471558\n","[Training Epoch 0] Batch 2104, Loss 0.4944806396961212\n","[Training Epoch 0] Batch 2105, Loss 0.5054851770401001\n","[Training Epoch 0] Batch 2106, Loss 0.5172276496887207\n","[Training Epoch 0] Batch 2107, Loss 0.5215452909469604\n","[Training Epoch 0] Batch 2108, Loss 0.5130709409713745\n","[Training Epoch 0] Batch 2109, Loss 0.5013480186462402\n","[Training Epoch 0] Batch 2110, Loss 0.4988890290260315\n","[Training Epoch 0] Batch 2111, Loss 0.5178290605545044\n","[Training Epoch 0] Batch 2112, Loss 0.5125407576560974\n","[Training Epoch 0] Batch 2113, Loss 0.5169813632965088\n","[Training Epoch 0] Batch 2114, Loss 0.47579312324523926\n","[Training Epoch 0] Batch 2115, Loss 0.5179536938667297\n","[Training Epoch 0] Batch 2116, Loss 0.48885470628738403\n","[Training Epoch 0] Batch 2117, Loss 0.5089437365531921\n","[Training Epoch 0] Batch 2118, Loss 0.4809221029281616\n","[Training Epoch 0] Batch 2119, Loss 0.5138776302337646\n","[Training Epoch 0] Batch 2120, Loss 0.5105240345001221\n","[Training Epoch 0] Batch 2121, Loss 0.49074190855026245\n","[Training Epoch 0] Batch 2122, Loss 0.5057445764541626\n","[Training Epoch 0] Batch 2123, Loss 0.5151880383491516\n","[Training Epoch 0] Batch 2124, Loss 0.5112731456756592\n","[Training Epoch 0] Batch 2125, Loss 0.530112624168396\n","[Training Epoch 0] Batch 2126, Loss 0.4988771975040436\n","[Training Epoch 0] Batch 2127, Loss 0.5233662128448486\n","[Training Epoch 0] Batch 2128, Loss 0.5201719999313354\n","[Training Epoch 0] Batch 2129, Loss 0.4997492730617523\n","[Training Epoch 0] Batch 2130, Loss 0.5058136582374573\n","[Training Epoch 0] Batch 2131, Loss 0.5089297294616699\n","[Training Epoch 0] Batch 2132, Loss 0.5146650671958923\n","[Training Epoch 0] Batch 2133, Loss 0.5065152645111084\n","[Training Epoch 0] Batch 2134, Loss 0.5072460174560547\n","[Training Epoch 0] Batch 2135, Loss 0.5233101844787598\n","[Training Epoch 0] Batch 2136, Loss 0.5084747672080994\n","[Training Epoch 0] Batch 2137, Loss 0.48278915882110596\n","[Training Epoch 0] Batch 2138, Loss 0.5052331686019897\n","[Training Epoch 0] Batch 2139, Loss 0.507575273513794\n","[Training Epoch 0] Batch 2140, Loss 0.4923124313354492\n","[Training Epoch 0] Batch 2141, Loss 0.5153087377548218\n","[Training Epoch 0] Batch 2142, Loss 0.503491997718811\n","[Training Epoch 0] Batch 2143, Loss 0.5073583722114563\n","[Training Epoch 0] Batch 2144, Loss 0.5186899900436401\n","[Training Epoch 0] Batch 2145, Loss 0.497697651386261\n","[Training Epoch 0] Batch 2146, Loss 0.4930416941642761\n","[Training Epoch 0] Batch 2147, Loss 0.5063642859458923\n","[Training Epoch 0] Batch 2148, Loss 0.5127972364425659\n","[Training Epoch 0] Batch 2149, Loss 0.5063967704772949\n","[Training Epoch 0] Batch 2150, Loss 0.4964042007923126\n","[Training Epoch 0] Batch 2151, Loss 0.5321947336196899\n","[Training Epoch 0] Batch 2152, Loss 0.4977787137031555\n","[Training Epoch 0] Batch 2153, Loss 0.5065476298332214\n","[Training Epoch 0] Batch 2154, Loss 0.5221891403198242\n","[Training Epoch 0] Batch 2155, Loss 0.5010307431221008\n","[Training Epoch 0] Batch 2156, Loss 0.5084446668624878\n","[Training Epoch 0] Batch 2157, Loss 0.5025255084037781\n","[Training Epoch 0] Batch 2158, Loss 0.49420857429504395\n","[Training Epoch 0] Batch 2159, Loss 0.5108243227005005\n","[Training Epoch 0] Batch 2160, Loss 0.5210202932357788\n","[Training Epoch 0] Batch 2161, Loss 0.5083984732627869\n","[Training Epoch 0] Batch 2162, Loss 0.5178519487380981\n","[Training Epoch 0] Batch 2163, Loss 0.5040771961212158\n","[Training Epoch 0] Batch 2164, Loss 0.5104292035102844\n","[Training Epoch 0] Batch 2165, Loss 0.500623345375061\n","[Training Epoch 0] Batch 2166, Loss 0.5004392862319946\n","[Training Epoch 0] Batch 2167, Loss 0.4826146960258484\n","[Training Epoch 0] Batch 2168, Loss 0.5152193903923035\n","[Training Epoch 0] Batch 2169, Loss 0.4901951849460602\n","[Training Epoch 0] Batch 2170, Loss 0.5048403739929199\n","[Training Epoch 0] Batch 2171, Loss 0.5149883031845093\n","[Training Epoch 0] Batch 2172, Loss 0.5127763748168945\n","[Training Epoch 0] Batch 2173, Loss 0.5095176696777344\n","[Training Epoch 0] Batch 2174, Loss 0.5331898927688599\n","[Training Epoch 0] Batch 2175, Loss 0.4834239184856415\n","[Training Epoch 0] Batch 2176, Loss 0.49933356046676636\n","[Training Epoch 0] Batch 2177, Loss 0.5117826461791992\n","[Training Epoch 0] Batch 2178, Loss 0.49816733598709106\n","[Training Epoch 0] Batch 2179, Loss 0.5126473903656006\n","[Training Epoch 0] Batch 2180, Loss 0.48211562633514404\n","[Training Epoch 0] Batch 2181, Loss 0.5071988701820374\n","[Training Epoch 0] Batch 2182, Loss 0.4980882406234741\n","[Training Epoch 0] Batch 2183, Loss 0.4763450622558594\n","[Training Epoch 0] Batch 2184, Loss 0.4859124720096588\n","[Training Epoch 0] Batch 2185, Loss 0.5050051212310791\n","[Training Epoch 0] Batch 2186, Loss 0.519562840461731\n","[Training Epoch 0] Batch 2187, Loss 0.5193521976470947\n","[Training Epoch 0] Batch 2188, Loss 0.518333911895752\n","[Training Epoch 0] Batch 2189, Loss 0.4980138838291168\n","[Training Epoch 0] Batch 2190, Loss 0.4832046627998352\n","[Training Epoch 0] Batch 2191, Loss 0.5070662498474121\n","[Training Epoch 0] Batch 2192, Loss 0.509368896484375\n","[Training Epoch 0] Batch 2193, Loss 0.529963493347168\n","[Training Epoch 0] Batch 2194, Loss 0.5149619579315186\n","[Training Epoch 0] Batch 2195, Loss 0.5141010284423828\n","[Training Epoch 0] Batch 2196, Loss 0.49774888157844543\n","[Training Epoch 0] Batch 2197, Loss 0.4945077896118164\n","[Training Epoch 0] Batch 2198, Loss 0.5118188858032227\n","[Training Epoch 0] Batch 2199, Loss 0.5090644955635071\n","[Training Epoch 0] Batch 2200, Loss 0.4966968894004822\n","[Training Epoch 0] Batch 2201, Loss 0.5140247941017151\n","[Training Epoch 0] Batch 2202, Loss 0.502463698387146\n","[Training Epoch 0] Batch 2203, Loss 0.520717978477478\n","[Training Epoch 0] Batch 2204, Loss 0.5160955190658569\n","[Training Epoch 0] Batch 2205, Loss 0.4935683608055115\n","[Training Epoch 0] Batch 2206, Loss 0.5125919580459595\n","[Training Epoch 0] Batch 2207, Loss 0.4916730523109436\n","[Training Epoch 0] Batch 2208, Loss 0.5056419968605042\n","[Training Epoch 0] Batch 2209, Loss 0.51711106300354\n","[Training Epoch 0] Batch 2210, Loss 0.49436917901039124\n","[Training Epoch 0] Batch 2211, Loss 0.5057362914085388\n","[Training Epoch 0] Batch 2212, Loss 0.49419277906417847\n","[Training Epoch 0] Batch 2213, Loss 0.5010223388671875\n","[Training Epoch 0] Batch 2214, Loss 0.4860590100288391\n","[Training Epoch 0] Batch 2215, Loss 0.5160657167434692\n","[Training Epoch 0] Batch 2216, Loss 0.4712511897087097\n","[Training Epoch 0] Batch 2217, Loss 0.5290039777755737\n","[Training Epoch 0] Batch 2218, Loss 0.5171958804130554\n","[Training Epoch 0] Batch 2219, Loss 0.4979438781738281\n","[Training Epoch 0] Batch 2220, Loss 0.4931291341781616\n","[Training Epoch 0] Batch 2221, Loss 0.4873824715614319\n","[Training Epoch 0] Batch 2222, Loss 0.4760667681694031\n","[Training Epoch 0] Batch 2223, Loss 0.513568639755249\n","[Training Epoch 0] Batch 2224, Loss 0.5023168325424194\n","[Training Epoch 0] Batch 2225, Loss 0.4918402433395386\n","[Training Epoch 0] Batch 2226, Loss 0.5086078643798828\n","[Training Epoch 0] Batch 2227, Loss 0.5111849308013916\n","[Training Epoch 0] Batch 2228, Loss 0.5016768574714661\n","[Training Epoch 0] Batch 2229, Loss 0.49163445830345154\n","[Training Epoch 0] Batch 2230, Loss 0.5092455148696899\n","[Training Epoch 0] Batch 2231, Loss 0.49243873357772827\n","[Training Epoch 0] Batch 2232, Loss 0.5096085667610168\n","[Training Epoch 0] Batch 2233, Loss 0.49996963143348694\n","[Training Epoch 0] Batch 2234, Loss 0.5142316818237305\n","[Training Epoch 0] Batch 2235, Loss 0.5066632032394409\n","[Training Epoch 0] Batch 2236, Loss 0.5019481182098389\n","[Training Epoch 0] Batch 2237, Loss 0.5102260112762451\n","[Training Epoch 0] Batch 2238, Loss 0.5271997451782227\n","[Training Epoch 0] Batch 2239, Loss 0.4849131107330322\n","[Training Epoch 0] Batch 2240, Loss 0.5033049583435059\n","[Training Epoch 0] Batch 2241, Loss 0.5099271535873413\n","[Training Epoch 0] Batch 2242, Loss 0.5101966857910156\n","[Training Epoch 0] Batch 2243, Loss 0.4791926145553589\n","[Training Epoch 0] Batch 2244, Loss 0.48254871368408203\n","[Training Epoch 0] Batch 2245, Loss 0.4853251874446869\n","[Training Epoch 0] Batch 2246, Loss 0.4893226623535156\n","[Training Epoch 0] Batch 2247, Loss 0.5051727294921875\n","[Training Epoch 0] Batch 2248, Loss 0.5181806087493896\n","[Training Epoch 0] Batch 2249, Loss 0.4892747104167938\n","[Training Epoch 0] Batch 2250, Loss 0.5259292125701904\n","[Training Epoch 0] Batch 2251, Loss 0.49996304512023926\n","[Training Epoch 0] Batch 2252, Loss 0.5097317695617676\n","[Training Epoch 0] Batch 2253, Loss 0.5005561709403992\n","[Training Epoch 0] Batch 2254, Loss 0.5168496370315552\n","[Training Epoch 0] Batch 2255, Loss 0.4965350031852722\n","[Training Epoch 0] Batch 2256, Loss 0.5259689092636108\n","[Training Epoch 0] Batch 2257, Loss 0.4835180640220642\n","[Training Epoch 0] Batch 2258, Loss 0.5074950456619263\n","[Training Epoch 0] Batch 2259, Loss 0.5180363059043884\n","[Training Epoch 0] Batch 2260, Loss 0.4764048755168915\n","[Training Epoch 0] Batch 2261, Loss 0.5061614513397217\n","[Training Epoch 0] Batch 2262, Loss 0.4950602054595947\n","[Training Epoch 0] Batch 2263, Loss 0.4971303939819336\n","[Training Epoch 0] Batch 2264, Loss 0.5318586826324463\n","[Training Epoch 0] Batch 2265, Loss 0.5062529444694519\n","[Training Epoch 0] Batch 2266, Loss 0.5204803943634033\n","[Training Epoch 0] Batch 2267, Loss 0.4902873933315277\n","[Training Epoch 0] Batch 2268, Loss 0.5174123048782349\n","[Training Epoch 0] Batch 2269, Loss 0.5190213918685913\n","[Training Epoch 0] Batch 2270, Loss 0.49006956815719604\n","[Training Epoch 0] Batch 2271, Loss 0.5144151449203491\n","[Training Epoch 0] Batch 2272, Loss 0.5323222875595093\n","[Training Epoch 0] Batch 2273, Loss 0.49687185883522034\n","[Training Epoch 0] Batch 2274, Loss 0.5176377296447754\n","[Training Epoch 0] Batch 2275, Loss 0.5178654193878174\n","[Training Epoch 0] Batch 2276, Loss 0.5029955506324768\n","[Training Epoch 0] Batch 2277, Loss 0.49371618032455444\n","[Training Epoch 0] Batch 2278, Loss 0.5311738848686218\n","[Training Epoch 0] Batch 2279, Loss 0.5042662024497986\n","[Training Epoch 0] Batch 2280, Loss 0.4860081076622009\n","[Training Epoch 0] Batch 2281, Loss 0.48991984128952026\n","[Training Epoch 0] Batch 2282, Loss 0.48583146929740906\n","[Training Epoch 0] Batch 2283, Loss 0.5133380889892578\n","[Training Epoch 0] Batch 2284, Loss 0.5143133997917175\n","[Training Epoch 0] Batch 2285, Loss 0.5063216686248779\n","[Training Epoch 0] Batch 2286, Loss 0.497041255235672\n","[Training Epoch 0] Batch 2287, Loss 0.510441780090332\n","[Training Epoch 0] Batch 2288, Loss 0.4769524931907654\n","[Training Epoch 0] Batch 2289, Loss 0.48427435755729675\n","[Training Epoch 0] Batch 2290, Loss 0.5032563209533691\n","[Training Epoch 0] Batch 2291, Loss 0.4919123649597168\n","[Training Epoch 0] Batch 2292, Loss 0.5036318898200989\n","[Training Epoch 0] Batch 2293, Loss 0.5116525888442993\n","[Training Epoch 0] Batch 2294, Loss 0.5208799242973328\n","[Training Epoch 0] Batch 2295, Loss 0.4675162434577942\n","[Training Epoch 0] Batch 2296, Loss 0.5253869295120239\n","[Training Epoch 0] Batch 2297, Loss 0.47775983810424805\n","[Training Epoch 0] Batch 2298, Loss 0.5065144300460815\n","[Training Epoch 0] Batch 2299, Loss 0.5116048455238342\n","[Training Epoch 0] Batch 2300, Loss 0.4852805435657501\n","[Training Epoch 0] Batch 2301, Loss 0.4954831600189209\n","[Training Epoch 0] Batch 2302, Loss 0.48405373096466064\n","[Training Epoch 0] Batch 2303, Loss 0.495476633310318\n","[Training Epoch 0] Batch 2304, Loss 0.49793046712875366\n","[Training Epoch 0] Batch 2305, Loss 0.49009910225868225\n","[Training Epoch 0] Batch 2306, Loss 0.49893489480018616\n","[Training Epoch 0] Batch 2307, Loss 0.5282176733016968\n","[Training Epoch 0] Batch 2308, Loss 0.4814833402633667\n","[Training Epoch 0] Batch 2309, Loss 0.504993200302124\n","[Training Epoch 0] Batch 2310, Loss 0.5288103222846985\n","[Training Epoch 0] Batch 2311, Loss 0.5433002710342407\n","[Training Epoch 0] Batch 2312, Loss 0.5090811252593994\n","[Training Epoch 0] Batch 2313, Loss 0.5123237371444702\n","[Training Epoch 0] Batch 2314, Loss 0.4839814305305481\n","[Training Epoch 0] Batch 2315, Loss 0.5009903311729431\n","[Training Epoch 0] Batch 2316, Loss 0.5082294344902039\n","[Training Epoch 0] Batch 2317, Loss 0.526242196559906\n","[Training Epoch 0] Batch 2318, Loss 0.5032594203948975\n","[Training Epoch 0] Batch 2319, Loss 0.5106865167617798\n","[Training Epoch 0] Batch 2320, Loss 0.5164541602134705\n","[Training Epoch 0] Batch 2321, Loss 0.503891110420227\n","[Training Epoch 0] Batch 2322, Loss 0.47679373621940613\n","[Training Epoch 0] Batch 2323, Loss 0.5118981599807739\n","[Training Epoch 0] Batch 2324, Loss 0.5325214266777039\n","[Training Epoch 0] Batch 2325, Loss 0.5044392943382263\n","[Training Epoch 0] Batch 2326, Loss 0.5198322534561157\n","[Training Epoch 0] Batch 2327, Loss 0.496920108795166\n","[Training Epoch 0] Batch 2328, Loss 0.5149674415588379\n","[Training Epoch 0] Batch 2329, Loss 0.49567368626594543\n","[Training Epoch 0] Batch 2330, Loss 0.5303503274917603\n","[Training Epoch 0] Batch 2331, Loss 0.48459023237228394\n","[Training Epoch 0] Batch 2332, Loss 0.5082177519798279\n","[Training Epoch 0] Batch 2333, Loss 0.5116022825241089\n","[Training Epoch 0] Batch 2334, Loss 0.47918832302093506\n","[Training Epoch 0] Batch 2335, Loss 0.5083000063896179\n","[Training Epoch 0] Batch 2336, Loss 0.5303856134414673\n","[Training Epoch 0] Batch 2337, Loss 0.5091800689697266\n","[Training Epoch 0] Batch 2338, Loss 0.49969446659088135\n","[Training Epoch 0] Batch 2339, Loss 0.48545384407043457\n","[Training Epoch 0] Batch 2340, Loss 0.5102667808532715\n","[Training Epoch 0] Batch 2341, Loss 0.504730224609375\n","[Training Epoch 0] Batch 2342, Loss 0.4903848171234131\n","[Training Epoch 0] Batch 2343, Loss 0.4834133982658386\n","[Training Epoch 0] Batch 2344, Loss 0.4469239115715027\n","[Training Epoch 0] Batch 2345, Loss 0.507121205329895\n","[Training Epoch 0] Batch 2346, Loss 0.5129340291023254\n","[Training Epoch 0] Batch 2347, Loss 0.49397820234298706\n","[Training Epoch 0] Batch 2348, Loss 0.5105557441711426\n","[Training Epoch 0] Batch 2349, Loss 0.47513872385025024\n","[Training Epoch 0] Batch 2350, Loss 0.5162519216537476\n","[Training Epoch 0] Batch 2351, Loss 0.4799552261829376\n","[Training Epoch 0] Batch 2352, Loss 0.5142037868499756\n","[Training Epoch 0] Batch 2353, Loss 0.5294265151023865\n","[Training Epoch 0] Batch 2354, Loss 0.49975499510765076\n","[Training Epoch 0] Batch 2355, Loss 0.5089439153671265\n","[Training Epoch 0] Batch 2356, Loss 0.5147889852523804\n","[Training Epoch 0] Batch 2357, Loss 0.5117666721343994\n","[Training Epoch 0] Batch 2358, Loss 0.48890116810798645\n","[Training Epoch 0] Batch 2359, Loss 0.5089767575263977\n","[Training Epoch 0] Batch 2360, Loss 0.5280119180679321\n","[Training Epoch 0] Batch 2361, Loss 0.5206456184387207\n","[Training Epoch 0] Batch 2362, Loss 0.4996517598628998\n","[Training Epoch 0] Batch 2363, Loss 0.5045305490493774\n","[Training Epoch 0] Batch 2364, Loss 0.5023320913314819\n","[Training Epoch 0] Batch 2365, Loss 0.5028292536735535\n","[Training Epoch 0] Batch 2366, Loss 0.4867423176765442\n","[Training Epoch 0] Batch 2367, Loss 0.513587474822998\n","[Training Epoch 0] Batch 2368, Loss 0.5020096302032471\n","[Training Epoch 0] Batch 2369, Loss 0.48885858058929443\n","[Training Epoch 0] Batch 2370, Loss 0.5159507989883423\n","[Training Epoch 0] Batch 2371, Loss 0.5295481085777283\n","[Training Epoch 0] Batch 2372, Loss 0.4746275544166565\n","[Training Epoch 0] Batch 2373, Loss 0.48778194189071655\n","[Training Epoch 0] Batch 2374, Loss 0.5241899490356445\n","[Training Epoch 0] Batch 2375, Loss 0.5053959488868713\n","[Training Epoch 0] Batch 2376, Loss 0.48383447527885437\n","[Training Epoch 0] Batch 2377, Loss 0.4971804916858673\n","[Training Epoch 0] Batch 2378, Loss 0.4974070191383362\n","[Training Epoch 0] Batch 2379, Loss 0.5056125521659851\n","[Training Epoch 0] Batch 2380, Loss 0.5186977386474609\n","[Training Epoch 0] Batch 2381, Loss 0.4910048842430115\n","[Training Epoch 0] Batch 2382, Loss 0.5125612020492554\n","[Training Epoch 0] Batch 2383, Loss 0.5087468028068542\n","[Training Epoch 0] Batch 2384, Loss 0.49606168270111084\n","[Training Epoch 0] Batch 2385, Loss 0.49608153104782104\n","[Training Epoch 0] Batch 2386, Loss 0.47440105676651\n","[Training Epoch 0] Batch 2387, Loss 0.5086758732795715\n","[Training Epoch 0] Batch 2388, Loss 0.4910503029823303\n","[Training Epoch 0] Batch 2389, Loss 0.5030532479286194\n","[Training Epoch 0] Batch 2390, Loss 0.5054239630699158\n","[Training Epoch 0] Batch 2391, Loss 0.5010679960250854\n","[Training Epoch 0] Batch 2392, Loss 0.48943978548049927\n","[Training Epoch 0] Batch 2393, Loss 0.5011311769485474\n","[Training Epoch 0] Batch 2394, Loss 0.4971165060997009\n","[Training Epoch 0] Batch 2395, Loss 0.5114895701408386\n","[Training Epoch 0] Batch 2396, Loss 0.4730590283870697\n","[Training Epoch 0] Batch 2397, Loss 0.507072925567627\n","[Training Epoch 0] Batch 2398, Loss 0.5007284879684448\n","[Training Epoch 0] Batch 2399, Loss 0.5053411722183228\n","[Training Epoch 0] Batch 2400, Loss 0.5411900281906128\n","[Training Epoch 0] Batch 2401, Loss 0.4838581383228302\n","[Training Epoch 0] Batch 2402, Loss 0.5038655996322632\n","[Training Epoch 0] Batch 2403, Loss 0.47930294275283813\n","[Training Epoch 0] Batch 2404, Loss 0.5049020648002625\n","[Training Epoch 0] Batch 2405, Loss 0.48998546600341797\n","[Training Epoch 0] Batch 2406, Loss 0.5057789087295532\n","[Training Epoch 0] Batch 2407, Loss 0.5350146889686584\n","[Training Epoch 0] Batch 2408, Loss 0.4883807897567749\n","[Training Epoch 0] Batch 2409, Loss 0.5271154642105103\n","[Training Epoch 0] Batch 2410, Loss 0.5173908472061157\n","[Training Epoch 0] Batch 2411, Loss 0.5129458904266357\n","[Training Epoch 0] Batch 2412, Loss 0.4826803207397461\n","[Training Epoch 0] Batch 2413, Loss 0.49544209241867065\n","[Training Epoch 0] Batch 2414, Loss 0.48153451085090637\n","[Training Epoch 0] Batch 2415, Loss 0.47549164295196533\n","[Training Epoch 0] Batch 2416, Loss 0.48602890968322754\n","[Training Epoch 0] Batch 2417, Loss 0.47411030530929565\n","[Training Epoch 0] Batch 2418, Loss 0.4920610189437866\n","[Training Epoch 0] Batch 2419, Loss 0.4931166172027588\n","[Training Epoch 0] Batch 2420, Loss 0.5460007190704346\n","[Training Epoch 0] Batch 2421, Loss 0.5026882290840149\n","[Training Epoch 0] Batch 2422, Loss 0.4862312376499176\n","[Training Epoch 0] Batch 2423, Loss 0.48049288988113403\n","[Training Epoch 0] Batch 2424, Loss 0.5026752352714539\n","[Training Epoch 0] Batch 2425, Loss 0.5086070895195007\n","[Training Epoch 0] Batch 2426, Loss 0.5073471665382385\n","[Training Epoch 0] Batch 2427, Loss 0.52527916431427\n","[Training Epoch 0] Batch 2428, Loss 0.5000321269035339\n","[Training Epoch 0] Batch 2429, Loss 0.5100218653678894\n","[Training Epoch 0] Batch 2430, Loss 0.4909808933734894\n","[Training Epoch 0] Batch 2431, Loss 0.49783843755722046\n","[Training Epoch 0] Batch 2432, Loss 0.4882180690765381\n","[Training Epoch 0] Batch 2433, Loss 0.4824857711791992\n","[Training Epoch 0] Batch 2434, Loss 0.5472551584243774\n","[Training Epoch 0] Batch 2435, Loss 0.5180980563163757\n","[Training Epoch 0] Batch 2436, Loss 0.5291591882705688\n","[Training Epoch 0] Batch 2437, Loss 0.5158596038818359\n","[Training Epoch 0] Batch 2438, Loss 0.49316293001174927\n","[Training Epoch 0] Batch 2439, Loss 0.4893251061439514\n","[Training Epoch 0] Batch 2440, Loss 0.516311526298523\n","[Training Epoch 0] Batch 2441, Loss 0.4953702688217163\n","[Training Epoch 0] Batch 2442, Loss 0.5049334764480591\n","[Training Epoch 0] Batch 2443, Loss 0.49483373761177063\n","[Training Epoch 0] Batch 2444, Loss 0.5082111954689026\n","[Training Epoch 0] Batch 2445, Loss 0.4847741723060608\n","[Training Epoch 0] Batch 2446, Loss 0.5306520462036133\n","[Training Epoch 0] Batch 2447, Loss 0.5210403203964233\n","[Training Epoch 0] Batch 2448, Loss 0.5194645524024963\n","[Training Epoch 0] Batch 2449, Loss 0.46759164333343506\n","[Training Epoch 0] Batch 2450, Loss 0.5280373096466064\n","[Training Epoch 0] Batch 2451, Loss 0.4953848123550415\n","[Training Epoch 0] Batch 2452, Loss 0.5090368986129761\n","[Training Epoch 0] Batch 2453, Loss 0.4994522035121918\n","[Training Epoch 0] Batch 2454, Loss 0.5389127731323242\n","[Training Epoch 0] Batch 2455, Loss 0.49761536717414856\n","[Training Epoch 0] Batch 2456, Loss 0.5145729780197144\n","[Training Epoch 0] Batch 2457, Loss 0.4902363419532776\n","[Training Epoch 0] Batch 2458, Loss 0.5086019039154053\n","[Training Epoch 0] Batch 2459, Loss 0.48446589708328247\n","[Training Epoch 0] Batch 2460, Loss 0.5037566423416138\n","[Training Epoch 0] Batch 2461, Loss 0.48143813014030457\n","[Training Epoch 0] Batch 2462, Loss 0.4978675842285156\n","[Training Epoch 0] Batch 2463, Loss 0.5094985365867615\n","[Training Epoch 0] Batch 2464, Loss 0.5168570280075073\n","[Training Epoch 0] Batch 2465, Loss 0.5085758566856384\n","[Training Epoch 0] Batch 2466, Loss 0.4987953007221222\n","[Training Epoch 0] Batch 2467, Loss 0.4868978261947632\n","[Training Epoch 0] Batch 2468, Loss 0.4805014431476593\n","[Training Epoch 0] Batch 2469, Loss 0.48939985036849976\n","[Training Epoch 0] Batch 2470, Loss 0.48349514603614807\n","[Training Epoch 0] Batch 2471, Loss 0.5190813541412354\n","[Training Epoch 0] Batch 2472, Loss 0.49528372287750244\n","[Training Epoch 0] Batch 2473, Loss 0.47484368085861206\n","[Training Epoch 0] Batch 2474, Loss 0.46606048941612244\n","[Training Epoch 0] Batch 2475, Loss 0.5012305974960327\n","[Training Epoch 0] Batch 2476, Loss 0.5026265978813171\n","[Training Epoch 0] Batch 2477, Loss 0.4844190776348114\n","[Training Epoch 0] Batch 2478, Loss 0.4942655563354492\n","[Training Epoch 0] Batch 2479, Loss 0.515923261642456\n","[Training Epoch 0] Batch 2480, Loss 0.510780930519104\n","[Training Epoch 0] Batch 2481, Loss 0.5011512041091919\n","[Training Epoch 0] Batch 2482, Loss 0.5002532601356506\n","[Training Epoch 0] Batch 2483, Loss 0.5013732314109802\n","[Training Epoch 0] Batch 2484, Loss 0.5034828186035156\n","[Training Epoch 0] Batch 2485, Loss 0.5106703042984009\n","[Training Epoch 0] Batch 2486, Loss 0.48598915338516235\n","[Training Epoch 0] Batch 2487, Loss 0.47684481739997864\n","[Training Epoch 0] Batch 2488, Loss 0.5037412047386169\n","[Training Epoch 0] Batch 2489, Loss 0.5212788581848145\n","[Training Epoch 0] Batch 2490, Loss 0.5086653232574463\n","[Training Epoch 0] Batch 2491, Loss 0.49174764752388\n","[Training Epoch 0] Batch 2492, Loss 0.5097063779830933\n","[Training Epoch 0] Batch 2493, Loss 0.5039742588996887\n","[Training Epoch 0] Batch 2494, Loss 0.5253000259399414\n","[Training Epoch 0] Batch 2495, Loss 0.512081503868103\n","[Training Epoch 0] Batch 2496, Loss 0.4928893446922302\n","[Training Epoch 0] Batch 2497, Loss 0.5046732425689697\n","[Training Epoch 0] Batch 2498, Loss 0.516869306564331\n","[Training Epoch 0] Batch 2499, Loss 0.4950796365737915\n","[Training Epoch 0] Batch 2500, Loss 0.529168963432312\n","[Training Epoch 0] Batch 2501, Loss 0.48296311497688293\n","[Training Epoch 0] Batch 2502, Loss 0.5095434784889221\n","[Training Epoch 0] Batch 2503, Loss 0.4975135624408722\n","[Training Epoch 0] Batch 2504, Loss 0.4916645586490631\n","[Training Epoch 0] Batch 2505, Loss 0.5115672945976257\n","[Training Epoch 0] Batch 2506, Loss 0.499919593334198\n","[Training Epoch 0] Batch 2507, Loss 0.45635178685188293\n","[Training Epoch 0] Batch 2508, Loss 0.5117980241775513\n","[Training Epoch 0] Batch 2509, Loss 0.5172187089920044\n","[Training Epoch 0] Batch 2510, Loss 0.5035373568534851\n","[Training Epoch 0] Batch 2511, Loss 0.5337884426116943\n","[Training Epoch 0] Batch 2512, Loss 0.5206924080848694\n","[Training Epoch 0] Batch 2513, Loss 0.49474629759788513\n","[Training Epoch 0] Batch 2514, Loss 0.4995957016944885\n","[Training Epoch 0] Batch 2515, Loss 0.5006920099258423\n","[Training Epoch 0] Batch 2516, Loss 0.5095927119255066\n","[Training Epoch 0] Batch 2517, Loss 0.5093441009521484\n","[Training Epoch 0] Batch 2518, Loss 0.5085113644599915\n","[Training Epoch 0] Batch 2519, Loss 0.5020008087158203\n","[Training Epoch 0] Batch 2520, Loss 0.5033341646194458\n","[Training Epoch 0] Batch 2521, Loss 0.49995309114456177\n","[Training Epoch 0] Batch 2522, Loss 0.5031045079231262\n","[Training Epoch 0] Batch 2523, Loss 0.4854832887649536\n","[Training Epoch 0] Batch 2524, Loss 0.5058484077453613\n","[Training Epoch 0] Batch 2525, Loss 0.49857255816459656\n","[Training Epoch 0] Batch 2526, Loss 0.5021758675575256\n","[Training Epoch 0] Batch 2527, Loss 0.5117874145507812\n","[Training Epoch 0] Batch 2528, Loss 0.4908667206764221\n","[Training Epoch 0] Batch 2529, Loss 0.49497169256210327\n","[Training Epoch 0] Batch 2530, Loss 0.5051002502441406\n","[Training Epoch 0] Batch 2531, Loss 0.5098665952682495\n","[Training Epoch 0] Batch 2532, Loss 0.5157372951507568\n","[Training Epoch 0] Batch 2533, Loss 0.49904268980026245\n","[Training Epoch 0] Batch 2534, Loss 0.4994397759437561\n","[Training Epoch 0] Batch 2535, Loss 0.49760034680366516\n","[Training Epoch 0] Batch 2536, Loss 0.5289345383644104\n","[Training Epoch 0] Batch 2537, Loss 0.5159184336662292\n","[Training Epoch 0] Batch 2538, Loss 0.49397212266921997\n","[Training Epoch 0] Batch 2539, Loss 0.4844509959220886\n","[Training Epoch 0] Batch 2540, Loss 0.5056276321411133\n","[Training Epoch 0] Batch 2541, Loss 0.49866771697998047\n","[Training Epoch 0] Batch 2542, Loss 0.4847637414932251\n","[Training Epoch 0] Batch 2543, Loss 0.4884648323059082\n","[Training Epoch 0] Batch 2544, Loss 0.48277491331100464\n","[Training Epoch 0] Batch 2545, Loss 0.5030707120895386\n","[Training Epoch 0] Batch 2546, Loss 0.5065464973449707\n","[Training Epoch 0] Batch 2547, Loss 0.5018849968910217\n","[Training Epoch 0] Batch 2548, Loss 0.5107555389404297\n","[Training Epoch 0] Batch 2549, Loss 0.47357404232025146\n","[Training Epoch 0] Batch 2550, Loss 0.5010191798210144\n","[Training Epoch 0] Batch 2551, Loss 0.5022485852241516\n","[Training Epoch 0] Batch 2552, Loss 0.4888427257537842\n","[Training Epoch 0] Batch 2553, Loss 0.49551406502723694\n","[Training Epoch 0] Batch 2554, Loss 0.5059497356414795\n","[Training Epoch 0] Batch 2555, Loss 0.4810797870159149\n","[Training Epoch 0] Batch 2556, Loss 0.5079604983329773\n","[Training Epoch 0] Batch 2557, Loss 0.4974997937679291\n","[Training Epoch 0] Batch 2558, Loss 0.4813649356365204\n","[Training Epoch 0] Batch 2559, Loss 0.5164414644241333\n","[Training Epoch 0] Batch 2560, Loss 0.5042543411254883\n","[Training Epoch 0] Batch 2561, Loss 0.5098937153816223\n","[Training Epoch 0] Batch 2562, Loss 0.5197009444236755\n","[Training Epoch 0] Batch 2563, Loss 0.5055330991744995\n","[Training Epoch 0] Batch 2564, Loss 0.5138687491416931\n","[Training Epoch 0] Batch 2565, Loss 0.5153651237487793\n","[Training Epoch 0] Batch 2566, Loss 0.5044274926185608\n","[Training Epoch 0] Batch 2567, Loss 0.5165947675704956\n","[Training Epoch 0] Batch 2568, Loss 0.5009355545043945\n","[Training Epoch 0] Batch 2569, Loss 0.48453083634376526\n","[Training Epoch 0] Batch 2570, Loss 0.4758473038673401\n","[Training Epoch 0] Batch 2571, Loss 0.46901288628578186\n","[Training Epoch 0] Batch 2572, Loss 0.5032941699028015\n","[Training Epoch 0] Batch 2573, Loss 0.5130745768547058\n","[Training Epoch 0] Batch 2574, Loss 0.5085216164588928\n","[Training Epoch 0] Batch 2575, Loss 0.4958432912826538\n","[Training Epoch 0] Batch 2576, Loss 0.5198816061019897\n","[Training Epoch 0] Batch 2577, Loss 0.5060209035873413\n","[Training Epoch 0] Batch 2578, Loss 0.4960412383079529\n","[Training Epoch 0] Batch 2579, Loss 0.48823282122612\n","[Training Epoch 0] Batch 2580, Loss 0.49271586537361145\n","[Training Epoch 0] Batch 2581, Loss 0.4965118169784546\n","[Training Epoch 0] Batch 2582, Loss 0.5386809706687927\n","[Training Epoch 0] Batch 2583, Loss 0.5029875040054321\n","[Training Epoch 0] Batch 2584, Loss 0.4982004761695862\n","[Training Epoch 0] Batch 2585, Loss 0.5039458274841309\n","[Training Epoch 0] Batch 2586, Loss 0.4993247985839844\n","[Training Epoch 0] Batch 2587, Loss 0.5203840732574463\n","[Training Epoch 0] Batch 2588, Loss 0.5013350248336792\n","[Training Epoch 0] Batch 2589, Loss 0.504048228263855\n","[Training Epoch 0] Batch 2590, Loss 0.5122552514076233\n","[Training Epoch 0] Batch 2591, Loss 0.49506378173828125\n","[Training Epoch 0] Batch 2592, Loss 0.4786584675312042\n","[Training Epoch 0] Batch 2593, Loss 0.5234509706497192\n","[Training Epoch 0] Batch 2594, Loss 0.5228606462478638\n","[Training Epoch 0] Batch 2595, Loss 0.5042937994003296\n","[Training Epoch 0] Batch 2596, Loss 0.47982409596443176\n","[Training Epoch 0] Batch 2597, Loss 0.5249923467636108\n","[Training Epoch 0] Batch 2598, Loss 0.5006626844406128\n","[Training Epoch 0] Batch 2599, Loss 0.5135153532028198\n","[Training Epoch 0] Batch 2600, Loss 0.490818053483963\n","[Training Epoch 0] Batch 2601, Loss 0.47300171852111816\n","[Training Epoch 0] Batch 2602, Loss 0.49196338653564453\n","[Training Epoch 0] Batch 2603, Loss 0.4925212860107422\n","[Training Epoch 0] Batch 2604, Loss 0.502740204334259\n","[Training Epoch 0] Batch 2605, Loss 0.49870187044143677\n","[Training Epoch 0] Batch 2606, Loss 0.4762377440929413\n","[Training Epoch 0] Batch 2607, Loss 0.5103507041931152\n","[Training Epoch 0] Batch 2608, Loss 0.5191684365272522\n","[Training Epoch 0] Batch 2609, Loss 0.49207305908203125\n","[Training Epoch 0] Batch 2610, Loss 0.4846475124359131\n","[Training Epoch 0] Batch 2611, Loss 0.5140040516853333\n","[Training Epoch 0] Batch 2612, Loss 0.4903647303581238\n","[Training Epoch 0] Batch 2613, Loss 0.49667006731033325\n","[Training Epoch 0] Batch 2614, Loss 0.5079929232597351\n","[Training Epoch 0] Batch 2615, Loss 0.503052294254303\n","[Training Epoch 0] Batch 2616, Loss 0.5205941200256348\n","[Training Epoch 0] Batch 2617, Loss 0.5043201446533203\n","[Training Epoch 0] Batch 2618, Loss 0.5054094195365906\n","[Training Epoch 0] Batch 2619, Loss 0.5061406493186951\n","[Training Epoch 0] Batch 2620, Loss 0.5065327286720276\n","[Training Epoch 0] Batch 2621, Loss 0.504797101020813\n","[Training Epoch 0] Batch 2622, Loss 0.5106515884399414\n","[Training Epoch 0] Batch 2623, Loss 0.5144890546798706\n","[Training Epoch 0] Batch 2624, Loss 0.5258487462997437\n","[Training Epoch 0] Batch 2625, Loss 0.4970954656600952\n","[Training Epoch 0] Batch 2626, Loss 0.4838716983795166\n","[Training Epoch 0] Batch 2627, Loss 0.5111125707626343\n","[Training Epoch 0] Batch 2628, Loss 0.49932610988616943\n","[Training Epoch 0] Batch 2629, Loss 0.49078473448753357\n","[Training Epoch 0] Batch 2630, Loss 0.5040333271026611\n","[Training Epoch 0] Batch 2631, Loss 0.5105870962142944\n","[Training Epoch 0] Batch 2632, Loss 0.5267335176467896\n","[Training Epoch 0] Batch 2633, Loss 0.4866952896118164\n","[Training Epoch 0] Batch 2634, Loss 0.4957542419433594\n","[Training Epoch 0] Batch 2635, Loss 0.536359429359436\n","[Training Epoch 0] Batch 2636, Loss 0.5163070559501648\n","[Training Epoch 0] Batch 2637, Loss 0.5056511163711548\n","[Training Epoch 0] Batch 2638, Loss 0.5104192495346069\n","[Training Epoch 0] Batch 2639, Loss 0.5180070400238037\n","[Training Epoch 0] Batch 2640, Loss 0.486908495426178\n","[Training Epoch 0] Batch 2641, Loss 0.5105059146881104\n","[Training Epoch 0] Batch 2642, Loss 0.5129884481430054\n","[Training Epoch 0] Batch 2643, Loss 0.5138295888900757\n","[Training Epoch 0] Batch 2644, Loss 0.5328919887542725\n","[Training Epoch 0] Batch 2645, Loss 0.5141828060150146\n","[Training Epoch 0] Batch 2646, Loss 0.5069175958633423\n","[Training Epoch 0] Batch 2647, Loss 0.5188654661178589\n","[Training Epoch 0] Batch 2648, Loss 0.5006478428840637\n","[Training Epoch 0] Batch 2649, Loss 0.49440422654151917\n","[Training Epoch 0] Batch 2650, Loss 0.5249785780906677\n","[Training Epoch 0] Batch 2651, Loss 0.49040210247039795\n","[Training Epoch 0] Batch 2652, Loss 0.4879312515258789\n","[Training Epoch 0] Batch 2653, Loss 0.5263334512710571\n","[Training Epoch 0] Batch 2654, Loss 0.5017351508140564\n","[Training Epoch 0] Batch 2655, Loss 0.46435314416885376\n","[Training Epoch 0] Batch 2656, Loss 0.477829247713089\n","[Training Epoch 0] Batch 2657, Loss 0.5267316102981567\n","[Training Epoch 0] Batch 2658, Loss 0.5005309581756592\n","[Training Epoch 0] Batch 2659, Loss 0.5158010125160217\n","[Training Epoch 0] Batch 2660, Loss 0.5005069971084595\n","[Training Epoch 0] Batch 2661, Loss 0.484635591506958\n","[Training Epoch 0] Batch 2662, Loss 0.4908018708229065\n","[Training Epoch 0] Batch 2663, Loss 0.5078853368759155\n","[Training Epoch 0] Batch 2664, Loss 0.48196184635162354\n","[Training Epoch 0] Batch 2665, Loss 0.4939609467983246\n","[Training Epoch 0] Batch 2666, Loss 0.5141640305519104\n","[Training Epoch 0] Batch 2667, Loss 0.4943208396434784\n","[Training Epoch 0] Batch 2668, Loss 0.4963536560535431\n","[Training Epoch 0] Batch 2669, Loss 0.49417656660079956\n","[Training Epoch 0] Batch 2670, Loss 0.4793359637260437\n","[Training Epoch 0] Batch 2671, Loss 0.5227038860321045\n","[Training Epoch 0] Batch 2672, Loss 0.4668198227882385\n","[Training Epoch 0] Batch 2673, Loss 0.4978454113006592\n","[Training Epoch 0] Batch 2674, Loss 0.5325843691825867\n","[Training Epoch 0] Batch 2675, Loss 0.48574820160865784\n","[Training Epoch 0] Batch 2676, Loss 0.5029146671295166\n","[Training Epoch 0] Batch 2677, Loss 0.4928760230541229\n","[Training Epoch 0] Batch 2678, Loss 0.4790891408920288\n","[Training Epoch 0] Batch 2679, Loss 0.4967941641807556\n","[Training Epoch 0] Batch 2680, Loss 0.48940253257751465\n","[Training Epoch 0] Batch 2681, Loss 0.491560697555542\n","[Training Epoch 0] Batch 2682, Loss 0.5203181505203247\n","[Training Epoch 0] Batch 2683, Loss 0.49300500750541687\n","[Training Epoch 0] Batch 2684, Loss 0.5003365278244019\n","[Training Epoch 0] Batch 2685, Loss 0.4852299094200134\n","[Training Epoch 0] Batch 2686, Loss 0.5040582418441772\n","[Training Epoch 0] Batch 2687, Loss 0.4841233491897583\n","[Training Epoch 0] Batch 2688, Loss 0.4863806664943695\n","[Training Epoch 0] Batch 2689, Loss 0.5430127382278442\n","[Training Epoch 0] Batch 2690, Loss 0.4889903962612152\n","[Training Epoch 0] Batch 2691, Loss 0.4943152964115143\n","[Training Epoch 0] Batch 2692, Loss 0.4939194619655609\n","[Training Epoch 0] Batch 2693, Loss 0.5139801502227783\n","[Training Epoch 0] Batch 2694, Loss 0.4740513563156128\n","[Training Epoch 0] Batch 2695, Loss 0.513859212398529\n","[Training Epoch 0] Batch 2696, Loss 0.49547046422958374\n","[Training Epoch 0] Batch 2697, Loss 0.5159995555877686\n","[Training Epoch 0] Batch 2698, Loss 0.5021573305130005\n","[Training Epoch 0] Batch 2699, Loss 0.4892347455024719\n","[Training Epoch 0] Batch 2700, Loss 0.5181698799133301\n","[Training Epoch 0] Batch 2701, Loss 0.5017410516738892\n","[Training Epoch 0] Batch 2702, Loss 0.49624449014663696\n","[Training Epoch 0] Batch 2703, Loss 0.4965428411960602\n","[Training Epoch 0] Batch 2704, Loss 0.4917443096637726\n","[Training Epoch 0] Batch 2705, Loss 0.44634151458740234\n","[Training Epoch 0] Batch 2706, Loss 0.5075733661651611\n","[Training Epoch 0] Batch 2707, Loss 0.49153459072113037\n","[Training Epoch 0] Batch 2708, Loss 0.5167514085769653\n","[Training Epoch 0] Batch 2709, Loss 0.4800298810005188\n","[Training Epoch 0] Batch 2710, Loss 0.5126744508743286\n","[Training Epoch 0] Batch 2711, Loss 0.48499351739883423\n","[Training Epoch 0] Batch 2712, Loss 0.5067077875137329\n","[Training Epoch 0] Batch 2713, Loss 0.46660083532333374\n","[Training Epoch 0] Batch 2714, Loss 0.48645609617233276\n","[Training Epoch 0] Batch 2715, Loss 0.5017423629760742\n","[Training Epoch 0] Batch 2716, Loss 0.4900253713130951\n","[Training Epoch 0] Batch 2717, Loss 0.4961957335472107\n","[Training Epoch 0] Batch 2718, Loss 0.5214207172393799\n","[Training Epoch 0] Batch 2719, Loss 0.48777443170547485\n","[Training Epoch 0] Batch 2720, Loss 0.4990684986114502\n","[Training Epoch 0] Batch 2721, Loss 0.5017561912536621\n","[Training Epoch 0] Batch 2722, Loss 0.48875564336776733\n","[Training Epoch 0] Batch 2723, Loss 0.5062460899353027\n","[Training Epoch 0] Batch 2724, Loss 0.5006715059280396\n","[Training Epoch 0] Batch 2725, Loss 0.5087628364562988\n","[Training Epoch 0] Batch 2726, Loss 0.496437132358551\n","[Training Epoch 0] Batch 2727, Loss 0.5065716505050659\n","[Training Epoch 0] Batch 2728, Loss 0.5025641322135925\n","[Training Epoch 0] Batch 2729, Loss 0.527906060218811\n","[Training Epoch 0] Batch 2730, Loss 0.486491322517395\n","[Training Epoch 0] Batch 2731, Loss 0.4828479290008545\n","[Training Epoch 0] Batch 2732, Loss 0.5126913785934448\n","[Training Epoch 0] Batch 2733, Loss 0.5125688314437866\n","[Training Epoch 0] Batch 2734, Loss 0.49277573823928833\n","[Training Epoch 0] Batch 2735, Loss 0.521720290184021\n","[Training Epoch 0] Batch 2736, Loss 0.5129228830337524\n","[Training Epoch 0] Batch 2737, Loss 0.47114914655685425\n","[Training Epoch 0] Batch 2738, Loss 0.4850228726863861\n","[Training Epoch 0] Batch 2739, Loss 0.5112081170082092\n","[Training Epoch 0] Batch 2740, Loss 0.4800845980644226\n","[Training Epoch 0] Batch 2741, Loss 0.49376264214515686\n","[Training Epoch 0] Batch 2742, Loss 0.5014637112617493\n","[Training Epoch 0] Batch 2743, Loss 0.5074869394302368\n","[Training Epoch 0] Batch 2744, Loss 0.4997574985027313\n","[Training Epoch 0] Batch 2745, Loss 0.5116918087005615\n","[Training Epoch 0] Batch 2746, Loss 0.46087783575057983\n","[Training Epoch 0] Batch 2747, Loss 0.502912163734436\n","[Training Epoch 0] Batch 2748, Loss 0.4823828339576721\n","[Training Epoch 0] Batch 2749, Loss 0.5068466663360596\n","[Training Epoch 0] Batch 2750, Loss 0.49110931158065796\n","[Training Epoch 0] Batch 2751, Loss 0.5296210050582886\n","[Training Epoch 0] Batch 2752, Loss 0.4973515570163727\n","[Training Epoch 0] Batch 2753, Loss 0.5002250671386719\n","[Training Epoch 0] Batch 2754, Loss 0.5303401947021484\n","[Training Epoch 0] Batch 2755, Loss 0.4786554276943207\n","[Training Epoch 0] Batch 2756, Loss 0.49271896481513977\n","[Training Epoch 0] Batch 2757, Loss 0.5012964606285095\n","[Training Epoch 0] Batch 2758, Loss 0.49087655544281006\n","[Training Epoch 0] Batch 2759, Loss 0.4793788194656372\n","[Training Epoch 0] Batch 2760, Loss 0.5190243721008301\n","[Training Epoch 0] Batch 2761, Loss 0.4922064244747162\n","[Training Epoch 0] Batch 2762, Loss 0.5201721787452698\n","[Training Epoch 0] Batch 2763, Loss 0.5146371126174927\n","[Training Epoch 0] Batch 2764, Loss 0.5227522253990173\n","[Training Epoch 0] Batch 2765, Loss 0.5017512440681458\n","[Training Epoch 0] Batch 2766, Loss 0.5078614950180054\n","[Training Epoch 0] Batch 2767, Loss 0.5064635872840881\n","[Training Epoch 0] Batch 2768, Loss 0.49237310886383057\n","[Training Epoch 0] Batch 2769, Loss 0.5102741718292236\n","[Training Epoch 0] Batch 2770, Loss 0.5050665140151978\n","[Training Epoch 0] Batch 2771, Loss 0.5088929533958435\n","[Training Epoch 0] Batch 2772, Loss 0.5086336135864258\n","[Training Epoch 0] Batch 2773, Loss 0.5066074132919312\n","[Training Epoch 0] Batch 2774, Loss 0.5104904770851135\n","[Training Epoch 0] Batch 2775, Loss 0.47228607535362244\n","[Training Epoch 0] Batch 2776, Loss 0.4972933530807495\n","[Training Epoch 0] Batch 2777, Loss 0.4820901155471802\n","[Training Epoch 0] Batch 2778, Loss 0.49645286798477173\n","[Training Epoch 0] Batch 2779, Loss 0.5025119781494141\n","[Training Epoch 0] Batch 2780, Loss 0.5179522633552551\n","[Training Epoch 0] Batch 2781, Loss 0.49456721544265747\n","[Training Epoch 0] Batch 2782, Loss 0.5137584209442139\n","[Training Epoch 0] Batch 2783, Loss 0.5139570236206055\n","[Training Epoch 0] Batch 2784, Loss 0.5102390050888062\n","[Training Epoch 0] Batch 2785, Loss 0.5200386047363281\n","[Training Epoch 0] Batch 2786, Loss 0.4925060272216797\n","[Training Epoch 0] Batch 2787, Loss 0.4949331283569336\n","[Training Epoch 0] Batch 2788, Loss 0.5126962661743164\n","[Training Epoch 0] Batch 2789, Loss 0.48848065733909607\n","[Training Epoch 0] Batch 2790, Loss 0.49789541959762573\n","[Training Epoch 0] Batch 2791, Loss 0.4929836392402649\n","[Training Epoch 0] Batch 2792, Loss 0.5141161680221558\n","[Training Epoch 0] Batch 2793, Loss 0.4798457622528076\n","[Training Epoch 0] Batch 2794, Loss 0.5150783658027649\n","[Training Epoch 0] Batch 2795, Loss 0.49606359004974365\n","[Training Epoch 0] Batch 2796, Loss 0.5027742385864258\n","[Training Epoch 0] Batch 2797, Loss 0.4760523736476898\n","[Training Epoch 0] Batch 2798, Loss 0.5392528176307678\n","[Training Epoch 0] Batch 2799, Loss 0.5251072645187378\n","[Training Epoch 0] Batch 2800, Loss 0.4975411295890808\n","[Training Epoch 0] Batch 2801, Loss 0.5216568112373352\n","[Training Epoch 0] Batch 2802, Loss 0.47063571214675903\n","[Training Epoch 0] Batch 2803, Loss 0.506193220615387\n","[Training Epoch 0] Batch 2804, Loss 0.48093920946121216\n","[Training Epoch 0] Batch 2805, Loss 0.5112787485122681\n","[Training Epoch 0] Batch 2806, Loss 0.5027379989624023\n","[Training Epoch 0] Batch 2807, Loss 0.4820884168148041\n","[Training Epoch 0] Batch 2808, Loss 0.5278218984603882\n","[Training Epoch 0] Batch 2809, Loss 0.5176416039466858\n","[Training Epoch 0] Batch 2810, Loss 0.4620978534221649\n","[Training Epoch 0] Batch 2811, Loss 0.4925636947154999\n","[Training Epoch 0] Batch 2812, Loss 0.5251786708831787\n","[Training Epoch 0] Batch 2813, Loss 0.4884265065193176\n","[Training Epoch 0] Batch 2814, Loss 0.48912662267684937\n","[Training Epoch 0] Batch 2815, Loss 0.49618685245513916\n","[Training Epoch 0] Batch 2816, Loss 0.5003613233566284\n","[Training Epoch 0] Batch 2817, Loss 0.493719220161438\n","[Training Epoch 0] Batch 2818, Loss 0.4784359335899353\n","[Training Epoch 0] Batch 2819, Loss 0.4999837279319763\n","[Training Epoch 0] Batch 2820, Loss 0.5255923271179199\n","[Training Epoch 0] Batch 2821, Loss 0.49085673689842224\n","[Training Epoch 0] Batch 2822, Loss 0.48476213216781616\n","[Training Epoch 0] Batch 2823, Loss 0.5062659978866577\n","[Training Epoch 0] Batch 2824, Loss 0.48237746953964233\n","[Training Epoch 0] Batch 2825, Loss 0.5420376062393188\n","[Training Epoch 0] Batch 2826, Loss 0.5010197758674622\n","[Training Epoch 0] Batch 2827, Loss 0.4845113754272461\n","[Training Epoch 0] Batch 2828, Loss 0.4873814582824707\n","[Training Epoch 0] Batch 2829, Loss 0.5039480328559875\n","[Training Epoch 0] Batch 2830, Loss 0.4633007049560547\n","[Training Epoch 0] Batch 2831, Loss 0.5037146210670471\n","[Training Epoch 0] Batch 2832, Loss 0.5024130344390869\n","[Training Epoch 0] Batch 2833, Loss 0.48007112741470337\n","[Training Epoch 0] Batch 2834, Loss 0.5099767446517944\n","[Training Epoch 0] Batch 2835, Loss 0.4986884593963623\n","[Training Epoch 0] Batch 2836, Loss 0.5165348052978516\n","[Training Epoch 0] Batch 2837, Loss 0.5048899054527283\n","[Training Epoch 0] Batch 2838, Loss 0.49233078956604004\n","[Training Epoch 0] Batch 2839, Loss 0.5049888491630554\n","[Training Epoch 0] Batch 2840, Loss 0.5064305663108826\n","[Training Epoch 0] Batch 2841, Loss 0.4871886074542999\n","[Training Epoch 0] Batch 2842, Loss 0.5053247213363647\n","[Training Epoch 0] Batch 2843, Loss 0.5358880162239075\n","[Training Epoch 0] Batch 2844, Loss 0.4921291768550873\n","[Training Epoch 0] Batch 2845, Loss 0.49363571405410767\n","[Training Epoch 0] Batch 2846, Loss 0.4949139952659607\n","[Training Epoch 0] Batch 2847, Loss 0.5204102993011475\n","[Training Epoch 0] Batch 2848, Loss 0.49863171577453613\n","[Training Epoch 0] Batch 2849, Loss 0.5002265572547913\n","[Training Epoch 0] Batch 2850, Loss 0.5000393986701965\n","[Training Epoch 0] Batch 2851, Loss 0.4984254837036133\n","[Training Epoch 0] Batch 2852, Loss 0.5177257061004639\n","[Training Epoch 0] Batch 2853, Loss 0.4680209159851074\n","[Training Epoch 0] Batch 2854, Loss 0.5089216828346252\n","[Training Epoch 0] Batch 2855, Loss 0.5217259526252747\n","[Training Epoch 0] Batch 2856, Loss 0.5036195516586304\n","[Training Epoch 0] Batch 2857, Loss 0.5024114847183228\n","[Training Epoch 0] Batch 2858, Loss 0.4962230622768402\n","[Training Epoch 0] Batch 2859, Loss 0.4999311864376068\n","[Training Epoch 0] Batch 2860, Loss 0.48318108916282654\n","[Training Epoch 0] Batch 2861, Loss 0.5152381658554077\n","[Training Epoch 0] Batch 2862, Loss 0.4730222821235657\n","[Training Epoch 0] Batch 2863, Loss 0.4754844307899475\n","[Training Epoch 0] Batch 2864, Loss 0.4808835983276367\n","[Training Epoch 0] Batch 2865, Loss 0.46796220541000366\n","[Training Epoch 0] Batch 2866, Loss 0.5189775228500366\n","[Training Epoch 0] Batch 2867, Loss 0.5049252510070801\n","[Training Epoch 0] Batch 2868, Loss 0.5191646814346313\n","[Training Epoch 0] Batch 2869, Loss 0.5126968026161194\n","[Training Epoch 0] Batch 2870, Loss 0.505084216594696\n","[Training Epoch 0] Batch 2871, Loss 0.5022132396697998\n","[Training Epoch 0] Batch 2872, Loss 0.49856582283973694\n","[Training Epoch 0] Batch 2873, Loss 0.5254516005516052\n","[Training Epoch 0] Batch 2874, Loss 0.5060358047485352\n","[Training Epoch 0] Batch 2875, Loss 0.5114452838897705\n","[Training Epoch 0] Batch 2876, Loss 0.5140552520751953\n","[Training Epoch 0] Batch 2877, Loss 0.5113644599914551\n","[Training Epoch 0] Batch 2878, Loss 0.5050287246704102\n","[Training Epoch 0] Batch 2879, Loss 0.49580568075180054\n","[Training Epoch 0] Batch 2880, Loss 0.4642176032066345\n","[Training Epoch 0] Batch 2881, Loss 0.4893607795238495\n","[Training Epoch 0] Batch 2882, Loss 0.5190413594245911\n","[Training Epoch 0] Batch 2883, Loss 0.5329303741455078\n","[Training Epoch 0] Batch 2884, Loss 0.4858531951904297\n","[Training Epoch 0] Batch 2885, Loss 0.4896247386932373\n","[Training Epoch 0] Batch 2886, Loss 0.5229241847991943\n","[Training Epoch 0] Batch 2887, Loss 0.48704639077186584\n","[Training Epoch 0] Batch 2888, Loss 0.49491214752197266\n","[Training Epoch 0] Batch 2889, Loss 0.504862368106842\n","[Training Epoch 0] Batch 2890, Loss 0.49344557523727417\n","[Training Epoch 0] Batch 2891, Loss 0.5138241052627563\n","[Training Epoch 0] Batch 2892, Loss 0.46638956665992737\n","[Training Epoch 0] Batch 2893, Loss 0.526711106300354\n","[Training Epoch 0] Batch 2894, Loss 0.4638601541519165\n","[Training Epoch 0] Batch 2895, Loss 0.520702600479126\n","[Training Epoch 0] Batch 2896, Loss 0.4525977373123169\n","[Training Epoch 0] Batch 2897, Loss 0.5141299962997437\n","[Training Epoch 0] Batch 2898, Loss 0.4934217929840088\n","[Training Epoch 0] Batch 2899, Loss 0.5022672414779663\n","[Training Epoch 0] Batch 2900, Loss 0.4882388710975647\n","[Training Epoch 0] Batch 2901, Loss 0.5333077311515808\n","[Training Epoch 0] Batch 2902, Loss 0.47167375683784485\n","[Training Epoch 0] Batch 2903, Loss 0.49838870763778687\n","[Training Epoch 0] Batch 2904, Loss 0.5282906293869019\n","[Training Epoch 0] Batch 2905, Loss 0.5396885871887207\n","[Training Epoch 0] Batch 2906, Loss 0.4932449162006378\n","[Training Epoch 0] Batch 2907, Loss 0.5035278797149658\n","[Training Epoch 0] Batch 2908, Loss 0.5111846923828125\n","[Training Epoch 0] Batch 2909, Loss 0.4933517873287201\n","[Training Epoch 0] Batch 2910, Loss 0.5202056169509888\n","[Training Epoch 0] Batch 2911, Loss 0.49329036474227905\n","[Training Epoch 0] Batch 2912, Loss 0.48703625798225403\n","[Training Epoch 0] Batch 2913, Loss 0.5085597634315491\n","[Training Epoch 0] Batch 2914, Loss 0.4946211278438568\n","[Training Epoch 0] Batch 2915, Loss 0.49732670187950134\n","[Training Epoch 0] Batch 2916, Loss 0.5058674812316895\n","[Training Epoch 0] Batch 2917, Loss 0.5103719234466553\n","[Training Epoch 0] Batch 2918, Loss 0.5267682075500488\n","[Training Epoch 0] Batch 2919, Loss 0.49876296520233154\n","[Training Epoch 0] Batch 2920, Loss 0.5127701759338379\n","[Training Epoch 0] Batch 2921, Loss 0.502326488494873\n","[Training Epoch 0] Batch 2922, Loss 0.5141366720199585\n","[Training Epoch 0] Batch 2923, Loss 0.49992120265960693\n","[Training Epoch 0] Batch 2924, Loss 0.5384562015533447\n","[Training Epoch 0] Batch 2925, Loss 0.498365581035614\n","[Training Epoch 0] Batch 2926, Loss 0.5155274868011475\n","[Training Epoch 0] Batch 2927, Loss 0.5088261365890503\n","[Training Epoch 0] Batch 2928, Loss 0.5230438113212585\n","[Training Epoch 0] Batch 2929, Loss 0.4881131649017334\n","[Training Epoch 0] Batch 2930, Loss 0.496654748916626\n","[Training Epoch 0] Batch 2931, Loss 0.5137966871261597\n","[Training Epoch 0] Batch 2932, Loss 0.5039279460906982\n","[Training Epoch 0] Batch 2933, Loss 0.49605321884155273\n","[Training Epoch 0] Batch 2934, Loss 0.5113288164138794\n","[Training Epoch 0] Batch 2935, Loss 0.5278393626213074\n","[Training Epoch 0] Batch 2936, Loss 0.5023679733276367\n","[Training Epoch 0] Batch 2937, Loss 0.5049889087677002\n","[Training Epoch 0] Batch 2938, Loss 0.49191516637802124\n","[Training Epoch 0] Batch 2939, Loss 0.4921167492866516\n","[Training Epoch 0] Batch 2940, Loss 0.5073232650756836\n","[Training Epoch 0] Batch 2941, Loss 0.5077126026153564\n","[Training Epoch 0] Batch 2942, Loss 0.4921191930770874\n","[Training Epoch 0] Batch 2943, Loss 0.4881448447704315\n","[Training Epoch 0] Batch 2944, Loss 0.4904674291610718\n","[Training Epoch 0] Batch 2945, Loss 0.4893815219402313\n","[Training Epoch 0] Batch 2946, Loss 0.4676034152507782\n","[Training Epoch 0] Batch 2947, Loss 0.49449431896209717\n","[Training Epoch 0] Batch 2948, Loss 0.5359092950820923\n","[Training Epoch 0] Batch 2949, Loss 0.4819459915161133\n","[Training Epoch 0] Batch 2950, Loss 0.5088894963264465\n","[Training Epoch 0] Batch 2951, Loss 0.502490222454071\n","[Training Epoch 0] Batch 2952, Loss 0.4996630549430847\n","[Training Epoch 0] Batch 2953, Loss 0.5279802680015564\n","[Training Epoch 0] Batch 2954, Loss 0.49470171332359314\n","[Training Epoch 0] Batch 2955, Loss 0.5218250751495361\n","[Training Epoch 0] Batch 2956, Loss 0.4870327413082123\n","[Training Epoch 0] Batch 2957, Loss 0.4997996389865875\n","[Training Epoch 0] Batch 2958, Loss 0.46759873628616333\n","[Training Epoch 0] Batch 2959, Loss 0.4611724019050598\n","[Training Epoch 0] Batch 2960, Loss 0.46228086948394775\n","[Training Epoch 0] Batch 2961, Loss 0.4894433617591858\n","[Training Epoch 0] Batch 2962, Loss 0.483084499835968\n","[Training Epoch 0] Batch 2963, Loss 0.5281946659088135\n","[Training Epoch 0] Batch 2964, Loss 0.5023432970046997\n","[Training Epoch 0] Batch 2965, Loss 0.4932243227958679\n","[Training Epoch 0] Batch 2966, Loss 0.49063342809677124\n","[Training Epoch 0] Batch 2967, Loss 0.5279335379600525\n","[Training Epoch 0] Batch 2968, Loss 0.48162782192230225\n","[Training Epoch 0] Batch 2969, Loss 0.4817269742488861\n","[Training Epoch 0] Batch 2970, Loss 0.5164838433265686\n","[Training Epoch 0] Batch 2971, Loss 0.5230587124824524\n","[Training Epoch 0] Batch 2972, Loss 0.513972282409668\n","[Training Epoch 0] Batch 2973, Loss 0.49965351819992065\n","[Training Epoch 0] Batch 2974, Loss 0.494533896446228\n","[Training Epoch 0] Batch 2975, Loss 0.4867446720600128\n","[Training Epoch 0] Batch 2976, Loss 0.513875424861908\n","[Training Epoch 0] Batch 2977, Loss 0.5035907030105591\n","[Training Epoch 0] Batch 2978, Loss 0.4930858314037323\n","[Training Epoch 0] Batch 2979, Loss 0.47247910499572754\n","[Training Epoch 0] Batch 2980, Loss 0.4880942702293396\n","[Training Epoch 0] Batch 2981, Loss 0.4983704090118408\n","[Training Epoch 0] Batch 2982, Loss 0.49071359634399414\n","[Training Epoch 0] Batch 2983, Loss 0.5113596320152283\n","[Training Epoch 0] Batch 2984, Loss 0.508733868598938\n","[Training Epoch 0] Batch 2985, Loss 0.49338534474372864\n","[Training Epoch 0] Batch 2986, Loss 0.4892810881137848\n","[Training Epoch 0] Batch 2987, Loss 0.5011121034622192\n","[Training Epoch 0] Batch 2988, Loss 0.5034462809562683\n","[Training Epoch 0] Batch 2989, Loss 0.5203651785850525\n","[Training Epoch 0] Batch 2990, Loss 0.5099668502807617\n","[Training Epoch 0] Batch 2991, Loss 0.48669999837875366\n","[Training Epoch 0] Batch 2992, Loss 0.526766836643219\n","[Training Epoch 0] Batch 2993, Loss 0.5257325172424316\n","[Training Epoch 0] Batch 2994, Loss 0.5150327086448669\n","[Training Epoch 0] Batch 2995, Loss 0.5242237448692322\n","[Training Epoch 0] Batch 2996, Loss 0.4738613963127136\n","[Training Epoch 0] Batch 2997, Loss 0.5121302008628845\n","[Training Epoch 0] Batch 2998, Loss 0.5162934064865112\n","[Training Epoch 0] Batch 2999, Loss 0.49675050377845764\n","[Training Epoch 0] Batch 3000, Loss 0.48669180274009705\n","[Training Epoch 0] Batch 3001, Loss 0.4777584373950958\n","[Training Epoch 0] Batch 3002, Loss 0.48690909147262573\n","[Training Epoch 0] Batch 3003, Loss 0.5231699347496033\n","[Training Epoch 0] Batch 3004, Loss 0.5085757970809937\n","[Training Epoch 0] Batch 3005, Loss 0.48297378420829773\n","[Training Epoch 0] Batch 3006, Loss 0.4999510645866394\n","[Training Epoch 0] Batch 3007, Loss 0.48815926909446716\n","[Training Epoch 0] Batch 3008, Loss 0.5113706588745117\n","[Training Epoch 0] Batch 3009, Loss 0.49948441982269287\n","[Training Epoch 0] Batch 3010, Loss 0.48043227195739746\n","[Training Epoch 0] Batch 3011, Loss 0.5113546848297119\n","[Training Epoch 0] Batch 3012, Loss 0.49058619141578674\n","[Training Epoch 0] Batch 3013, Loss 0.4932269752025604\n","[Training Epoch 0] Batch 3014, Loss 0.5164796113967896\n","[Training Epoch 0] Batch 3015, Loss 0.5153079628944397\n","[Training Epoch 0] Batch 3016, Loss 0.48160693049430847\n","[Training Epoch 0] Batch 3017, Loss 0.5360990762710571\n","[Training Epoch 0] Batch 3018, Loss 0.48788729310035706\n","[Training Epoch 0] Batch 3019, Loss 0.5071994066238403\n","[Training Epoch 0] Batch 3020, Loss 0.5370509028434753\n","[Training Epoch 0] Batch 3021, Loss 0.5126954317092896\n","[Training Epoch 0] Batch 3022, Loss 0.49297794699668884\n","[Training Epoch 0] Batch 3023, Loss 0.5439115762710571\n","[Training Epoch 0] Batch 3024, Loss 0.49959325790405273\n","[Training Epoch 0] Batch 3025, Loss 0.5058315992355347\n","[Training Epoch 0] Batch 3026, Loss 0.5048943758010864\n","[Training Epoch 0] Batch 3027, Loss 0.48816126585006714\n","[Training Epoch 0] Batch 3028, Loss 0.5047636032104492\n","[Training Epoch 0] Batch 3029, Loss 0.47232896089553833\n","[Training Epoch 0] Batch 3030, Loss 0.4944007396697998\n","[Training Epoch 0] Batch 3031, Loss 0.5085559487342834\n","[Training Epoch 0] Batch 3032, Loss 0.5076854228973389\n","[Training Epoch 0] Batch 3033, Loss 0.4735146760940552\n","[Training Epoch 0] Batch 3034, Loss 0.5154819488525391\n","[Training Epoch 0] Batch 3035, Loss 0.4983382821083069\n","[Training Epoch 0] Batch 3036, Loss 0.5022047162055969\n","[Training Epoch 0] Batch 3037, Loss 0.5077049136161804\n","[Training Epoch 0] Batch 3038, Loss 0.5126118063926697\n","[Training Epoch 0] Batch 3039, Loss 0.507356584072113\n","[Training Epoch 0] Batch 3040, Loss 0.53134685754776\n","[Training Epoch 0] Batch 3041, Loss 0.5321542620658875\n","[Training Epoch 0] Batch 3042, Loss 0.48669442534446716\n","[Training Epoch 0] Batch 3043, Loss 0.518096923828125\n","[Training Epoch 0] Batch 3044, Loss 0.5138609409332275\n","[Training Epoch 0] Batch 3045, Loss 0.5010662078857422\n","[Training Epoch 0] Batch 3046, Loss 0.5230421423912048\n","[Training Epoch 0] Batch 3047, Loss 0.5020847320556641\n","[Training Epoch 0] Batch 3048, Loss 0.4813704490661621\n","[Training Epoch 0] Batch 3049, Loss 0.49547138810157776\n","[Training Epoch 0] Batch 3050, Loss 0.5167761445045471\n","[Training Epoch 0] Batch 3051, Loss 0.5252939462661743\n","[Training Epoch 0] Batch 3052, Loss 0.47245532274246216\n","[Training Epoch 0] Batch 3053, Loss 0.5023477077484131\n","[Training Epoch 0] Batch 3054, Loss 0.5230520963668823\n","[Training Epoch 0] Batch 3055, Loss 0.49577274918556213\n","[Training Epoch 0] Batch 3056, Loss 0.48789048194885254\n","[Training Epoch 0] Batch 3057, Loss 0.4944934844970703\n","[Training Epoch 0] Batch 3058, Loss 0.5191460847854614\n","[Training Epoch 0] Batch 3059, Loss 0.5100057125091553\n","[Training Epoch 0] Batch 3060, Loss 0.5244097709655762\n","[Training Epoch 0] Batch 3061, Loss 0.490299791097641\n","[Training Epoch 0] Batch 3062, Loss 0.51629638671875\n","[Training Epoch 0] Batch 3063, Loss 0.4672046899795532\n","[Training Epoch 0] Batch 3064, Loss 0.48279619216918945\n","[Training Epoch 0] Batch 3065, Loss 0.5193041563034058\n","[Training Epoch 0] Batch 3066, Loss 0.4773826599121094\n","[Training Epoch 0] Batch 3067, Loss 0.5308060646057129\n","[Training Epoch 0] Batch 3068, Loss 0.5071175694465637\n","[Training Epoch 0] Batch 3069, Loss 0.5072944164276123\n","[Training Epoch 0] Batch 3070, Loss 0.45283186435699463\n","[Training Epoch 0] Batch 3071, Loss 0.5126766562461853\n","[Training Epoch 0] Batch 3072, Loss 0.5255034565925598\n","[Training Epoch 0] Batch 3073, Loss 0.5307629704475403\n","[Training Epoch 0] Batch 3074, Loss 0.49539732933044434\n","[Training Epoch 0] Batch 3075, Loss 0.48397380113601685\n","[Training Epoch 0] Batch 3076, Loss 0.49385398626327515\n","[Training Epoch 0] Batch 3077, Loss 0.4893808662891388\n","[Training Epoch 0] Batch 3078, Loss 0.4944193959236145\n","[Training Epoch 0] Batch 3079, Loss 0.48256951570510864\n","[Training Epoch 0] Batch 3080, Loss 0.4943321943283081\n","[Training Epoch 0] Batch 3081, Loss 0.4919799268245697\n","[Training Epoch 0] Batch 3082, Loss 0.48785948753356934\n","[Training Epoch 0] Batch 3083, Loss 0.5151677131652832\n","[Training Epoch 0] Batch 3084, Loss 0.49449577927589417\n","[Training Epoch 0] Batch 3085, Loss 0.4854298233985901\n","[Training Epoch 0] Batch 3086, Loss 0.5104275941848755\n","[Training Epoch 0] Batch 3087, Loss 0.5257807970046997\n","[Training Epoch 0] Batch 3088, Loss 0.48124921321868896\n","[Training Epoch 0] Batch 3089, Loss 0.4932279586791992\n","[Training Epoch 0] Batch 3090, Loss 0.4826679229736328\n","[Training Epoch 0] Batch 3091, Loss 0.4816321134567261\n","[Training Epoch 0] Batch 3092, Loss 0.5085783004760742\n","[Training Epoch 0] Batch 3093, Loss 0.4968019723892212\n","[Training Epoch 0] Batch 3094, Loss 0.5178516507148743\n","[Training Epoch 0] Batch 3095, Loss 0.4968159794807434\n","[Training Epoch 0] Batch 3096, Loss 0.4758678078651428\n","[Training Epoch 0] Batch 3097, Loss 0.5216556191444397\n","[Training Epoch 0] Batch 3098, Loss 0.5020980834960938\n","[Training Epoch 0] Batch 3099, Loss 0.5088614821434021\n","[Training Epoch 0] Batch 3100, Loss 0.47463709115982056\n","[Training Epoch 0] Batch 3101, Loss 0.49047744274139404\n","[Training Epoch 0] Batch 3102, Loss 0.5258257389068604\n","[Training Epoch 0] Batch 3103, Loss 0.5124565958976746\n","[Training Epoch 0] Batch 3104, Loss 0.4880152940750122\n","[Training Epoch 0] Batch 3105, Loss 0.4931250214576721\n","[Training Epoch 0] Batch 3106, Loss 0.5126773715019226\n","[Training Epoch 0] Batch 3107, Loss 0.4959913492202759\n","[Training Epoch 0] Batch 3108, Loss 0.49131762981414795\n","[Training Epoch 0] Batch 3109, Loss 0.5294078588485718\n","[Training Epoch 0] Batch 3110, Loss 0.4878699481487274\n","[Training Epoch 0] Batch 3111, Loss 0.5067262649536133\n","[Training Epoch 0] Batch 3112, Loss 0.5008822679519653\n","[Training Epoch 0] Batch 3113, Loss 0.48987117409706116\n","[Training Epoch 0] Batch 3114, Loss 0.5298137664794922\n","[Training Epoch 0] Batch 3115, Loss 0.4983977973461151\n","[Training Epoch 0] Batch 3116, Loss 0.5151083469390869\n","[Training Epoch 0] Batch 3117, Loss 0.47699958086013794\n","[Training Epoch 0] Batch 3118, Loss 0.5282557010650635\n","[Training Epoch 0] Batch 3119, Loss 0.520216166973114\n","[Training Epoch 0] Batch 3120, Loss 0.5152021050453186\n","[Training Epoch 0] Batch 3121, Loss 0.5075642466545105\n","[Training Epoch 0] Batch 3122, Loss 0.49807924032211304\n","[Training Epoch 0] Batch 3123, Loss 0.5009790658950806\n","[Training Epoch 0] Batch 3124, Loss 0.5180891752243042\n","[Training Epoch 0] Batch 3125, Loss 0.5114471912384033\n","[Training Epoch 0] Batch 3126, Loss 0.5150436162948608\n","[Training Epoch 0] Batch 3127, Loss 0.5113645195960999\n","[Training Epoch 0] Batch 3128, Loss 0.5138810873031616\n","[Training Epoch 0] Batch 3129, Loss 0.49136942625045776\n","[Training Epoch 0] Batch 3130, Loss 0.4978342354297638\n","[Training Epoch 0] Batch 3131, Loss 0.5018138289451599\n","[Training Epoch 0] Batch 3132, Loss 0.5218011140823364\n","[Training Epoch 0] Batch 3133, Loss 0.5201808214187622\n","[Training Epoch 0] Batch 3134, Loss 0.4896221160888672\n","[Training Epoch 0] Batch 3135, Loss 0.4958409368991852\n","[Training Epoch 0] Batch 3136, Loss 0.5122275352478027\n","[Training Epoch 0] Batch 3137, Loss 0.5257337093353271\n","[Training Epoch 0] Batch 3138, Loss 0.4848637878894806\n","[Training Epoch 0] Batch 3139, Loss 0.44853323698043823\n","[Training Epoch 0] Batch 3140, Loss 0.4849730432033539\n","[Training Epoch 0] Batch 3141, Loss 0.4967181384563446\n","[Training Epoch 0] Batch 3142, Loss 0.49556419253349304\n","[Training Epoch 0] Batch 3143, Loss 0.4906616806983948\n","[Training Epoch 0] Batch 3144, Loss 0.5125669836997986\n","[Training Epoch 0] Batch 3145, Loss 0.48552030324935913\n","[Training Epoch 0] Batch 3146, Loss 0.5087019801139832\n","[Training Epoch 0] Batch 3147, Loss 0.5127779841423035\n","[Training Epoch 0] Batch 3148, Loss 0.49610722064971924\n","[Training Epoch 0] Batch 3149, Loss 0.4723641872406006\n","[Training Epoch 0] Batch 3150, Loss 0.4879716634750366\n","[Training Epoch 0] Batch 3151, Loss 0.49107179045677185\n","[Training Epoch 0] Batch 3152, Loss 0.4834155738353729\n","[Training Epoch 0] Batch 3153, Loss 0.5165644288063049\n","[Training Epoch 0] Batch 3154, Loss 0.5008595585823059\n","[Training Epoch 0] Batch 3155, Loss 0.5167721509933472\n","[Training Epoch 0] Batch 3156, Loss 0.542447566986084\n","[Training Epoch 0] Batch 3157, Loss 0.5140208005905151\n","[Training Epoch 0] Batch 3158, Loss 0.49149155616760254\n","[Training Epoch 0] Batch 3159, Loss 0.5114296078681946\n","[Training Epoch 0] Batch 3160, Loss 0.5155396461486816\n","[Training Epoch 0] Batch 3161, Loss 0.5033663511276245\n","[Training Epoch 0] Batch 3162, Loss 0.48679643869400024\n","[Training Epoch 0] Batch 3163, Loss 0.5336440205574036\n","[Training Epoch 0] Batch 3164, Loss 0.5335185527801514\n","[Training Epoch 0] Batch 3165, Loss 0.4981979429721832\n","[Training Epoch 0] Batch 3166, Loss 0.4945739507675171\n","[Training Epoch 0] Batch 3167, Loss 0.5074368715286255\n","[Training Epoch 0] Batch 3168, Loss 0.49711087346076965\n","[Training Epoch 0] Batch 3169, Loss 0.5136356353759766\n","[Training Epoch 0] Batch 3170, Loss 0.4928390681743622\n","[Training Epoch 0] Batch 3171, Loss 0.49682319164276123\n","[Training Epoch 0] Batch 3172, Loss 0.5140090584754944\n","[Training Epoch 0] Batch 3173, Loss 0.5162811279296875\n","[Training Epoch 0] Batch 3174, Loss 0.545200765132904\n","[Training Epoch 0] Batch 3175, Loss 0.49400967359542847\n","[Training Epoch 0] Batch 3176, Loss 0.5205112099647522\n","[Training Epoch 0] Batch 3177, Loss 0.5017890930175781\n","[Training Epoch 0] Batch 3178, Loss 0.5060677528381348\n","[Training Epoch 0] Batch 3179, Loss 0.48949986696243286\n","[Training Epoch 0] Batch 3180, Loss 0.4996732473373413\n","[Training Epoch 0] Batch 3181, Loss 0.4997943341732025\n","[Training Epoch 0] Batch 3182, Loss 0.5036401152610779\n","[Training Epoch 0] Batch 3183, Loss 0.49575087428092957\n","[Training Epoch 0] Batch 3184, Loss 0.5032974481582642\n","[Training Epoch 0] Batch 3185, Loss 0.5442057847976685\n","[Training Epoch 0] Batch 3186, Loss 0.49040234088897705\n","[Training Epoch 0] Batch 3187, Loss 0.47488412261009216\n","[Training Epoch 0] Batch 3188, Loss 0.4991385340690613\n","[Training Epoch 0] Batch 3189, Loss 0.47577011585235596\n","[Training Epoch 0] Batch 3190, Loss 0.5152384042739868\n","[Training Epoch 0] Batch 3191, Loss 0.5086488723754883\n","[Training Epoch 0] Batch 3192, Loss 0.4928576648235321\n","[Training Epoch 0] Batch 3193, Loss 0.4821561872959137\n","[Training Epoch 0] Batch 3194, Loss 0.4876548945903778\n","[Training Epoch 0] Batch 3195, Loss 0.5389180183410645\n","[Training Epoch 0] Batch 3196, Loss 0.5270341038703918\n","[Training Epoch 0] Batch 3197, Loss 0.5011386871337891\n","[Training Epoch 0] Batch 3198, Loss 0.47989386320114136\n","[Training Epoch 0] Batch 3199, Loss 0.4905932545661926\n","[Training Epoch 0] Batch 3200, Loss 0.5005698204040527\n","[Training Epoch 0] Batch 3201, Loss 0.5076320171356201\n","[Training Epoch 0] Batch 3202, Loss 0.4820955991744995\n","[Training Epoch 0] Batch 3203, Loss 0.5138039588928223\n","[Training Epoch 0] Batch 3204, Loss 0.5073203444480896\n","[Training Epoch 0] Batch 3205, Loss 0.5055561065673828\n","[Training Epoch 0] Batch 3206, Loss 0.5019651651382446\n","[Training Epoch 0] Batch 3207, Loss 0.4748597741127014\n","[Training Epoch 0] Batch 3208, Loss 0.5179697275161743\n","[Training Epoch 0] Batch 3209, Loss 0.531365692615509\n","[Training Epoch 0] Batch 3210, Loss 0.5164799094200134\n","[Training Epoch 0] Batch 3211, Loss 0.5354241132736206\n","[Training Epoch 0] Batch 3212, Loss 0.5172478556632996\n","[Training Epoch 0] Batch 3213, Loss 0.4905966520309448\n","[Training Epoch 0] Batch 3214, Loss 0.5074199438095093\n","[Training Epoch 0] Batch 3215, Loss 0.533807635307312\n","[Training Epoch 0] Batch 3216, Loss 0.4839741587638855\n","[Training Epoch 0] Batch 3217, Loss 0.5110809803009033\n","[Training Epoch 0] Batch 3218, Loss 0.4943529963493347\n","[Training Epoch 0] Batch 3219, Loss 0.5148307085037231\n","[Training Epoch 0] Batch 3220, Loss 0.5029196739196777\n","[Training Epoch 0] Batch 3221, Loss 0.5360410809516907\n","[Training Epoch 0] Batch 3222, Loss 0.4932989478111267\n","[Training Epoch 0] Batch 3223, Loss 0.5094512701034546\n","[Training Epoch 0] Batch 3224, Loss 0.4923775792121887\n","[Training Epoch 0] Batch 3225, Loss 0.4916107654571533\n","[Training Epoch 0] Batch 3226, Loss 0.4994726777076721\n","[Training Epoch 0] Batch 3227, Loss 0.5177766680717468\n","[Training Epoch 0] Batch 3228, Loss 0.5218803882598877\n","[Training Epoch 0] Batch 3229, Loss 0.5034011602401733\n","[Training Epoch 0] Batch 3230, Loss 0.4779554605484009\n","[Training Epoch 0] Batch 3231, Loss 0.4941233694553375\n","[Training Epoch 0] Batch 3232, Loss 0.4706460237503052\n","[Training Epoch 0] Batch 3233, Loss 0.48499277234077454\n","[Training Epoch 0] Batch 3234, Loss 0.45792701840400696\n","[Training Epoch 0] Batch 3235, Loss 0.4815713167190552\n","[Training Epoch 0] Batch 3236, Loss 0.5216716527938843\n","[Training Epoch 0] Batch 3237, Loss 0.49929535388946533\n","[Training Epoch 0] Batch 3238, Loss 0.5295549631118774\n","[Training Epoch 0] Batch 3239, Loss 0.5127964019775391\n","[Training Epoch 0] Batch 3240, Loss 0.5068150758743286\n","[Training Epoch 0] Batch 3241, Loss 0.5009359121322632\n","[Training Epoch 0] Batch 3242, Loss 0.49018561840057373\n","[Training Epoch 0] Batch 3243, Loss 0.5294464230537415\n","[Training Epoch 0] Batch 3244, Loss 0.49048444628715515\n","[Training Epoch 0] Batch 3245, Loss 0.5112412571907043\n","[Training Epoch 0] Batch 3246, Loss 0.5088457465171814\n","[Training Epoch 0] Batch 3247, Loss 0.5042451620101929\n","[Training Epoch 0] Batch 3248, Loss 0.5125696659088135\n","[Training Epoch 0] Batch 3249, Loss 0.4837603271007538\n","[Training Epoch 0] Batch 3250, Loss 0.5197890400886536\n","[Training Epoch 0] Batch 3251, Loss 0.5230109691619873\n","[Training Epoch 0] Batch 3252, Loss 0.5019097924232483\n","[Training Epoch 0] Batch 3253, Loss 0.5223001837730408\n","[Training Epoch 0] Batch 3254, Loss 0.5023011565208435\n","[Training Epoch 0] Batch 3255, Loss 0.5222527980804443\n","[Training Epoch 0] Batch 3256, Loss 0.49701279401779175\n","[Training Epoch 0] Batch 3257, Loss 0.5169128775596619\n","[Training Epoch 0] Batch 3258, Loss 0.5070666074752808\n","[Training Epoch 0] Batch 3259, Loss 0.540307343006134\n","[Training Epoch 0] Batch 3260, Loss 0.5491381883621216\n","[Training Epoch 0] Batch 3261, Loss 0.49299561977386475\n","[Training Epoch 0] Batch 3262, Loss 0.5113641023635864\n","[Training Epoch 0] Batch 3263, Loss 0.5215668678283691\n","[Training Epoch 0] Batch 3264, Loss 0.5030795335769653\n","[Training Epoch 0] Batch 3265, Loss 0.4787282943725586\n","[Training Epoch 0] Batch 3266, Loss 0.5104210376739502\n","[Training Epoch 0] Batch 3267, Loss 0.5101829767227173\n","[Training Epoch 0] Batch 3268, Loss 0.5101410746574402\n","[Training Epoch 0] Batch 3269, Loss 0.5217723250389099\n","[Training Epoch 0] Batch 3270, Loss 0.4943161606788635\n","[Training Epoch 0] Batch 3271, Loss 0.514114499092102\n","[Training Epoch 0] Batch 3272, Loss 0.5104066133499146\n","[Training Epoch 0] Batch 3273, Loss 0.5071033239364624\n","[Training Epoch 0] Batch 3274, Loss 0.4811800718307495\n","[Training Epoch 0] Batch 3275, Loss 0.5196260809898376\n","[Training Epoch 0] Batch 3276, Loss 0.5020483732223511\n","[Training Epoch 0] Batch 3277, Loss 0.5087000131607056\n","[Training Epoch 0] Batch 3278, Loss 0.508853018283844\n","[Training Epoch 0] Batch 3279, Loss 0.47998619079589844\n","[Training Epoch 0] Batch 3280, Loss 0.5075082778930664\n","[Training Epoch 0] Batch 3281, Loss 0.5031905770301819\n","[Training Epoch 0] Batch 3282, Loss 0.5217379331588745\n","[Training Epoch 0] Batch 3283, Loss 0.5242853164672852\n","[Training Epoch 0] Batch 3284, Loss 0.5055182576179504\n","[Training Epoch 0] Batch 3285, Loss 0.5127766132354736\n","[Training Epoch 0] Batch 3286, Loss 0.4851706624031067\n","[Training Epoch 0] Batch 3287, Loss 0.5441328287124634\n","[Training Epoch 0] Batch 3288, Loss 0.49187275767326355\n","[Training Epoch 0] Batch 3289, Loss 0.5281667113304138\n","[Training Epoch 0] Batch 3290, Loss 0.47212421894073486\n","[Training Epoch 0] Batch 3291, Loss 0.48653942346572876\n","[Training Epoch 0] Batch 3292, Loss 0.4828122556209564\n","[Training Epoch 0] Batch 3293, Loss 0.4853106141090393\n","[Training Epoch 0] Batch 3294, Loss 0.5190411806106567\n","[Training Epoch 0] Batch 3295, Loss 0.527193546295166\n","[Training Epoch 0] Batch 3296, Loss 0.4546802043914795\n","[Training Epoch 0] Batch 3297, Loss 0.5062488317489624\n","[Training Epoch 0] Batch 3298, Loss 0.46498584747314453\n","[Training Epoch 0] Batch 3299, Loss 0.5379416346549988\n","[Training Epoch 0] Batch 3300, Loss 0.5022014379501343\n","[Training Epoch 0] Batch 3301, Loss 0.5075774192810059\n","[Training Epoch 0] Batch 3302, Loss 0.5100530385971069\n","[Training Epoch 0] Batch 3303, Loss 0.5203148126602173\n","[Training Epoch 0] Batch 3304, Loss 0.5125110149383545\n","[Training Epoch 0] Batch 3305, Loss 0.5063937902450562\n","[Training Epoch 0] Batch 3306, Loss 0.5010972023010254\n","[Training Epoch 0] Batch 3307, Loss 0.5113468170166016\n","[Training Epoch 0] Batch 3308, Loss 0.4839465320110321\n","[Training Epoch 0] Batch 3309, Loss 0.4903520941734314\n","[Training Epoch 0] Batch 3310, Loss 0.5088175535202026\n","[Training Epoch 0] Batch 3311, Loss 0.5402084589004517\n","[Training Epoch 0] Batch 3312, Loss 0.5251960754394531\n","[Training Epoch 0] Batch 3313, Loss 0.4758995771408081\n","[Training Epoch 0] Batch 3314, Loss 0.5203616619110107\n","[Training Epoch 0] Batch 3315, Loss 0.486184686422348\n","[Training Epoch 0] Batch 3316, Loss 0.5086392164230347\n","[Training Epoch 0] Batch 3317, Loss 0.489197701215744\n","[Training Epoch 0] Batch 3318, Loss 0.5151825547218323\n","[Training Epoch 0] Batch 3319, Loss 0.47937458753585815\n","[Training Epoch 0] Batch 3320, Loss 0.4917530417442322\n","[Training Epoch 0] Batch 3321, Loss 0.508489191532135\n","[Training Epoch 0] Batch 3322, Loss 0.5178591012954712\n","[Training Epoch 0] Batch 3323, Loss 0.5073707699775696\n","[Training Epoch 0] Batch 3324, Loss 0.4968964457511902\n","[Training Epoch 0] Batch 3325, Loss 0.496265172958374\n","[Training Epoch 0] Batch 3326, Loss 0.5006447434425354\n","[Training Epoch 0] Batch 3327, Loss 0.49170053005218506\n","[Training Epoch 0] Batch 3328, Loss 0.5112438201904297\n","[Training Epoch 0] Batch 3329, Loss 0.5086490511894226\n","[Training Epoch 0] Batch 3330, Loss 0.5036319494247437\n","[Training Epoch 0] Batch 3331, Loss 0.5073458552360535\n","[Training Epoch 0] Batch 3332, Loss 0.4995608925819397\n","[Training Epoch 0] Batch 3333, Loss 0.5352697968482971\n","[Training Epoch 0] Batch 3334, Loss 0.48116326332092285\n","[Training Epoch 0] Batch 3335, Loss 0.5008869171142578\n","[Training Epoch 0] Batch 3336, Loss 0.5208297967910767\n","[Training Epoch 0] Batch 3337, Loss 0.4890436828136444\n","[Training Epoch 0] Batch 3338, Loss 0.491531103849411\n","[Training Epoch 0] Batch 3339, Loss 0.49287670850753784\n","[Training Epoch 0] Batch 3340, Loss 0.5297571420669556\n","[Training Epoch 0] Batch 3341, Loss 0.5204195976257324\n","[Training Epoch 0] Batch 3342, Loss 0.5230445861816406\n","[Training Epoch 0] Batch 3343, Loss 0.47438305616378784\n","[Training Epoch 0] Batch 3344, Loss 0.5204619765281677\n","[Training Epoch 0] Batch 3345, Loss 0.4970756769180298\n","[Training Epoch 0] Batch 3346, Loss 0.5151718258857727\n","[Training Epoch 0] Batch 3347, Loss 0.49306976795196533\n","[Training Epoch 0] Batch 3348, Loss 0.4966350495815277\n","[Training Epoch 0] Batch 3349, Loss 0.46158504486083984\n","[Training Epoch 0] Batch 3350, Loss 0.4955527186393738\n","[Training Epoch 0] Batch 3351, Loss 0.4967338740825653\n","[Training Epoch 0] Batch 3352, Loss 0.5086677670478821\n","[Training Epoch 0] Batch 3353, Loss 0.5209313631057739\n","[Training Epoch 0] Batch 3354, Loss 0.5234693884849548\n","[Training Epoch 0] Batch 3355, Loss 0.48775410652160645\n","[Training Epoch 0] Batch 3356, Loss 0.4655253291130066\n","[Training Epoch 0] Batch 3357, Loss 0.4837265908718109\n","[Training Epoch 0] Batch 3358, Loss 0.49044787883758545\n","[Training Epoch 0] Batch 3359, Loss 0.5097769498825073\n","[Training Epoch 0] Batch 3360, Loss 0.5353264212608337\n","[Training Epoch 0] Batch 3361, Loss 0.49145084619522095\n","[Training Epoch 0] Batch 3362, Loss 0.49833542108535767\n","[Training Epoch 0] Batch 3363, Loss 0.5007891654968262\n","[Training Epoch 0] Batch 3364, Loss 0.4797993302345276\n","[Training Epoch 0] Batch 3365, Loss 0.4994680881500244\n","[Training Epoch 0] Batch 3366, Loss 0.4824219346046448\n","[Training Epoch 0] Batch 3367, Loss 0.5194864273071289\n","[Training Epoch 0] Batch 3368, Loss 0.49964621663093567\n","[Training Epoch 0] Batch 3369, Loss 0.5087419748306274\n","[Training Epoch 0] Batch 3370, Loss 0.48772314190864563\n","[Training Epoch 0] Batch 3371, Loss 0.5112994909286499\n","[Training Epoch 0] Batch 3372, Loss 0.4851651191711426\n","[Training Epoch 0] Batch 3373, Loss 0.5022183656692505\n","[Training Epoch 0] Batch 3374, Loss 0.5230388045310974\n","[Training Epoch 0] Batch 3375, Loss 0.503272533416748\n","[Training Epoch 0] Batch 3376, Loss 0.49027320742607117\n","[Training Epoch 0] Batch 3377, Loss 0.47163158655166626\n","[Training Epoch 0] Batch 3378, Loss 0.5193069577217102\n","[Training Epoch 0] Batch 3379, Loss 0.5021364092826843\n","[Training Epoch 0] Batch 3380, Loss 0.5298943519592285\n","[Training Epoch 0] Batch 3381, Loss 0.5100164413452148\n","[Training Epoch 0] Batch 3382, Loss 0.47970589995384216\n","[Training Epoch 0] Batch 3383, Loss 0.5180838704109192\n","[Training Epoch 0] Batch 3384, Loss 0.5020385980606079\n","[Training Epoch 0] Batch 3385, Loss 0.48634204268455505\n","[Training Epoch 0] Batch 3386, Loss 0.5219177007675171\n","[Training Epoch 0] Batch 3387, Loss 0.49425724148750305\n","[Training Epoch 0] Batch 3388, Loss 0.5007540583610535\n","[Training Epoch 0] Batch 3389, Loss 0.4704362154006958\n","[Training Epoch 0] Batch 3390, Loss 0.4889048635959625\n","[Training Epoch 0] Batch 3391, Loss 0.4703376293182373\n","[Training Epoch 0] Batch 3392, Loss 0.48616936802864075\n","[Training Epoch 0] Batch 3393, Loss 0.49279001355171204\n","[Training Epoch 0] Batch 3394, Loss 0.5152783989906311\n","[Training Epoch 0] Batch 3395, Loss 0.5114738941192627\n","[Training Epoch 0] Batch 3396, Loss 0.4929007887840271\n","[Training Epoch 0] Batch 3397, Loss 0.4980742335319519\n","[Training Epoch 0] Batch 3398, Loss 0.4952855110168457\n","[Training Epoch 0] Batch 3399, Loss 0.5165616869926453\n","[Training Epoch 0] Batch 3400, Loss 0.5113003253936768\n","[Training Epoch 0] Batch 3401, Loss 0.4638248085975647\n","[Training Epoch 0] Batch 3402, Loss 0.5113104581832886\n","[Training Epoch 0] Batch 3403, Loss 0.5112431049346924\n","[Training Epoch 0] Batch 3404, Loss 0.48348909616470337\n","[Training Epoch 0] Batch 3405, Loss 0.5058931708335876\n","[Training Epoch 0] Batch 3406, Loss 0.5138336420059204\n","[Training Epoch 0] Batch 3407, Loss 0.5285027027130127\n","[Training Epoch 0] Batch 3408, Loss 0.5165038108825684\n","[Training Epoch 0] Batch 3409, Loss 0.5047119855880737\n","[Training Epoch 0] Batch 3410, Loss 0.4926704466342926\n","[Training Epoch 0] Batch 3411, Loss 0.49827098846435547\n","[Training Epoch 0] Batch 3412, Loss 0.5206968188285828\n","[Training Epoch 0] Batch 3413, Loss 0.5061417818069458\n","[Training Epoch 0] Batch 3414, Loss 0.4810037612915039\n","[Training Epoch 0] Batch 3415, Loss 0.471670925617218\n","[Training Epoch 0] Batch 3416, Loss 0.4702422618865967\n","[Training Epoch 0] Batch 3417, Loss 0.48981040716171265\n","[Training Epoch 0] Batch 3418, Loss 0.5127462148666382\n","[Training Epoch 0] Batch 3419, Loss 0.5071432590484619\n","[Training Epoch 0] Batch 3420, Loss 0.4872157573699951\n","[Training Epoch 0] Batch 3421, Loss 0.49799782037734985\n","[Training Epoch 0] Batch 3422, Loss 0.48376452922821045\n","[Training Epoch 0] Batch 3423, Loss 0.48784151673316956\n","[Training Epoch 0] Batch 3424, Loss 0.5129179358482361\n","[Training Epoch 0] Batch 3425, Loss 0.5188853740692139\n","[Training Epoch 0] Batch 3426, Loss 0.5007449984550476\n","[Training Epoch 0] Batch 3427, Loss 0.4856174886226654\n","[Training Epoch 0] Batch 3428, Loss 0.5020509362220764\n","[Training Epoch 0] Batch 3429, Loss 0.5195082426071167\n","[Training Epoch 0] Batch 3430, Loss 0.48360568284988403\n","[Training Epoch 0] Batch 3431, Loss 0.5117114186286926\n","[Training Epoch 0] Batch 3432, Loss 0.4742835760116577\n","[Training Epoch 0] Batch 3433, Loss 0.5285741686820984\n","[Training Epoch 0] Batch 3434, Loss 0.5127655267715454\n","[Training Epoch 0] Batch 3435, Loss 0.5169452428817749\n","[Training Epoch 0] Batch 3436, Loss 0.4954429268836975\n","[Training Epoch 0] Batch 3437, Loss 0.5153038501739502\n","[Training Epoch 0] Batch 3438, Loss 0.5062930583953857\n","[Training Epoch 0] Batch 3439, Loss 0.49023598432540894\n","[Training Epoch 0] Batch 3440, Loss 0.5032333135604858\n","[Training Epoch 0] Batch 3441, Loss 0.5126914978027344\n","[Training Epoch 0] Batch 3442, Loss 0.5086082220077515\n","[Training Epoch 0] Batch 3443, Loss 0.5099253058433533\n","[Training Epoch 0] Batch 3444, Loss 0.5055333375930786\n","[Training Epoch 0] Batch 3445, Loss 0.5141969919204712\n","[Training Epoch 0] Batch 3446, Loss 0.48640134930610657\n","[Training Epoch 0] Batch 3447, Loss 0.5167790055274963\n","[Training Epoch 0] Batch 3448, Loss 0.49667298793792725\n","[Training Epoch 0] Batch 3449, Loss 0.49013739824295044\n","[Training Epoch 0] Batch 3450, Loss 0.5340884327888489\n","[Training Epoch 0] Batch 3451, Loss 0.4870506525039673\n","[Training Epoch 0] Batch 3452, Loss 0.535608172416687\n","[Training Epoch 0] Batch 3453, Loss 0.48471325635910034\n","[Training Epoch 0] Batch 3454, Loss 0.48760801553726196\n","[Training Epoch 0] Batch 3455, Loss 0.47526857256889343\n","[Training Epoch 0] Batch 3456, Loss 0.5152900218963623\n","[Training Epoch 0] Batch 3457, Loss 0.4606720805168152\n","[Training Epoch 0] Batch 3458, Loss 0.48995453119277954\n","[Training Epoch 0] Batch 3459, Loss 0.47947147488594055\n","[Training Epoch 0] Batch 3460, Loss 0.48740699887275696\n","[Training Epoch 0] Batch 3461, Loss 0.4860798716545105\n","[Training Epoch 0] Batch 3462, Loss 0.47971659898757935\n","[Training Epoch 0] Batch 3463, Loss 0.47938311100006104\n","[Training Epoch 0] Batch 3464, Loss 0.49536842107772827\n","[Training Epoch 0] Batch 3465, Loss 0.49020254611968994\n","[Training Epoch 0] Batch 3466, Loss 0.5336685180664062\n","[Training Epoch 0] Batch 3467, Loss 0.515577495098114\n","[Training Epoch 0] Batch 3468, Loss 0.508382260799408\n","[Training Epoch 0] Batch 3469, Loss 0.5104212760925293\n","[Training Epoch 0] Batch 3470, Loss 0.48849329352378845\n","[Training Epoch 0] Batch 3471, Loss 0.5071557760238647\n","[Training Epoch 0] Batch 3472, Loss 0.5051701664924622\n","[Training Epoch 0] Batch 3473, Loss 0.5139809846878052\n","[Training Epoch 0] Batch 3474, Loss 0.5006005167961121\n","[Training Epoch 0] Batch 3475, Loss 0.4979430139064789\n","[Training Epoch 0] Batch 3476, Loss 0.5139445066452026\n","[Training Epoch 0] Batch 3477, Loss 0.5042967796325684\n","[Training Epoch 0] Batch 3478, Loss 0.48884299397468567\n","[Training Epoch 0] Batch 3479, Loss 0.5064643025398254\n","[Training Epoch 0] Batch 3480, Loss 0.47977203130722046\n","[Training Epoch 0] Batch 3481, Loss 0.4968135952949524\n","[Training Epoch 0] Batch 3482, Loss 0.5025463700294495\n","[Training Epoch 0] Batch 3483, Loss 0.4923705458641052\n","[Training Epoch 0] Batch 3484, Loss 0.5109410285949707\n","[Training Epoch 0] Batch 3485, Loss 0.5152455568313599\n","[Training Epoch 0] Batch 3486, Loss 0.5086414217948914\n","[Training Epoch 0] Batch 3487, Loss 0.5263630747795105\n","[Training Epoch 0] Batch 3488, Loss 0.4850686490535736\n","[Training Epoch 0] Batch 3489, Loss 0.49798583984375\n","[Training Epoch 0] Batch 3490, Loss 0.5216867327690125\n","[Training Epoch 0] Batch 3491, Loss 0.507346510887146\n","[Training Epoch 0] Batch 3492, Loss 0.4782022535800934\n","[Training Epoch 0] Batch 3493, Loss 0.476895809173584\n","[Training Epoch 0] Batch 3494, Loss 0.5081071853637695\n","[Training Epoch 0] Batch 3495, Loss 0.5046529173851013\n","[Training Epoch 0] Batch 3496, Loss 0.4883228540420532\n","[Training Epoch 0] Batch 3497, Loss 0.536652147769928\n","[Training Epoch 0] Batch 3498, Loss 0.5130491256713867\n","[Training Epoch 0] Batch 3499, Loss 0.48738059401512146\n","[Training Epoch 0] Batch 3500, Loss 0.4953019917011261\n","[Training Epoch 0] Batch 3501, Loss 0.4978804588317871\n","[Training Epoch 0] Batch 3502, Loss 0.5035280585289001\n","[Training Epoch 0] Batch 3503, Loss 0.50630784034729\n","[Training Epoch 0] Batch 3504, Loss 0.454292356967926\n","[Training Epoch 0] Batch 3505, Loss 0.48770809173583984\n","[Training Epoch 0] Batch 3506, Loss 0.5361215472221375\n","[Training Epoch 0] Batch 3507, Loss 0.4741303026676178\n","[Training Epoch 0] Batch 3508, Loss 0.48702186346054077\n","[Training Epoch 0] Batch 3509, Loss 0.5124963521957397\n","[Training Epoch 0] Batch 3510, Loss 0.5202292203903198\n","[Training Epoch 0] Batch 3511, Loss 0.49576669931411743\n","[Training Epoch 0] Batch 3512, Loss 0.4996415078639984\n","[Training Epoch 0] Batch 3513, Loss 0.5008364915847778\n","[Training Epoch 0] Batch 3514, Loss 0.4697621464729309\n","[Training Epoch 0] Batch 3515, Loss 0.5044698715209961\n","[Training Epoch 0] Batch 3516, Loss 0.48085683584213257\n","[Training Epoch 0] Batch 3517, Loss 0.5048479437828064\n","[Training Epoch 0] Batch 3518, Loss 0.4809984266757965\n","[Training Epoch 0] Batch 3519, Loss 0.48477455973625183\n","[Training Epoch 0] Batch 3520, Loss 0.5195519924163818\n","[Training Epoch 0] Batch 3521, Loss 0.5255777835845947\n","[Training Epoch 0] Batch 3522, Loss 0.4994086027145386\n","[Training Epoch 0] Batch 3523, Loss 0.4927709996700287\n","[Training Epoch 0] Batch 3524, Loss 0.5326633453369141\n","[Training Epoch 0] Batch 3525, Loss 0.49127286672592163\n","[Training Epoch 0] Batch 3526, Loss 0.517990231513977\n","[Training Epoch 0] Batch 3527, Loss 0.4940986633300781\n","[Training Epoch 0] Batch 3528, Loss 0.49999505281448364\n","[Training Epoch 0] Batch 3529, Loss 0.5033048391342163\n","[Training Epoch 0] Batch 3530, Loss 0.530971348285675\n","[Training Epoch 0] Batch 3531, Loss 0.5064659118652344\n","[Training Epoch 0] Batch 3532, Loss 0.4901447892189026\n","[Training Epoch 0] Batch 3533, Loss 0.48467111587524414\n","[Training Epoch 0] Batch 3534, Loss 0.5006363391876221\n","[Training Epoch 0] Batch 3535, Loss 0.49952432513237\n","[Training Epoch 0] Batch 3536, Loss 0.4846673607826233\n","[Training Epoch 0] Batch 3537, Loss 0.501785933971405\n","[Training Epoch 0] Batch 3538, Loss 0.5407313108444214\n","[Training Epoch 0] Batch 3539, Loss 0.49237483739852905\n","[Training Epoch 0] Batch 3540, Loss 0.5071415305137634\n","[Training Epoch 0] Batch 3541, Loss 0.48349106311798096\n","[Training Epoch 0] Batch 3542, Loss 0.5143747329711914\n","[Training Epoch 0] Batch 3543, Loss 0.5260542631149292\n","[Training Epoch 0] Batch 3544, Loss 0.47558194398880005\n","[Training Epoch 0] Batch 3545, Loss 0.4836578071117401\n","[Training Epoch 0] Batch 3546, Loss 0.47177839279174805\n","[Training Epoch 0] Batch 3547, Loss 0.4939432144165039\n","[Training Epoch 0] Batch 3548, Loss 0.5129925012588501\n","[Training Epoch 0] Batch 3549, Loss 0.4570813775062561\n","[Training Epoch 0] Batch 3550, Loss 0.4874589741230011\n","[Training Epoch 0] Batch 3551, Loss 0.48071756958961487\n","[Training Epoch 0] Batch 3552, Loss 0.496767520904541\n","[Training Epoch 0] Batch 3553, Loss 0.5126126408576965\n","[Training Epoch 0] Batch 3554, Loss 0.5003288388252258\n","[Training Epoch 0] Batch 3555, Loss 0.5182584524154663\n","[Training Epoch 0] Batch 3556, Loss 0.47276732325553894\n","[Training Epoch 0] Batch 3557, Loss 0.5219568014144897\n","[Training Epoch 0] Batch 3558, Loss 0.5045444965362549\n","[Training Epoch 0] Batch 3559, Loss 0.5248859524726868\n","[Training Epoch 0] Batch 3560, Loss 0.5058876276016235\n","[Training Epoch 0] Batch 3561, Loss 0.5169556140899658\n","[Training Epoch 0] Batch 3562, Loss 0.5139816999435425\n","[Training Epoch 0] Batch 3563, Loss 0.5022848844528198\n","[Training Epoch 0] Batch 3564, Loss 0.5045638680458069\n","[Training Epoch 0] Batch 3565, Loss 0.4592171311378479\n","[Training Epoch 0] Batch 3566, Loss 0.5222470760345459\n","[Training Epoch 0] Batch 3567, Loss 0.4991591274738312\n","[Training Epoch 0] Batch 3568, Loss 0.49255114793777466\n","[Training Epoch 0] Batch 3569, Loss 0.48204129934310913\n","[Training Epoch 0] Batch 3570, Loss 0.45928487181663513\n","[Training Epoch 0] Batch 3571, Loss 0.4981222152709961\n","[Training Epoch 0] Batch 3572, Loss 0.49266886711120605\n","[Training Epoch 0] Batch 3573, Loss 0.47389107942581177\n","[Training Epoch 0] Batch 3574, Loss 0.5206870436668396\n","[Training Epoch 0] Batch 3575, Loss 0.5171124935150146\n","[Training Epoch 0] Batch 3576, Loss 0.4860146939754486\n","[Training Epoch 0] Batch 3577, Loss 0.515510618686676\n","[Training Epoch 0] Batch 3578, Loss 0.5036802887916565\n","[Training Epoch 0] Batch 3579, Loss 0.49005016684532166\n","[Training Epoch 0] Batch 3580, Loss 0.5050537586212158\n","[Training Epoch 0] Batch 3581, Loss 0.49829015135765076\n","[Training Epoch 0] Batch 3582, Loss 0.5206687450408936\n","[Training Epoch 0] Batch 3583, Loss 0.5088458061218262\n","[Training Epoch 0] Batch 3584, Loss 0.5155077576637268\n","[Training Epoch 0] Batch 3585, Loss 0.4793657064437866\n","[Training Epoch 0] Batch 3586, Loss 0.4900738000869751\n","[Training Epoch 0] Batch 3587, Loss 0.4769579768180847\n","[Training Epoch 0] Batch 3588, Loss 0.5219864845275879\n","[Training Epoch 0] Batch 3589, Loss 0.5218745470046997\n","[Training Epoch 0] Batch 3590, Loss 0.4846303164958954\n","[Training Epoch 0] Batch 3591, Loss 0.49253618717193604\n","[Training Epoch 0] Batch 3592, Loss 0.5153613090515137\n","[Training Epoch 0] Batch 3593, Loss 0.5300216674804688\n","[Training Epoch 0] Batch 3594, Loss 0.4755098223686218\n","[Training Epoch 0] Batch 3595, Loss 0.4885934293270111\n","[Training Epoch 0] Batch 3596, Loss 0.5234803557395935\n","[Training Epoch 0] Batch 3597, Loss 0.4751632511615753\n","[Training Epoch 0] Batch 3598, Loss 0.5169252753257751\n","[Training Epoch 0] Batch 3599, Loss 0.5328254699707031\n","[Training Epoch 0] Batch 3600, Loss 0.49674326181411743\n","[Training Epoch 0] Batch 3601, Loss 0.5370327830314636\n","[Training Epoch 0] Batch 3602, Loss 0.4674821197986603\n","[Training Epoch 0] Batch 3603, Loss 0.507344126701355\n","[Training Epoch 0] Batch 3604, Loss 0.5193982124328613\n","[Training Epoch 0] Batch 3605, Loss 0.5100053548812866\n","[Training Epoch 0] Batch 3606, Loss 0.5006741881370544\n","[Training Epoch 0] Batch 3607, Loss 0.492750346660614\n","[Training Epoch 0] Batch 3608, Loss 0.5341517925262451\n","[Training Epoch 0] Batch 3609, Loss 0.47274190187454224\n","[Training Epoch 0] Batch 3610, Loss 0.476611465215683\n","[Training Epoch 0] Batch 3611, Loss 0.4916091561317444\n","[Training Epoch 0] Batch 3612, Loss 0.5048073530197144\n","[Training Epoch 0] Batch 3613, Loss 0.5179541707038879\n","[Training Epoch 0] Batch 3614, Loss 0.48587313294410706\n","[Training Epoch 0] Batch 3615, Loss 0.4980112612247467\n","[Training Epoch 0] Batch 3616, Loss 0.5195146799087524\n","[Training Epoch 0] Batch 3617, Loss 0.5130648016929626\n","[Training Epoch 0] Batch 3618, Loss 0.5023756623268127\n","[Training Epoch 0] Batch 3619, Loss 0.5286226272583008\n","[Training Epoch 0] Batch 3620, Loss 0.501977801322937\n","[Training Epoch 0] Batch 3621, Loss 0.5315648913383484\n","[Training Epoch 0] Batch 3622, Loss 0.46856942772865295\n","[Training Epoch 0] Batch 3623, Loss 0.47126930952072144\n","[Training Epoch 0] Batch 3624, Loss 0.4765816032886505\n","[Training Epoch 0] Batch 3625, Loss 0.4874159097671509\n","[Training Epoch 0] Batch 3626, Loss 0.514136552810669\n","[Training Epoch 0] Batch 3627, Loss 0.4793955087661743\n","[Training Epoch 0] Batch 3628, Loss 0.5072776675224304\n","[Training Epoch 0] Batch 3629, Loss 0.4965033531188965\n","[Training Epoch 0] Batch 3630, Loss 0.4806971251964569\n","[Training Epoch 0] Batch 3631, Loss 0.4700183868408203\n","[Training Epoch 0] Batch 3632, Loss 0.48078352212905884\n","[Training Epoch 0] Batch 3633, Loss 0.5380690097808838\n","[Training Epoch 0] Batch 3634, Loss 0.4967716932296753\n","[Training Epoch 0] Batch 3635, Loss 0.5260100364685059\n","[Training Epoch 0] Batch 3636, Loss 0.48321226239204407\n","[Training Epoch 0] Batch 3637, Loss 0.5235401391983032\n","[Training Epoch 0] Batch 3638, Loss 0.48463568091392517\n","[Training Epoch 0] Batch 3639, Loss 0.5344147682189941\n","[Training Epoch 0] Batch 3640, Loss 0.5075109004974365\n","[Training Epoch 0] Batch 3641, Loss 0.5129238367080688\n","[Training Epoch 0] Batch 3642, Loss 0.48723340034484863\n","[Training Epoch 0] Batch 3643, Loss 0.48720115423202515\n","[Training Epoch 0] Batch 3644, Loss 0.5020087957382202\n","[Training Epoch 0] Batch 3645, Loss 0.5222200155258179\n","[Training Epoch 0] Batch 3646, Loss 0.5005413293838501\n","[Training Epoch 0] Batch 3647, Loss 0.4862663149833679\n","[Training Epoch 0] Batch 3648, Loss 0.49831506609916687\n","[Training Epoch 0] Batch 3649, Loss 0.47678107023239136\n","[Training Epoch 0] Batch 3650, Loss 0.507288932800293\n","[Training Epoch 0] Batch 3651, Loss 0.5244807004928589\n","[Training Epoch 0] Batch 3652, Loss 0.4579554498195648\n","[Training Epoch 0] Batch 3653, Loss 0.47655290365219116\n","[Training Epoch 0] Batch 3654, Loss 0.4888598322868347\n","[Training Epoch 0] Batch 3655, Loss 0.5326616168022156\n","[Training Epoch 0] Batch 3656, Loss 0.48061084747314453\n","[Training Epoch 0] Batch 3657, Loss 0.4821246266365051\n","[Training Epoch 0] Batch 3658, Loss 0.48746833205223083\n","[Training Epoch 0] Batch 3659, Loss 0.5007596611976624\n","[Training Epoch 0] Batch 3660, Loss 0.483233243227005\n","[Training Epoch 0] Batch 3661, Loss 0.49534159898757935\n","[Training Epoch 0] Batch 3662, Loss 0.5047600269317627\n","[Training Epoch 0] Batch 3663, Loss 0.5338606238365173\n","[Training Epoch 0] Batch 3664, Loss 0.4780595302581787\n","[Training Epoch 0] Batch 3665, Loss 0.5312621593475342\n","[Training Epoch 0] Batch 3666, Loss 0.48326778411865234\n","[Training Epoch 0] Batch 3667, Loss 0.5328547954559326\n","[Training Epoch 0] Batch 3668, Loss 0.49645888805389404\n","[Training Epoch 0] Batch 3669, Loss 0.486255943775177\n","[Training Epoch 0] Batch 3670, Loss 0.4793331027030945\n","[Training Epoch 0] Batch 3671, Loss 0.4915023148059845\n","[Training Epoch 0] Batch 3672, Loss 0.4564496874809265\n","[Training Epoch 0] Batch 3673, Loss 0.49780189990997314\n","[Training Epoch 0] Batch 3674, Loss 0.5140622854232788\n","[Training Epoch 0] Batch 3675, Loss 0.4605509638786316\n","[Training Epoch 0] Batch 3676, Loss 0.5128547549247742\n","[Training Epoch 0] Batch 3677, Loss 0.4939001202583313\n","[Training Epoch 0] Batch 3678, Loss 0.5085732936859131\n","[Training Epoch 0] Batch 3679, Loss 0.5059036612510681\n","[Training Epoch 0] Batch 3680, Loss 0.5063214302062988\n","[Training Epoch 0] Batch 3681, Loss 0.5007327795028687\n","[Training Epoch 0] Batch 3682, Loss 0.500950276851654\n","[Training Epoch 0] Batch 3683, Loss 0.49924904108047485\n","[Training Epoch 0] Batch 3684, Loss 0.49133920669555664\n","[Training Epoch 0] Batch 3685, Loss 0.4791296720504761\n","[Training Epoch 0] Batch 3686, Loss 0.5219112038612366\n","[Training Epoch 0] Batch 3687, Loss 0.5033473968505859\n","[Training Epoch 0] Batch 3688, Loss 0.5033639669418335\n","[Training Epoch 0] Batch 3689, Loss 0.4846750497817993\n","[Training Epoch 0] Batch 3690, Loss 0.5021135807037354\n","[Training Epoch 0] Batch 3691, Loss 0.5103769898414612\n","[Training Epoch 0] Batch 3692, Loss 0.4941781163215637\n","[Training Epoch 0] Batch 3693, Loss 0.49665188789367676\n","[Training Epoch 0] Batch 3694, Loss 0.5138319730758667\n","[Training Epoch 0] Batch 3695, Loss 0.5021718144416809\n","[Training Epoch 0] Batch 3696, Loss 0.5235083103179932\n","[Training Epoch 0] Batch 3697, Loss 0.4805324077606201\n","[Training Epoch 0] Batch 3698, Loss 0.5233891606330872\n","[Training Epoch 0] Batch 3699, Loss 0.49807825684547424\n","[Training Epoch 0] Batch 3700, Loss 0.5153083205223083\n","[Training Epoch 0] Batch 3701, Loss 0.5179619789123535\n","[Training Epoch 0] Batch 3702, Loss 0.5087034702301025\n","[Training Epoch 0] Batch 3703, Loss 0.5086090564727783\n","[Training Epoch 0] Batch 3704, Loss 0.5233585238456726\n","[Training Epoch 0] Batch 3705, Loss 0.5060914754867554\n","[Training Epoch 0] Batch 3706, Loss 0.5261342525482178\n","[Training Epoch 0] Batch 3707, Loss 0.46841955184936523\n","[Training Epoch 0] Batch 3708, Loss 0.5395060777664185\n","[Training Epoch 0] Batch 3709, Loss 0.48999345302581787\n","[Training Epoch 0] Batch 3710, Loss 0.49921077489852905\n","[Training Epoch 0] Batch 3711, Loss 0.4740464985370636\n","[Training Epoch 0] Batch 3712, Loss 0.5007020831108093\n","[Training Epoch 0] Batch 3713, Loss 0.511439323425293\n","[Training Epoch 0] Batch 3714, Loss 0.5348505973815918\n","[Training Epoch 0] Batch 3715, Loss 0.4923255443572998\n","[Training Epoch 0] Batch 3716, Loss 0.5130263566970825\n","[Training Epoch 0] Batch 3717, Loss 0.48724365234375\n","[Training Epoch 0] Batch 3718, Loss 0.5205155611038208\n","[Training Epoch 0] Batch 3719, Loss 0.5255318880081177\n","[Training Epoch 0] Batch 3720, Loss 0.4992360472679138\n","[Training Epoch 0] Batch 3721, Loss 0.4913474917411804\n","[Training Epoch 0] Batch 3722, Loss 0.5058431029319763\n","[Training Epoch 0] Batch 3723, Loss 0.47918587923049927\n","[Training Epoch 0] Batch 3724, Loss 0.5045130252838135\n","[Training Epoch 0] Batch 3725, Loss 0.5167100429534912\n","[Training Epoch 0] Batch 3726, Loss 0.49517956376075745\n","[Training Epoch 0] Batch 3727, Loss 0.5035141110420227\n","[Training Epoch 0] Batch 3728, Loss 0.49833208322525024\n","[Training Epoch 0] Batch 3729, Loss 0.497158944606781\n","[Training Epoch 0] Batch 3730, Loss 0.5111923217773438\n","[Training Epoch 0] Batch 3731, Loss 0.5128622055053711\n","[Training Epoch 0] Batch 3732, Loss 0.5418579578399658\n","[Training Epoch 0] Batch 3733, Loss 0.4819743037223816\n","[Training Epoch 0] Batch 3734, Loss 0.49791520833969116\n","[Training Epoch 0] Batch 3735, Loss 0.49163395166397095\n","[Training Epoch 0] Batch 3736, Loss 0.5101653933525085\n","[Training Epoch 0] Batch 3737, Loss 0.5085564851760864\n","[Training Epoch 0] Batch 3738, Loss 0.4927089810371399\n","[Training Epoch 0] Batch 3739, Loss 0.45658111572265625\n","[Training Epoch 0] Batch 3740, Loss 0.5130282640457153\n","[Training Epoch 0] Batch 3741, Loss 0.5100017786026001\n","[Training Epoch 0] Batch 3742, Loss 0.51466965675354\n","[Training Epoch 0] Batch 3743, Loss 0.47822195291519165\n","[Training Epoch 0] Batch 3744, Loss 0.5181849598884583\n","[Training Epoch 0] Batch 3745, Loss 0.4777865409851074\n","[Training Epoch 0] Batch 3746, Loss 0.47782987356185913\n","[Training Epoch 0] Batch 3747, Loss 0.49623557925224304\n","[Training Epoch 0] Batch 3748, Loss 0.4893339276313782\n","[Training Epoch 0] Batch 3749, Loss 0.5031507015228271\n","[Training Epoch 0] Batch 3750, Loss 0.4859597086906433\n","[Training Epoch 0] Batch 3751, Loss 0.5116308927536011\n","[Training Epoch 0] Batch 3752, Loss 0.5114391446113586\n","[Training Epoch 0] Batch 3753, Loss 0.5155180096626282\n","[Training Epoch 0] Batch 3754, Loss 0.5047706961631775\n","[Training Epoch 0] Batch 3755, Loss 0.5205974578857422\n","[Training Epoch 0] Batch 3756, Loss 0.47784850001335144\n","[Training Epoch 0] Batch 3757, Loss 0.49959617853164673\n","[Training Epoch 0] Batch 3758, Loss 0.5091713666915894\n","[Training Epoch 0] Batch 3759, Loss 0.4897749423980713\n","[Training Epoch 0] Batch 3760, Loss 0.5130800604820251\n","[Training Epoch 0] Batch 3761, Loss 0.518397331237793\n","[Training Epoch 0] Batch 3762, Loss 0.5316266417503357\n","[Training Epoch 0] Batch 3763, Loss 0.47990682721138\n","[Training Epoch 0] Batch 3764, Loss 0.5072857737541199\n","[Training Epoch 0] Batch 3765, Loss 0.4796052575111389\n","[Training Epoch 0] Batch 3766, Loss 0.47786617279052734\n","[Training Epoch 0] Batch 3767, Loss 0.5142570734024048\n","[Training Epoch 0] Batch 3768, Loss 0.4900508224964142\n","[Training Epoch 0] Batch 3769, Loss 0.5099897384643555\n","[Training Epoch 0] Batch 3770, Loss 0.48962533473968506\n","[Training Epoch 0] Batch 3771, Loss 0.48089686036109924\n","[Training Epoch 0] Batch 3772, Loss 0.514457106590271\n","[Training Epoch 0] Batch 3773, Loss 0.4968106746673584\n","[Training Epoch 0] Batch 3774, Loss 0.4835667610168457\n","[Training Epoch 0] Batch 3775, Loss 0.47930246591567993\n","[Training Epoch 0] Batch 3776, Loss 0.49272677302360535\n","[Training Epoch 0] Batch 3777, Loss 0.47133034467697144\n","[Training Epoch 0] Batch 3778, Loss 0.4925305247306824\n","[Training Epoch 0] Batch 3779, Loss 0.5023139715194702\n","[Training Epoch 0] Batch 3780, Loss 0.47385314106941223\n","[Training Epoch 0] Batch 3781, Loss 0.5007588267326355\n","[Training Epoch 0] Batch 3782, Loss 0.4924108386039734\n","[Training Epoch 0] Batch 3783, Loss 0.45386844873428345\n","[Training Epoch 0] Batch 3784, Loss 0.47527754306793213\n","[Training Epoch 0] Batch 3785, Loss 0.529179573059082\n","[Training Epoch 0] Batch 3786, Loss 0.4858487844467163\n","[Training Epoch 0] Batch 3787, Loss 0.48319315910339355\n","[Training Epoch 0] Batch 3788, Loss 0.4911803603172302\n","[Training Epoch 0] Batch 3789, Loss 0.4806821346282959\n","[Training Epoch 0] Batch 3790, Loss 0.4834798574447632\n","[Training Epoch 0] Batch 3791, Loss 0.4941619634628296\n","[Training Epoch 0] Batch 3792, Loss 0.4886453151702881\n","[Training Epoch 0] Batch 3793, Loss 0.47243553400039673\n","[Training Epoch 0] Batch 3794, Loss 0.491374135017395\n","[Training Epoch 0] Batch 3795, Loss 0.4956084191799164\n","[Training Epoch 0] Batch 3796, Loss 0.5197372436523438\n","[Training Epoch 0] Batch 3797, Loss 0.4833109974861145\n","[Training Epoch 0] Batch 3798, Loss 0.5060465335845947\n","[Training Epoch 0] Batch 3799, Loss 0.4937519133090973\n","[Training Epoch 0] Batch 3800, Loss 0.5086938738822937\n","[Training Epoch 0] Batch 3801, Loss 0.49382197856903076\n","[Training Epoch 0] Batch 3802, Loss 0.5115653872489929\n","[Training Epoch 0] Batch 3803, Loss 0.45899397134780884\n","[Training Epoch 0] Batch 3804, Loss 0.4830694794654846\n","[Training Epoch 0] Batch 3805, Loss 0.5193896293640137\n","[Training Epoch 0] Batch 3806, Loss 0.4899885654449463\n","[Training Epoch 0] Batch 3807, Loss 0.48870849609375\n","[Training Epoch 0] Batch 3808, Loss 0.49383607506752014\n","[Training Epoch 0] Batch 3809, Loss 0.5033912062644958\n","[Training Epoch 0] Batch 3810, Loss 0.48348692059516907\n","[Training Epoch 0] Batch 3811, Loss 0.5194668173789978\n","[Training Epoch 0] Batch 3812, Loss 0.5007059574127197\n","[Training Epoch 0] Batch 3813, Loss 0.49408042430877686\n","[Training Epoch 0] Batch 3814, Loss 0.4883965849876404\n","[Training Epoch 0] Batch 3815, Loss 0.4775644540786743\n","[Training Epoch 0] Batch 3816, Loss 0.49663257598876953\n","[Training Epoch 0] Batch 3817, Loss 0.49007654190063477\n","[Training Epoch 0] Batch 3818, Loss 0.5044375658035278\n","[Training Epoch 0] Batch 3819, Loss 0.4846087098121643\n","[Training Epoch 0] Batch 3820, Loss 0.46820521354675293\n","[Training Epoch 0] Batch 3821, Loss 0.47616350650787354\n","[Training Epoch 0] Batch 3822, Loss 0.5251466035842896\n","[Training Epoch 0] Batch 3823, Loss 0.4618108868598938\n","[Training Epoch 0] Batch 3824, Loss 0.5087705254554749\n","[Training Epoch 0] Batch 3825, Loss 0.5199002027511597\n","[Training Epoch 0] Batch 3826, Loss 0.4762282967567444\n","[Training Epoch 0] Batch 3827, Loss 0.5005371570587158\n","[Training Epoch 0] Batch 3828, Loss 0.4735878109931946\n","[Training Epoch 0] Batch 3829, Loss 0.5074944496154785\n","[Training Epoch 0] Batch 3830, Loss 0.5007357001304626\n","[Training Epoch 0] Batch 3831, Loss 0.5005912780761719\n","[Training Epoch 0] Batch 3832, Loss 0.48704639077186584\n","[Training Epoch 0] Batch 3833, Loss 0.48215097188949585\n","[Training Epoch 0] Batch 3834, Loss 0.5005604028701782\n","[Training Epoch 0] Batch 3835, Loss 0.5153816938400269\n","[Training Epoch 0] Batch 3836, Loss 0.5158922672271729\n","[Training Epoch 0] Batch 3837, Loss 0.5246608257293701\n","[Training Epoch 0] Batch 3838, Loss 0.4976433515548706\n","[Training Epoch 0] Batch 3839, Loss 0.5170077681541443\n","[Training Epoch 0] Batch 3840, Loss 0.5221871137619019\n","[Training Epoch 0] Batch 3841, Loss 0.47904056310653687\n","[Training Epoch 0] Batch 3842, Loss 0.49678391218185425\n","[Training Epoch 0] Batch 3843, Loss 0.49267178773880005\n","[Training Epoch 0] Batch 3844, Loss 0.5088499188423157\n","[Training Epoch 0] Batch 3845, Loss 0.5111901164054871\n","[Training Epoch 0] Batch 3846, Loss 0.49809566140174866\n","[Training Epoch 0] Batch 3847, Loss 0.5043763518333435\n","[Training Epoch 0] Batch 3848, Loss 0.48554831743240356\n","[Training Epoch 0] Batch 3849, Loss 0.49520522356033325\n","[Training Epoch 0] Batch 3850, Loss 0.5113770365715027\n","[Training Epoch 0] Batch 3851, Loss 0.47148311138153076\n","[Training Epoch 0] Batch 3852, Loss 0.4909598231315613\n","[Training Epoch 0] Batch 3853, Loss 0.49011632800102234\n","[Training Epoch 0] Batch 3854, Loss 0.4818214178085327\n","[Training Epoch 0] Batch 3855, Loss 0.5035629272460938\n","[Training Epoch 0] Batch 3856, Loss 0.515333354473114\n","[Training Epoch 0] Batch 3857, Loss 0.5113502740859985\n","[Training Epoch 0] Batch 3858, Loss 0.4926200807094574\n","[Training Epoch 0] Batch 3859, Loss 0.5104773044586182\n","[Training Epoch 0] Batch 3860, Loss 0.48261892795562744\n","[Training Epoch 0] Batch 3861, Loss 0.483325719833374\n","[Training Epoch 0] Batch 3862, Loss 0.4900285005569458\n","[Training Epoch 0] Batch 3863, Loss 0.535830020904541\n","[Training Epoch 0] Batch 3864, Loss 0.5103168487548828\n","[Training Epoch 0] Batch 3865, Loss 0.5007948279380798\n","[Training Epoch 0] Batch 3866, Loss 0.49099016189575195\n","[Training Epoch 0] Batch 3867, Loss 0.4575321674346924\n","[Training Epoch 0] Batch 3868, Loss 0.5179538726806641\n","[Training Epoch 0] Batch 3869, Loss 0.5128411054611206\n","[Training Epoch 0] Batch 3870, Loss 0.5273687839508057\n","[Training Epoch 0] Batch 3871, Loss 0.5415482521057129\n","[Training Epoch 0] Batch 3872, Loss 0.46400904655456543\n","[Training Epoch 0] Batch 3873, Loss 0.5288509130477905\n","[Training Epoch 0] Batch 3874, Loss 0.4680057764053345\n","[Training Epoch 0] Batch 3875, Loss 0.5117403268814087\n","[Training Epoch 0] Batch 3876, Loss 0.5265655517578125\n","[Training Epoch 0] Batch 3877, Loss 0.5099254846572876\n","[Training Epoch 0] Batch 3878, Loss 0.48727214336395264\n","[Training Epoch 0] Batch 3879, Loss 0.4891316294670105\n","[Training Epoch 0] Batch 3880, Loss 0.4925520122051239\n","[Training Epoch 0] Batch 3881, Loss 0.48290491104125977\n","[Training Epoch 0] Batch 3882, Loss 0.5103749632835388\n","[Training Epoch 0] Batch 3883, Loss 0.49659502506256104\n","[Training Epoch 0] Batch 3884, Loss 0.5168954133987427\n","[Training Epoch 0] Batch 3885, Loss 0.5019447803497314\n","[Training Epoch 0] Batch 3886, Loss 0.5357130765914917\n","[Training Epoch 0] Batch 3887, Loss 0.48979729413986206\n","[Training Epoch 0] Batch 3888, Loss 0.49386894702911377\n","[Training Epoch 0] Batch 3889, Loss 0.5141947269439697\n","[Training Epoch 0] Batch 3890, Loss 0.49817419052124023\n","[Training Epoch 0] Batch 3891, Loss 0.5022082328796387\n","[Training Epoch 0] Batch 3892, Loss 0.5139465928077698\n","[Training Epoch 0] Batch 3893, Loss 0.4683975577354431\n","[Training Epoch 0] Batch 3894, Loss 0.5193334817886353\n","[Training Epoch 0] Batch 3895, Loss 0.5006819367408752\n","[Training Epoch 0] Batch 3896, Loss 0.49515044689178467\n","[Training Epoch 0] Batch 3897, Loss 0.491122841835022\n","[Training Epoch 0] Batch 3898, Loss 0.5119115114212036\n","[Training Epoch 0] Batch 3899, Loss 0.5209982991218567\n","[Training Epoch 0] Batch 3900, Loss 0.5234847068786621\n","[Training Epoch 0] Batch 3901, Loss 0.5045837759971619\n","[Training Epoch 0] Batch 3902, Loss 0.4953192472457886\n","[Training Epoch 0] Batch 3903, Loss 0.49250829219818115\n","[Training Epoch 0] Batch 3904, Loss 0.5261048674583435\n","[Training Epoch 0] Batch 3905, Loss 0.4978605806827545\n","[Training Epoch 0] Batch 3906, Loss 0.4698694944381714\n","[Training Epoch 0] Batch 3907, Loss 0.5049624443054199\n","[Training Epoch 0] Batch 3908, Loss 0.511264443397522\n","[Training Epoch 0] Batch 3909, Loss 0.5173548460006714\n","[Training Epoch 0] Batch 3910, Loss 0.47480881214141846\n","[Training Epoch 0] Batch 3911, Loss 0.4995063543319702\n","[Training Epoch 0] Batch 3912, Loss 0.4884621798992157\n","[Training Epoch 0] Batch 3913, Loss 0.49559301137924194\n","[Training Epoch 0] Batch 3914, Loss 0.4806966185569763\n","[Training Epoch 0] Batch 3915, Loss 0.483063280582428\n","[Training Epoch 0] Batch 3916, Loss 0.4734812378883362\n","[Training Epoch 0] Batch 3917, Loss 0.5006452202796936\n","[Training Epoch 0] Batch 3918, Loss 0.48606839776039124\n","[Training Epoch 0] Batch 3919, Loss 0.5184712409973145\n","[Training Epoch 0] Batch 3920, Loss 0.5035704374313354\n","[Training Epoch 0] Batch 3921, Loss 0.4872680604457855\n","[Training Epoch 0] Batch 3922, Loss 0.5093110799789429\n","[Training Epoch 0] Batch 3923, Loss 0.5315380692481995\n","[Training Epoch 0] Batch 3924, Loss 0.5027300119400024\n","[Training Epoch 0] Batch 3925, Loss 0.4967494308948517\n","[Training Epoch 0] Batch 3926, Loss 0.4952618181705475\n","[Training Epoch 0] Batch 3927, Loss 0.5056954622268677\n","[Training Epoch 0] Batch 3928, Loss 0.4980074167251587\n","[Training Epoch 0] Batch 3929, Loss 0.5050790309906006\n","[Training Epoch 0] Batch 3930, Loss 0.4804062247276306\n","[Training Epoch 0] Batch 3931, Loss 0.4953073561191559\n","[Training Epoch 0] Batch 3932, Loss 0.47360092401504517\n","[Training Epoch 0] Batch 3933, Loss 0.4709559679031372\n","[Training Epoch 0] Batch 3934, Loss 0.48987266421318054\n","[Training Epoch 0] Batch 3935, Loss 0.5020446181297302\n","[Training Epoch 0] Batch 3936, Loss 0.49098464846611023\n","[Training Epoch 0] Batch 3937, Loss 0.46962693333625793\n","[Training Epoch 0] Batch 3938, Loss 0.5020564198493958\n","[Training Epoch 0] Batch 3939, Loss 0.4941679835319519\n","[Training Epoch 0] Batch 3940, Loss 0.5072402954101562\n","[Training Epoch 0] Batch 3941, Loss 0.4991711378097534\n","[Training Epoch 0] Batch 3942, Loss 0.5045669078826904\n","[Training Epoch 0] Batch 3943, Loss 0.5101016759872437\n","[Training Epoch 0] Batch 3944, Loss 0.49789711833000183\n","[Training Epoch 0] Batch 3945, Loss 0.48707786202430725\n","[Training Epoch 0] Batch 3946, Loss 0.48713594675064087\n","[Training Epoch 0] Batch 3947, Loss 0.5115871429443359\n","[Training Epoch 0] Batch 3948, Loss 0.4936448931694031\n","[Training Epoch 0] Batch 3949, Loss 0.5184376835823059\n","[Training Epoch 0] Batch 3950, Loss 0.46838879585266113\n","[Training Epoch 0] Batch 3951, Loss 0.502131462097168\n","[Training Epoch 0] Batch 3952, Loss 0.4883924722671509\n","[Training Epoch 0] Batch 3953, Loss 0.5061437487602234\n","[Training Epoch 0] Batch 3954, Loss 0.5003594160079956\n","[Training Epoch 0] Batch 3955, Loss 0.48033520579338074\n","[Training Epoch 0] Batch 3956, Loss 0.5237666368484497\n","[Training Epoch 0] Batch 3957, Loss 0.5222318172454834\n","[Training Epoch 0] Batch 3958, Loss 0.5104414224624634\n","[Training Epoch 0] Batch 3959, Loss 0.5142464637756348\n","[Training Epoch 0] Batch 3960, Loss 0.506164014339447\n","[Training Epoch 0] Batch 3961, Loss 0.4749133586883545\n","[Training Epoch 0] Batch 3962, Loss 0.4899129867553711\n","[Training Epoch 0] Batch 3963, Loss 0.5237113833427429\n","[Training Epoch 0] Batch 3964, Loss 0.47777456045150757\n","[Training Epoch 0] Batch 3965, Loss 0.49121734499931335\n","[Training Epoch 0] Batch 3966, Loss 0.47744715213775635\n","[Training Epoch 0] Batch 3967, Loss 0.497913122177124\n","[Training Epoch 0] Batch 3968, Loss 0.5018939971923828\n","[Training Epoch 0] Batch 3969, Loss 0.5142407417297363\n","[Training Epoch 0] Batch 3970, Loss 0.4897603392601013\n","[Training Epoch 0] Batch 3971, Loss 0.5223084092140198\n","[Training Epoch 0] Batch 3972, Loss 0.47873273491859436\n","[Training Epoch 0] Batch 3973, Loss 0.49923384189605713\n","[Training Epoch 0] Batch 3974, Loss 0.5247752666473389\n","[Training Epoch 0] Batch 3975, Loss 0.49933046102523804\n","[Training Epoch 0] Batch 3976, Loss 0.48302462697029114\n","[Training Epoch 0] Batch 3977, Loss 0.49273228645324707\n","[Training Epoch 0] Batch 3978, Loss 0.4854583740234375\n","[Training Epoch 0] Batch 3979, Loss 0.4951418340206146\n","[Training Epoch 0] Batch 3980, Loss 0.4819575548171997\n","[Training Epoch 0] Batch 3981, Loss 0.5016113519668579\n","[Training Epoch 0] Batch 3982, Loss 0.47126504778862\n","[Training Epoch 0] Batch 3983, Loss 0.520268440246582\n","[Training Epoch 0] Batch 3984, Loss 0.4818178415298462\n","[Training Epoch 0] Batch 3985, Loss 0.4860214591026306\n","[Training Epoch 0] Batch 3986, Loss 0.48302161693573\n","[Training Epoch 0] Batch 3987, Loss 0.4968077838420868\n","[Training Epoch 0] Batch 3988, Loss 0.5065844655036926\n","[Training Epoch 0] Batch 3989, Loss 0.48081153631210327\n","[Training Epoch 0] Batch 3990, Loss 0.5148823261260986\n","[Training Epoch 0] Batch 3991, Loss 0.515886664390564\n","[Training Epoch 0] Batch 3992, Loss 0.5158603191375732\n","[Training Epoch 0] Batch 3993, Loss 0.5223324298858643\n","[Training Epoch 0] Batch 3994, Loss 0.4920995235443115\n","[Training Epoch 0] Batch 3995, Loss 0.4901086091995239\n","[Training Epoch 0] Batch 3996, Loss 0.5181127786636353\n","[Training Epoch 0] Batch 3997, Loss 0.5252797603607178\n","[Training Epoch 0] Batch 3998, Loss 0.4871963858604431\n","[Training Epoch 0] Batch 3999, Loss 0.4749622941017151\n","[Training Epoch 0] Batch 4000, Loss 0.512627363204956\n","[Training Epoch 0] Batch 4001, Loss 0.48185786604881287\n","[Training Epoch 0] Batch 4002, Loss 0.47507190704345703\n","[Training Epoch 0] Batch 4003, Loss 0.5050985217094421\n","[Training Epoch 0] Batch 4004, Loss 0.49647876620292664\n","[Training Epoch 0] Batch 4005, Loss 0.5059351921081543\n","[Training Epoch 0] Batch 4006, Loss 0.5072633028030396\n","[Training Epoch 0] Batch 4007, Loss 0.498024046421051\n","[Training Epoch 0] Batch 4008, Loss 0.5032143592834473\n","[Training Epoch 0] Batch 4009, Loss 0.5224207639694214\n","[Training Epoch 0] Batch 4010, Loss 0.49123966693878174\n","[Training Epoch 0] Batch 4011, Loss 0.51777184009552\n","[Training Epoch 0] Batch 4012, Loss 0.5118137001991272\n","[Training Epoch 0] Batch 4013, Loss 0.493560791015625\n","[Training Epoch 0] Batch 4014, Loss 0.5197057723999023\n","[Training Epoch 0] Batch 4015, Loss 0.5058926939964294\n","[Training Epoch 0] Batch 4016, Loss 0.49532854557037354\n","[Training Epoch 0] Batch 4017, Loss 0.5071713924407959\n","[Training Epoch 0] Batch 4018, Loss 0.5112941265106201\n","[Training Epoch 0] Batch 4019, Loss 0.4949284791946411\n","[Training Epoch 0] Batch 4020, Loss 0.48830780386924744\n","[Training Epoch 0] Batch 4021, Loss 0.5049082040786743\n","[Training Epoch 0] Batch 4022, Loss 0.4786645770072937\n","[Training Epoch 0] Batch 4023, Loss 0.5076441168785095\n","[Training Epoch 0] Batch 4024, Loss 0.5032482743263245\n","[Training Epoch 0] Batch 4025, Loss 0.5239112377166748\n","[Training Epoch 0] Batch 4026, Loss 0.5250657796859741\n","[Training Epoch 0] Batch 4027, Loss 0.5100915431976318\n","[Training Epoch 0] Batch 4028, Loss 0.5112112164497375\n","[Training Epoch 0] Batch 4029, Loss 0.5142331123352051\n","[Training Epoch 0] Batch 4030, Loss 0.5067175626754761\n","[Training Epoch 0] Batch 4031, Loss 0.4587844908237457\n","[Training Epoch 0] Batch 4032, Loss 0.5204269886016846\n","[Training Epoch 0] Batch 4033, Loss 0.510233998298645\n","[Training Epoch 0] Batch 4034, Loss 0.5004768967628479\n","[Training Epoch 0] Batch 4035, Loss 0.4763220548629761\n","[Training Epoch 0] Batch 4036, Loss 0.5036290884017944\n","[Training Epoch 0] Batch 4037, Loss 0.5108667612075806\n","[Training Epoch 0] Batch 4038, Loss 0.507104218006134\n","[Training Epoch 0] Batch 4039, Loss 0.496589720249176\n","[Training Epoch 0] Batch 4040, Loss 0.5153384208679199\n","[Training Epoch 0] Batch 4041, Loss 0.5075706243515015\n","[Training Epoch 0] Batch 4042, Loss 0.4753011465072632\n","[Training Epoch 0] Batch 4043, Loss 0.4672108292579651\n","[Training Epoch 0] Batch 4044, Loss 0.5101206302642822\n","[Training Epoch 0] Batch 4045, Loss 0.5355030298233032\n","[Training Epoch 0] Batch 4046, Loss 0.4938077926635742\n","[Training Epoch 0] Batch 4047, Loss 0.4750412106513977\n","[Training Epoch 0] Batch 4048, Loss 0.48405981063842773\n","[Training Epoch 0] Batch 4049, Loss 0.48049381375312805\n","[Training Epoch 0] Batch 4050, Loss 0.5221714973449707\n","[Training Epoch 0] Batch 4051, Loss 0.49805131554603577\n","[Training Epoch 0] Batch 4052, Loss 0.511462926864624\n","[Training Epoch 0] Batch 4053, Loss 0.512752890586853\n","[Training Epoch 0] Batch 4054, Loss 0.479139506816864\n","[Training Epoch 0] Batch 4055, Loss 0.50621098279953\n","[Training Epoch 0] Batch 4056, Loss 0.49299973249435425\n","[Training Epoch 0] Batch 4057, Loss 0.4901314377784729\n","[Training Epoch 0] Batch 4058, Loss 0.5340714454650879\n","[Training Epoch 0] Batch 4059, Loss 0.5009287595748901\n","[Training Epoch 0] Batch 4060, Loss 0.5395749807357788\n","[Training Epoch 0] Batch 4061, Loss 0.49754247069358826\n","[Training Epoch 0] Batch 4062, Loss 0.4515041708946228\n","[Training Epoch 0] Batch 4063, Loss 0.5358813405036926\n","[Training Epoch 0] Batch 4064, Loss 0.48228198289871216\n","[Training Epoch 0] Batch 4065, Loss 0.5053999423980713\n","[Training Epoch 0] Batch 4066, Loss 0.5124026536941528\n","[Training Epoch 0] Batch 4067, Loss 0.49704572558403015\n","[Training Epoch 0] Batch 4068, Loss 0.4955618977546692\n","[Training Epoch 0] Batch 4069, Loss 0.49649831652641296\n","[Training Epoch 0] Batch 4070, Loss 0.5035384893417358\n","[Training Epoch 0] Batch 4071, Loss 0.5089128017425537\n","[Training Epoch 0] Batch 4072, Loss 0.498443067073822\n","[Training Epoch 0] Batch 4073, Loss 0.5209628939628601\n","[Training Epoch 0] Batch 4074, Loss 0.5012811422348022\n","[Training Epoch 0] Batch 4075, Loss 0.5134648680686951\n","[Training Epoch 0] Batch 4076, Loss 0.4981135129928589\n","[Training Epoch 0] Batch 4077, Loss 0.5191255807876587\n","[Training Epoch 0] Batch 4078, Loss 0.4859403967857361\n","[Training Epoch 0] Batch 4079, Loss 0.4883543848991394\n","[Training Epoch 0] Batch 4080, Loss 0.4926764965057373\n","[Training Epoch 0] Batch 4081, Loss 0.5115711688995361\n","[Training Epoch 0] Batch 4082, Loss 0.49245697259902954\n","[Training Epoch 0] Batch 4083, Loss 0.5192133784294128\n","[Training Epoch 0] Batch 4084, Loss 0.4949215054512024\n","[Training Epoch 0] Batch 4085, Loss 0.4867357611656189\n","[Training Epoch 0] Batch 4086, Loss 0.5073980093002319\n","[Training Epoch 0] Batch 4087, Loss 0.502083957195282\n","[Training Epoch 0] Batch 4088, Loss 0.48632121086120605\n","[Training Epoch 0] Batch 4089, Loss 0.516973078250885\n","[Training Epoch 0] Batch 4090, Loss 0.5235599279403687\n","[Training Epoch 0] Batch 4091, Loss 0.4801532030105591\n","[Training Epoch 0] Batch 4092, Loss 0.5209131836891174\n","[Training Epoch 0] Batch 4093, Loss 0.5159608125686646\n","[Training Epoch 0] Batch 4094, Loss 0.47662726044654846\n","[Training Epoch 0] Batch 4095, Loss 0.5120725631713867\n","[Training Epoch 0] Batch 4096, Loss 0.48510703444480896\n","[Training Epoch 0] Batch 4097, Loss 0.5012955665588379\n","[Training Epoch 0] Batch 4098, Loss 0.5412247180938721\n","[Training Epoch 0] Batch 4099, Loss 0.4874002933502197\n","[Training Epoch 0] Batch 4100, Loss 0.4736633002758026\n","[Training Epoch 0] Batch 4101, Loss 0.5159412622451782\n","[Training Epoch 0] Batch 4102, Loss 0.493954062461853\n","[Training Epoch 0] Batch 4103, Loss 0.5073925852775574\n","[Training Epoch 0] Batch 4104, Loss 0.5141368508338928\n","[Training Epoch 0] Batch 4105, Loss 0.4981774687767029\n","[Training Epoch 0] Batch 4106, Loss 0.5005136728286743\n","[Training Epoch 0] Batch 4107, Loss 0.4831763505935669\n","[Training Epoch 0] Batch 4108, Loss 0.4735037386417389\n","[Training Epoch 0] Batch 4109, Loss 0.5010595321655273\n","[Training Epoch 0] Batch 4110, Loss 0.504952073097229\n","[Training Epoch 0] Batch 4111, Loss 0.458681583404541\n","[Training Epoch 0] Batch 4112, Loss 0.5044442415237427\n","[Training Epoch 0] Batch 4113, Loss 0.510644793510437\n","[Training Epoch 0] Batch 4114, Loss 0.49374574422836304\n","[Training Epoch 0] Batch 4115, Loss 0.4883793592453003\n","[Training Epoch 0] Batch 4116, Loss 0.4804522395133972\n","[Training Epoch 0] Batch 4117, Loss 0.488139271736145\n","[Training Epoch 0] Batch 4118, Loss 0.4707435369491577\n","[Training Epoch 0] Batch 4119, Loss 0.5356806516647339\n","[Training Epoch 0] Batch 4120, Loss 0.4883075952529907\n","[Training Epoch 0] Batch 4121, Loss 0.5049488544464111\n","[Training Epoch 0] Batch 4122, Loss 0.5142589211463928\n","[Training Epoch 0] Batch 4123, Loss 0.5145243406295776\n","[Training Epoch 0] Batch 4124, Loss 0.5138242244720459\n","[Training Epoch 0] Batch 4125, Loss 0.5232189893722534\n","[Training Epoch 0] Batch 4126, Loss 0.5240283012390137\n","[Training Epoch 0] Batch 4127, Loss 0.5062621831893921\n","[Training Epoch 0] Batch 4128, Loss 0.489629328250885\n","[Training Epoch 0] Batch 4129, Loss 0.48454731702804565\n","[Training Epoch 0] Batch 4130, Loss 0.5019973516464233\n","[Training Epoch 0] Batch 4131, Loss 0.5442545413970947\n","[Training Epoch 0] Batch 4132, Loss 0.5019113421440125\n","[Training Epoch 0] Batch 4133, Loss 0.4875161051750183\n","[Training Epoch 0] Batch 4134, Loss 0.4977751672267914\n","[Training Epoch 0] Batch 4135, Loss 0.5019849538803101\n","[Training Epoch 0] Batch 4136, Loss 0.48288607597351074\n","[Training Epoch 0] Batch 4137, Loss 0.528451681137085\n","[Training Epoch 0] Batch 4138, Loss 0.5109738111495972\n","[Training Epoch 0] Batch 4139, Loss 0.4737861156463623\n","[Training Epoch 0] Batch 4140, Loss 0.5019714832305908\n","[Training Epoch 0] Batch 4141, Loss 0.5369975566864014\n","[Training Epoch 0] Batch 4142, Loss 0.4920574426651001\n","[Training Epoch 0] Batch 4143, Loss 0.4734834134578705\n","[Training Epoch 0] Batch 4144, Loss 0.5008933544158936\n","[Training Epoch 0] Batch 4145, Loss 0.49252039194107056\n","[Training Epoch 0] Batch 4146, Loss 0.49266088008880615\n","[Training Epoch 0] Batch 4147, Loss 0.49935710430145264\n","[Training Epoch 0] Batch 4148, Loss 0.5302101373672485\n","[Training Epoch 0] Batch 4149, Loss 0.5131115317344666\n","[Training Epoch 0] Batch 4150, Loss 0.4615735411643982\n","[Training Epoch 0] Batch 4151, Loss 0.479123592376709\n","[Training Epoch 0] Batch 4152, Loss 0.4944998025894165\n","[Training Epoch 0] Batch 4153, Loss 0.49355918169021606\n","[Training Epoch 0] Batch 4154, Loss 0.4951390326023102\n","[Training Epoch 0] Batch 4155, Loss 0.4924798011779785\n","[Training Epoch 0] Batch 4156, Loss 0.5258272886276245\n","[Training Epoch 0] Batch 4157, Loss 0.5057479739189148\n","[Training Epoch 0] Batch 4158, Loss 0.4883296489715576\n","[Training Epoch 0] Batch 4159, Loss 0.49646681547164917\n","[Training Epoch 0] Batch 4160, Loss 0.48844781517982483\n","[Training Epoch 0] Batch 4161, Loss 0.5060077905654907\n","[Training Epoch 0] Batch 4162, Loss 0.5062077045440674\n","[Training Epoch 0] Batch 4163, Loss 0.5317966938018799\n","[Training Epoch 0] Batch 4164, Loss 0.48530685901641846\n","[Training Epoch 0] Batch 4165, Loss 0.5005577802658081\n","[Training Epoch 0] Batch 4166, Loss 0.4969475269317627\n","[Training Epoch 0] Batch 4167, Loss 0.47899961471557617\n","[Training Epoch 0] Batch 4168, Loss 0.49227994680404663\n","[Training Epoch 0] Batch 4169, Loss 0.5219771862030029\n","[Training Epoch 0] Batch 4170, Loss 0.49684804677963257\n","[Training Epoch 0] Batch 4171, Loss 0.5083398818969727\n","[Training Epoch 0] Batch 4172, Loss 0.5060300827026367\n","[Training Epoch 0] Batch 4173, Loss 0.5194507837295532\n","[Training Epoch 0] Batch 4174, Loss 0.4994094967842102\n","[Training Epoch 0] Batch 4175, Loss 0.48986053466796875\n","[Training Epoch 0] Batch 4176, Loss 0.45443952083587646\n","[Training Epoch 0] Batch 4177, Loss 0.5032671093940735\n","[Training Epoch 0] Batch 4178, Loss 0.5114755630493164\n","[Training Epoch 0] Batch 4179, Loss 0.49991321563720703\n","[Training Epoch 0] Batch 4180, Loss 0.5382612943649292\n","[Training Epoch 0] Batch 4181, Loss 0.5075355768203735\n","[Training Epoch 0] Batch 4182, Loss 0.5020176768302917\n","[Training Epoch 0] Batch 4183, Loss 0.47624245285987854\n","[Training Epoch 0] Batch 4184, Loss 0.5455673336982727\n","[Training Epoch 0] Batch 4185, Loss 0.5019717812538147\n","[Training Epoch 0] Batch 4186, Loss 0.5027049779891968\n","[Training Epoch 0] Batch 4187, Loss 0.4923108220100403\n","[Training Epoch 0] Batch 4188, Loss 0.5454456210136414\n","[Training Epoch 0] Batch 4189, Loss 0.4958578944206238\n","[Training Epoch 0] Batch 4190, Loss 0.5165829658508301\n","[Training Epoch 0] Batch 4191, Loss 0.4664084315299988\n","[Training Epoch 0] Batch 4192, Loss 0.5052554607391357\n","[Training Epoch 0] Batch 4193, Loss 0.5045962929725647\n","[Training Epoch 0] Batch 4194, Loss 0.5051568746566772\n","[Training Epoch 0] Batch 4195, Loss 0.49143025279045105\n","[Training Epoch 0] Batch 4196, Loss 0.49539807438850403\n","[Training Epoch 0] Batch 4197, Loss 0.5281565189361572\n","[Training Epoch 0] Batch 4198, Loss 0.5033960938453674\n","[Training Epoch 0] Batch 4199, Loss 0.5393937826156616\n","[Training Epoch 0] Batch 4200, Loss 0.4888402819633484\n","[Training Epoch 0] Batch 4201, Loss 0.47963452339172363\n","[Training Epoch 0] Batch 4202, Loss 0.5171284079551697\n","[Training Epoch 0] Batch 4203, Loss 0.5344796776771545\n","[Training Epoch 0] Batch 4204, Loss 0.5221337080001831\n","[Training Epoch 0] Batch 4205, Loss 0.527605414390564\n","[Training Epoch 0] Batch 4206, Loss 0.49642330408096313\n","[Training Epoch 0] Batch 4207, Loss 0.48616039752960205\n","[Training Epoch 0] Batch 4208, Loss 0.5048969984054565\n","[Training Epoch 0] Batch 4209, Loss 0.5138981342315674\n","[Training Epoch 0] Batch 4210, Loss 0.5004091262817383\n","[Training Epoch 0] Batch 4211, Loss 0.522368848323822\n","[Training Epoch 0] Batch 4212, Loss 0.525115966796875\n","[Training Epoch 0] Batch 4213, Loss 0.5425986647605896\n","[Training Epoch 0] Batch 4214, Loss 0.4830625653266907\n","[Training Epoch 0] Batch 4215, Loss 0.492684006690979\n","[Training Epoch 0] Batch 4216, Loss 0.4942304790019989\n","[Training Epoch 0] Batch 4217, Loss 0.5032097697257996\n","[Training Epoch 0] Batch 4218, Loss 0.4832497239112854\n","[Training Epoch 0] Batch 4219, Loss 0.5180011987686157\n","[Training Epoch 0] Batch 4220, Loss 0.5115072727203369\n","[Training Epoch 0] Batch 4221, Loss 0.49667805433273315\n","[Training Epoch 0] Batch 4222, Loss 0.5006827712059021\n","[Training Epoch 0] Batch 4223, Loss 0.5017741322517395\n","[Training Epoch 0] Batch 4224, Loss 0.5060173869132996\n","[Training Epoch 0] Batch 4225, Loss 0.4966704249382019\n","[Training Epoch 0] Batch 4226, Loss 0.49638983607292175\n","[Training Epoch 0] Batch 4227, Loss 0.4873594641685486\n","[Training Epoch 0] Batch 4228, Loss 0.48596999049186707\n","[Training Epoch 0] Batch 4229, Loss 0.49681007862091064\n","[Training Epoch 0] Batch 4230, Loss 0.4794877767562866\n","[Training Epoch 0] Batch 4231, Loss 0.5054674744606018\n","[Training Epoch 0] Batch 4232, Loss 0.4951916038990021\n","[Training Epoch 0] Batch 4233, Loss 0.51285719871521\n","[Training Epoch 0] Batch 4234, Loss 0.4952104687690735\n","[Training Epoch 0] Batch 4235, Loss 0.516961932182312\n","[Training Epoch 0] Batch 4236, Loss 0.5200915932655334\n","[Training Epoch 0] Batch 4237, Loss 0.5076469779014587\n","[Training Epoch 0] Batch 4238, Loss 0.5224045515060425\n","[Training Epoch 0] Batch 4239, Loss 0.5198649168014526\n","[Training Epoch 0] Batch 4240, Loss 0.5167772769927979\n","[Training Epoch 0] Batch 4241, Loss 0.4915578365325928\n","[Training Epoch 0] Batch 4242, Loss 0.48174744844436646\n","[Training Epoch 0] Batch 4243, Loss 0.5074110627174377\n","[Training Epoch 0] Batch 4244, Loss 0.5086814165115356\n","[Training Epoch 0] Batch 4245, Loss 0.5246874690055847\n","[Training Epoch 0] Batch 4246, Loss 0.48337772488594055\n","[Training Epoch 0] Batch 4247, Loss 0.49533796310424805\n","[Training Epoch 0] Batch 4248, Loss 0.4937514662742615\n","[Training Epoch 0] Batch 4249, Loss 0.5021761059761047\n","[Training Epoch 0] Batch 4250, Loss 0.5034618377685547\n","[Training Epoch 0] Batch 4251, Loss 0.4681430757045746\n","[Training Epoch 0] Batch 4252, Loss 0.5007201433181763\n","[Training Epoch 0] Batch 4253, Loss 0.5104296207427979\n","[Training Epoch 0] Batch 4254, Loss 0.4808526933193207\n","[Training Epoch 0] Batch 4255, Loss 0.5004797577857971\n","[Training Epoch 0] Batch 4256, Loss 0.5074692964553833\n","[Training Epoch 0] Batch 4257, Loss 0.498063862323761\n","[Training Epoch 0] Batch 4258, Loss 0.5262650847434998\n","[Training Epoch 0] Batch 4259, Loss 0.5008386373519897\n","[Training Epoch 0] Batch 4260, Loss 0.508651077747345\n","[Training Epoch 0] Batch 4261, Loss 0.4884864389896393\n","[Training Epoch 0] Batch 4262, Loss 0.4830491244792938\n","[Training Epoch 0] Batch 4263, Loss 0.4976870119571686\n","[Training Epoch 0] Batch 4264, Loss 0.477649986743927\n","[Training Epoch 0] Batch 4265, Loss 0.5031890273094177\n","[Training Epoch 0] Batch 4266, Loss 0.5198044776916504\n","[Training Epoch 0] Batch 4267, Loss 0.5127298831939697\n","[Training Epoch 0] Batch 4268, Loss 0.49273884296417236\n","[Training Epoch 0] Batch 4269, Loss 0.48767319321632385\n","[Training Epoch 0] Batch 4270, Loss 0.49679386615753174\n","[Training Epoch 0] Batch 4271, Loss 0.5113219022750854\n","[Training Epoch 0] Batch 4272, Loss 0.49409013986587524\n","[Training Epoch 0] Batch 4273, Loss 0.4980577230453491\n","[Training Epoch 0] Batch 4274, Loss 0.5089752078056335\n","[Training Epoch 0] Batch 4275, Loss 0.4926322102546692\n","[Training Epoch 0] Batch 4276, Loss 0.4780959486961365\n","[Training Epoch 0] Batch 4277, Loss 0.48018741607666016\n","[Training Epoch 0] Batch 4278, Loss 0.5047189593315125\n","[Training Epoch 0] Batch 4279, Loss 0.4790365397930145\n","[Training Epoch 0] Batch 4280, Loss 0.5046818852424622\n","[Training Epoch 0] Batch 4281, Loss 0.5099881291389465\n","[Training Epoch 0] Batch 4282, Loss 0.46007412672042847\n","[Training Epoch 0] Batch 4283, Loss 0.5072566866874695\n","[Training Epoch 0] Batch 4284, Loss 0.4820196032524109\n","[Training Epoch 0] Batch 4285, Loss 0.5221415758132935\n","[Training Epoch 0] Batch 4286, Loss 0.4964931905269623\n","[Training Epoch 0] Batch 4287, Loss 0.511691153049469\n","[Training Epoch 0] Batch 4288, Loss 0.4940681755542755\n","[Training Epoch 0] Batch 4289, Loss 0.4888249933719635\n","[Training Epoch 0] Batch 4290, Loss 0.4899448752403259\n","[Training Epoch 0] Batch 4291, Loss 0.4914861023426056\n","[Training Epoch 0] Batch 4292, Loss 0.4952179193496704\n","[Training Epoch 0] Batch 4293, Loss 0.49643540382385254\n","[Training Epoch 0] Batch 4294, Loss 0.5131831169128418\n","[Training Epoch 0] Batch 4295, Loss 0.5089874267578125\n","[Training Epoch 0] Batch 4296, Loss 0.48596540093421936\n","[Training Epoch 0] Batch 4297, Loss 0.5495277643203735\n","[Training Epoch 0] Batch 4298, Loss 0.4965421259403229\n","[Training Epoch 0] Batch 4299, Loss 0.48602598905563354\n","[Training Epoch 0] Batch 4300, Loss 0.46424344182014465\n","[Training Epoch 0] Batch 4301, Loss 0.4887170195579529\n","[Training Epoch 0] Batch 4302, Loss 0.49412375688552856\n","[Training Epoch 0] Batch 4303, Loss 0.481764018535614\n","[Training Epoch 0] Batch 4304, Loss 0.5184062719345093\n","[Training Epoch 0] Batch 4305, Loss 0.4790804982185364\n","[Training Epoch 0] Batch 4306, Loss 0.4765096604824066\n","[Training Epoch 0] Batch 4307, Loss 0.4993048310279846\n","[Training Epoch 0] Batch 4308, Loss 0.4843272566795349\n","[Training Epoch 0] Batch 4309, Loss 0.5089547634124756\n","[Training Epoch 0] Batch 4310, Loss 0.4964950978755951\n","[Training Epoch 0] Batch 4311, Loss 0.5114865303039551\n","[Training Epoch 0] Batch 4312, Loss 0.5047567486763\n","[Training Epoch 0] Batch 4313, Loss 0.481794148683548\n","[Training Epoch 0] Batch 4314, Loss 0.47086581587791443\n","[Training Epoch 0] Batch 4315, Loss 0.5102649331092834\n","[Training Epoch 0] Batch 4316, Loss 0.5198098421096802\n","[Training Epoch 0] Batch 4317, Loss 0.5265088677406311\n","[Training Epoch 0] Batch 4318, Loss 0.5033876895904541\n","[Training Epoch 0] Batch 4319, Loss 0.5101221799850464\n","[Training Epoch 0] Batch 4320, Loss 0.5101985335350037\n","[Training Epoch 0] Batch 4321, Loss 0.4802899956703186\n","[Training Epoch 0] Batch 4322, Loss 0.5020318031311035\n","[Training Epoch 0] Batch 4323, Loss 0.5047301650047302\n","[Training Epoch 0] Batch 4324, Loss 0.5168706178665161\n","[Training Epoch 0] Batch 4325, Loss 0.5073990821838379\n","[Training Epoch 0] Batch 4326, Loss 0.5129513144493103\n","[Training Epoch 0] Batch 4327, Loss 0.5277191996574402\n","[Training Epoch 0] Batch 4328, Loss 0.5155655145645142\n","[Training Epoch 0] Batch 4329, Loss 0.47616657614707947\n","[Training Epoch 0] Batch 4330, Loss 0.4831339120864868\n","[Training Epoch 0] Batch 4331, Loss 0.49536025524139404\n","[Training Epoch 0] Batch 4332, Loss 0.5170339345932007\n","[Training Epoch 0] Batch 4333, Loss 0.4991994798183441\n","[Training Epoch 0] Batch 4334, Loss 0.5059641599655151\n","[Training Epoch 0] Batch 4335, Loss 0.5047621726989746\n","[Training Epoch 0] Batch 4336, Loss 0.4993971586227417\n","[Training Epoch 0] Batch 4337, Loss 0.4763523042201996\n","[Training Epoch 0] Batch 4338, Loss 0.5073792934417725\n","[Training Epoch 0] Batch 4339, Loss 0.48997193574905396\n","[Training Epoch 0] Batch 4340, Loss 0.47104859352111816\n","[Training Epoch 0] Batch 4341, Loss 0.49814873933792114\n","[Training Epoch 0] Batch 4342, Loss 0.5074954032897949\n","[Training Epoch 0] Batch 4343, Loss 0.4709244966506958\n","[Training Epoch 0] Batch 4344, Loss 0.522245466709137\n","[Training Epoch 0] Batch 4345, Loss 0.5114601850509644\n","[Training Epoch 0] Batch 4346, Loss 0.5222322344779968\n","[Training Epoch 0] Batch 4347, Loss 0.515465259552002\n","[Training Epoch 0] Batch 4348, Loss 0.5061256885528564\n","[Training Epoch 0] Batch 4349, Loss 0.48455387353897095\n","[Training Epoch 0] Batch 4350, Loss 0.4990527033805847\n","[Training Epoch 0] Batch 4351, Loss 0.49930471181869507\n","[Training Epoch 0] Batch 4352, Loss 0.48731550574302673\n","[Training Epoch 0] Batch 4353, Loss 0.4816315770149231\n","[Training Epoch 0] Batch 4354, Loss 0.5075616240501404\n","[Training Epoch 0] Batch 4355, Loss 0.49415647983551025\n","[Training Epoch 0] Batch 4356, Loss 0.4778032600879669\n","[Training Epoch 0] Batch 4357, Loss 0.5073749423027039\n","[Training Epoch 0] Batch 4358, Loss 0.5073162913322449\n","[Training Epoch 0] Batch 4359, Loss 0.46530210971832275\n","[Training Epoch 0] Batch 4360, Loss 0.5058743357658386\n","[Training Epoch 0] Batch 4361, Loss 0.49103984236717224\n","[Training Epoch 0] Batch 4362, Loss 0.5087350606918335\n","[Training Epoch 0] Batch 4363, Loss 0.49401935935020447\n","[Training Epoch 0] Batch 4364, Loss 0.5223332047462463\n","[Training Epoch 0] Batch 4365, Loss 0.5007224082946777\n","[Training Epoch 0] Batch 4366, Loss 0.4789623022079468\n","[Training Epoch 0] Batch 4367, Loss 0.5210410356521606\n","[Training Epoch 0] Batch 4368, Loss 0.47370368242263794\n","[Training Epoch 0] Batch 4369, Loss 0.5036861896514893\n","[Training Epoch 0] Batch 4370, Loss 0.500836193561554\n","[Training Epoch 0] Batch 4371, Loss 0.5115751028060913\n","[Training Epoch 0] Batch 4372, Loss 0.5049517154693604\n","[Training Epoch 0] Batch 4373, Loss 0.5115158557891846\n","[Training Epoch 0] Batch 4374, Loss 0.5032649040222168\n","[Training Epoch 0] Batch 4375, Loss 0.5130121111869812\n","[Training Epoch 0] Batch 4376, Loss 0.4830240309238434\n","[Training Epoch 0] Batch 4377, Loss 0.49527156352996826\n","[Training Epoch 0] Batch 4378, Loss 0.4804319143295288\n","[Training Epoch 0] Batch 4379, Loss 0.47898879647254944\n","[Training Epoch 0] Batch 4380, Loss 0.4939631223678589\n","[Training Epoch 0] Batch 4381, Loss 0.48178794980049133\n","[Training Epoch 0] Batch 4382, Loss 0.49401500821113586\n","[Training Epoch 0] Batch 4383, Loss 0.4747989773750305\n","[Training Epoch 0] Batch 4384, Loss 0.4749496579170227\n","[Training Epoch 0] Batch 4385, Loss 0.517125129699707\n","[Training Epoch 0] Batch 4386, Loss 0.4588889479637146\n","[Training Epoch 0] Batch 4387, Loss 0.5087474584579468\n","[Training Epoch 0] Batch 4388, Loss 0.4777222275733948\n","[Training Epoch 0] Batch 4389, Loss 0.4898422956466675\n","[Training Epoch 0] Batch 4390, Loss 0.4967089593410492\n","[Training Epoch 0] Batch 4391, Loss 0.508967399597168\n","[Training Epoch 0] Batch 4392, Loss 0.5020996332168579\n","[Training Epoch 0] Batch 4393, Loss 0.5154575109481812\n","[Training Epoch 0] Batch 4394, Loss 0.4736936688423157\n","[Training Epoch 0] Batch 4395, Loss 0.4624456763267517\n","[Training Epoch 0] Batch 4396, Loss 0.5303457379341125\n","[Training Epoch 0] Batch 4397, Loss 0.501835286617279\n","[Training Epoch 0] Batch 4398, Loss 0.4953051805496216\n","[Training Epoch 0] Batch 4399, Loss 0.48447805643081665\n","[Training Epoch 0] Batch 4400, Loss 0.493710458278656\n","[Training Epoch 0] Batch 4401, Loss 0.5061707496643066\n","[Training Epoch 0] Batch 4402, Loss 0.5182445049285889\n","[Training Epoch 0] Batch 4403, Loss 0.5078529119491577\n","[Training Epoch 0] Batch 4404, Loss 0.5293381214141846\n","[Training Epoch 0] Batch 4405, Loss 0.5169637203216553\n","[Training Epoch 0] Batch 4406, Loss 0.5046549439430237\n","[Training Epoch 0] Batch 4407, Loss 0.4816339612007141\n","[Training Epoch 0] Batch 4408, Loss 0.4870767891407013\n","[Training Epoch 0] Batch 4409, Loss 0.5142511129379272\n","[Training Epoch 0] Batch 4410, Loss 0.47355085611343384\n","[Training Epoch 0] Batch 4411, Loss 0.4844914972782135\n","[Training Epoch 0] Batch 4412, Loss 0.4979330599308014\n","[Training Epoch 0] Batch 4413, Loss 0.484632283449173\n","[Training Epoch 0] Batch 4414, Loss 0.5074793100357056\n","[Training Epoch 0] Batch 4415, Loss 0.48027926683425903\n","[Training Epoch 0] Batch 4416, Loss 0.4814505875110626\n","[Training Epoch 0] Batch 4417, Loss 0.4586643576622009\n","[Training Epoch 0] Batch 4418, Loss 0.49822255969047546\n","[Training Epoch 0] Batch 4419, Loss 0.4994123578071594\n","[Training Epoch 0] Batch 4420, Loss 0.4938318431377411\n","[Training Epoch 0] Batch 4421, Loss 0.4898573160171509\n","[Training Epoch 0] Batch 4422, Loss 0.49287766218185425\n","[Training Epoch 0] Batch 4423, Loss 0.4954470992088318\n","[Training Epoch 0] Batch 4424, Loss 0.4802630543708801\n","[Training Epoch 0] Batch 4425, Loss 0.5251755118370056\n","[Training Epoch 0] Batch 4426, Loss 0.560344934463501\n","[Training Epoch 0] Batch 4427, Loss 0.47787338495254517\n","[Training Epoch 0] Batch 4428, Loss 0.5035139918327332\n","[Training Epoch 0] Batch 4429, Loss 0.5073316097259521\n","[Training Epoch 0] Batch 4430, Loss 0.5089023113250732\n","[Training Epoch 0] Batch 4431, Loss 0.48719364404678345\n","[Training Epoch 0] Batch 4432, Loss 0.489971786737442\n","[Training Epoch 0] Batch 4433, Loss 0.4910059869289398\n","[Training Epoch 0] Batch 4434, Loss 0.4952859878540039\n","[Training Epoch 0] Batch 4435, Loss 0.5100893974304199\n","[Training Epoch 0] Batch 4436, Loss 0.5075322389602661\n","[Training Epoch 0] Batch 4437, Loss 0.4938048720359802\n","[Training Epoch 0] Batch 4438, Loss 0.4775022864341736\n","[Training Epoch 0] Batch 4439, Loss 0.49808990955352783\n","[Training Epoch 0] Batch 4440, Loss 0.4967937469482422\n","[Training Epoch 0] Batch 4441, Loss 0.5200049877166748\n","[Training Epoch 0] Batch 4442, Loss 0.5390222072601318\n","[Training Epoch 0] Batch 4443, Loss 0.49801209568977356\n","[Training Epoch 0] Batch 4444, Loss 0.4899047017097473\n","[Training Epoch 0] Batch 4445, Loss 0.5020718574523926\n","[Training Epoch 0] Batch 4446, Loss 0.5099703669548035\n","[Training Epoch 0] Batch 4447, Loss 0.5250912308692932\n","[Training Epoch 0] Batch 4448, Loss 0.5129574537277222\n","[Training Epoch 0] Batch 4449, Loss 0.48579931259155273\n","[Training Epoch 0] Batch 4450, Loss 0.526695191860199\n","[Training Epoch 0] Batch 4451, Loss 0.516890287399292\n","[Training Epoch 0] Batch 4452, Loss 0.47771379351615906\n","[Training Epoch 0] Batch 4453, Loss 0.4734373688697815\n","[Training Epoch 0] Batch 4454, Loss 0.48856550455093384\n","[Training Epoch 0] Batch 4455, Loss 0.4802230894565582\n","[Training Epoch 0] Batch 4456, Loss 0.5142500996589661\n","[Training Epoch 0] Batch 4457, Loss 0.4992430508136749\n","[Training Epoch 0] Batch 4458, Loss 0.5182419419288635\n","[Training Epoch 0] Batch 4459, Loss 0.5049303770065308\n","[Training Epoch 0] Batch 4460, Loss 0.49106740951538086\n","[Training Epoch 0] Batch 4461, Loss 0.5330532789230347\n","[Training Epoch 0] Batch 4462, Loss 0.4938678741455078\n","[Training Epoch 0] Batch 4463, Loss 0.5235254764556885\n","[Training Epoch 0] Batch 4464, Loss 0.5113213062286377\n","[Training Epoch 0] Batch 4465, Loss 0.5086805820465088\n","[Training Epoch 0] Batch 4466, Loss 0.48164576292037964\n","[Training Epoch 0] Batch 4467, Loss 0.48862212896347046\n","[Training Epoch 0] Batch 4468, Loss 0.4694865942001343\n","[Training Epoch 0] Batch 4469, Loss 0.49498966336250305\n","[Training Epoch 0] Batch 4470, Loss 0.49676668643951416\n","[Training Epoch 0] Batch 4471, Loss 0.4899168014526367\n","[Training Epoch 0] Batch 4472, Loss 0.5307285785675049\n","[Training Epoch 0] Batch 4473, Loss 0.49129539728164673\n","[Training Epoch 0] Batch 4474, Loss 0.4941422939300537\n","[Training Epoch 0] Batch 4475, Loss 0.4911975562572479\n","[Training Epoch 0] Batch 4476, Loss 0.5321297645568848\n","[Training Epoch 0] Batch 4477, Loss 0.48733556270599365\n","[Training Epoch 0] Batch 4478, Loss 0.48841339349746704\n","[Training Epoch 0] Batch 4479, Loss 0.490979939699173\n","[Training Epoch 0] Batch 4480, Loss 0.5152690410614014\n","[Training Epoch 0] Batch 4481, Loss 0.48994767665863037\n","[Training Epoch 0] Batch 4482, Loss 0.47764909267425537\n","[Training Epoch 0] Batch 4483, Loss 0.5236895084381104\n","[Training Epoch 0] Batch 4484, Loss 0.5171364545822144\n","[Training Epoch 0] Batch 4485, Loss 0.5276586413383484\n","[Training Epoch 0] Batch 4486, Loss 0.5070653557777405\n","[Training Epoch 0] Batch 4487, Loss 0.5131120681762695\n","[Training Epoch 0] Batch 4488, Loss 0.47771283984184265\n","[Training Epoch 0] Batch 4489, Loss 0.514471173286438\n","[Training Epoch 0] Batch 4490, Loss 0.46670636534690857\n","[Training Epoch 0] Batch 4491, Loss 0.46674275398254395\n","[Training Epoch 0] Batch 4492, Loss 0.4761630892753601\n","[Training Epoch 0] Batch 4493, Loss 0.4665071964263916\n","[Training Epoch 0] Batch 4494, Loss 0.5075731873512268\n","[Training Epoch 0] Batch 4495, Loss 0.5237705707550049\n","[Training Epoch 0] Batch 4496, Loss 0.5141195058822632\n","[Training Epoch 0] Batch 4497, Loss 0.525983989238739\n","[Training Epoch 0] Batch 4498, Loss 0.4778617024421692\n","[Training Epoch 0] Batch 4499, Loss 0.48176729679107666\n","[Training Epoch 0] Batch 4500, Loss 0.5006376504898071\n","[Training Epoch 0] Batch 4501, Loss 0.4941633641719818\n","[Training Epoch 0] Batch 4502, Loss 0.5455266237258911\n","[Training Epoch 0] Batch 4503, Loss 0.46818941831588745\n","[Training Epoch 0] Batch 4504, Loss 0.5075977444648743\n","[Training Epoch 0] Batch 4505, Loss 0.5247854590415955\n","[Training Epoch 0] Batch 4506, Loss 0.5089825391769409\n","[Training Epoch 0] Batch 4507, Loss 0.47102829813957214\n","[Training Epoch 0] Batch 4508, Loss 0.5116838216781616\n","[Training Epoch 0] Batch 4509, Loss 0.489785373210907\n","[Training Epoch 0] Batch 4510, Loss 0.4993920922279358\n","[Training Epoch 0] Batch 4511, Loss 0.5183623433113098\n","[Training Epoch 0] Batch 4512, Loss 0.49231183528900146\n","[Training Epoch 0] Batch 4513, Loss 0.48170799016952515\n","[Training Epoch 0] Batch 4514, Loss 0.498965322971344\n","[Training Epoch 0] Batch 4515, Loss 0.49241071939468384\n","[Training Epoch 0] Batch 4516, Loss 0.45993027091026306\n","[Training Epoch 0] Batch 4517, Loss 0.4776766002178192\n","[Training Epoch 0] Batch 4518, Loss 0.49638891220092773\n","[Training Epoch 0] Batch 4519, Loss 0.49402928352355957\n","[Training Epoch 0] Batch 4520, Loss 0.49411463737487793\n","[Training Epoch 0] Batch 4521, Loss 0.4912481904029846\n","[Training Epoch 0] Batch 4522, Loss 0.5038949251174927\n","[Training Epoch 0] Batch 4523, Loss 0.5293079018592834\n","[Training Epoch 0] Batch 4524, Loss 0.501867413520813\n","[Training Epoch 0] Batch 4525, Loss 0.5072288513183594\n","[Training Epoch 0] Batch 4526, Loss 0.4927904009819031\n","[Training Epoch 0] Batch 4527, Loss 0.5034837126731873\n","[Training Epoch 0] Batch 4528, Loss 0.5076327919960022\n","[Training Epoch 0] Batch 4529, Loss 0.4872283935546875\n","[Training Epoch 0] Batch 4530, Loss 0.5036811828613281\n","[Training Epoch 0] Batch 4531, Loss 0.4863280653953552\n","[Training Epoch 0] Batch 4532, Loss 0.4979015588760376\n","[Training Epoch 0] Batch 4533, Loss 0.485623836517334\n","[Training Epoch 0] Batch 4534, Loss 0.5139923095703125\n","[Training Epoch 0] Batch 4535, Loss 0.49798351526260376\n","[Training Epoch 0] Batch 4536, Loss 0.4967116713523865\n","[Training Epoch 0] Batch 4537, Loss 0.5424162149429321\n","[Training Epoch 0] Batch 4538, Loss 0.4816220998764038\n","[Training Epoch 0] Batch 4539, Loss 0.537214994430542\n","[Training Epoch 0] Batch 4540, Loss 0.4813968539237976\n","[Training Epoch 0] Batch 4541, Loss 0.4463872015476227\n","[Training Epoch 0] Batch 4542, Loss 0.5042794942855835\n","[Training Epoch 0] Batch 4543, Loss 0.49388831853866577\n","[Training Epoch 0] Batch 4544, Loss 0.4776017367839813\n","[Training Epoch 0] Batch 4545, Loss 0.5102975368499756\n","[Training Epoch 0] Batch 4546, Loss 0.5194272994995117\n","[Training Epoch 0] Batch 4547, Loss 0.4757646918296814\n","[Training Epoch 0] Batch 4548, Loss 0.5392139554023743\n","[Training Epoch 0] Batch 4549, Loss 0.5161447525024414\n","[Training Epoch 0] Batch 4550, Loss 0.4940474033355713\n","[Training Epoch 0] Batch 4551, Loss 0.5171135663986206\n","[Training Epoch 0] Batch 4552, Loss 0.49948593974113464\n","[Training Epoch 0] Batch 4553, Loss 0.48267412185668945\n","[Training Epoch 0] Batch 4554, Loss 0.5074173212051392\n","[Training Epoch 0] Batch 4555, Loss 0.4845120310783386\n","[Training Epoch 0] Batch 4556, Loss 0.49667465686798096\n","[Training Epoch 0] Batch 4557, Loss 0.5170709490776062\n","[Training Epoch 0] Batch 4558, Loss 0.51813805103302\n","[Training Epoch 0] Batch 4559, Loss 0.48281538486480713\n","[Training Epoch 0] Batch 4560, Loss 0.4885551929473877\n","[Training Epoch 0] Batch 4561, Loss 0.4749830365180969\n","[Training Epoch 0] Batch 4562, Loss 0.5165538787841797\n","[Training Epoch 0] Batch 4563, Loss 0.48856258392333984\n","[Training Epoch 0] Batch 4564, Loss 0.4912726879119873\n","[Training Epoch 0] Batch 4565, Loss 0.5172265768051147\n","[Training Epoch 0] Batch 4566, Loss 0.4805021286010742\n","[Training Epoch 0] Batch 4567, Loss 0.48480457067489624\n","[Training Epoch 0] Batch 4568, Loss 0.48469191789627075\n","[Training Epoch 0] Batch 4569, Loss 0.5188950300216675\n","[Training Epoch 0] Batch 4570, Loss 0.5003653764724731\n","[Training Epoch 0] Batch 4571, Loss 0.4955063462257385\n","[Training Epoch 0] Batch 4572, Loss 0.5171394348144531\n","[Training Epoch 0] Batch 4573, Loss 0.4950438141822815\n","[Training Epoch 0] Batch 4574, Loss 0.5035613775253296\n","[Training Epoch 0] Batch 4575, Loss 0.5085592865943909\n","[Training Epoch 0] Batch 4576, Loss 0.4943770170211792\n","[Training Epoch 0] Batch 4577, Loss 0.4957226514816284\n","[Training Epoch 0] Batch 4578, Loss 0.5305884480476379\n","[Training Epoch 0] Batch 4579, Loss 0.5253999829292297\n","[Training Epoch 0] Batch 4580, Loss 0.505989670753479\n","[Training Epoch 0] Batch 4581, Loss 0.47371959686279297\n","[Training Epoch 0] Batch 4582, Loss 0.5159525275230408\n","[Training Epoch 0] Batch 4583, Loss 0.5253630876541138\n","[Training Epoch 0] Batch 4584, Loss 0.5096397995948792\n","[Training Epoch 0] Batch 4585, Loss 0.5264518857002258\n","[Training Epoch 0] Batch 4586, Loss 0.4967074394226074\n","[Training Epoch 0] Batch 4587, Loss 0.46655070781707764\n","[Training Epoch 0] Batch 4588, Loss 0.48562225699424744\n","[Training Epoch 0] Batch 4589, Loss 0.5017310976982117\n","[Training Epoch 0] Batch 4590, Loss 0.5035765171051025\n","[Training Epoch 0] Batch 4591, Loss 0.5267829895019531\n","[Training Epoch 0] Batch 4592, Loss 0.5293808579444885\n","[Training Epoch 0] Batch 4593, Loss 0.522322416305542\n","[Training Epoch 0] Batch 4594, Loss 0.47745782136917114\n","[Training Epoch 0] Batch 4595, Loss 0.5005553364753723\n","[Training Epoch 0] Batch 4596, Loss 0.47791871428489685\n","[Training Epoch 0] Batch 4597, Loss 0.47355276346206665\n","[Training Epoch 0] Batch 4598, Loss 0.5154623985290527\n","[Training Epoch 0] Batch 4599, Loss 0.46274495124816895\n","[Training Epoch 0] Batch 4600, Loss 0.5141797065734863\n","[Training Epoch 0] Batch 4601, Loss 0.48019546270370483\n","[Training Epoch 0] Batch 4602, Loss 0.5074507594108582\n","[Training Epoch 0] Batch 4603, Loss 0.5389741659164429\n","[Training Epoch 0] Batch 4604, Loss 0.5006951093673706\n","[Training Epoch 0] Batch 4605, Loss 0.5224800109863281\n","[Training Epoch 0] Batch 4606, Loss 0.49378880858421326\n","[Training Epoch 0] Batch 4607, Loss 0.5035684108734131\n","[Training Epoch 0] Batch 4608, Loss 0.49788135290145874\n","[Training Epoch 0] Batch 4609, Loss 0.5018452405929565\n","[Training Epoch 0] Batch 4610, Loss 0.5212864875793457\n","[Training Epoch 0] Batch 4611, Loss 0.5195451974868774\n","[Training Epoch 0] Batch 4612, Loss 0.5007609724998474\n","[Training Epoch 0] Batch 4613, Loss 0.4909142255783081\n","[Training Epoch 0] Batch 4614, Loss 0.487226665019989\n","[Training Epoch 0] Batch 4615, Loss 0.5033119916915894\n","[Training Epoch 0] Batch 4616, Loss 0.49533578753471375\n","[Training Epoch 0] Batch 4617, Loss 0.5118568539619446\n","[Training Epoch 0] Batch 4618, Loss 0.472088098526001\n","[Training Epoch 0] Batch 4619, Loss 0.5131280422210693\n","[Training Epoch 0] Batch 4620, Loss 0.4873983860015869\n","[Training Epoch 0] Batch 4621, Loss 0.4695776700973511\n","[Training Epoch 0] Batch 4622, Loss 0.5061874985694885\n","[Training Epoch 0] Batch 4623, Loss 0.49274539947509766\n","[Training Epoch 0] Batch 4624, Loss 0.4967607855796814\n","[Training Epoch 0] Batch 4625, Loss 0.5273998975753784\n","[Training Epoch 0] Batch 4626, Loss 0.4856397211551666\n","[Training Epoch 0] Batch 4627, Loss 0.5088220834732056\n","[Training Epoch 0] Batch 4628, Loss 0.5005953907966614\n","[Training Epoch 0] Batch 4629, Loss 0.5117366313934326\n","[Training Epoch 0] Batch 4630, Loss 0.5046994090080261\n","[Training Epoch 0] Batch 4631, Loss 0.47890645265579224\n","[Training Epoch 0] Batch 4632, Loss 0.5017625093460083\n","[Training Epoch 0] Batch 4633, Loss 0.5225749015808105\n","[Training Epoch 0] Batch 4634, Loss 0.4774409234523773\n","[Training Epoch 0] Batch 4635, Loss 0.49938201904296875\n","[Training Epoch 0] Batch 4636, Loss 0.4964278042316437\n","[Training Epoch 0] Batch 4637, Loss 0.5116226673126221\n","[Training Epoch 0] Batch 4638, Loss 0.4670349359512329\n","[Training Epoch 0] Batch 4639, Loss 0.4993646442890167\n","[Training Epoch 0] Batch 4640, Loss 0.4803510904312134\n","[Training Epoch 0] Batch 4641, Loss 0.5008154511451721\n","[Training Epoch 0] Batch 4642, Loss 0.5289655923843384\n","[Training Epoch 0] Batch 4643, Loss 0.5539405345916748\n","[Training Epoch 0] Batch 4644, Loss 0.5059095621109009\n","[Training Epoch 0] Batch 4645, Loss 0.5154167413711548\n","[Training Epoch 0] Batch 4646, Loss 0.5091691613197327\n","[Training Epoch 0] Batch 4647, Loss 0.5032883882522583\n","[Training Epoch 0] Batch 4648, Loss 0.46671369671821594\n","[Training Epoch 0] Batch 4649, Loss 0.46244609355926514\n","[Training Epoch 0] Batch 4650, Loss 0.49382293224334717\n","[Training Epoch 0] Batch 4651, Loss 0.5158061385154724\n","[Training Epoch 0] Batch 4652, Loss 0.5060690641403198\n","[Training Epoch 0] Batch 4653, Loss 0.5139323472976685\n","[Training Epoch 0] Batch 4654, Loss 0.5155609846115112\n","[Training Epoch 0] Batch 4655, Loss 0.49561965465545654\n","[Training Epoch 0] Batch 4656, Loss 0.5318909287452698\n","[Training Epoch 0] Batch 4657, Loss 0.5032353401184082\n","[Training Epoch 0] Batch 4658, Loss 0.5047876834869385\n","[Training Epoch 0] Batch 4659, Loss 0.48516303300857544\n","[Training Epoch 0] Batch 4660, Loss 0.49770814180374146\n","[Training Epoch 0] Batch 4661, Loss 0.47495418787002563\n","[Training Epoch 0] Batch 4662, Loss 0.523550271987915\n","[Training Epoch 0] Batch 4663, Loss 0.4996122121810913\n","[Training Epoch 0] Batch 4664, Loss 0.5319186449050903\n","[Training Epoch 0] Batch 4665, Loss 0.5237175226211548\n","[Training Epoch 0] Batch 4666, Loss 0.5101848840713501\n","[Training Epoch 0] Batch 4667, Loss 0.5441560745239258\n","[Training Epoch 0] Batch 4668, Loss 0.5065925121307373\n","[Training Epoch 0] Batch 4669, Loss 0.5017869472503662\n","[Training Epoch 0] Batch 4670, Loss 0.49941718578338623\n","[Training Epoch 0] Batch 4671, Loss 0.5074636936187744\n","[Training Epoch 0] Batch 4672, Loss 0.5034364461898804\n","[Training Epoch 0] Batch 4673, Loss 0.5334212183952332\n","[Training Epoch 0] Batch 4674, Loss 0.4981275498867035\n","[Training Epoch 0] Batch 4675, Loss 0.54425048828125\n","[Training Epoch 0] Batch 4676, Loss 0.5268047451972961\n","[Training Epoch 0] Batch 4677, Loss 0.49516957998275757\n","[Training Epoch 0] Batch 4678, Loss 0.5262402296066284\n","[Training Epoch 0] Batch 4679, Loss 0.4925351142883301\n","[Training Epoch 0] Batch 4680, Loss 0.4763195812702179\n","[Training Epoch 0] Batch 4681, Loss 0.5009985566139221\n","[Training Epoch 0] Batch 4682, Loss 0.5009421110153198\n","[Training Epoch 0] Batch 4683, Loss 0.5208271741867065\n","[Training Epoch 0] Batch 4684, Loss 0.5100265741348267\n","[Training Epoch 0] Batch 4685, Loss 0.5060654878616333\n","[Training Epoch 0] Batch 4686, Loss 0.5035611391067505\n","[Training Epoch 0] Batch 4687, Loss 0.4761732220649719\n","[Training Epoch 0] Batch 4688, Loss 0.5041936039924622\n","[Training Epoch 0] Batch 4689, Loss 0.5010647773742676\n","[Training Epoch 0] Batch 4690, Loss 0.4776647090911865\n","[Training Epoch 0] Batch 4691, Loss 0.5149964094161987\n","[Training Epoch 0] Batch 4692, Loss 0.5140253305435181\n","[Training Epoch 0] Batch 4693, Loss 0.4802108108997345\n","[Training Epoch 0] Batch 4694, Loss 0.48317626118659973\n","[Training Epoch 0] Batch 4695, Loss 0.5031492114067078\n","[Training Epoch 0] Batch 4696, Loss 0.5278021693229675\n","[Training Epoch 0] Batch 4697, Loss 0.49002504348754883\n","[Training Epoch 0] Batch 4698, Loss 0.49248480796813965\n","[Training Epoch 0] Batch 4699, Loss 0.5158853530883789\n","[Training Epoch 0] Batch 4700, Loss 0.49678897857666016\n","[Training Epoch 0] Batch 4701, Loss 0.5165646076202393\n","[Training Epoch 0] Batch 4702, Loss 0.5035593509674072\n","[Training Epoch 0] Batch 4703, Loss 0.46982836723327637\n","[Training Epoch 0] Batch 4704, Loss 0.49120181798934937\n","[Training Epoch 0] Batch 4705, Loss 0.49555718898773193\n","[Training Epoch 0] Batch 4706, Loss 0.5022082328796387\n","[Training Epoch 0] Batch 4707, Loss 0.4817596971988678\n","[Training Epoch 0] Batch 4708, Loss 0.47487959265708923\n","[Training Epoch 0] Batch 4709, Loss 0.473818838596344\n","[Training Epoch 0] Batch 4710, Loss 0.5086407661437988\n","[Training Epoch 0] Batch 4711, Loss 0.4978153109550476\n","[Training Epoch 0] Batch 4712, Loss 0.5235844850540161\n","[Training Epoch 0] Batch 4713, Loss 0.4761757254600525\n","[Training Epoch 0] Batch 4714, Loss 0.47131627798080444\n","[Training Epoch 0] Batch 4715, Loss 0.5125874280929565\n","[Training Epoch 0] Batch 4716, Loss 0.499245285987854\n","[Training Epoch 0] Batch 4717, Loss 0.4894963502883911\n","[Training Epoch 0] Batch 4718, Loss 0.490950345993042\n","[Training Epoch 0] Batch 4719, Loss 0.49832627177238464\n","[Training Epoch 0] Batch 4720, Loss 0.4843207597732544\n","[Training Epoch 0] Batch 4721, Loss 0.5050427913665771\n","[Training Epoch 0] Batch 4722, Loss 0.52400141954422\n","[Training Epoch 0] Batch 4723, Loss 0.5080671906471252\n","[Training Epoch 0] Batch 4724, Loss 0.5238964557647705\n","[Training Epoch 0] Batch 4725, Loss 0.48422887921333313\n","[Training Epoch 0] Batch 4726, Loss 0.49936288595199585\n","[Training Epoch 0] Batch 4727, Loss 0.5032183527946472\n","[Training Epoch 0] Batch 4728, Loss 0.5033155679702759\n","[Training Epoch 0] Batch 4729, Loss 0.5038655996322632\n","[Training Epoch 0] Batch 4730, Loss 0.5037190318107605\n","[Training Epoch 0] Batch 4731, Loss 0.49276888370513916\n","[Training Epoch 0] Batch 4732, Loss 0.5022926330566406\n","[Training Epoch 0] Batch 4733, Loss 0.5159328579902649\n","[Training Epoch 0] Batch 4734, Loss 0.4653732180595398\n","[Training Epoch 0] Batch 4735, Loss 0.4700765609741211\n","[Training Epoch 0] Batch 4736, Loss 0.46398866176605225\n","[Training Epoch 0] Batch 4737, Loss 0.4979304075241089\n","[Training Epoch 0] Batch 4738, Loss 0.5114470720291138\n","[Training Epoch 0] Batch 4739, Loss 0.5131639242172241\n","[Training Epoch 0] Batch 4740, Loss 0.5169676542282104\n","[Training Epoch 0] Batch 4741, Loss 0.4814220368862152\n","[Training Epoch 0] Batch 4742, Loss 0.4846116900444031\n","[Training Epoch 0] Batch 4743, Loss 0.4736824035644531\n","[Training Epoch 0] Batch 4744, Loss 0.4869023561477661\n","[Training Epoch 0] Batch 4745, Loss 0.500613808631897\n","[Training Epoch 0] Batch 4746, Loss 0.5049553513526917\n","[Training Epoch 0] Batch 4747, Loss 0.508928656578064\n","[Training Epoch 0] Batch 4748, Loss 0.5144745111465454\n","[Training Epoch 0] Batch 4749, Loss 0.47505301237106323\n","[Training Epoch 0] Batch 4750, Loss 0.49654465913772583\n","[Training Epoch 0] Batch 4751, Loss 0.5223692655563354\n","[Training Epoch 0] Batch 4752, Loss 0.4913460314273834\n","[Training Epoch 0] Batch 4753, Loss 0.4681720733642578\n","[Training Epoch 0] Batch 4754, Loss 0.5181605815887451\n","[Training Epoch 0] Batch 4755, Loss 0.5223128795623779\n","[Training Epoch 0] Batch 4756, Loss 0.5061261057853699\n","[Training Epoch 0] Batch 4757, Loss 0.5049134492874146\n","[Training Epoch 0] Batch 4758, Loss 0.5158061981201172\n","[Training Epoch 0] Batch 4759, Loss 0.5181992650032043\n","[Training Epoch 0] Batch 4760, Loss 0.5046483278274536\n","[Training Epoch 0] Batch 4761, Loss 0.4992845952510834\n","[Training Epoch 0] Batch 4762, Loss 0.5008548498153687\n","[Training Epoch 0] Batch 4763, Loss 0.49684083461761475\n","[Training Epoch 0] Batch 4764, Loss 0.5102002024650574\n","[Training Epoch 0] Batch 4765, Loss 0.5263784527778625\n","[Training Epoch 0] Batch 4766, Loss 0.549563467502594\n","[Training Epoch 0] Batch 4767, Loss 0.508912205696106\n","[Training Epoch 0] Batch 4768, Loss 0.49517467617988586\n","[Training Epoch 0] Batch 4769, Loss 0.48574191331863403\n","[Training Epoch 0] Batch 4770, Loss 0.49647951126098633\n","[Training Epoch 0] Batch 4771, Loss 0.5100634098052979\n","[Training Epoch 0] Batch 4772, Loss 0.5073403120040894\n","[Training Epoch 0] Batch 4773, Loss 0.5112559199333191\n","[Training Epoch 0] Batch 4774, Loss 0.5060824751853943\n","[Training Epoch 0] Batch 4775, Loss 0.4818110466003418\n","[Training Epoch 0] Batch 4776, Loss 0.5116590261459351\n","[Training Epoch 0] Batch 4777, Loss 0.5115402340888977\n","[Training Epoch 0] Batch 4778, Loss 0.4926418662071228\n","[Training Epoch 0] Batch 4779, Loss 0.478819876909256\n","[Training Epoch 0] Batch 4780, Loss 0.5157465934753418\n","[Training Epoch 0] Batch 4781, Loss 0.48428016901016235\n","[Training Epoch 0] Batch 4782, Loss 0.525168776512146\n","[Training Epoch 0] Batch 4783, Loss 0.48858359456062317\n","[Training Epoch 0] Batch 4784, Loss 0.4900211691856384\n","[Training Epoch 0] Batch 4785, Loss 0.5089294910430908\n","[Training Epoch 0] Batch 4786, Loss 0.50877845287323\n","[Training Epoch 0] Batch 4787, Loss 0.5221739411354065\n","[Training Epoch 0] Batch 4788, Loss 0.500751256942749\n","[Training Epoch 0] Batch 4789, Loss 0.49265918135643005\n","[Training Epoch 0] Batch 4790, Loss 0.4723063111305237\n","[Training Epoch 0] Batch 4791, Loss 0.5397012233734131\n","[Training Epoch 0] Batch 4792, Loss 0.5072047114372253\n","[Training Epoch 0] Batch 4793, Loss 0.5307228565216064\n","[Training Epoch 0] Batch 4794, Loss 0.48844581842422485\n","[Training Epoch 0] Batch 4795, Loss 0.5115328431129456\n","[Training Epoch 0] Batch 4796, Loss 0.5184726119041443\n","[Training Epoch 0] Batch 4797, Loss 0.49803727865219116\n","[Training Epoch 0] Batch 4798, Loss 0.506165087223053\n","[Training Epoch 0] Batch 4799, Loss 0.4775943458080292\n","[Training Epoch 0] Batch 4800, Loss 0.5213116407394409\n","[Training Epoch 0] Batch 4801, Loss 0.49810677766799927\n","[Training Epoch 0] Batch 4802, Loss 0.506117582321167\n","[Training Epoch 0] Batch 4803, Loss 0.49011191725730896\n","[Training Epoch 0] Batch 4804, Loss 0.4951656460762024\n","[Training Epoch 0] Batch 4805, Loss 0.5198330283164978\n","[Training Epoch 0] Batch 4806, Loss 0.5167282223701477\n","[Training Epoch 0] Batch 4807, Loss 0.49526387453079224\n","[Training Epoch 0] Batch 4808, Loss 0.47919055819511414\n","[Training Epoch 0] Batch 4809, Loss 0.5007042288780212\n","[Training Epoch 0] Batch 4810, Loss 0.49515587091445923\n","[Training Epoch 0] Batch 4811, Loss 0.5427334308624268\n","[Training Epoch 0] Batch 4812, Loss 0.5252412557601929\n","[Training Epoch 0] Batch 4813, Loss 0.5316629409790039\n","[Training Epoch 0] Batch 4814, Loss 0.5128352642059326\n","[Training Epoch 0] Batch 4815, Loss 0.5234439969062805\n","[Training Epoch 0] Batch 4816, Loss 0.49129703640937805\n","[Training Epoch 0] Batch 4817, Loss 0.510344386100769\n","[Training Epoch 0] Batch 4818, Loss 0.5019702911376953\n","[Training Epoch 0] Batch 4819, Loss 0.47510725259780884\n","[Training Epoch 0] Batch 4820, Loss 0.5196757912635803\n","[Training Epoch 0] Batch 4821, Loss 0.5103331804275513\n","[Training Epoch 0] Batch 4822, Loss 0.520916223526001\n","[Training Epoch 0] Batch 4823, Loss 0.4816280007362366\n","[Training Epoch 0] Batch 4824, Loss 0.4860560894012451\n","[Training Epoch 0] Batch 4825, Loss 0.4749855697154999\n","[Training Epoch 0] Batch 4826, Loss 0.5022011995315552\n","[Training Epoch 0] Batch 4827, Loss 0.5197774171829224\n","[Training Epoch 0] Batch 4828, Loss 0.49796628952026367\n","[Training Epoch 0] Batch 4829, Loss 0.5169669389724731\n","[Training Epoch 0] Batch 4830, Loss 0.488593190908432\n","[Training Epoch 0] Batch 4831, Loss 0.515464723110199\n","[Training Epoch 0] Batch 4832, Loss 0.4913581311702728\n","[Training Epoch 0] Batch 4833, Loss 0.5250182151794434\n","[Training Epoch 0] Batch 4834, Loss 0.5142043232917786\n","[Training Epoch 0] Batch 4835, Loss 0.4857640266418457\n","[Training Epoch 0] Batch 4836, Loss 0.4846372604370117\n","[Training Epoch 0] Batch 4837, Loss 0.49523159861564636\n","[Training Epoch 0] Batch 4838, Loss 0.4898218512535095\n","[Training Epoch 0] Batch 4839, Loss 0.48847872018814087\n","[Training Epoch 0] Batch 4840, Loss 0.5168477296829224\n","[Training Epoch 0] Batch 4841, Loss 0.5195603370666504\n","[Training Epoch 0] Batch 4842, Loss 0.5193943381309509\n","[Training Epoch 0] Batch 4843, Loss 0.5020209550857544\n","[Training Epoch 0] Batch 4844, Loss 0.5128076672554016\n","[Training Epoch 0] Batch 4845, Loss 0.47242316603660583\n","[Training Epoch 0] Batch 4846, Loss 0.5007525086402893\n","[Training Epoch 0] Batch 4847, Loss 0.5208485126495361\n","[Training Epoch 0] Batch 4848, Loss 0.5034089088439941\n","[Training Epoch 0] Batch 4849, Loss 0.4790729880332947\n","[Training Epoch 0] Batch 4850, Loss 0.4790686368942261\n","[Training Epoch 0] Batch 4851, Loss 0.4994632601737976\n","[Training Epoch 0] Batch 4852, Loss 0.4925268292427063\n","[Training Epoch 0] Batch 4853, Loss 0.4927283823490143\n","[Training Epoch 0] Batch 4854, Loss 0.4808160960674286\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:52: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame.\n","Try using .loc[row_indexer,col_indexer] = value instead\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"]},{"name":"stdout","output_type":"stream","text":["[Evluating Epoch 0] HR = 0.0988, NDCG = 0.0455\n","Epoch 1 starts !\n","--------------------------------------------------------------------------------\n","[Training Epoch 1] Batch 0, Loss 0.5144452452659607\n","[Training Epoch 1] Batch 1, Loss 0.4817847013473511\n","[Training Epoch 1] Batch 2, Loss 0.4861018657684326\n","[Training Epoch 1] Batch 3, Loss 0.48064279556274414\n","[Training Epoch 1] Batch 4, Loss 0.5155874490737915\n","[Training Epoch 1] Batch 5, Loss 0.503577470779419\n","[Training Epoch 1] Batch 6, Loss 0.469748318195343\n","[Training Epoch 1] Batch 7, Loss 0.4965239465236664\n","[Training Epoch 1] Batch 8, Loss 0.5102258920669556\n","[Training Epoch 1] Batch 9, Loss 0.4830924868583679\n","[Training Epoch 1] Batch 10, Loss 0.48854386806488037\n","[Training Epoch 1] Batch 11, Loss 0.5182993412017822\n","[Training Epoch 1] Batch 12, Loss 0.5183030366897583\n","[Training Epoch 1] Batch 13, Loss 0.5141175389289856\n","[Training Epoch 1] Batch 14, Loss 0.5046960115432739\n","[Training Epoch 1] Batch 15, Loss 0.4836096167564392\n","[Training Epoch 1] Batch 16, Loss 0.5047162175178528\n","[Training Epoch 1] Batch 17, Loss 0.4492458403110504\n","[Training Epoch 1] Batch 18, Loss 0.4901602566242218\n","[Training Epoch 1] Batch 19, Loss 0.5170835256576538\n","[Training Epoch 1] Batch 20, Loss 0.5153813362121582\n","[Training Epoch 1] Batch 21, Loss 0.5115059614181519\n","[Training Epoch 1] Batch 22, Loss 0.5115070343017578\n","[Training Epoch 1] Batch 23, Loss 0.5158008933067322\n","[Training Epoch 1] Batch 24, Loss 0.4897589385509491\n","[Training Epoch 1] Batch 25, Loss 0.5178400278091431\n","[Training Epoch 1] Batch 26, Loss 0.49513521790504456\n","[Training Epoch 1] Batch 27, Loss 0.5129567384719849\n","[Training Epoch 1] Batch 28, Loss 0.50848788022995\n","[Training Epoch 1] Batch 29, Loss 0.4925474524497986\n","[Training Epoch 1] Batch 30, Loss 0.5166728496551514\n","[Training Epoch 1] Batch 31, Loss 0.5034715533256531\n","[Training Epoch 1] Batch 32, Loss 0.4884290099143982\n","[Training Epoch 1] Batch 33, Loss 0.49522513151168823\n","[Training Epoch 1] Batch 34, Loss 0.5020822286605835\n","[Training Epoch 1] Batch 35, Loss 0.49478888511657715\n","[Training Epoch 1] Batch 36, Loss 0.5493935346603394\n","[Training Epoch 1] Batch 37, Loss 0.5034542083740234\n","[Training Epoch 1] Batch 38, Loss 0.5250774621963501\n","[Training Epoch 1] Batch 39, Loss 0.4750286340713501\n","[Training Epoch 1] Batch 40, Loss 0.5059117078781128\n","[Training Epoch 1] Batch 41, Loss 0.5143914222717285\n","[Training Epoch 1] Batch 42, Loss 0.49650049209594727\n","[Training Epoch 1] Batch 43, Loss 0.5090975165367126\n","[Training Epoch 1] Batch 44, Loss 0.5060546398162842\n","[Training Epoch 1] Batch 45, Loss 0.47937455773353577\n","[Training Epoch 1] Batch 46, Loss 0.5167667865753174\n","[Training Epoch 1] Batch 47, Loss 0.49798911809921265\n","[Training Epoch 1] Batch 48, Loss 0.4806331694126129\n","[Training Epoch 1] Batch 49, Loss 0.4861369729042053\n","[Training Epoch 1] Batch 50, Loss 0.5087699890136719\n","[Training Epoch 1] Batch 51, Loss 0.4942154288291931\n","[Training Epoch 1] Batch 52, Loss 0.48160338401794434\n","[Training Epoch 1] Batch 53, Loss 0.5008357763290405\n","[Training Epoch 1] Batch 54, Loss 0.5157697200775146\n","[Training Epoch 1] Batch 55, Loss 0.5331198573112488\n","[Training Epoch 1] Batch 56, Loss 0.4870927333831787\n","[Training Epoch 1] Batch 57, Loss 0.4683329164981842\n","[Training Epoch 1] Batch 58, Loss 0.5220744609832764\n","[Training Epoch 1] Batch 59, Loss 0.5291209816932678\n","[Training Epoch 1] Batch 60, Loss 0.4857581853866577\n","[Training Epoch 1] Batch 61, Loss 0.47787487506866455\n","[Training Epoch 1] Batch 62, Loss 0.5021136999130249\n","[Training Epoch 1] Batch 63, Loss 0.5289130210876465\n","[Training Epoch 1] Batch 64, Loss 0.4954336881637573\n","[Training Epoch 1] Batch 65, Loss 0.4899252653121948\n","[Training Epoch 1] Batch 66, Loss 0.5128698348999023\n","[Training Epoch 1] Batch 67, Loss 0.4966863989830017\n","[Training Epoch 1] Batch 68, Loss 0.485691636800766\n","[Training Epoch 1] Batch 69, Loss 0.5240687131881714\n","[Training Epoch 1] Batch 70, Loss 0.5086954832077026\n","[Training Epoch 1] Batch 71, Loss 0.4804990291595459\n","[Training Epoch 1] Batch 72, Loss 0.49801966547966003\n","[Training Epoch 1] Batch 73, Loss 0.5100248456001282\n","[Training Epoch 1] Batch 74, Loss 0.5024175047874451\n","[Training Epoch 1] Batch 75, Loss 0.4861172139644623\n","[Training Epoch 1] Batch 76, Loss 0.5009179711341858\n","[Training Epoch 1] Batch 77, Loss 0.472301721572876\n","[Training Epoch 1] Batch 78, Loss 0.5224008560180664\n","[Training Epoch 1] Batch 79, Loss 0.5221024751663208\n","[Training Epoch 1] Batch 80, Loss 0.49129053950309753\n","[Training Epoch 1] Batch 81, Loss 0.5142384767532349\n","[Training Epoch 1] Batch 82, Loss 0.489874929189682\n","[Training Epoch 1] Batch 83, Loss 0.49386292695999146\n","[Training Epoch 1] Batch 84, Loss 0.491341769695282\n","[Training Epoch 1] Batch 85, Loss 0.49240177869796753\n","[Training Epoch 1] Batch 86, Loss 0.5248894691467285\n","[Training Epoch 1] Batch 87, Loss 0.5263636112213135\n","[Training Epoch 1] Batch 88, Loss 0.4912160634994507\n","[Training Epoch 1] Batch 89, Loss 0.4843505322933197\n","[Training Epoch 1] Batch 90, Loss 0.4979943633079529\n","[Training Epoch 1] Batch 91, Loss 0.5088440179824829\n","[Training Epoch 1] Batch 92, Loss 0.4872150123119354\n","[Training Epoch 1] Batch 93, Loss 0.500589907169342\n","[Training Epoch 1] Batch 94, Loss 0.48185408115386963\n","[Training Epoch 1] Batch 95, Loss 0.4695402979850769\n","[Training Epoch 1] Batch 96, Loss 0.5126215219497681\n","[Training Epoch 1] Batch 97, Loss 0.4925495982170105\n","[Training Epoch 1] Batch 98, Loss 0.49362772703170776\n","[Training Epoch 1] Batch 99, Loss 0.524700403213501\n","[Training Epoch 1] Batch 100, Loss 0.49791404604911804\n","[Training Epoch 1] Batch 101, Loss 0.5276889204978943\n","[Training Epoch 1] Batch 102, Loss 0.5064670443534851\n","[Training Epoch 1] Batch 103, Loss 0.4897857904434204\n","[Training Epoch 1] Batch 104, Loss 0.48871132731437683\n","[Training Epoch 1] Batch 105, Loss 0.4965824484825134\n","[Training Epoch 1] Batch 106, Loss 0.4967825412750244\n","[Training Epoch 1] Batch 107, Loss 0.46816137433052063\n","[Training Epoch 1] Batch 108, Loss 0.47639697790145874\n","[Training Epoch 1] Batch 109, Loss 0.5276800990104675\n","[Training Epoch 1] Batch 110, Loss 0.4695048928260803\n","[Training Epoch 1] Batch 111, Loss 0.4883665442466736\n","[Training Epoch 1] Batch 112, Loss 0.5140281915664673\n","[Training Epoch 1] Batch 113, Loss 0.5221281051635742\n","[Training Epoch 1] Batch 114, Loss 0.501933217048645\n","[Training Epoch 1] Batch 115, Loss 0.47735878825187683\n","[Training Epoch 1] Batch 116, Loss 0.5301786065101624\n","[Training Epoch 1] Batch 117, Loss 0.5020031929016113\n","[Training Epoch 1] Batch 118, Loss 0.5196488499641418\n","[Training Epoch 1] Batch 119, Loss 0.5146439075469971\n","[Training Epoch 1] Batch 120, Loss 0.49648356437683105\n","[Training Epoch 1] Batch 121, Loss 0.5120307207107544\n","[Training Epoch 1] Batch 122, Loss 0.514057457447052\n","[Training Epoch 1] Batch 123, Loss 0.501186728477478\n","[Training Epoch 1] Batch 124, Loss 0.5279125571250916\n","[Training Epoch 1] Batch 125, Loss 0.5112156271934509\n","[Training Epoch 1] Batch 126, Loss 0.4965273141860962\n","[Training Epoch 1] Batch 127, Loss 0.4789588451385498\n","[Training Epoch 1] Batch 128, Loss 0.5237112045288086\n","[Training Epoch 1] Batch 129, Loss 0.49224352836608887\n","[Training Epoch 1] Batch 130, Loss 0.5241564512252808\n","[Training Epoch 1] Batch 131, Loss 0.5158973932266235\n","[Training Epoch 1] Batch 132, Loss 0.5008670091629028\n","[Training Epoch 1] Batch 133, Loss 0.530148983001709\n","[Training Epoch 1] Batch 134, Loss 0.49705833196640015\n","[Training Epoch 1] Batch 135, Loss 0.4949437379837036\n","[Training Epoch 1] Batch 136, Loss 0.5125229358673096\n","[Training Epoch 1] Batch 137, Loss 0.504969596862793\n","[Training Epoch 1] Batch 138, Loss 0.4956011176109314\n","[Training Epoch 1] Batch 139, Loss 0.49322569370269775\n","[Training Epoch 1] Batch 140, Loss 0.5042554140090942\n","[Training Epoch 1] Batch 141, Loss 0.504891574382782\n","[Training Epoch 1] Batch 142, Loss 0.49910545349121094\n","[Training Epoch 1] Batch 143, Loss 0.4942121207714081\n","[Training Epoch 1] Batch 144, Loss 0.5225666761398315\n","[Training Epoch 1] Batch 145, Loss 0.4901356101036072\n","[Training Epoch 1] Batch 146, Loss 0.5261201858520508\n","[Training Epoch 1] Batch 147, Loss 0.5059961080551147\n","[Training Epoch 1] Batch 148, Loss 0.5034567713737488\n","[Training Epoch 1] Batch 149, Loss 0.5207418203353882\n","[Training Epoch 1] Batch 150, Loss 0.5100337266921997\n","[Training Epoch 1] Batch 151, Loss 0.4926276206970215\n","[Training Epoch 1] Batch 152, Loss 0.48195332288742065\n","[Training Epoch 1] Batch 153, Loss 0.48865070939064026\n","[Training Epoch 1] Batch 154, Loss 0.5032349824905396\n","[Training Epoch 1] Batch 155, Loss 0.5156315565109253\n","[Training Epoch 1] Batch 156, Loss 0.4993601441383362\n","[Training Epoch 1] Batch 157, Loss 0.4848504662513733\n","[Training Epoch 1] Batch 158, Loss 0.5033807754516602\n","[Training Epoch 1] Batch 159, Loss 0.4885648190975189\n","[Training Epoch 1] Batch 160, Loss 0.5209090709686279\n","[Training Epoch 1] Batch 161, Loss 0.503510057926178\n","[Training Epoch 1] Batch 162, Loss 0.5126427412033081\n","[Training Epoch 1] Batch 163, Loss 0.5005395412445068\n","[Training Epoch 1] Batch 164, Loss 0.48983025550842285\n","[Training Epoch 1] Batch 165, Loss 0.49771034717559814\n","[Training Epoch 1] Batch 166, Loss 0.492567777633667\n","[Training Epoch 1] Batch 167, Loss 0.5139708518981934\n","[Training Epoch 1] Batch 168, Loss 0.5024369955062866\n","[Training Epoch 1] Batch 169, Loss 0.4750567078590393\n","[Training Epoch 1] Batch 170, Loss 0.47240978479385376\n","[Training Epoch 1] Batch 171, Loss 0.5019952058792114\n","[Training Epoch 1] Batch 172, Loss 0.5088703632354736\n","[Training Epoch 1] Batch 173, Loss 0.5249109268188477\n","[Training Epoch 1] Batch 174, Loss 0.5063183307647705\n","[Training Epoch 1] Batch 175, Loss 0.5060219764709473\n","[Training Epoch 1] Batch 176, Loss 0.48054832220077515\n","[Training Epoch 1] Batch 177, Loss 0.49374693632125854\n","[Training Epoch 1] Batch 178, Loss 0.5183132290840149\n","[Training Epoch 1] Batch 179, Loss 0.5212307572364807\n","[Training Epoch 1] Batch 180, Loss 0.4953516721725464\n","[Training Epoch 1] Batch 181, Loss 0.509687066078186\n","[Training Epoch 1] Batch 182, Loss 0.4896124601364136\n","[Training Epoch 1] Batch 183, Loss 0.5034207105636597\n","[Training Epoch 1] Batch 184, Loss 0.5017979741096497\n","[Training Epoch 1] Batch 185, Loss 0.49377185106277466\n","[Training Epoch 1] Batch 186, Loss 0.4669990539550781\n","[Training Epoch 1] Batch 187, Loss 0.5346222519874573\n","[Training Epoch 1] Batch 188, Loss 0.4831787645816803\n","[Training Epoch 1] Batch 189, Loss 0.5018823146820068\n","[Training Epoch 1] Batch 190, Loss 0.493635356426239\n","[Training Epoch 1] Batch 191, Loss 0.5154234170913696\n","[Training Epoch 1] Batch 192, Loss 0.49855130910873413\n","[Training Epoch 1] Batch 193, Loss 0.5101974606513977\n","[Training Epoch 1] Batch 194, Loss 0.49533456563949585\n","[Training Epoch 1] Batch 195, Loss 0.5035112500190735\n","[Training Epoch 1] Batch 196, Loss 0.4733023941516876\n","[Training Epoch 1] Batch 197, Loss 0.511720597743988\n","[Training Epoch 1] Batch 198, Loss 0.5029376745223999\n","[Training Epoch 1] Batch 199, Loss 0.5090222358703613\n","[Training Epoch 1] Batch 200, Loss 0.49536460638046265\n","[Training Epoch 1] Batch 201, Loss 0.4821535050868988\n","[Training Epoch 1] Batch 202, Loss 0.4856888949871063\n","[Training Epoch 1] Batch 203, Loss 0.48974084854125977\n","[Training Epoch 1] Batch 204, Loss 0.5010047554969788\n","[Training Epoch 1] Batch 205, Loss 0.5183287858963013\n","[Training Epoch 1] Batch 206, Loss 0.5115954875946045\n","[Training Epoch 1] Batch 207, Loss 0.506350040435791\n","[Training Epoch 1] Batch 208, Loss 0.5225241184234619\n","[Training Epoch 1] Batch 209, Loss 0.5153900384902954\n","[Training Epoch 1] Batch 210, Loss 0.49691760540008545\n","[Training Epoch 1] Batch 211, Loss 0.46677085757255554\n","[Training Epoch 1] Batch 212, Loss 0.5354772806167603\n","[Training Epoch 1] Batch 213, Loss 0.5157430171966553\n","[Training Epoch 1] Batch 214, Loss 0.509070098400116\n","[Training Epoch 1] Batch 215, Loss 0.49908027052879333\n","[Training Epoch 1] Batch 216, Loss 0.5319961905479431\n","[Training Epoch 1] Batch 217, Loss 0.485701322555542\n","[Training Epoch 1] Batch 218, Loss 0.5089155435562134\n","[Training Epoch 1] Batch 219, Loss 0.5181552767753601\n","[Training Epoch 1] Batch 220, Loss 0.49390244483947754\n","[Training Epoch 1] Batch 221, Loss 0.5100340843200684\n","[Training Epoch 1] Batch 222, Loss 0.504697322845459\n","[Training Epoch 1] Batch 223, Loss 0.4723673462867737\n","[Training Epoch 1] Batch 224, Loss 0.5182691812515259\n","[Training Epoch 1] Batch 225, Loss 0.48189374804496765\n","[Training Epoch 1] Batch 226, Loss 0.48312628269195557\n","[Training Epoch 1] Batch 227, Loss 0.4980575740337372\n","[Training Epoch 1] Batch 228, Loss 0.5063199996948242\n","[Training Epoch 1] Batch 229, Loss 0.499015212059021\n","[Training Epoch 1] Batch 230, Loss 0.48333877325057983\n","[Training Epoch 1] Batch 231, Loss 0.5222722887992859\n","[Training Epoch 1] Batch 232, Loss 0.4970586895942688\n","[Training Epoch 1] Batch 233, Loss 0.5057722330093384\n","[Training Epoch 1] Batch 234, Loss 0.5143945217132568\n","[Training Epoch 1] Batch 235, Loss 0.49653640389442444\n","[Training Epoch 1] Batch 236, Loss 0.469639390707016\n","[Training Epoch 1] Batch 237, Loss 0.4722806215286255\n","[Training Epoch 1] Batch 238, Loss 0.5114905834197998\n","[Training Epoch 1] Batch 239, Loss 0.49822908639907837\n","[Training Epoch 1] Batch 240, Loss 0.5005993247032166\n","[Training Epoch 1] Batch 241, Loss 0.4992600977420807\n","[Training Epoch 1] Batch 242, Loss 0.49408960342407227\n","[Training Epoch 1] Batch 243, Loss 0.5060120820999146\n","[Training Epoch 1] Batch 244, Loss 0.5114724040031433\n","[Training Epoch 1] Batch 245, Loss 0.5291346907615662\n","[Training Epoch 1] Batch 246, Loss 0.488390177488327\n","[Training Epoch 1] Batch 247, Loss 0.5005704164505005\n","[Training Epoch 1] Batch 248, Loss 0.5034884810447693\n","[Training Epoch 1] Batch 249, Loss 0.48045480251312256\n","[Training Epoch 1] Batch 250, Loss 0.47225555777549744\n","[Training Epoch 1] Batch 251, Loss 0.49676963686943054\n","[Training Epoch 1] Batch 252, Loss 0.5236167907714844\n","[Training Epoch 1] Batch 253, Loss 0.48564985394477844\n","[Training Epoch 1] Batch 254, Loss 0.493571400642395\n","[Training Epoch 1] Batch 255, Loss 0.4631193280220032\n","[Training Epoch 1] Batch 256, Loss 0.5182419419288635\n","[Training Epoch 1] Batch 257, Loss 0.49927467107772827\n","[Training Epoch 1] Batch 258, Loss 0.5235112309455872\n","[Training Epoch 1] Batch 259, Loss 0.464304119348526\n","[Training Epoch 1] Batch 260, Loss 0.5218538045883179\n","[Training Epoch 1] Batch 261, Loss 0.4896951913833618\n","[Training Epoch 1] Batch 262, Loss 0.5130488276481628\n","[Training Epoch 1] Batch 263, Loss 0.49946165084838867\n","[Training Epoch 1] Batch 264, Loss 0.5169709920883179\n","[Training Epoch 1] Batch 265, Loss 0.47498926520347595\n","[Training Epoch 1] Batch 266, Loss 0.5169742107391357\n","[Training Epoch 1] Batch 267, Loss 0.48433196544647217\n","[Training Epoch 1] Batch 268, Loss 0.47767820954322815\n","[Training Epoch 1] Batch 269, Loss 0.5008784532546997\n","[Training Epoch 1] Batch 270, Loss 0.49955448508262634\n","[Training Epoch 1] Batch 271, Loss 0.5008766651153564\n","[Training Epoch 1] Batch 272, Loss 0.5184475183486938\n","[Training Epoch 1] Batch 273, Loss 0.5084312558174133\n","[Training Epoch 1] Batch 274, Loss 0.49381136894226074\n","[Training Epoch 1] Batch 275, Loss 0.4884888827800751\n","[Training Epoch 1] Batch 276, Loss 0.5073941946029663\n","[Training Epoch 1] Batch 277, Loss 0.4870075583457947\n","[Training Epoch 1] Batch 278, Loss 0.5115283131599426\n","[Training Epoch 1] Batch 279, Loss 0.5139937996864319\n","[Training Epoch 1] Batch 280, Loss 0.5154427289962769\n","[Training Epoch 1] Batch 281, Loss 0.5372227430343628\n","[Training Epoch 1] Batch 282, Loss 0.493644118309021\n","[Training Epoch 1] Batch 283, Loss 0.5023053884506226\n","[Training Epoch 1] Batch 284, Loss 0.5192699432373047\n","[Training Epoch 1] Batch 285, Loss 0.5049322843551636\n","[Training Epoch 1] Batch 286, Loss 0.5100566148757935\n","[Training Epoch 1] Batch 287, Loss 0.48071056604385376\n","[Training Epoch 1] Batch 288, Loss 0.4978494942188263\n","[Training Epoch 1] Batch 289, Loss 0.5266734957695007\n","[Training Epoch 1] Batch 290, Loss 0.5046780109405518\n","[Training Epoch 1] Batch 291, Loss 0.5019229650497437\n","[Training Epoch 1] Batch 292, Loss 0.4843975305557251\n","[Training Epoch 1] Batch 293, Loss 0.5264125466346741\n","[Training Epoch 1] Batch 294, Loss 0.5047770738601685\n","[Training Epoch 1] Batch 295, Loss 0.4992806911468506\n","[Training Epoch 1] Batch 296, Loss 0.5116723775863647\n","[Training Epoch 1] Batch 297, Loss 0.4907137155532837\n","[Training Epoch 1] Batch 298, Loss 0.5003256797790527\n","[Training Epoch 1] Batch 299, Loss 0.4935283958911896\n","[Training Epoch 1] Batch 300, Loss 0.49557679891586304\n","[Training Epoch 1] Batch 301, Loss 0.49134013056755066\n","[Training Epoch 1] Batch 302, Loss 0.5083791017532349\n","[Training Epoch 1] Batch 303, Loss 0.5303554534912109\n","[Training Epoch 1] Batch 304, Loss 0.4737117886543274\n","[Training Epoch 1] Batch 305, Loss 0.4942181706428528\n","[Training Epoch 1] Batch 306, Loss 0.5033245086669922\n","[Training Epoch 1] Batch 307, Loss 0.4720853567123413\n","[Training Epoch 1] Batch 308, Loss 0.4993092119693756\n","[Training Epoch 1] Batch 309, Loss 0.5083968043327332\n","[Training Epoch 1] Batch 310, Loss 0.49659278988838196\n","[Training Epoch 1] Batch 311, Loss 0.5023947954177856\n","[Training Epoch 1] Batch 312, Loss 0.5033984184265137\n","[Training Epoch 1] Batch 313, Loss 0.5088965892791748\n","[Training Epoch 1] Batch 314, Loss 0.5290358662605286\n","[Training Epoch 1] Batch 315, Loss 0.48103803396224976\n","[Training Epoch 1] Batch 316, Loss 0.4815276861190796\n","[Training Epoch 1] Batch 317, Loss 0.4979630708694458\n","[Training Epoch 1] Batch 318, Loss 0.4628748297691345\n","[Training Epoch 1] Batch 319, Loss 0.524806022644043\n","[Training Epoch 1] Batch 320, Loss 0.5093743801116943\n","[Training Epoch 1] Batch 321, Loss 0.4885646402835846\n","[Training Epoch 1] Batch 322, Loss 0.5031291842460632\n","[Training Epoch 1] Batch 323, Loss 0.5384369492530823\n","[Training Epoch 1] Batch 324, Loss 0.5067747831344604\n","[Training Epoch 1] Batch 325, Loss 0.5275510549545288\n","[Training Epoch 1] Batch 326, Loss 0.49380993843078613\n","[Training Epoch 1] Batch 327, Loss 0.5063756704330444\n","[Training Epoch 1] Batch 328, Loss 0.523859441280365\n","[Training Epoch 1] Batch 329, Loss 0.4881635904312134\n","[Training Epoch 1] Batch 330, Loss 0.5153266191482544\n","[Training Epoch 1] Batch 331, Loss 0.541294515132904\n","[Training Epoch 1] Batch 332, Loss 0.4951862394809723\n","[Training Epoch 1] Batch 333, Loss 0.48186951875686646\n","[Training Epoch 1] Batch 334, Loss 0.529077410697937\n","[Training Epoch 1] Batch 335, Loss 0.5078748464584351\n","[Training Epoch 1] Batch 336, Loss 0.507123589515686\n","[Training Epoch 1] Batch 337, Loss 0.48785078525543213\n","[Training Epoch 1] Batch 338, Loss 0.47646474838256836\n","[Training Epoch 1] Batch 339, Loss 0.4990409314632416\n","[Training Epoch 1] Batch 340, Loss 0.47516965866088867\n","[Training Epoch 1] Batch 341, Loss 0.5153194665908813\n","[Training Epoch 1] Batch 342, Loss 0.5084562301635742\n","[Training Epoch 1] Batch 343, Loss 0.5254875421524048\n","[Training Epoch 1] Batch 344, Loss 0.49787646532058716\n","[Training Epoch 1] Batch 345, Loss 0.4977770149707794\n","[Training Epoch 1] Batch 346, Loss 0.5233670473098755\n","[Training Epoch 1] Batch 347, Loss 0.4617187976837158\n","[Training Epoch 1] Batch 348, Loss 0.49123460054397583\n","[Training Epoch 1] Batch 349, Loss 0.5071322917938232\n","[Training Epoch 1] Batch 350, Loss 0.5008226633071899\n","[Training Epoch 1] Batch 351, Loss 0.49121129512786865\n","[Training Epoch 1] Batch 352, Loss 0.4978445768356323\n","[Training Epoch 1] Batch 353, Loss 0.5045534372329712\n","[Training Epoch 1] Batch 354, Loss 0.5354909896850586\n","[Training Epoch 1] Batch 355, Loss 0.4800279140472412\n","[Training Epoch 1] Batch 356, Loss 0.4951380491256714\n","[Training Epoch 1] Batch 357, Loss 0.46977394819259644\n","[Training Epoch 1] Batch 358, Loss 0.5007034540176392\n","[Training Epoch 1] Batch 359, Loss 0.5493608713150024\n","[Training Epoch 1] Batch 360, Loss 0.5194779634475708\n","[Training Epoch 1] Batch 361, Loss 0.4978456497192383\n","[Training Epoch 1] Batch 362, Loss 0.5137100219726562\n","[Training Epoch 1] Batch 363, Loss 0.4820425510406494\n","[Training Epoch 1] Batch 364, Loss 0.4874240756034851\n","[Training Epoch 1] Batch 365, Loss 0.5073506832122803\n","[Training Epoch 1] Batch 366, Loss 0.5116813778877258\n","[Training Epoch 1] Batch 367, Loss 0.5179616212844849\n","[Training Epoch 1] Batch 368, Loss 0.5033380389213562\n","[Training Epoch 1] Batch 369, Loss 0.5099371671676636\n","[Training Epoch 1] Batch 370, Loss 0.49135100841522217\n","[Training Epoch 1] Batch 371, Loss 0.506079375743866\n","[Training Epoch 1] Batch 372, Loss 0.49391189217567444\n","[Training Epoch 1] Batch 373, Loss 0.49653130769729614\n","[Training Epoch 1] Batch 374, Loss 0.5117278099060059\n","[Training Epoch 1] Batch 375, Loss 0.5099818110466003\n","[Training Epoch 1] Batch 376, Loss 0.5169044733047485\n","[Training Epoch 1] Batch 377, Loss 0.4914185404777527\n","[Training Epoch 1] Batch 378, Loss 0.4941316246986389\n","[Training Epoch 1] Batch 379, Loss 0.5071837902069092\n","[Training Epoch 1] Batch 380, Loss 0.489449143409729\n","[Training Epoch 1] Batch 381, Loss 0.49154412746429443\n","[Training Epoch 1] Batch 382, Loss 0.48552030324935913\n","[Training Epoch 1] Batch 383, Loss 0.5049589276313782\n","[Training Epoch 1] Batch 384, Loss 0.5036295652389526\n","[Training Epoch 1] Batch 385, Loss 0.5165342092514038\n","[Training Epoch 1] Batch 386, Loss 0.5061130523681641\n","[Training Epoch 1] Batch 387, Loss 0.5046387314796448\n","[Training Epoch 1] Batch 388, Loss 0.4927964210510254\n","[Training Epoch 1] Batch 389, Loss 0.5130589008331299\n","[Training Epoch 1] Batch 390, Loss 0.5157857537269592\n","[Training Epoch 1] Batch 391, Loss 0.5021384954452515\n","[Training Epoch 1] Batch 392, Loss 0.48884159326553345\n","[Training Epoch 1] Batch 393, Loss 0.4943174719810486\n","[Training Epoch 1] Batch 394, Loss 0.4981994330883026\n","[Training Epoch 1] Batch 395, Loss 0.5088975429534912\n","[Training Epoch 1] Batch 396, Loss 0.4871640205383301\n","[Training Epoch 1] Batch 397, Loss 0.48712530732154846\n","[Training Epoch 1] Batch 398, Loss 0.5071820020675659\n","[Training Epoch 1] Batch 399, Loss 0.4858385920524597\n","[Training Epoch 1] Batch 400, Loss 0.49344396591186523\n","[Training Epoch 1] Batch 401, Loss 0.4918810725212097\n","[Training Epoch 1] Batch 402, Loss 0.4952727258205414\n","[Training Epoch 1] Batch 403, Loss 0.5044817924499512\n","[Training Epoch 1] Batch 404, Loss 0.5123878717422485\n","[Training Epoch 1] Batch 405, Loss 0.512541651725769\n","[Training Epoch 1] Batch 406, Loss 0.5277678966522217\n","[Training Epoch 1] Batch 407, Loss 0.5105881690979004\n","[Training Epoch 1] Batch 408, Loss 0.4843585193157196\n","[Training Epoch 1] Batch 409, Loss 0.5092189311981201\n","[Training Epoch 1] Batch 410, Loss 0.47518059611320496\n","[Training Epoch 1] Batch 411, Loss 0.4950565993785858\n","[Training Epoch 1] Batch 412, Loss 0.50352942943573\n","[Training Epoch 1] Batch 413, Loss 0.48911041021347046\n","[Training Epoch 1] Batch 414, Loss 0.5102810859680176\n","[Training Epoch 1] Batch 415, Loss 0.4924246668815613\n","[Training Epoch 1] Batch 416, Loss 0.5147085189819336\n","[Training Epoch 1] Batch 417, Loss 0.4875977337360382\n","[Training Epoch 1] Batch 418, Loss 0.5046714544296265\n","[Training Epoch 1] Batch 419, Loss 0.5182393789291382\n","[Training Epoch 1] Batch 420, Loss 0.5006417036056519\n","[Training Epoch 1] Batch 421, Loss 0.5050271153450012\n","[Training Epoch 1] Batch 422, Loss 0.4767066240310669\n","[Training Epoch 1] Batch 423, Loss 0.48184359073638916\n","[Training Epoch 1] Batch 424, Loss 0.47272995114326477\n","[Training Epoch 1] Batch 425, Loss 0.4980209171772003\n","[Training Epoch 1] Batch 426, Loss 0.49951252341270447\n","[Training Epoch 1] Batch 427, Loss 0.5076500177383423\n","[Training Epoch 1] Batch 428, Loss 0.4876176118850708\n","[Training Epoch 1] Batch 429, Loss 0.4790785610675812\n","[Training Epoch 1] Batch 430, Loss 0.49932557344436646\n","[Training Epoch 1] Batch 431, Loss 0.5100255608558655\n","[Training Epoch 1] Batch 432, Loss 0.5047125816345215\n","[Training Epoch 1] Batch 433, Loss 0.4816715717315674\n","[Training Epoch 1] Batch 434, Loss 0.48707646131515503\n","[Training Epoch 1] Batch 435, Loss 0.5074344277381897\n","[Training Epoch 1] Batch 436, Loss 0.5009299516677856\n","[Training Epoch 1] Batch 437, Loss 0.5048848390579224\n","[Training Epoch 1] Batch 438, Loss 0.4816477596759796\n","[Training Epoch 1] Batch 439, Loss 0.5131700038909912\n","[Training Epoch 1] Batch 440, Loss 0.516944408416748\n","[Training Epoch 1] Batch 441, Loss 0.5020558834075928\n","[Training Epoch 1] Batch 442, Loss 0.4833133816719055\n","[Training Epoch 1] Batch 443, Loss 0.4965136647224426\n","[Training Epoch 1] Batch 444, Loss 0.5071927309036255\n","[Training Epoch 1] Batch 445, Loss 0.47787490487098694\n","[Training Epoch 1] Batch 446, Loss 0.4898660182952881\n","[Training Epoch 1] Batch 447, Loss 0.5006242990493774\n","[Training Epoch 1] Batch 448, Loss 0.4885927736759186\n","[Training Epoch 1] Batch 449, Loss 0.4965062439441681\n","[Training Epoch 1] Batch 450, Loss 0.5060834884643555\n","[Training Epoch 1] Batch 451, Loss 0.49241673946380615\n","[Training Epoch 1] Batch 452, Loss 0.49284082651138306\n","[Training Epoch 1] Batch 453, Loss 0.5004827380180359\n","[Training Epoch 1] Batch 454, Loss 0.49665841460227966\n","[Training Epoch 1] Batch 455, Loss 0.49145907163619995\n","[Training Epoch 1] Batch 456, Loss 0.5112659931182861\n","[Training Epoch 1] Batch 457, Loss 0.4896236062049866\n","[Training Epoch 1] Batch 458, Loss 0.4857366979122162\n","[Training Epoch 1] Batch 459, Loss 0.5303135514259338\n","[Training Epoch 1] Batch 460, Loss 0.5086283683776855\n","[Training Epoch 1] Batch 461, Loss 0.49809712171554565\n","[Training Epoch 1] Batch 462, Loss 0.4832346439361572\n","[Training Epoch 1] Batch 463, Loss 0.5209180116653442\n","[Training Epoch 1] Batch 464, Loss 0.5343619585037231\n","[Training Epoch 1] Batch 465, Loss 0.4943453073501587\n","[Training Epoch 1] Batch 466, Loss 0.5077006816864014\n","[Training Epoch 1] Batch 467, Loss 0.5223942995071411\n","[Training Epoch 1] Batch 468, Loss 0.5075521469116211\n","[Training Epoch 1] Batch 469, Loss 0.4978715181350708\n","[Training Epoch 1] Batch 470, Loss 0.49928945302963257\n","[Training Epoch 1] Batch 471, Loss 0.4805126488208771\n","[Training Epoch 1] Batch 472, Loss 0.49537572264671326\n","[Training Epoch 1] Batch 473, Loss 0.5024135112762451\n","[Training Epoch 1] Batch 474, Loss 0.5305972099304199\n","[Training Epoch 1] Batch 475, Loss 0.503423273563385\n","[Training Epoch 1] Batch 476, Loss 0.48051249980926514\n","[Training Epoch 1] Batch 477, Loss 0.5196710824966431\n","[Training Epoch 1] Batch 478, Loss 0.5213139057159424\n","[Training Epoch 1] Batch 479, Loss 0.5386455059051514\n","[Training Epoch 1] Batch 480, Loss 0.4993910789489746\n","[Training Epoch 1] Batch 481, Loss 0.5034134984016418\n","[Training Epoch 1] Batch 482, Loss 0.4992634057998657\n","[Training Epoch 1] Batch 483, Loss 0.500809907913208\n","[Training Epoch 1] Batch 484, Loss 0.49268490076065063\n","[Training Epoch 1] Batch 485, Loss 0.5276970863342285\n","[Training Epoch 1] Batch 486, Loss 0.48722171783447266\n","[Training Epoch 1] Batch 487, Loss 0.4897705316543579\n","[Training Epoch 1] Batch 488, Loss 0.5182033777236938\n","[Training Epoch 1] Batch 489, Loss 0.5035037398338318\n","[Training Epoch 1] Batch 490, Loss 0.4965604543685913\n","[Training Epoch 1] Batch 491, Loss 0.5018249154090881\n","[Training Epoch 1] Batch 492, Loss 0.48986417055130005\n","[Training Epoch 1] Batch 493, Loss 0.4696512520313263\n","[Training Epoch 1] Batch 494, Loss 0.4814659357070923\n","[Training Epoch 1] Batch 495, Loss 0.5195479393005371\n","[Training Epoch 1] Batch 496, Loss 0.48187345266342163\n","[Training Epoch 1] Batch 497, Loss 0.4990769028663635\n","[Training Epoch 1] Batch 498, Loss 0.4981462359428406\n","[Training Epoch 1] Batch 499, Loss 0.4981534779071808\n","[Training Epoch 1] Batch 500, Loss 0.47638535499572754\n","[Training Epoch 1] Batch 501, Loss 0.5113787651062012\n","[Training Epoch 1] Batch 502, Loss 0.4937364459037781\n","[Training Epoch 1] Batch 503, Loss 0.49659934639930725\n","[Training Epoch 1] Batch 504, Loss 0.525161623954773\n","[Training Epoch 1] Batch 505, Loss 0.49637192487716675\n","[Training Epoch 1] Batch 506, Loss 0.5102572441101074\n","[Training Epoch 1] Batch 507, Loss 0.48854315280914307\n","[Training Epoch 1] Batch 508, Loss 0.5115187168121338\n","[Training Epoch 1] Batch 509, Loss 0.48982110619544983\n","[Training Epoch 1] Batch 510, Loss 0.4969028830528259\n","[Training Epoch 1] Batch 511, Loss 0.4671013355255127\n","[Training Epoch 1] Batch 512, Loss 0.4940948784351349\n","[Training Epoch 1] Batch 513, Loss 0.5088167190551758\n","[Training Epoch 1] Batch 514, Loss 0.49638915061950684\n","[Training Epoch 1] Batch 515, Loss 0.49640166759490967\n","[Training Epoch 1] Batch 516, Loss 0.4751276969909668\n","[Training Epoch 1] Batch 517, Loss 0.5197111368179321\n","[Training Epoch 1] Batch 518, Loss 0.5252323746681213\n","[Training Epoch 1] Batch 519, Loss 0.5049591064453125\n","[Training Epoch 1] Batch 520, Loss 0.5021166801452637\n","[Training Epoch 1] Batch 521, Loss 0.5170992612838745\n","[Training Epoch 1] Batch 522, Loss 0.5128844976425171\n","[Training Epoch 1] Batch 523, Loss 0.48861271142959595\n","[Training Epoch 1] Batch 524, Loss 0.47369450330734253\n","[Training Epoch 1] Batch 525, Loss 0.5103152990341187\n","[Training Epoch 1] Batch 526, Loss 0.5089468359947205\n","[Training Epoch 1] Batch 527, Loss 0.515513002872467\n","[Training Epoch 1] Batch 528, Loss 0.45635882019996643\n","[Training Epoch 1] Batch 529, Loss 0.5004874467849731\n","[Training Epoch 1] Batch 530, Loss 0.5127456188201904\n","[Training Epoch 1] Batch 531, Loss 0.48726603388786316\n","[Training Epoch 1] Batch 532, Loss 0.48719048500061035\n","[Training Epoch 1] Batch 533, Loss 0.5116091370582581\n","[Training Epoch 1] Batch 534, Loss 0.4992729425430298\n","[Training Epoch 1] Batch 535, Loss 0.5194711089134216\n","[Training Epoch 1] Batch 536, Loss 0.5155807733535767\n","[Training Epoch 1] Batch 537, Loss 0.5384182929992676\n","[Training Epoch 1] Batch 538, Loss 0.5278300046920776\n","[Training Epoch 1] Batch 539, Loss 0.4993794560432434\n","[Training Epoch 1] Batch 540, Loss 0.5209019780158997\n","[Training Epoch 1] Batch 541, Loss 0.4925426244735718\n","[Training Epoch 1] Batch 542, Loss 0.5207237601280212\n","[Training Epoch 1] Batch 543, Loss 0.49953407049179077\n","[Training Epoch 1] Batch 544, Loss 0.504523515701294\n","[Training Epoch 1] Batch 545, Loss 0.49649477005004883\n","[Training Epoch 1] Batch 546, Loss 0.5033628940582275\n","[Training Epoch 1] Batch 547, Loss 0.5089312791824341\n","[Training Epoch 1] Batch 548, Loss 0.4926813244819641\n","[Training Epoch 1] Batch 549, Loss 0.49940025806427\n","[Training Epoch 1] Batch 550, Loss 0.4911381006240845\n","[Training Epoch 1] Batch 551, Loss 0.4858549237251282\n","[Training Epoch 1] Batch 552, Loss 0.5007184743881226\n","[Training Epoch 1] Batch 553, Loss 0.49667888879776\n","[Training Epoch 1] Batch 554, Loss 0.495453804731369\n","[Training Epoch 1] Batch 555, Loss 0.5219419002532959\n","[Training Epoch 1] Batch 556, Loss 0.5062451362609863\n","[Training Epoch 1] Batch 557, Loss 0.4980783760547638\n","[Training Epoch 1] Batch 558, Loss 0.4913076162338257\n","[Training Epoch 1] Batch 559, Loss 0.5128965377807617\n","[Training Epoch 1] Batch 560, Loss 0.5194669961929321\n","[Training Epoch 1] Batch 561, Loss 0.4655105471611023\n","[Training Epoch 1] Batch 562, Loss 0.5101590156555176\n","[Training Epoch 1] Batch 563, Loss 0.4843999743461609\n","[Training Epoch 1] Batch 564, Loss 0.47898098826408386\n","[Training Epoch 1] Batch 565, Loss 0.5017859935760498\n","[Training Epoch 1] Batch 566, Loss 0.512832760810852\n","[Training Epoch 1] Batch 567, Loss 0.4990653693675995\n","[Training Epoch 1] Batch 568, Loss 0.48583370447158813\n","[Training Epoch 1] Batch 569, Loss 0.5301792621612549\n","[Training Epoch 1] Batch 570, Loss 0.4937382936477661\n","[Training Epoch 1] Batch 571, Loss 0.5438534617424011\n","[Training Epoch 1] Batch 572, Loss 0.5037773847579956\n","[Training Epoch 1] Batch 573, Loss 0.5103954076766968\n","[Training Epoch 1] Batch 574, Loss 0.47885578870773315\n","[Training Epoch 1] Batch 575, Loss 0.5074734687805176\n","[Training Epoch 1] Batch 576, Loss 0.5186139345169067\n","[Training Epoch 1] Batch 577, Loss 0.4870549440383911\n","[Training Epoch 1] Batch 578, Loss 0.5032010674476624\n","[Training Epoch 1] Batch 579, Loss 0.5022401213645935\n","[Training Epoch 1] Batch 580, Loss 0.49394458532333374\n","[Training Epoch 1] Batch 581, Loss 0.4694654047489166\n","[Training Epoch 1] Batch 582, Loss 0.5128862857818604\n","[Training Epoch 1] Batch 583, Loss 0.5196309089660645\n","[Training Epoch 1] Batch 584, Loss 0.4645842909812927\n","[Training Epoch 1] Batch 585, Loss 0.499560683965683\n","[Training Epoch 1] Batch 586, Loss 0.49124523997306824\n","[Training Epoch 1] Batch 587, Loss 0.4968213140964508\n","[Training Epoch 1] Batch 588, Loss 0.5137447714805603\n","[Training Epoch 1] Batch 589, Loss 0.5098252296447754\n","[Training Epoch 1] Batch 590, Loss 0.47513657808303833\n","[Training Epoch 1] Batch 591, Loss 0.49945253133773804\n","[Training Epoch 1] Batch 592, Loss 0.45770496129989624\n","[Training Epoch 1] Batch 593, Loss 0.5223075151443481\n","[Training Epoch 1] Batch 594, Loss 0.5186963081359863\n","[Training Epoch 1] Batch 595, Loss 0.49421778321266174\n","[Training Epoch 1] Batch 596, Loss 0.4832744896411896\n","[Training Epoch 1] Batch 597, Loss 0.48335930705070496\n","[Training Epoch 1] Batch 598, Loss 0.4968234896659851\n","[Training Epoch 1] Batch 599, Loss 0.49674224853515625\n","[Training Epoch 1] Batch 600, Loss 0.5004622340202332\n","[Training Epoch 1] Batch 601, Loss 0.4587055742740631\n","[Training Epoch 1] Batch 602, Loss 0.5153251886367798\n","[Training Epoch 1] Batch 603, Loss 0.5046710968017578\n","[Training Epoch 1] Batch 604, Loss 0.48037809133529663\n","[Training Epoch 1] Batch 605, Loss 0.4954094886779785\n","[Training Epoch 1] Batch 606, Loss 0.5005948543548584\n","[Training Epoch 1] Batch 607, Loss 0.5233733654022217\n","[Training Epoch 1] Batch 608, Loss 0.4776843190193176\n","[Training Epoch 1] Batch 609, Loss 0.5290242433547974\n","[Training Epoch 1] Batch 610, Loss 0.4910123348236084\n","[Training Epoch 1] Batch 611, Loss 0.4938336908817291\n","[Training Epoch 1] Batch 612, Loss 0.48989978432655334\n","[Training Epoch 1] Batch 613, Loss 0.4992073178291321\n","[Training Epoch 1] Batch 614, Loss 0.4939959645271301\n","[Training Epoch 1] Batch 615, Loss 0.5223275423049927\n","[Training Epoch 1] Batch 616, Loss 0.490767240524292\n","[Training Epoch 1] Batch 617, Loss 0.499561607837677\n","[Training Epoch 1] Batch 618, Loss 0.5154690146446228\n","[Training Epoch 1] Batch 619, Loss 0.5318155884742737\n","[Training Epoch 1] Batch 620, Loss 0.504505455493927\n","[Training Epoch 1] Batch 621, Loss 0.5133926868438721\n","[Training Epoch 1] Batch 622, Loss 0.5156673192977905\n","[Training Epoch 1] Batch 623, Loss 0.505997359752655\n","[Training Epoch 1] Batch 624, Loss 0.48043331503868103\n","[Training Epoch 1] Batch 625, Loss 0.49780189990997314\n","[Training Epoch 1] Batch 626, Loss 0.5030649900436401\n","[Training Epoch 1] Batch 627, Loss 0.48974543809890747\n","[Training Epoch 1] Batch 628, Loss 0.49124324321746826\n","[Training Epoch 1] Batch 629, Loss 0.4817148745059967\n","[Training Epoch 1] Batch 630, Loss 0.5163406133651733\n","[Training Epoch 1] Batch 631, Loss 0.5183824300765991\n","[Training Epoch 1] Batch 632, Loss 0.4915529489517212\n","[Training Epoch 1] Batch 633, Loss 0.49096277356147766\n","[Training Epoch 1] Batch 634, Loss 0.5223547220230103\n","[Training Epoch 1] Batch 635, Loss 0.48125791549682617\n","[Training Epoch 1] Batch 636, Loss 0.5195087790489197\n","[Training Epoch 1] Batch 637, Loss 0.4937429428100586\n","[Training Epoch 1] Batch 638, Loss 0.49228784441947937\n","[Training Epoch 1] Batch 639, Loss 0.49664533138275146\n","[Training Epoch 1] Batch 640, Loss 0.5131618976593018\n","[Training Epoch 1] Batch 641, Loss 0.4925960600376129\n","[Training Epoch 1] Batch 642, Loss 0.4963563084602356\n","[Training Epoch 1] Batch 643, Loss 0.47226542234420776\n","[Training Epoch 1] Batch 644, Loss 0.4869190454483032\n","[Training Epoch 1] Batch 645, Loss 0.5126124620437622\n","[Training Epoch 1] Batch 646, Loss 0.5020120739936829\n","[Training Epoch 1] Batch 647, Loss 0.4964267909526825\n","[Training Epoch 1] Batch 648, Loss 0.51185542345047\n","[Training Epoch 1] Batch 649, Loss 0.498052179813385\n","[Training Epoch 1] Batch 650, Loss 0.5047074556350708\n","[Training Epoch 1] Batch 651, Loss 0.4998073875904083\n","[Training Epoch 1] Batch 652, Loss 0.5399919152259827\n","[Training Epoch 1] Batch 653, Loss 0.48305001854896545\n","[Training Epoch 1] Batch 654, Loss 0.48324280977249146\n","[Training Epoch 1] Batch 655, Loss 0.47768068313598633\n","[Training Epoch 1] Batch 656, Loss 0.5130319595336914\n","[Training Epoch 1] Batch 657, Loss 0.5020345449447632\n","[Training Epoch 1] Batch 658, Loss 0.5210972428321838\n","[Training Epoch 1] Batch 659, Loss 0.48463764786720276\n","[Training Epoch 1] Batch 660, Loss 0.4914301335811615\n","[Training Epoch 1] Batch 661, Loss 0.49932605028152466\n","[Training Epoch 1] Batch 662, Loss 0.48822012543678284\n","[Training Epoch 1] Batch 663, Loss 0.495668888092041\n","[Training Epoch 1] Batch 664, Loss 0.5010429620742798\n","[Training Epoch 1] Batch 665, Loss 0.49938538670539856\n","[Training Epoch 1] Batch 666, Loss 0.5152454376220703\n","[Training Epoch 1] Batch 667, Loss 0.4951099455356598\n","[Training Epoch 1] Batch 668, Loss 0.5142040252685547\n","[Training Epoch 1] Batch 669, Loss 0.47496742010116577\n","[Training Epoch 1] Batch 670, Loss 0.49088335037231445\n","[Training Epoch 1] Batch 671, Loss 0.4940795600414276\n","[Training Epoch 1] Batch 672, Loss 0.49503177404403687\n","[Training Epoch 1] Batch 673, Loss 0.49949556589126587\n","[Training Epoch 1] Batch 674, Loss 0.5214760899543762\n","[Training Epoch 1] Batch 675, Loss 0.4722699522972107\n","[Training Epoch 1] Batch 676, Loss 0.4885396957397461\n","[Training Epoch 1] Batch 677, Loss 0.5006994605064392\n","[Training Epoch 1] Batch 678, Loss 0.4775617718696594\n","[Training Epoch 1] Batch 679, Loss 0.48691681027412415\n","[Training Epoch 1] Batch 680, Loss 0.4889010190963745\n","[Training Epoch 1] Batch 681, Loss 0.48445165157318115\n","[Training Epoch 1] Batch 682, Loss 0.4896448254585266\n","[Training Epoch 1] Batch 683, Loss 0.47632521390914917\n","[Training Epoch 1] Batch 684, Loss 0.5173624753952026\n","[Training Epoch 1] Batch 685, Loss 0.5294566750526428\n","[Training Epoch 1] Batch 686, Loss 0.4912438988685608\n","[Training Epoch 1] Batch 687, Loss 0.5129859447479248\n","[Training Epoch 1] Batch 688, Loss 0.5184786319732666\n","[Training Epoch 1] Batch 689, Loss 0.5117982625961304\n","[Training Epoch 1] Batch 690, Loss 0.4992022216320038\n","[Training Epoch 1] Batch 691, Loss 0.5086186528205872\n","[Training Epoch 1] Batch 692, Loss 0.4843236804008484\n","[Training Epoch 1] Batch 693, Loss 0.4846215844154358\n","[Training Epoch 1] Batch 694, Loss 0.4728250205516815\n","[Training Epoch 1] Batch 695, Loss 0.5105146169662476\n","[Training Epoch 1] Batch 696, Loss 0.488983690738678\n","[Training Epoch 1] Batch 697, Loss 0.4574086666107178\n","[Training Epoch 1] Batch 698, Loss 0.507236897945404\n","[Training Epoch 1] Batch 699, Loss 0.5194119811058044\n","[Training Epoch 1] Batch 700, Loss 0.5224279165267944\n","[Training Epoch 1] Batch 701, Loss 0.49395257234573364\n","[Training Epoch 1] Batch 702, Loss 0.5130669474601746\n","[Training Epoch 1] Batch 703, Loss 0.5308951735496521\n","[Training Epoch 1] Batch 704, Loss 0.503461480140686\n","[Training Epoch 1] Batch 705, Loss 0.4959826171398163\n","[Training Epoch 1] Batch 706, Loss 0.5111263990402222\n","[Training Epoch 1] Batch 707, Loss 0.5158010721206665\n","[Training Epoch 1] Batch 708, Loss 0.5034580230712891\n","[Training Epoch 1] Batch 709, Loss 0.4868758022785187\n","[Training Epoch 1] Batch 710, Loss 0.5090234279632568\n","[Training Epoch 1] Batch 711, Loss 0.5061444044113159\n","[Training Epoch 1] Batch 712, Loss 0.4883032739162445\n","[Training Epoch 1] Batch 713, Loss 0.510438859462738\n","[Training Epoch 1] Batch 714, Loss 0.5129473209381104\n","[Training Epoch 1] Batch 715, Loss 0.5103574395179749\n","[Training Epoch 1] Batch 716, Loss 0.45474082231521606\n","[Training Epoch 1] Batch 717, Loss 0.4774165749549866\n","[Training Epoch 1] Batch 718, Loss 0.5004244446754456\n","[Training Epoch 1] Batch 719, Loss 0.4748871326446533\n","[Training Epoch 1] Batch 720, Loss 0.4733266830444336\n","[Training Epoch 1] Batch 721, Loss 0.5101151466369629\n","[Training Epoch 1] Batch 722, Loss 0.4870101511478424\n","[Training Epoch 1] Batch 723, Loss 0.5049988627433777\n","[Training Epoch 1] Batch 724, Loss 0.5330120325088501\n","[Training Epoch 1] Batch 725, Loss 0.4925307631492615\n","[Training Epoch 1] Batch 726, Loss 0.48454660177230835\n","[Training Epoch 1] Batch 727, Loss 0.5207290053367615\n","[Training Epoch 1] Batch 728, Loss 0.4840475916862488\n","[Training Epoch 1] Batch 729, Loss 0.49248212575912476\n","[Training Epoch 1] Batch 730, Loss 0.5090922713279724\n","[Training Epoch 1] Batch 731, Loss 0.49946221709251404\n","[Training Epoch 1] Batch 732, Loss 0.5011067986488342\n","[Training Epoch 1] Batch 733, Loss 0.5336041450500488\n","[Training Epoch 1] Batch 734, Loss 0.46393340826034546\n","[Training Epoch 1] Batch 735, Loss 0.499394953250885\n","[Training Epoch 1] Batch 736, Loss 0.48737192153930664\n","[Training Epoch 1] Batch 737, Loss 0.4899866580963135\n","[Training Epoch 1] Batch 738, Loss 0.5007850527763367\n","[Training Epoch 1] Batch 739, Loss 0.49641257524490356\n","[Training Epoch 1] Batch 740, Loss 0.5210041403770447\n","[Training Epoch 1] Batch 741, Loss 0.46817639470100403\n","[Training Epoch 1] Batch 742, Loss 0.49259084463119507\n","[Training Epoch 1] Batch 743, Loss 0.5113531351089478\n","[Training Epoch 1] Batch 744, Loss 0.5199658274650574\n","[Training Epoch 1] Batch 745, Loss 0.5047816634178162\n","[Training Epoch 1] Batch 746, Loss 0.5141368508338928\n","[Training Epoch 1] Batch 747, Loss 0.5090317726135254\n","[Training Epoch 1] Batch 748, Loss 0.5089480876922607\n","[Training Epoch 1] Batch 749, Loss 0.4989868104457855\n","[Training Epoch 1] Batch 750, Loss 0.5117313265800476\n","[Training Epoch 1] Batch 751, Loss 0.5046840310096741\n","[Training Epoch 1] Batch 752, Loss 0.5262176990509033\n","[Training Epoch 1] Batch 753, Loss 0.5117718577384949\n","[Training Epoch 1] Batch 754, Loss 0.5114222764968872\n","[Training Epoch 1] Batch 755, Loss 0.4967503547668457\n","[Training Epoch 1] Batch 756, Loss 0.4935072362422943\n","[Training Epoch 1] Batch 757, Loss 0.46944504976272583\n","[Training Epoch 1] Batch 758, Loss 0.4978477954864502\n","[Training Epoch 1] Batch 759, Loss 0.49520570039749146\n","[Training Epoch 1] Batch 760, Loss 0.515723705291748\n","[Training Epoch 1] Batch 761, Loss 0.4870486259460449\n","[Training Epoch 1] Batch 762, Loss 0.4882959723472595\n","[Training Epoch 1] Batch 763, Loss 0.47620394825935364\n","[Training Epoch 1] Batch 764, Loss 0.5115131139755249\n","[Training Epoch 1] Batch 765, Loss 0.4979381561279297\n","[Training Epoch 1] Batch 766, Loss 0.4951106905937195\n","[Training Epoch 1] Batch 767, Loss 0.5035486221313477\n","[Training Epoch 1] Batch 768, Loss 0.480282187461853\n","[Training Epoch 1] Batch 769, Loss 0.4848921298980713\n","[Training Epoch 1] Batch 770, Loss 0.539151668548584\n","[Training Epoch 1] Batch 771, Loss 0.4869818091392517\n","[Training Epoch 1] Batch 772, Loss 0.5282714366912842\n","[Training Epoch 1] Batch 773, Loss 0.5180225372314453\n","[Training Epoch 1] Batch 774, Loss 0.5016762018203735\n","[Training Epoch 1] Batch 775, Loss 0.5057746767997742\n","[Training Epoch 1] Batch 776, Loss 0.519930362701416\n","[Training Epoch 1] Batch 777, Loss 0.4956372380256653\n","[Training Epoch 1] Batch 778, Loss 0.495464563369751\n","[Training Epoch 1] Batch 779, Loss 0.535545825958252\n","[Training Epoch 1] Batch 780, Loss 0.49024638533592224\n","[Training Epoch 1] Batch 781, Loss 0.5008461475372314\n","[Training Epoch 1] Batch 782, Loss 0.5145254135131836\n","[Training Epoch 1] Batch 783, Loss 0.5236982107162476\n","[Training Epoch 1] Batch 784, Loss 0.47195395827293396\n","[Training Epoch 1] Batch 785, Loss 0.48314422369003296\n","[Training Epoch 1] Batch 786, Loss 0.48452118039131165\n","[Training Epoch 1] Batch 787, Loss 0.48145025968551636\n","[Training Epoch 1] Batch 788, Loss 0.5157299041748047\n","[Training Epoch 1] Batch 789, Loss 0.4993535280227661\n","[Training Epoch 1] Batch 790, Loss 0.4750923812389374\n","[Training Epoch 1] Batch 791, Loss 0.4872381091117859\n","[Training Epoch 1] Batch 792, Loss 0.4572407007217407\n","[Training Epoch 1] Batch 793, Loss 0.5128698348999023\n","[Training Epoch 1] Batch 794, Loss 0.49269625544548035\n","[Training Epoch 1] Batch 795, Loss 0.49836021661758423\n","[Training Epoch 1] Batch 796, Loss 0.4465499520301819\n","[Training Epoch 1] Batch 797, Loss 0.5116010904312134\n","[Training Epoch 1] Batch 798, Loss 0.5009372234344482\n","[Training Epoch 1] Batch 799, Loss 0.5200451612472534\n","[Training Epoch 1] Batch 800, Loss 0.5046701431274414\n","[Training Epoch 1] Batch 801, Loss 0.5225369930267334\n","[Training Epoch 1] Batch 802, Loss 0.4505079388618469\n","[Training Epoch 1] Batch 803, Loss 0.47743096947669983\n","[Training Epoch 1] Batch 804, Loss 0.49506717920303345\n","[Training Epoch 1] Batch 805, Loss 0.49100789427757263\n","[Training Epoch 1] Batch 806, Loss 0.4843921959400177\n","[Training Epoch 1] Batch 807, Loss 0.5237273573875427\n","[Training Epoch 1] Batch 808, Loss 0.5222684144973755\n","[Training Epoch 1] Batch 809, Loss 0.4993663728237152\n","[Training Epoch 1] Batch 810, Loss 0.504821240901947\n","[Training Epoch 1] Batch 811, Loss 0.522517740726471\n","[Training Epoch 1] Batch 812, Loss 0.5139984488487244\n","[Training Epoch 1] Batch 813, Loss 0.525220513343811\n","[Training Epoch 1] Batch 814, Loss 0.5184537172317505\n","[Training Epoch 1] Batch 815, Loss 0.47890907526016235\n","[Training Epoch 1] Batch 816, Loss 0.5127518177032471\n","[Training Epoch 1] Batch 817, Loss 0.5035460591316223\n","[Training Epoch 1] Batch 818, Loss 0.4857306182384491\n","[Training Epoch 1] Batch 819, Loss 0.5129393935203552\n","[Training Epoch 1] Batch 820, Loss 0.5049829483032227\n","[Training Epoch 1] Batch 821, Loss 0.5198321342468262\n","[Training Epoch 1] Batch 822, Loss 0.4965274930000305\n","[Training Epoch 1] Batch 823, Loss 0.4979594945907593\n","[Training Epoch 1] Batch 824, Loss 0.4749378263950348\n","[Training Epoch 1] Batch 825, Loss 0.49246281385421753\n","[Training Epoch 1] Batch 826, Loss 0.5086839199066162\n","[Training Epoch 1] Batch 827, Loss 0.510037899017334\n","[Training Epoch 1] Batch 828, Loss 0.4640812575817108\n","[Training Epoch 1] Batch 829, Loss 0.5047513246536255\n","[Training Epoch 1] Batch 830, Loss 0.47769343852996826\n","[Training Epoch 1] Batch 831, Loss 0.4929209351539612\n","[Training Epoch 1] Batch 832, Loss 0.5386557579040527\n","[Training Epoch 1] Batch 833, Loss 0.5332463383674622\n","[Training Epoch 1] Batch 834, Loss 0.4778507351875305\n","[Training Epoch 1] Batch 835, Loss 0.4653005003929138\n","[Training Epoch 1] Batch 836, Loss 0.489685595035553\n","[Training Epoch 1] Batch 837, Loss 0.4996952414512634\n","[Training Epoch 1] Batch 838, Loss 0.4790722131729126\n","[Training Epoch 1] Batch 839, Loss 0.4789077937602997\n","[Training Epoch 1] Batch 840, Loss 0.5537105202674866\n","[Training Epoch 1] Batch 841, Loss 0.5440946817398071\n","[Training Epoch 1] Batch 842, Loss 0.5087226033210754\n","[Training Epoch 1] Batch 843, Loss 0.5222792029380798\n","[Training Epoch 1] Batch 844, Loss 0.49926668405532837\n","[Training Epoch 1] Batch 845, Loss 0.4815337061882019\n","[Training Epoch 1] Batch 846, Loss 0.46534866094589233\n","[Training Epoch 1] Batch 847, Loss 0.4995172619819641\n","[Training Epoch 1] Batch 848, Loss 0.4951450228691101\n","[Training Epoch 1] Batch 849, Loss 0.49503594636917114\n","[Training Epoch 1] Batch 850, Loss 0.5035773515701294\n","[Training Epoch 1] Batch 851, Loss 0.48970848321914673\n","[Training Epoch 1] Batch 852, Loss 0.47101765871047974\n","[Training Epoch 1] Batch 853, Loss 0.5143216848373413\n","[Training Epoch 1] Batch 854, Loss 0.4912518262863159\n","[Training Epoch 1] Batch 855, Loss 0.522404134273529\n","[Training Epoch 1] Batch 856, Loss 0.5020085573196411\n","[Training Epoch 1] Batch 857, Loss 0.4803149998188019\n","[Training Epoch 1] Batch 858, Loss 0.46939289569854736\n","[Training Epoch 1] Batch 859, Loss 0.4926830232143402\n","[Training Epoch 1] Batch 860, Loss 0.4875568747520447\n","[Training Epoch 1] Batch 861, Loss 0.5114254355430603\n","[Training Epoch 1] Batch 862, Loss 0.47740042209625244\n","[Training Epoch 1] Batch 863, Loss 0.5292314291000366\n","[Training Epoch 1] Batch 864, Loss 0.48976439237594604\n","[Training Epoch 1] Batch 865, Loss 0.4696093201637268\n","[Training Epoch 1] Batch 866, Loss 0.4841446280479431\n","[Training Epoch 1] Batch 867, Loss 0.5004492998123169\n","[Training Epoch 1] Batch 868, Loss 0.5018817186355591\n","[Training Epoch 1] Batch 869, Loss 0.49806392192840576\n","[Training Epoch 1] Batch 870, Loss 0.519288957118988\n","[Training Epoch 1] Batch 871, Loss 0.5279538631439209\n","[Training Epoch 1] Batch 872, Loss 0.4694902300834656\n","[Training Epoch 1] Batch 873, Loss 0.500633955001831\n","[Training Epoch 1] Batch 874, Loss 0.5046300292015076\n","[Training Epoch 1] Batch 875, Loss 0.503288984298706\n","[Training Epoch 1] Batch 876, Loss 0.5019194483757019\n","[Training Epoch 1] Batch 877, Loss 0.5062812566757202\n","[Training Epoch 1] Batch 878, Loss 0.4625181555747986\n","[Training Epoch 1] Batch 879, Loss 0.5090051889419556\n","[Training Epoch 1] Batch 880, Loss 0.5171856880187988\n","[Training Epoch 1] Batch 881, Loss 0.49426034092903137\n","[Training Epoch 1] Batch 882, Loss 0.5264970660209656\n","[Training Epoch 1] Batch 883, Loss 0.48306334018707275\n","[Training Epoch 1] Batch 884, Loss 0.49550753831863403\n","[Training Epoch 1] Batch 885, Loss 0.4843406081199646\n","[Training Epoch 1] Batch 886, Loss 0.48981305956840515\n","[Training Epoch 1] Batch 887, Loss 0.4804895520210266\n","[Training Epoch 1] Batch 888, Loss 0.5074583888053894\n","[Training Epoch 1] Batch 889, Loss 0.5152873992919922\n","[Training Epoch 1] Batch 890, Loss 0.5006551146507263\n","[Training Epoch 1] Batch 891, Loss 0.5113838911056519\n","[Training Epoch 1] Batch 892, Loss 0.5090739727020264\n","[Training Epoch 1] Batch 893, Loss 0.5032348036766052\n","[Training Epoch 1] Batch 894, Loss 0.4870569705963135\n","[Training Epoch 1] Batch 895, Loss 0.4950236678123474\n","[Training Epoch 1] Batch 896, Loss 0.5213404893875122\n","[Training Epoch 1] Batch 897, Loss 0.5101990699768066\n","[Training Epoch 1] Batch 898, Loss 0.49822553992271423\n","[Training Epoch 1] Batch 899, Loss 0.5264876484870911\n","[Training Epoch 1] Batch 900, Loss 0.4826657176017761\n","[Training Epoch 1] Batch 901, Loss 0.4859018325805664\n","[Training Epoch 1] Batch 902, Loss 0.49415960907936096\n","[Training Epoch 1] Batch 903, Loss 0.4761095643043518\n","[Training Epoch 1] Batch 904, Loss 0.5212510824203491\n","[Training Epoch 1] Batch 905, Loss 0.5009374618530273\n","[Training Epoch 1] Batch 906, Loss 0.48766303062438965\n","[Training Epoch 1] Batch 907, Loss 0.5076645016670227\n","[Training Epoch 1] Batch 908, Loss 0.5026000738143921\n","[Training Epoch 1] Batch 909, Loss 0.5021189451217651\n","[Training Epoch 1] Batch 910, Loss 0.5165361762046814\n","[Training Epoch 1] Batch 911, Loss 0.5222481489181519\n","[Training Epoch 1] Batch 912, Loss 0.5004231333732605\n","[Training Epoch 1] Batch 913, Loss 0.4951331615447998\n","[Training Epoch 1] Batch 914, Loss 0.48135465383529663\n","[Training Epoch 1] Batch 915, Loss 0.5210059881210327\n","[Training Epoch 1] Batch 916, Loss 0.5198306441307068\n","[Training Epoch 1] Batch 917, Loss 0.477860689163208\n","[Training Epoch 1] Batch 918, Loss 0.46970057487487793\n","[Training Epoch 1] Batch 919, Loss 0.4818528890609741\n","[Training Epoch 1] Batch 920, Loss 0.5034593939781189\n","[Training Epoch 1] Batch 921, Loss 0.4938356280326843\n","[Training Epoch 1] Batch 922, Loss 0.49508538842201233\n","[Training Epoch 1] Batch 923, Loss 0.5032782554626465\n","[Training Epoch 1] Batch 924, Loss 0.4966975450515747\n","[Training Epoch 1] Batch 925, Loss 0.49127161502838135\n","[Training Epoch 1] Batch 926, Loss 0.48864424228668213\n","[Training Epoch 1] Batch 927, Loss 0.48482537269592285\n","[Training Epoch 1] Batch 928, Loss 0.46557578444480896\n","[Training Epoch 1] Batch 929, Loss 0.4925563931465149\n","[Training Epoch 1] Batch 930, Loss 0.488437294960022\n","[Training Epoch 1] Batch 931, Loss 0.4801100492477417\n","[Training Epoch 1] Batch 932, Loss 0.4890158176422119\n","[Training Epoch 1] Batch 933, Loss 0.5074957609176636\n","[Training Epoch 1] Batch 934, Loss 0.48864686489105225\n","[Training Epoch 1] Batch 935, Loss 0.4829385578632355\n","[Training Epoch 1] Batch 936, Loss 0.4845304787158966\n","[Training Epoch 1] Batch 937, Loss 0.4804416596889496\n","[Training Epoch 1] Batch 938, Loss 0.48434358835220337\n","[Training Epoch 1] Batch 939, Loss 0.49261152744293213\n","[Training Epoch 1] Batch 940, Loss 0.4747735857963562\n","[Training Epoch 1] Batch 941, Loss 0.5019034147262573\n","[Training Epoch 1] Batch 942, Loss 0.5319284200668335\n","[Training Epoch 1] Batch 943, Loss 0.48991650342941284\n","[Training Epoch 1] Batch 944, Loss 0.5088845491409302\n","[Training Epoch 1] Batch 945, Loss 0.5034385919570923\n","[Training Epoch 1] Batch 946, Loss 0.5183613300323486\n","[Training Epoch 1] Batch 947, Loss 0.4883175194263458\n","[Training Epoch 1] Batch 948, Loss 0.48551806807518005\n","[Training Epoch 1] Batch 949, Loss 0.49242329597473145\n","[Training Epoch 1] Batch 950, Loss 0.5197531580924988\n","[Training Epoch 1] Batch 951, Loss 0.5347083806991577\n","[Training Epoch 1] Batch 952, Loss 0.5102914571762085\n","[Training Epoch 1] Batch 953, Loss 0.49965399503707886\n","[Training Epoch 1] Batch 954, Loss 0.5091638565063477\n","[Training Epoch 1] Batch 955, Loss 0.5117673277854919\n","[Training Epoch 1] Batch 956, Loss 0.5101461410522461\n","[Training Epoch 1] Batch 957, Loss 0.4928530156612396\n","[Training Epoch 1] Batch 958, Loss 0.5210133194923401\n","[Training Epoch 1] Batch 959, Loss 0.46934258937835693\n","[Training Epoch 1] Batch 960, Loss 0.5179188251495361\n","[Training Epoch 1] Batch 961, Loss 0.4857887029647827\n","[Training Epoch 1] Batch 962, Loss 0.4755990505218506\n","[Training Epoch 1] Batch 963, Loss 0.48819538950920105\n","[Training Epoch 1] Batch 964, Loss 0.48055511713027954\n","[Training Epoch 1] Batch 965, Loss 0.5073630809783936\n","[Training Epoch 1] Batch 966, Loss 0.5267236828804016\n","[Training Epoch 1] Batch 967, Loss 0.5062889456748962\n","[Training Epoch 1] Batch 968, Loss 0.4641677141189575\n","[Training Epoch 1] Batch 969, Loss 0.48461952805519104\n","[Training Epoch 1] Batch 970, Loss 0.4862257242202759\n","[Training Epoch 1] Batch 971, Loss 0.4911646842956543\n","[Training Epoch 1] Batch 972, Loss 0.5183377265930176\n","[Training Epoch 1] Batch 973, Loss 0.5228987336158752\n","[Training Epoch 1] Batch 974, Loss 0.5100142359733582\n","[Training Epoch 1] Batch 975, Loss 0.510460615158081\n","[Training Epoch 1] Batch 976, Loss 0.5292791128158569\n","[Training Epoch 1] Batch 977, Loss 0.5196617841720581\n","[Training Epoch 1] Batch 978, Loss 0.5047792196273804\n","[Training Epoch 1] Batch 979, Loss 0.491119384765625\n","[Training Epoch 1] Batch 980, Loss 0.4816727042198181\n","[Training Epoch 1] Batch 981, Loss 0.518515944480896\n","[Training Epoch 1] Batch 982, Loss 0.5074495673179626\n","[Training Epoch 1] Batch 983, Loss 0.5200125575065613\n","[Training Epoch 1] Batch 984, Loss 0.5399564504623413\n","[Training Epoch 1] Batch 985, Loss 0.5168406367301941\n","[Training Epoch 1] Batch 986, Loss 0.504610002040863\n","[Training Epoch 1] Batch 987, Loss 0.5011060833930969\n","[Training Epoch 1] Batch 988, Loss 0.49520909786224365\n","[Training Epoch 1] Batch 989, Loss 0.5180498361587524\n","[Training Epoch 1] Batch 990, Loss 0.5033824443817139\n","[Training Epoch 1] Batch 991, Loss 0.4991534650325775\n","[Training Epoch 1] Batch 992, Loss 0.49251455068588257\n","[Training Epoch 1] Batch 993, Loss 0.5004708766937256\n","[Training Epoch 1] Batch 994, Loss 0.5005377531051636\n","[Training Epoch 1] Batch 995, Loss 0.5010256171226501\n","[Training Epoch 1] Batch 996, Loss 0.512886643409729\n","[Training Epoch 1] Batch 997, Loss 0.5143095254898071\n","[Training Epoch 1] Batch 998, Loss 0.520958423614502\n","[Training Epoch 1] Batch 999, Loss 0.5071314573287964\n","[Training Epoch 1] Batch 1000, Loss 0.4885771572589874\n","[Training Epoch 1] Batch 1001, Loss 0.4774309992790222\n","[Training Epoch 1] Batch 1002, Loss 0.5456777811050415\n","[Training Epoch 1] Batch 1003, Loss 0.5428721904754639\n","[Training Epoch 1] Batch 1004, Loss 0.5008951425552368\n","[Training Epoch 1] Batch 1005, Loss 0.5186520218849182\n","[Training Epoch 1] Batch 1006, Loss 0.5307332277297974\n","[Training Epoch 1] Batch 1007, Loss 0.4844638407230377\n","[Training Epoch 1] Batch 1008, Loss 0.4870007336139679\n","[Training Epoch 1] Batch 1009, Loss 0.5035958290100098\n","[Training Epoch 1] Batch 1010, Loss 0.5155421495437622\n","[Training Epoch 1] Batch 1011, Loss 0.4803504943847656\n","[Training Epoch 1] Batch 1012, Loss 0.49391603469848633\n","[Training Epoch 1] Batch 1013, Loss 0.5139589905738831\n","[Training Epoch 1] Batch 1014, Loss 0.5180230140686035\n","[Training Epoch 1] Batch 1015, Loss 0.46856772899627686\n","[Training Epoch 1] Batch 1016, Loss 0.4749469459056854\n","[Training Epoch 1] Batch 1017, Loss 0.4860224425792694\n","[Training Epoch 1] Batch 1018, Loss 0.5037616491317749\n","[Training Epoch 1] Batch 1019, Loss 0.4980114698410034\n","[Training Epoch 1] Batch 1020, Loss 0.4909980893135071\n","[Training Epoch 1] Batch 1021, Loss 0.4802372455596924\n","[Training Epoch 1] Batch 1022, Loss 0.5088071227073669\n","[Training Epoch 1] Batch 1023, Loss 0.49650874733924866\n","[Training Epoch 1] Batch 1024, Loss 0.4895845651626587\n","[Training Epoch 1] Batch 1025, Loss 0.500950813293457\n","[Training Epoch 1] Batch 1026, Loss 0.5197133421897888\n","[Training Epoch 1] Batch 1027, Loss 0.4967494308948517\n","[Training Epoch 1] Batch 1028, Loss 0.47063499689102173\n","[Training Epoch 1] Batch 1029, Loss 0.5318732261657715\n","[Training Epoch 1] Batch 1030, Loss 0.5170555114746094\n","[Training Epoch 1] Batch 1031, Loss 0.5088597536087036\n","[Training Epoch 1] Batch 1032, Loss 0.5156553983688354\n","[Training Epoch 1] Batch 1033, Loss 0.5181922912597656\n","[Training Epoch 1] Batch 1034, Loss 0.5198911428451538\n","[Training Epoch 1] Batch 1035, Loss 0.480541855096817\n","[Training Epoch 1] Batch 1036, Loss 0.49512699246406555\n","[Training Epoch 1] Batch 1037, Loss 0.514112114906311\n","[Training Epoch 1] Batch 1038, Loss 0.481680691242218\n","[Training Epoch 1] Batch 1039, Loss 0.48970532417297363\n","[Training Epoch 1] Batch 1040, Loss 0.48066383600234985\n","[Training Epoch 1] Batch 1041, Loss 0.47207462787628174\n","[Training Epoch 1] Batch 1042, Loss 0.5088114738464355\n","[Training Epoch 1] Batch 1043, Loss 0.48865240812301636\n","[Training Epoch 1] Batch 1044, Loss 0.4776526987552643\n","[Training Epoch 1] Batch 1045, Loss 0.5059173107147217\n","[Training Epoch 1] Batch 1046, Loss 0.49083617329597473\n","[Training Epoch 1] Batch 1047, Loss 0.4893401265144348\n","[Training Epoch 1] Batch 1048, Loss 0.4709218144416809\n","[Training Epoch 1] Batch 1049, Loss 0.5033198595046997\n","[Training Epoch 1] Batch 1050, Loss 0.5074578523635864\n","[Training Epoch 1] Batch 1051, Loss 0.5196326971054077\n","[Training Epoch 1] Batch 1052, Loss 0.49659761786460876\n","[Training Epoch 1] Batch 1053, Loss 0.5308500528335571\n","[Training Epoch 1] Batch 1054, Loss 0.5020592212677002\n","[Training Epoch 1] Batch 1055, Loss 0.4954776167869568\n","[Training Epoch 1] Batch 1056, Loss 0.5091341733932495\n","[Training Epoch 1] Batch 1057, Loss 0.5196857452392578\n","[Training Epoch 1] Batch 1058, Loss 0.507805585861206\n","[Training Epoch 1] Batch 1059, Loss 0.501969039440155\n","[Training Epoch 1] Batch 1060, Loss 0.5111709833145142\n","[Training Epoch 1] Batch 1061, Loss 0.5479012727737427\n","[Training Epoch 1] Batch 1062, Loss 0.4980557858943939\n","[Training Epoch 1] Batch 1063, Loss 0.48192980885505676\n","[Training Epoch 1] Batch 1064, Loss 0.5022183656692505\n","[Training Epoch 1] Batch 1065, Loss 0.5009022355079651\n","[Training Epoch 1] Batch 1066, Loss 0.4316449463367462\n","[Training Epoch 1] Batch 1067, Loss 0.5126770734786987\n","[Training Epoch 1] Batch 1068, Loss 0.5201898813247681\n","[Training Epoch 1] Batch 1069, Loss 0.5062012672424316\n","[Training Epoch 1] Batch 1070, Loss 0.5185322761535645\n","[Training Epoch 1] Batch 1071, Loss 0.492831826210022\n","[Training Epoch 1] Batch 1072, Loss 0.529668927192688\n","[Training Epoch 1] Batch 1073, Loss 0.5073351860046387\n","[Training Epoch 1] Batch 1074, Loss 0.5100518465042114\n","[Training Epoch 1] Batch 1075, Loss 0.5100088119506836\n","[Training Epoch 1] Batch 1076, Loss 0.5181032419204712\n","[Training Epoch 1] Batch 1077, Loss 0.519713819026947\n","[Training Epoch 1] Batch 1078, Loss 0.499433308839798\n","[Training Epoch 1] Batch 1079, Loss 0.49539244174957275\n","[Training Epoch 1] Batch 1080, Loss 0.5153602361679077\n","[Training Epoch 1] Batch 1081, Loss 0.5077990293502808\n","[Training Epoch 1] Batch 1082, Loss 0.5020450949668884\n","[Training Epoch 1] Batch 1083, Loss 0.5046820044517517\n","[Training Epoch 1] Batch 1084, Loss 0.47789686918258667\n","[Training Epoch 1] Batch 1085, Loss 0.49428826570510864\n","[Training Epoch 1] Batch 1086, Loss 0.49667036533355713\n","[Training Epoch 1] Batch 1087, Loss 0.5210535526275635\n","[Training Epoch 1] Batch 1088, Loss 0.48314768075942993\n","[Training Epoch 1] Batch 1089, Loss 0.5208373069763184\n","[Training Epoch 1] Batch 1090, Loss 0.5192571878433228\n","[Training Epoch 1] Batch 1091, Loss 0.505973756313324\n","[Training Epoch 1] Batch 1092, Loss 0.49383509159088135\n","[Training Epoch 1] Batch 1093, Loss 0.5209749937057495\n","[Training Epoch 1] Batch 1094, Loss 0.4844585061073303\n","[Training Epoch 1] Batch 1095, Loss 0.49529001116752625\n","[Training Epoch 1] Batch 1096, Loss 0.4711529612541199\n","[Training Epoch 1] Batch 1097, Loss 0.5154988765716553\n","[Training Epoch 1] Batch 1098, Loss 0.4843757748603821\n","[Training Epoch 1] Batch 1099, Loss 0.5117177963256836\n","[Training Epoch 1] Batch 1100, Loss 0.49080410599708557\n","[Training Epoch 1] Batch 1101, Loss 0.5250396132469177\n","[Training Epoch 1] Batch 1102, Loss 0.5117696523666382\n","[Training Epoch 1] Batch 1103, Loss 0.4819665551185608\n","[Training Epoch 1] Batch 1104, Loss 0.4954254925251007\n","[Training Epoch 1] Batch 1105, Loss 0.5003625154495239\n","[Training Epoch 1] Batch 1106, Loss 0.47488656640052795\n","[Training Epoch 1] Batch 1107, Loss 0.5169293880462646\n","[Training Epoch 1] Batch 1108, Loss 0.5331066250801086\n","[Training Epoch 1] Batch 1109, Loss 0.4790036678314209\n","[Training Epoch 1] Batch 1110, Loss 0.5070611238479614\n","[Training Epoch 1] Batch 1111, Loss 0.5006549954414368\n","[Training Epoch 1] Batch 1112, Loss 0.5030149221420288\n","[Training Epoch 1] Batch 1113, Loss 0.5061690807342529\n","[Training Epoch 1] Batch 1114, Loss 0.5181413888931274\n","[Training Epoch 1] Batch 1115, Loss 0.47993579506874084\n","[Training Epoch 1] Batch 1116, Loss 0.5272260904312134\n","[Training Epoch 1] Batch 1117, Loss 0.4957835376262665\n","[Training Epoch 1] Batch 1118, Loss 0.5009087920188904\n","[Training Epoch 1] Batch 1119, Loss 0.5130704641342163\n","[Training Epoch 1] Batch 1120, Loss 0.5180771350860596\n","[Training Epoch 1] Batch 1121, Loss 0.5255545377731323\n","[Training Epoch 1] Batch 1122, Loss 0.49935686588287354\n","[Training Epoch 1] Batch 1123, Loss 0.5329596400260925\n","[Training Epoch 1] Batch 1124, Loss 0.4992787837982178\n","[Training Epoch 1] Batch 1125, Loss 0.48147648572921753\n","[Training Epoch 1] Batch 1126, Loss 0.5140650272369385\n","[Training Epoch 1] Batch 1127, Loss 0.4945247769355774\n","[Training Epoch 1] Batch 1128, Loss 0.5372428894042969\n","[Training Epoch 1] Batch 1129, Loss 0.4995787739753723\n","[Training Epoch 1] Batch 1130, Loss 0.5373539924621582\n","[Training Epoch 1] Batch 1131, Loss 0.47941213846206665\n","[Training Epoch 1] Batch 1132, Loss 0.5301814079284668\n","[Training Epoch 1] Batch 1133, Loss 0.5008373260498047\n","[Training Epoch 1] Batch 1134, Loss 0.5111982822418213\n","[Training Epoch 1] Batch 1135, Loss 0.5262496471405029\n","[Training Epoch 1] Batch 1136, Loss 0.5143844485282898\n","[Training Epoch 1] Batch 1137, Loss 0.48737671971321106\n","[Training Epoch 1] Batch 1138, Loss 0.4988159239292145\n","[Training Epoch 1] Batch 1139, Loss 0.5176700949668884\n","[Training Epoch 1] Batch 1140, Loss 0.5266000628471375\n","[Training Epoch 1] Batch 1141, Loss 0.522433876991272\n","[Training Epoch 1] Batch 1142, Loss 0.49289828538894653\n","[Training Epoch 1] Batch 1143, Loss 0.5115513205528259\n","[Training Epoch 1] Batch 1144, Loss 0.5367581248283386\n","[Training Epoch 1] Batch 1145, Loss 0.5058334469795227\n","[Training Epoch 1] Batch 1146, Loss 0.4874826669692993\n","[Training Epoch 1] Batch 1147, Loss 0.4967629313468933\n","[Training Epoch 1] Batch 1148, Loss 0.5018718242645264\n","[Training Epoch 1] Batch 1149, Loss 0.4969711899757385\n","[Training Epoch 1] Batch 1150, Loss 0.4990965723991394\n","[Training Epoch 1] Batch 1151, Loss 0.5007421374320984\n","[Training Epoch 1] Batch 1152, Loss 0.4842449724674225\n","[Training Epoch 1] Batch 1153, Loss 0.495813250541687\n","[Training Epoch 1] Batch 1154, Loss 0.5207536220550537\n","[Training Epoch 1] Batch 1155, Loss 0.4951459765434265\n","[Training Epoch 1] Batch 1156, Loss 0.49149176478385925\n","[Training Epoch 1] Batch 1157, Loss 0.49429869651794434\n","[Training Epoch 1] Batch 1158, Loss 0.47758060693740845\n","[Training Epoch 1] Batch 1159, Loss 0.5054754018783569\n","[Training Epoch 1] Batch 1160, Loss 0.5061171650886536\n","[Training Epoch 1] Batch 1161, Loss 0.549036979675293\n","[Training Epoch 1] Batch 1162, Loss 0.5016040205955505\n","[Training Epoch 1] Batch 1163, Loss 0.5035982131958008\n","[Training Epoch 1] Batch 1164, Loss 0.4888915419578552\n","[Training Epoch 1] Batch 1165, Loss 0.5088179111480713\n","[Training Epoch 1] Batch 1166, Loss 0.47421956062316895\n","[Training Epoch 1] Batch 1167, Loss 0.5115966796875\n","[Training Epoch 1] Batch 1168, Loss 0.4863314926624298\n","[Training Epoch 1] Batch 1169, Loss 0.4911060333251953\n","[Training Epoch 1] Batch 1170, Loss 0.49855154752731323\n","[Training Epoch 1] Batch 1171, Loss 0.48588746786117554\n","[Training Epoch 1] Batch 1172, Loss 0.4995259940624237\n","[Training Epoch 1] Batch 1173, Loss 0.5037349462509155\n","[Training Epoch 1] Batch 1174, Loss 0.5288680791854858\n","[Training Epoch 1] Batch 1175, Loss 0.4924091398715973\n","[Training Epoch 1] Batch 1176, Loss 0.49266117811203003\n","[Training Epoch 1] Batch 1177, Loss 0.4964786171913147\n","[Training Epoch 1] Batch 1178, Loss 0.550019383430481\n","[Training Epoch 1] Batch 1179, Loss 0.4925369620323181\n","[Training Epoch 1] Batch 1180, Loss 0.4898267686367035\n","[Training Epoch 1] Batch 1181, Loss 0.5006533861160278\n","[Training Epoch 1] Batch 1182, Loss 0.50090092420578\n","[Training Epoch 1] Batch 1183, Loss 0.4993867576122284\n","[Training Epoch 1] Batch 1184, Loss 0.527513325214386\n","[Training Epoch 1] Batch 1185, Loss 0.4818905293941498\n","[Training Epoch 1] Batch 1186, Loss 0.4766506552696228\n","[Training Epoch 1] Batch 1187, Loss 0.48738235235214233\n","[Training Epoch 1] Batch 1188, Loss 0.49396103620529175\n","[Training Epoch 1] Batch 1189, Loss 0.5164439678192139\n","[Training Epoch 1] Batch 1190, Loss 0.5154086947441101\n","[Training Epoch 1] Batch 1191, Loss 0.47124868631362915\n","[Training Epoch 1] Batch 1192, Loss 0.5048173069953918\n","[Training Epoch 1] Batch 1193, Loss 0.5007675290107727\n","[Training Epoch 1] Batch 1194, Loss 0.5063526630401611\n","[Training Epoch 1] Batch 1195, Loss 0.49660724401474\n","[Training Epoch 1] Batch 1196, Loss 0.4910610616207123\n","[Training Epoch 1] Batch 1197, Loss 0.4952557384967804\n","[Training Epoch 1] Batch 1198, Loss 0.49957239627838135\n","[Training Epoch 1] Batch 1199, Loss 0.5069257616996765\n","[Training Epoch 1] Batch 1200, Loss 0.5011087656021118\n","[Training Epoch 1] Batch 1201, Loss 0.5022387504577637\n","[Training Epoch 1] Batch 1202, Loss 0.5020214915275574\n","[Training Epoch 1] Batch 1203, Loss 0.5146135687828064\n","[Training Epoch 1] Batch 1204, Loss 0.5072126388549805\n","[Training Epoch 1] Batch 1205, Loss 0.5103480219841003\n","[Training Epoch 1] Batch 1206, Loss 0.5087932348251343\n","[Training Epoch 1] Batch 1207, Loss 0.48007678985595703\n","[Training Epoch 1] Batch 1208, Loss 0.48967036604881287\n","[Training Epoch 1] Batch 1209, Loss 0.47634100914001465\n","[Training Epoch 1] Batch 1210, Loss 0.5246909260749817\n","[Training Epoch 1] Batch 1211, Loss 0.4872382581233978\n","[Training Epoch 1] Batch 1212, Loss 0.49895501136779785\n","[Training Epoch 1] Batch 1213, Loss 0.5101032853126526\n","[Training Epoch 1] Batch 1214, Loss 0.4807504713535309\n","[Training Epoch 1] Batch 1215, Loss 0.5036321878433228\n","[Training Epoch 1] Batch 1216, Loss 0.5224682092666626\n","[Training Epoch 1] Batch 1217, Loss 0.4791843891143799\n","[Training Epoch 1] Batch 1218, Loss 0.4738374352455139\n","[Training Epoch 1] Batch 1219, Loss 0.5046991109848022\n","[Training Epoch 1] Batch 1220, Loss 0.513137936592102\n","[Training Epoch 1] Batch 1221, Loss 0.49828553199768066\n","[Training Epoch 1] Batch 1222, Loss 0.48881298303604126\n","[Training Epoch 1] Batch 1223, Loss 0.5085676312446594\n","[Training Epoch 1] Batch 1224, Loss 0.4966370463371277\n","[Training Epoch 1] Batch 1225, Loss 0.5356433391571045\n","[Training Epoch 1] Batch 1226, Loss 0.5223665237426758\n","[Training Epoch 1] Batch 1227, Loss 0.506305456161499\n","[Training Epoch 1] Batch 1228, Loss 0.5397640466690063\n","[Training Epoch 1] Batch 1229, Loss 0.4801240861415863\n","[Training Epoch 1] Batch 1230, Loss 0.5153428912162781\n","[Training Epoch 1] Batch 1231, Loss 0.5155335664749146\n","[Training Epoch 1] Batch 1232, Loss 0.5070581436157227\n","[Training Epoch 1] Batch 1233, Loss 0.5168672204017639\n","[Training Epoch 1] Batch 1234, Loss 0.4951394498348236\n","[Training Epoch 1] Batch 1235, Loss 0.4897530674934387\n","[Training Epoch 1] Batch 1236, Loss 0.5096537470817566\n","[Training Epoch 1] Batch 1237, Loss 0.49810659885406494\n","[Training Epoch 1] Batch 1238, Loss 0.5100492835044861\n","[Training Epoch 1] Batch 1239, Loss 0.5073326230049133\n","[Training Epoch 1] Batch 1240, Loss 0.4738018214702606\n","[Training Epoch 1] Batch 1241, Loss 0.4886285364627838\n","[Training Epoch 1] Batch 1242, Loss 0.49503761529922485\n","[Training Epoch 1] Batch 1243, Loss 0.49156028032302856\n","[Training Epoch 1] Batch 1244, Loss 0.48730483651161194\n","[Training Epoch 1] Batch 1245, Loss 0.5181429386138916\n","[Training Epoch 1] Batch 1246, Loss 0.5138259530067444\n","[Training Epoch 1] Batch 1247, Loss 0.5081133246421814\n","[Training Epoch 1] Batch 1248, Loss 0.47795578837394714\n","[Training Epoch 1] Batch 1249, Loss 0.5007220506668091\n","[Training Epoch 1] Batch 1250, Loss 0.5167023539543152\n","[Training Epoch 1] Batch 1251, Loss 0.49267563223838806\n","[Training Epoch 1] Batch 1252, Loss 0.500575065612793\n","[Training Epoch 1] Batch 1253, Loss 0.5073987245559692\n","[Training Epoch 1] Batch 1254, Loss 0.5091091394424438\n","[Training Epoch 1] Batch 1255, Loss 0.522553563117981\n","[Training Epoch 1] Batch 1256, Loss 0.5048995018005371\n","[Training Epoch 1] Batch 1257, Loss 0.5072504281997681\n","[Training Epoch 1] Batch 1258, Loss 0.48185956478118896\n","[Training Epoch 1] Batch 1259, Loss 0.49106645584106445\n","[Training Epoch 1] Batch 1260, Loss 0.5182547569274902\n","[Training Epoch 1] Batch 1261, Loss 0.48175984621047974\n","[Training Epoch 1] Batch 1262, Loss 0.4897892475128174\n","[Training Epoch 1] Batch 1263, Loss 0.4848936200141907\n","[Training Epoch 1] Batch 1264, Loss 0.491275817155838\n","[Training Epoch 1] Batch 1265, Loss 0.4888611435890198\n","[Training Epoch 1] Batch 1266, Loss 0.4885461926460266\n","[Training Epoch 1] Batch 1267, Loss 0.49520760774612427\n","[Training Epoch 1] Batch 1268, Loss 0.5275530219078064\n","[Training Epoch 1] Batch 1269, Loss 0.5062175989151001\n","[Training Epoch 1] Batch 1270, Loss 0.496853232383728\n","[Training Epoch 1] Batch 1271, Loss 0.47803834080696106\n","[Training Epoch 1] Batch 1272, Loss 0.47991877794265747\n","[Training Epoch 1] Batch 1273, Loss 0.5125566720962524\n","[Training Epoch 1] Batch 1274, Loss 0.4917173385620117\n","[Training Epoch 1] Batch 1275, Loss 0.5052237510681152\n","[Training Epoch 1] Batch 1276, Loss 0.49381223320961\n","[Training Epoch 1] Batch 1277, Loss 0.48040392994880676\n","[Training Epoch 1] Batch 1278, Loss 0.523786723613739\n","[Training Epoch 1] Batch 1279, Loss 0.474862277507782\n","[Training Epoch 1] Batch 1280, Loss 0.47604209184646606\n","[Training Epoch 1] Batch 1281, Loss 0.49808043241500854\n","[Training Epoch 1] Batch 1282, Loss 0.49039703607559204\n","[Training Epoch 1] Batch 1283, Loss 0.5177878141403198\n","[Training Epoch 1] Batch 1284, Loss 0.5019890666007996\n","[Training Epoch 1] Batch 1285, Loss 0.4894194006919861\n","[Training Epoch 1] Batch 1286, Loss 0.5476078987121582\n","[Training Epoch 1] Batch 1287, Loss 0.49027514457702637\n","[Training Epoch 1] Batch 1288, Loss 0.4565470218658447\n","[Training Epoch 1] Batch 1289, Loss 0.506089448928833\n","[Training Epoch 1] Batch 1290, Loss 0.48053842782974243\n","[Training Epoch 1] Batch 1291, Loss 0.5397306084632874\n","[Training Epoch 1] Batch 1292, Loss 0.48324695229530334\n","[Training Epoch 1] Batch 1293, Loss 0.49214792251586914\n","[Training Epoch 1] Batch 1294, Loss 0.48988842964172363\n","[Training Epoch 1] Batch 1295, Loss 0.5085967779159546\n","[Training Epoch 1] Batch 1296, Loss 0.5360298156738281\n","[Training Epoch 1] Batch 1297, Loss 0.4941175878047943\n","[Training Epoch 1] Batch 1298, Loss 0.5656164884567261\n","[Training Epoch 1] Batch 1299, Loss 0.5020433068275452\n","[Training Epoch 1] Batch 1300, Loss 0.514089822769165\n","[Training Epoch 1] Batch 1301, Loss 0.5139455199241638\n","[Training Epoch 1] Batch 1302, Loss 0.5045135021209717\n","[Training Epoch 1] Batch 1303, Loss 0.4971689283847809\n","[Training Epoch 1] Batch 1304, Loss 0.48978710174560547\n","[Training Epoch 1] Batch 1305, Loss 0.4722881615161896\n","[Training Epoch 1] Batch 1306, Loss 0.5094956755638123\n","[Training Epoch 1] Batch 1307, Loss 0.4953514337539673\n","[Training Epoch 1] Batch 1308, Loss 0.46907171607017517\n","[Training Epoch 1] Batch 1309, Loss 0.4774293303489685\n","[Training Epoch 1] Batch 1310, Loss 0.48298379778862\n","[Training Epoch 1] Batch 1311, Loss 0.5084773898124695\n","[Training Epoch 1] Batch 1312, Loss 0.5061443448066711\n","[Training Epoch 1] Batch 1313, Loss 0.509859561920166\n","[Training Epoch 1] Batch 1314, Loss 0.4951367974281311\n","[Training Epoch 1] Batch 1315, Loss 0.5165125131607056\n","[Training Epoch 1] Batch 1316, Loss 0.4816555380821228\n","[Training Epoch 1] Batch 1317, Loss 0.5211846828460693\n","[Training Epoch 1] Batch 1318, Loss 0.4914149045944214\n","[Training Epoch 1] Batch 1319, Loss 0.507369875907898\n","[Training Epoch 1] Batch 1320, Loss 0.5083699226379395\n","[Training Epoch 1] Batch 1321, Loss 0.4910251200199127\n","[Training Epoch 1] Batch 1322, Loss 0.5020589828491211\n","[Training Epoch 1] Batch 1323, Loss 0.4899921119213104\n","[Training Epoch 1] Batch 1324, Loss 0.5264718532562256\n","[Training Epoch 1] Batch 1325, Loss 0.5000271797180176\n","[Training Epoch 1] Batch 1326, Loss 0.5364192724227905\n","[Training Epoch 1] Batch 1327, Loss 0.48479974269866943\n","[Training Epoch 1] Batch 1328, Loss 0.5050874352455139\n","[Training Epoch 1] Batch 1329, Loss 0.5129436254501343\n","[Training Epoch 1] Batch 1330, Loss 0.47739219665527344\n","[Training Epoch 1] Batch 1331, Loss 0.5085919499397278\n","[Training Epoch 1] Batch 1332, Loss 0.5057159662246704\n","[Training Epoch 1] Batch 1333, Loss 0.4642679691314697\n","[Training Epoch 1] Batch 1334, Loss 0.5059414505958557\n","[Training Epoch 1] Batch 1335, Loss 0.48298612236976624\n","[Training Epoch 1] Batch 1336, Loss 0.4977586567401886\n","[Training Epoch 1] Batch 1337, Loss 0.5078321695327759\n","[Training Epoch 1] Batch 1338, Loss 0.47789889574050903\n","[Training Epoch 1] Batch 1339, Loss 0.522456169128418\n","[Training Epoch 1] Batch 1340, Loss 0.4835031032562256\n","[Training Epoch 1] Batch 1341, Loss 0.5007174611091614\n","[Training Epoch 1] Batch 1342, Loss 0.47479671239852905\n","[Training Epoch 1] Batch 1343, Loss 0.5251427292823792\n","[Training Epoch 1] Batch 1344, Loss 0.47862792015075684\n","[Training Epoch 1] Batch 1345, Loss 0.5057445168495178\n","[Training Epoch 1] Batch 1346, Loss 0.5414005517959595\n","[Training Epoch 1] Batch 1347, Loss 0.4966314435005188\n","[Training Epoch 1] Batch 1348, Loss 0.5467073917388916\n","[Training Epoch 1] Batch 1349, Loss 0.5018199682235718\n","[Training Epoch 1] Batch 1350, Loss 0.48974597454071045\n","[Training Epoch 1] Batch 1351, Loss 0.500688910484314\n","[Training Epoch 1] Batch 1352, Loss 0.5025952458381653\n","[Training Epoch 1] Batch 1353, Loss 0.49997612833976746\n","[Training Epoch 1] Batch 1354, Loss 0.4774067997932434\n","[Training Epoch 1] Batch 1355, Loss 0.5005374550819397\n","[Training Epoch 1] Batch 1356, Loss 0.4779633581638336\n","[Training Epoch 1] Batch 1357, Loss 0.47751814126968384\n","[Training Epoch 1] Batch 1358, Loss 0.4927380084991455\n","[Training Epoch 1] Batch 1359, Loss 0.4843359589576721\n","[Training Epoch 1] Batch 1360, Loss 0.514798641204834\n","[Training Epoch 1] Batch 1361, Loss 0.5107278823852539\n","[Training Epoch 1] Batch 1362, Loss 0.49726366996765137\n","[Training Epoch 1] Batch 1363, Loss 0.4995822310447693\n","[Training Epoch 1] Batch 1364, Loss 0.48297959566116333\n","[Training Epoch 1] Batch 1365, Loss 0.48538750410079956\n","[Training Epoch 1] Batch 1366, Loss 0.503541111946106\n","[Training Epoch 1] Batch 1367, Loss 0.5183135271072388\n","[Training Epoch 1] Batch 1368, Loss 0.4990854263305664\n","[Training Epoch 1] Batch 1369, Loss 0.4871695637702942\n","[Training Epoch 1] Batch 1370, Loss 0.46652138233184814\n","[Training Epoch 1] Batch 1371, Loss 0.4788920283317566\n","[Training Epoch 1] Batch 1372, Loss 0.5263742804527283\n","[Training Epoch 1] Batch 1373, Loss 0.4853956699371338\n","[Training Epoch 1] Batch 1374, Loss 0.49112948775291443\n","[Training Epoch 1] Batch 1375, Loss 0.4812358021736145\n","[Training Epoch 1] Batch 1376, Loss 0.48502129316329956\n","[Training Epoch 1] Batch 1377, Loss 0.492348849773407\n","[Training Epoch 1] Batch 1378, Loss 0.508796215057373\n","[Training Epoch 1] Batch 1379, Loss 0.48563212156295776\n","[Training Epoch 1] Batch 1380, Loss 0.510566234588623\n","[Training Epoch 1] Batch 1381, Loss 0.5020692944526672\n","[Training Epoch 1] Batch 1382, Loss 0.5172185301780701\n","[Training Epoch 1] Batch 1383, Loss 0.4940064549446106\n","[Training Epoch 1] Batch 1384, Loss 0.48295074701309204\n","[Training Epoch 1] Batch 1385, Loss 0.48794692754745483\n","[Training Epoch 1] Batch 1386, Loss 0.5129060745239258\n","[Training Epoch 1] Batch 1387, Loss 0.4991317391395569\n","[Training Epoch 1] Batch 1388, Loss 0.5227681398391724\n","[Training Epoch 1] Batch 1389, Loss 0.49942678213119507\n","[Training Epoch 1] Batch 1390, Loss 0.48273172974586487\n","[Training Epoch 1] Batch 1391, Loss 0.4780568480491638\n","[Training Epoch 1] Batch 1392, Loss 0.4759097695350647\n","[Training Epoch 1] Batch 1393, Loss 0.5302064418792725\n","[Training Epoch 1] Batch 1394, Loss 0.5121333599090576\n","[Training Epoch 1] Batch 1395, Loss 0.47758206725120544\n","[Training Epoch 1] Batch 1396, Loss 0.47433844208717346\n","[Training Epoch 1] Batch 1397, Loss 0.48908665776252747\n","[Training Epoch 1] Batch 1398, Loss 0.5172091722488403\n","[Training Epoch 1] Batch 1399, Loss 0.5235158205032349\n","[Training Epoch 1] Batch 1400, Loss 0.48843711614608765\n","[Training Epoch 1] Batch 1401, Loss 0.5006412267684937\n","[Training Epoch 1] Batch 1402, Loss 0.5237631797790527\n","[Training Epoch 1] Batch 1403, Loss 0.4886980354785919\n","[Training Epoch 1] Batch 1404, Loss 0.5049618482589722\n","[Training Epoch 1] Batch 1405, Loss 0.5296583771705627\n","[Training Epoch 1] Batch 1406, Loss 0.5071607828140259\n","[Training Epoch 1] Batch 1407, Loss 0.4884245991706848\n","[Training Epoch 1] Batch 1408, Loss 0.4602031707763672\n","[Training Epoch 1] Batch 1409, Loss 0.5005183219909668\n","[Training Epoch 1] Batch 1410, Loss 0.5118818283081055\n","[Training Epoch 1] Batch 1411, Loss 0.5080165863037109\n","[Training Epoch 1] Batch 1412, Loss 0.47795459628105164\n","[Training Epoch 1] Batch 1413, Loss 0.5029176473617554\n","[Training Epoch 1] Batch 1414, Loss 0.513634443283081\n","[Training Epoch 1] Batch 1415, Loss 0.4815292954444885\n","[Training Epoch 1] Batch 1416, Loss 0.48602044582366943\n","[Training Epoch 1] Batch 1417, Loss 0.537125825881958\n","[Training Epoch 1] Batch 1418, Loss 0.5193912982940674\n","[Training Epoch 1] Batch 1419, Loss 0.4877840280532837\n","[Training Epoch 1] Batch 1420, Loss 0.53769850730896\n","[Training Epoch 1] Batch 1421, Loss 0.5162460803985596\n","[Training Epoch 1] Batch 1422, Loss 0.5142253637313843\n","[Training Epoch 1] Batch 1423, Loss 0.47759509086608887\n","[Training Epoch 1] Batch 1424, Loss 0.48704251646995544\n","[Training Epoch 1] Batch 1425, Loss 0.5155868530273438\n","[Training Epoch 1] Batch 1426, Loss 0.4761425852775574\n","[Training Epoch 1] Batch 1427, Loss 0.5033199787139893\n","[Training Epoch 1] Batch 1428, Loss 0.4940856099128723\n","[Training Epoch 1] Batch 1429, Loss 0.4836878180503845\n","[Training Epoch 1] Batch 1430, Loss 0.48675763607025146\n","[Training Epoch 1] Batch 1431, Loss 0.5059354901313782\n","[Training Epoch 1] Batch 1432, Loss 0.5168781280517578\n","[Training Epoch 1] Batch 1433, Loss 0.48052743077278137\n","[Training Epoch 1] Batch 1434, Loss 0.497857928276062\n","[Training Epoch 1] Batch 1435, Loss 0.4745330810546875\n","[Training Epoch 1] Batch 1436, Loss 0.4773346185684204\n","[Training Epoch 1] Batch 1437, Loss 0.5209698677062988\n","[Training Epoch 1] Batch 1438, Loss 0.5061140060424805\n","[Training Epoch 1] Batch 1439, Loss 0.49710944294929504\n","[Training Epoch 1] Batch 1440, Loss 0.5106672644615173\n","[Training Epoch 1] Batch 1441, Loss 0.49974554777145386\n","[Training Epoch 1] Batch 1442, Loss 0.47248485684394836\n","[Training Epoch 1] Batch 1443, Loss 0.49935588240623474\n","[Training Epoch 1] Batch 1444, Loss 0.5225963592529297\n","[Training Epoch 1] Batch 1445, Loss 0.4927878975868225\n","[Training Epoch 1] Batch 1446, Loss 0.5005767941474915\n","[Training Epoch 1] Batch 1447, Loss 0.48454707860946655\n","[Training Epoch 1] Batch 1448, Loss 0.5167092084884644\n","[Training Epoch 1] Batch 1449, Loss 0.4962579011917114\n","[Training Epoch 1] Batch 1450, Loss 0.48017942905426025\n","[Training Epoch 1] Batch 1451, Loss 0.4857565760612488\n","[Training Epoch 1] Batch 1452, Loss 0.5329521298408508\n","[Training Epoch 1] Batch 1453, Loss 0.47813230752944946\n","[Training Epoch 1] Batch 1454, Loss 0.4761464595794678\n","[Training Epoch 1] Batch 1455, Loss 0.5063750743865967\n","[Training Epoch 1] Batch 1456, Loss 0.4804173409938812\n","[Training Epoch 1] Batch 1457, Loss 0.5354288816452026\n","[Training Epoch 1] Batch 1458, Loss 0.5021359324455261\n","[Training Epoch 1] Batch 1459, Loss 0.49516400694847107\n","[Training Epoch 1] Batch 1460, Loss 0.48159492015838623\n","[Training Epoch 1] Batch 1461, Loss 0.48028135299682617\n","[Training Epoch 1] Batch 1462, Loss 0.5130407214164734\n","[Training Epoch 1] Batch 1463, Loss 0.4810187518596649\n","[Training Epoch 1] Batch 1464, Loss 0.48025211691856384\n","[Training Epoch 1] Batch 1465, Loss 0.47493505477905273\n","[Training Epoch 1] Batch 1466, Loss 0.486840158700943\n","[Training Epoch 1] Batch 1467, Loss 0.5374617576599121\n","[Training Epoch 1] Batch 1468, Loss 0.5189379453659058\n","[Training Epoch 1] Batch 1469, Loss 0.5153800249099731\n","[Training Epoch 1] Batch 1470, Loss 0.510857880115509\n","[Training Epoch 1] Batch 1471, Loss 0.4883352220058441\n","[Training Epoch 1] Batch 1472, Loss 0.5134663581848145\n","[Training Epoch 1] Batch 1473, Loss 0.5047190189361572\n","[Training Epoch 1] Batch 1474, Loss 0.5278812646865845\n","[Training Epoch 1] Batch 1475, Loss 0.49432888627052307\n","[Training Epoch 1] Batch 1476, Loss 0.4980311393737793\n","[Training Epoch 1] Batch 1477, Loss 0.48386794328689575\n","[Training Epoch 1] Batch 1478, Loss 0.46931153535842896\n","[Training Epoch 1] Batch 1479, Loss 0.5014328956604004\n","[Training Epoch 1] Batch 1480, Loss 0.47074368596076965\n","[Training Epoch 1] Batch 1481, Loss 0.4925646185874939\n","[Training Epoch 1] Batch 1482, Loss 0.5291091203689575\n","[Training Epoch 1] Batch 1483, Loss 0.5060585737228394\n","[Training Epoch 1] Batch 1484, Loss 0.515411376953125\n","[Training Epoch 1] Batch 1485, Loss 0.5209285020828247\n","[Training Epoch 1] Batch 1486, Loss 0.5099471807479858\n","[Training Epoch 1] Batch 1487, Loss 0.4647315740585327\n","[Training Epoch 1] Batch 1488, Loss 0.49021315574645996\n","[Training Epoch 1] Batch 1489, Loss 0.4936795234680176\n","[Training Epoch 1] Batch 1490, Loss 0.5133442282676697\n","[Training Epoch 1] Batch 1491, Loss 0.5070587396621704\n","[Training Epoch 1] Batch 1492, Loss 0.5065600275993347\n","[Training Epoch 1] Batch 1493, Loss 0.5173239707946777\n","[Training Epoch 1] Batch 1494, Loss 0.4993000030517578\n","[Training Epoch 1] Batch 1495, Loss 0.5102048516273499\n","[Training Epoch 1] Batch 1496, Loss 0.5365936756134033\n","[Training Epoch 1] Batch 1497, Loss 0.5094608068466187\n","[Training Epoch 1] Batch 1498, Loss 0.48791778087615967\n","[Training Epoch 1] Batch 1499, Loss 0.4991978406906128\n","[Training Epoch 1] Batch 1500, Loss 0.49891695380210876\n","[Training Epoch 1] Batch 1501, Loss 0.4962655305862427\n","[Training Epoch 1] Batch 1502, Loss 0.4799559414386749\n","[Training Epoch 1] Batch 1503, Loss 0.501543402671814\n","[Training Epoch 1] Batch 1504, Loss 0.5007989406585693\n","[Training Epoch 1] Batch 1505, Loss 0.5031145811080933\n","[Training Epoch 1] Batch 1506, Loss 0.5089915990829468\n","[Training Epoch 1] Batch 1507, Loss 0.5198211073875427\n","[Training Epoch 1] Batch 1508, Loss 0.5285292863845825\n","[Training Epoch 1] Batch 1509, Loss 0.4882959723472595\n","[Training Epoch 1] Batch 1510, Loss 0.49623262882232666\n","[Training Epoch 1] Batch 1511, Loss 0.5149140954017639\n","[Training Epoch 1] Batch 1512, Loss 0.4894946217536926\n","[Training Epoch 1] Batch 1513, Loss 0.5182256102561951\n","[Training Epoch 1] Batch 1514, Loss 0.4713882505893707\n","[Training Epoch 1] Batch 1515, Loss 0.5088319778442383\n","[Training Epoch 1] Batch 1516, Loss 0.48903587460517883\n","[Training Epoch 1] Batch 1517, Loss 0.4939446449279785\n","[Training Epoch 1] Batch 1518, Loss 0.4950118064880371\n","[Training Epoch 1] Batch 1519, Loss 0.516085147857666\n","[Training Epoch 1] Batch 1520, Loss 0.5130786895751953\n","[Training Epoch 1] Batch 1521, Loss 0.5287469625473022\n","[Training Epoch 1] Batch 1522, Loss 0.4884977340698242\n","[Training Epoch 1] Batch 1523, Loss 0.5225486159324646\n","[Training Epoch 1] Batch 1524, Loss 0.4966896176338196\n","[Training Epoch 1] Batch 1525, Loss 0.500156581401825\n","[Training Epoch 1] Batch 1526, Loss 0.500879168510437\n","[Training Epoch 1] Batch 1527, Loss 0.4944086968898773\n","[Training Epoch 1] Batch 1528, Loss 0.5053633451461792\n","[Training Epoch 1] Batch 1529, Loss 0.5085406303405762\n","[Training Epoch 1] Batch 1530, Loss 0.47237080335617065\n","[Training Epoch 1] Batch 1531, Loss 0.5072834491729736\n","[Training Epoch 1] Batch 1532, Loss 0.5075377821922302\n","[Training Epoch 1] Batch 1533, Loss 0.5082926750183105\n","[Training Epoch 1] Batch 1534, Loss 0.5080897808074951\n","[Training Epoch 1] Batch 1535, Loss 0.5214049816131592\n","[Training Epoch 1] Batch 1536, Loss 0.4888648986816406\n","[Training Epoch 1] Batch 1537, Loss 0.4861942231655121\n","[Training Epoch 1] Batch 1538, Loss 0.4721960723400116\n","[Training Epoch 1] Batch 1539, Loss 0.514000654220581\n","[Training Epoch 1] Batch 1540, Loss 0.501836359500885\n","[Training Epoch 1] Batch 1541, Loss 0.4961034655570984\n","[Training Epoch 1] Batch 1542, Loss 0.4680940508842468\n","[Training Epoch 1] Batch 1543, Loss 0.5074365139007568\n","[Training Epoch 1] Batch 1544, Loss 0.48735475540161133\n","[Training Epoch 1] Batch 1545, Loss 0.48751306533813477\n","[Training Epoch 1] Batch 1546, Loss 0.5059983730316162\n","[Training Epoch 1] Batch 1547, Loss 0.5087404251098633\n","[Training Epoch 1] Batch 1548, Loss 0.4896277189254761\n","[Training Epoch 1] Batch 1549, Loss 0.49019938707351685\n","[Training Epoch 1] Batch 1550, Loss 0.5195255279541016\n","[Training Epoch 1] Batch 1551, Loss 0.4829629063606262\n","[Training Epoch 1] Batch 1552, Loss 0.4726405143737793\n","[Training Epoch 1] Batch 1553, Loss 0.47103026509284973\n","[Training Epoch 1] Batch 1554, Loss 0.4888550043106079\n","[Training Epoch 1] Batch 1555, Loss 0.5005484819412231\n","[Training Epoch 1] Batch 1556, Loss 0.4611988365650177\n","[Training Epoch 1] Batch 1557, Loss 0.49531033635139465\n","[Training Epoch 1] Batch 1558, Loss 0.5100758075714111\n","[Training Epoch 1] Batch 1559, Loss 0.49524426460266113\n","[Training Epoch 1] Batch 1560, Loss 0.5049680471420288\n","[Training Epoch 1] Batch 1561, Loss 0.47906774282455444\n","[Training Epoch 1] Batch 1562, Loss 0.485452800989151\n","[Training Epoch 1] Batch 1563, Loss 0.49556514620780945\n","[Training Epoch 1] Batch 1564, Loss 0.4983031153678894\n","[Training Epoch 1] Batch 1565, Loss 0.5170074105262756\n","[Training Epoch 1] Batch 1566, Loss 0.4770044982433319\n","[Training Epoch 1] Batch 1567, Loss 0.5010609030723572\n","[Training Epoch 1] Batch 1568, Loss 0.5279048085212708\n","[Training Epoch 1] Batch 1569, Loss 0.5372194647789001\n","[Training Epoch 1] Batch 1570, Loss 0.5049898624420166\n","[Training Epoch 1] Batch 1571, Loss 0.5057085752487183\n","[Training Epoch 1] Batch 1572, Loss 0.5226929783821106\n","[Training Epoch 1] Batch 1573, Loss 0.4926403760910034\n","[Training Epoch 1] Batch 1574, Loss 0.5295882821083069\n","[Training Epoch 1] Batch 1575, Loss 0.49004024267196655\n","[Training Epoch 1] Batch 1576, Loss 0.4831407964229584\n","[Training Epoch 1] Batch 1577, Loss 0.5017861723899841\n","[Training Epoch 1] Batch 1578, Loss 0.5004546642303467\n","[Training Epoch 1] Batch 1579, Loss 0.5182726383209229\n","[Training Epoch 1] Batch 1580, Loss 0.5342477560043335\n","[Training Epoch 1] Batch 1581, Loss 0.47657811641693115\n","[Training Epoch 1] Batch 1582, Loss 0.5345370173454285\n","[Training Epoch 1] Batch 1583, Loss 0.5036747455596924\n","[Training Epoch 1] Batch 1584, Loss 0.5440845489501953\n","[Training Epoch 1] Batch 1585, Loss 0.5170064568519592\n","[Training Epoch 1] Batch 1586, Loss 0.5060838460922241\n","[Training Epoch 1] Batch 1587, Loss 0.5142929553985596\n","[Training Epoch 1] Batch 1588, Loss 0.4806695580482483\n","[Training Epoch 1] Batch 1589, Loss 0.4871438443660736\n","[Training Epoch 1] Batch 1590, Loss 0.48983433842658997\n","[Training Epoch 1] Batch 1591, Loss 0.5115521550178528\n","[Training Epoch 1] Batch 1592, Loss 0.5047280788421631\n","[Training Epoch 1] Batch 1593, Loss 0.4600537419319153\n","[Training Epoch 1] Batch 1594, Loss 0.4872714877128601\n","[Training Epoch 1] Batch 1595, Loss 0.5101971626281738\n","[Training Epoch 1] Batch 1596, Loss 0.5140889883041382\n","[Training Epoch 1] Batch 1597, Loss 0.4722406566143036\n","[Training Epoch 1] Batch 1598, Loss 0.4899004101753235\n","[Training Epoch 1] Batch 1599, Loss 0.4857799708843231\n","[Training Epoch 1] Batch 1600, Loss 0.50157630443573\n","[Training Epoch 1] Batch 1601, Loss 0.5050300359725952\n","[Training Epoch 1] Batch 1602, Loss 0.5170102119445801\n","[Training Epoch 1] Batch 1603, Loss 0.5020405054092407\n","[Training Epoch 1] Batch 1604, Loss 0.5074446201324463\n","[Training Epoch 1] Batch 1605, Loss 0.4732498526573181\n","[Training Epoch 1] Batch 1606, Loss 0.4993881583213806\n","[Training Epoch 1] Batch 1607, Loss 0.5061408281326294\n","[Training Epoch 1] Batch 1608, Loss 0.49528175592422485\n","[Training Epoch 1] Batch 1609, Loss 0.507631778717041\n","[Training Epoch 1] Batch 1610, Loss 0.4965803027153015\n","[Training Epoch 1] Batch 1611, Loss 0.4910454750061035\n","[Training Epoch 1] Batch 1612, Loss 0.48605233430862427\n","[Training Epoch 1] Batch 1613, Loss 0.5234918594360352\n","[Training Epoch 1] Batch 1614, Loss 0.49351954460144043\n","[Training Epoch 1] Batch 1615, Loss 0.4707717299461365\n","[Training Epoch 1] Batch 1616, Loss 0.5062006115913391\n","[Training Epoch 1] Batch 1617, Loss 0.5088455677032471\n","[Training Epoch 1] Batch 1618, Loss 0.4964950680732727\n","[Training Epoch 1] Batch 1619, Loss 0.5063230395317078\n","[Training Epoch 1] Batch 1620, Loss 0.4848442077636719\n","[Training Epoch 1] Batch 1621, Loss 0.5035724639892578\n","[Training Epoch 1] Batch 1622, Loss 0.4843719005584717\n","[Training Epoch 1] Batch 1623, Loss 0.5046613812446594\n","[Training Epoch 1] Batch 1624, Loss 0.4980531632900238\n","[Training Epoch 1] Batch 1625, Loss 0.5183312296867371\n","[Training Epoch 1] Batch 1626, Loss 0.49276354908943176\n","[Training Epoch 1] Batch 1627, Loss 0.5061007142066956\n","[Training Epoch 1] Batch 1628, Loss 0.4951809048652649\n","[Training Epoch 1] Batch 1629, Loss 0.4996214509010315\n","[Training Epoch 1] Batch 1630, Loss 0.5142512917518616\n","[Training Epoch 1] Batch 1631, Loss 0.5321439504623413\n","[Training Epoch 1] Batch 1632, Loss 0.4887840151786804\n","[Training Epoch 1] Batch 1633, Loss 0.47997909784317017\n","[Training Epoch 1] Batch 1634, Loss 0.49654513597488403\n","[Training Epoch 1] Batch 1635, Loss 0.5063999891281128\n","[Training Epoch 1] Batch 1636, Loss 0.5115801095962524\n","[Training Epoch 1] Batch 1637, Loss 0.503282904624939\n","[Training Epoch 1] Batch 1638, Loss 0.4964030683040619\n","[Training Epoch 1] Batch 1639, Loss 0.5034961104393005\n","[Training Epoch 1] Batch 1640, Loss 0.5005179047584534\n","[Training Epoch 1] Batch 1641, Loss 0.48310697078704834\n","[Training Epoch 1] Batch 1642, Loss 0.5059834718704224\n","[Training Epoch 1] Batch 1643, Loss 0.48418498039245605\n","[Training Epoch 1] Batch 1644, Loss 0.5101733207702637\n","[Training Epoch 1] Batch 1645, Loss 0.4832875728607178\n","[Training Epoch 1] Batch 1646, Loss 0.48758721351623535\n","[Training Epoch 1] Batch 1647, Loss 0.47747787833213806\n","[Training Epoch 1] Batch 1648, Loss 0.5088322758674622\n","[Training Epoch 1] Batch 1649, Loss 0.4751073718070984\n","[Training Epoch 1] Batch 1650, Loss 0.4941008985042572\n","[Training Epoch 1] Batch 1651, Loss 0.5211841464042664\n","[Training Epoch 1] Batch 1652, Loss 0.4897121787071228\n","[Training Epoch 1] Batch 1653, Loss 0.4666248559951782\n","[Training Epoch 1] Batch 1654, Loss 0.49120140075683594\n","[Training Epoch 1] Batch 1655, Loss 0.5141865015029907\n","[Training Epoch 1] Batch 1656, Loss 0.5021299123764038\n","[Training Epoch 1] Batch 1657, Loss 0.4776076078414917\n","[Training Epoch 1] Batch 1658, Loss 0.500872790813446\n","[Training Epoch 1] Batch 1659, Loss 0.5181136131286621\n","[Training Epoch 1] Batch 1660, Loss 0.4730975925922394\n","[Training Epoch 1] Batch 1661, Loss 0.5163010358810425\n","[Training Epoch 1] Batch 1662, Loss 0.5292034149169922\n","[Training Epoch 1] Batch 1663, Loss 0.4899114966392517\n","[Training Epoch 1] Batch 1664, Loss 0.49385741353034973\n","[Training Epoch 1] Batch 1665, Loss 0.48955726623535156\n","[Training Epoch 1] Batch 1666, Loss 0.47330766916275024\n","[Training Epoch 1] Batch 1667, Loss 0.4955406188964844\n","[Training Epoch 1] Batch 1668, Loss 0.5064602494239807\n","[Training Epoch 1] Batch 1669, Loss 0.4992494285106659\n","[Training Epoch 1] Batch 1670, Loss 0.4913300573825836\n","[Training Epoch 1] Batch 1671, Loss 0.5212486982345581\n","[Training Epoch 1] Batch 1672, Loss 0.524086594581604\n","[Training Epoch 1] Batch 1673, Loss 0.5115465521812439\n","[Training Epoch 1] Batch 1674, Loss 0.5077514052391052\n","[Training Epoch 1] Batch 1675, Loss 0.4711982309818268\n","[Training Epoch 1] Batch 1676, Loss 0.49411770701408386\n","[Training Epoch 1] Batch 1677, Loss 0.5020942687988281\n","[Training Epoch 1] Batch 1678, Loss 0.5402548909187317\n","[Training Epoch 1] Batch 1679, Loss 0.48723316192626953\n","[Training Epoch 1] Batch 1680, Loss 0.4873496890068054\n","[Training Epoch 1] Batch 1681, Loss 0.4667564034461975\n","[Training Epoch 1] Batch 1682, Loss 0.48560214042663574\n","[Training Epoch 1] Batch 1683, Loss 0.5009740591049194\n","[Training Epoch 1] Batch 1684, Loss 0.5098845958709717\n","[Training Epoch 1] Batch 1685, Loss 0.5279004573822021\n","[Training Epoch 1] Batch 1686, Loss 0.5060158371925354\n","[Training Epoch 1] Batch 1687, Loss 0.5005236864089966\n","[Training Epoch 1] Batch 1688, Loss 0.5035017132759094\n","[Training Epoch 1] Batch 1689, Loss 0.5007304549217224\n","[Training Epoch 1] Batch 1690, Loss 0.5211065411567688\n","[Training Epoch 1] Batch 1691, Loss 0.4941475987434387\n","[Training Epoch 1] Batch 1692, Loss 0.5006657838821411\n","[Training Epoch 1] Batch 1693, Loss 0.48836973309516907\n","[Training Epoch 1] Batch 1694, Loss 0.4992161989212036\n","[Training Epoch 1] Batch 1695, Loss 0.4841293692588806\n","[Training Epoch 1] Batch 1696, Loss 0.497996985912323\n","[Training Epoch 1] Batch 1697, Loss 0.5046430230140686\n","[Training Epoch 1] Batch 1698, Loss 0.49620959162712097\n","[Training Epoch 1] Batch 1699, Loss 0.48720425367355347\n","[Training Epoch 1] Batch 1700, Loss 0.5008736252784729\n","[Training Epoch 1] Batch 1701, Loss 0.5293842554092407\n","[Training Epoch 1] Batch 1702, Loss 0.48445552587509155\n","[Training Epoch 1] Batch 1703, Loss 0.5047992467880249\n","[Training Epoch 1] Batch 1704, Loss 0.4936722218990326\n","[Training Epoch 1] Batch 1705, Loss 0.4965516924858093\n","[Training Epoch 1] Batch 1706, Loss 0.4682595729827881\n","[Training Epoch 1] Batch 1707, Loss 0.5048360824584961\n","[Training Epoch 1] Batch 1708, Loss 0.49411630630493164\n","[Training Epoch 1] Batch 1709, Loss 0.49277180433273315\n","[Training Epoch 1] Batch 1710, Loss 0.5075238347053528\n","[Training Epoch 1] Batch 1711, Loss 0.4914032816886902\n","[Training Epoch 1] Batch 1712, Loss 0.49813318252563477\n","[Training Epoch 1] Batch 1713, Loss 0.5333434343338013\n","[Training Epoch 1] Batch 1714, Loss 0.5007491707801819\n","[Training Epoch 1] Batch 1715, Loss 0.48708152770996094\n","[Training Epoch 1] Batch 1716, Loss 0.5007344484329224\n","[Training Epoch 1] Batch 1717, Loss 0.48165762424468994\n","[Training Epoch 1] Batch 1718, Loss 0.512845516204834\n","[Training Epoch 1] Batch 1719, Loss 0.5171736478805542\n","[Training Epoch 1] Batch 1720, Loss 0.5048304200172424\n","[Training Epoch 1] Batch 1721, Loss 0.5019515752792358\n","[Training Epoch 1] Batch 1722, Loss 0.5387038588523865\n","[Training Epoch 1] Batch 1723, Loss 0.4870530664920807\n","[Training Epoch 1] Batch 1724, Loss 0.5497924089431763\n","[Training Epoch 1] Batch 1725, Loss 0.5181607007980347\n","[Training Epoch 1] Batch 1726, Loss 0.5059710144996643\n","[Training Epoch 1] Batch 1727, Loss 0.5008002519607544\n","[Training Epoch 1] Batch 1728, Loss 0.5142917633056641\n","[Training Epoch 1] Batch 1729, Loss 0.4844372272491455\n","[Training Epoch 1] Batch 1730, Loss 0.5346035957336426\n","[Training Epoch 1] Batch 1731, Loss 0.5233930945396423\n","[Training Epoch 1] Batch 1732, Loss 0.4874001741409302\n","[Training Epoch 1] Batch 1733, Loss 0.5060568451881409\n","[Training Epoch 1] Batch 1734, Loss 0.5224542617797852\n","[Training Epoch 1] Batch 1735, Loss 0.5293027758598328\n","[Training Epoch 1] Batch 1736, Loss 0.49235278367996216\n","[Training Epoch 1] Batch 1737, Loss 0.5017904043197632\n","[Training Epoch 1] Batch 1738, Loss 0.5018846988677979\n","[Training Epoch 1] Batch 1739, Loss 0.49106162786483765\n","[Training Epoch 1] Batch 1740, Loss 0.5073930621147156\n","[Training Epoch 1] Batch 1741, Loss 0.49790334701538086\n","[Training Epoch 1] Batch 1742, Loss 0.5208341479301453\n","[Training Epoch 1] Batch 1743, Loss 0.5289499759674072\n","[Training Epoch 1] Batch 1744, Loss 0.4835170805454254\n","[Training Epoch 1] Batch 1745, Loss 0.5507112145423889\n","[Training Epoch 1] Batch 1746, Loss 0.49792352318763733\n","[Training Epoch 1] Batch 1747, Loss 0.4858691394329071\n","[Training Epoch 1] Batch 1748, Loss 0.4940902888774872\n","[Training Epoch 1] Batch 1749, Loss 0.4778413474559784\n","[Training Epoch 1] Batch 1750, Loss 0.48985588550567627\n","[Training Epoch 1] Batch 1751, Loss 0.5127855539321899\n","[Training Epoch 1] Batch 1752, Loss 0.49264436960220337\n","[Training Epoch 1] Batch 1753, Loss 0.4843584895133972\n","[Training Epoch 1] Batch 1754, Loss 0.4925917685031891\n","[Training Epoch 1] Batch 1755, Loss 0.5023031234741211\n","[Training Epoch 1] Batch 1756, Loss 0.47346749901771545\n","[Training Epoch 1] Batch 1757, Loss 0.48458507657051086\n","[Training Epoch 1] Batch 1758, Loss 0.4914713203907013\n","[Training Epoch 1] Batch 1759, Loss 0.5086938142776489\n","[Training Epoch 1] Batch 1760, Loss 0.5031898021697998\n","[Training Epoch 1] Batch 1761, Loss 0.5046841502189636\n","[Training Epoch 1] Batch 1762, Loss 0.511713445186615\n","[Training Epoch 1] Batch 1763, Loss 0.5177503824234009\n","[Training Epoch 1] Batch 1764, Loss 0.5221506953239441\n","[Training Epoch 1] Batch 1765, Loss 0.5103222131729126\n","[Training Epoch 1] Batch 1766, Loss 0.5132322907447815\n","[Training Epoch 1] Batch 1767, Loss 0.49971333146095276\n","[Training Epoch 1] Batch 1768, Loss 0.48296523094177246\n","[Training Epoch 1] Batch 1769, Loss 0.5069685578346252\n","[Training Epoch 1] Batch 1770, Loss 0.5012398958206177\n","[Training Epoch 1] Batch 1771, Loss 0.5065144896507263\n","[Training Epoch 1] Batch 1772, Loss 0.48710179328918457\n","[Training Epoch 1] Batch 1773, Loss 0.5085365772247314\n","[Training Epoch 1] Batch 1774, Loss 0.5166545510292053\n","[Training Epoch 1] Batch 1775, Loss 0.49653252959251404\n","[Training Epoch 1] Batch 1776, Loss 0.4845607280731201\n","[Training Epoch 1] Batch 1777, Loss 0.4860590100288391\n","[Training Epoch 1] Batch 1778, Loss 0.4548161029815674\n","[Training Epoch 1] Batch 1779, Loss 0.534540057182312\n","[Training Epoch 1] Batch 1780, Loss 0.49788153171539307\n","[Training Epoch 1] Batch 1781, Loss 0.49655142426490784\n","[Training Epoch 1] Batch 1782, Loss 0.5344657301902771\n","[Training Epoch 1] Batch 1783, Loss 0.4897342920303345\n","[Training Epoch 1] Batch 1784, Loss 0.5115976333618164\n","[Training Epoch 1] Batch 1785, Loss 0.5034107565879822\n","[Training Epoch 1] Batch 1786, Loss 0.4859280586242676\n","[Training Epoch 1] Batch 1787, Loss 0.5222384929656982\n","[Training Epoch 1] Batch 1788, Loss 0.5329537391662598\n","[Training Epoch 1] Batch 1789, Loss 0.49634572863578796\n","[Training Epoch 1] Batch 1790, Loss 0.5344736576080322\n","[Training Epoch 1] Batch 1791, Loss 0.49113577604293823\n","[Training Epoch 1] Batch 1792, Loss 0.5092495679855347\n","[Training Epoch 1] Batch 1793, Loss 0.5030394792556763\n","[Training Epoch 1] Batch 1794, Loss 0.503755509853363\n","[Training Epoch 1] Batch 1795, Loss 0.5020303130149841\n","[Training Epoch 1] Batch 1796, Loss 0.49833372235298157\n","[Training Epoch 1] Batch 1797, Loss 0.5082976222038269\n","[Training Epoch 1] Batch 1798, Loss 0.47490155696868896\n","[Training Epoch 1] Batch 1799, Loss 0.479289174079895\n","[Training Epoch 1] Batch 1800, Loss 0.493985652923584\n","[Training Epoch 1] Batch 1801, Loss 0.5058403015136719\n","[Training Epoch 1] Batch 1802, Loss 0.505057692527771\n","[Training Epoch 1] Batch 1803, Loss 0.5341918468475342\n","[Training Epoch 1] Batch 1804, Loss 0.49285387992858887\n","[Training Epoch 1] Batch 1805, Loss 0.4751547574996948\n","[Training Epoch 1] Batch 1806, Loss 0.5050231218338013\n","[Training Epoch 1] Batch 1807, Loss 0.5252126455307007\n","[Training Epoch 1] Batch 1808, Loss 0.5088862180709839\n","[Training Epoch 1] Batch 1809, Loss 0.5018846392631531\n","[Training Epoch 1] Batch 1810, Loss 0.5043556690216064\n","[Training Epoch 1] Batch 1811, Loss 0.5252883434295654\n","[Training Epoch 1] Batch 1812, Loss 0.47231507301330566\n","[Training Epoch 1] Batch 1813, Loss 0.49646636843681335\n","[Training Epoch 1] Batch 1814, Loss 0.48484688997268677\n","[Training Epoch 1] Batch 1815, Loss 0.5114883780479431\n","[Training Epoch 1] Batch 1816, Loss 0.4885849058628082\n","[Training Epoch 1] Batch 1817, Loss 0.4873849153518677\n","[Training Epoch 1] Batch 1818, Loss 0.46970856189727783\n","[Training Epoch 1] Batch 1819, Loss 0.5100100040435791\n","[Training Epoch 1] Batch 1820, Loss 0.5035386085510254\n","[Training Epoch 1] Batch 1821, Loss 0.488609254360199\n","[Training Epoch 1] Batch 1822, Loss 0.488273024559021\n","[Training Epoch 1] Batch 1823, Loss 0.5315800905227661\n","[Training Epoch 1] Batch 1824, Loss 0.500643789768219\n","[Training Epoch 1] Batch 1825, Loss 0.5073136687278748\n","[Training Epoch 1] Batch 1826, Loss 0.47250062227249146\n","[Training Epoch 1] Batch 1827, Loss 0.4887562394142151\n","[Training Epoch 1] Batch 1828, Loss 0.49250316619873047\n","[Training Epoch 1] Batch 1829, Loss 0.4830043315887451\n","[Training Epoch 1] Batch 1830, Loss 0.5088926553726196\n","[Training Epoch 1] Batch 1831, Loss 0.5005830526351929\n","[Training Epoch 1] Batch 1832, Loss 0.49687790870666504\n","[Training Epoch 1] Batch 1833, Loss 0.5159897804260254\n","[Training Epoch 1] Batch 1834, Loss 0.49233478307724\n","[Training Epoch 1] Batch 1835, Loss 0.48582690954208374\n","[Training Epoch 1] Batch 1836, Loss 0.47658562660217285\n","[Training Epoch 1] Batch 1837, Loss 0.49094358086586\n","[Training Epoch 1] Batch 1838, Loss 0.48299068212509155\n","[Training Epoch 1] Batch 1839, Loss 0.4964739680290222\n","[Training Epoch 1] Batch 1840, Loss 0.5388263463973999\n","[Training Epoch 1] Batch 1841, Loss 0.49373841285705566\n","[Training Epoch 1] Batch 1842, Loss 0.5237361192703247\n","[Training Epoch 1] Batch 1843, Loss 0.5151471495628357\n","[Training Epoch 1] Batch 1844, Loss 0.5277922749519348\n","[Training Epoch 1] Batch 1845, Loss 0.5114908814430237\n","[Training Epoch 1] Batch 1846, Loss 0.47755762934684753\n","[Training Epoch 1] Batch 1847, Loss 0.5118225812911987\n","[Training Epoch 1] Batch 1848, Loss 0.49923279881477356\n","[Training Epoch 1] Batch 1849, Loss 0.49770763516426086\n","[Training Epoch 1] Batch 1850, Loss 0.49240684509277344\n","[Training Epoch 1] Batch 1851, Loss 0.5119205713272095\n","[Training Epoch 1] Batch 1852, Loss 0.4870716631412506\n","[Training Epoch 1] Batch 1853, Loss 0.5209445357322693\n","[Training Epoch 1] Batch 1854, Loss 0.5015613436698914\n","[Training Epoch 1] Batch 1855, Loss 0.5172238945960999\n","[Training Epoch 1] Batch 1856, Loss 0.5031099319458008\n","[Training Epoch 1] Batch 1857, Loss 0.4924437403678894\n","[Training Epoch 1] Batch 1858, Loss 0.49651312828063965\n","[Training Epoch 1] Batch 1859, Loss 0.4882209002971649\n","[Training Epoch 1] Batch 1860, Loss 0.5077428817749023\n","[Training Epoch 1] Batch 1861, Loss 0.5171211957931519\n","[Training Epoch 1] Batch 1862, Loss 0.5168321132659912\n","[Training Epoch 1] Batch 1863, Loss 0.5076984763145447\n","[Training Epoch 1] Batch 1864, Loss 0.5046704411506653\n","[Training Epoch 1] Batch 1865, Loss 0.4898868501186371\n","[Training Epoch 1] Batch 1866, Loss 0.5252324342727661\n","[Training Epoch 1] Batch 1867, Loss 0.5387202501296997\n","[Training Epoch 1] Batch 1868, Loss 0.5346022844314575\n","[Training Epoch 1] Batch 1869, Loss 0.5084421634674072\n","[Training Epoch 1] Batch 1870, Loss 0.5343800783157349\n","[Training Epoch 1] Batch 1871, Loss 0.5166770815849304\n","[Training Epoch 1] Batch 1872, Loss 0.5008550882339478\n","[Training Epoch 1] Batch 1873, Loss 0.5236485004425049\n","[Training Epoch 1] Batch 1874, Loss 0.5194159746170044\n","[Training Epoch 1] Batch 1875, Loss 0.48616310954093933\n","[Training Epoch 1] Batch 1876, Loss 0.5060641765594482\n","[Training Epoch 1] Batch 1877, Loss 0.4965091347694397\n","[Training Epoch 1] Batch 1878, Loss 0.50226229429245\n","[Training Epoch 1] Batch 1879, Loss 0.5059172511100769\n","[Training Epoch 1] Batch 1880, Loss 0.484417200088501\n","[Training Epoch 1] Batch 1881, Loss 0.5481879115104675\n","[Training Epoch 1] Batch 1882, Loss 0.4965178966522217\n","[Training Epoch 1] Batch 1883, Loss 0.49700555205345154\n","[Training Epoch 1] Batch 1884, Loss 0.4886547327041626\n","[Training Epoch 1] Batch 1885, Loss 0.4950847625732422\n","[Training Epoch 1] Batch 1886, Loss 0.4791226387023926\n","[Training Epoch 1] Batch 1887, Loss 0.5384870171546936\n","[Training Epoch 1] Batch 1888, Loss 0.49129900336265564\n","[Training Epoch 1] Batch 1889, Loss 0.5072664022445679\n","[Training Epoch 1] Batch 1890, Loss 0.49394500255584717\n","[Training Epoch 1] Batch 1891, Loss 0.4861147701740265\n","[Training Epoch 1] Batch 1892, Loss 0.49322623014450073\n","[Training Epoch 1] Batch 1893, Loss 0.49539101123809814\n","[Training Epoch 1] Batch 1894, Loss 0.48316681385040283\n","[Training Epoch 1] Batch 1895, Loss 0.511404275894165\n","[Training Epoch 1] Batch 1896, Loss 0.47908997535705566\n","[Training Epoch 1] Batch 1897, Loss 0.5049428343772888\n","[Training Epoch 1] Batch 1898, Loss 0.47933363914489746\n","[Training Epoch 1] Batch 1899, Loss 0.5078201293945312\n","[Training Epoch 1] Batch 1900, Loss 0.5154476165771484\n","[Training Epoch 1] Batch 1901, Loss 0.45220237970352173\n","[Training Epoch 1] Batch 1902, Loss 0.5143412351608276\n","[Training Epoch 1] Batch 1903, Loss 0.5262939929962158\n","[Training Epoch 1] Batch 1904, Loss 0.5083789825439453\n","[Training Epoch 1] Batch 1905, Loss 0.46292370557785034\n","[Training Epoch 1] Batch 1906, Loss 0.5010653138160706\n","[Training Epoch 1] Batch 1907, Loss 0.5035467743873596\n","[Training Epoch 1] Batch 1908, Loss 0.4675682783126831\n","[Training Epoch 1] Batch 1909, Loss 0.4996437430381775\n","[Training Epoch 1] Batch 1910, Loss 0.4770093560218811\n","[Training Epoch 1] Batch 1911, Loss 0.4913604259490967\n","[Training Epoch 1] Batch 1912, Loss 0.4857172966003418\n","[Training Epoch 1] Batch 1913, Loss 0.4722890853881836\n","[Training Epoch 1] Batch 1914, Loss 0.5172715187072754\n","[Training Epoch 1] Batch 1915, Loss 0.5098731517791748\n","[Training Epoch 1] Batch 1916, Loss 0.45779934525489807\n","[Training Epoch 1] Batch 1917, Loss 0.5020713210105896\n","[Training Epoch 1] Batch 1918, Loss 0.48189622163772583\n","[Training Epoch 1] Batch 1919, Loss 0.49824047088623047\n","[Training Epoch 1] Batch 1920, Loss 0.518419086933136\n","[Training Epoch 1] Batch 1921, Loss 0.47922787070274353\n","[Training Epoch 1] Batch 1922, Loss 0.4683205485343933\n","[Training Epoch 1] Batch 1923, Loss 0.5034420490264893\n","[Training Epoch 1] Batch 1924, Loss 0.56428062915802\n","[Training Epoch 1] Batch 1925, Loss 0.4965599775314331\n","[Training Epoch 1] Batch 1926, Loss 0.5115747451782227\n","[Training Epoch 1] Batch 1927, Loss 0.4991809129714966\n","[Training Epoch 1] Batch 1928, Loss 0.49108779430389404\n","[Training Epoch 1] Batch 1929, Loss 0.5059912204742432\n","[Training Epoch 1] Batch 1930, Loss 0.48835453391075134\n","[Training Epoch 1] Batch 1931, Loss 0.5086888074874878\n","[Training Epoch 1] Batch 1932, Loss 0.47633427381515503\n","[Training Epoch 1] Batch 1933, Loss 0.49253609776496887\n","[Training Epoch 1] Batch 1934, Loss 0.5101235508918762\n","[Training Epoch 1] Batch 1935, Loss 0.5329774618148804\n","[Training Epoch 1] Batch 1936, Loss 0.526526689529419\n","[Training Epoch 1] Batch 1937, Loss 0.5101856589317322\n","[Training Epoch 1] Batch 1938, Loss 0.5021405220031738\n","[Training Epoch 1] Batch 1939, Loss 0.5033320188522339\n","[Training Epoch 1] Batch 1940, Loss 0.4869897961616516\n","[Training Epoch 1] Batch 1941, Loss 0.5547733306884766\n","[Training Epoch 1] Batch 1942, Loss 0.5139354467391968\n","[Training Epoch 1] Batch 1943, Loss 0.4952106773853302\n","[Training Epoch 1] Batch 1944, Loss 0.4854215681552887\n","[Training Epoch 1] Batch 1945, Loss 0.5143372416496277\n","[Training Epoch 1] Batch 1946, Loss 0.49498945474624634\n","[Training Epoch 1] Batch 1947, Loss 0.4913094639778137\n","[Training Epoch 1] Batch 1948, Loss 0.4863417446613312\n","[Training Epoch 1] Batch 1949, Loss 0.48848432302474976\n","[Training Epoch 1] Batch 1950, Loss 0.511283814907074\n","[Training Epoch 1] Batch 1951, Loss 0.5085107088088989\n","[Training Epoch 1] Batch 1952, Loss 0.5114688873291016\n","[Training Epoch 1] Batch 1953, Loss 0.48575150966644287\n","[Training Epoch 1] Batch 1954, Loss 0.5085902214050293\n","[Training Epoch 1] Batch 1955, Loss 0.49676817655563354\n","[Training Epoch 1] Batch 1956, Loss 0.5059711933135986\n","[Training Epoch 1] Batch 1957, Loss 0.503445565700531\n","[Training Epoch 1] Batch 1958, Loss 0.5151153206825256\n","[Training Epoch 1] Batch 1959, Loss 0.5218212008476257\n","[Training Epoch 1] Batch 1960, Loss 0.49246692657470703\n","[Training Epoch 1] Batch 1961, Loss 0.4774995446205139\n","[Training Epoch 1] Batch 1962, Loss 0.4780257046222687\n","[Training Epoch 1] Batch 1963, Loss 0.5019469857215881\n","[Training Epoch 1] Batch 1964, Loss 0.4763973355293274\n","[Training Epoch 1] Batch 1965, Loss 0.5182400941848755\n","[Training Epoch 1] Batch 1966, Loss 0.50357985496521\n","[Training Epoch 1] Batch 1967, Loss 0.49834197759628296\n","[Training Epoch 1] Batch 1968, Loss 0.453315794467926\n","[Training Epoch 1] Batch 1969, Loss 0.4828767776489258\n","[Training Epoch 1] Batch 1970, Loss 0.46300047636032104\n","[Training Epoch 1] Batch 1971, Loss 0.4681282341480255\n","[Training Epoch 1] Batch 1972, Loss 0.4991747736930847\n","[Training Epoch 1] Batch 1973, Loss 0.5023881196975708\n","[Training Epoch 1] Batch 1974, Loss 0.498087614774704\n","[Training Epoch 1] Batch 1975, Loss 0.49659448862075806\n","[Training Epoch 1] Batch 1976, Loss 0.49766215682029724\n","[Training Epoch 1] Batch 1977, Loss 0.5006669759750366\n","[Training Epoch 1] Batch 1978, Loss 0.4841606616973877\n","[Training Epoch 1] Batch 1979, Loss 0.5001111030578613\n","[Training Epoch 1] Batch 1980, Loss 0.48322826623916626\n","[Training Epoch 1] Batch 1981, Loss 0.5167246460914612\n","[Training Epoch 1] Batch 1982, Loss 0.5157122611999512\n","[Training Epoch 1] Batch 1983, Loss 0.5076780915260315\n","[Training Epoch 1] Batch 1984, Loss 0.4995526671409607\n","[Training Epoch 1] Batch 1985, Loss 0.4844001531600952\n","[Training Epoch 1] Batch 1986, Loss 0.5011218190193176\n","[Training Epoch 1] Batch 1987, Loss 0.5181664228439331\n","[Training Epoch 1] Batch 1988, Loss 0.48974841833114624\n","[Training Epoch 1] Batch 1989, Loss 0.47913026809692383\n","[Training Epoch 1] Batch 1990, Loss 0.49798935651779175\n","[Training Epoch 1] Batch 1991, Loss 0.5141854882240295\n","[Training Epoch 1] Batch 1992, Loss 0.49225687980651855\n","[Training Epoch 1] Batch 1993, Loss 0.4926530718803406\n","[Training Epoch 1] Batch 1994, Loss 0.5062382221221924\n","[Training Epoch 1] Batch 1995, Loss 0.494871586561203\n","[Training Epoch 1] Batch 1996, Loss 0.5074710249900818\n","[Training Epoch 1] Batch 1997, Loss 0.4595985412597656\n","[Training Epoch 1] Batch 1998, Loss 0.5119805335998535\n","[Training Epoch 1] Batch 1999, Loss 0.48017823696136475\n","[Training Epoch 1] Batch 2000, Loss 0.4967690110206604\n","[Training Epoch 1] Batch 2001, Loss 0.4980093836784363\n","[Training Epoch 1] Batch 2002, Loss 0.5346149206161499\n","[Training Epoch 1] Batch 2003, Loss 0.5067527294158936\n","[Training Epoch 1] Batch 2004, Loss 0.4748290777206421\n","[Training Epoch 1] Batch 2005, Loss 0.4805101752281189\n","[Training Epoch 1] Batch 2006, Loss 0.4870355725288391\n","[Training Epoch 1] Batch 2007, Loss 0.49118772149086\n","[Training Epoch 1] Batch 2008, Loss 0.5209916234016418\n","[Training Epoch 1] Batch 2009, Loss 0.5224053859710693\n","[Training Epoch 1] Batch 2010, Loss 0.5042914748191833\n","[Training Epoch 1] Batch 2011, Loss 0.5039054155349731\n","[Training Epoch 1] Batch 2012, Loss 0.5280130505561829\n","[Training Epoch 1] Batch 2013, Loss 0.49916917085647583\n","[Training Epoch 1] Batch 2014, Loss 0.4794405698776245\n","[Training Epoch 1] Batch 2015, Loss 0.5075178742408752\n","[Training Epoch 1] Batch 2016, Loss 0.5143872499465942\n","[Training Epoch 1] Batch 2017, Loss 0.4763437807559967\n","[Training Epoch 1] Batch 2018, Loss 0.47891902923583984\n","[Training Epoch 1] Batch 2019, Loss 0.5173354744911194\n","[Training Epoch 1] Batch 2020, Loss 0.5065352320671082\n","[Training Epoch 1] Batch 2021, Loss 0.5210698843002319\n","[Training Epoch 1] Batch 2022, Loss 0.518155574798584\n","[Training Epoch 1] Batch 2023, Loss 0.5100778341293335\n","[Training Epoch 1] Batch 2024, Loss 0.48718738555908203\n","[Training Epoch 1] Batch 2025, Loss 0.49763280153274536\n","[Training Epoch 1] Batch 2026, Loss 0.4746047258377075\n","[Training Epoch 1] Batch 2027, Loss 0.496478408575058\n","[Training Epoch 1] Batch 2028, Loss 0.5021845698356628\n","[Training Epoch 1] Batch 2029, Loss 0.4966701865196228\n","[Training Epoch 1] Batch 2030, Loss 0.48803985118865967\n","[Training Epoch 1] Batch 2031, Loss 0.5043373107910156\n","[Training Epoch 1] Batch 2032, Loss 0.4654754400253296\n","[Training Epoch 1] Batch 2033, Loss 0.5196449756622314\n","[Training Epoch 1] Batch 2034, Loss 0.51824551820755\n","[Training Epoch 1] Batch 2035, Loss 0.5112308859825134\n","[Training Epoch 1] Batch 2036, Loss 0.48945778608322144\n","[Training Epoch 1] Batch 2037, Loss 0.49519163370132446\n","[Training Epoch 1] Batch 2038, Loss 0.510208249092102\n","[Training Epoch 1] Batch 2039, Loss 0.48049038648605347\n","[Training Epoch 1] Batch 2040, Loss 0.49817705154418945\n","[Training Epoch 1] Batch 2041, Loss 0.4868978261947632\n","[Training Epoch 1] Batch 2042, Loss 0.4979029595851898\n","[Training Epoch 1] Batch 2043, Loss 0.4912368059158325\n","[Training Epoch 1] Batch 2044, Loss 0.49336689710617065\n","[Training Epoch 1] Batch 2045, Loss 0.5156561136245728\n","[Training Epoch 1] Batch 2046, Loss 0.47621557116508484\n","[Training Epoch 1] Batch 2047, Loss 0.5356254577636719\n","[Training Epoch 1] Batch 2048, Loss 0.5051531791687012\n","[Training Epoch 1] Batch 2049, Loss 0.4967177212238312\n","[Training Epoch 1] Batch 2050, Loss 0.4941214323043823\n","[Training Epoch 1] Batch 2051, Loss 0.4802767336368561\n","[Training Epoch 1] Batch 2052, Loss 0.5085222721099854\n","[Training Epoch 1] Batch 2053, Loss 0.5193852186203003\n","[Training Epoch 1] Batch 2054, Loss 0.4601161777973175\n","[Training Epoch 1] Batch 2055, Loss 0.5158665776252747\n","[Training Epoch 1] Batch 2056, Loss 0.49837926030158997\n","[Training Epoch 1] Batch 2057, Loss 0.517059862613678\n","[Training Epoch 1] Batch 2058, Loss 0.4762171506881714\n","[Training Epoch 1] Batch 2059, Loss 0.48305055499076843\n","[Training Epoch 1] Batch 2060, Loss 0.5206393599510193\n","[Training Epoch 1] Batch 2061, Loss 0.5275094509124756\n","[Training Epoch 1] Batch 2062, Loss 0.4797137975692749\n","[Training Epoch 1] Batch 2063, Loss 0.5117854475975037\n","[Training Epoch 1] Batch 2064, Loss 0.5222184658050537\n","[Training Epoch 1] Batch 2065, Loss 0.49782150983810425\n","[Training Epoch 1] Batch 2066, Loss 0.4805622398853302\n","[Training Epoch 1] Batch 2067, Loss 0.5035210847854614\n","[Training Epoch 1] Batch 2068, Loss 0.5022565126419067\n","[Training Epoch 1] Batch 2069, Loss 0.49108412861824036\n","[Training Epoch 1] Batch 2070, Loss 0.5008101463317871\n","[Training Epoch 1] Batch 2071, Loss 0.4873962104320526\n","[Training Epoch 1] Batch 2072, Loss 0.5212221145629883\n","[Training Epoch 1] Batch 2073, Loss 0.4995388984680176\n","[Training Epoch 1] Batch 2074, Loss 0.5106457471847534\n","[Training Epoch 1] Batch 2075, Loss 0.4954010248184204\n","[Training Epoch 1] Batch 2076, Loss 0.5310795307159424\n","[Training Epoch 1] Batch 2077, Loss 0.4950224757194519\n","[Training Epoch 1] Batch 2078, Loss 0.533151388168335\n","[Training Epoch 1] Batch 2079, Loss 0.4954378306865692\n","[Training Epoch 1] Batch 2080, Loss 0.4886798560619354\n","[Training Epoch 1] Batch 2081, Loss 0.516793966293335\n","[Training Epoch 1] Batch 2082, Loss 0.5401993989944458\n","[Training Epoch 1] Batch 2083, Loss 0.5128068327903748\n","[Training Epoch 1] Batch 2084, Loss 0.49778157472610474\n","[Training Epoch 1] Batch 2085, Loss 0.5005587339401245\n","[Training Epoch 1] Batch 2086, Loss 0.4878672659397125\n","[Training Epoch 1] Batch 2087, Loss 0.47073543071746826\n","[Training Epoch 1] Batch 2088, Loss 0.5174120664596558\n","[Training Epoch 1] Batch 2089, Loss 0.5195423364639282\n","[Training Epoch 1] Batch 2090, Loss 0.5158145427703857\n","[Training Epoch 1] Batch 2091, Loss 0.4815938472747803\n","[Training Epoch 1] Batch 2092, Loss 0.517041802406311\n","[Training Epoch 1] Batch 2093, Loss 0.5044662952423096\n","[Training Epoch 1] Batch 2094, Loss 0.4672199785709381\n","[Training Epoch 1] Batch 2095, Loss 0.517288327217102\n","[Training Epoch 1] Batch 2096, Loss 0.5225786566734314\n","[Training Epoch 1] Batch 2097, Loss 0.5032507181167603\n","[Training Epoch 1] Batch 2098, Loss 0.49093443155288696\n","[Training Epoch 1] Batch 2099, Loss 0.4951319098472595\n","[Training Epoch 1] Batch 2100, Loss 0.519554853439331\n","[Training Epoch 1] Batch 2101, Loss 0.4859480857849121\n","[Training Epoch 1] Batch 2102, Loss 0.5208368897438049\n","[Training Epoch 1] Batch 2103, Loss 0.4559236764907837\n","[Training Epoch 1] Batch 2104, Loss 0.4977039098739624\n","[Training Epoch 1] Batch 2105, Loss 0.5114668011665344\n","[Training Epoch 1] Batch 2106, Loss 0.4901292324066162\n","[Training Epoch 1] Batch 2107, Loss 0.5114349126815796\n","[Training Epoch 1] Batch 2108, Loss 0.5149489641189575\n","[Training Epoch 1] Batch 2109, Loss 0.49760425090789795\n","[Training Epoch 1] Batch 2110, Loss 0.5059311389923096\n","[Training Epoch 1] Batch 2111, Loss 0.4829919934272766\n","[Training Epoch 1] Batch 2112, Loss 0.4974399209022522\n","[Training Epoch 1] Batch 2113, Loss 0.4975733160972595\n","[Training Epoch 1] Batch 2114, Loss 0.4865013062953949\n","[Training Epoch 1] Batch 2115, Loss 0.48304420709609985\n","[Training Epoch 1] Batch 2116, Loss 0.4939406216144562\n","[Training Epoch 1] Batch 2117, Loss 0.5273054242134094\n","[Training Epoch 1] Batch 2118, Loss 0.5022101402282715\n","[Training Epoch 1] Batch 2119, Loss 0.5019298195838928\n","[Training Epoch 1] Batch 2120, Loss 0.5065164566040039\n","[Training Epoch 1] Batch 2121, Loss 0.5310710072517395\n","[Training Epoch 1] Batch 2122, Loss 0.49913328886032104\n","[Training Epoch 1] Batch 2123, Loss 0.5090885162353516\n","[Training Epoch 1] Batch 2124, Loss 0.485678493976593\n","[Training Epoch 1] Batch 2125, Loss 0.5122988224029541\n","[Training Epoch 1] Batch 2126, Loss 0.4879024922847748\n","[Training Epoch 1] Batch 2127, Loss 0.49728482961654663\n","[Training Epoch 1] Batch 2128, Loss 0.4876534044742584\n","[Training Epoch 1] Batch 2129, Loss 0.5064467191696167\n","[Training Epoch 1] Batch 2130, Loss 0.5268164277076721\n","[Training Epoch 1] Batch 2131, Loss 0.4733101725578308\n","[Training Epoch 1] Batch 2132, Loss 0.5168849229812622\n","[Training Epoch 1] Batch 2133, Loss 0.4975011348724365\n","[Training Epoch 1] Batch 2134, Loss 0.4900454878807068\n","[Training Epoch 1] Batch 2135, Loss 0.4872824251651764\n","[Training Epoch 1] Batch 2136, Loss 0.4887230098247528\n","[Training Epoch 1] Batch 2137, Loss 0.50876784324646\n","[Training Epoch 1] Batch 2138, Loss 0.4991835057735443\n","[Training Epoch 1] Batch 2139, Loss 0.5060062408447266\n","[Training Epoch 1] Batch 2140, Loss 0.4871620535850525\n","[Training Epoch 1] Batch 2141, Loss 0.4969245195388794\n","[Training Epoch 1] Batch 2142, Loss 0.48036807775497437\n","[Training Epoch 1] Batch 2143, Loss 0.5266523361206055\n","[Training Epoch 1] Batch 2144, Loss 0.49808502197265625\n","[Training Epoch 1] Batch 2145, Loss 0.5063654184341431\n","[Training Epoch 1] Batch 2146, Loss 0.5090524554252625\n","[Training Epoch 1] Batch 2147, Loss 0.47067809104919434\n","[Training Epoch 1] Batch 2148, Loss 0.4991496205329895\n","[Training Epoch 1] Batch 2149, Loss 0.5035806894302368\n","[Training Epoch 1] Batch 2150, Loss 0.5278072357177734\n","[Training Epoch 1] Batch 2151, Loss 0.5058708190917969\n","[Training Epoch 1] Batch 2152, Loss 0.5181465148925781\n","[Training Epoch 1] Batch 2153, Loss 0.49407342076301575\n","[Training Epoch 1] Batch 2154, Loss 0.4827357828617096\n","[Training Epoch 1] Batch 2155, Loss 0.500754177570343\n","[Training Epoch 1] Batch 2156, Loss 0.4884427785873413\n","[Training Epoch 1] Batch 2157, Loss 0.522358775138855\n","[Training Epoch 1] Batch 2158, Loss 0.4802290201187134\n","[Training Epoch 1] Batch 2159, Loss 0.4968770742416382\n","[Training Epoch 1] Batch 2160, Loss 0.5264557600021362\n","[Training Epoch 1] Batch 2161, Loss 0.49915164709091187\n","[Training Epoch 1] Batch 2162, Loss 0.5001575946807861\n","[Training Epoch 1] Batch 2163, Loss 0.4996436536312103\n","[Training Epoch 1] Batch 2164, Loss 0.5220218896865845\n","[Training Epoch 1] Batch 2165, Loss 0.5070067644119263\n","[Training Epoch 1] Batch 2166, Loss 0.483085960149765\n","[Training Epoch 1] Batch 2167, Loss 0.49499914050102234\n","[Training Epoch 1] Batch 2168, Loss 0.489890456199646\n","[Training Epoch 1] Batch 2169, Loss 0.5100739002227783\n","[Training Epoch 1] Batch 2170, Loss 0.4802970886230469\n","[Training Epoch 1] Batch 2171, Loss 0.5157625675201416\n","[Training Epoch 1] Batch 2172, Loss 0.4990733563899994\n","[Training Epoch 1] Batch 2173, Loss 0.4922478497028351\n","[Training Epoch 1] Batch 2174, Loss 0.47601938247680664\n","[Training Epoch 1] Batch 2175, Loss 0.4974795877933502\n","[Training Epoch 1] Batch 2176, Loss 0.5037010312080383\n","[Training Epoch 1] Batch 2177, Loss 0.5085498094558716\n","[Training Epoch 1] Batch 2178, Loss 0.5141763687133789\n","[Training Epoch 1] Batch 2179, Loss 0.4870595932006836\n","[Training Epoch 1] Batch 2180, Loss 0.4944431781768799\n","[Training Epoch 1] Batch 2181, Loss 0.53841233253479\n","[Training Epoch 1] Batch 2182, Loss 0.5160983204841614\n","[Training Epoch 1] Batch 2183, Loss 0.48533308506011963\n","[Training Epoch 1] Batch 2184, Loss 0.49971121549606323\n","[Training Epoch 1] Batch 2185, Loss 0.4777466058731079\n","[Training Epoch 1] Batch 2186, Loss 0.4948062300682068\n","[Training Epoch 1] Batch 2187, Loss 0.5289700031280518\n","[Training Epoch 1] Batch 2188, Loss 0.5073131918907166\n","[Training Epoch 1] Batch 2189, Loss 0.5110645294189453\n","[Training Epoch 1] Batch 2190, Loss 0.5050453543663025\n","[Training Epoch 1] Batch 2191, Loss 0.515598475933075\n","[Training Epoch 1] Batch 2192, Loss 0.516870379447937\n","[Training Epoch 1] Batch 2193, Loss 0.5209528803825378\n","[Training Epoch 1] Batch 2194, Loss 0.5007625818252563\n","[Training Epoch 1] Batch 2195, Loss 0.5094701647758484\n","[Training Epoch 1] Batch 2196, Loss 0.48524969816207886\n","[Training Epoch 1] Batch 2197, Loss 0.5150933265686035\n","[Training Epoch 1] Batch 2198, Loss 0.4965178072452545\n","[Training Epoch 1] Batch 2199, Loss 0.4831130802631378\n","[Training Epoch 1] Batch 2200, Loss 0.5186022520065308\n","[Training Epoch 1] Batch 2201, Loss 0.4835941791534424\n","[Training Epoch 1] Batch 2202, Loss 0.5067698955535889\n","[Training Epoch 1] Batch 2203, Loss 0.49920234084129333\n","[Training Epoch 1] Batch 2204, Loss 0.5049050450325012\n","[Training Epoch 1] Batch 2205, Loss 0.5098305940628052\n","[Training Epoch 1] Batch 2206, Loss 0.4927480220794678\n","[Training Epoch 1] Batch 2207, Loss 0.5278887748718262\n","[Training Epoch 1] Batch 2208, Loss 0.5210285186767578\n","[Training Epoch 1] Batch 2209, Loss 0.5078901052474976\n","[Training Epoch 1] Batch 2210, Loss 0.4889717698097229\n","[Training Epoch 1] Batch 2211, Loss 0.508524477481842\n","[Training Epoch 1] Batch 2212, Loss 0.5074371099472046\n","[Training Epoch 1] Batch 2213, Loss 0.5139762163162231\n","[Training Epoch 1] Batch 2214, Loss 0.4997137784957886\n","[Training Epoch 1] Batch 2215, Loss 0.47878387570381165\n","[Training Epoch 1] Batch 2216, Loss 0.4904361665248871\n","[Training Epoch 1] Batch 2217, Loss 0.4995790719985962\n","[Training Epoch 1] Batch 2218, Loss 0.5125430822372437\n","[Training Epoch 1] Batch 2219, Loss 0.4979461431503296\n","[Training Epoch 1] Batch 2220, Loss 0.5265229940414429\n","[Training Epoch 1] Batch 2221, Loss 0.4925885796546936\n","[Training Epoch 1] Batch 2222, Loss 0.5132194757461548\n","[Training Epoch 1] Batch 2223, Loss 0.48841607570648193\n","[Training Epoch 1] Batch 2224, Loss 0.5098656415939331\n","[Training Epoch 1] Batch 2225, Loss 0.4896553158760071\n","[Training Epoch 1] Batch 2226, Loss 0.5096662044525146\n","[Training Epoch 1] Batch 2227, Loss 0.515705943107605\n","[Training Epoch 1] Batch 2228, Loss 0.46734946966171265\n","[Training Epoch 1] Batch 2229, Loss 0.48605597019195557\n","[Training Epoch 1] Batch 2230, Loss 0.517951250076294\n","[Training Epoch 1] Batch 2231, Loss 0.5152769088745117\n","[Training Epoch 1] Batch 2232, Loss 0.48609501123428345\n","[Training Epoch 1] Batch 2233, Loss 0.5030204653739929\n","[Training Epoch 1] Batch 2234, Loss 0.48352348804473877\n","[Training Epoch 1] Batch 2235, Loss 0.5058642625808716\n","[Training Epoch 1] Batch 2236, Loss 0.49528759717941284\n","[Training Epoch 1] Batch 2237, Loss 0.5045680999755859\n","[Training Epoch 1] Batch 2238, Loss 0.5018250346183777\n","[Training Epoch 1] Batch 2239, Loss 0.5066049098968506\n","[Training Epoch 1] Batch 2240, Loss 0.46880465745925903\n","[Training Epoch 1] Batch 2241, Loss 0.5292439460754395\n","[Training Epoch 1] Batch 2242, Loss 0.516381025314331\n","[Training Epoch 1] Batch 2243, Loss 0.5075060725212097\n","[Training Epoch 1] Batch 2244, Loss 0.5253962278366089\n","[Training Epoch 1] Batch 2245, Loss 0.5345297455787659\n","[Training Epoch 1] Batch 2246, Loss 0.47514116764068604\n","[Training Epoch 1] Batch 2247, Loss 0.4844348728656769\n","[Training Epoch 1] Batch 2248, Loss 0.5017920732498169\n","[Training Epoch 1] Batch 2249, Loss 0.48868006467819214\n","[Training Epoch 1] Batch 2250, Loss 0.5180485844612122\n","[Training Epoch 1] Batch 2251, Loss 0.48197904229164124\n","[Training Epoch 1] Batch 2252, Loss 0.4967707395553589\n","[Training Epoch 1] Batch 2253, Loss 0.4763520359992981\n","[Training Epoch 1] Batch 2254, Loss 0.5381927490234375\n","[Training Epoch 1] Batch 2255, Loss 0.47598791122436523\n","[Training Epoch 1] Batch 2256, Loss 0.5022403001785278\n","[Training Epoch 1] Batch 2257, Loss 0.49801045656204224\n","[Training Epoch 1] Batch 2258, Loss 0.4628044366836548\n","[Training Epoch 1] Batch 2259, Loss 0.5249212980270386\n","[Training Epoch 1] Batch 2260, Loss 0.49855169653892517\n","[Training Epoch 1] Batch 2261, Loss 0.47199422121047974\n","[Training Epoch 1] Batch 2262, Loss 0.5047965049743652\n","[Training Epoch 1] Batch 2263, Loss 0.47797372937202454\n","[Training Epoch 1] Batch 2264, Loss 0.5182391405105591\n","[Training Epoch 1] Batch 2265, Loss 0.5166645050048828\n","[Training Epoch 1] Batch 2266, Loss 0.4739447832107544\n","[Training Epoch 1] Batch 2267, Loss 0.5113891363143921\n","[Training Epoch 1] Batch 2268, Loss 0.5035631656646729\n","[Training Epoch 1] Batch 2269, Loss 0.4922507703304291\n","[Training Epoch 1] Batch 2270, Loss 0.511783242225647\n","[Training Epoch 1] Batch 2271, Loss 0.5302855372428894\n","[Training Epoch 1] Batch 2272, Loss 0.5339078903198242\n","[Training Epoch 1] Batch 2273, Loss 0.48034802079200745\n","[Training Epoch 1] Batch 2274, Loss 0.4862704277038574\n","[Training Epoch 1] Batch 2275, Loss 0.5009520053863525\n","[Training Epoch 1] Batch 2276, Loss 0.521032452583313\n","[Training Epoch 1] Batch 2277, Loss 0.4706263840198517\n","[Training Epoch 1] Batch 2278, Loss 0.5259639024734497\n","[Training Epoch 1] Batch 2279, Loss 0.4902634918689728\n","[Training Epoch 1] Batch 2280, Loss 0.4897176921367645\n","[Training Epoch 1] Batch 2281, Loss 0.49075889587402344\n","[Training Epoch 1] Batch 2282, Loss 0.4818863868713379\n","[Training Epoch 1] Batch 2283, Loss 0.5303277969360352\n","[Training Epoch 1] Batch 2284, Loss 0.5156518220901489\n","[Training Epoch 1] Batch 2285, Loss 0.49663567543029785\n","[Training Epoch 1] Batch 2286, Loss 0.47760093212127686\n","[Training Epoch 1] Batch 2287, Loss 0.49360382556915283\n","[Training Epoch 1] Batch 2288, Loss 0.4695931673049927\n","[Training Epoch 1] Batch 2289, Loss 0.49938878417015076\n","[Training Epoch 1] Batch 2290, Loss 0.5193425416946411\n","[Training Epoch 1] Batch 2291, Loss 0.54239821434021\n","[Training Epoch 1] Batch 2292, Loss 0.47654855251312256\n","[Training Epoch 1] Batch 2293, Loss 0.4739554524421692\n","[Training Epoch 1] Batch 2294, Loss 0.5051027536392212\n","[Training Epoch 1] Batch 2295, Loss 0.5031858682632446\n","[Training Epoch 1] Batch 2296, Loss 0.4915732145309448\n","[Training Epoch 1] Batch 2297, Loss 0.4775373935699463\n","[Training Epoch 1] Batch 2298, Loss 0.4801635146141052\n","[Training Epoch 1] Batch 2299, Loss 0.5116013288497925\n","[Training Epoch 1] Batch 2300, Loss 0.5237174034118652\n","[Training Epoch 1] Batch 2301, Loss 0.47486039996147156\n","[Training Epoch 1] Batch 2302, Loss 0.522290050983429\n","[Training Epoch 1] Batch 2303, Loss 0.5021284222602844\n","[Training Epoch 1] Batch 2304, Loss 0.4752817749977112\n","[Training Epoch 1] Batch 2305, Loss 0.4992469549179077\n","[Training Epoch 1] Batch 2306, Loss 0.4935791492462158\n","[Training Epoch 1] Batch 2307, Loss 0.5055369138717651\n","[Training Epoch 1] Batch 2308, Loss 0.49268418550491333\n","[Training Epoch 1] Batch 2309, Loss 0.5036641955375671\n","[Training Epoch 1] Batch 2310, Loss 0.5139697790145874\n","[Training Epoch 1] Batch 2311, Loss 0.4957963228225708\n","[Training Epoch 1] Batch 2312, Loss 0.4925691485404968\n","[Training Epoch 1] Batch 2313, Loss 0.5124706625938416\n","[Training Epoch 1] Batch 2314, Loss 0.5020675659179688\n","[Training Epoch 1] Batch 2315, Loss 0.4888739585876465\n","[Training Epoch 1] Batch 2316, Loss 0.4794248938560486\n","[Training Epoch 1] Batch 2317, Loss 0.4998660385608673\n","[Training Epoch 1] Batch 2318, Loss 0.5279155969619751\n","[Training Epoch 1] Batch 2319, Loss 0.48606032133102417\n","[Training Epoch 1] Batch 2320, Loss 0.5181005001068115\n","[Training Epoch 1] Batch 2321, Loss 0.5126403570175171\n","[Training Epoch 1] Batch 2322, Loss 0.5023728013038635\n","[Training Epoch 1] Batch 2323, Loss 0.5362415313720703\n","[Training Epoch 1] Batch 2324, Loss 0.47887566685676575\n","[Training Epoch 1] Batch 2325, Loss 0.46426060795783997\n","[Training Epoch 1] Batch 2326, Loss 0.4774072766304016\n","[Training Epoch 1] Batch 2327, Loss 0.4677448868751526\n","[Training Epoch 1] Batch 2328, Loss 0.5076623558998108\n","[Training Epoch 1] Batch 2329, Loss 0.5021511912345886\n","[Training Epoch 1] Batch 2330, Loss 0.5289490222930908\n","[Training Epoch 1] Batch 2331, Loss 0.4775886535644531\n","[Training Epoch 1] Batch 2332, Loss 0.5073700547218323\n","[Training Epoch 1] Batch 2333, Loss 0.5232089757919312\n","[Training Epoch 1] Batch 2334, Loss 0.5076786875724792\n","[Training Epoch 1] Batch 2335, Loss 0.5073760747909546\n","[Training Epoch 1] Batch 2336, Loss 0.4832732677459717\n","[Training Epoch 1] Batch 2337, Loss 0.5074456930160522\n","[Training Epoch 1] Batch 2338, Loss 0.5140297412872314\n","[Training Epoch 1] Batch 2339, Loss 0.5226032733917236\n","[Training Epoch 1] Batch 2340, Loss 0.5058988332748413\n","[Training Epoch 1] Batch 2341, Loss 0.5199919939041138\n","[Training Epoch 1] Batch 2342, Loss 0.48010972142219543\n","[Training Epoch 1] Batch 2343, Loss 0.5021684765815735\n","[Training Epoch 1] Batch 2344, Loss 0.5084260702133179\n","[Training Epoch 1] Batch 2345, Loss 0.5129626989364624\n","[Training Epoch 1] Batch 2346, Loss 0.5092808604240417\n","[Training Epoch 1] Batch 2347, Loss 0.5114405751228333\n","[Training Epoch 1] Batch 2348, Loss 0.5128438472747803\n","[Training Epoch 1] Batch 2349, Loss 0.5403304100036621\n","[Training Epoch 1] Batch 2350, Loss 0.4928678274154663\n","[Training Epoch 1] Batch 2351, Loss 0.47905564308166504\n","[Training Epoch 1] Batch 2352, Loss 0.49785590171813965\n","[Training Epoch 1] Batch 2353, Loss 0.501920759677887\n","[Training Epoch 1] Batch 2354, Loss 0.5059893727302551\n","[Training Epoch 1] Batch 2355, Loss 0.5060984492301941\n","[Training Epoch 1] Batch 2356, Loss 0.4777359366416931\n","[Training Epoch 1] Batch 2357, Loss 0.5006618499755859\n","[Training Epoch 1] Batch 2358, Loss 0.49395549297332764\n","[Training Epoch 1] Batch 2359, Loss 0.5032987594604492\n","[Training Epoch 1] Batch 2360, Loss 0.5020339488983154\n","[Training Epoch 1] Batch 2361, Loss 0.5006618499755859\n","[Training Epoch 1] Batch 2362, Loss 0.5007826685905457\n","[Training Epoch 1] Batch 2363, Loss 0.48591798543930054\n","[Training Epoch 1] Batch 2364, Loss 0.5356104969978333\n","[Training Epoch 1] Batch 2365, Loss 0.5090861916542053\n","[Training Epoch 1] Batch 2366, Loss 0.49091020226478577\n","[Training Epoch 1] Batch 2367, Loss 0.47897687554359436\n","[Training Epoch 1] Batch 2368, Loss 0.4668412506580353\n","[Training Epoch 1] Batch 2369, Loss 0.5153768062591553\n","[Training Epoch 1] Batch 2370, Loss 0.48844319581985474\n","[Training Epoch 1] Batch 2371, Loss 0.4978715777397156\n","[Training Epoch 1] Batch 2372, Loss 0.47903013229370117\n","[Training Epoch 1] Batch 2373, Loss 0.49410706758499146\n","[Training Epoch 1] Batch 2374, Loss 0.47489869594573975\n","[Training Epoch 1] Batch 2375, Loss 0.5287598371505737\n","[Training Epoch 1] Batch 2376, Loss 0.505024790763855\n","[Training Epoch 1] Batch 2377, Loss 0.48978251218795776\n","[Training Epoch 1] Batch 2378, Loss 0.5036396980285645\n","[Training Epoch 1] Batch 2379, Loss 0.489910751581192\n","[Training Epoch 1] Batch 2380, Loss 0.48987340927124023\n","[Training Epoch 1] Batch 2381, Loss 0.5044631361961365\n","[Training Epoch 1] Batch 2382, Loss 0.4708741009235382\n","[Training Epoch 1] Batch 2383, Loss 0.5346877574920654\n","[Training Epoch 1] Batch 2384, Loss 0.4925379753112793\n","[Training Epoch 1] Batch 2385, Loss 0.5104081630706787\n","[Training Epoch 1] Batch 2386, Loss 0.49664732813835144\n","[Training Epoch 1] Batch 2387, Loss 0.4911883473396301\n","[Training Epoch 1] Batch 2388, Loss 0.4925808012485504\n","[Training Epoch 1] Batch 2389, Loss 0.49383705854415894\n","[Training Epoch 1] Batch 2390, Loss 0.5005471706390381\n","[Training Epoch 1] Batch 2391, Loss 0.5193670988082886\n","[Training Epoch 1] Batch 2392, Loss 0.5208371877670288\n","[Training Epoch 1] Batch 2393, Loss 0.48983603715896606\n","[Training Epoch 1] Batch 2394, Loss 0.50053471326828\n","[Training Epoch 1] Batch 2395, Loss 0.4831249713897705\n","[Training Epoch 1] Batch 2396, Loss 0.502158522605896\n","[Training Epoch 1] Batch 2397, Loss 0.5126621723175049\n","[Training Epoch 1] Batch 2398, Loss 0.5279256105422974\n","[Training Epoch 1] Batch 2399, Loss 0.4884164035320282\n","[Training Epoch 1] Batch 2400, Loss 0.4749966859817505\n","[Training Epoch 1] Batch 2401, Loss 0.5062090754508972\n","[Training Epoch 1] Batch 2402, Loss 0.4926005005836487\n","[Training Epoch 1] Batch 2403, Loss 0.492479145526886\n","[Training Epoch 1] Batch 2404, Loss 0.49642863869667053\n","[Training Epoch 1] Batch 2405, Loss 0.5202884674072266\n","[Training Epoch 1] Batch 2406, Loss 0.5055186748504639\n","[Training Epoch 1] Batch 2407, Loss 0.5104847550392151\n","[Training Epoch 1] Batch 2408, Loss 0.5179845094680786\n","[Training Epoch 1] Batch 2409, Loss 0.4882286787033081\n","[Training Epoch 1] Batch 2410, Loss 0.5023778676986694\n","[Training Epoch 1] Batch 2411, Loss 0.49631762504577637\n","[Training Epoch 1] Batch 2412, Loss 0.5143079161643982\n","[Training Epoch 1] Batch 2413, Loss 0.4969184100627899\n","[Training Epoch 1] Batch 2414, Loss 0.5282441973686218\n","[Training Epoch 1] Batch 2415, Loss 0.48979032039642334\n","[Training Epoch 1] Batch 2416, Loss 0.48326361179351807\n","[Training Epoch 1] Batch 2417, Loss 0.5074099898338318\n","[Training Epoch 1] Batch 2418, Loss 0.4957222640514374\n","[Training Epoch 1] Batch 2419, Loss 0.4697478413581848\n","[Training Epoch 1] Batch 2420, Loss 0.5158578157424927\n","[Training Epoch 1] Batch 2421, Loss 0.50079745054245\n","[Training Epoch 1] Batch 2422, Loss 0.5220813751220703\n","[Training Epoch 1] Batch 2423, Loss 0.49546492099761963\n","[Training Epoch 1] Batch 2424, Loss 0.49372154474258423\n","[Training Epoch 1] Batch 2425, Loss 0.5092708468437195\n","[Training Epoch 1] Batch 2426, Loss 0.5134305953979492\n","[Training Epoch 1] Batch 2427, Loss 0.5212559103965759\n","[Training Epoch 1] Batch 2428, Loss 0.5113390684127808\n","[Training Epoch 1] Batch 2429, Loss 0.5117343664169312\n","[Training Epoch 1] Batch 2430, Loss 0.5045977830886841\n","[Training Epoch 1] Batch 2431, Loss 0.4964134395122528\n","[Training Epoch 1] Batch 2432, Loss 0.4924214482307434\n","[Training Epoch 1] Batch 2433, Loss 0.49918538331985474\n","[Training Epoch 1] Batch 2434, Loss 0.5213714838027954\n","[Training Epoch 1] Batch 2435, Loss 0.4686381220817566\n","[Training Epoch 1] Batch 2436, Loss 0.49901795387268066\n","[Training Epoch 1] Batch 2437, Loss 0.5088948011398315\n","[Training Epoch 1] Batch 2438, Loss 0.4855392277240753\n","[Training Epoch 1] Batch 2439, Loss 0.49518850445747375\n","[Training Epoch 1] Batch 2440, Loss 0.49155691266059875\n","[Training Epoch 1] Batch 2441, Loss 0.4994634985923767\n","[Training Epoch 1] Batch 2442, Loss 0.4894554615020752\n","[Training Epoch 1] Batch 2443, Loss 0.49961942434310913\n","[Training Epoch 1] Batch 2444, Loss 0.5127161741256714\n","[Training Epoch 1] Batch 2445, Loss 0.5072722434997559\n","[Training Epoch 1] Batch 2446, Loss 0.5076452493667603\n","[Training Epoch 1] Batch 2447, Loss 0.5033249258995056\n","[Training Epoch 1] Batch 2448, Loss 0.483497679233551\n","[Training Epoch 1] Batch 2449, Loss 0.5086204409599304\n","[Training Epoch 1] Batch 2450, Loss 0.5004380345344543\n","[Training Epoch 1] Batch 2451, Loss 0.47367775440216064\n","[Training Epoch 1] Batch 2452, Loss 0.4825182557106018\n","[Training Epoch 1] Batch 2453, Loss 0.5058571100234985\n","[Training Epoch 1] Batch 2454, Loss 0.501028299331665\n","[Training Epoch 1] Batch 2455, Loss 0.49150171875953674\n","[Training Epoch 1] Batch 2456, Loss 0.5000696778297424\n","[Training Epoch 1] Batch 2457, Loss 0.48715874552726746\n","[Training Epoch 1] Batch 2458, Loss 0.49521952867507935\n","[Training Epoch 1] Batch 2459, Loss 0.4853971004486084\n","[Training Epoch 1] Batch 2460, Loss 0.49497464299201965\n","[Training Epoch 1] Batch 2461, Loss 0.509920597076416\n","[Training Epoch 1] Batch 2462, Loss 0.47755640745162964\n","[Training Epoch 1] Batch 2463, Loss 0.5131765604019165\n","[Training Epoch 1] Batch 2464, Loss 0.49230635166168213\n","[Training Epoch 1] Batch 2465, Loss 0.48418128490448\n","[Training Epoch 1] Batch 2466, Loss 0.47864508628845215\n","[Training Epoch 1] Batch 2467, Loss 0.5019599199295044\n","[Training Epoch 1] Batch 2468, Loss 0.5088992118835449\n","[Training Epoch 1] Batch 2469, Loss 0.5088180899620056\n","[Training Epoch 1] Batch 2470, Loss 0.5141763687133789\n","[Training Epoch 1] Batch 2471, Loss 0.5197271108627319\n","[Training Epoch 1] Batch 2472, Loss 0.4822138547897339\n","[Training Epoch 1] Batch 2473, Loss 0.49344944953918457\n","[Training Epoch 1] Batch 2474, Loss 0.5161029100418091\n","[Training Epoch 1] Batch 2475, Loss 0.4992905259132385\n","[Training Epoch 1] Batch 2476, Loss 0.45838361978530884\n","[Training Epoch 1] Batch 2477, Loss 0.4952927529811859\n","[Training Epoch 1] Batch 2478, Loss 0.5128844976425171\n","[Training Epoch 1] Batch 2479, Loss 0.48463642597198486\n","[Training Epoch 1] Batch 2480, Loss 0.4828508496284485\n","[Training Epoch 1] Batch 2481, Loss 0.48868101835250854\n","[Training Epoch 1] Batch 2482, Loss 0.46644866466522217\n","[Training Epoch 1] Batch 2483, Loss 0.4908352494239807\n","[Training Epoch 1] Batch 2484, Loss 0.5019598007202148\n","[Training Epoch 1] Batch 2485, Loss 0.5169550180435181\n","[Training Epoch 1] Batch 2486, Loss 0.4920746088027954\n","[Training Epoch 1] Batch 2487, Loss 0.5018330812454224\n","[Training Epoch 1] Batch 2488, Loss 0.53305584192276\n","[Training Epoch 1] Batch 2489, Loss 0.5127413272857666\n","[Training Epoch 1] Batch 2490, Loss 0.487096905708313\n","[Training Epoch 1] Batch 2491, Loss 0.49910083413124084\n","[Training Epoch 1] Batch 2492, Loss 0.5003483891487122\n","[Training Epoch 1] Batch 2493, Loss 0.4763706624507904\n","[Training Epoch 1] Batch 2494, Loss 0.48618414998054504\n","[Training Epoch 1] Batch 2495, Loss 0.4981330633163452\n","[Training Epoch 1] Batch 2496, Loss 0.5170344114303589\n","[Training Epoch 1] Batch 2497, Loss 0.4688705801963806\n","[Training Epoch 1] Batch 2498, Loss 0.5156403183937073\n","[Training Epoch 1] Batch 2499, Loss 0.5129830837249756\n","[Training Epoch 1] Batch 2500, Loss 0.5228290557861328\n","[Training Epoch 1] Batch 2501, Loss 0.5090322494506836\n","[Training Epoch 1] Batch 2502, Loss 0.5062700510025024\n","[Training Epoch 1] Batch 2503, Loss 0.5172075033187866\n","[Training Epoch 1] Batch 2504, Loss 0.47752895951271057\n","[Training Epoch 1] Batch 2505, Loss 0.5128428339958191\n","[Training Epoch 1] Batch 2506, Loss 0.5060961246490479\n","[Training Epoch 1] Batch 2507, Loss 0.497600793838501\n","[Training Epoch 1] Batch 2508, Loss 0.5179550647735596\n","[Training Epoch 1] Batch 2509, Loss 0.47237861156463623\n","[Training Epoch 1] Batch 2510, Loss 0.4965001940727234\n","[Training Epoch 1] Batch 2511, Loss 0.4728216528892517\n","[Training Epoch 1] Batch 2512, Loss 0.4802471101284027\n","[Training Epoch 1] Batch 2513, Loss 0.5014673471450806\n","[Training Epoch 1] Batch 2514, Loss 0.5048392415046692\n","[Training Epoch 1] Batch 2515, Loss 0.4702436923980713\n","[Training Epoch 1] Batch 2516, Loss 0.46919798851013184\n","[Training Epoch 1] Batch 2517, Loss 0.505287230014801\n","[Training Epoch 1] Batch 2518, Loss 0.4936425983905792\n","[Training Epoch 1] Batch 2519, Loss 0.539959192276001\n","[Training Epoch 1] Batch 2520, Loss 0.49478477239608765\n","[Training Epoch 1] Batch 2521, Loss 0.47051841020584106\n","[Training Epoch 1] Batch 2522, Loss 0.5170953273773193\n","[Training Epoch 1] Batch 2523, Loss 0.5132877826690674\n","[Training Epoch 1] Batch 2524, Loss 0.4803355038166046\n","[Training Epoch 1] Batch 2525, Loss 0.5161283612251282\n","[Training Epoch 1] Batch 2526, Loss 0.5071954727172852\n","[Training Epoch 1] Batch 2527, Loss 0.5195823311805725\n","[Training Epoch 1] Batch 2528, Loss 0.5223463773727417\n","[Training Epoch 1] Batch 2529, Loss 0.5150711536407471\n","[Training Epoch 1] Batch 2530, Loss 0.5181279182434082\n","[Training Epoch 1] Batch 2531, Loss 0.5227125883102417\n","[Training Epoch 1] Batch 2532, Loss 0.5030361413955688\n","[Training Epoch 1] Batch 2533, Loss 0.4883950352668762\n","[Training Epoch 1] Batch 2534, Loss 0.4802038073539734\n","[Training Epoch 1] Batch 2535, Loss 0.5129176378250122\n","[Training Epoch 1] Batch 2536, Loss 0.5100589394569397\n","[Training Epoch 1] Batch 2537, Loss 0.5141168236732483\n","[Training Epoch 1] Batch 2538, Loss 0.4883912205696106\n","[Training Epoch 1] Batch 2539, Loss 0.48830586671829224\n","[Training Epoch 1] Batch 2540, Loss 0.502992570400238\n","[Training Epoch 1] Batch 2541, Loss 0.4924240708351135\n","[Training Epoch 1] Batch 2542, Loss 0.5030578374862671\n","[Training Epoch 1] Batch 2543, Loss 0.49751514196395874\n","[Training Epoch 1] Batch 2544, Loss 0.522526741027832\n","[Training Epoch 1] Batch 2545, Loss 0.5245263576507568\n","[Training Epoch 1] Batch 2546, Loss 0.4930148720741272\n","[Training Epoch 1] Batch 2547, Loss 0.46218568086624146\n","[Training Epoch 1] Batch 2548, Loss 0.5038390159606934\n","[Training Epoch 1] Batch 2549, Loss 0.5242201089859009\n","[Training Epoch 1] Batch 2550, Loss 0.47914448380470276\n","[Training Epoch 1] Batch 2551, Loss 0.5250514149665833\n","[Training Epoch 1] Batch 2552, Loss 0.5166293382644653\n","[Training Epoch 1] Batch 2553, Loss 0.503374457359314\n","[Training Epoch 1] Batch 2554, Loss 0.49677735567092896\n","[Training Epoch 1] Batch 2555, Loss 0.4752674102783203\n","[Training Epoch 1] Batch 2556, Loss 0.4898265302181244\n","[Training Epoch 1] Batch 2557, Loss 0.4758981168270111\n","[Training Epoch 1] Batch 2558, Loss 0.47054165601730347\n","[Training Epoch 1] Batch 2559, Loss 0.48599931597709656\n","[Training Epoch 1] Batch 2560, Loss 0.4843161106109619\n","[Training Epoch 1] Batch 2561, Loss 0.5007114410400391\n","[Training Epoch 1] Batch 2562, Loss 0.5131097435951233\n","[Training Epoch 1] Batch 2563, Loss 0.5326405763626099\n","[Training Epoch 1] Batch 2564, Loss 0.5079430341720581\n","[Training Epoch 1] Batch 2565, Loss 0.5213267803192139\n","[Training Epoch 1] Batch 2566, Loss 0.5025779008865356\n","[Training Epoch 1] Batch 2567, Loss 0.47727441787719727\n","[Training Epoch 1] Batch 2568, Loss 0.5198348164558411\n","[Training Epoch 1] Batch 2569, Loss 0.49682629108428955\n","[Training Epoch 1] Batch 2570, Loss 0.5004004240036011\n","[Training Epoch 1] Batch 2571, Loss 0.4977503716945648\n","[Training Epoch 1] Batch 2572, Loss 0.49512118101119995\n","[Training Epoch 1] Batch 2573, Loss 0.5136498808860779\n","[Training Epoch 1] Batch 2574, Loss 0.5208976864814758\n","[Training Epoch 1] Batch 2575, Loss 0.49826353788375854\n","[Training Epoch 1] Batch 2576, Loss 0.485772967338562\n","[Training Epoch 1] Batch 2577, Loss 0.5118727684020996\n","[Training Epoch 1] Batch 2578, Loss 0.4926721453666687\n","[Training Epoch 1] Batch 2579, Loss 0.49463438987731934\n","[Training Epoch 1] Batch 2580, Loss 0.5115944743156433\n","[Training Epoch 1] Batch 2581, Loss 0.4843423664569855\n","[Training Epoch 1] Batch 2582, Loss 0.472745805978775\n","[Training Epoch 1] Batch 2583, Loss 0.5487502813339233\n","[Training Epoch 1] Batch 2584, Loss 0.5094239711761475\n","[Training Epoch 1] Batch 2585, Loss 0.504633367061615\n","[Training Epoch 1] Batch 2586, Loss 0.5005534887313843\n","[Training Epoch 1] Batch 2587, Loss 0.4791886806488037\n","[Training Epoch 1] Batch 2588, Loss 0.4882728159427643\n","[Training Epoch 1] Batch 2589, Loss 0.5440031290054321\n","[Training Epoch 1] Batch 2590, Loss 0.5099264979362488\n","[Training Epoch 1] Batch 2591, Loss 0.49811694025993347\n","[Training Epoch 1] Batch 2592, Loss 0.5111969709396362\n","[Training Epoch 1] Batch 2593, Loss 0.48499155044555664\n","[Training Epoch 1] Batch 2594, Loss 0.4975557327270508\n","[Training Epoch 1] Batch 2595, Loss 0.5075461864471436\n","[Training Epoch 1] Batch 2596, Loss 0.5086921453475952\n","[Training Epoch 1] Batch 2597, Loss 0.4860527515411377\n","[Training Epoch 1] Batch 2598, Loss 0.4982178211212158\n","[Training Epoch 1] Batch 2599, Loss 0.5100613832473755\n","[Training Epoch 1] Batch 2600, Loss 0.4977852702140808\n","[Training Epoch 1] Batch 2601, Loss 0.5150711536407471\n","[Training Epoch 1] Batch 2602, Loss 0.5206257104873657\n","[Training Epoch 1] Batch 2603, Loss 0.4995184540748596\n","[Training Epoch 1] Batch 2604, Loss 0.5019048452377319\n","[Training Epoch 1] Batch 2605, Loss 0.4999110996723175\n","[Training Epoch 1] Batch 2606, Loss 0.5125774145126343\n","[Training Epoch 1] Batch 2607, Loss 0.4996379613876343\n","[Training Epoch 1] Batch 2608, Loss 0.49761027097702026\n","[Training Epoch 1] Batch 2609, Loss 0.5017377138137817\n","[Training Epoch 1] Batch 2610, Loss 0.5043165683746338\n","[Training Epoch 1] Batch 2611, Loss 0.5037485361099243\n","[Training Epoch 1] Batch 2612, Loss 0.5153728723526001\n","[Training Epoch 1] Batch 2613, Loss 0.5019735097885132\n","[Training Epoch 1] Batch 2614, Loss 0.502153754234314\n","[Training Epoch 1] Batch 2615, Loss 0.48140230774879456\n","[Training Epoch 1] Batch 2616, Loss 0.4928610920906067\n","[Training Epoch 1] Batch 2617, Loss 0.5183180570602417\n","[Training Epoch 1] Batch 2618, Loss 0.4801885485649109\n","[Training Epoch 1] Batch 2619, Loss 0.4936150014400482\n","[Training Epoch 1] Batch 2620, Loss 0.5223866701126099\n","[Training Epoch 1] Batch 2621, Loss 0.49821901321411133\n","[Training Epoch 1] Batch 2622, Loss 0.46671491861343384\n","[Training Epoch 1] Batch 2623, Loss 0.5077452659606934\n","[Training Epoch 1] Batch 2624, Loss 0.4982767105102539\n","[Training Epoch 1] Batch 2625, Loss 0.48435357213020325\n","[Training Epoch 1] Batch 2626, Loss 0.48593640327453613\n","[Training Epoch 1] Batch 2627, Loss 0.4980800747871399\n","[Training Epoch 1] Batch 2628, Loss 0.5168166160583496\n","[Training Epoch 1] Batch 2629, Loss 0.5010592341423035\n","[Training Epoch 1] Batch 2630, Loss 0.4641326069831848\n","[Training Epoch 1] Batch 2631, Loss 0.5019991993904114\n","[Training Epoch 1] Batch 2632, Loss 0.5265053510665894\n","[Training Epoch 1] Batch 2633, Loss 0.48185113072395325\n","[Training Epoch 1] Batch 2634, Loss 0.48613667488098145\n","[Training Epoch 1] Batch 2635, Loss 0.4985596835613251\n","[Training Epoch 1] Batch 2636, Loss 0.5212287902832031\n","[Training Epoch 1] Batch 2637, Loss 0.4803871512413025\n","[Training Epoch 1] Batch 2638, Loss 0.51659095287323\n","[Training Epoch 1] Batch 2639, Loss 0.4905332922935486\n","[Training Epoch 1] Batch 2640, Loss 0.4941757917404175\n","[Training Epoch 1] Batch 2641, Loss 0.511452853679657\n","[Training Epoch 1] Batch 2642, Loss 0.5007697343826294\n","[Training Epoch 1] Batch 2643, Loss 0.5220522880554199\n","[Training Epoch 1] Batch 2644, Loss 0.47079408168792725\n","[Training Epoch 1] Batch 2645, Loss 0.49232661724090576\n","[Training Epoch 1] Batch 2646, Loss 0.4751374125480652\n","[Training Epoch 1] Batch 2647, Loss 0.4545971751213074\n","[Training Epoch 1] Batch 2648, Loss 0.4641963839530945\n","[Training Epoch 1] Batch 2649, Loss 0.5104931592941284\n","[Training Epoch 1] Batch 2650, Loss 0.501793622970581\n","[Training Epoch 1] Batch 2651, Loss 0.5372154712677002\n","[Training Epoch 1] Batch 2652, Loss 0.4731238782405853\n","[Training Epoch 1] Batch 2653, Loss 0.49501514434814453\n","[Training Epoch 1] Batch 2654, Loss 0.5097314119338989\n","[Training Epoch 1] Batch 2655, Loss 0.5047500133514404\n","[Training Epoch 1] Batch 2656, Loss 0.4963645339012146\n","[Training Epoch 1] Batch 2657, Loss 0.47733786702156067\n","[Training Epoch 1] Batch 2658, Loss 0.4904833436012268\n","[Training Epoch 1] Batch 2659, Loss 0.48355770111083984\n","[Training Epoch 1] Batch 2660, Loss 0.492436945438385\n","[Training Epoch 1] Batch 2661, Loss 0.5125149488449097\n","[Training Epoch 1] Batch 2662, Loss 0.49536216259002686\n","[Training Epoch 1] Batch 2663, Loss 0.507597804069519\n","[Training Epoch 1] Batch 2664, Loss 0.4681919813156128\n","[Training Epoch 1] Batch 2665, Loss 0.503940761089325\n","[Training Epoch 1] Batch 2666, Loss 0.5022064447402954\n","[Training Epoch 1] Batch 2667, Loss 0.5064657926559448\n","[Training Epoch 1] Batch 2668, Loss 0.5034912824630737\n","[Training Epoch 1] Batch 2669, Loss 0.4802281856536865\n","[Training Epoch 1] Batch 2670, Loss 0.5224002599716187\n","[Training Epoch 1] Batch 2671, Loss 0.49125856161117554\n","[Training Epoch 1] Batch 2672, Loss 0.49515300989151\n","[Training Epoch 1] Batch 2673, Loss 0.4893372058868408\n","[Training Epoch 1] Batch 2674, Loss 0.5169161558151245\n","[Training Epoch 1] Batch 2675, Loss 0.5248175859451294\n","[Training Epoch 1] Batch 2676, Loss 0.4762744903564453\n","[Training Epoch 1] Batch 2677, Loss 0.5240959525108337\n","[Training Epoch 1] Batch 2678, Loss 0.5007030963897705\n","[Training Epoch 1] Batch 2679, Loss 0.47212374210357666\n","[Training Epoch 1] Batch 2680, Loss 0.4978005290031433\n","[Training Epoch 1] Batch 2681, Loss 0.48065316677093506\n","[Training Epoch 1] Batch 2682, Loss 0.5003989934921265\n","[Training Epoch 1] Batch 2683, Loss 0.4774930477142334\n","[Training Epoch 1] Batch 2684, Loss 0.4923045039176941\n","[Training Epoch 1] Batch 2685, Loss 0.49235081672668457\n","[Training Epoch 1] Batch 2686, Loss 0.4791659116744995\n","[Training Epoch 1] Batch 2687, Loss 0.5292470455169678\n","[Training Epoch 1] Batch 2688, Loss 0.5173187255859375\n","[Training Epoch 1] Batch 2689, Loss 0.5117610096931458\n","[Training Epoch 1] Batch 2690, Loss 0.5060232877731323\n","[Training Epoch 1] Batch 2691, Loss 0.48257333040237427\n","[Training Epoch 1] Batch 2692, Loss 0.5132112503051758\n","[Training Epoch 1] Batch 2693, Loss 0.4921581447124481\n","[Training Epoch 1] Batch 2694, Loss 0.4993843138217926\n","[Training Epoch 1] Batch 2695, Loss 0.5237658619880676\n","[Training Epoch 1] Batch 2696, Loss 0.5055335164070129\n","[Training Epoch 1] Batch 2697, Loss 0.4990367293357849\n","[Training Epoch 1] Batch 2698, Loss 0.5155743360519409\n","[Training Epoch 1] Batch 2699, Loss 0.4803568124771118\n","[Training Epoch 1] Batch 2700, Loss 0.5073455572128296\n","[Training Epoch 1] Batch 2701, Loss 0.5197297930717468\n","[Training Epoch 1] Batch 2702, Loss 0.5171139240264893\n","[Training Epoch 1] Batch 2703, Loss 0.5451321601867676\n","[Training Epoch 1] Batch 2704, Loss 0.5494117736816406\n","[Training Epoch 1] Batch 2705, Loss 0.5003595352172852\n","[Training Epoch 1] Batch 2706, Loss 0.4969233572483063\n","[Training Epoch 1] Batch 2707, Loss 0.5265448093414307\n","[Training Epoch 1] Batch 2708, Loss 0.5004298090934753\n","[Training Epoch 1] Batch 2709, Loss 0.48445960879325867\n","[Training Epoch 1] Batch 2710, Loss 0.5156165361404419\n","[Training Epoch 1] Batch 2711, Loss 0.5024340152740479\n","[Training Epoch 1] Batch 2712, Loss 0.5221208930015564\n","[Training Epoch 1] Batch 2713, Loss 0.46532538533210754\n","[Training Epoch 1] Batch 2714, Loss 0.49099624156951904\n","[Training Epoch 1] Batch 2715, Loss 0.49848347902297974\n","[Training Epoch 1] Batch 2716, Loss 0.5033712387084961\n","[Training Epoch 1] Batch 2717, Loss 0.49250343441963196\n","[Training Epoch 1] Batch 2718, Loss 0.46564990282058716\n","[Training Epoch 1] Batch 2719, Loss 0.5245822668075562\n","[Training Epoch 1] Batch 2720, Loss 0.5134077668190002\n","[Training Epoch 1] Batch 2721, Loss 0.5320001840591431\n","[Training Epoch 1] Batch 2722, Loss 0.4909879267215729\n","[Training Epoch 1] Batch 2723, Loss 0.4775984287261963\n","[Training Epoch 1] Batch 2724, Loss 0.5010679960250854\n","[Training Epoch 1] Batch 2725, Loss 0.5073158740997314\n","[Training Epoch 1] Batch 2726, Loss 0.479126900434494\n","[Training Epoch 1] Batch 2727, Loss 0.5168511867523193\n","[Training Epoch 1] Batch 2728, Loss 0.4995023012161255\n","[Training Epoch 1] Batch 2729, Loss 0.4805302619934082\n","[Training Epoch 1] Batch 2730, Loss 0.5236731767654419\n","[Training Epoch 1] Batch 2731, Loss 0.5045299530029297\n","[Training Epoch 1] Batch 2732, Loss 0.47238650918006897\n","[Training Epoch 1] Batch 2733, Loss 0.5016434192657471\n","[Training Epoch 1] Batch 2734, Loss 0.5123735666275024\n","[Training Epoch 1] Batch 2735, Loss 0.5166879892349243\n","[Training Epoch 1] Batch 2736, Loss 0.5101356506347656\n","[Training Epoch 1] Batch 2737, Loss 0.488276869058609\n","[Training Epoch 1] Batch 2738, Loss 0.5062588453292847\n","[Training Epoch 1] Batch 2739, Loss 0.4727447032928467\n","[Training Epoch 1] Batch 2740, Loss 0.49802660942077637\n","[Training Epoch 1] Batch 2741, Loss 0.5410939455032349\n","[Training Epoch 1] Batch 2742, Loss 0.4915059208869934\n","[Training Epoch 1] Batch 2743, Loss 0.4992591440677643\n","[Training Epoch 1] Batch 2744, Loss 0.48287808895111084\n","[Training Epoch 1] Batch 2745, Loss 0.4884454905986786\n","[Training Epoch 1] Batch 2746, Loss 0.5027835965156555\n","[Training Epoch 1] Batch 2747, Loss 0.5195424556732178\n","[Training Epoch 1] Batch 2748, Loss 0.5128110647201538\n","[Training Epoch 1] Batch 2749, Loss 0.502068042755127\n","[Training Epoch 1] Batch 2750, Loss 0.48600268363952637\n","[Training Epoch 1] Batch 2751, Loss 0.5110868811607361\n","[Training Epoch 1] Batch 2752, Loss 0.5061180591583252\n","[Training Epoch 1] Batch 2753, Loss 0.5033730268478394\n","[Training Epoch 1] Batch 2754, Loss 0.48863446712493896\n","[Training Epoch 1] Batch 2755, Loss 0.5102862119674683\n","[Training Epoch 1] Batch 2756, Loss 0.511283814907074\n","[Training Epoch 1] Batch 2757, Loss 0.5048441886901855\n","[Training Epoch 1] Batch 2758, Loss 0.47679537534713745\n","[Training Epoch 1] Batch 2759, Loss 0.5166783332824707\n","[Training Epoch 1] Batch 2760, Loss 0.506132960319519\n","[Training Epoch 1] Batch 2761, Loss 0.5033795237541199\n","[Training Epoch 1] Batch 2762, Loss 0.47947531938552856\n","[Training Epoch 1] Batch 2763, Loss 0.4966594874858856\n","[Training Epoch 1] Batch 2764, Loss 0.5006746053695679\n","[Training Epoch 1] Batch 2765, Loss 0.51186603307724\n","[Training Epoch 1] Batch 2766, Loss 0.5211751461029053\n","[Training Epoch 1] Batch 2767, Loss 0.4950248599052429\n","[Training Epoch 1] Batch 2768, Loss 0.5023649334907532\n","[Training Epoch 1] Batch 2769, Loss 0.48780104517936707\n","[Training Epoch 1] Batch 2770, Loss 0.5065756440162659\n","[Training Epoch 1] Batch 2771, Loss 0.47047552466392517\n","[Training Epoch 1] Batch 2772, Loss 0.5090382099151611\n","[Training Epoch 1] Batch 2773, Loss 0.5061572194099426\n","[Training Epoch 1] Batch 2774, Loss 0.5062353610992432\n","[Training Epoch 1] Batch 2775, Loss 0.515569806098938\n","[Training Epoch 1] Batch 2776, Loss 0.5073883533477783\n","[Training Epoch 1] Batch 2777, Loss 0.47364741563796997\n","[Training Epoch 1] Batch 2778, Loss 0.5342665910720825\n","[Training Epoch 1] Batch 2779, Loss 0.506026566028595\n","[Training Epoch 1] Batch 2780, Loss 0.4978679418563843\n","[Training Epoch 1] Batch 2781, Loss 0.4803960621356964\n","[Training Epoch 1] Batch 2782, Loss 0.5330315828323364\n","[Training Epoch 1] Batch 2783, Loss 0.5101211071014404\n","[Training Epoch 1] Batch 2784, Loss 0.473539263010025\n","[Training Epoch 1] Batch 2785, Loss 0.502039909362793\n","[Training Epoch 1] Batch 2786, Loss 0.4884450137615204\n","[Training Epoch 1] Batch 2787, Loss 0.4844140410423279\n","[Training Epoch 1] Batch 2788, Loss 0.49107205867767334\n","[Training Epoch 1] Batch 2789, Loss 0.49653154611587524\n","[Training Epoch 1] Batch 2790, Loss 0.4883156418800354\n","[Training Epoch 1] Batch 2791, Loss 0.5074488520622253\n","[Training Epoch 1] Batch 2792, Loss 0.5016220211982727\n","[Training Epoch 1] Batch 2793, Loss 0.49537408351898193\n","[Training Epoch 1] Batch 2794, Loss 0.4978332221508026\n","[Training Epoch 1] Batch 2795, Loss 0.48307469487190247\n","[Training Epoch 1] Batch 2796, Loss 0.45991456508636475\n","[Training Epoch 1] Batch 2797, Loss 0.4887132942676544\n","[Training Epoch 1] Batch 2798, Loss 0.5278623700141907\n","[Training Epoch 1] Batch 2799, Loss 0.5075137615203857\n","[Training Epoch 1] Batch 2800, Loss 0.5023070573806763\n","[Training Epoch 1] Batch 2801, Loss 0.5142244696617126\n","[Training Epoch 1] Batch 2802, Loss 0.503561794757843\n","[Training Epoch 1] Batch 2803, Loss 0.47775599360466003\n","[Training Epoch 1] Batch 2804, Loss 0.49405163526535034\n","[Training Epoch 1] Batch 2805, Loss 0.49281591176986694\n","[Training Epoch 1] Batch 2806, Loss 0.4886265695095062\n","[Training Epoch 1] Batch 2807, Loss 0.5073351860046387\n","[Training Epoch 1] Batch 2808, Loss 0.5046515464782715\n","[Training Epoch 1] Batch 2809, Loss 0.5426031351089478\n","[Training Epoch 1] Batch 2810, Loss 0.4804462790489197\n","[Training Epoch 1] Batch 2811, Loss 0.537431001663208\n","[Training Epoch 1] Batch 2812, Loss 0.4746284484863281\n","[Training Epoch 1] Batch 2813, Loss 0.4710189700126648\n","[Training Epoch 1] Batch 2814, Loss 0.5145198106765747\n","[Training Epoch 1] Batch 2815, Loss 0.5142219066619873\n","[Training Epoch 1] Batch 2816, Loss 0.48708009719848633\n","[Training Epoch 1] Batch 2817, Loss 0.5147321224212646\n","[Training Epoch 1] Batch 2818, Loss 0.502183735370636\n","[Training Epoch 1] Batch 2819, Loss 0.49129486083984375\n","[Training Epoch 1] Batch 2820, Loss 0.4953128695487976\n","[Training Epoch 1] Batch 2821, Loss 0.5016935467720032\n","[Training Epoch 1] Batch 2822, Loss 0.5003602504730225\n","[Training Epoch 1] Batch 2823, Loss 0.5030909776687622\n","[Training Epoch 1] Batch 2824, Loss 0.4990134537220001\n","[Training Epoch 1] Batch 2825, Loss 0.5072911381721497\n","[Training Epoch 1] Batch 2826, Loss 0.49675828218460083\n","[Training Epoch 1] Batch 2827, Loss 0.4900132417678833\n","[Training Epoch 1] Batch 2828, Loss 0.5084405541419983\n","[Training Epoch 1] Batch 2829, Loss 0.4979592561721802\n","[Training Epoch 1] Batch 2830, Loss 0.47766637802124023\n","[Training Epoch 1] Batch 2831, Loss 0.488150954246521\n","[Training Epoch 1] Batch 2832, Loss 0.5103830099105835\n","[Training Epoch 1] Batch 2833, Loss 0.5023421049118042\n","[Training Epoch 1] Batch 2834, Loss 0.5021824836730957\n","[Training Epoch 1] Batch 2835, Loss 0.4940091371536255\n","[Training Epoch 1] Batch 2836, Loss 0.5022393465042114\n","[Training Epoch 1] Batch 2837, Loss 0.522489070892334\n","[Training Epoch 1] Batch 2838, Loss 0.5072659850120544\n","[Training Epoch 1] Batch 2839, Loss 0.4774992763996124\n","[Training Epoch 1] Batch 2840, Loss 0.47907599806785583\n","[Training Epoch 1] Batch 2841, Loss 0.49783438444137573\n","[Training Epoch 1] Batch 2842, Loss 0.489352285861969\n","[Training Epoch 1] Batch 2843, Loss 0.4899172782897949\n","[Training Epoch 1] Batch 2844, Loss 0.49837392568588257\n","[Training Epoch 1] Batch 2845, Loss 0.4869472086429596\n","[Training Epoch 1] Batch 2846, Loss 0.5278533697128296\n","[Training Epoch 1] Batch 2847, Loss 0.4939393997192383\n","[Training Epoch 1] Batch 2848, Loss 0.5138638019561768\n","[Training Epoch 1] Batch 2849, Loss 0.48974162340164185\n","[Training Epoch 1] Batch 2850, Loss 0.5061837434768677\n","[Training Epoch 1] Batch 2851, Loss 0.504832923412323\n","[Training Epoch 1] Batch 2852, Loss 0.4841231107711792\n","[Training Epoch 1] Batch 2853, Loss 0.517015278339386\n","[Training Epoch 1] Batch 2854, Loss 0.5197345018386841\n","[Training Epoch 1] Batch 2855, Loss 0.5100612640380859\n","[Training Epoch 1] Batch 2856, Loss 0.506257176399231\n","[Training Epoch 1] Batch 2857, Loss 0.5098339915275574\n","[Training Epoch 1] Batch 2858, Loss 0.49787241220474243\n","[Training Epoch 1] Batch 2859, Loss 0.49357885122299194\n","[Training Epoch 1] Batch 2860, Loss 0.4979860186576843\n","[Training Epoch 1] Batch 2861, Loss 0.46915918588638306\n","[Training Epoch 1] Batch 2862, Loss 0.5279579162597656\n","[Training Epoch 1] Batch 2863, Loss 0.5144433975219727\n","[Training Epoch 1] Batch 2864, Loss 0.4930173456668854\n","[Training Epoch 1] Batch 2865, Loss 0.4825478494167328\n","[Training Epoch 1] Batch 2866, Loss 0.515767514705658\n","[Training Epoch 1] Batch 2867, Loss 0.49290552735328674\n","[Training Epoch 1] Batch 2868, Loss 0.5033037662506104\n","[Training Epoch 1] Batch 2869, Loss 0.4898763597011566\n","[Training Epoch 1] Batch 2870, Loss 0.551749587059021\n","[Training Epoch 1] Batch 2871, Loss 0.5254115462303162\n","[Training Epoch 1] Batch 2872, Loss 0.49787279963493347\n","[Training Epoch 1] Batch 2873, Loss 0.5192705988883972\n","[Training Epoch 1] Batch 2874, Loss 0.5170077085494995\n","[Training Epoch 1] Batch 2875, Loss 0.4844661056995392\n","[Training Epoch 1] Batch 2876, Loss 0.5045919418334961\n","[Training Epoch 1] Batch 2877, Loss 0.49668219685554504\n","[Training Epoch 1] Batch 2878, Loss 0.5142322182655334\n","[Training Epoch 1] Batch 2879, Loss 0.4921925663948059\n","[Training Epoch 1] Batch 2880, Loss 0.4884393811225891\n","[Training Epoch 1] Batch 2881, Loss 0.4943621754646301\n","[Training Epoch 1] Batch 2882, Loss 0.5335588455200195\n","[Training Epoch 1] Batch 2883, Loss 0.48621994256973267\n","[Training Epoch 1] Batch 2884, Loss 0.5036587715148926\n","[Training Epoch 1] Batch 2885, Loss 0.4696715772151947\n","[Training Epoch 1] Batch 2886, Loss 0.5114886164665222\n","[Training Epoch 1] Batch 2887, Loss 0.4765026569366455\n","[Training Epoch 1] Batch 2888, Loss 0.5040903091430664\n","[Training Epoch 1] Batch 2889, Loss 0.49106481671333313\n","[Training Epoch 1] Batch 2890, Loss 0.5199293494224548\n","[Training Epoch 1] Batch 2891, Loss 0.49779534339904785\n","[Training Epoch 1] Batch 2892, Loss 0.4898878335952759\n","[Training Epoch 1] Batch 2893, Loss 0.48486262559890747\n","[Training Epoch 1] Batch 2894, Loss 0.4952407479286194\n","[Training Epoch 1] Batch 2895, Loss 0.5061898231506348\n","[Training Epoch 1] Batch 2896, Loss 0.5088174939155579\n","[Training Epoch 1] Batch 2897, Loss 0.4943539798259735\n","[Training Epoch 1] Batch 2898, Loss 0.5162501335144043\n","[Training Epoch 1] Batch 2899, Loss 0.5085147619247437\n","[Training Epoch 1] Batch 2900, Loss 0.49474823474884033\n","[Training Epoch 1] Batch 2901, Loss 0.4936677813529968\n","[Training Epoch 1] Batch 2902, Loss 0.4724767208099365\n","[Training Epoch 1] Batch 2903, Loss 0.5075002908706665\n","[Training Epoch 1] Batch 2904, Loss 0.4941399097442627\n","[Training Epoch 1] Batch 2905, Loss 0.5100940465927124\n","[Training Epoch 1] Batch 2906, Loss 0.513529360294342\n","[Training Epoch 1] Batch 2907, Loss 0.5184279680252075\n","[Training Epoch 1] Batch 2908, Loss 0.5077177286148071\n","[Training Epoch 1] Batch 2909, Loss 0.4981014132499695\n","[Training Epoch 1] Batch 2910, Loss 0.46819740533828735\n","[Training Epoch 1] Batch 2911, Loss 0.5147596001625061\n","[Training Epoch 1] Batch 2912, Loss 0.4900103807449341\n","[Training Epoch 1] Batch 2913, Loss 0.48176130652427673\n","[Training Epoch 1] Batch 2914, Loss 0.521936297416687\n","[Training Epoch 1] Batch 2915, Loss 0.5103518962860107\n","[Training Epoch 1] Batch 2916, Loss 0.49226081371307373\n","[Training Epoch 1] Batch 2917, Loss 0.5151764750480652\n","[Training Epoch 1] Batch 2918, Loss 0.49823999404907227\n","[Training Epoch 1] Batch 2919, Loss 0.5196305513381958\n","[Training Epoch 1] Batch 2920, Loss 0.49088165163993835\n","[Training Epoch 1] Batch 2921, Loss 0.5156901478767395\n","[Training Epoch 1] Batch 2922, Loss 0.4732606112957001\n","[Training Epoch 1] Batch 2923, Loss 0.49111583828926086\n","[Training Epoch 1] Batch 2924, Loss 0.48480334877967834\n","[Training Epoch 1] Batch 2925, Loss 0.4962000846862793\n","[Training Epoch 1] Batch 2926, Loss 0.5142072439193726\n","[Training Epoch 1] Batch 2927, Loss 0.5159522891044617\n","[Training Epoch 1] Batch 2928, Loss 0.5050020813941956\n","[Training Epoch 1] Batch 2929, Loss 0.4887354075908661\n","[Training Epoch 1] Batch 2930, Loss 0.5124063491821289\n","[Training Epoch 1] Batch 2931, Loss 0.5546536445617676\n","[Training Epoch 1] Batch 2932, Loss 0.4954911470413208\n","[Training Epoch 1] Batch 2933, Loss 0.4990573525428772\n","[Training Epoch 1] Batch 2934, Loss 0.492442786693573\n","[Training Epoch 1] Batch 2935, Loss 0.49657946825027466\n","[Training Epoch 1] Batch 2936, Loss 0.5035938024520874\n","[Training Epoch 1] Batch 2937, Loss 0.4940420389175415\n","[Training Epoch 1] Batch 2938, Loss 0.5155189037322998\n","[Training Epoch 1] Batch 2939, Loss 0.4997028708457947\n","[Training Epoch 1] Batch 2940, Loss 0.480476438999176\n","[Training Epoch 1] Batch 2941, Loss 0.49533358216285706\n","[Training Epoch 1] Batch 2942, Loss 0.5157939195632935\n","[Training Epoch 1] Batch 2943, Loss 0.4739660620689392\n","[Training Epoch 1] Batch 2944, Loss 0.47756636142730713\n","[Training Epoch 1] Batch 2945, Loss 0.46552935242652893\n","[Training Epoch 1] Batch 2946, Loss 0.5164952278137207\n","[Training Epoch 1] Batch 2947, Loss 0.49534016847610474\n","[Training Epoch 1] Batch 2948, Loss 0.46943825483322144\n","[Training Epoch 1] Batch 2949, Loss 0.5173153281211853\n","[Training Epoch 1] Batch 2950, Loss 0.4977853298187256\n","[Training Epoch 1] Batch 2951, Loss 0.5182536244392395\n","[Training Epoch 1] Batch 2952, Loss 0.5292925834655762\n","[Training Epoch 1] Batch 2953, Loss 0.5090861916542053\n","[Training Epoch 1] Batch 2954, Loss 0.5180483460426331\n","[Training Epoch 1] Batch 2955, Loss 0.5022841691970825\n","[Training Epoch 1] Batch 2956, Loss 0.4897204041481018\n","[Training Epoch 1] Batch 2957, Loss 0.4963109791278839\n","[Training Epoch 1] Batch 2958, Loss 0.5061349272727966\n","[Training Epoch 1] Batch 2959, Loss 0.4789575934410095\n","[Training Epoch 1] Batch 2960, Loss 0.5181680917739868\n","[Training Epoch 1] Batch 2961, Loss 0.47066646814346313\n","[Training Epoch 1] Batch 2962, Loss 0.4602940082550049\n","[Training Epoch 1] Batch 2963, Loss 0.5230459570884705\n","[Training Epoch 1] Batch 2964, Loss 0.4950469136238098\n","[Training Epoch 1] Batch 2965, Loss 0.47789162397384644\n","[Training Epoch 1] Batch 2966, Loss 0.4720047414302826\n","[Training Epoch 1] Batch 2967, Loss 0.48402121663093567\n","[Training Epoch 1] Batch 2968, Loss 0.48122626543045044\n","[Training Epoch 1] Batch 2969, Loss 0.4843471050262451\n","[Training Epoch 1] Batch 2970, Loss 0.49910667538642883\n","[Training Epoch 1] Batch 2971, Loss 0.5076026320457458\n","[Training Epoch 1] Batch 2972, Loss 0.49809420108795166\n","[Training Epoch 1] Batch 2973, Loss 0.5214273929595947\n","[Training Epoch 1] Batch 2974, Loss 0.4829123616218567\n","[Training Epoch 1] Batch 2975, Loss 0.5075510740280151\n","[Training Epoch 1] Batch 2976, Loss 0.5120668411254883\n","[Training Epoch 1] Batch 2977, Loss 0.4897139072418213\n","[Training Epoch 1] Batch 2978, Loss 0.5186107158660889\n","[Training Epoch 1] Batch 2979, Loss 0.5155998468399048\n","[Training Epoch 1] Batch 2980, Loss 0.4980311989784241\n","[Training Epoch 1] Batch 2981, Loss 0.4859210252761841\n","[Training Epoch 1] Batch 2982, Loss 0.5118629336357117\n","[Training Epoch 1] Batch 2983, Loss 0.5213994979858398\n","[Training Epoch 1] Batch 2984, Loss 0.4668886065483093\n","[Training Epoch 1] Batch 2985, Loss 0.48529359698295593\n","[Training Epoch 1] Batch 2986, Loss 0.5226999521255493\n","[Training Epoch 1] Batch 2987, Loss 0.5034189224243164\n","[Training Epoch 1] Batch 2988, Loss 0.4760672152042389\n","[Training Epoch 1] Batch 2989, Loss 0.48868489265441895\n","[Training Epoch 1] Batch 2990, Loss 0.4995476305484772\n","[Training Epoch 1] Batch 2991, Loss 0.5084762573242188\n","[Training Epoch 1] Batch 2992, Loss 0.5384840369224548\n","[Training Epoch 1] Batch 2993, Loss 0.4800628423690796\n","[Training Epoch 1] Batch 2994, Loss 0.4925473928451538\n","[Training Epoch 1] Batch 2995, Loss 0.5047621130943298\n","[Training Epoch 1] Batch 2996, Loss 0.5106185078620911\n","[Training Epoch 1] Batch 2997, Loss 0.4935939311981201\n","[Training Epoch 1] Batch 2998, Loss 0.5047657489776611\n","[Training Epoch 1] Batch 2999, Loss 0.4964829385280609\n","[Training Epoch 1] Batch 3000, Loss 0.5062967538833618\n","[Training Epoch 1] Batch 3001, Loss 0.5009592771530151\n","[Training Epoch 1] Batch 3002, Loss 0.49794724583625793\n","[Training Epoch 1] Batch 3003, Loss 0.5157783031463623\n","[Training Epoch 1] Batch 3004, Loss 0.47091320157051086\n","[Training Epoch 1] Batch 3005, Loss 0.5034864544868469\n","[Training Epoch 1] Batch 3006, Loss 0.5196871757507324\n","[Training Epoch 1] Batch 3007, Loss 0.5291748046875\n","[Training Epoch 1] Batch 3008, Loss 0.4980725049972534\n","[Training Epoch 1] Batch 3009, Loss 0.49248820543289185\n","[Training Epoch 1] Batch 3010, Loss 0.5254525542259216\n","[Training Epoch 1] Batch 3011, Loss 0.5060784220695496\n","[Training Epoch 1] Batch 3012, Loss 0.5097723007202148\n","[Training Epoch 1] Batch 3013, Loss 0.5006025433540344\n","[Training Epoch 1] Batch 3014, Loss 0.48554515838623047\n","[Training Epoch 1] Batch 3015, Loss 0.47639501094818115\n","[Training Epoch 1] Batch 3016, Loss 0.5183650255203247\n","[Training Epoch 1] Batch 3017, Loss 0.4873654842376709\n","[Training Epoch 1] Batch 3018, Loss 0.5210601091384888\n","[Training Epoch 1] Batch 3019, Loss 0.4911063313484192\n","[Training Epoch 1] Batch 3020, Loss 0.5116553902626038\n","[Training Epoch 1] Batch 3021, Loss 0.4815288186073303\n","[Training Epoch 1] Batch 3022, Loss 0.4912341237068176\n","[Training Epoch 1] Batch 3023, Loss 0.5113427639007568\n","[Training Epoch 1] Batch 3024, Loss 0.5262342691421509\n","[Training Epoch 1] Batch 3025, Loss 0.5252795219421387\n","[Training Epoch 1] Batch 3026, Loss 0.48171213269233704\n","[Training Epoch 1] Batch 3027, Loss 0.47898972034454346\n","[Training Epoch 1] Batch 3028, Loss 0.5064051747322083\n","[Training Epoch 1] Batch 3029, Loss 0.49101781845092773\n","[Training Epoch 1] Batch 3030, Loss 0.514013409614563\n","[Training Epoch 1] Batch 3031, Loss 0.5303172469139099\n","[Training Epoch 1] Batch 3032, Loss 0.5183045268058777\n","[Training Epoch 1] Batch 3033, Loss 0.5359167456626892\n","[Training Epoch 1] Batch 3034, Loss 0.4721989631652832\n","[Training Epoch 1] Batch 3035, Loss 0.5291537046432495\n","[Training Epoch 1] Batch 3036, Loss 0.5182805061340332\n","[Training Epoch 1] Batch 3037, Loss 0.5197750329971313\n","[Training Epoch 1] Batch 3038, Loss 0.4940681457519531\n","[Training Epoch 1] Batch 3039, Loss 0.48870494961738586\n","[Training Epoch 1] Batch 3040, Loss 0.4832809567451477\n","[Training Epoch 1] Batch 3041, Loss 0.4803938865661621\n","[Training Epoch 1] Batch 3042, Loss 0.49657270312309265\n","[Training Epoch 1] Batch 3043, Loss 0.4790697991847992\n","[Training Epoch 1] Batch 3044, Loss 0.5017178654670715\n","[Training Epoch 1] Batch 3045, Loss 0.5047644376754761\n","[Training Epoch 1] Batch 3046, Loss 0.5142483711242676\n","[Training Epoch 1] Batch 3047, Loss 0.5009539127349854\n","[Training Epoch 1] Batch 3048, Loss 0.47785550355911255\n","[Training Epoch 1] Batch 3049, Loss 0.47488006949424744\n","[Training Epoch 1] Batch 3050, Loss 0.5370660424232483\n","[Training Epoch 1] Batch 3051, Loss 0.49959510564804077\n","[Training Epoch 1] Batch 3052, Loss 0.5036184787750244\n","[Training Epoch 1] Batch 3053, Loss 0.5482280850410461\n","[Training Epoch 1] Batch 3054, Loss 0.5141338109970093\n","[Training Epoch 1] Batch 3055, Loss 0.5047309994697571\n","[Training Epoch 1] Batch 3056, Loss 0.5194311141967773\n","[Training Epoch 1] Batch 3057, Loss 0.5128191113471985\n","[Training Epoch 1] Batch 3058, Loss 0.48170438408851624\n","[Training Epoch 1] Batch 3059, Loss 0.506057620048523\n","[Training Epoch 1] Batch 3060, Loss 0.4802629351615906\n","[Training Epoch 1] Batch 3061, Loss 0.5155506730079651\n","[Training Epoch 1] Batch 3062, Loss 0.5126835703849792\n","[Training Epoch 1] Batch 3063, Loss 0.5197962522506714\n","[Training Epoch 1] Batch 3064, Loss 0.5346636772155762\n","[Training Epoch 1] Batch 3065, Loss 0.47764813899993896\n","[Training Epoch 1] Batch 3066, Loss 0.49399334192276\n","[Training Epoch 1] Batch 3067, Loss 0.5130122900009155\n","[Training Epoch 1] Batch 3068, Loss 0.4981808066368103\n","[Training Epoch 1] Batch 3069, Loss 0.4604874849319458\n","[Training Epoch 1] Batch 3070, Loss 0.5627662539482117\n","[Training Epoch 1] Batch 3071, Loss 0.49397021532058716\n","[Training Epoch 1] Batch 3072, Loss 0.48054391145706177\n","[Training Epoch 1] Batch 3073, Loss 0.5035505890846252\n","[Training Epoch 1] Batch 3074, Loss 0.5207464694976807\n","[Training Epoch 1] Batch 3075, Loss 0.49648624658584595\n","[Training Epoch 1] Batch 3076, Loss 0.4938075840473175\n","[Training Epoch 1] Batch 3077, Loss 0.481972336769104\n","[Training Epoch 1] Batch 3078, Loss 0.48322775959968567\n","[Training Epoch 1] Batch 3079, Loss 0.5125912427902222\n","[Training Epoch 1] Batch 3080, Loss 0.4845213294029236\n","[Training Epoch 1] Batch 3081, Loss 0.48298180103302\n","[Training Epoch 1] Batch 3082, Loss 0.507655143737793\n","[Training Epoch 1] Batch 3083, Loss 0.5317970514297485\n","[Training Epoch 1] Batch 3084, Loss 0.4871878921985626\n","[Training Epoch 1] Batch 3085, Loss 0.48454153537750244\n","[Training Epoch 1] Batch 3086, Loss 0.4831409454345703\n","[Training Epoch 1] Batch 3087, Loss 0.508669912815094\n","[Training Epoch 1] Batch 3088, Loss 0.4792764186859131\n","[Training Epoch 1] Batch 3089, Loss 0.5018200874328613\n","[Training Epoch 1] Batch 3090, Loss 0.5305476188659668\n","[Training Epoch 1] Batch 3091, Loss 0.5006557703018188\n","[Training Epoch 1] Batch 3092, Loss 0.507121205329895\n","[Training Epoch 1] Batch 3093, Loss 0.5062189102172852\n","[Training Epoch 1] Batch 3094, Loss 0.5035055875778198\n","[Training Epoch 1] Batch 3095, Loss 0.48184138536453247\n","[Training Epoch 1] Batch 3096, Loss 0.4925159811973572\n","[Training Epoch 1] Batch 3097, Loss 0.46690669655799866\n","[Training Epoch 1] Batch 3098, Loss 0.48027223348617554\n","[Training Epoch 1] Batch 3099, Loss 0.5034283995628357\n","[Training Epoch 1] Batch 3100, Loss 0.49642691016197205\n","[Training Epoch 1] Batch 3101, Loss 0.5032255053520203\n","[Training Epoch 1] Batch 3102, Loss 0.5116887092590332\n","[Training Epoch 1] Batch 3103, Loss 0.4709271192550659\n","[Training Epoch 1] Batch 3104, Loss 0.5207849144935608\n","[Training Epoch 1] Batch 3105, Loss 0.498145192861557\n","[Training Epoch 1] Batch 3106, Loss 0.4781648516654968\n","[Training Epoch 1] Batch 3107, Loss 0.511724591255188\n","[Training Epoch 1] Batch 3108, Loss 0.47886747121810913\n","[Training Epoch 1] Batch 3109, Loss 0.5088451504707336\n","[Training Epoch 1] Batch 3110, Loss 0.485660582780838\n","[Training Epoch 1] Batch 3111, Loss 0.481823205947876\n","[Training Epoch 1] Batch 3112, Loss 0.5227007865905762\n","[Training Epoch 1] Batch 3113, Loss 0.5006606578826904\n","[Training Epoch 1] Batch 3114, Loss 0.5151514410972595\n","[Training Epoch 1] Batch 3115, Loss 0.4734119176864624\n","[Training Epoch 1] Batch 3116, Loss 0.49122264981269836\n","[Training Epoch 1] Batch 3117, Loss 0.49378567934036255\n","[Training Epoch 1] Batch 3118, Loss 0.5087496042251587\n","[Training Epoch 1] Batch 3119, Loss 0.4995403289794922\n","[Training Epoch 1] Batch 3120, Loss 0.4967156946659088\n","[Training Epoch 1] Batch 3121, Loss 0.46948620676994324\n","[Training Epoch 1] Batch 3122, Loss 0.5018563270568848\n","[Training Epoch 1] Batch 3123, Loss 0.4776046872138977\n","[Training Epoch 1] Batch 3124, Loss 0.46826857328414917\n","[Training Epoch 1] Batch 3125, Loss 0.4922596216201782\n","[Training Epoch 1] Batch 3126, Loss 0.5046536922454834\n","[Training Epoch 1] Batch 3127, Loss 0.48033320903778076\n","[Training Epoch 1] Batch 3128, Loss 0.5154322385787964\n","[Training Epoch 1] Batch 3129, Loss 0.48196160793304443\n","[Training Epoch 1] Batch 3130, Loss 0.47321105003356934\n","[Training Epoch 1] Batch 3131, Loss 0.5031167268753052\n","[Training Epoch 1] Batch 3132, Loss 0.5125802159309387\n","[Training Epoch 1] Batch 3133, Loss 0.511467456817627\n","[Training Epoch 1] Batch 3134, Loss 0.5060470700263977\n","[Training Epoch 1] Batch 3135, Loss 0.4611365497112274\n","[Training Epoch 1] Batch 3136, Loss 0.48193925619125366\n","[Training Epoch 1] Batch 3137, Loss 0.5002895593643188\n","[Training Epoch 1] Batch 3138, Loss 0.534906268119812\n","[Training Epoch 1] Batch 3139, Loss 0.502303421497345\n","[Training Epoch 1] Batch 3140, Loss 0.5349507331848145\n","[Training Epoch 1] Batch 3141, Loss 0.4923516809940338\n","[Training Epoch 1] Batch 3142, Loss 0.47396576404571533\n","[Training Epoch 1] Batch 3143, Loss 0.5118176937103271\n","[Training Epoch 1] Batch 3144, Loss 0.4896976053714752\n","[Training Epoch 1] Batch 3145, Loss 0.5033340454101562\n","[Training Epoch 1] Batch 3146, Loss 0.4842455983161926\n","[Training Epoch 1] Batch 3147, Loss 0.48856043815612793\n","[Training Epoch 1] Batch 3148, Loss 0.48442769050598145\n","[Training Epoch 1] Batch 3149, Loss 0.5005626678466797\n","[Training Epoch 1] Batch 3150, Loss 0.5154985189437866\n","[Training Epoch 1] Batch 3151, Loss 0.5350124835968018\n","[Training Epoch 1] Batch 3152, Loss 0.4992787837982178\n","[Training Epoch 1] Batch 3153, Loss 0.4854952394962311\n","[Training Epoch 1] Batch 3154, Loss 0.5173033475875854\n","[Training Epoch 1] Batch 3155, Loss 0.5016888380050659\n","[Training Epoch 1] Batch 3156, Loss 0.4763043522834778\n","[Training Epoch 1] Batch 3157, Loss 0.49016010761260986\n","[Training Epoch 1] Batch 3158, Loss 0.49243393540382385\n","[Training Epoch 1] Batch 3159, Loss 0.5240493416786194\n","[Training Epoch 1] Batch 3160, Loss 0.5318077802658081\n","[Training Epoch 1] Batch 3161, Loss 0.5062423348426819\n","[Training Epoch 1] Batch 3162, Loss 0.5075926780700684\n","[Training Epoch 1] Batch 3163, Loss 0.4869217872619629\n","[Training Epoch 1] Batch 3164, Loss 0.5128143429756165\n","[Training Epoch 1] Batch 3165, Loss 0.5268092155456543\n","[Training Epoch 1] Batch 3166, Loss 0.4845525920391083\n","[Training Epoch 1] Batch 3167, Loss 0.4694996476173401\n","[Training Epoch 1] Batch 3168, Loss 0.5003006458282471\n","[Training Epoch 1] Batch 3169, Loss 0.5138663053512573\n","[Training Epoch 1] Batch 3170, Loss 0.48988914489746094\n","[Training Epoch 1] Batch 3171, Loss 0.4799919128417969\n","[Training Epoch 1] Batch 3172, Loss 0.5029456615447998\n","[Training Epoch 1] Batch 3173, Loss 0.5087270736694336\n","[Training Epoch 1] Batch 3174, Loss 0.5050054788589478\n","[Training Epoch 1] Batch 3175, Loss 0.48686981201171875\n","[Training Epoch 1] Batch 3176, Loss 0.48283129930496216\n","[Training Epoch 1] Batch 3177, Loss 0.5196930170059204\n","[Training Epoch 1] Batch 3178, Loss 0.5497499704360962\n","[Training Epoch 1] Batch 3179, Loss 0.5052180886268616\n","[Training Epoch 1] Batch 3180, Loss 0.5020513534545898\n","[Training Epoch 1] Batch 3181, Loss 0.4870568811893463\n","[Training Epoch 1] Batch 3182, Loss 0.5033907890319824\n","[Training Epoch 1] Batch 3183, Loss 0.5098518133163452\n","[Training Epoch 1] Batch 3184, Loss 0.5144717693328857\n","[Training Epoch 1] Batch 3185, Loss 0.49093183875083923\n","[Training Epoch 1] Batch 3186, Loss 0.5132004022598267\n","[Training Epoch 1] Batch 3187, Loss 0.502434492111206\n","[Training Epoch 1] Batch 3188, Loss 0.4788423180580139\n","[Training Epoch 1] Batch 3189, Loss 0.5265064239501953\n","[Training Epoch 1] Batch 3190, Loss 0.5062035322189331\n","[Training Epoch 1] Batch 3191, Loss 0.48713013529777527\n","[Training Epoch 1] Batch 3192, Loss 0.5144057273864746\n","[Training Epoch 1] Batch 3193, Loss 0.5169895887374878\n","[Training Epoch 1] Batch 3194, Loss 0.5029363632202148\n","[Training Epoch 1] Batch 3195, Loss 0.4814465641975403\n","[Training Epoch 1] Batch 3196, Loss 0.5072253942489624\n","[Training Epoch 1] Batch 3197, Loss 0.4862216114997864\n","[Training Epoch 1] Batch 3198, Loss 0.5058621764183044\n","[Training Epoch 1] Batch 3199, Loss 0.5058847665786743\n","[Training Epoch 1] Batch 3200, Loss 0.4709385931491852\n","[Training Epoch 1] Batch 3201, Loss 0.5125414133071899\n","[Training Epoch 1] Batch 3202, Loss 0.4939855933189392\n","[Training Epoch 1] Batch 3203, Loss 0.5183535814285278\n","[Training Epoch 1] Batch 3204, Loss 0.4682891368865967\n","[Training Epoch 1] Batch 3205, Loss 0.4722347855567932\n","[Training Epoch 1] Batch 3206, Loss 0.48696169257164\n","[Training Epoch 1] Batch 3207, Loss 0.4762599468231201\n","[Training Epoch 1] Batch 3208, Loss 0.5318915247917175\n","[Training Epoch 1] Batch 3209, Loss 0.5254619717597961\n","[Training Epoch 1] Batch 3210, Loss 0.48123157024383545\n","[Training Epoch 1] Batch 3211, Loss 0.4694654941558838\n","[Training Epoch 1] Batch 3212, Loss 0.4830196797847748\n","[Training Epoch 1] Batch 3213, Loss 0.5186693072319031\n","[Training Epoch 1] Batch 3214, Loss 0.4764256477355957\n","[Training Epoch 1] Batch 3215, Loss 0.5062270164489746\n","[Training Epoch 1] Batch 3216, Loss 0.50789475440979\n","[Training Epoch 1] Batch 3217, Loss 0.5209697484970093\n","[Training Epoch 1] Batch 3218, Loss 0.4938129782676697\n","[Training Epoch 1] Batch 3219, Loss 0.4867404103279114\n","[Training Epoch 1] Batch 3220, Loss 0.510581374168396\n","[Training Epoch 1] Batch 3221, Loss 0.5141573548316956\n","[Training Epoch 1] Batch 3222, Loss 0.5067735314369202\n","[Training Epoch 1] Batch 3223, Loss 0.5089910626411438\n","[Training Epoch 1] Batch 3224, Loss 0.5184788703918457\n","[Training Epoch 1] Batch 3225, Loss 0.5048022270202637\n","[Training Epoch 1] Batch 3226, Loss 0.47620850801467896\n","[Training Epoch 1] Batch 3227, Loss 0.4617893099784851\n","[Training Epoch 1] Batch 3228, Loss 0.4968239665031433\n","[Training Epoch 1] Batch 3229, Loss 0.49782878160476685\n","[Training Epoch 1] Batch 3230, Loss 0.5084179639816284\n","[Training Epoch 1] Batch 3231, Loss 0.48731425404548645\n","[Training Epoch 1] Batch 3232, Loss 0.49966198205947876\n","[Training Epoch 1] Batch 3233, Loss 0.5479304790496826\n","[Training Epoch 1] Batch 3234, Loss 0.49076080322265625\n","[Training Epoch 1] Batch 3235, Loss 0.5264215469360352\n","[Training Epoch 1] Batch 3236, Loss 0.5275019407272339\n","[Training Epoch 1] Batch 3237, Loss 0.49088436365127563\n","[Training Epoch 1] Batch 3238, Loss 0.49130889773368835\n","[Training Epoch 1] Batch 3239, Loss 0.5008965730667114\n","[Training Epoch 1] Batch 3240, Loss 0.5167920589447021\n","[Training Epoch 1] Batch 3241, Loss 0.49497056007385254\n","[Training Epoch 1] Batch 3242, Loss 0.5192359685897827\n","[Training Epoch 1] Batch 3243, Loss 0.5017290115356445\n","[Training Epoch 1] Batch 3244, Loss 0.4678104817867279\n","[Training Epoch 1] Batch 3245, Loss 0.5063937306404114\n","[Training Epoch 1] Batch 3246, Loss 0.5142629146575928\n","[Training Epoch 1] Batch 3247, Loss 0.5188345909118652\n","[Training Epoch 1] Batch 3248, Loss 0.48858681321144104\n","[Training Epoch 1] Batch 3249, Loss 0.5009490251541138\n","[Training Epoch 1] Batch 3250, Loss 0.5119660496711731\n","[Training Epoch 1] Batch 3251, Loss 0.5032755136489868\n","[Training Epoch 1] Batch 3252, Loss 0.5063244104385376\n","[Training Epoch 1] Batch 3253, Loss 0.4787595272064209\n","[Training Epoch 1] Batch 3254, Loss 0.49147653579711914\n","[Training Epoch 1] Batch 3255, Loss 0.5035854578018188\n","[Training Epoch 1] Batch 3256, Loss 0.45451024174690247\n","[Training Epoch 1] Batch 3257, Loss 0.4816029667854309\n","[Training Epoch 1] Batch 3258, Loss 0.5143232941627502\n","[Training Epoch 1] Batch 3259, Loss 0.4858623743057251\n","[Training Epoch 1] Batch 3260, Loss 0.5022702813148499\n","[Training Epoch 1] Batch 3261, Loss 0.5155135989189148\n","[Training Epoch 1] Batch 3262, Loss 0.5165711641311646\n","[Training Epoch 1] Batch 3263, Loss 0.493843674659729\n","[Training Epoch 1] Batch 3264, Loss 0.5306789875030518\n","[Training Epoch 1] Batch 3265, Loss 0.5043132305145264\n","[Training Epoch 1] Batch 3266, Loss 0.47755929827690125\n","[Training Epoch 1] Batch 3267, Loss 0.520972490310669\n","[Training Epoch 1] Batch 3268, Loss 0.48163902759552\n","[Training Epoch 1] Batch 3269, Loss 0.5267202854156494\n","[Training Epoch 1] Batch 3270, Loss 0.48461973667144775\n","[Training Epoch 1] Batch 3271, Loss 0.5163539052009583\n","[Training Epoch 1] Batch 3272, Loss 0.5046883225440979\n","[Training Epoch 1] Batch 3273, Loss 0.5087801218032837\n","[Training Epoch 1] Batch 3274, Loss 0.504770040512085\n","[Training Epoch 1] Batch 3275, Loss 0.4790767729282379\n","[Training Epoch 1] Batch 3276, Loss 0.5325886607170105\n","[Training Epoch 1] Batch 3277, Loss 0.516816258430481\n","[Training Epoch 1] Batch 3278, Loss 0.5142456293106079\n","[Training Epoch 1] Batch 3279, Loss 0.5054976940155029\n","[Training Epoch 1] Batch 3280, Loss 0.495344877243042\n","[Training Epoch 1] Batch 3281, Loss 0.49781012535095215\n","[Training Epoch 1] Batch 3282, Loss 0.5084161162376404\n","[Training Epoch 1] Batch 3283, Loss 0.5103601813316345\n","[Training Epoch 1] Batch 3284, Loss 0.4872210919857025\n","[Training Epoch 1] Batch 3285, Loss 0.506477952003479\n","[Training Epoch 1] Batch 3286, Loss 0.5035505890846252\n","[Training Epoch 1] Batch 3287, Loss 0.4809947609901428\n","[Training Epoch 1] Batch 3288, Loss 0.5043567419052124\n","[Training Epoch 1] Batch 3289, Loss 0.5101596713066101\n","[Training Epoch 1] Batch 3290, Loss 0.5141361951828003\n","[Training Epoch 1] Batch 3291, Loss 0.47508704662323\n","[Training Epoch 1] Batch 3292, Loss 0.4616513252258301\n","[Training Epoch 1] Batch 3293, Loss 0.4749767780303955\n","[Training Epoch 1] Batch 3294, Loss 0.5193873047828674\n","[Training Epoch 1] Batch 3295, Loss 0.513964056968689\n","[Training Epoch 1] Batch 3296, Loss 0.5415414571762085\n","[Training Epoch 1] Batch 3297, Loss 0.5260601043701172\n","[Training Epoch 1] Batch 3298, Loss 0.4804821014404297\n","[Training Epoch 1] Batch 3299, Loss 0.5059219002723694\n","[Training Epoch 1] Batch 3300, Loss 0.49655118584632874\n","[Training Epoch 1] Batch 3301, Loss 0.5193864107131958\n","[Training Epoch 1] Batch 3302, Loss 0.4869537353515625\n","[Training Epoch 1] Batch 3303, Loss 0.49812009930610657\n","[Training Epoch 1] Batch 3304, Loss 0.48186254501342773\n","[Training Epoch 1] Batch 3305, Loss 0.5207617282867432\n","[Training Epoch 1] Batch 3306, Loss 0.5019513964653015\n","[Training Epoch 1] Batch 3307, Loss 0.5374048948287964\n","[Training Epoch 1] Batch 3308, Loss 0.5168372988700867\n","[Training Epoch 1] Batch 3309, Loss 0.5299537181854248\n","[Training Epoch 1] Batch 3310, Loss 0.5036527514457703\n","[Training Epoch 1] Batch 3311, Loss 0.5113052129745483\n","[Training Epoch 1] Batch 3312, Loss 0.49249267578125\n","[Training Epoch 1] Batch 3313, Loss 0.5195460319519043\n","[Training Epoch 1] Batch 3314, Loss 0.47397667169570923\n","[Training Epoch 1] Batch 3315, Loss 0.4870467483997345\n","[Training Epoch 1] Batch 3316, Loss 0.5181334614753723\n","[Training Epoch 1] Batch 3317, Loss 0.4719124734401703\n","[Training Epoch 1] Batch 3318, Loss 0.45204222202301025\n","[Training Epoch 1] Batch 3319, Loss 0.5115844011306763\n","[Training Epoch 1] Batch 3320, Loss 0.49906331300735474\n","[Training Epoch 1] Batch 3321, Loss 0.5070849657058716\n","[Training Epoch 1] Batch 3322, Loss 0.4915315806865692\n","[Training Epoch 1] Batch 3323, Loss 0.4857083559036255\n","[Training Epoch 1] Batch 3324, Loss 0.5069873332977295\n","[Training Epoch 1] Batch 3325, Loss 0.5031715631484985\n","[Training Epoch 1] Batch 3326, Loss 0.5122302174568176\n","[Training Epoch 1] Batch 3327, Loss 0.5115787386894226\n","[Training Epoch 1] Batch 3328, Loss 0.48849570751190186\n","[Training Epoch 1] Batch 3329, Loss 0.4955873191356659\n","[Training Epoch 1] Batch 3330, Loss 0.5237520337104797\n","[Training Epoch 1] Batch 3331, Loss 0.5165013074874878\n","[Training Epoch 1] Batch 3332, Loss 0.4978022575378418\n","[Training Epoch 1] Batch 3333, Loss 0.49254339933395386\n","[Training Epoch 1] Batch 3334, Loss 0.49807223677635193\n","[Training Epoch 1] Batch 3335, Loss 0.47781407833099365\n","[Training Epoch 1] Batch 3336, Loss 0.490438848733902\n","[Training Epoch 1] Batch 3337, Loss 0.4918573796749115\n","[Training Epoch 1] Batch 3338, Loss 0.5125354528427124\n","[Training Epoch 1] Batch 3339, Loss 0.47094231843948364\n","[Training Epoch 1] Batch 3340, Loss 0.5102217793464661\n","[Training Epoch 1] Batch 3341, Loss 0.4824165403842926\n","[Training Epoch 1] Batch 3342, Loss 0.4793291687965393\n","[Training Epoch 1] Batch 3343, Loss 0.48299241065979004\n","[Training Epoch 1] Batch 3344, Loss 0.5275978446006775\n","[Training Epoch 1] Batch 3345, Loss 0.5041054487228394\n","[Training Epoch 1] Batch 3346, Loss 0.5419085621833801\n","[Training Epoch 1] Batch 3347, Loss 0.49815526604652405\n","[Training Epoch 1] Batch 3348, Loss 0.5103621482849121\n","[Training Epoch 1] Batch 3349, Loss 0.48854342103004456\n","[Training Epoch 1] Batch 3350, Loss 0.48939263820648193\n","[Training Epoch 1] Batch 3351, Loss 0.5209569334983826\n","[Training Epoch 1] Batch 3352, Loss 0.4991414546966553\n","[Training Epoch 1] Batch 3353, Loss 0.5047141909599304\n","[Training Epoch 1] Batch 3354, Loss 0.5143835544586182\n","[Training Epoch 1] Batch 3355, Loss 0.4767592251300812\n","[Training Epoch 1] Batch 3356, Loss 0.5298287272453308\n","[Training Epoch 1] Batch 3357, Loss 0.4938820004463196\n","[Training Epoch 1] Batch 3358, Loss 0.515710711479187\n","[Training Epoch 1] Batch 3359, Loss 0.48693132400512695\n","[Training Epoch 1] Batch 3360, Loss 0.5135909914970398\n","[Training Epoch 1] Batch 3361, Loss 0.4816674590110779\n","[Training Epoch 1] Batch 3362, Loss 0.4905639886856079\n","[Training Epoch 1] Batch 3363, Loss 0.5030425786972046\n","[Training Epoch 1] Batch 3364, Loss 0.5142167806625366\n","[Training Epoch 1] Batch 3365, Loss 0.494998037815094\n","[Training Epoch 1] Batch 3366, Loss 0.4909898042678833\n","[Training Epoch 1] Batch 3367, Loss 0.49635785818099976\n","[Training Epoch 1] Batch 3368, Loss 0.49141448736190796\n","[Training Epoch 1] Batch 3369, Loss 0.5047708749771118\n","[Training Epoch 1] Batch 3370, Loss 0.5154143571853638\n","[Training Epoch 1] Batch 3371, Loss 0.4535364508628845\n","[Training Epoch 1] Batch 3372, Loss 0.4856560230255127\n","[Training Epoch 1] Batch 3373, Loss 0.5126965045928955\n","[Training Epoch 1] Batch 3374, Loss 0.5212475061416626\n","[Training Epoch 1] Batch 3375, Loss 0.47007864713668823\n","[Training Epoch 1] Batch 3376, Loss 0.5048179030418396\n","[Training Epoch 1] Batch 3377, Loss 0.47626084089279175\n","[Training Epoch 1] Batch 3378, Loss 0.5005127191543579\n","[Training Epoch 1] Batch 3379, Loss 0.494488388299942\n","[Training Epoch 1] Batch 3380, Loss 0.5074785947799683\n","[Training Epoch 1] Batch 3381, Loss 0.5125752687454224\n","[Training Epoch 1] Batch 3382, Loss 0.5060691833496094\n","[Training Epoch 1] Batch 3383, Loss 0.49128714203834534\n","[Training Epoch 1] Batch 3384, Loss 0.5111607313156128\n","[Training Epoch 1] Batch 3385, Loss 0.5175854563713074\n","[Training Epoch 1] Batch 3386, Loss 0.510195791721344\n","[Training Epoch 1] Batch 3387, Loss 0.5006870031356812\n","[Training Epoch 1] Batch 3388, Loss 0.4867687225341797\n","[Training Epoch 1] Batch 3389, Loss 0.5323413610458374\n","[Training Epoch 1] Batch 3390, Loss 0.4870752990245819\n","[Training Epoch 1] Batch 3391, Loss 0.49031949043273926\n","[Training Epoch 1] Batch 3392, Loss 0.4938427209854126\n","[Training Epoch 1] Batch 3393, Loss 0.5077227354049683\n","[Training Epoch 1] Batch 3394, Loss 0.5316327214241028\n","[Training Epoch 1] Batch 3395, Loss 0.5096606612205505\n","[Training Epoch 1] Batch 3396, Loss 0.5232081413269043\n","[Training Epoch 1] Batch 3397, Loss 0.49637842178344727\n","[Training Epoch 1] Batch 3398, Loss 0.5056312084197998\n","[Training Epoch 1] Batch 3399, Loss 0.4788862466812134\n","[Training Epoch 1] Batch 3400, Loss 0.4897478520870209\n","[Training Epoch 1] Batch 3401, Loss 0.49962013959884644\n","[Training Epoch 1] Batch 3402, Loss 0.49286600947380066\n","[Training Epoch 1] Batch 3403, Loss 0.49204930663108826\n","[Training Epoch 1] Batch 3404, Loss 0.5173478126525879\n","[Training Epoch 1] Batch 3405, Loss 0.519437313079834\n","[Training Epoch 1] Batch 3406, Loss 0.508952796459198\n","[Training Epoch 1] Batch 3407, Loss 0.49977999925613403\n","[Training Epoch 1] Batch 3408, Loss 0.5231878161430359\n","[Training Epoch 1] Batch 3409, Loss 0.5169895887374878\n","[Training Epoch 1] Batch 3410, Loss 0.4971024990081787\n","[Training Epoch 1] Batch 3411, Loss 0.47112321853637695\n","[Training Epoch 1] Batch 3412, Loss 0.4938599467277527\n","[Training Epoch 1] Batch 3413, Loss 0.4981521964073181\n","[Training Epoch 1] Batch 3414, Loss 0.47506362199783325\n","[Training Epoch 1] Batch 3415, Loss 0.5019137263298035\n","[Training Epoch 1] Batch 3416, Loss 0.4899651110172272\n","[Training Epoch 1] Batch 3417, Loss 0.5048511028289795\n","[Training Epoch 1] Batch 3418, Loss 0.4869557321071625\n","[Training Epoch 1] Batch 3419, Loss 0.5091257691383362\n","[Training Epoch 1] Batch 3420, Loss 0.5157933831214905\n","[Training Epoch 1] Batch 3421, Loss 0.4783146381378174\n","[Training Epoch 1] Batch 3422, Loss 0.5076252222061157\n","[Training Epoch 1] Batch 3423, Loss 0.48350873589515686\n","[Training Epoch 1] Batch 3424, Loss 0.5058740377426147\n","[Training Epoch 1] Batch 3425, Loss 0.516973614692688\n","[Training Epoch 1] Batch 3426, Loss 0.5006973743438721\n","[Training Epoch 1] Batch 3427, Loss 0.5020902156829834\n","[Training Epoch 1] Batch 3428, Loss 0.5036883354187012\n","[Training Epoch 1] Batch 3429, Loss 0.49708712100982666\n","[Training Epoch 1] Batch 3430, Loss 0.48143914341926575\n","[Training Epoch 1] Batch 3431, Loss 0.4911699593067169\n","[Training Epoch 1] Batch 3432, Loss 0.4844883680343628\n","[Training Epoch 1] Batch 3433, Loss 0.47369399666786194\n","[Training Epoch 1] Batch 3434, Loss 0.49923038482666016\n","[Training Epoch 1] Batch 3435, Loss 0.5057492852210999\n","[Training Epoch 1] Batch 3436, Loss 0.47035327553749084\n","[Training Epoch 1] Batch 3437, Loss 0.48734036087989807\n","[Training Epoch 1] Batch 3438, Loss 0.49549350142478943\n","[Training Epoch 1] Batch 3439, Loss 0.5192989706993103\n","[Training Epoch 1] Batch 3440, Loss 0.5036186575889587\n","[Training Epoch 1] Batch 3441, Loss 0.5204755067825317\n","[Training Epoch 1] Batch 3442, Loss 0.5072917938232422\n","[Training Epoch 1] Batch 3443, Loss 0.5057767629623413\n","[Training Epoch 1] Batch 3444, Loss 0.4669094383716583\n","[Training Epoch 1] Batch 3445, Loss 0.4746268689632416\n","[Training Epoch 1] Batch 3446, Loss 0.4914330542087555\n","[Training Epoch 1] Batch 3447, Loss 0.504673421382904\n","[Training Epoch 1] Batch 3448, Loss 0.5083909034729004\n","[Training Epoch 1] Batch 3449, Loss 0.5112502574920654\n","[Training Epoch 1] Batch 3450, Loss 0.5274773836135864\n","[Training Epoch 1] Batch 3451, Loss 0.5071393251419067\n","[Training Epoch 1] Batch 3452, Loss 0.518054723739624\n","[Training Epoch 1] Batch 3453, Loss 0.48698189854621887\n","[Training Epoch 1] Batch 3454, Loss 0.4950326383113861\n","[Training Epoch 1] Batch 3455, Loss 0.46971243619918823\n","[Training Epoch 1] Batch 3456, Loss 0.5005223155021667\n","[Training Epoch 1] Batch 3457, Loss 0.5062457323074341\n","[Training Epoch 1] Batch 3458, Loss 0.5155917406082153\n","[Training Epoch 1] Batch 3459, Loss 0.5045486688613892\n","[Training Epoch 1] Batch 3460, Loss 0.46494120359420776\n","[Training Epoch 1] Batch 3461, Loss 0.5063119530677795\n","[Training Epoch 1] Batch 3462, Loss 0.4671732783317566\n","[Training Epoch 1] Batch 3463, Loss 0.4801669716835022\n","[Training Epoch 1] Batch 3464, Loss 0.5174663662910461\n","[Training Epoch 1] Batch 3465, Loss 0.4833890199661255\n","[Training Epoch 1] Batch 3466, Loss 0.519315242767334\n","[Training Epoch 1] Batch 3467, Loss 0.48577213287353516\n","[Training Epoch 1] Batch 3468, Loss 0.5154617428779602\n","[Training Epoch 1] Batch 3469, Loss 0.5162585973739624\n","[Training Epoch 1] Batch 3470, Loss 0.4891493618488312\n","[Training Epoch 1] Batch 3471, Loss 0.4905143082141876\n","[Training Epoch 1] Batch 3472, Loss 0.4771648049354553\n","[Training Epoch 1] Batch 3473, Loss 0.4980278015136719\n","[Training Epoch 1] Batch 3474, Loss 0.500409722328186\n","[Training Epoch 1] Batch 3475, Loss 0.5371359586715698\n","[Training Epoch 1] Batch 3476, Loss 0.5171153545379639\n","[Training Epoch 1] Batch 3477, Loss 0.4686000347137451\n","[Training Epoch 1] Batch 3478, Loss 0.49306613206863403\n","[Training Epoch 1] Batch 3479, Loss 0.49446260929107666\n","[Training Epoch 1] Batch 3480, Loss 0.5214307308197021\n","[Training Epoch 1] Batch 3481, Loss 0.48697054386138916\n","[Training Epoch 1] Batch 3482, Loss 0.48105138540267944\n","[Training Epoch 1] Batch 3483, Loss 0.5399903059005737\n","[Training Epoch 1] Batch 3484, Loss 0.5345083475112915\n","[Training Epoch 1] Batch 3485, Loss 0.5185304880142212\n","[Training Epoch 1] Batch 3486, Loss 0.47305867075920105\n","[Training Epoch 1] Batch 3487, Loss 0.496102511882782\n","[Training Epoch 1] Batch 3488, Loss 0.4718182682991028\n","[Training Epoch 1] Batch 3489, Loss 0.5128130912780762\n","[Training Epoch 1] Batch 3490, Loss 0.500429093837738\n","[Training Epoch 1] Batch 3491, Loss 0.48062169551849365\n","[Training Epoch 1] Batch 3492, Loss 0.5374698638916016\n","[Training Epoch 1] Batch 3493, Loss 0.5041429400444031\n","[Training Epoch 1] Batch 3494, Loss 0.4794646203517914\n","[Training Epoch 1] Batch 3495, Loss 0.48153406381607056\n","[Training Epoch 1] Batch 3496, Loss 0.4847356677055359\n","[Training Epoch 1] Batch 3497, Loss 0.4764466881752014\n","[Training Epoch 1] Batch 3498, Loss 0.4995354413986206\n","[Training Epoch 1] Batch 3499, Loss 0.4816203713417053\n","[Training Epoch 1] Batch 3500, Loss 0.5195460319519043\n","[Training Epoch 1] Batch 3501, Loss 0.5315863490104675\n","[Training Epoch 1] Batch 3502, Loss 0.49376511573791504\n","[Training Epoch 1] Batch 3503, Loss 0.5018652677536011\n","[Training Epoch 1] Batch 3504, Loss 0.48363396525382996\n","[Training Epoch 1] Batch 3505, Loss 0.493661105632782\n","[Training Epoch 1] Batch 3506, Loss 0.5076278448104858\n","[Training Epoch 1] Batch 3507, Loss 0.4943520426750183\n","[Training Epoch 1] Batch 3508, Loss 0.5016595125198364\n","[Training Epoch 1] Batch 3509, Loss 0.5091637969017029\n","[Training Epoch 1] Batch 3510, Loss 0.4735493063926697\n","[Training Epoch 1] Batch 3511, Loss 0.4864227771759033\n","[Training Epoch 1] Batch 3512, Loss 0.5089497566223145\n","[Training Epoch 1] Batch 3513, Loss 0.4694647192955017\n","[Training Epoch 1] Batch 3514, Loss 0.4842492938041687\n","[Training Epoch 1] Batch 3515, Loss 0.4873420298099518\n","[Training Epoch 1] Batch 3516, Loss 0.48830345273017883\n","[Training Epoch 1] Batch 3517, Loss 0.5498886108398438\n","[Training Epoch 1] Batch 3518, Loss 0.5076399445533752\n","[Training Epoch 1] Batch 3519, Loss 0.49397578835487366\n","[Training Epoch 1] Batch 3520, Loss 0.47231942415237427\n","[Training Epoch 1] Batch 3521, Loss 0.5050088763237\n","[Training Epoch 1] Batch 3522, Loss 0.5323617458343506\n","[Training Epoch 1] Batch 3523, Loss 0.499176561832428\n","[Training Epoch 1] Batch 3524, Loss 0.5100061893463135\n","[Training Epoch 1] Batch 3525, Loss 0.5151058435440063\n","[Training Epoch 1] Batch 3526, Loss 0.4637530446052551\n","[Training Epoch 1] Batch 3527, Loss 0.5146872997283936\n","[Training Epoch 1] Batch 3528, Loss 0.5156192779541016\n","[Training Epoch 1] Batch 3529, Loss 0.5058283805847168\n","[Training Epoch 1] Batch 3530, Loss 0.5232673287391663\n","[Training Epoch 1] Batch 3531, Loss 0.51179039478302\n","[Training Epoch 1] Batch 3532, Loss 0.5222889184951782\n","[Training Epoch 1] Batch 3533, Loss 0.49104759097099304\n","[Training Epoch 1] Batch 3534, Loss 0.5102642178535461\n","[Training Epoch 1] Batch 3535, Loss 0.518216609954834\n","[Training Epoch 1] Batch 3536, Loss 0.47938570380210876\n","[Training Epoch 1] Batch 3537, Loss 0.49905890226364136\n","[Training Epoch 1] Batch 3538, Loss 0.5071386694908142\n","[Training Epoch 1] Batch 3539, Loss 0.4911087155342102\n","[Training Epoch 1] Batch 3540, Loss 0.505252480506897\n","[Training Epoch 1] Batch 3541, Loss 0.4926561713218689\n","[Training Epoch 1] Batch 3542, Loss 0.5058925747871399\n","[Training Epoch 1] Batch 3543, Loss 0.5402124524116516\n","[Training Epoch 1] Batch 3544, Loss 0.5360671281814575\n","[Training Epoch 1] Batch 3545, Loss 0.4813259243965149\n","[Training Epoch 1] Batch 3546, Loss 0.4977602958679199\n","[Training Epoch 1] Batch 3547, Loss 0.5587339401245117\n","[Training Epoch 1] Batch 3548, Loss 0.49812379479408264\n","[Training Epoch 1] Batch 3549, Loss 0.5197876691818237\n","[Training Epoch 1] Batch 3550, Loss 0.5308318734169006\n","[Training Epoch 1] Batch 3551, Loss 0.5110524892807007\n","[Training Epoch 1] Batch 3552, Loss 0.5017009973526001\n","[Training Epoch 1] Batch 3553, Loss 0.5374826192855835\n","[Training Epoch 1] Batch 3554, Loss 0.5102505683898926\n","[Training Epoch 1] Batch 3555, Loss 0.5167331695556641\n","[Training Epoch 1] Batch 3556, Loss 0.5231844186782837\n","[Training Epoch 1] Batch 3557, Loss 0.4924927353858948\n","[Training Epoch 1] Batch 3558, Loss 0.4925474524497986\n","[Training Epoch 1] Batch 3559, Loss 0.46554338932037354\n","[Training Epoch 1] Batch 3560, Loss 0.5006542205810547\n","[Training Epoch 1] Batch 3561, Loss 0.4639207124710083\n","[Training Epoch 1] Batch 3562, Loss 0.4707925319671631\n","[Training Epoch 1] Batch 3563, Loss 0.4807898998260498\n","[Training Epoch 1] Batch 3564, Loss 0.48753005266189575\n","[Training Epoch 1] Batch 3565, Loss 0.48852014541625977\n","[Training Epoch 1] Batch 3566, Loss 0.4805646538734436\n","[Training Epoch 1] Batch 3567, Loss 0.5006317496299744\n","[Training Epoch 1] Batch 3568, Loss 0.49089205265045166\n","[Training Epoch 1] Batch 3569, Loss 0.5117417573928833\n","[Training Epoch 1] Batch 3570, Loss 0.4857773184776306\n","[Training Epoch 1] Batch 3571, Loss 0.476537823677063\n","[Training Epoch 1] Batch 3572, Loss 0.5006564855575562\n","[Training Epoch 1] Batch 3573, Loss 0.5211756229400635\n","[Training Epoch 1] Batch 3574, Loss 0.5073400139808655\n","[Training Epoch 1] Batch 3575, Loss 0.48574298620224\n","[Training Epoch 1] Batch 3576, Loss 0.5194873213768005\n","[Training Epoch 1] Batch 3577, Loss 0.4834889769554138\n","[Training Epoch 1] Batch 3578, Loss 0.4857093095779419\n","[Training Epoch 1] Batch 3579, Loss 0.49928975105285645\n","[Training Epoch 1] Batch 3580, Loss 0.5026522278785706\n","[Training Epoch 1] Batch 3581, Loss 0.4904528558254242\n","[Training Epoch 1] Batch 3582, Loss 0.5289754271507263\n","[Training Epoch 1] Batch 3583, Loss 0.4845999479293823\n","[Training Epoch 1] Batch 3584, Loss 0.49961167573928833\n","[Training Epoch 1] Batch 3585, Loss 0.4994226098060608\n","[Training Epoch 1] Batch 3586, Loss 0.4788326025009155\n","[Training Epoch 1] Batch 3587, Loss 0.48261648416519165\n","[Training Epoch 1] Batch 3588, Loss 0.5195698738098145\n","[Training Epoch 1] Batch 3589, Loss 0.49782755970954895\n","[Training Epoch 1] Batch 3590, Loss 0.5275354385375977\n","[Training Epoch 1] Batch 3591, Loss 0.4763818383216858\n","[Training Epoch 1] Batch 3592, Loss 0.5040799379348755\n","[Training Epoch 1] Batch 3593, Loss 0.48619574308395386\n","[Training Epoch 1] Batch 3594, Loss 0.492442786693573\n","[Training Epoch 1] Batch 3595, Loss 0.48461833596229553\n","[Training Epoch 1] Batch 3596, Loss 0.521030068397522\n","[Training Epoch 1] Batch 3597, Loss 0.5013072490692139\n","[Training Epoch 1] Batch 3598, Loss 0.5168423652648926\n","[Training Epoch 1] Batch 3599, Loss 0.5330204963684082\n","[Training Epoch 1] Batch 3600, Loss 0.5086888074874878\n","[Training Epoch 1] Batch 3601, Loss 0.48883524537086487\n","[Training Epoch 1] Batch 3602, Loss 0.5160470008850098\n","[Training Epoch 1] Batch 3603, Loss 0.48388251662254333\n","[Training Epoch 1] Batch 3604, Loss 0.4783663749694824\n","[Training Epoch 1] Batch 3605, Loss 0.5260330438613892\n","[Training Epoch 1] Batch 3606, Loss 0.49132806062698364\n","[Training Epoch 1] Batch 3607, Loss 0.48945051431655884\n","[Training Epoch 1] Batch 3608, Loss 0.5310623049736023\n","[Training Epoch 1] Batch 3609, Loss 0.46882766485214233\n","[Training Epoch 1] Batch 3610, Loss 0.5039470195770264\n","[Training Epoch 1] Batch 3611, Loss 0.48199498653411865\n","[Training Epoch 1] Batch 3612, Loss 0.5126636028289795\n","[Training Epoch 1] Batch 3613, Loss 0.5108072757720947\n","[Training Epoch 1] Batch 3614, Loss 0.540207028388977\n","[Training Epoch 1] Batch 3615, Loss 0.49219298362731934\n","[Training Epoch 1] Batch 3616, Loss 0.4755379855632782\n","[Training Epoch 1] Batch 3617, Loss 0.5043343901634216\n","[Training Epoch 1] Batch 3618, Loss 0.5063454508781433\n","[Training Epoch 1] Batch 3619, Loss 0.5071882009506226\n","[Training Epoch 1] Batch 3620, Loss 0.49965518712997437\n","[Training Epoch 1] Batch 3621, Loss 0.4889594316482544\n","[Training Epoch 1] Batch 3622, Loss 0.5245429277420044\n","[Training Epoch 1] Batch 3623, Loss 0.47065451741218567\n","[Training Epoch 1] Batch 3624, Loss 0.47893282771110535\n","[Training Epoch 1] Batch 3625, Loss 0.5022191405296326\n","[Training Epoch 1] Batch 3626, Loss 0.5053813457489014\n","[Training Epoch 1] Batch 3627, Loss 0.49895817041397095\n","[Training Epoch 1] Batch 3628, Loss 0.5080009698867798\n","[Training Epoch 1] Batch 3629, Loss 0.47051167488098145\n","[Training Epoch 1] Batch 3630, Loss 0.5284868478775024\n","[Training Epoch 1] Batch 3631, Loss 0.507309079170227\n","[Training Epoch 1] Batch 3632, Loss 0.499528706073761\n","[Training Epoch 1] Batch 3633, Loss 0.48781439661979675\n","[Training Epoch 1] Batch 3634, Loss 0.5082369446754456\n","[Training Epoch 1] Batch 3635, Loss 0.541924238204956\n","[Training Epoch 1] Batch 3636, Loss 0.4977371394634247\n","[Training Epoch 1] Batch 3637, Loss 0.5291014909744263\n","[Training Epoch 1] Batch 3638, Loss 0.5075535774230957\n","[Training Epoch 1] Batch 3639, Loss 0.5044894814491272\n","[Training Epoch 1] Batch 3640, Loss 0.4658103585243225\n","[Training Epoch 1] Batch 3641, Loss 0.5261379480361938\n","[Training Epoch 1] Batch 3642, Loss 0.4903002679347992\n","[Training Epoch 1] Batch 3643, Loss 0.5178443789482117\n","[Training Epoch 1] Batch 3644, Loss 0.5246322154998779\n","[Training Epoch 1] Batch 3645, Loss 0.4846194386482239\n","[Training Epoch 1] Batch 3646, Loss 0.5382928252220154\n","[Training Epoch 1] Batch 3647, Loss 0.4879890978336334\n","[Training Epoch 1] Batch 3648, Loss 0.49765005707740784\n","[Training Epoch 1] Batch 3649, Loss 0.5034106969833374\n","[Training Epoch 1] Batch 3650, Loss 0.4890899956226349\n","[Training Epoch 1] Batch 3651, Loss 0.4875812530517578\n","[Training Epoch 1] Batch 3652, Loss 0.5081256628036499\n","[Training Epoch 1] Batch 3653, Loss 0.5155683755874634\n","[Training Epoch 1] Batch 3654, Loss 0.47907355427742004\n","[Training Epoch 1] Batch 3655, Loss 0.4510229825973511\n","[Training Epoch 1] Batch 3656, Loss 0.5187773704528809\n","[Training Epoch 1] Batch 3657, Loss 0.49111664295196533\n","[Training Epoch 1] Batch 3658, Loss 0.5110056400299072\n","[Training Epoch 1] Batch 3659, Loss 0.49560269713401794\n","[Training Epoch 1] Batch 3660, Loss 0.48575645685195923\n","[Training Epoch 1] Batch 3661, Loss 0.5019460320472717\n","[Training Epoch 1] Batch 3662, Loss 0.48722782731056213\n","[Training Epoch 1] Batch 3663, Loss 0.5084840655326843\n","[Training Epoch 1] Batch 3664, Loss 0.5091689229011536\n","[Training Epoch 1] Batch 3665, Loss 0.5150144696235657\n","[Training Epoch 1] Batch 3666, Loss 0.5092658400535583\n","[Training Epoch 1] Batch 3667, Loss 0.48837971687316895\n","[Training Epoch 1] Batch 3668, Loss 0.49236008524894714\n","[Training Epoch 1] Batch 3669, Loss 0.5164726972579956\n","[Training Epoch 1] Batch 3670, Loss 0.5205326080322266\n","[Training Epoch 1] Batch 3671, Loss 0.48435306549072266\n","[Training Epoch 1] Batch 3672, Loss 0.5129697322845459\n","[Training Epoch 1] Batch 3673, Loss 0.5646666884422302\n","[Training Epoch 1] Batch 3674, Loss 0.4924394488334656\n","[Training Epoch 1] Batch 3675, Loss 0.5070050954818726\n","[Training Epoch 1] Batch 3676, Loss 0.49729979038238525\n","[Training Epoch 1] Batch 3677, Loss 0.5016236305236816\n","[Training Epoch 1] Batch 3678, Loss 0.5069341659545898\n","[Training Epoch 1] Batch 3679, Loss 0.4835803508758545\n","[Training Epoch 1] Batch 3680, Loss 0.5035220384597778\n","[Training Epoch 1] Batch 3681, Loss 0.5268905162811279\n","[Training Epoch 1] Batch 3682, Loss 0.5228068828582764\n","[Training Epoch 1] Batch 3683, Loss 0.4656868875026703\n","[Training Epoch 1] Batch 3684, Loss 0.49826741218566895\n","[Training Epoch 1] Batch 3685, Loss 0.5030001401901245\n","[Training Epoch 1] Batch 3686, Loss 0.5036717653274536\n","[Training Epoch 1] Batch 3687, Loss 0.48183268308639526\n","[Training Epoch 1] Batch 3688, Loss 0.5026791095733643\n","[Training Epoch 1] Batch 3689, Loss 0.4586789011955261\n","[Training Epoch 1] Batch 3690, Loss 0.47979646921157837\n","[Training Epoch 1] Batch 3691, Loss 0.4973614811897278\n","[Training Epoch 1] Batch 3692, Loss 0.5119462013244629\n","[Training Epoch 1] Batch 3693, Loss 0.4901706576347351\n","[Training Epoch 1] Batch 3694, Loss 0.4851174056529999\n","[Training Epoch 1] Batch 3695, Loss 0.49124425649642944\n","[Training Epoch 1] Batch 3696, Loss 0.5004907846450806\n","[Training Epoch 1] Batch 3697, Loss 0.5279356241226196\n","[Training Epoch 1] Batch 3698, Loss 0.5390551686286926\n","[Training Epoch 1] Batch 3699, Loss 0.5209915637969971\n","[Training Epoch 1] Batch 3700, Loss 0.5127878189086914\n","[Training Epoch 1] Batch 3701, Loss 0.5004547834396362\n","[Training Epoch 1] Batch 3702, Loss 0.5033459663391113\n","[Training Epoch 1] Batch 3703, Loss 0.4799848198890686\n","[Training Epoch 1] Batch 3704, Loss 0.5117987394332886\n","[Training Epoch 1] Batch 3705, Loss 0.517608642578125\n","[Training Epoch 1] Batch 3706, Loss 0.49857279658317566\n","[Training Epoch 1] Batch 3707, Loss 0.49142569303512573\n","[Training Epoch 1] Batch 3708, Loss 0.495010644197464\n","[Training Epoch 1] Batch 3709, Loss 0.45718052983283997\n","[Training Epoch 1] Batch 3710, Loss 0.5079641342163086\n","[Training Epoch 1] Batch 3711, Loss 0.48234421014785767\n","[Training Epoch 1] Batch 3712, Loss 0.49598461389541626\n","[Training Epoch 1] Batch 3713, Loss 0.48561257123947144\n","[Training Epoch 1] Batch 3714, Loss 0.503656804561615\n","[Training Epoch 1] Batch 3715, Loss 0.5209169983863831\n","[Training Epoch 1] Batch 3716, Loss 0.5188237428665161\n","[Training Epoch 1] Batch 3717, Loss 0.49920228123664856\n","[Training Epoch 1] Batch 3718, Loss 0.5008745789527893\n","[Training Epoch 1] Batch 3719, Loss 0.5096302032470703\n","[Training Epoch 1] Batch 3720, Loss 0.504399299621582\n","[Training Epoch 1] Batch 3721, Loss 0.474998414516449\n","[Training Epoch 1] Batch 3722, Loss 0.4839237332344055\n","[Training Epoch 1] Batch 3723, Loss 0.47718536853790283\n","[Training Epoch 1] Batch 3724, Loss 0.5091228485107422\n","[Training Epoch 1] Batch 3725, Loss 0.47879207134246826\n","[Training Epoch 1] Batch 3726, Loss 0.4802093207836151\n","[Training Epoch 1] Batch 3727, Loss 0.4925541281700134\n","[Training Epoch 1] Batch 3728, Loss 0.49105304479599\n","[Training Epoch 1] Batch 3729, Loss 0.4843530058860779\n","[Training Epoch 1] Batch 3730, Loss 0.47928038239479065\n","[Training Epoch 1] Batch 3731, Loss 0.5051608085632324\n","[Training Epoch 1] Batch 3732, Loss 0.4737985134124756\n","[Training Epoch 1] Batch 3733, Loss 0.5078002214431763\n","[Training Epoch 1] Batch 3734, Loss 0.47257497906684875\n","[Training Epoch 1] Batch 3735, Loss 0.48585382103919983\n","[Training Epoch 1] Batch 3736, Loss 0.4888184666633606\n","[Training Epoch 1] Batch 3737, Loss 0.530231237411499\n","[Training Epoch 1] Batch 3738, Loss 0.49413156509399414\n","[Training Epoch 1] Batch 3739, Loss 0.5074734091758728\n","[Training Epoch 1] Batch 3740, Loss 0.5164100527763367\n","[Training Epoch 1] Batch 3741, Loss 0.5077629089355469\n","[Training Epoch 1] Batch 3742, Loss 0.4918074905872345\n","[Training Epoch 1] Batch 3743, Loss 0.4843170642852783\n","[Training Epoch 1] Batch 3744, Loss 0.5119420289993286\n","[Training Epoch 1] Batch 3745, Loss 0.5018624067306519\n","[Training Epoch 1] Batch 3746, Loss 0.5144226551055908\n","[Training Epoch 1] Batch 3747, Loss 0.5127148032188416\n","[Training Epoch 1] Batch 3748, Loss 0.4710029363632202\n","[Training Epoch 1] Batch 3749, Loss 0.4833245575428009\n","[Training Epoch 1] Batch 3750, Loss 0.47641003131866455\n","[Training Epoch 1] Batch 3751, Loss 0.48694032430648804\n","[Training Epoch 1] Batch 3752, Loss 0.48847389221191406\n","[Training Epoch 1] Batch 3753, Loss 0.4773690700531006\n","[Training Epoch 1] Batch 3754, Loss 0.5048426389694214\n","[Training Epoch 1] Batch 3755, Loss 0.5045838356018066\n","[Training Epoch 1] Batch 3756, Loss 0.5142159461975098\n","[Training Epoch 1] Batch 3757, Loss 0.5401415824890137\n","[Training Epoch 1] Batch 3758, Loss 0.5113474726676941\n","[Training Epoch 1] Batch 3759, Loss 0.4747072458267212\n","[Training Epoch 1] Batch 3760, Loss 0.47653573751449585\n","[Training Epoch 1] Batch 3761, Loss 0.5059165954589844\n","[Training Epoch 1] Batch 3762, Loss 0.523506224155426\n","[Training Epoch 1] Batch 3763, Loss 0.5035510063171387\n","[Training Epoch 1] Batch 3764, Loss 0.5006946325302124\n","[Training Epoch 1] Batch 3765, Loss 0.536264955997467\n","[Training Epoch 1] Batch 3766, Loss 0.5000244379043579\n","[Training Epoch 1] Batch 3767, Loss 0.49245330691337585\n","[Training Epoch 1] Batch 3768, Loss 0.5052744150161743\n","[Training Epoch 1] Batch 3769, Loss 0.5125105381011963\n","[Training Epoch 1] Batch 3770, Loss 0.4857165813446045\n","[Training Epoch 1] Batch 3771, Loss 0.47444653511047363\n","[Training Epoch 1] Batch 3772, Loss 0.4958206117153168\n","[Training Epoch 1] Batch 3773, Loss 0.4983441233634949\n","[Training Epoch 1] Batch 3774, Loss 0.5100151300430298\n","[Training Epoch 1] Batch 3775, Loss 0.5084654092788696\n","[Training Epoch 1] Batch 3776, Loss 0.5185081958770752\n","[Training Epoch 1] Batch 3777, Loss 0.5009536743164062\n","[Training Epoch 1] Batch 3778, Loss 0.4805789887905121\n","[Training Epoch 1] Batch 3779, Loss 0.5044373869895935\n","[Training Epoch 1] Batch 3780, Loss 0.485348641872406\n","[Training Epoch 1] Batch 3781, Loss 0.4815315008163452\n","[Training Epoch 1] Batch 3782, Loss 0.491196870803833\n","[Training Epoch 1] Batch 3783, Loss 0.5117846131324768\n","[Training Epoch 1] Batch 3784, Loss 0.5028569102287292\n","[Training Epoch 1] Batch 3785, Loss 0.5208981037139893\n","[Training Epoch 1] Batch 3786, Loss 0.5089993476867676\n","[Training Epoch 1] Batch 3787, Loss 0.5143718719482422\n","[Training Epoch 1] Batch 3788, Loss 0.47370195388793945\n","[Training Epoch 1] Batch 3789, Loss 0.5063889026641846\n","[Training Epoch 1] Batch 3790, Loss 0.514194130897522\n","[Training Epoch 1] Batch 3791, Loss 0.5347227454185486\n","[Training Epoch 1] Batch 3792, Loss 0.4887121021747589\n","[Training Epoch 1] Batch 3793, Loss 0.4989388585090637\n","[Training Epoch 1] Batch 3794, Loss 0.5261598825454712\n","[Training Epoch 1] Batch 3795, Loss 0.4882737398147583\n","[Training Epoch 1] Batch 3796, Loss 0.5046507716178894\n","[Training Epoch 1] Batch 3797, Loss 0.47370418906211853\n","[Training Epoch 1] Batch 3798, Loss 0.5132333636283875\n","[Training Epoch 1] Batch 3799, Loss 0.5277587175369263\n","[Training Epoch 1] Batch 3800, Loss 0.4812649190425873\n","[Training Epoch 1] Batch 3801, Loss 0.5235230922698975\n","[Training Epoch 1] Batch 3802, Loss 0.47592729330062866\n","[Training Epoch 1] Batch 3803, Loss 0.5006120204925537\n","[Training Epoch 1] Batch 3804, Loss 0.5052657127380371\n","[Training Epoch 1] Batch 3805, Loss 0.5118371248245239\n","[Training Epoch 1] Batch 3806, Loss 0.529828667640686\n","[Training Epoch 1] Batch 3807, Loss 0.5030714273452759\n","[Training Epoch 1] Batch 3808, Loss 0.5172277688980103\n","[Training Epoch 1] Batch 3809, Loss 0.4763699471950531\n","[Training Epoch 1] Batch 3810, Loss 0.5049000978469849\n","[Training Epoch 1] Batch 3811, Loss 0.4868396520614624\n","[Training Epoch 1] Batch 3812, Loss 0.47177037596702576\n","[Training Epoch 1] Batch 3813, Loss 0.4953418970108032\n","[Training Epoch 1] Batch 3814, Loss 0.5127789974212646\n","[Training Epoch 1] Batch 3815, Loss 0.4894540309906006\n","[Training Epoch 1] Batch 3816, Loss 0.46585536003112793\n","[Training Epoch 1] Batch 3817, Loss 0.5001714825630188\n","[Training Epoch 1] Batch 3818, Loss 0.4724738299846649\n","[Training Epoch 1] Batch 3819, Loss 0.5003188252449036\n","[Training Epoch 1] Batch 3820, Loss 0.47382110357284546\n","[Training Epoch 1] Batch 3821, Loss 0.49526870250701904\n","[Training Epoch 1] Batch 3822, Loss 0.5266402959823608\n","[Training Epoch 1] Batch 3823, Loss 0.4830408990383148\n","[Training Epoch 1] Batch 3824, Loss 0.5063489675521851\n","[Training Epoch 1] Batch 3825, Loss 0.49629634618759155\n","[Training Epoch 1] Batch 3826, Loss 0.4888318181037903\n","[Training Epoch 1] Batch 3827, Loss 0.4869917929172516\n","[Training Epoch 1] Batch 3828, Loss 0.49718177318573\n","[Training Epoch 1] Batch 3829, Loss 0.48928818106651306\n","[Training Epoch 1] Batch 3830, Loss 0.48368722200393677\n","[Training Epoch 1] Batch 3831, Loss 0.5020327568054199\n","[Training Epoch 1] Batch 3832, Loss 0.4761952757835388\n","[Training Epoch 1] Batch 3833, Loss 0.4702688753604889\n","[Training Epoch 1] Batch 3834, Loss 0.5250914692878723\n","[Training Epoch 1] Batch 3835, Loss 0.4993402361869812\n","[Training Epoch 1] Batch 3836, Loss 0.4976295828819275\n","[Training Epoch 1] Batch 3837, Loss 0.5034085512161255\n","[Training Epoch 1] Batch 3838, Loss 0.48546111583709717\n","[Training Epoch 1] Batch 3839, Loss 0.48124247789382935\n","[Training Epoch 1] Batch 3840, Loss 0.4649876356124878\n","[Training Epoch 1] Batch 3841, Loss 0.5021907091140747\n","[Training Epoch 1] Batch 3842, Loss 0.5127377510070801\n","[Training Epoch 1] Batch 3843, Loss 0.4951600730419159\n","[Training Epoch 1] Batch 3844, Loss 0.5114420056343079\n","[Training Epoch 1] Batch 3845, Loss 0.5035197138786316\n","[Training Epoch 1] Batch 3846, Loss 0.45844000577926636\n","[Training Epoch 1] Batch 3847, Loss 0.5075700283050537\n","[Training Epoch 1] Batch 3848, Loss 0.4955576956272125\n","[Training Epoch 1] Batch 3849, Loss 0.5069456100463867\n","[Training Epoch 1] Batch 3850, Loss 0.46658647060394287\n","[Training Epoch 1] Batch 3851, Loss 0.47897160053253174\n","[Training Epoch 1] Batch 3852, Loss 0.49394553899765015\n","[Training Epoch 1] Batch 3853, Loss 0.5229505300521851\n","[Training Epoch 1] Batch 3854, Loss 0.5070273876190186\n","[Training Epoch 1] Batch 3855, Loss 0.5011048913002014\n","[Training Epoch 1] Batch 3856, Loss 0.5094199180603027\n","[Training Epoch 1] Batch 3857, Loss 0.5213171243667603\n","[Training Epoch 1] Batch 3858, Loss 0.49500471353530884\n","[Training Epoch 1] Batch 3859, Loss 0.5118202567100525\n","[Training Epoch 1] Batch 3860, Loss 0.5043154954910278\n","[Training Epoch 1] Batch 3861, Loss 0.5354882478713989\n","[Training Epoch 1] Batch 3862, Loss 0.5225330591201782\n","[Training Epoch 1] Batch 3863, Loss 0.5054584741592407\n","[Training Epoch 1] Batch 3864, Loss 0.48008379340171814\n","[Training Epoch 1] Batch 3865, Loss 0.4965669512748718\n","[Training Epoch 1] Batch 3866, Loss 0.5048524737358093\n","[Training Epoch 1] Batch 3867, Loss 0.5115143656730652\n","[Training Epoch 1] Batch 3868, Loss 0.5162646770477295\n","[Training Epoch 1] Batch 3869, Loss 0.5021316409111023\n","[Training Epoch 1] Batch 3870, Loss 0.5229141712188721\n","[Training Epoch 1] Batch 3871, Loss 0.5149325132369995\n","[Training Epoch 1] Batch 3872, Loss 0.4884171485900879\n","[Training Epoch 1] Batch 3873, Loss 0.4956880211830139\n","[Training Epoch 1] Batch 3874, Loss 0.5237351059913635\n","[Training Epoch 1] Batch 3875, Loss 0.48959630727767944\n","[Training Epoch 1] Batch 3876, Loss 0.5266803503036499\n","[Training Epoch 1] Batch 3877, Loss 0.5118682980537415\n","[Training Epoch 1] Batch 3878, Loss 0.5112856030464172\n","[Training Epoch 1] Batch 3879, Loss 0.5154997110366821\n","[Training Epoch 1] Batch 3880, Loss 0.48526135087013245\n","[Training Epoch 1] Batch 3881, Loss 0.5069181323051453\n","[Training Epoch 1] Batch 3882, Loss 0.5210845470428467\n","[Training Epoch 1] Batch 3883, Loss 0.4589211940765381\n","[Training Epoch 1] Batch 3884, Loss 0.4861490726470947\n","[Training Epoch 1] Batch 3885, Loss 0.5079829692840576\n","[Training Epoch 1] Batch 3886, Loss 0.4867597818374634\n","[Training Epoch 1] Batch 3887, Loss 0.4897596538066864\n","[Training Epoch 1] Batch 3888, Loss 0.4971877336502075\n","[Training Epoch 1] Batch 3889, Loss 0.4718504548072815\n","[Training Epoch 1] Batch 3890, Loss 0.5032442808151245\n","[Training Epoch 1] Batch 3891, Loss 0.5073754191398621\n","[Training Epoch 1] Batch 3892, Loss 0.5073554515838623\n","[Training Epoch 1] Batch 3893, Loss 0.5141457319259644\n","[Training Epoch 1] Batch 3894, Loss 0.49448704719543457\n","[Training Epoch 1] Batch 3895, Loss 0.523404598236084\n","[Training Epoch 1] Batch 3896, Loss 0.4911952018737793\n","[Training Epoch 1] Batch 3897, Loss 0.4911213219165802\n","[Training Epoch 1] Batch 3898, Loss 0.5009346008300781\n","[Training Epoch 1] Batch 3899, Loss 0.47952741384506226\n","[Training Epoch 1] Batch 3900, Loss 0.49872347712516785\n","[Training Epoch 1] Batch 3901, Loss 0.5043109059333801\n","[Training Epoch 1] Batch 3902, Loss 0.5308375358581543\n","[Training Epoch 1] Batch 3903, Loss 0.4675137400627136\n","[Training Epoch 1] Batch 3904, Loss 0.5191329717636108\n","[Training Epoch 1] Batch 3905, Loss 0.5088399648666382\n","[Training Epoch 1] Batch 3906, Loss 0.5103970766067505\n","[Training Epoch 1] Batch 3907, Loss 0.5061739683151245\n","[Training Epoch 1] Batch 3908, Loss 0.5219851136207581\n","[Training Epoch 1] Batch 3909, Loss 0.5113139152526855\n","[Training Epoch 1] Batch 3910, Loss 0.5150165557861328\n","[Training Epoch 1] Batch 3911, Loss 0.46527576446533203\n","[Training Epoch 1] Batch 3912, Loss 0.48143887519836426\n","[Training Epoch 1] Batch 3913, Loss 0.49019038677215576\n","[Training Epoch 1] Batch 3914, Loss 0.5131301879882812\n","[Training Epoch 1] Batch 3915, Loss 0.4978742003440857\n","[Training Epoch 1] Batch 3916, Loss 0.5071859955787659\n","[Training Epoch 1] Batch 3917, Loss 0.502860426902771\n","[Training Epoch 1] Batch 3918, Loss 0.4709453582763672\n","[Training Epoch 1] Batch 3919, Loss 0.4993487596511841\n","[Training Epoch 1] Batch 3920, Loss 0.48473984003067017\n","[Training Epoch 1] Batch 3921, Loss 0.4928548336029053\n","[Training Epoch 1] Batch 3922, Loss 0.5091199278831482\n","[Training Epoch 1] Batch 3923, Loss 0.49810314178466797\n","[Training Epoch 1] Batch 3924, Loss 0.48301422595977783\n","[Training Epoch 1] Batch 3925, Loss 0.5013599395751953\n","[Training Epoch 1] Batch 3926, Loss 0.47704988718032837\n","[Training Epoch 1] Batch 3927, Loss 0.524335503578186\n","[Training Epoch 1] Batch 3928, Loss 0.4871790409088135\n","[Training Epoch 1] Batch 3929, Loss 0.49463897943496704\n","[Training Epoch 1] Batch 3930, Loss 0.5166776776313782\n","[Training Epoch 1] Batch 3931, Loss 0.49106723070144653\n","[Training Epoch 1] Batch 3932, Loss 0.4994592070579529\n","[Training Epoch 1] Batch 3933, Loss 0.5083070993423462\n","[Training Epoch 1] Batch 3934, Loss 0.49499303102493286\n","[Training Epoch 1] Batch 3935, Loss 0.4988592863082886\n","[Training Epoch 1] Batch 3936, Loss 0.46452125906944275\n","[Training Epoch 1] Batch 3937, Loss 0.49687668681144714\n","[Training Epoch 1] Batch 3938, Loss 0.4912109375\n","[Training Epoch 1] Batch 3939, Loss 0.5071799755096436\n","[Training Epoch 1] Batch 3940, Loss 0.5368520617485046\n","[Training Epoch 1] Batch 3941, Loss 0.5081753730773926\n","[Training Epoch 1] Batch 3942, Loss 0.4841212034225464\n","[Training Epoch 1] Batch 3943, Loss 0.49804168939590454\n","[Training Epoch 1] Batch 3944, Loss 0.5090872049331665\n","[Training Epoch 1] Batch 3945, Loss 0.45701074600219727\n","[Training Epoch 1] Batch 3946, Loss 0.49293315410614014\n","[Training Epoch 1] Batch 3947, Loss 0.5144040584564209\n","[Training Epoch 1] Batch 3948, Loss 0.4842264652252197\n","[Training Epoch 1] Batch 3949, Loss 0.5361429452896118\n","[Training Epoch 1] Batch 3950, Loss 0.5040774345397949\n","[Training Epoch 1] Batch 3951, Loss 0.4875325560569763\n","[Training Epoch 1] Batch 3952, Loss 0.5018032789230347\n","[Training Epoch 1] Batch 3953, Loss 0.4937855005264282\n","[Training Epoch 1] Batch 3954, Loss 0.5156276226043701\n","[Training Epoch 1] Batch 3955, Loss 0.5109992027282715\n","[Training Epoch 1] Batch 3956, Loss 0.48836973309516907\n","[Training Epoch 1] Batch 3957, Loss 0.49749109148979187\n","[Training Epoch 1] Batch 3958, Loss 0.5032880902290344\n","[Training Epoch 1] Batch 3959, Loss 0.5261562466621399\n","[Training Epoch 1] Batch 3960, Loss 0.49249976873397827\n","[Training Epoch 1] Batch 3961, Loss 0.49683910608291626\n","[Training Epoch 1] Batch 3962, Loss 0.4978799819946289\n","[Training Epoch 1] Batch 3963, Loss 0.48416829109191895\n","[Training Epoch 1] Batch 3964, Loss 0.4899686574935913\n","[Training Epoch 1] Batch 3965, Loss 0.5084173679351807\n","[Training Epoch 1] Batch 3966, Loss 0.5101485848426819\n","[Training Epoch 1] Batch 3967, Loss 0.5045552253723145\n","[Training Epoch 1] Batch 3968, Loss 0.5202749967575073\n","[Training Epoch 1] Batch 3969, Loss 0.5073047876358032\n","[Training Epoch 1] Batch 3970, Loss 0.502299427986145\n","[Training Epoch 1] Batch 3971, Loss 0.5276556611061096\n","[Training Epoch 1] Batch 3972, Loss 0.49654003977775574\n","[Training Epoch 1] Batch 3973, Loss 0.5177621245384216\n","[Training Epoch 1] Batch 3974, Loss 0.5006335973739624\n","[Training Epoch 1] Batch 3975, Loss 0.4899548292160034\n","[Training Epoch 1] Batch 3976, Loss 0.5150898694992065\n","[Training Epoch 1] Batch 3977, Loss 0.5131954550743103\n","[Training Epoch 1] Batch 3978, Loss 0.49078214168548584\n","[Training Epoch 1] Batch 3979, Loss 0.5453444719314575\n","[Training Epoch 1] Batch 3980, Loss 0.5015929937362671\n","[Training Epoch 1] Batch 3981, Loss 0.5005267262458801\n","[Training Epoch 1] Batch 3982, Loss 0.48453444242477417\n","[Training Epoch 1] Batch 3983, Loss 0.4967219829559326\n","[Training Epoch 1] Batch 3984, Loss 0.49839910864830017\n","[Training Epoch 1] Batch 3985, Loss 0.5073221921920776\n","[Training Epoch 1] Batch 3986, Loss 0.49801555275917053\n","[Training Epoch 1] Batch 3987, Loss 0.5213097333908081\n","[Training Epoch 1] Batch 3988, Loss 0.501282811164856\n","[Training Epoch 1] Batch 3989, Loss 0.5371264219284058\n","[Training Epoch 1] Batch 3990, Loss 0.5028401613235474\n","[Training Epoch 1] Batch 3991, Loss 0.49123620986938477\n","[Training Epoch 1] Batch 3992, Loss 0.48133039474487305\n","[Training Epoch 1] Batch 3993, Loss 0.4802316427230835\n","[Training Epoch 1] Batch 3994, Loss 0.48436516523361206\n","[Training Epoch 1] Batch 3995, Loss 0.5079957246780396\n","[Training Epoch 1] Batch 3996, Loss 0.4827759563922882\n","[Training Epoch 1] Batch 3997, Loss 0.4988263249397278\n","[Training Epoch 1] Batch 3998, Loss 0.491120308637619\n","[Training Epoch 1] Batch 3999, Loss 0.48027876019477844\n","[Training Epoch 1] Batch 4000, Loss 0.5028076171875\n","[Training Epoch 1] Batch 4001, Loss 0.5147292017936707\n","[Training Epoch 1] Batch 4002, Loss 0.5065832734107971\n","[Training Epoch 1] Batch 4003, Loss 0.48953142762184143\n","[Training Epoch 1] Batch 4004, Loss 0.5103359222412109\n","[Training Epoch 1] Batch 4005, Loss 0.5206019878387451\n","[Training Epoch 1] Batch 4006, Loss 0.5076286196708679\n","[Training Epoch 1] Batch 4007, Loss 0.5039253234863281\n","[Training Epoch 1] Batch 4008, Loss 0.5289198160171509\n","[Training Epoch 1] Batch 4009, Loss 0.4825255870819092\n","[Training Epoch 1] Batch 4010, Loss 0.483817994594574\n","[Training Epoch 1] Batch 4011, Loss 0.5086357593536377\n","[Training Epoch 1] Batch 4012, Loss 0.4967382550239563\n","[Training Epoch 1] Batch 4013, Loss 0.498785138130188\n","[Training Epoch 1] Batch 4014, Loss 0.49637454748153687\n","[Training Epoch 1] Batch 4015, Loss 0.5014547109603882\n","[Training Epoch 1] Batch 4016, Loss 0.4941641688346863\n","[Training Epoch 1] Batch 4017, Loss 0.5594208240509033\n","[Training Epoch 1] Batch 4018, Loss 0.49303489923477173\n","[Training Epoch 1] Batch 4019, Loss 0.5091041922569275\n","[Training Epoch 1] Batch 4020, Loss 0.49214091897010803\n","[Training Epoch 1] Batch 4021, Loss 0.5241787433624268\n","[Training Epoch 1] Batch 4022, Loss 0.4887442886829376\n","[Training Epoch 1] Batch 4023, Loss 0.4965418875217438\n","[Training Epoch 1] Batch 4024, Loss 0.5307528972625732\n","[Training Epoch 1] Batch 4025, Loss 0.5171287059783936\n","[Training Epoch 1] Batch 4026, Loss 0.4997049570083618\n","[Training Epoch 1] Batch 4027, Loss 0.5123196244239807\n","[Training Epoch 1] Batch 4028, Loss 0.5414314270019531\n","[Training Epoch 1] Batch 4029, Loss 0.5217374563217163\n","[Training Epoch 1] Batch 4030, Loss 0.5067998766899109\n","[Training Epoch 1] Batch 4031, Loss 0.49777358770370483\n","[Training Epoch 1] Batch 4032, Loss 0.503852903842926\n","[Training Epoch 1] Batch 4033, Loss 0.5063639879226685\n","[Training Epoch 1] Batch 4034, Loss 0.5035641193389893\n","[Training Epoch 1] Batch 4035, Loss 0.5221064686775208\n","[Training Epoch 1] Batch 4036, Loss 0.5286701917648315\n","[Training Epoch 1] Batch 4037, Loss 0.5156676769256592\n","[Training Epoch 1] Batch 4038, Loss 0.48604995012283325\n","[Training Epoch 1] Batch 4039, Loss 0.5374622344970703\n","[Training Epoch 1] Batch 4040, Loss 0.47793638706207275\n","[Training Epoch 1] Batch 4041, Loss 0.5001935362815857\n","[Training Epoch 1] Batch 4042, Loss 0.4798005223274231\n","[Training Epoch 1] Batch 4043, Loss 0.5172954797744751\n","[Training Epoch 1] Batch 4044, Loss 0.4615827202796936\n","[Training Epoch 1] Batch 4045, Loss 0.48546263575553894\n","[Training Epoch 1] Batch 4046, Loss 0.5062101483345032\n","[Training Epoch 1] Batch 4047, Loss 0.5098175406455994\n","[Training Epoch 1] Batch 4048, Loss 0.5263800024986267\n","[Training Epoch 1] Batch 4049, Loss 0.48445218801498413\n","[Training Epoch 1] Batch 4050, Loss 0.4929981827735901\n","[Training Epoch 1] Batch 4051, Loss 0.49659454822540283\n","[Training Epoch 1] Batch 4052, Loss 0.4747825264930725\n","[Training Epoch 1] Batch 4053, Loss 0.47207707166671753\n","[Training Epoch 1] Batch 4054, Loss 0.5089837908744812\n","[Training Epoch 1] Batch 4055, Loss 0.49240267276763916\n","[Training Epoch 1] Batch 4056, Loss 0.489488422870636\n","[Training Epoch 1] Batch 4057, Loss 0.5000498294830322\n","[Training Epoch 1] Batch 4058, Loss 0.5127915143966675\n","[Training Epoch 1] Batch 4059, Loss 0.49173104763031006\n","[Training Epoch 1] Batch 4060, Loss 0.5062121152877808\n","[Training Epoch 1] Batch 4061, Loss 0.4959283471107483\n","[Training Epoch 1] Batch 4062, Loss 0.5108959078788757\n","[Training Epoch 1] Batch 4063, Loss 0.5226560831069946\n","[Training Epoch 1] Batch 4064, Loss 0.5034228563308716\n","[Training Epoch 1] Batch 4065, Loss 0.5261908769607544\n","[Training Epoch 1] Batch 4066, Loss 0.5397828817367554\n","[Training Epoch 1] Batch 4067, Loss 0.49104124307632446\n","[Training Epoch 1] Batch 4068, Loss 0.48944562673568726\n","[Training Epoch 1] Batch 4069, Loss 0.5069010853767395\n","[Training Epoch 1] Batch 4070, Loss 0.5019255876541138\n","[Training Epoch 1] Batch 4071, Loss 0.47874361276626587\n","[Training Epoch 1] Batch 4072, Loss 0.48299217224121094\n","[Training Epoch 1] Batch 4073, Loss 0.5188024044036865\n","[Training Epoch 1] Batch 4074, Loss 0.493066668510437\n","[Training Epoch 1] Batch 4075, Loss 0.4793552756309509\n","[Training Epoch 1] Batch 4076, Loss 0.5025767087936401\n","[Training Epoch 1] Batch 4077, Loss 0.5270543694496155\n","[Training Epoch 1] Batch 4078, Loss 0.472609281539917\n","[Training Epoch 1] Batch 4079, Loss 0.497591495513916\n","[Training Epoch 1] Batch 4080, Loss 0.4564521908760071\n","[Training Epoch 1] Batch 4081, Loss 0.46936333179473877\n","[Training Epoch 1] Batch 4082, Loss 0.4817144274711609\n","[Training Epoch 1] Batch 4083, Loss 0.4808199405670166\n","[Training Epoch 1] Batch 4084, Loss 0.4801217019557953\n","[Training Epoch 1] Batch 4085, Loss 0.48977774381637573\n","[Training Epoch 1] Batch 4086, Loss 0.46422314643859863\n","[Training Epoch 1] Batch 4087, Loss 0.5054463744163513\n","[Training Epoch 1] Batch 4088, Loss 0.4975832998752594\n","[Training Epoch 1] Batch 4089, Loss 0.49154266715049744\n","[Training Epoch 1] Batch 4090, Loss 0.49917519092559814\n","[Training Epoch 1] Batch 4091, Loss 0.4784775972366333\n","[Training Epoch 1] Batch 4092, Loss 0.5209710001945496\n","[Training Epoch 1] Batch 4093, Loss 0.515261709690094\n","[Training Epoch 1] Batch 4094, Loss 0.4977053105831146\n","[Training Epoch 1] Batch 4095, Loss 0.5047112703323364\n","[Training Epoch 1] Batch 4096, Loss 0.4925067126750946\n","[Training Epoch 1] Batch 4097, Loss 0.5017580986022949\n","[Training Epoch 1] Batch 4098, Loss 0.5150033235549927\n","[Training Epoch 1] Batch 4099, Loss 0.5230927467346191\n","[Training Epoch 1] Batch 4100, Loss 0.5326170921325684\n","[Training Epoch 1] Batch 4101, Loss 0.49909141659736633\n","[Training Epoch 1] Batch 4102, Loss 0.5066532492637634\n","[Training Epoch 1] Batch 4103, Loss 0.533950686454773\n","[Training Epoch 1] Batch 4104, Loss 0.47902530431747437\n","[Training Epoch 1] Batch 4105, Loss 0.4960089325904846\n","[Training Epoch 1] Batch 4106, Loss 0.5037066340446472\n","[Training Epoch 1] Batch 4107, Loss 0.5146533250808716\n","[Training Epoch 1] Batch 4108, Loss 0.5022467374801636\n","[Training Epoch 1] Batch 4109, Loss 0.49503570795059204\n","[Training Epoch 1] Batch 4110, Loss 0.46777456998825073\n","[Training Epoch 1] Batch 4111, Loss 0.471661239862442\n","[Training Epoch 1] Batch 4112, Loss 0.5000772476196289\n","[Training Epoch 1] Batch 4113, Loss 0.48031628131866455\n","[Training Epoch 1] Batch 4114, Loss 0.5071676969528198\n","[Training Epoch 1] Batch 4115, Loss 0.5085585713386536\n","[Training Epoch 1] Batch 4116, Loss 0.5251709222793579\n","[Training Epoch 1] Batch 4117, Loss 0.5166099071502686\n","[Training Epoch 1] Batch 4118, Loss 0.5236999988555908\n","[Training Epoch 1] Batch 4119, Loss 0.5050244331359863\n","[Training Epoch 1] Batch 4120, Loss 0.518139123916626\n","[Training Epoch 1] Batch 4121, Loss 0.5143225193023682\n","[Training Epoch 1] Batch 4122, Loss 0.505115807056427\n","[Training Epoch 1] Batch 4123, Loss 0.48797252774238586\n","[Training Epoch 1] Batch 4124, Loss 0.5360570549964905\n","[Training Epoch 1] Batch 4125, Loss 0.48413288593292236\n","[Training Epoch 1] Batch 4126, Loss 0.5104482769966125\n","[Training Epoch 1] Batch 4127, Loss 0.49361947178840637\n","[Training Epoch 1] Batch 4128, Loss 0.4925193190574646\n","[Training Epoch 1] Batch 4129, Loss 0.5031949877738953\n","[Training Epoch 1] Batch 4130, Loss 0.5352091193199158\n","[Training Epoch 1] Batch 4131, Loss 0.5253219604492188\n","[Training Epoch 1] Batch 4132, Loss 0.5044217109680176\n","[Training Epoch 1] Batch 4133, Loss 0.480202317237854\n","[Training Epoch 1] Batch 4134, Loss 0.4994344115257263\n","[Training Epoch 1] Batch 4135, Loss 0.5132192373275757\n","[Training Epoch 1] Batch 4136, Loss 0.48339515924453735\n","[Training Epoch 1] Batch 4137, Loss 0.5264557600021362\n","[Training Epoch 1] Batch 4138, Loss 0.495693564414978\n","[Training Epoch 1] Batch 4139, Loss 0.4880116581916809\n","[Training Epoch 1] Batch 4140, Loss 0.48113924264907837\n","[Training Epoch 1] Batch 4141, Loss 0.5059085488319397\n","[Training Epoch 1] Batch 4142, Loss 0.5300650000572205\n","[Training Epoch 1] Batch 4143, Loss 0.48968324065208435\n","[Training Epoch 1] Batch 4144, Loss 0.5164390802383423\n","[Training Epoch 1] Batch 4145, Loss 0.5174669027328491\n","[Training Epoch 1] Batch 4146, Loss 0.49422574043273926\n","[Training Epoch 1] Batch 4147, Loss 0.4936595559120178\n","[Training Epoch 1] Batch 4148, Loss 0.5114783048629761\n","[Training Epoch 1] Batch 4149, Loss 0.494762122631073\n","[Training Epoch 1] Batch 4150, Loss 0.49551016092300415\n","[Training Epoch 1] Batch 4151, Loss 0.5125230550765991\n","[Training Epoch 1] Batch 4152, Loss 0.4822673201560974\n","[Training Epoch 1] Batch 4153, Loss 0.5033570528030396\n","[Training Epoch 1] Batch 4154, Loss 0.49380284547805786\n","[Training Epoch 1] Batch 4155, Loss 0.5123178958892822\n","[Training Epoch 1] Batch 4156, Loss 0.5200071930885315\n","[Training Epoch 1] Batch 4157, Loss 0.48692166805267334\n","[Training Epoch 1] Batch 4158, Loss 0.46917814016342163\n","[Training Epoch 1] Batch 4159, Loss 0.47229334712028503\n","[Training Epoch 1] Batch 4160, Loss 0.5146777033805847\n","[Training Epoch 1] Batch 4161, Loss 0.5020109415054321\n","[Training Epoch 1] Batch 4162, Loss 0.4760690927505493\n","[Training Epoch 1] Batch 4163, Loss 0.5157567262649536\n","[Training Epoch 1] Batch 4164, Loss 0.4887159466743469\n","[Training Epoch 1] Batch 4165, Loss 0.503995418548584\n","[Training Epoch 1] Batch 4166, Loss 0.5058199763298035\n","[Training Epoch 1] Batch 4167, Loss 0.5468341112136841\n","[Training Epoch 1] Batch 4168, Loss 0.4868867099285126\n","[Training Epoch 1] Batch 4169, Loss 0.4930118918418884\n","[Training Epoch 1] Batch 4170, Loss 0.5230633020401001\n","[Training Epoch 1] Batch 4171, Loss 0.5077965259552002\n","[Training Epoch 1] Batch 4172, Loss 0.511884331703186\n","[Training Epoch 1] Batch 4173, Loss 0.47976186871528625\n","[Training Epoch 1] Batch 4174, Loss 0.5169363021850586\n","[Training Epoch 1] Batch 4175, Loss 0.48261889815330505\n","[Training Epoch 1] Batch 4176, Loss 0.5196449756622314\n","[Training Epoch 1] Batch 4177, Loss 0.5069050788879395\n","[Training Epoch 1] Batch 4178, Loss 0.5309360027313232\n","[Training Epoch 1] Batch 4179, Loss 0.4785468876361847\n","[Training Epoch 1] Batch 4180, Loss 0.48391658067703247\n","[Training Epoch 1] Batch 4181, Loss 0.5137609243392944\n","[Training Epoch 1] Batch 4182, Loss 0.4756068289279938\n","[Training Epoch 1] Batch 4183, Loss 0.5083592534065247\n","[Training Epoch 1] Batch 4184, Loss 0.4978083670139313\n","[Training Epoch 1] Batch 4185, Loss 0.49255916476249695\n","[Training Epoch 1] Batch 4186, Loss 0.5070887804031372\n","[Training Epoch 1] Batch 4187, Loss 0.48620128631591797\n","[Training Epoch 1] Batch 4188, Loss 0.4709340035915375\n","[Training Epoch 1] Batch 4189, Loss 0.5028665065765381\n","[Training Epoch 1] Batch 4190, Loss 0.5097367763519287\n","[Training Epoch 1] Batch 4191, Loss 0.5071336030960083\n","[Training Epoch 1] Batch 4192, Loss 0.5082100033760071\n","[Training Epoch 1] Batch 4193, Loss 0.5179548263549805\n","[Training Epoch 1] Batch 4194, Loss 0.4965323805809021\n","[Training Epoch 1] Batch 4195, Loss 0.4935249090194702\n","[Training Epoch 1] Batch 4196, Loss 0.5300360918045044\n","[Training Epoch 1] Batch 4197, Loss 0.48107123374938965\n","[Training Epoch 1] Batch 4198, Loss 0.5180151462554932\n","[Training Epoch 1] Batch 4199, Loss 0.4900338053703308\n","[Training Epoch 1] Batch 4200, Loss 0.4995935559272766\n","[Training Epoch 1] Batch 4201, Loss 0.4759696125984192\n","[Training Epoch 1] Batch 4202, Loss 0.48379820585250854\n","[Training Epoch 1] Batch 4203, Loss 0.5064811706542969\n","[Training Epoch 1] Batch 4204, Loss 0.4989737868309021\n","[Training Epoch 1] Batch 4205, Loss 0.4809911251068115\n","[Training Epoch 1] Batch 4206, Loss 0.4995003342628479\n","[Training Epoch 1] Batch 4207, Loss 0.48120665550231934\n","[Training Epoch 1] Batch 4208, Loss 0.5041528940200806\n","[Training Epoch 1] Batch 4209, Loss 0.5130770802497864\n","[Training Epoch 1] Batch 4210, Loss 0.5253517627716064\n","[Training Epoch 1] Batch 4211, Loss 0.4974445700645447\n","[Training Epoch 1] Batch 4212, Loss 0.48744815587997437\n","[Training Epoch 1] Batch 4213, Loss 0.48747652769088745\n","[Training Epoch 1] Batch 4214, Loss 0.515109658241272\n","[Training Epoch 1] Batch 4215, Loss 0.5088866949081421\n","[Training Epoch 1] Batch 4216, Loss 0.47875574231147766\n","[Training Epoch 1] Batch 4217, Loss 0.5237061977386475\n","[Training Epoch 1] Batch 4218, Loss 0.4691629409790039\n","[Training Epoch 1] Batch 4219, Loss 0.4864981174468994\n","[Training Epoch 1] Batch 4220, Loss 0.5098001956939697\n","[Training Epoch 1] Batch 4221, Loss 0.4912189245223999\n","[Training Epoch 1] Batch 4222, Loss 0.5020686984062195\n","[Training Epoch 1] Batch 4223, Loss 0.5282948613166809\n","[Training Epoch 1] Batch 4224, Loss 0.4895402193069458\n","[Training Epoch 1] Batch 4225, Loss 0.4891904592514038\n","[Training Epoch 1] Batch 4226, Loss 0.47656354308128357\n","[Training Epoch 1] Batch 4227, Loss 0.5047546625137329\n","[Training Epoch 1] Batch 4228, Loss 0.49071264266967773\n","[Training Epoch 1] Batch 4229, Loss 0.4898514151573181\n","[Training Epoch 1] Batch 4230, Loss 0.49915969371795654\n","[Training Epoch 1] Batch 4231, Loss 0.4881683588027954\n","[Training Epoch 1] Batch 4232, Loss 0.48671385645866394\n","[Training Epoch 1] Batch 4233, Loss 0.4895250201225281\n","[Training Epoch 1] Batch 4234, Loss 0.5131595134735107\n","[Training Epoch 1] Batch 4235, Loss 0.49506354331970215\n","[Training Epoch 1] Batch 4236, Loss 0.5002330541610718\n","[Training Epoch 1] Batch 4237, Loss 0.5142143368721008\n","[Training Epoch 1] Batch 4238, Loss 0.4731082022190094\n","[Training Epoch 1] Batch 4239, Loss 0.5146928429603577\n","[Training Epoch 1] Batch 4240, Loss 0.5278383493423462\n","[Training Epoch 1] Batch 4241, Loss 0.49146342277526855\n","[Training Epoch 1] Batch 4242, Loss 0.5103588104248047\n","[Training Epoch 1] Batch 4243, Loss 0.5082679986953735\n","[Training Epoch 1] Batch 4244, Loss 0.4929811954498291\n","[Training Epoch 1] Batch 4245, Loss 0.4520222544670105\n","[Training Epoch 1] Batch 4246, Loss 0.4617094397544861\n","[Training Epoch 1] Batch 4247, Loss 0.5053163766860962\n","[Training Epoch 1] Batch 4248, Loss 0.4919317960739136\n","[Training Epoch 1] Batch 4249, Loss 0.48287978768348694\n","[Training Epoch 1] Batch 4250, Loss 0.5457870960235596\n","[Training Epoch 1] Batch 4251, Loss 0.49660825729370117\n","[Training Epoch 1] Batch 4252, Loss 0.49931013584136963\n","[Training Epoch 1] Batch 4253, Loss 0.501276433467865\n","[Training Epoch 1] Batch 4254, Loss 0.48671889305114746\n","[Training Epoch 1] Batch 4255, Loss 0.47253119945526123\n","[Training Epoch 1] Batch 4256, Loss 0.502080500125885\n","[Training Epoch 1] Batch 4257, Loss 0.4899720549583435\n","[Training Epoch 1] Batch 4258, Loss 0.5133925676345825\n","[Training Epoch 1] Batch 4259, Loss 0.5079078674316406\n","[Training Epoch 1] Batch 4260, Loss 0.5045055150985718\n","[Training Epoch 1] Batch 4261, Loss 0.4882657527923584\n","[Training Epoch 1] Batch 4262, Loss 0.5241424441337585\n","[Training Epoch 1] Batch 4263, Loss 0.4877256751060486\n","[Training Epoch 1] Batch 4264, Loss 0.518837571144104\n","[Training Epoch 1] Batch 4265, Loss 0.5091421604156494\n","[Training Epoch 1] Batch 4266, Loss 0.4995105266571045\n","[Training Epoch 1] Batch 4267, Loss 0.5456579923629761\n","[Training Epoch 1] Batch 4268, Loss 0.46078094840049744\n","[Training Epoch 1] Batch 4269, Loss 0.49212393164634705\n","[Training Epoch 1] Batch 4270, Loss 0.5256964564323425\n","[Training Epoch 1] Batch 4271, Loss 0.47929948568344116\n","[Training Epoch 1] Batch 4272, Loss 0.5000095367431641\n","[Training Epoch 1] Batch 4273, Loss 0.4937306344509125\n","[Training Epoch 1] Batch 4274, Loss 0.5172386169433594\n","[Training Epoch 1] Batch 4275, Loss 0.5341018438339233\n","[Training Epoch 1] Batch 4276, Loss 0.4977322816848755\n","[Training Epoch 1] Batch 4277, Loss 0.5054687261581421\n","[Training Epoch 1] Batch 4278, Loss 0.4962315857410431\n","[Training Epoch 1] Batch 4279, Loss 0.503973662853241\n","[Training Epoch 1] Batch 4280, Loss 0.47606268525123596\n","[Training Epoch 1] Batch 4281, Loss 0.5001405477523804\n","[Training Epoch 1] Batch 4282, Loss 0.49783089756965637\n","[Training Epoch 1] Batch 4283, Loss 0.48727479577064514\n","[Training Epoch 1] Batch 4284, Loss 0.5058854818344116\n","[Training Epoch 1] Batch 4285, Loss 0.483467161655426\n","[Training Epoch 1] Batch 4286, Loss 0.5180649757385254\n","[Training Epoch 1] Batch 4287, Loss 0.4812315106391907\n","[Training Epoch 1] Batch 4288, Loss 0.5153936743736267\n","[Training Epoch 1] Batch 4289, Loss 0.49382585287094116\n","[Training Epoch 1] Batch 4290, Loss 0.505269467830658\n","[Training Epoch 1] Batch 4291, Loss 0.49174314737319946\n","[Training Epoch 1] Batch 4292, Loss 0.5274012684822083\n","[Training Epoch 1] Batch 4293, Loss 0.5108753442764282\n","[Training Epoch 1] Batch 4294, Loss 0.47599101066589355\n","[Training Epoch 1] Batch 4295, Loss 0.49111586809158325\n","[Training Epoch 1] Batch 4296, Loss 0.5198494791984558\n","[Training Epoch 1] Batch 4297, Loss 0.5176189541816711\n","[Training Epoch 1] Batch 4298, Loss 0.4898354411125183\n","[Training Epoch 1] Batch 4299, Loss 0.528967022895813\n","[Training Epoch 1] Batch 4300, Loss 0.48801183700561523\n","[Training Epoch 1] Batch 4301, Loss 0.5163150429725647\n","[Training Epoch 1] Batch 4302, Loss 0.5083008408546448\n","[Training Epoch 1] Batch 4303, Loss 0.5215761661529541\n","[Training Epoch 1] Batch 4304, Loss 0.5215877890586853\n","[Training Epoch 1] Batch 4305, Loss 0.4888955354690552\n","[Training Epoch 1] Batch 4306, Loss 0.5217795372009277\n","[Training Epoch 1] Batch 4307, Loss 0.46468648314476013\n","[Training Epoch 1] Batch 4308, Loss 0.5181288719177246\n","[Training Epoch 1] Batch 4309, Loss 0.5101865530014038\n","[Training Epoch 1] Batch 4310, Loss 0.5342611074447632\n","[Training Epoch 1] Batch 4311, Loss 0.48377975821495056\n","[Training Epoch 1] Batch 4312, Loss 0.5060853958129883\n","[Training Epoch 1] Batch 4313, Loss 0.4776884615421295\n","[Training Epoch 1] Batch 4314, Loss 0.4981028735637665\n","[Training Epoch 1] Batch 4315, Loss 0.5279977321624756\n","[Training Epoch 1] Batch 4316, Loss 0.4868771731853485\n","[Training Epoch 1] Batch 4317, Loss 0.49618470668792725\n","[Training Epoch 1] Batch 4318, Loss 0.5105743408203125\n","[Training Epoch 1] Batch 4319, Loss 0.5067786574363708\n","[Training Epoch 1] Batch 4320, Loss 0.4825032353401184\n","[Training Epoch 1] Batch 4321, Loss 0.5128852725028992\n","[Training Epoch 1] Batch 4322, Loss 0.48766618967056274\n","[Training Epoch 1] Batch 4323, Loss 0.46357855200767517\n","[Training Epoch 1] Batch 4324, Loss 0.5149523615837097\n","[Training Epoch 1] Batch 4325, Loss 0.5080124139785767\n","[Training Epoch 1] Batch 4326, Loss 0.5093128681182861\n","[Training Epoch 1] Batch 4327, Loss 0.45839059352874756\n","[Training Epoch 1] Batch 4328, Loss 0.5135831832885742\n","[Training Epoch 1] Batch 4329, Loss 0.5000960230827332\n","[Training Epoch 1] Batch 4330, Loss 0.5276066064834595\n","[Training Epoch 1] Batch 4331, Loss 0.506492018699646\n","[Training Epoch 1] Batch 4332, Loss 0.49476730823516846\n","[Training Epoch 1] Batch 4333, Loss 0.5088305473327637\n","[Training Epoch 1] Batch 4334, Loss 0.4917459189891815\n","[Training Epoch 1] Batch 4335, Loss 0.5308681726455688\n","[Training Epoch 1] Batch 4336, Loss 0.5322149991989136\n","[Training Epoch 1] Batch 4337, Loss 0.5067300200462341\n","[Training Epoch 1] Batch 4338, Loss 0.49125903844833374\n","[Training Epoch 1] Batch 4339, Loss 0.4894064962863922\n","[Training Epoch 1] Batch 4340, Loss 0.5218821167945862\n","[Training Epoch 1] Batch 4341, Loss 0.49851179122924805\n","[Training Epoch 1] Batch 4342, Loss 0.48924416303634644\n","[Training Epoch 1] Batch 4343, Loss 0.4739019274711609\n","[Training Epoch 1] Batch 4344, Loss 0.48905134201049805\n","[Training Epoch 1] Batch 4345, Loss 0.4946750998497009\n","[Training Epoch 1] Batch 4346, Loss 0.49841490387916565\n","[Training Epoch 1] Batch 4347, Loss 0.5048282742500305\n","[Training Epoch 1] Batch 4348, Loss 0.49893417954444885\n","[Training Epoch 1] Batch 4349, Loss 0.490744948387146\n","[Training Epoch 1] Batch 4350, Loss 0.491102397441864\n","[Training Epoch 1] Batch 4351, Loss 0.5149775743484497\n","[Training Epoch 1] Batch 4352, Loss 0.5319288372993469\n","[Training Epoch 1] Batch 4353, Loss 0.49619555473327637\n","[Training Epoch 1] Batch 4354, Loss 0.5211200714111328\n","[Training Epoch 1] Batch 4355, Loss 0.5080359578132629\n","[Training Epoch 1] Batch 4356, Loss 0.4990096986293793\n","[Training Epoch 1] Batch 4357, Loss 0.5099607706069946\n","[Training Epoch 1] Batch 4358, Loss 0.4942209720611572\n","[Training Epoch 1] Batch 4359, Loss 0.5001563429832458\n","[Training Epoch 1] Batch 4360, Loss 0.5055928230285645\n","[Training Epoch 1] Batch 4361, Loss 0.533703088760376\n","[Training Epoch 1] Batch 4362, Loss 0.47439903020858765\n","[Training Epoch 1] Batch 4363, Loss 0.5005902647972107\n","[Training Epoch 1] Batch 4364, Loss 0.5383473634719849\n","[Training Epoch 1] Batch 4365, Loss 0.4882854223251343\n","[Training Epoch 1] Batch 4366, Loss 0.4733968675136566\n","[Training Epoch 1] Batch 4367, Loss 0.4873541593551636\n","[Training Epoch 1] Batch 4368, Loss 0.45387670397758484\n","[Training Epoch 1] Batch 4369, Loss 0.47276461124420166\n","[Training Epoch 1] Batch 4370, Loss 0.512991189956665\n","[Training Epoch 1] Batch 4371, Loss 0.5001972317695618\n","[Training Epoch 1] Batch 4372, Loss 0.5192443132400513\n","[Training Epoch 1] Batch 4373, Loss 0.49275028705596924\n","[Training Epoch 1] Batch 4374, Loss 0.48054441809654236\n","[Training Epoch 1] Batch 4375, Loss 0.4995424747467041\n","[Training Epoch 1] Batch 4376, Loss 0.5247128009796143\n","[Training Epoch 1] Batch 4377, Loss 0.507470428943634\n","[Training Epoch 1] Batch 4378, Loss 0.5198154449462891\n","[Training Epoch 1] Batch 4379, Loss 0.492523193359375\n","[Training Epoch 1] Batch 4380, Loss 0.495917946100235\n","[Training Epoch 1] Batch 4381, Loss 0.4935557544231415\n","[Training Epoch 1] Batch 4382, Loss 0.4744492769241333\n","[Training Epoch 1] Batch 4383, Loss 0.4881100654602051\n","[Training Epoch 1] Batch 4384, Loss 0.5215112566947937\n","[Training Epoch 1] Batch 4385, Loss 0.5110344290733337\n","[Training Epoch 1] Batch 4386, Loss 0.5355740785598755\n","[Training Epoch 1] Batch 4387, Loss 0.4636197090148926\n","[Training Epoch 1] Batch 4388, Loss 0.531453013420105\n","[Training Epoch 1] Batch 4389, Loss 0.47089025378227234\n","[Training Epoch 1] Batch 4390, Loss 0.5060518980026245\n","[Training Epoch 1] Batch 4391, Loss 0.5048810243606567\n","[Training Epoch 1] Batch 4392, Loss 0.48997288942337036\n","[Training Epoch 1] Batch 4393, Loss 0.5288036465644836\n","[Training Epoch 1] Batch 4394, Loss 0.5125617384910583\n","[Training Epoch 1] Batch 4395, Loss 0.4899047315120697\n","[Training Epoch 1] Batch 4396, Loss 0.5062597990036011\n","[Training Epoch 1] Batch 4397, Loss 0.5141754150390625\n","[Training Epoch 1] Batch 4398, Loss 0.5097306370735168\n","[Training Epoch 1] Batch 4399, Loss 0.5332866907119751\n","[Training Epoch 1] Batch 4400, Loss 0.497897207736969\n","[Training Epoch 1] Batch 4401, Loss 0.5304228067398071\n","[Training Epoch 1] Batch 4402, Loss 0.4944944679737091\n","[Training Epoch 1] Batch 4403, Loss 0.4956071972846985\n","[Training Epoch 1] Batch 4404, Loss 0.4925532937049866\n","[Training Epoch 1] Batch 4405, Loss 0.4852510690689087\n","[Training Epoch 1] Batch 4406, Loss 0.469765841960907\n","[Training Epoch 1] Batch 4407, Loss 0.48557713627815247\n","[Training Epoch 1] Batch 4408, Loss 0.5143912434577942\n","[Training Epoch 1] Batch 4409, Loss 0.4757881164550781\n","[Training Epoch 1] Batch 4410, Loss 0.48262134194374084\n","[Training Epoch 1] Batch 4411, Loss 0.5075350403785706\n","[Training Epoch 1] Batch 4412, Loss 0.5106288194656372\n","[Training Epoch 1] Batch 4413, Loss 0.4904496967792511\n","[Training Epoch 1] Batch 4414, Loss 0.5145263671875\n","[Training Epoch 1] Batch 4415, Loss 0.5332027673721313\n","[Training Epoch 1] Batch 4416, Loss 0.49603670835494995\n","[Training Epoch 1] Batch 4417, Loss 0.5041195154190063\n","[Training Epoch 1] Batch 4418, Loss 0.5031540393829346\n","[Training Epoch 1] Batch 4419, Loss 0.5015671849250793\n","[Training Epoch 1] Batch 4420, Loss 0.5102404952049255\n","[Training Epoch 1] Batch 4421, Loss 0.4986772835254669\n","[Training Epoch 1] Batch 4422, Loss 0.5179456472396851\n","[Training Epoch 1] Batch 4423, Loss 0.4924989342689514\n","[Training Epoch 1] Batch 4424, Loss 0.5195673704147339\n","[Training Epoch 1] Batch 4425, Loss 0.5048774480819702\n","[Training Epoch 1] Batch 4426, Loss 0.5079869031906128\n","[Training Epoch 1] Batch 4427, Loss 0.4897489547729492\n","[Training Epoch 1] Batch 4428, Loss 0.48485463857650757\n","[Training Epoch 1] Batch 4429, Loss 0.4895640015602112\n","[Training Epoch 1] Batch 4430, Loss 0.5022222995758057\n","[Training Epoch 1] Batch 4431, Loss 0.4929680824279785\n","[Training Epoch 1] Batch 4432, Loss 0.47824275493621826\n","[Training Epoch 1] Batch 4433, Loss 0.5054083466529846\n","[Training Epoch 1] Batch 4434, Loss 0.5013951063156128\n","[Training Epoch 1] Batch 4435, Loss 0.47189056873321533\n","[Training Epoch 1] Batch 4436, Loss 0.4885128438472748\n","[Training Epoch 1] Batch 4437, Loss 0.5136661529541016\n","[Training Epoch 1] Batch 4438, Loss 0.5090342164039612\n","[Training Epoch 1] Batch 4439, Loss 0.5181776881217957\n","[Training Epoch 1] Batch 4440, Loss 0.5041749477386475\n","[Training Epoch 1] Batch 4441, Loss 0.5293399095535278\n","[Training Epoch 1] Batch 4442, Loss 0.5285066366195679\n","[Training Epoch 1] Batch 4443, Loss 0.49633461236953735\n","[Training Epoch 1] Batch 4444, Loss 0.4746987223625183\n","[Training Epoch 1] Batch 4445, Loss 0.5054435729980469\n","[Training Epoch 1] Batch 4446, Loss 0.4685320556163788\n","[Training Epoch 1] Batch 4447, Loss 0.49769172072410583\n","[Training Epoch 1] Batch 4448, Loss 0.5342224836349487\n","[Training Epoch 1] Batch 4449, Loss 0.4776027202606201\n","[Training Epoch 1] Batch 4450, Loss 0.4832613170146942\n","[Training Epoch 1] Batch 4451, Loss 0.49484580755233765\n","[Training Epoch 1] Batch 4452, Loss 0.494035542011261\n","[Training Epoch 1] Batch 4453, Loss 0.4907153844833374\n","[Training Epoch 1] Batch 4454, Loss 0.4870642125606537\n","[Training Epoch 1] Batch 4455, Loss 0.5151705741882324\n","[Training Epoch 1] Batch 4456, Loss 0.4945247173309326\n","[Training Epoch 1] Batch 4457, Loss 0.469670832157135\n","[Training Epoch 1] Batch 4458, Loss 0.5168224573135376\n","[Training Epoch 1] Batch 4459, Loss 0.508841872215271\n","[Training Epoch 1] Batch 4460, Loss 0.5063558220863342\n","[Training Epoch 1] Batch 4461, Loss 0.498737096786499\n","[Training Epoch 1] Batch 4462, Loss 0.47185656428337097\n","[Training Epoch 1] Batch 4463, Loss 0.5302714109420776\n","[Training Epoch 1] Batch 4464, Loss 0.5251466035842896\n","[Training Epoch 1] Batch 4465, Loss 0.5260224342346191\n","[Training Epoch 1] Batch 4466, Loss 0.4805850386619568\n","[Training Epoch 1] Batch 4467, Loss 0.507859468460083\n","[Training Epoch 1] Batch 4468, Loss 0.5208569169044495\n","[Training Epoch 1] Batch 4469, Loss 0.5160109400749207\n","[Training Epoch 1] Batch 4470, Loss 0.47530627250671387\n","[Training Epoch 1] Batch 4471, Loss 0.5035975575447083\n","[Training Epoch 1] Batch 4472, Loss 0.5075993537902832\n","[Training Epoch 1] Batch 4473, Loss 0.4764143228530884\n","[Training Epoch 1] Batch 4474, Loss 0.48657551407814026\n","[Training Epoch 1] Batch 4475, Loss 0.50495445728302\n","[Training Epoch 1] Batch 4476, Loss 0.493735671043396\n","[Training Epoch 1] Batch 4477, Loss 0.516583263874054\n","[Training Epoch 1] Batch 4478, Loss 0.5167929530143738\n","[Training Epoch 1] Batch 4479, Loss 0.5191547870635986\n","[Training Epoch 1] Batch 4480, Loss 0.5240922570228577\n","[Training Epoch 1] Batch 4481, Loss 0.5123782157897949\n","[Training Epoch 1] Batch 4482, Loss 0.5031818151473999\n","[Training Epoch 1] Batch 4483, Loss 0.5125901699066162\n","[Training Epoch 1] Batch 4484, Loss 0.49752670526504517\n","[Training Epoch 1] Batch 4485, Loss 0.5023239850997925\n","[Training Epoch 1] Batch 4486, Loss 0.4929651618003845\n","[Training Epoch 1] Batch 4487, Loss 0.4856208562850952\n","[Training Epoch 1] Batch 4488, Loss 0.4942523241043091\n","[Training Epoch 1] Batch 4489, Loss 0.49102455377578735\n","[Training Epoch 1] Batch 4490, Loss 0.48999500274658203\n","[Training Epoch 1] Batch 4491, Loss 0.4848954677581787\n","[Training Epoch 1] Batch 4492, Loss 0.49750834703445435\n","[Training Epoch 1] Batch 4493, Loss 0.521058201789856\n","[Training Epoch 1] Batch 4494, Loss 0.4793543815612793\n","[Training Epoch 1] Batch 4495, Loss 0.5222126841545105\n","[Training Epoch 1] Batch 4496, Loss 0.5009366273880005\n","[Training Epoch 1] Batch 4497, Loss 0.5150071978569031\n","[Training Epoch 1] Batch 4498, Loss 0.5158802270889282\n","[Training Epoch 1] Batch 4499, Loss 0.49724268913269043\n","[Training Epoch 1] Batch 4500, Loss 0.5262525677680969\n","[Training Epoch 1] Batch 4501, Loss 0.5045186281204224\n","[Training Epoch 1] Batch 4502, Loss 0.5068216919898987\n","[Training Epoch 1] Batch 4503, Loss 0.48446518182754517\n","[Training Epoch 1] Batch 4504, Loss 0.5078210830688477\n","[Training Epoch 1] Batch 4505, Loss 0.47767698764801025\n","[Training Epoch 1] Batch 4506, Loss 0.5020687580108643\n","[Training Epoch 1] Batch 4507, Loss 0.5046610236167908\n","[Training Epoch 1] Batch 4508, Loss 0.5106354355812073\n","[Training Epoch 1] Batch 4509, Loss 0.5008646249771118\n","[Training Epoch 1] Batch 4510, Loss 0.47567933797836304\n","[Training Epoch 1] Batch 4511, Loss 0.4707043766975403\n","[Training Epoch 1] Batch 4512, Loss 0.5002806782722473\n","[Training Epoch 1] Batch 4513, Loss 0.4898177981376648\n","[Training Epoch 1] Batch 4514, Loss 0.4843566417694092\n","[Training Epoch 1] Batch 4515, Loss 0.5118619203567505\n","[Training Epoch 1] Batch 4516, Loss 0.5181859731674194\n","[Training Epoch 1] Batch 4517, Loss 0.4975970983505249\n","[Training Epoch 1] Batch 4518, Loss 0.5077073574066162\n","[Training Epoch 1] Batch 4519, Loss 0.5131030082702637\n","[Training Epoch 1] Batch 4520, Loss 0.5040359497070312\n","[Training Epoch 1] Batch 4521, Loss 0.48811808228492737\n","[Training Epoch 1] Batch 4522, Loss 0.504115104675293\n","[Training Epoch 1] Batch 4523, Loss 0.5189879536628723\n","[Training Epoch 1] Batch 4524, Loss 0.5005590319633484\n","[Training Epoch 1] Batch 4525, Loss 0.4555180072784424\n","[Training Epoch 1] Batch 4526, Loss 0.5060470104217529\n","[Training Epoch 1] Batch 4527, Loss 0.5120294690132141\n","[Training Epoch 1] Batch 4528, Loss 0.4886133372783661\n","[Training Epoch 1] Batch 4529, Loss 0.5095594525337219\n","[Training Epoch 1] Batch 4530, Loss 0.4827963411808014\n","[Training Epoch 1] Batch 4531, Loss 0.519298791885376\n","[Training Epoch 1] Batch 4532, Loss 0.5149455070495605\n","[Training Epoch 1] Batch 4533, Loss 0.4808405041694641\n","[Training Epoch 1] Batch 4534, Loss 0.5187283754348755\n","[Training Epoch 1] Batch 4535, Loss 0.5133405327796936\n","[Training Epoch 1] Batch 4536, Loss 0.5250377655029297\n","[Training Epoch 1] Batch 4537, Loss 0.4773152470588684\n","[Training Epoch 1] Batch 4538, Loss 0.4924718737602234\n","[Training Epoch 1] Batch 4539, Loss 0.497196763753891\n","[Training Epoch 1] Batch 4540, Loss 0.5030415058135986\n","[Training Epoch 1] Batch 4541, Loss 0.4966994524002075\n","[Training Epoch 1] Batch 4542, Loss 0.5301967263221741\n","[Training Epoch 1] Batch 4543, Loss 0.5219053626060486\n","[Training Epoch 1] Batch 4544, Loss 0.4949101507663727\n","[Training Epoch 1] Batch 4545, Loss 0.49495378136634827\n","[Training Epoch 1] Batch 4546, Loss 0.47324326634407043\n","[Training Epoch 1] Batch 4547, Loss 0.46510136127471924\n","[Training Epoch 1] Batch 4548, Loss 0.5030590891838074\n","[Training Epoch 1] Batch 4549, Loss 0.5119068622589111\n","[Training Epoch 1] Batch 4550, Loss 0.47846996784210205\n","[Training Epoch 1] Batch 4551, Loss 0.5113391280174255\n","[Training Epoch 1] Batch 4552, Loss 0.5030511021614075\n","[Training Epoch 1] Batch 4553, Loss 0.4956739544868469\n","[Training Epoch 1] Batch 4554, Loss 0.5070008039474487\n","[Training Epoch 1] Batch 4555, Loss 0.533923864364624\n","[Training Epoch 1] Batch 4556, Loss 0.4967830181121826\n","[Training Epoch 1] Batch 4557, Loss 0.4885499179363251\n","[Training Epoch 1] Batch 4558, Loss 0.5006700754165649\n","[Training Epoch 1] Batch 4559, Loss 0.5112366676330566\n","[Training Epoch 1] Batch 4560, Loss 0.4964211881160736\n","[Training Epoch 1] Batch 4561, Loss 0.5179311633110046\n","[Training Epoch 1] Batch 4562, Loss 0.481458842754364\n","[Training Epoch 1] Batch 4563, Loss 0.4768913984298706\n","[Training Epoch 1] Batch 4564, Loss 0.49403834342956543\n","[Training Epoch 1] Batch 4565, Loss 0.4584014415740967\n","[Training Epoch 1] Batch 4566, Loss 0.505058765411377\n","[Training Epoch 1] Batch 4567, Loss 0.4809519052505493\n","[Training Epoch 1] Batch 4568, Loss 0.4900851249694824\n","[Training Epoch 1] Batch 4569, Loss 0.5382720232009888\n","[Training Epoch 1] Batch 4570, Loss 0.47129979729652405\n","[Training Epoch 1] Batch 4571, Loss 0.5272630453109741\n","[Training Epoch 1] Batch 4572, Loss 0.5044172406196594\n","[Training Epoch 1] Batch 4573, Loss 0.48492172360420227\n","[Training Epoch 1] Batch 4574, Loss 0.4936703145503998\n","[Training Epoch 1] Batch 4575, Loss 0.47381508350372314\n","[Training Epoch 1] Batch 4576, Loss 0.49616557359695435\n","[Training Epoch 1] Batch 4577, Loss 0.5051135420799255\n","[Training Epoch 1] Batch 4578, Loss 0.498974084854126\n","[Training Epoch 1] Batch 4579, Loss 0.48156997561454773\n","[Training Epoch 1] Batch 4580, Loss 0.5164787769317627\n","[Training Epoch 1] Batch 4581, Loss 0.5205539464950562\n","[Training Epoch 1] Batch 4582, Loss 0.516376256942749\n","[Training Epoch 1] Batch 4583, Loss 0.4772041440010071\n","[Training Epoch 1] Batch 4584, Loss 0.5062143206596375\n","[Training Epoch 1] Batch 4585, Loss 0.47569185495376587\n","[Training Epoch 1] Batch 4586, Loss 0.4983377456665039\n","[Training Epoch 1] Batch 4587, Loss 0.49148693680763245\n","[Training Epoch 1] Batch 4588, Loss 0.4972607493400574\n","[Training Epoch 1] Batch 4589, Loss 0.49502086639404297\n","[Training Epoch 1] Batch 4590, Loss 0.47625213861465454\n","[Training Epoch 1] Batch 4591, Loss 0.47813159227371216\n","[Training Epoch 1] Batch 4592, Loss 0.492632657289505\n","[Training Epoch 1] Batch 4593, Loss 0.5062372088432312\n","[Training Epoch 1] Batch 4594, Loss 0.49883168935775757\n","[Training Epoch 1] Batch 4595, Loss 0.47378140687942505\n","[Training Epoch 1] Batch 4596, Loss 0.4755646586418152\n","[Training Epoch 1] Batch 4597, Loss 0.513319730758667\n","[Training Epoch 1] Batch 4598, Loss 0.49454939365386963\n","[Training Epoch 1] Batch 4599, Loss 0.5345628261566162\n","[Training Epoch 1] Batch 4600, Loss 0.517850399017334\n","[Training Epoch 1] Batch 4601, Loss 0.469044029712677\n","[Training Epoch 1] Batch 4602, Loss 0.4850348234176636\n","[Training Epoch 1] Batch 4603, Loss 0.4967402219772339\n","[Training Epoch 1] Batch 4604, Loss 0.49981629848480225\n","[Training Epoch 1] Batch 4605, Loss 0.4942821264266968\n","[Training Epoch 1] Batch 4606, Loss 0.5175465941429138\n","[Training Epoch 1] Batch 4607, Loss 0.45133858919143677\n","[Training Epoch 1] Batch 4608, Loss 0.5420854091644287\n","[Training Epoch 1] Batch 4609, Loss 0.5155474543571472\n","[Training Epoch 1] Batch 4610, Loss 0.5093063712120056\n","[Training Epoch 1] Batch 4611, Loss 0.49775582551956177\n","[Training Epoch 1] Batch 4612, Loss 0.47202563285827637\n","[Training Epoch 1] Batch 4613, Loss 0.5052313804626465\n","[Training Epoch 1] Batch 4614, Loss 0.5271817445755005\n","[Training Epoch 1] Batch 4615, Loss 0.5203434228897095\n","[Training Epoch 1] Batch 4616, Loss 0.47749409079551697\n","[Training Epoch 1] Batch 4617, Loss 0.5061808228492737\n","[Training Epoch 1] Batch 4618, Loss 0.49206915497779846\n","[Training Epoch 1] Batch 4619, Loss 0.5110126733779907\n","[Training Epoch 1] Batch 4620, Loss 0.5324174165725708\n","[Training Epoch 1] Batch 4621, Loss 0.5215036869049072\n","[Training Epoch 1] Batch 4622, Loss 0.5110332369804382\n","[Training Epoch 1] Batch 4623, Loss 0.5073931813240051\n","[Training Epoch 1] Batch 4624, Loss 0.4874744713306427\n","[Training Epoch 1] Batch 4625, Loss 0.49968966841697693\n","[Training Epoch 1] Batch 4626, Loss 0.49659544229507446\n","[Training Epoch 1] Batch 4627, Loss 0.4874534606933594\n","[Training Epoch 1] Batch 4628, Loss 0.47269588708877563\n","[Training Epoch 1] Batch 4629, Loss 0.4934536814689636\n","[Training Epoch 1] Batch 4630, Loss 0.49906277656555176\n","[Training Epoch 1] Batch 4631, Loss 0.48884257674217224\n","[Training Epoch 1] Batch 4632, Loss 0.46778878569602966\n","[Training Epoch 1] Batch 4633, Loss 0.5135319828987122\n","[Training Epoch 1] Batch 4634, Loss 0.5027874112129211\n","[Training Epoch 1] Batch 4635, Loss 0.5166314244270325\n","[Training Epoch 1] Batch 4636, Loss 0.48575839400291443\n","[Training Epoch 1] Batch 4637, Loss 0.49271360039711\n","[Training Epoch 1] Batch 4638, Loss 0.47398704290390015\n","[Training Epoch 1] Batch 4639, Loss 0.511273205280304\n","[Training Epoch 1] Batch 4640, Loss 0.4688788056373596\n","[Training Epoch 1] Batch 4641, Loss 0.48549413681030273\n","[Training Epoch 1] Batch 4642, Loss 0.4995964765548706\n","[Training Epoch 1] Batch 4643, Loss 0.539620041847229\n","[Training Epoch 1] Batch 4644, Loss 0.49649593234062195\n","[Training Epoch 1] Batch 4645, Loss 0.5265535116195679\n","[Training Epoch 1] Batch 4646, Loss 0.5073571801185608\n","[Training Epoch 1] Batch 4647, Loss 0.5090785026550293\n","[Training Epoch 1] Batch 4648, Loss 0.5296440124511719\n","[Training Epoch 1] Batch 4649, Loss 0.49103426933288574\n","[Training Epoch 1] Batch 4650, Loss 0.513189971446991\n","[Training Epoch 1] Batch 4651, Loss 0.533639132976532\n","[Training Epoch 1] Batch 4652, Loss 0.47421061992645264\n","[Training Epoch 1] Batch 4653, Loss 0.5051945447921753\n","[Training Epoch 1] Batch 4654, Loss 0.5065850019454956\n","[Training Epoch 1] Batch 4655, Loss 0.4833606779575348\n","[Training Epoch 1] Batch 4656, Loss 0.5036877989768982\n","[Training Epoch 1] Batch 4657, Loss 0.5007807016372681\n","[Training Epoch 1] Batch 4658, Loss 0.5012500882148743\n","[Training Epoch 1] Batch 4659, Loss 0.48711439967155457\n","[Training Epoch 1] Batch 4660, Loss 0.478690505027771\n","[Training Epoch 1] Batch 4661, Loss 0.5047513246536255\n","[Training Epoch 1] Batch 4662, Loss 0.4932413697242737\n","[Training Epoch 1] Batch 4663, Loss 0.4953024387359619\n","[Training Epoch 1] Batch 4664, Loss 0.4920503497123718\n","[Training Epoch 1] Batch 4665, Loss 0.4858509302139282\n","[Training Epoch 1] Batch 4666, Loss 0.4920312166213989\n","[Training Epoch 1] Batch 4667, Loss 0.5127260088920593\n","[Training Epoch 1] Batch 4668, Loss 0.547264575958252\n","[Training Epoch 1] Batch 4669, Loss 0.501499354839325\n","[Training Epoch 1] Batch 4670, Loss 0.4755987823009491\n","[Training Epoch 1] Batch 4671, Loss 0.5340545177459717\n","[Training Epoch 1] Batch 4672, Loss 0.49271273612976074\n","[Training Epoch 1] Batch 4673, Loss 0.5004554986953735\n","[Training Epoch 1] Batch 4674, Loss 0.4813763499259949\n","[Training Epoch 1] Batch 4675, Loss 0.5107812881469727\n","[Training Epoch 1] Batch 4676, Loss 0.4844493269920349\n","[Training Epoch 1] Batch 4677, Loss 0.49831539392471313\n","[Training Epoch 1] Batch 4678, Loss 0.5154575109481812\n","[Training Epoch 1] Batch 4679, Loss 0.4998222887516022\n","[Training Epoch 1] Batch 4680, Loss 0.5041922330856323\n","[Training Epoch 1] Batch 4681, Loss 0.5050815343856812\n","[Training Epoch 1] Batch 4682, Loss 0.49910300970077515\n","[Training Epoch 1] Batch 4683, Loss 0.4951736032962799\n","[Training Epoch 1] Batch 4684, Loss 0.47868549823760986\n","[Training Epoch 1] Batch 4685, Loss 0.5160289406776428\n","[Training Epoch 1] Batch 4686, Loss 0.5206224918365479\n","[Training Epoch 1] Batch 4687, Loss 0.48214858770370483\n","[Training Epoch 1] Batch 4688, Loss 0.48246967792510986\n","[Training Epoch 1] Batch 4689, Loss 0.48526298999786377\n","[Training Epoch 1] Batch 4690, Loss 0.4972798824310303\n","[Training Epoch 1] Batch 4691, Loss 0.5076851844787598\n","[Training Epoch 1] Batch 4692, Loss 0.47286179661750793\n","[Training Epoch 1] Batch 4693, Loss 0.4700388014316559\n","[Training Epoch 1] Batch 4694, Loss 0.5005638003349304\n","[Training Epoch 1] Batch 4695, Loss 0.49162209033966064\n","[Training Epoch 1] Batch 4696, Loss 0.5018784999847412\n","[Training Epoch 1] Batch 4697, Loss 0.48899874091148376\n","[Training Epoch 1] Batch 4698, Loss 0.5094165802001953\n","[Training Epoch 1] Batch 4699, Loss 0.5000516176223755\n","[Training Epoch 1] Batch 4700, Loss 0.5106365084648132\n","[Training Epoch 1] Batch 4701, Loss 0.5072205662727356\n","[Training Epoch 1] Batch 4702, Loss 0.503463625907898\n","[Training Epoch 1] Batch 4703, Loss 0.5126456022262573\n","[Training Epoch 1] Batch 4704, Loss 0.504808247089386\n","[Training Epoch 1] Batch 4705, Loss 0.4924513101577759\n","[Training Epoch 1] Batch 4706, Loss 0.5133723020553589\n","[Training Epoch 1] Batch 4707, Loss 0.5015590786933899\n","[Training Epoch 1] Batch 4708, Loss 0.5216948986053467\n","[Training Epoch 1] Batch 4709, Loss 0.5208340287208557\n","[Training Epoch 1] Batch 4710, Loss 0.5251225233078003\n","[Training Epoch 1] Batch 4711, Loss 0.5088581442832947\n","[Training Epoch 1] Batch 4712, Loss 0.5158501267433167\n","[Training Epoch 1] Batch 4713, Loss 0.5242903232574463\n","[Training Epoch 1] Batch 4714, Loss 0.5050811767578125\n","[Training Epoch 1] Batch 4715, Loss 0.47044187784194946\n","[Training Epoch 1] Batch 4716, Loss 0.48392176628112793\n","[Training Epoch 1] Batch 4717, Loss 0.4901733100414276\n","[Training Epoch 1] Batch 4718, Loss 0.4934012293815613\n","[Training Epoch 1] Batch 4719, Loss 0.5206803679466248\n","[Training Epoch 1] Batch 4720, Loss 0.4893326163291931\n","[Training Epoch 1] Batch 4721, Loss 0.5059670209884644\n","[Training Epoch 1] Batch 4722, Loss 0.5152806043624878\n","[Training Epoch 1] Batch 4723, Loss 0.49810951948165894\n","[Training Epoch 1] Batch 4724, Loss 0.5134523510932922\n","[Training Epoch 1] Batch 4725, Loss 0.5162875652313232\n","[Training Epoch 1] Batch 4726, Loss 0.5151891708374023\n","[Training Epoch 1] Batch 4727, Loss 0.4775812029838562\n","[Training Epoch 1] Batch 4728, Loss 0.48519301414489746\n","[Training Epoch 1] Batch 4729, Loss 0.48282912373542786\n","[Training Epoch 1] Batch 4730, Loss 0.492714524269104\n","[Training Epoch 1] Batch 4731, Loss 0.5121883153915405\n","[Training Epoch 1] Batch 4732, Loss 0.4747697710990906\n","[Training Epoch 1] Batch 4733, Loss 0.4938277304172516\n","[Training Epoch 1] Batch 4734, Loss 0.46779289841651917\n","[Training Epoch 1] Batch 4735, Loss 0.49401378631591797\n","[Training Epoch 1] Batch 4736, Loss 0.4811928868293762\n","[Training Epoch 1] Batch 4737, Loss 0.49060261249542236\n","[Training Epoch 1] Batch 4738, Loss 0.4898601174354553\n","[Training Epoch 1] Batch 4739, Loss 0.47279706597328186\n","[Training Epoch 1] Batch 4740, Loss 0.48944926261901855\n","[Training Epoch 1] Batch 4741, Loss 0.5232006311416626\n","[Training Epoch 1] Batch 4742, Loss 0.5029330253601074\n","[Training Epoch 1] Batch 4743, Loss 0.4775956869125366\n","[Training Epoch 1] Batch 4744, Loss 0.4894951581954956\n","[Training Epoch 1] Batch 4745, Loss 0.5031681060791016\n","[Training Epoch 1] Batch 4746, Loss 0.4911835193634033\n","[Training Epoch 1] Batch 4747, Loss 0.49540722370147705\n","[Training Epoch 1] Batch 4748, Loss 0.5290732383728027\n","[Training Epoch 1] Batch 4749, Loss 0.5181832313537598\n","[Training Epoch 1] Batch 4750, Loss 0.5294985771179199\n","[Training Epoch 1] Batch 4751, Loss 0.4974706470966339\n","[Training Epoch 1] Batch 4752, Loss 0.49383458495140076\n","[Training Epoch 1] Batch 4753, Loss 0.5015758275985718\n","[Training Epoch 1] Batch 4754, Loss 0.4888603091239929\n","[Training Epoch 1] Batch 4755, Loss 0.4947120249271393\n","[Training Epoch 1] Batch 4756, Loss 0.5030224323272705\n","[Training Epoch 1] Batch 4757, Loss 0.48785218596458435\n","[Training Epoch 1] Batch 4758, Loss 0.47216081619262695\n","[Training Epoch 1] Batch 4759, Loss 0.49585995078086853\n","[Training Epoch 1] Batch 4760, Loss 0.5085274577140808\n","[Training Epoch 1] Batch 4761, Loss 0.5133816599845886\n","[Training Epoch 1] Batch 4762, Loss 0.4788680672645569\n","[Training Epoch 1] Batch 4763, Loss 0.49669378995895386\n","[Training Epoch 1] Batch 4764, Loss 0.5020602941513062\n","[Training Epoch 1] Batch 4765, Loss 0.49606308341026306\n","[Training Epoch 1] Batch 4766, Loss 0.5079744458198547\n","[Training Epoch 1] Batch 4767, Loss 0.5167548060417175\n","[Training Epoch 1] Batch 4768, Loss 0.5108839273452759\n","[Training Epoch 1] Batch 4769, Loss 0.5133885145187378\n","[Training Epoch 1] Batch 4770, Loss 0.5458427667617798\n","[Training Epoch 1] Batch 4771, Loss 0.5164110064506531\n","[Training Epoch 1] Batch 4772, Loss 0.48501405119895935\n","[Training Epoch 1] Batch 4773, Loss 0.4923195540904999\n","[Training Epoch 1] Batch 4774, Loss 0.4998010993003845\n","[Training Epoch 1] Batch 4775, Loss 0.5042924880981445\n","[Training Epoch 1] Batch 4776, Loss 0.5088092088699341\n","[Training Epoch 1] Batch 4777, Loss 0.5054144859313965\n","[Training Epoch 1] Batch 4778, Loss 0.4668540954589844\n","[Training Epoch 1] Batch 4779, Loss 0.4917072057723999\n","[Training Epoch 1] Batch 4780, Loss 0.5088438391685486\n","[Training Epoch 1] Batch 4781, Loss 0.5083370208740234\n","[Training Epoch 1] Batch 4782, Loss 0.4799410104751587\n","[Training Epoch 1] Batch 4783, Loss 0.49970313906669617\n","[Training Epoch 1] Batch 4784, Loss 0.49021852016448975\n","[Training Epoch 1] Batch 4785, Loss 0.48981228470802307\n","[Training Epoch 1] Batch 4786, Loss 0.5119633078575134\n","[Training Epoch 1] Batch 4787, Loss 0.4981171488761902\n","[Training Epoch 1] Batch 4788, Loss 0.5124074816703796\n","[Training Epoch 1] Batch 4789, Loss 0.4597214162349701\n","[Training Epoch 1] Batch 4790, Loss 0.4924403727054596\n","[Training Epoch 1] Batch 4791, Loss 0.5054866075515747\n","[Training Epoch 1] Batch 4792, Loss 0.47392499446868896\n","[Training Epoch 1] Batch 4793, Loss 0.49565234780311584\n","[Training Epoch 1] Batch 4794, Loss 0.49462106823921204\n","[Training Epoch 1] Batch 4795, Loss 0.49623262882232666\n","[Training Epoch 1] Batch 4796, Loss 0.4743140935897827\n","[Training Epoch 1] Batch 4797, Loss 0.4916088581085205\n","[Training Epoch 1] Batch 4798, Loss 0.5040311217308044\n","[Training Epoch 1] Batch 4799, Loss 0.5269113183021545\n","[Training Epoch 1] Batch 4800, Loss 0.5364716649055481\n","[Training Epoch 1] Batch 4801, Loss 0.5204834938049316\n","[Training Epoch 1] Batch 4802, Loss 0.5057834386825562\n","[Training Epoch 1] Batch 4803, Loss 0.4729987382888794\n","[Training Epoch 1] Batch 4804, Loss 0.4649193584918976\n","[Training Epoch 1] Batch 4805, Loss 0.5078965425491333\n","[Training Epoch 1] Batch 4806, Loss 0.4930530786514282\n","[Training Epoch 1] Batch 4807, Loss 0.5134134888648987\n","[Training Epoch 1] Batch 4808, Loss 0.49164626002311707\n","[Training Epoch 1] Batch 4809, Loss 0.4960029423236847\n","[Training Epoch 1] Batch 4810, Loss 0.4962860345840454\n","[Training Epoch 1] Batch 4811, Loss 0.49504247307777405\n","[Training Epoch 1] Batch 4812, Loss 0.5002343654632568\n","[Training Epoch 1] Batch 4813, Loss 0.49198079109191895\n","[Training Epoch 1] Batch 4814, Loss 0.4881480932235718\n","[Training Epoch 1] Batch 4815, Loss 0.4796026051044464\n","[Training Epoch 1] Batch 4816, Loss 0.5183019638061523\n","[Training Epoch 1] Batch 4817, Loss 0.46392595767974854\n","[Training Epoch 1] Batch 4818, Loss 0.49953213334083557\n","[Training Epoch 1] Batch 4819, Loss 0.5127891302108765\n","[Training Epoch 1] Batch 4820, Loss 0.5137840509414673\n","[Training Epoch 1] Batch 4821, Loss 0.4931516945362091\n","[Training Epoch 1] Batch 4822, Loss 0.5242611169815063\n","[Training Epoch 1] Batch 4823, Loss 0.4926653206348419\n","[Training Epoch 1] Batch 4824, Loss 0.5032870769500732\n","[Training Epoch 1] Batch 4825, Loss 0.5034675598144531\n","[Training Epoch 1] Batch 4826, Loss 0.5033785104751587\n","[Training Epoch 1] Batch 4827, Loss 0.5049183368682861\n","[Training Epoch 1] Batch 4828, Loss 0.5060611367225647\n","[Training Epoch 1] Batch 4829, Loss 0.4872303605079651\n","[Training Epoch 1] Batch 4830, Loss 0.49097299575805664\n","[Training Epoch 1] Batch 4831, Loss 0.5097441673278809\n","[Training Epoch 1] Batch 4832, Loss 0.485917329788208\n","[Training Epoch 1] Batch 4833, Loss 0.5115534067153931\n","[Training Epoch 1] Batch 4834, Loss 0.5058148503303528\n","[Training Epoch 1] Batch 4835, Loss 0.4816552698612213\n","[Training Epoch 1] Batch 4836, Loss 0.47755634784698486\n","[Training Epoch 1] Batch 4837, Loss 0.5120930075645447\n","[Training Epoch 1] Batch 4838, Loss 0.4911618232727051\n","[Training Epoch 1] Batch 4839, Loss 0.518873929977417\n","[Training Epoch 1] Batch 4840, Loss 0.5173883438110352\n","[Training Epoch 1] Batch 4841, Loss 0.5130606889724731\n","[Training Epoch 1] Batch 4842, Loss 0.4915103614330292\n","[Training Epoch 1] Batch 4843, Loss 0.5048801302909851\n","[Training Epoch 1] Batch 4844, Loss 0.4510292410850525\n","[Training Epoch 1] Batch 4845, Loss 0.5058069825172424\n","[Training Epoch 1] Batch 4846, Loss 0.498725026845932\n","[Training Epoch 1] Batch 4847, Loss 0.5058626532554626\n","[Training Epoch 1] Batch 4848, Loss 0.5131379961967468\n","[Training Epoch 1] Batch 4849, Loss 0.49865007400512695\n","[Training Epoch 1] Batch 4850, Loss 0.5118221044540405\n","[Training Epoch 1] Batch 4851, Loss 0.5074363946914673\n","[Training Epoch 1] Batch 4852, Loss 0.4888865351676941\n","[Training Epoch 1] Batch 4853, Loss 0.5166335701942444\n","[Training Epoch 1] Batch 4854, Loss 0.5024408102035522\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:52: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame.\n","Try using .loc[row_indexer,col_indexer] = value instead\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"]},{"name":"stdout","output_type":"stream","text":["[Evluating Epoch 1] HR = 0.1121, NDCG = 0.0526\n","Epoch 2 starts !\n","--------------------------------------------------------------------------------\n","[Training Epoch 2] Batch 0, Loss 0.4970337450504303\n","[Training Epoch 2] Batch 1, Loss 0.504122257232666\n","[Training Epoch 2] Batch 2, Loss 0.48990797996520996\n","[Training Epoch 2] Batch 3, Loss 0.5213944911956787\n","[Training Epoch 2] Batch 4, Loss 0.4923635423183441\n","[Training Epoch 2] Batch 5, Loss 0.5141510367393494\n","[Training Epoch 2] Batch 6, Loss 0.46673068404197693\n","[Training Epoch 2] Batch 7, Loss 0.5010063052177429\n","[Training Epoch 2] Batch 8, Loss 0.4988630712032318\n","[Training Epoch 2] Batch 9, Loss 0.5158907771110535\n","[Training Epoch 2] Batch 10, Loss 0.5052897930145264\n","[Training Epoch 2] Batch 11, Loss 0.5184845924377441\n","[Training Epoch 2] Batch 12, Loss 0.535929799079895\n","[Training Epoch 2] Batch 13, Loss 0.530860424041748\n","[Training Epoch 2] Batch 14, Loss 0.5163631439208984\n","[Training Epoch 2] Batch 15, Loss 0.48113271594047546\n","[Training Epoch 2] Batch 16, Loss 0.4859076738357544\n","[Training Epoch 2] Batch 17, Loss 0.49676764011383057\n","[Training Epoch 2] Batch 18, Loss 0.5045732259750366\n","[Training Epoch 2] Batch 19, Loss 0.5259758234024048\n","[Training Epoch 2] Batch 20, Loss 0.500741720199585\n","[Training Epoch 2] Batch 21, Loss 0.4982532858848572\n","[Training Epoch 2] Batch 22, Loss 0.48797258734703064\n","[Training Epoch 2] Batch 23, Loss 0.50511634349823\n","[Training Epoch 2] Batch 24, Loss 0.4978477358818054\n","[Training Epoch 2] Batch 25, Loss 0.4738011658191681\n","[Training Epoch 2] Batch 26, Loss 0.5016132593154907\n","[Training Epoch 2] Batch 27, Loss 0.47064656019210815\n","[Training Epoch 2] Batch 28, Loss 0.46957623958587646\n","[Training Epoch 2] Batch 29, Loss 0.5146956443786621\n","[Training Epoch 2] Batch 30, Loss 0.48120659589767456\n","[Training Epoch 2] Batch 31, Loss 0.5045479536056519\n","[Training Epoch 2] Batch 32, Loss 0.4730004072189331\n","[Training Epoch 2] Batch 33, Loss 0.5082942843437195\n","[Training Epoch 2] Batch 34, Loss 0.4573215842247009\n","[Training Epoch 2] Batch 35, Loss 0.4984133243560791\n","[Training Epoch 2] Batch 36, Loss 0.5102653503417969\n","[Training Epoch 2] Batch 37, Loss 0.4974592924118042\n","[Training Epoch 2] Batch 38, Loss 0.5055362582206726\n","[Training Epoch 2] Batch 39, Loss 0.5025505423545837\n","[Training Epoch 2] Batch 40, Loss 0.4986029863357544\n","[Training Epoch 2] Batch 41, Loss 0.4920811355113983\n","[Training Epoch 2] Batch 42, Loss 0.518484354019165\n","[Training Epoch 2] Batch 43, Loss 0.48390722274780273\n","[Training Epoch 2] Batch 44, Loss 0.48622164130210876\n","[Training Epoch 2] Batch 45, Loss 0.5237669944763184\n","[Training Epoch 2] Batch 46, Loss 0.518609881401062\n","[Training Epoch 2] Batch 47, Loss 0.5186381340026855\n","[Training Epoch 2] Batch 48, Loss 0.5219331979751587\n","[Training Epoch 2] Batch 49, Loss 0.5143698453903198\n","[Training Epoch 2] Batch 50, Loss 0.47988075017929077\n","[Training Epoch 2] Batch 51, Loss 0.5214177370071411\n","[Training Epoch 2] Batch 52, Loss 0.5104781985282898\n","[Training Epoch 2] Batch 53, Loss 0.5297178030014038\n","[Training Epoch 2] Batch 54, Loss 0.5102217197418213\n","[Training Epoch 2] Batch 55, Loss 0.4851122796535492\n","[Training Epoch 2] Batch 56, Loss 0.4957628846168518\n","[Training Epoch 2] Batch 57, Loss 0.4903515577316284\n","[Training Epoch 2] Batch 58, Loss 0.5021815299987793\n","[Training Epoch 2] Batch 59, Loss 0.5200635194778442\n","[Training Epoch 2] Batch 60, Loss 0.5206018090248108\n","[Training Epoch 2] Batch 61, Loss 0.5003038644790649\n","[Training Epoch 2] Batch 62, Loss 0.5097596049308777\n","[Training Epoch 2] Batch 63, Loss 0.5289818048477173\n","[Training Epoch 2] Batch 64, Loss 0.5012513399124146\n","[Training Epoch 2] Batch 65, Loss 0.4864159822463989\n","[Training Epoch 2] Batch 66, Loss 0.4975839853286743\n","[Training Epoch 2] Batch 67, Loss 0.48111480474472046\n","[Training Epoch 2] Batch 68, Loss 0.5050668716430664\n","[Training Epoch 2] Batch 69, Loss 0.5190334320068359\n","[Training Epoch 2] Batch 70, Loss 0.4839678108692169\n","[Training Epoch 2] Batch 71, Loss 0.4975387454032898\n","[Training Epoch 2] Batch 72, Loss 0.4675268530845642\n","[Training Epoch 2] Batch 73, Loss 0.4726129174232483\n","[Training Epoch 2] Batch 74, Loss 0.4837093949317932\n","[Training Epoch 2] Batch 75, Loss 0.5025165677070618\n","[Training Epoch 2] Batch 76, Loss 0.493765652179718\n","[Training Epoch 2] Batch 77, Loss 0.5016494989395142\n","[Training Epoch 2] Batch 78, Loss 0.49020978808403015\n","[Training Epoch 2] Batch 79, Loss 0.4743559658527374\n","[Training Epoch 2] Batch 80, Loss 0.49743369221687317\n","[Training Epoch 2] Batch 81, Loss 0.47471004724502563\n","[Training Epoch 2] Batch 82, Loss 0.44766920804977417\n","[Training Epoch 2] Batch 83, Loss 0.4979022145271301\n","[Training Epoch 2] Batch 84, Loss 0.49132877588272095\n","[Training Epoch 2] Batch 85, Loss 0.48565542697906494\n","[Training Epoch 2] Batch 86, Loss 0.5161543488502502\n","[Training Epoch 2] Batch 87, Loss 0.4904366135597229\n","[Training Epoch 2] Batch 88, Loss 0.5407204627990723\n","[Training Epoch 2] Batch 89, Loss 0.4936476945877075\n","[Training Epoch 2] Batch 90, Loss 0.487096905708313\n","[Training Epoch 2] Batch 91, Loss 0.4850262403488159\n","[Training Epoch 2] Batch 92, Loss 0.5179396867752075\n","[Training Epoch 2] Batch 93, Loss 0.5113322734832764\n","[Training Epoch 2] Batch 94, Loss 0.5388924479484558\n","[Training Epoch 2] Batch 95, Loss 0.48262202739715576\n","[Training Epoch 2] Batch 96, Loss 0.5027409195899963\n","[Training Epoch 2] Batch 97, Loss 0.5082927942276001\n","[Training Epoch 2] Batch 98, Loss 0.5194014310836792\n","[Training Epoch 2] Batch 99, Loss 0.49097540974617004\n","[Training Epoch 2] Batch 100, Loss 0.5213238000869751\n","[Training Epoch 2] Batch 101, Loss 0.5232231616973877\n","[Training Epoch 2] Batch 102, Loss 0.5056349039077759\n","[Training Epoch 2] Batch 103, Loss 0.48998427391052246\n","[Training Epoch 2] Batch 104, Loss 0.5154325366020203\n","[Training Epoch 2] Batch 105, Loss 0.5187560319900513\n","[Training Epoch 2] Batch 106, Loss 0.5321452617645264\n","[Training Epoch 2] Batch 107, Loss 0.5213075876235962\n","[Training Epoch 2] Batch 108, Loss 0.514117419719696\n","[Training Epoch 2] Batch 109, Loss 0.4833373427391052\n","[Training Epoch 2] Batch 110, Loss 0.5176780819892883\n","[Training Epoch 2] Batch 111, Loss 0.49739527702331543\n","[Training Epoch 2] Batch 112, Loss 0.46616798639297485\n","[Training Epoch 2] Batch 113, Loss 0.4931139349937439\n","[Training Epoch 2] Batch 114, Loss 0.5006717443466187\n","[Training Epoch 2] Batch 115, Loss 0.48143473267555237\n","[Training Epoch 2] Batch 116, Loss 0.4941788613796234\n","[Training Epoch 2] Batch 117, Loss 0.493108332157135\n","[Training Epoch 2] Batch 118, Loss 0.49859362840652466\n","[Training Epoch 2] Batch 119, Loss 0.512291669845581\n","[Training Epoch 2] Batch 120, Loss 0.5146570205688477\n","[Training Epoch 2] Batch 121, Loss 0.49014613032341003\n","[Training Epoch 2] Batch 122, Loss 0.48748379945755005\n","[Training Epoch 2] Batch 123, Loss 0.5034275054931641\n","[Training Epoch 2] Batch 124, Loss 0.5236240029335022\n","[Training Epoch 2] Batch 125, Loss 0.5108823776245117\n","[Training Epoch 2] Batch 126, Loss 0.5046486258506775\n","[Training Epoch 2] Batch 127, Loss 0.5125759840011597\n","[Training Epoch 2] Batch 128, Loss 0.4951527714729309\n","[Training Epoch 2] Batch 129, Loss 0.5227807760238647\n","[Training Epoch 2] Batch 130, Loss 0.509568452835083\n","[Training Epoch 2] Batch 131, Loss 0.485943078994751\n","[Training Epoch 2] Batch 132, Loss 0.5199383497238159\n","[Training Epoch 2] Batch 133, Loss 0.5191746950149536\n","[Training Epoch 2] Batch 134, Loss 0.5148345232009888\n","[Training Epoch 2] Batch 135, Loss 0.46770724654197693\n","[Training Epoch 2] Batch 136, Loss 0.5062826871871948\n","[Training Epoch 2] Batch 137, Loss 0.5035624504089355\n","[Training Epoch 2] Batch 138, Loss 0.48867130279541016\n","[Training Epoch 2] Batch 139, Loss 0.4743083119392395\n","[Training Epoch 2] Batch 140, Loss 0.47394686937332153\n","[Training Epoch 2] Batch 141, Loss 0.475632905960083\n","[Training Epoch 2] Batch 142, Loss 0.4985257685184479\n","[Training Epoch 2] Batch 143, Loss 0.49645254015922546\n","[Training Epoch 2] Batch 144, Loss 0.50126051902771\n","[Training Epoch 2] Batch 145, Loss 0.5040152072906494\n","[Training Epoch 2] Batch 146, Loss 0.5091127753257751\n","[Training Epoch 2] Batch 147, Loss 0.470500648021698\n","[Training Epoch 2] Batch 148, Loss 0.4820798635482788\n","[Training Epoch 2] Batch 149, Loss 0.49531665444374084\n","[Training Epoch 2] Batch 150, Loss 0.4789142608642578\n","[Training Epoch 2] Batch 151, Loss 0.49767476320266724\n","[Training Epoch 2] Batch 152, Loss 0.49534687399864197\n","[Training Epoch 2] Batch 153, Loss 0.4947689473628998\n","[Training Epoch 2] Batch 154, Loss 0.5345211625099182\n","[Training Epoch 2] Batch 155, Loss 0.4971727728843689\n","[Training Epoch 2] Batch 156, Loss 0.47954338788986206\n","[Training Epoch 2] Batch 157, Loss 0.4992193579673767\n","[Training Epoch 2] Batch 158, Loss 0.4953947365283966\n","[Training Epoch 2] Batch 159, Loss 0.5245667695999146\n","[Training Epoch 2] Batch 160, Loss 0.4935154616832733\n","[Training Epoch 2] Batch 161, Loss 0.4981127381324768\n","[Training Epoch 2] Batch 162, Loss 0.4793262481689453\n","[Training Epoch 2] Batch 163, Loss 0.46777093410491943\n","[Training Epoch 2] Batch 164, Loss 0.4832717776298523\n","[Training Epoch 2] Batch 165, Loss 0.4964204430580139\n","[Training Epoch 2] Batch 166, Loss 0.48815247416496277\n","[Training Epoch 2] Batch 167, Loss 0.5042509436607361\n","[Training Epoch 2] Batch 168, Loss 0.48206645250320435\n","[Training Epoch 2] Batch 169, Loss 0.4980967044830322\n","[Training Epoch 2] Batch 170, Loss 0.4918399751186371\n","[Training Epoch 2] Batch 171, Loss 0.5396829843521118\n","[Training Epoch 2] Batch 172, Loss 0.5263347625732422\n","[Training Epoch 2] Batch 173, Loss 0.503588080406189\n","[Training Epoch 2] Batch 174, Loss 0.45946457982063293\n","[Training Epoch 2] Batch 175, Loss 0.49523234367370605\n","[Training Epoch 2] Batch 176, Loss 0.5341192483901978\n","[Training Epoch 2] Batch 177, Loss 0.5159465670585632\n","[Training Epoch 2] Batch 178, Loss 0.5187878608703613\n","[Training Epoch 2] Batch 179, Loss 0.49639230966567993\n","[Training Epoch 2] Batch 180, Loss 0.5030133724212646\n","[Training Epoch 2] Batch 181, Loss 0.5023112297058105\n","[Training Epoch 2] Batch 182, Loss 0.49949029088020325\n","[Training Epoch 2] Batch 183, Loss 0.5312381982803345\n","[Training Epoch 2] Batch 184, Loss 0.5101507902145386\n","[Training Epoch 2] Batch 185, Loss 0.47641223669052124\n","[Training Epoch 2] Batch 186, Loss 0.510272204875946\n","[Training Epoch 2] Batch 187, Loss 0.4876554608345032\n","[Training Epoch 2] Batch 188, Loss 0.5208359360694885\n","[Training Epoch 2] Batch 189, Loss 0.5394454002380371\n","[Training Epoch 2] Batch 190, Loss 0.49460193514823914\n","[Training Epoch 2] Batch 191, Loss 0.5182744264602661\n","[Training Epoch 2] Batch 192, Loss 0.4967748820781708\n","[Training Epoch 2] Batch 193, Loss 0.5174750685691833\n","[Training Epoch 2] Batch 194, Loss 0.49268120527267456\n","[Training Epoch 2] Batch 195, Loss 0.49221816658973694\n","[Training Epoch 2] Batch 196, Loss 0.48209282755851746\n","[Training Epoch 2] Batch 197, Loss 0.48492562770843506\n","[Training Epoch 2] Batch 198, Loss 0.5022566914558411\n","[Training Epoch 2] Batch 199, Loss 0.5061803460121155\n","[Training Epoch 2] Batch 200, Loss 0.48985612392425537\n","[Training Epoch 2] Batch 201, Loss 0.5062878727912903\n","[Training Epoch 2] Batch 202, Loss 0.4874002933502197\n","[Training Epoch 2] Batch 203, Loss 0.5029918551445007\n","[Training Epoch 2] Batch 204, Loss 0.48671358823776245\n","[Training Epoch 2] Batch 205, Loss 0.5042356252670288\n","[Training Epoch 2] Batch 206, Loss 0.5086491107940674\n","[Training Epoch 2] Batch 207, Loss 0.5034084320068359\n","[Training Epoch 2] Batch 208, Loss 0.5102962851524353\n","[Training Epoch 2] Batch 209, Loss 0.4906194806098938\n","[Training Epoch 2] Batch 210, Loss 0.5128623247146606\n","[Training Epoch 2] Batch 211, Loss 0.4831460118293762\n","[Training Epoch 2] Batch 212, Loss 0.49834614992141724\n","[Training Epoch 2] Batch 213, Loss 0.47427234053611755\n","[Training Epoch 2] Batch 214, Loss 0.517379879951477\n","[Training Epoch 2] Batch 215, Loss 0.5099278092384338\n","[Training Epoch 2] Batch 216, Loss 0.4882173538208008\n","[Training Epoch 2] Batch 217, Loss 0.4858052432537079\n","[Training Epoch 2] Batch 218, Loss 0.5054759979248047\n","[Training Epoch 2] Batch 219, Loss 0.483537495136261\n","[Training Epoch 2] Batch 220, Loss 0.49821043014526367\n","[Training Epoch 2] Batch 221, Loss 0.4920862019062042\n","[Training Epoch 2] Batch 222, Loss 0.48843514919281006\n","[Training Epoch 2] Batch 223, Loss 0.4698076546192169\n","[Training Epoch 2] Batch 224, Loss 0.5083792805671692\n","[Training Epoch 2] Batch 225, Loss 0.47224515676498413\n","[Training Epoch 2] Batch 226, Loss 0.5186964869499207\n","[Training Epoch 2] Batch 227, Loss 0.5225218534469604\n","[Training Epoch 2] Batch 228, Loss 0.5373424291610718\n","[Training Epoch 2] Batch 229, Loss 0.5192613005638123\n","[Training Epoch 2] Batch 230, Loss 0.5016480684280396\n","[Training Epoch 2] Batch 231, Loss 0.4727119505405426\n","[Training Epoch 2] Batch 232, Loss 0.4891176223754883\n","[Training Epoch 2] Batch 233, Loss 0.48166346549987793\n","[Training Epoch 2] Batch 234, Loss 0.5181005597114563\n","[Training Epoch 2] Batch 235, Loss 0.4980170726776123\n","[Training Epoch 2] Batch 236, Loss 0.48999518156051636\n","[Training Epoch 2] Batch 237, Loss 0.5161285400390625\n","[Training Epoch 2] Batch 238, Loss 0.525492787361145\n","[Training Epoch 2] Batch 239, Loss 0.4865116775035858\n","[Training Epoch 2] Batch 240, Loss 0.4886186718940735\n","[Training Epoch 2] Batch 241, Loss 0.4939758777618408\n","[Training Epoch 2] Batch 242, Loss 0.5303586721420288\n","[Training Epoch 2] Batch 243, Loss 0.476272851228714\n","[Training Epoch 2] Batch 244, Loss 0.5255628824234009\n","[Training Epoch 2] Batch 245, Loss 0.508894681930542\n","[Training Epoch 2] Batch 246, Loss 0.4907967150211334\n","[Training Epoch 2] Batch 247, Loss 0.4886847138404846\n","[Training Epoch 2] Batch 248, Loss 0.4996853172779083\n","[Training Epoch 2] Batch 249, Loss 0.495297372341156\n","[Training Epoch 2] Batch 250, Loss 0.49545949697494507\n","[Training Epoch 2] Batch 251, Loss 0.4855382740497589\n","[Training Epoch 2] Batch 252, Loss 0.47800111770629883\n","[Training Epoch 2] Batch 253, Loss 0.5094050168991089\n","[Training Epoch 2] Batch 254, Loss 0.49174177646636963\n","[Training Epoch 2] Batch 255, Loss 0.5151945948600769\n","[Training Epoch 2] Batch 256, Loss 0.498085081577301\n","[Training Epoch 2] Batch 257, Loss 0.4955669045448303\n","[Training Epoch 2] Batch 258, Loss 0.5046935677528381\n","[Training Epoch 2] Batch 259, Loss 0.5067895650863647\n","[Training Epoch 2] Batch 260, Loss 0.5211209058761597\n","[Training Epoch 2] Batch 261, Loss 0.5180428624153137\n","[Training Epoch 2] Batch 262, Loss 0.525171160697937\n","[Training Epoch 2] Batch 263, Loss 0.5234830975532532\n","[Training Epoch 2] Batch 264, Loss 0.49090439081192017\n","[Training Epoch 2] Batch 265, Loss 0.5104495286941528\n","[Training Epoch 2] Batch 266, Loss 0.4910353720188141\n","[Training Epoch 2] Batch 267, Loss 0.5183695554733276\n","[Training Epoch 2] Batch 268, Loss 0.5010661482810974\n","[Training Epoch 2] Batch 269, Loss 0.5083001852035522\n","[Training Epoch 2] Batch 270, Loss 0.4704055190086365\n","[Training Epoch 2] Batch 271, Loss 0.5074869394302368\n","[Training Epoch 2] Batch 272, Loss 0.5150086879730225\n","[Training Epoch 2] Batch 273, Loss 0.4811137318611145\n","[Training Epoch 2] Batch 274, Loss 0.5027763247489929\n","[Training Epoch 2] Batch 275, Loss 0.5055339336395264\n","[Training Epoch 2] Batch 276, Loss 0.5258836150169373\n","[Training Epoch 2] Batch 277, Loss 0.5146797895431519\n","[Training Epoch 2] Batch 278, Loss 0.5256819725036621\n","[Training Epoch 2] Batch 279, Loss 0.5205850005149841\n","[Training Epoch 2] Batch 280, Loss 0.49258852005004883\n","[Training Epoch 2] Batch 281, Loss 0.5136987566947937\n","[Training Epoch 2] Batch 282, Loss 0.4734879434108734\n","[Training Epoch 2] Batch 283, Loss 0.4743928611278534\n","[Training Epoch 2] Batch 284, Loss 0.5008712410926819\n","[Training Epoch 2] Batch 285, Loss 0.5146636962890625\n","[Training Epoch 2] Batch 286, Loss 0.5110230445861816\n","[Training Epoch 2] Batch 287, Loss 0.5320782661437988\n","[Training Epoch 2] Batch 288, Loss 0.4854832887649536\n","[Training Epoch 2] Batch 289, Loss 0.5036599636077881\n","[Training Epoch 2] Batch 290, Loss 0.4900054931640625\n","[Training Epoch 2] Batch 291, Loss 0.50063157081604\n","[Training Epoch 2] Batch 292, Loss 0.5230459570884705\n","[Training Epoch 2] Batch 293, Loss 0.4997255504131317\n","[Training Epoch 2] Batch 294, Loss 0.4971112012863159\n","[Training Epoch 2] Batch 295, Loss 0.5176074504852295\n","[Training Epoch 2] Batch 296, Loss 0.49600446224212646\n","[Training Epoch 2] Batch 297, Loss 0.4904060363769531\n","[Training Epoch 2] Batch 298, Loss 0.5007016062736511\n","[Training Epoch 2] Batch 299, Loss 0.45225000381469727\n","[Training Epoch 2] Batch 300, Loss 0.5045212507247925\n","[Training Epoch 2] Batch 301, Loss 0.5128733515739441\n","[Training Epoch 2] Batch 302, Loss 0.5065121650695801\n","[Training Epoch 2] Batch 303, Loss 0.5035893321037292\n","[Training Epoch 2] Batch 304, Loss 0.49750852584838867\n","[Training Epoch 2] Batch 305, Loss 0.472779780626297\n","[Training Epoch 2] Batch 306, Loss 0.5152941346168518\n","[Training Epoch 2] Batch 307, Loss 0.5180098414421082\n","[Training Epoch 2] Batch 308, Loss 0.5167112350463867\n","[Training Epoch 2] Batch 309, Loss 0.495315283536911\n","[Training Epoch 2] Batch 310, Loss 0.4991491436958313\n","[Training Epoch 2] Batch 311, Loss 0.4608610272407532\n","[Training Epoch 2] Batch 312, Loss 0.5023751258850098\n","[Training Epoch 2] Batch 313, Loss 0.47447672486305237\n","[Training Epoch 2] Batch 314, Loss 0.5065107345581055\n","[Training Epoch 2] Batch 315, Loss 0.5029361844062805\n","[Training Epoch 2] Batch 316, Loss 0.5063570737838745\n","[Training Epoch 2] Batch 317, Loss 0.4895396828651428\n","[Training Epoch 2] Batch 318, Loss 0.46590256690979004\n","[Training Epoch 2] Batch 319, Loss 0.47251319885253906\n","[Training Epoch 2] Batch 320, Loss 0.5209610462188721\n","[Training Epoch 2] Batch 321, Loss 0.45442724227905273\n","[Training Epoch 2] Batch 322, Loss 0.47084540128707886\n","[Training Epoch 2] Batch 323, Loss 0.5121873021125793\n","[Training Epoch 2] Batch 324, Loss 0.4898771345615387\n","[Training Epoch 2] Batch 325, Loss 0.500511646270752\n","[Training Epoch 2] Batch 326, Loss 0.5026395320892334\n","[Training Epoch 2] Batch 327, Loss 0.49539685249328613\n","[Training Epoch 2] Batch 328, Loss 0.528631329536438\n","[Training Epoch 2] Batch 329, Loss 0.5101244449615479\n","[Training Epoch 2] Batch 330, Loss 0.48084598779678345\n","[Training Epoch 2] Batch 331, Loss 0.475789874792099\n","[Training Epoch 2] Batch 332, Loss 0.47784727811813354\n","[Training Epoch 2] Batch 333, Loss 0.4955053925514221\n","[Training Epoch 2] Batch 334, Loss 0.5137443542480469\n","[Training Epoch 2] Batch 335, Loss 0.48829132318496704\n","[Training Epoch 2] Batch 336, Loss 0.5149621963500977\n","[Training Epoch 2] Batch 337, Loss 0.5275131464004517\n","[Training Epoch 2] Batch 338, Loss 0.5131572484970093\n","[Training Epoch 2] Batch 339, Loss 0.5057220458984375\n","[Training Epoch 2] Batch 340, Loss 0.4748351573944092\n","[Training Epoch 2] Batch 341, Loss 0.4954674541950226\n","[Training Epoch 2] Batch 342, Loss 0.5219161510467529\n","[Training Epoch 2] Batch 343, Loss 0.48186761140823364\n","[Training Epoch 2] Batch 344, Loss 0.5139799118041992\n","[Training Epoch 2] Batch 345, Loss 0.5421536564826965\n","[Training Epoch 2] Batch 346, Loss 0.5029711127281189\n","[Training Epoch 2] Batch 347, Loss 0.47491544485092163\n","[Training Epoch 2] Batch 348, Loss 0.4994288682937622\n","[Training Epoch 2] Batch 349, Loss 0.48375844955444336\n","[Training Epoch 2] Batch 350, Loss 0.4535181224346161\n","[Training Epoch 2] Batch 351, Loss 0.5134175419807434\n","[Training Epoch 2] Batch 352, Loss 0.4997071623802185\n","[Training Epoch 2] Batch 353, Loss 0.5095347762107849\n","[Training Epoch 2] Batch 354, Loss 0.4934636950492859\n","[Training Epoch 2] Batch 355, Loss 0.49597102403640747\n","[Training Epoch 2] Batch 356, Loss 0.5109890103340149\n","[Training Epoch 2] Batch 357, Loss 0.4898067116737366\n","[Training Epoch 2] Batch 358, Loss 0.5184467434883118\n","[Training Epoch 2] Batch 359, Loss 0.5141512155532837\n","[Training Epoch 2] Batch 360, Loss 0.46514278650283813\n","[Training Epoch 2] Batch 361, Loss 0.49775755405426025\n","[Training Epoch 2] Batch 362, Loss 0.5118190050125122\n","[Training Epoch 2] Batch 363, Loss 0.48063331842422485\n","[Training Epoch 2] Batch 364, Loss 0.5492165088653564\n","[Training Epoch 2] Batch 365, Loss 0.5324184894561768\n","[Training Epoch 2] Batch 366, Loss 0.4813172221183777\n","[Training Epoch 2] Batch 367, Loss 0.45874255895614624\n","[Training Epoch 2] Batch 368, Loss 0.531539261341095\n","[Training Epoch 2] Batch 369, Loss 0.4701400399208069\n","[Training Epoch 2] Batch 370, Loss 0.49679869413375854\n","[Training Epoch 2] Batch 371, Loss 0.5161755084991455\n","[Training Epoch 2] Batch 372, Loss 0.5289486646652222\n","[Training Epoch 2] Batch 373, Loss 0.534136950969696\n","[Training Epoch 2] Batch 374, Loss 0.4962248206138611\n","[Training Epoch 2] Batch 375, Loss 0.5008203387260437\n","[Training Epoch 2] Batch 376, Loss 0.49419301748275757\n","[Training Epoch 2] Batch 377, Loss 0.49319007992744446\n","[Training Epoch 2] Batch 378, Loss 0.46901649236679077\n","[Training Epoch 2] Batch 379, Loss 0.5120075345039368\n","[Training Epoch 2] Batch 380, Loss 0.48469340801239014\n","[Training Epoch 2] Batch 381, Loss 0.518295407295227\n","[Training Epoch 2] Batch 382, Loss 0.4955941438674927\n","[Training Epoch 2] Batch 383, Loss 0.4915761649608612\n","[Training Epoch 2] Batch 384, Loss 0.5263921618461609\n","[Training Epoch 2] Batch 385, Loss 0.4827325940132141\n","[Training Epoch 2] Batch 386, Loss 0.47815975546836853\n","[Training Epoch 2] Batch 387, Loss 0.5144236087799072\n","[Training Epoch 2] Batch 388, Loss 0.505871593952179\n","[Training Epoch 2] Batch 389, Loss 0.5149269700050354\n","[Training Epoch 2] Batch 390, Loss 0.49011465907096863\n","[Training Epoch 2] Batch 391, Loss 0.4983493685722351\n","[Training Epoch 2] Batch 392, Loss 0.5050123333930969\n","[Training Epoch 2] Batch 393, Loss 0.4909488558769226\n","[Training Epoch 2] Batch 394, Loss 0.5060140490531921\n","[Training Epoch 2] Batch 395, Loss 0.503193736076355\n","[Training Epoch 2] Batch 396, Loss 0.4590837061405182\n","[Training Epoch 2] Batch 397, Loss 0.48773932456970215\n","[Training Epoch 2] Batch 398, Loss 0.5044772028923035\n","[Training Epoch 2] Batch 399, Loss 0.4973068833351135\n","[Training Epoch 2] Batch 400, Loss 0.5052866339683533\n","[Training Epoch 2] Batch 401, Loss 0.5064073801040649\n","[Training Epoch 2] Batch 402, Loss 0.49643653631210327\n","[Training Epoch 2] Batch 403, Loss 0.5113009214401245\n","[Training Epoch 2] Batch 404, Loss 0.4939340353012085\n","[Training Epoch 2] Batch 405, Loss 0.5065468549728394\n","[Training Epoch 2] Batch 406, Loss 0.5052974224090576\n","[Training Epoch 2] Batch 407, Loss 0.46977850794792175\n","[Training Epoch 2] Batch 408, Loss 0.5040538907051086\n","[Training Epoch 2] Batch 409, Loss 0.5260655879974365\n","[Training Epoch 2] Batch 410, Loss 0.48461660742759705\n","[Training Epoch 2] Batch 411, Loss 0.49366772174835205\n","[Training Epoch 2] Batch 412, Loss 0.48181939125061035\n","[Training Epoch 2] Batch 413, Loss 0.5083375573158264\n","[Training Epoch 2] Batch 414, Loss 0.4877932667732239\n","[Training Epoch 2] Batch 415, Loss 0.47710973024368286\n","[Training Epoch 2] Batch 416, Loss 0.5097061395645142\n","[Training Epoch 2] Batch 417, Loss 0.48667657375335693\n","[Training Epoch 2] Batch 418, Loss 0.5001689195632935\n","[Training Epoch 2] Batch 419, Loss 0.4967983365058899\n","[Training Epoch 2] Batch 420, Loss 0.4557192325592041\n","[Training Epoch 2] Batch 421, Loss 0.47605663537979126\n","[Training Epoch 2] Batch 422, Loss 0.5108195543289185\n","[Training Epoch 2] Batch 423, Loss 0.5020486116409302\n","[Training Epoch 2] Batch 424, Loss 0.4848620593547821\n","[Training Epoch 2] Batch 425, Loss 0.48139703273773193\n","[Training Epoch 2] Batch 426, Loss 0.4851369857788086\n","[Training Epoch 2] Batch 427, Loss 0.4888190031051636\n","[Training Epoch 2] Batch 428, Loss 0.4928542375564575\n","[Training Epoch 2] Batch 429, Loss 0.5199090242385864\n","[Training Epoch 2] Batch 430, Loss 0.5199030637741089\n","[Training Epoch 2] Batch 431, Loss 0.5001817941665649\n","[Training Epoch 2] Batch 432, Loss 0.4853839576244354\n","[Training Epoch 2] Batch 433, Loss 0.46766453981399536\n","[Training Epoch 2] Batch 434, Loss 0.5116174221038818\n","[Training Epoch 2] Batch 435, Loss 0.49767646193504333\n","[Training Epoch 2] Batch 436, Loss 0.5032824277877808\n","[Training Epoch 2] Batch 437, Loss 0.5255610942840576\n","[Training Epoch 2] Batch 438, Loss 0.5061297416687012\n","[Training Epoch 2] Batch 439, Loss 0.4850894808769226\n","[Training Epoch 2] Batch 440, Loss 0.4902362525463104\n","[Training Epoch 2] Batch 441, Loss 0.4797899127006531\n","[Training Epoch 2] Batch 442, Loss 0.5085161924362183\n","[Training Epoch 2] Batch 443, Loss 0.47147881984710693\n","[Training Epoch 2] Batch 444, Loss 0.506109356880188\n","[Training Epoch 2] Batch 445, Loss 0.5069800615310669\n","[Training Epoch 2] Batch 446, Loss 0.4938378930091858\n","[Training Epoch 2] Batch 447, Loss 0.536064624786377\n","[Training Epoch 2] Batch 448, Loss 0.5061808824539185\n","[Training Epoch 2] Batch 449, Loss 0.504773736000061\n","[Training Epoch 2] Batch 450, Loss 0.48003819584846497\n","[Training Epoch 2] Batch 451, Loss 0.48606136441230774\n","[Training Epoch 2] Batch 452, Loss 0.4975990951061249\n","[Training Epoch 2] Batch 453, Loss 0.4877707064151764\n","[Training Epoch 2] Batch 454, Loss 0.47101715207099915\n","[Training Epoch 2] Batch 455, Loss 0.5064988136291504\n","[Training Epoch 2] Batch 456, Loss 0.5034611821174622\n","[Training Epoch 2] Batch 457, Loss 0.48003247380256653\n","[Training Epoch 2] Batch 458, Loss 0.502028226852417\n","[Training Epoch 2] Batch 459, Loss 0.4877531826496124\n","[Training Epoch 2] Batch 460, Loss 0.5167692303657532\n","[Training Epoch 2] Batch 461, Loss 0.4933987259864807\n","[Training Epoch 2] Batch 462, Loss 0.5047104358673096\n","[Training Epoch 2] Batch 463, Loss 0.49302858114242554\n","[Training Epoch 2] Batch 464, Loss 0.5002101063728333\n","[Training Epoch 2] Batch 465, Loss 0.5118839740753174\n","[Training Epoch 2] Batch 466, Loss 0.5042866468429565\n","[Training Epoch 2] Batch 467, Loss 0.5191293954849243\n","[Training Epoch 2] Batch 468, Loss 0.47938382625579834\n","[Training Epoch 2] Batch 469, Loss 0.5015705227851868\n","[Training Epoch 2] Batch 470, Loss 0.48085129261016846\n","[Training Epoch 2] Batch 471, Loss 0.5080955624580383\n","[Training Epoch 2] Batch 472, Loss 0.5155137181282043\n","[Training Epoch 2] Batch 473, Loss 0.4839450418949127\n","[Training Epoch 2] Batch 474, Loss 0.4916629195213318\n","[Training Epoch 2] Batch 475, Loss 0.4789949953556061\n","[Training Epoch 2] Batch 476, Loss 0.5126031637191772\n","[Training Epoch 2] Batch 477, Loss 0.4999692738056183\n","[Training Epoch 2] Batch 478, Loss 0.49434909224510193\n","[Training Epoch 2] Batch 479, Loss 0.4953440725803375\n","[Training Epoch 2] Batch 480, Loss 0.4742603600025177\n","[Training Epoch 2] Batch 481, Loss 0.4958420693874359\n","[Training Epoch 2] Batch 482, Loss 0.5250601172447205\n","[Training Epoch 2] Batch 483, Loss 0.5160223245620728\n","[Training Epoch 2] Batch 484, Loss 0.48627638816833496\n","[Training Epoch 2] Batch 485, Loss 0.5093110799789429\n","[Training Epoch 2] Batch 486, Loss 0.5003941059112549\n","[Training Epoch 2] Batch 487, Loss 0.48382478952407837\n","[Training Epoch 2] Batch 488, Loss 0.5050116777420044\n","[Training Epoch 2] Batch 489, Loss 0.5096791386604309\n","[Training Epoch 2] Batch 490, Loss 0.47108083963394165\n","[Training Epoch 2] Batch 491, Loss 0.5001846551895142\n","[Training Epoch 2] Batch 492, Loss 0.49308785796165466\n","[Training Epoch 2] Batch 493, Loss 0.48170071840286255\n","[Training Epoch 2] Batch 494, Loss 0.5218114852905273\n","[Training Epoch 2] Batch 495, Loss 0.49654367566108704\n","[Training Epoch 2] Batch 496, Loss 0.5187838673591614\n","[Training Epoch 2] Batch 497, Loss 0.4968451261520386\n","[Training Epoch 2] Batch 498, Loss 0.47671255469322205\n","[Training Epoch 2] Batch 499, Loss 0.4863607585430145\n","[Training Epoch 2] Batch 500, Loss 0.5293176174163818\n","[Training Epoch 2] Batch 501, Loss 0.5175678730010986\n","[Training Epoch 2] Batch 502, Loss 0.5091751217842102\n","[Training Epoch 2] Batch 503, Loss 0.49760130047798157\n","[Training Epoch 2] Batch 504, Loss 0.4980286955833435\n","[Training Epoch 2] Batch 505, Loss 0.5105688571929932\n","[Training Epoch 2] Batch 506, Loss 0.49480608105659485\n","[Training Epoch 2] Batch 507, Loss 0.4833090901374817\n","[Training Epoch 2] Batch 508, Loss 0.4886697828769684\n","[Training Epoch 2] Batch 509, Loss 0.5122732520103455\n","[Training Epoch 2] Batch 510, Loss 0.484651654958725\n","[Training Epoch 2] Batch 511, Loss 0.5177121758460999\n","[Training Epoch 2] Batch 512, Loss 0.49832046031951904\n","[Training Epoch 2] Batch 513, Loss 0.5169995427131653\n","[Training Epoch 2] Batch 514, Loss 0.5229567289352417\n","[Training Epoch 2] Batch 515, Loss 0.5218318700790405\n","[Training Epoch 2] Batch 516, Loss 0.4767993688583374\n","[Training Epoch 2] Batch 517, Loss 0.4767814576625824\n","[Training Epoch 2] Batch 518, Loss 0.5163284540176392\n","[Training Epoch 2] Batch 519, Loss 0.5083001852035522\n","[Training Epoch 2] Batch 520, Loss 0.4943341910839081\n","[Training Epoch 2] Batch 521, Loss 0.504047155380249\n","[Training Epoch 2] Batch 522, Loss 0.5246282815933228\n","[Training Epoch 2] Batch 523, Loss 0.4809355139732361\n","[Training Epoch 2] Batch 524, Loss 0.5060613751411438\n","[Training Epoch 2] Batch 525, Loss 0.4940887689590454\n","[Training Epoch 2] Batch 526, Loss 0.5005038976669312\n","[Training Epoch 2] Batch 527, Loss 0.48756515979766846\n","[Training Epoch 2] Batch 528, Loss 0.5220566987991333\n","[Training Epoch 2] Batch 529, Loss 0.5151094794273376\n","[Training Epoch 2] Batch 530, Loss 0.49228906631469727\n","[Training Epoch 2] Batch 531, Loss 0.4744897484779358\n","[Training Epoch 2] Batch 532, Loss 0.49951231479644775\n","[Training Epoch 2] Batch 533, Loss 0.5143526196479797\n","[Training Epoch 2] Batch 534, Loss 0.5025830864906311\n","[Training Epoch 2] Batch 535, Loss 0.5123603343963623\n","[Training Epoch 2] Batch 536, Loss 0.4955912232398987\n","[Training Epoch 2] Batch 537, Loss 0.5272566080093384\n","[Training Epoch 2] Batch 538, Loss 0.46765002608299255\n","[Training Epoch 2] Batch 539, Loss 0.5300065279006958\n","[Training Epoch 2] Batch 540, Loss 0.5039221048355103\n","[Training Epoch 2] Batch 541, Loss 0.5180854797363281\n","[Training Epoch 2] Batch 542, Loss 0.4802459180355072\n","[Training Epoch 2] Batch 543, Loss 0.5059617757797241\n","[Training Epoch 2] Batch 544, Loss 0.46302688121795654\n","[Training Epoch 2] Batch 545, Loss 0.5233197212219238\n","[Training Epoch 2] Batch 546, Loss 0.5097396969795227\n","[Training Epoch 2] Batch 547, Loss 0.49899059534072876\n","[Training Epoch 2] Batch 548, Loss 0.47105085849761963\n","[Training Epoch 2] Batch 549, Loss 0.5152084827423096\n","[Training Epoch 2] Batch 550, Loss 0.47625717520713806\n","[Training Epoch 2] Batch 551, Loss 0.4698273241519928\n","[Training Epoch 2] Batch 552, Loss 0.5084247589111328\n","[Training Epoch 2] Batch 553, Loss 0.4961874485015869\n","[Training Epoch 2] Batch 554, Loss 0.5417617559432983\n","[Training Epoch 2] Batch 555, Loss 0.492011159658432\n","[Training Epoch 2] Batch 556, Loss 0.5105428695678711\n","[Training Epoch 2] Batch 557, Loss 0.5218580961227417\n","[Training Epoch 2] Batch 558, Loss 0.5006777048110962\n","[Training Epoch 2] Batch 559, Loss 0.5199096202850342\n","[Training Epoch 2] Batch 560, Loss 0.49263638257980347\n","[Training Epoch 2] Batch 561, Loss 0.47775551676750183\n","[Training Epoch 2] Batch 562, Loss 0.5342466831207275\n","[Training Epoch 2] Batch 563, Loss 0.5208474397659302\n","[Training Epoch 2] Batch 564, Loss 0.5064409971237183\n","[Training Epoch 2] Batch 565, Loss 0.5374646782875061\n","[Training Epoch 2] Batch 566, Loss 0.5142804980278015\n","[Training Epoch 2] Batch 567, Loss 0.507025957107544\n","[Training Epoch 2] Batch 568, Loss 0.5069189071655273\n","[Training Epoch 2] Batch 569, Loss 0.49806758761405945\n","[Training Epoch 2] Batch 570, Loss 0.5076634883880615\n","[Training Epoch 2] Batch 571, Loss 0.4763312041759491\n","[Training Epoch 2] Batch 572, Loss 0.5099018812179565\n","[Training Epoch 2] Batch 573, Loss 0.4998339116573334\n","[Training Epoch 2] Batch 574, Loss 0.5281432271003723\n","[Training Epoch 2] Batch 575, Loss 0.5300887823104858\n","[Training Epoch 2] Batch 576, Loss 0.47580575942993164\n","[Training Epoch 2] Batch 577, Loss 0.47102421522140503\n","[Training Epoch 2] Batch 578, Loss 0.47585737705230713\n","[Training Epoch 2] Batch 579, Loss 0.5075390934944153\n","[Training Epoch 2] Batch 580, Loss 0.5101748704910278\n","[Training Epoch 2] Batch 581, Loss 0.4938087463378906\n","[Training Epoch 2] Batch 582, Loss 0.49004390835762024\n","[Training Epoch 2] Batch 583, Loss 0.4947109818458557\n","[Training Epoch 2] Batch 584, Loss 0.5114956498146057\n","[Training Epoch 2] Batch 585, Loss 0.4932591915130615\n","[Training Epoch 2] Batch 586, Loss 0.49866920709609985\n","[Training Epoch 2] Batch 587, Loss 0.4917323589324951\n","[Training Epoch 2] Batch 588, Loss 0.48455142974853516\n","[Training Epoch 2] Batch 589, Loss 0.4868389070034027\n","[Training Epoch 2] Batch 590, Loss 0.5129309892654419\n","[Training Epoch 2] Batch 591, Loss 0.5038340091705322\n","[Training Epoch 2] Batch 592, Loss 0.48300594091415405\n","[Training Epoch 2] Batch 593, Loss 0.5031601190567017\n","[Training Epoch 2] Batch 594, Loss 0.5194530487060547\n","[Training Epoch 2] Batch 595, Loss 0.4830377399921417\n","[Training Epoch 2] Batch 596, Loss 0.4967212975025177\n","[Training Epoch 2] Batch 597, Loss 0.5043613910675049\n","[Training Epoch 2] Batch 598, Loss 0.5155901908874512\n","[Training Epoch 2] Batch 599, Loss 0.5163077116012573\n","[Training Epoch 2] Batch 600, Loss 0.5007126331329346\n","[Training Epoch 2] Batch 601, Loss 0.4652928113937378\n","[Training Epoch 2] Batch 602, Loss 0.486436128616333\n","[Training Epoch 2] Batch 603, Loss 0.5088191628456116\n","[Training Epoch 2] Batch 604, Loss 0.5197113752365112\n","[Training Epoch 2] Batch 605, Loss 0.47864043712615967\n","[Training Epoch 2] Batch 606, Loss 0.47929078340530396\n","[Training Epoch 2] Batch 607, Loss 0.4998694658279419\n","[Training Epoch 2] Batch 608, Loss 0.4800441265106201\n","[Training Epoch 2] Batch 609, Loss 0.5136121511459351\n","[Training Epoch 2] Batch 610, Loss 0.49584653973579407\n","[Training Epoch 2] Batch 611, Loss 0.4964389204978943\n","[Training Epoch 2] Batch 612, Loss 0.47297680377960205\n","[Training Epoch 2] Batch 613, Loss 0.5102584362030029\n","[Training Epoch 2] Batch 614, Loss 0.5283627510070801\n","[Training Epoch 2] Batch 615, Loss 0.48843616247177124\n","[Training Epoch 2] Batch 616, Loss 0.5014950037002563\n","[Training Epoch 2] Batch 617, Loss 0.5005183219909668\n","[Training Epoch 2] Batch 618, Loss 0.532146155834198\n","[Training Epoch 2] Batch 619, Loss 0.5068650245666504\n","[Training Epoch 2] Batch 620, Loss 0.502575159072876\n","[Training Epoch 2] Batch 621, Loss 0.4931836426258087\n","[Training Epoch 2] Batch 622, Loss 0.4634864330291748\n","[Training Epoch 2] Batch 623, Loss 0.4887370467185974\n","[Training Epoch 2] Batch 624, Loss 0.4873313307762146\n","[Training Epoch 2] Batch 625, Loss 0.5437543392181396\n","[Training Epoch 2] Batch 626, Loss 0.5101466774940491\n","[Training Epoch 2] Batch 627, Loss 0.4947320818901062\n","[Training Epoch 2] Batch 628, Loss 0.4833899736404419\n","[Training Epoch 2] Batch 629, Loss 0.5225095152854919\n","[Training Epoch 2] Batch 630, Loss 0.504076361656189\n","[Training Epoch 2] Batch 631, Loss 0.4840949773788452\n","[Training Epoch 2] Batch 632, Loss 0.5060811042785645\n","[Training Epoch 2] Batch 633, Loss 0.5050821304321289\n","[Training Epoch 2] Batch 634, Loss 0.4941371977329254\n","[Training Epoch 2] Batch 635, Loss 0.5063925385475159\n","[Training Epoch 2] Batch 636, Loss 0.4709565043449402\n","[Training Epoch 2] Batch 637, Loss 0.46848928928375244\n","[Training Epoch 2] Batch 638, Loss 0.49606847763061523\n","[Training Epoch 2] Batch 639, Loss 0.4772552251815796\n","[Training Epoch 2] Batch 640, Loss 0.4838668704032898\n","[Training Epoch 2] Batch 641, Loss 0.5236185193061829\n","[Training Epoch 2] Batch 642, Loss 0.4902423620223999\n","[Training Epoch 2] Batch 643, Loss 0.5044885873794556\n","[Training Epoch 2] Batch 644, Loss 0.5089178085327148\n","[Training Epoch 2] Batch 645, Loss 0.48792240023612976\n","[Training Epoch 2] Batch 646, Loss 0.5099594593048096\n","[Training Epoch 2] Batch 647, Loss 0.48989129066467285\n","[Training Epoch 2] Batch 648, Loss 0.512968122959137\n","[Training Epoch 2] Batch 649, Loss 0.48580217361450195\n","[Training Epoch 2] Batch 650, Loss 0.5231170654296875\n","[Training Epoch 2] Batch 651, Loss 0.5005037784576416\n","[Training Epoch 2] Batch 652, Loss 0.4978223741054535\n","[Training Epoch 2] Batch 653, Loss 0.5010049343109131\n","[Training Epoch 2] Batch 654, Loss 0.4870947599411011\n","[Training Epoch 2] Batch 655, Loss 0.5404521226882935\n","[Training Epoch 2] Batch 656, Loss 0.50688636302948\n","[Training Epoch 2] Batch 657, Loss 0.4976491332054138\n","[Training Epoch 2] Batch 658, Loss 0.5242444276809692\n","[Training Epoch 2] Batch 659, Loss 0.5163084268569946\n","[Training Epoch 2] Batch 660, Loss 0.5054835677146912\n","[Training Epoch 2] Batch 661, Loss 0.5144756436347961\n","[Training Epoch 2] Batch 662, Loss 0.5289911031723022\n","[Training Epoch 2] Batch 663, Loss 0.47967374324798584\n","[Training Epoch 2] Batch 664, Loss 0.4929855465888977\n","[Training Epoch 2] Batch 665, Loss 0.49460527300834656\n","[Training Epoch 2] Batch 666, Loss 0.5078338384628296\n","[Training Epoch 2] Batch 667, Loss 0.5043272376060486\n","[Training Epoch 2] Batch 668, Loss 0.4983852207660675\n","[Training Epoch 2] Batch 669, Loss 0.5113080739974976\n","[Training Epoch 2] Batch 670, Loss 0.48355865478515625\n","[Training Epoch 2] Batch 671, Loss 0.5025373697280884\n","[Training Epoch 2] Batch 672, Loss 0.5141409635543823\n","[Training Epoch 2] Batch 673, Loss 0.4727765619754791\n","[Training Epoch 2] Batch 674, Loss 0.5020878314971924\n","[Training Epoch 2] Batch 675, Loss 0.4902530014514923\n","[Training Epoch 2] Batch 676, Loss 0.46510764956474304\n","[Training Epoch 2] Batch 677, Loss 0.515305757522583\n","[Training Epoch 2] Batch 678, Loss 0.5045521855354309\n","[Training Epoch 2] Batch 679, Loss 0.511067271232605\n","[Training Epoch 2] Batch 680, Loss 0.5072652697563171\n","[Training Epoch 2] Batch 681, Loss 0.48872774839401245\n","[Training Epoch 2] Batch 682, Loss 0.5024372339248657\n","[Training Epoch 2] Batch 683, Loss 0.5129351019859314\n","[Training Epoch 2] Batch 684, Loss 0.49622321128845215\n","[Training Epoch 2] Batch 685, Loss 0.48600614070892334\n","[Training Epoch 2] Batch 686, Loss 0.5163426995277405\n","[Training Epoch 2] Batch 687, Loss 0.4841253459453583\n","[Training Epoch 2] Batch 688, Loss 0.49517494440078735\n","[Training Epoch 2] Batch 689, Loss 0.484142541885376\n","[Training Epoch 2] Batch 690, Loss 0.492085337638855\n","[Training Epoch 2] Batch 691, Loss 0.5224603414535522\n","[Training Epoch 2] Batch 692, Loss 0.4971207082271576\n","[Training Epoch 2] Batch 693, Loss 0.47762683033943176\n","[Training Epoch 2] Batch 694, Loss 0.5116938948631287\n","[Training Epoch 2] Batch 695, Loss 0.506870448589325\n","[Training Epoch 2] Batch 696, Loss 0.5012820959091187\n","[Training Epoch 2] Batch 697, Loss 0.49995890259742737\n","[Training Epoch 2] Batch 698, Loss 0.5165419578552246\n","[Training Epoch 2] Batch 699, Loss 0.5094417929649353\n","[Training Epoch 2] Batch 700, Loss 0.4762692451477051\n","[Training Epoch 2] Batch 701, Loss 0.5020440816879272\n","[Training Epoch 2] Batch 702, Loss 0.49613749980926514\n","[Training Epoch 2] Batch 703, Loss 0.48538511991500854\n","[Training Epoch 2] Batch 704, Loss 0.5235604643821716\n","[Training Epoch 2] Batch 705, Loss 0.47209542989730835\n","[Training Epoch 2] Batch 706, Loss 0.5166066884994507\n","[Training Epoch 2] Batch 707, Loss 0.4931381940841675\n","[Training Epoch 2] Batch 708, Loss 0.47277867794036865\n","[Training Epoch 2] Batch 709, Loss 0.48212575912475586\n","[Training Epoch 2] Batch 710, Loss 0.5037962794303894\n","[Training Epoch 2] Batch 711, Loss 0.5227954387664795\n","[Training Epoch 2] Batch 712, Loss 0.5068633556365967\n","[Training Epoch 2] Batch 713, Loss 0.5034744739532471\n","[Training Epoch 2] Batch 714, Loss 0.4888865351676941\n","[Training Epoch 2] Batch 715, Loss 0.48893818259239197\n","[Training Epoch 2] Batch 716, Loss 0.5143612623214722\n","[Training Epoch 2] Batch 717, Loss 0.4864334464073181\n","[Training Epoch 2] Batch 718, Loss 0.4601205587387085\n","[Training Epoch 2] Batch 719, Loss 0.4970126450061798\n","[Training Epoch 2] Batch 720, Loss 0.519783079624176\n","[Training Epoch 2] Batch 721, Loss 0.4596693515777588\n","[Training Epoch 2] Batch 722, Loss 0.5033124089241028\n","[Training Epoch 2] Batch 723, Loss 0.4927763342857361\n","[Training Epoch 2] Batch 724, Loss 0.49583810567855835\n","[Training Epoch 2] Batch 725, Loss 0.5054649114608765\n","[Training Epoch 2] Batch 726, Loss 0.5016360282897949\n","[Training Epoch 2] Batch 727, Loss 0.4883230924606323\n","[Training Epoch 2] Batch 728, Loss 0.4821164608001709\n","[Training Epoch 2] Batch 729, Loss 0.5164111852645874\n","[Training Epoch 2] Batch 730, Loss 0.5047144889831543\n","[Training Epoch 2] Batch 731, Loss 0.4801948666572571\n","[Training Epoch 2] Batch 732, Loss 0.5156340599060059\n","[Training Epoch 2] Batch 733, Loss 0.49597299098968506\n","[Training Epoch 2] Batch 734, Loss 0.5019416809082031\n","[Training Epoch 2] Batch 735, Loss 0.49315524101257324\n","[Training Epoch 2] Batch 736, Loss 0.527223527431488\n","[Training Epoch 2] Batch 737, Loss 0.47353595495224\n","[Training Epoch 2] Batch 738, Loss 0.4941806197166443\n","[Training Epoch 2] Batch 739, Loss 0.4907936453819275\n","[Training Epoch 2] Batch 740, Loss 0.509128212928772\n","[Training Epoch 2] Batch 741, Loss 0.5045211315155029\n","[Training Epoch 2] Batch 742, Loss 0.5098092555999756\n","[Training Epoch 2] Batch 743, Loss 0.4945235252380371\n","[Training Epoch 2] Batch 744, Loss 0.4795011281967163\n","[Training Epoch 2] Batch 745, Loss 0.49277201294898987\n","[Training Epoch 2] Batch 746, Loss 0.5020486116409302\n","[Training Epoch 2] Batch 747, Loss 0.4942164421081543\n","[Training Epoch 2] Batch 748, Loss 0.5197190046310425\n","[Training Epoch 2] Batch 749, Loss 0.502185046672821\n","[Training Epoch 2] Batch 750, Loss 0.46687787771224976\n","[Training Epoch 2] Batch 751, Loss 0.5111721754074097\n","[Training Epoch 2] Batch 752, Loss 0.4792456030845642\n","[Training Epoch 2] Batch 753, Loss 0.4785841405391693\n","[Training Epoch 2] Batch 754, Loss 0.5079135894775391\n","[Training Epoch 2] Batch 755, Loss 0.5344936847686768\n","[Training Epoch 2] Batch 756, Loss 0.47388750314712524\n","[Training Epoch 2] Batch 757, Loss 0.5094619989395142\n","[Training Epoch 2] Batch 758, Loss 0.4683665931224823\n","[Training Epoch 2] Batch 759, Loss 0.5192097425460815\n","[Training Epoch 2] Batch 760, Loss 0.49110615253448486\n","[Training Epoch 2] Batch 761, Loss 0.530902087688446\n","[Training Epoch 2] Batch 762, Loss 0.47054529190063477\n","[Training Epoch 2] Batch 763, Loss 0.4882551431655884\n","[Training Epoch 2] Batch 764, Loss 0.5010212063789368\n","[Training Epoch 2] Batch 765, Loss 0.483519047498703\n","[Training Epoch 2] Batch 766, Loss 0.5284433364868164\n","[Training Epoch 2] Batch 767, Loss 0.5080406665802002\n","[Training Epoch 2] Batch 768, Loss 0.4779829978942871\n","[Training Epoch 2] Batch 769, Loss 0.5030779838562012\n","[Training Epoch 2] Batch 770, Loss 0.502406656742096\n","[Training Epoch 2] Batch 771, Loss 0.4894564747810364\n","[Training Epoch 2] Batch 772, Loss 0.49053579568862915\n","[Training Epoch 2] Batch 773, Loss 0.48855385184288025\n","[Training Epoch 2] Batch 774, Loss 0.5052657127380371\n","[Training Epoch 2] Batch 775, Loss 0.5028090476989746\n","[Training Epoch 2] Batch 776, Loss 0.5226073861122131\n","[Training Epoch 2] Batch 777, Loss 0.4980373680591583\n","[Training Epoch 2] Batch 778, Loss 0.5205610990524292\n","[Training Epoch 2] Batch 779, Loss 0.5003995299339294\n","[Training Epoch 2] Batch 780, Loss 0.4681882858276367\n","[Training Epoch 2] Batch 781, Loss 0.4939526319503784\n","[Training Epoch 2] Batch 782, Loss 0.5004491806030273\n","[Training Epoch 2] Batch 783, Loss 0.47848767042160034\n","[Training Epoch 2] Batch 784, Loss 0.46361207962036133\n","[Training Epoch 2] Batch 785, Loss 0.4873697757720947\n","[Training Epoch 2] Batch 786, Loss 0.5092184543609619\n","[Training Epoch 2] Batch 787, Loss 0.476162314414978\n","[Training Epoch 2] Batch 788, Loss 0.489471971988678\n","[Training Epoch 2] Batch 789, Loss 0.49573421478271484\n","[Training Epoch 2] Batch 790, Loss 0.48263269662857056\n","[Training Epoch 2] Batch 791, Loss 0.5096741914749146\n","[Training Epoch 2] Batch 792, Loss 0.5187042951583862\n","[Training Epoch 2] Batch 793, Loss 0.488809734582901\n","[Training Epoch 2] Batch 794, Loss 0.501338541507721\n","[Training Epoch 2] Batch 795, Loss 0.5032740831375122\n","[Training Epoch 2] Batch 796, Loss 0.5151581168174744\n","[Training Epoch 2] Batch 797, Loss 0.5381647348403931\n","[Training Epoch 2] Batch 798, Loss 0.46781039237976074\n","[Training Epoch 2] Batch 799, Loss 0.5177026391029358\n","[Training Epoch 2] Batch 800, Loss 0.4781287908554077\n","[Training Epoch 2] Batch 801, Loss 0.4890732765197754\n","[Training Epoch 2] Batch 802, Loss 0.47927820682525635\n","[Training Epoch 2] Batch 803, Loss 0.4644041955471039\n","[Training Epoch 2] Batch 804, Loss 0.4978698194026947\n","[Training Epoch 2] Batch 805, Loss 0.5004021525382996\n","[Training Epoch 2] Batch 806, Loss 0.5227482914924622\n","[Training Epoch 2] Batch 807, Loss 0.4952862858772278\n","[Training Epoch 2] Batch 808, Loss 0.4946569502353668\n","[Training Epoch 2] Batch 809, Loss 0.5318984985351562\n","[Training Epoch 2] Batch 810, Loss 0.5030357837677002\n","[Training Epoch 2] Batch 811, Loss 0.5143366456031799\n","[Training Epoch 2] Batch 812, Loss 0.49528875946998596\n","[Training Epoch 2] Batch 813, Loss 0.5091173648834229\n","[Training Epoch 2] Batch 814, Loss 0.500220775604248\n","[Training Epoch 2] Batch 815, Loss 0.4989246726036072\n","[Training Epoch 2] Batch 816, Loss 0.4680232107639313\n","[Training Epoch 2] Batch 817, Loss 0.5362564325332642\n","[Training Epoch 2] Batch 818, Loss 0.48455044627189636\n","[Training Epoch 2] Batch 819, Loss 0.4963167905807495\n","[Training Epoch 2] Batch 820, Loss 0.5021005868911743\n","[Training Epoch 2] Batch 821, Loss 0.4925684928894043\n","[Training Epoch 2] Batch 822, Loss 0.5439022183418274\n","[Training Epoch 2] Batch 823, Loss 0.4691186845302582\n","[Training Epoch 2] Batch 824, Loss 0.5123527646064758\n","[Training Epoch 2] Batch 825, Loss 0.49194687604904175\n","[Training Epoch 2] Batch 826, Loss 0.5087717771530151\n","[Training Epoch 2] Batch 827, Loss 0.48218417167663574\n","[Training Epoch 2] Batch 828, Loss 0.5060721039772034\n","[Training Epoch 2] Batch 829, Loss 0.5143137574195862\n","[Training Epoch 2] Batch 830, Loss 0.48929470777511597\n","[Training Epoch 2] Batch 831, Loss 0.5056012272834778\n","[Training Epoch 2] Batch 832, Loss 0.5251592993736267\n","[Training Epoch 2] Batch 833, Loss 0.4816877841949463\n","[Training Epoch 2] Batch 834, Loss 0.5171780586242676\n","[Training Epoch 2] Batch 835, Loss 0.4971059262752533\n","[Training Epoch 2] Batch 836, Loss 0.49696671962738037\n","[Training Epoch 2] Batch 837, Loss 0.46719956398010254\n","[Training Epoch 2] Batch 838, Loss 0.4635699987411499\n","[Training Epoch 2] Batch 839, Loss 0.5236732959747314\n","[Training Epoch 2] Batch 840, Loss 0.4902876913547516\n","[Training Epoch 2] Batch 841, Loss 0.5319944620132446\n","[Training Epoch 2] Batch 842, Loss 0.5073755979537964\n","[Training Epoch 2] Batch 843, Loss 0.4806763231754303\n","[Training Epoch 2] Batch 844, Loss 0.47184354066848755\n","[Training Epoch 2] Batch 845, Loss 0.5074498653411865\n","[Training Epoch 2] Batch 846, Loss 0.4830579459667206\n","[Training Epoch 2] Batch 847, Loss 0.5265853404998779\n","[Training Epoch 2] Batch 848, Loss 0.5273227691650391\n","[Training Epoch 2] Batch 849, Loss 0.5062547922134399\n","[Training Epoch 2] Batch 850, Loss 0.5073317289352417\n","[Training Epoch 2] Batch 851, Loss 0.48790284991264343\n","[Training Epoch 2] Batch 852, Loss 0.5023177862167358\n","[Training Epoch 2] Batch 853, Loss 0.49419745802879333\n","[Training Epoch 2] Batch 854, Loss 0.49317559599876404\n","[Training Epoch 2] Batch 855, Loss 0.47948044538497925\n","[Training Epoch 2] Batch 856, Loss 0.5078136324882507\n","[Training Epoch 2] Batch 857, Loss 0.48614320158958435\n","[Training Epoch 2] Batch 858, Loss 0.4977025091648102\n","[Training Epoch 2] Batch 859, Loss 0.5309999585151672\n","[Training Epoch 2] Batch 860, Loss 0.5095808506011963\n","[Training Epoch 2] Batch 861, Loss 0.48161596059799194\n","[Training Epoch 2] Batch 862, Loss 0.48207569122314453\n","[Training Epoch 2] Batch 863, Loss 0.5080559253692627\n","[Training Epoch 2] Batch 864, Loss 0.49321189522743225\n","[Training Epoch 2] Batch 865, Loss 0.49622905254364014\n","[Training Epoch 2] Batch 866, Loss 0.499308705329895\n","[Training Epoch 2] Batch 867, Loss 0.5099180936813354\n","[Training Epoch 2] Batch 868, Loss 0.49064090847969055\n","[Training Epoch 2] Batch 869, Loss 0.520865797996521\n","[Training Epoch 2] Batch 870, Loss 0.5093517899513245\n","[Training Epoch 2] Batch 871, Loss 0.4662264585494995\n","[Training Epoch 2] Batch 872, Loss 0.4874151945114136\n","[Training Epoch 2] Batch 873, Loss 0.48621711134910583\n","[Training Epoch 2] Batch 874, Loss 0.5101932883262634\n","[Training Epoch 2] Batch 875, Loss 0.49762392044067383\n","[Training Epoch 2] Batch 876, Loss 0.503044843673706\n","[Training Epoch 2] Batch 877, Loss 0.5259115695953369\n","[Training Epoch 2] Batch 878, Loss 0.524125337600708\n","[Training Epoch 2] Batch 879, Loss 0.47090116143226624\n","[Training Epoch 2] Batch 880, Loss 0.5341262817382812\n","[Training Epoch 2] Batch 881, Loss 0.47917744517326355\n","[Training Epoch 2] Batch 882, Loss 0.45949244499206543\n","[Training Epoch 2] Batch 883, Loss 0.49622756242752075\n","[Training Epoch 2] Batch 884, Loss 0.5051809549331665\n","[Training Epoch 2] Batch 885, Loss 0.5030050277709961\n","[Training Epoch 2] Batch 886, Loss 0.4862239360809326\n","[Training Epoch 2] Batch 887, Loss 0.4857747554779053\n","[Training Epoch 2] Batch 888, Loss 0.46879899501800537\n","[Training Epoch 2] Batch 889, Loss 0.5189891457557678\n","[Training Epoch 2] Batch 890, Loss 0.48012977838516235\n","[Training Epoch 2] Batch 891, Loss 0.4950193464756012\n","[Training Epoch 2] Batch 892, Loss 0.48421722650527954\n","[Training Epoch 2] Batch 893, Loss 0.46992355585098267\n","[Training Epoch 2] Batch 894, Loss 0.5191197991371155\n","[Training Epoch 2] Batch 895, Loss 0.46906375885009766\n","[Training Epoch 2] Batch 896, Loss 0.4794348180294037\n","[Training Epoch 2] Batch 897, Loss 0.46304190158843994\n","[Training Epoch 2] Batch 898, Loss 0.4821457862854004\n","[Training Epoch 2] Batch 899, Loss 0.4841906428337097\n","[Training Epoch 2] Batch 900, Loss 0.5108040571212769\n","[Training Epoch 2] Batch 901, Loss 0.5189307928085327\n","[Training Epoch 2] Batch 902, Loss 0.5176668763160706\n","[Training Epoch 2] Batch 903, Loss 0.5117268562316895\n","[Training Epoch 2] Batch 904, Loss 0.5025262236595154\n","[Training Epoch 2] Batch 905, Loss 0.5310403108596802\n","[Training Epoch 2] Batch 906, Loss 0.509901225566864\n","[Training Epoch 2] Batch 907, Loss 0.49257218837738037\n","[Training Epoch 2] Batch 908, Loss 0.5109134912490845\n","[Training Epoch 2] Batch 909, Loss 0.5046932697296143\n","[Training Epoch 2] Batch 910, Loss 0.4869621992111206\n","[Training Epoch 2] Batch 911, Loss 0.46923118829727173\n","[Training Epoch 2] Batch 912, Loss 0.48037856817245483\n","[Training Epoch 2] Batch 913, Loss 0.49516037106513977\n","[Training Epoch 2] Batch 914, Loss 0.5349254608154297\n","[Training Epoch 2] Batch 915, Loss 0.5118899941444397\n","[Training Epoch 2] Batch 916, Loss 0.522546648979187\n","[Training Epoch 2] Batch 917, Loss 0.4993540048599243\n","[Training Epoch 2] Batch 918, Loss 0.49828076362609863\n","[Training Epoch 2] Batch 919, Loss 0.486217737197876\n","[Training Epoch 2] Batch 920, Loss 0.48310208320617676\n","[Training Epoch 2] Batch 921, Loss 0.4869592487812042\n","[Training Epoch 2] Batch 922, Loss 0.47662967443466187\n","[Training Epoch 2] Batch 923, Loss 0.4980487823486328\n","[Training Epoch 2] Batch 924, Loss 0.5216048955917358\n","[Training Epoch 2] Batch 925, Loss 0.5143426060676575\n","[Training Epoch 2] Batch 926, Loss 0.4871792197227478\n","[Training Epoch 2] Batch 927, Loss 0.5164827108383179\n","[Training Epoch 2] Batch 928, Loss 0.48119768500328064\n","[Training Epoch 2] Batch 929, Loss 0.5101163387298584\n","[Training Epoch 2] Batch 930, Loss 0.48687803745269775\n","[Training Epoch 2] Batch 931, Loss 0.4873926639556885\n","[Training Epoch 2] Batch 932, Loss 0.483617901802063\n","[Training Epoch 2] Batch 933, Loss 0.48915228247642517\n","[Training Epoch 2] Batch 934, Loss 0.47952571511268616\n","[Training Epoch 2] Batch 935, Loss 0.5161264538764954\n","[Training Epoch 2] Batch 936, Loss 0.4660690128803253\n","[Training Epoch 2] Batch 937, Loss 0.48871883749961853\n","[Training Epoch 2] Batch 938, Loss 0.512467086315155\n","[Training Epoch 2] Batch 939, Loss 0.504410982131958\n","[Training Epoch 2] Batch 940, Loss 0.48897603154182434\n","[Training Epoch 2] Batch 941, Loss 0.5037697553634644\n","[Training Epoch 2] Batch 942, Loss 0.47669491171836853\n","[Training Epoch 2] Batch 943, Loss 0.5131275653839111\n","[Training Epoch 2] Batch 944, Loss 0.5220280885696411\n","[Training Epoch 2] Batch 945, Loss 0.4795844852924347\n","[Training Epoch 2] Batch 946, Loss 0.4804171323776245\n","[Training Epoch 2] Batch 947, Loss 0.50016850233078\n","[Training Epoch 2] Batch 948, Loss 0.4858132004737854\n","[Training Epoch 2] Batch 949, Loss 0.5083338618278503\n","[Training Epoch 2] Batch 950, Loss 0.5219920873641968\n","[Training Epoch 2] Batch 951, Loss 0.5342047810554504\n","[Training Epoch 2] Batch 952, Loss 0.5161137580871582\n","[Training Epoch 2] Batch 953, Loss 0.5076602697372437\n","[Training Epoch 2] Batch 954, Loss 0.48528406023979187\n","[Training Epoch 2] Batch 955, Loss 0.5114587545394897\n","[Training Epoch 2] Batch 956, Loss 0.500679612159729\n","[Training Epoch 2] Batch 957, Loss 0.49583542346954346\n","[Training Epoch 2] Batch 958, Loss 0.5103549957275391\n","[Training Epoch 2] Batch 959, Loss 0.49681025743484497\n","[Training Epoch 2] Batch 960, Loss 0.4750332236289978\n","[Training Epoch 2] Batch 961, Loss 0.524897038936615\n","[Training Epoch 2] Batch 962, Loss 0.48789387941360474\n","[Training Epoch 2] Batch 963, Loss 0.5108535289764404\n","[Training Epoch 2] Batch 964, Loss 0.467804491519928\n","[Training Epoch 2] Batch 965, Loss 0.4773886799812317\n","[Training Epoch 2] Batch 966, Loss 0.48472753167152405\n","[Training Epoch 2] Batch 967, Loss 0.49819254875183105\n","[Training Epoch 2] Batch 968, Loss 0.4904119372367859\n","[Training Epoch 2] Batch 969, Loss 0.5005737543106079\n","[Training Epoch 2] Batch 970, Loss 0.4744676351547241\n","[Training Epoch 2] Batch 971, Loss 0.4700820744037628\n","[Training Epoch 2] Batch 972, Loss 0.5066663026809692\n","[Training Epoch 2] Batch 973, Loss 0.48371750116348267\n","[Training Epoch 2] Batch 974, Loss 0.5073738098144531\n","[Training Epoch 2] Batch 975, Loss 0.5119043588638306\n","[Training Epoch 2] Batch 976, Loss 0.5093469619750977\n","[Training Epoch 2] Batch 977, Loss 0.5432469844818115\n","[Training Epoch 2] Batch 978, Loss 0.510635256767273\n","[Training Epoch 2] Batch 979, Loss 0.47698405385017395\n","[Training Epoch 2] Batch 980, Loss 0.49869805574417114\n","[Training Epoch 2] Batch 981, Loss 0.48260757327079773\n","[Training Epoch 2] Batch 982, Loss 0.4864341914653778\n","[Training Epoch 2] Batch 983, Loss 0.5027925968170166\n","[Training Epoch 2] Batch 984, Loss 0.49979689717292786\n","[Training Epoch 2] Batch 985, Loss 0.5046870708465576\n","[Training Epoch 2] Batch 986, Loss 0.4838031530380249\n","[Training Epoch 2] Batch 987, Loss 0.49911296367645264\n","[Training Epoch 2] Batch 988, Loss 0.5280102491378784\n","[Training Epoch 2] Batch 989, Loss 0.5086263418197632\n","[Training Epoch 2] Batch 990, Loss 0.4830542802810669\n","[Training Epoch 2] Batch 991, Loss 0.4986591935157776\n","[Training Epoch 2] Batch 992, Loss 0.5425540208816528\n","[Training Epoch 2] Batch 993, Loss 0.4823056757450104\n","[Training Epoch 2] Batch 994, Loss 0.5043399333953857\n","[Training Epoch 2] Batch 995, Loss 0.4817943274974823\n","[Training Epoch 2] Batch 996, Loss 0.5297824144363403\n","[Training Epoch 2] Batch 997, Loss 0.5113883018493652\n","[Training Epoch 2] Batch 998, Loss 0.5071071982383728\n","[Training Epoch 2] Batch 999, Loss 0.5313726663589478\n","[Training Epoch 2] Batch 1000, Loss 0.5231663584709167\n","[Training Epoch 2] Batch 1001, Loss 0.48837798833847046\n","[Training Epoch 2] Batch 1002, Loss 0.516731858253479\n","[Training Epoch 2] Batch 1003, Loss 0.5203032493591309\n","[Training Epoch 2] Batch 1004, Loss 0.5061300992965698\n","[Training Epoch 2] Batch 1005, Loss 0.5184361338615417\n","[Training Epoch 2] Batch 1006, Loss 0.5369867086410522\n","[Training Epoch 2] Batch 1007, Loss 0.520927906036377\n","[Training Epoch 2] Batch 1008, Loss 0.48274338245391846\n","[Training Epoch 2] Batch 1009, Loss 0.5177549719810486\n","[Training Epoch 2] Batch 1010, Loss 0.5349663496017456\n","[Training Epoch 2] Batch 1011, Loss 0.5109596848487854\n","[Training Epoch 2] Batch 1012, Loss 0.4810318350791931\n","[Training Epoch 2] Batch 1013, Loss 0.4950791895389557\n","[Training Epoch 2] Batch 1014, Loss 0.4965215027332306\n","[Training Epoch 2] Batch 1015, Loss 0.4887210726737976\n","[Training Epoch 2] Batch 1016, Loss 0.513161301612854\n","[Training Epoch 2] Batch 1017, Loss 0.5191903710365295\n","[Training Epoch 2] Batch 1018, Loss 0.5115132927894592\n","[Training Epoch 2] Batch 1019, Loss 0.48403817415237427\n","[Training Epoch 2] Batch 1020, Loss 0.4824736416339874\n","[Training Epoch 2] Batch 1021, Loss 0.5003431439399719\n","[Training Epoch 2] Batch 1022, Loss 0.49489912390708923\n","[Training Epoch 2] Batch 1023, Loss 0.48691636323928833\n","[Training Epoch 2] Batch 1024, Loss 0.5026483535766602\n","[Training Epoch 2] Batch 1025, Loss 0.5042965412139893\n","[Training Epoch 2] Batch 1026, Loss 0.5000544786453247\n","[Training Epoch 2] Batch 1027, Loss 0.529168963432312\n","[Training Epoch 2] Batch 1028, Loss 0.5052879452705383\n","[Training Epoch 2] Batch 1029, Loss 0.5224220156669617\n","[Training Epoch 2] Batch 1030, Loss 0.4944482445716858\n","[Training Epoch 2] Batch 1031, Loss 0.48299580812454224\n","[Training Epoch 2] Batch 1032, Loss 0.524251401424408\n","[Training Epoch 2] Batch 1033, Loss 0.5225000381469727\n","[Training Epoch 2] Batch 1034, Loss 0.48901981115341187\n","[Training Epoch 2] Batch 1035, Loss 0.5115642547607422\n","[Training Epoch 2] Batch 1036, Loss 0.5031339526176453\n","[Training Epoch 2] Batch 1037, Loss 0.49117234349250793\n","[Training Epoch 2] Batch 1038, Loss 0.4865095615386963\n","[Training Epoch 2] Batch 1039, Loss 0.4610742926597595\n","[Training Epoch 2] Batch 1040, Loss 0.5451290607452393\n","[Training Epoch 2] Batch 1041, Loss 0.500640869140625\n","[Training Epoch 2] Batch 1042, Loss 0.5003923177719116\n","[Training Epoch 2] Batch 1043, Loss 0.47552603483200073\n","[Training Epoch 2] Batch 1044, Loss 0.520835280418396\n","[Training Epoch 2] Batch 1045, Loss 0.5061309337615967\n","[Training Epoch 2] Batch 1046, Loss 0.504144549369812\n","[Training Epoch 2] Batch 1047, Loss 0.48168033361434937\n","[Training Epoch 2] Batch 1048, Loss 0.47948479652404785\n","[Training Epoch 2] Batch 1049, Loss 0.482866108417511\n","[Training Epoch 2] Batch 1050, Loss 0.5072711110115051\n","[Training Epoch 2] Batch 1051, Loss 0.4998120367527008\n","[Training Epoch 2] Batch 1052, Loss 0.49203377962112427\n","[Training Epoch 2] Batch 1053, Loss 0.4796936810016632\n","[Training Epoch 2] Batch 1054, Loss 0.481522798538208\n","[Training Epoch 2] Batch 1055, Loss 0.5008600950241089\n","[Training Epoch 2] Batch 1056, Loss 0.4626540541648865\n","[Training Epoch 2] Batch 1057, Loss 0.4827011823654175\n","[Training Epoch 2] Batch 1058, Loss 0.4839751124382019\n","[Training Epoch 2] Batch 1059, Loss 0.49642109870910645\n","[Training Epoch 2] Batch 1060, Loss 0.5098335146903992\n","[Training Epoch 2] Batch 1061, Loss 0.5046733617782593\n","[Training Epoch 2] Batch 1062, Loss 0.4978071451187134\n","[Training Epoch 2] Batch 1063, Loss 0.4996238648891449\n","[Training Epoch 2] Batch 1064, Loss 0.48649027943611145\n","[Training Epoch 2] Batch 1065, Loss 0.5143778324127197\n","[Training Epoch 2] Batch 1066, Loss 0.5002700686454773\n","[Training Epoch 2] Batch 1067, Loss 0.48761171102523804\n","[Training Epoch 2] Batch 1068, Loss 0.47271010279655457\n","[Training Epoch 2] Batch 1069, Loss 0.5106158256530762\n","[Training Epoch 2] Batch 1070, Loss 0.5308278203010559\n","[Training Epoch 2] Batch 1071, Loss 0.48999446630477905\n","[Training Epoch 2] Batch 1072, Loss 0.4960584342479706\n","[Training Epoch 2] Batch 1073, Loss 0.48874279856681824\n","[Training Epoch 2] Batch 1074, Loss 0.49347078800201416\n","[Training Epoch 2] Batch 1075, Loss 0.5115203857421875\n","[Training Epoch 2] Batch 1076, Loss 0.49847501516342163\n","[Training Epoch 2] Batch 1077, Loss 0.5008944272994995\n","[Training Epoch 2] Batch 1078, Loss 0.5238965749740601\n","[Training Epoch 2] Batch 1079, Loss 0.4961066246032715\n","[Training Epoch 2] Batch 1080, Loss 0.517236590385437\n","[Training Epoch 2] Batch 1081, Loss 0.4787319302558899\n","[Training Epoch 2] Batch 1082, Loss 0.4801596403121948\n","[Training Epoch 2] Batch 1083, Loss 0.5029059648513794\n","[Training Epoch 2] Batch 1084, Loss 0.45733708143234253\n","[Training Epoch 2] Batch 1085, Loss 0.4717087149620056\n","[Training Epoch 2] Batch 1086, Loss 0.5071195363998413\n","[Training Epoch 2] Batch 1087, Loss 0.5106288194656372\n","[Training Epoch 2] Batch 1088, Loss 0.5005805492401123\n","[Training Epoch 2] Batch 1089, Loss 0.5000854730606079\n","[Training Epoch 2] Batch 1090, Loss 0.5241900086402893\n","[Training Epoch 2] Batch 1091, Loss 0.49751904606819153\n","[Training Epoch 2] Batch 1092, Loss 0.4893830120563507\n","[Training Epoch 2] Batch 1093, Loss 0.5151440501213074\n","[Training Epoch 2] Batch 1094, Loss 0.4840833842754364\n","[Training Epoch 2] Batch 1095, Loss 0.4763695299625397\n","[Training Epoch 2] Batch 1096, Loss 0.48860692977905273\n","[Training Epoch 2] Batch 1097, Loss 0.5041311979293823\n","[Training Epoch 2] Batch 1098, Loss 0.45828643441200256\n","[Training Epoch 2] Batch 1099, Loss 0.5047211647033691\n","[Training Epoch 2] Batch 1100, Loss 0.4862939715385437\n","[Training Epoch 2] Batch 1101, Loss 0.4757152795791626\n","[Training Epoch 2] Batch 1102, Loss 0.5047178864479065\n","[Training Epoch 2] Batch 1103, Loss 0.48858845233917236\n","[Training Epoch 2] Batch 1104, Loss 0.49445009231567383\n","[Training Epoch 2] Batch 1105, Loss 0.5120888352394104\n","[Training Epoch 2] Batch 1106, Loss 0.4973292350769043\n","[Training Epoch 2] Batch 1107, Loss 0.5044363737106323\n","[Training Epoch 2] Batch 1108, Loss 0.49204158782958984\n","[Training Epoch 2] Batch 1109, Loss 0.5033477544784546\n","[Training Epoch 2] Batch 1110, Loss 0.4968051314353943\n","[Training Epoch 2] Batch 1111, Loss 0.4948030710220337\n","[Training Epoch 2] Batch 1112, Loss 0.5161769390106201\n","[Training Epoch 2] Batch 1113, Loss 0.4732975959777832\n","[Training Epoch 2] Batch 1114, Loss 0.49989110231399536\n","[Training Epoch 2] Batch 1115, Loss 0.5268711447715759\n","[Training Epoch 2] Batch 1116, Loss 0.4984712302684784\n","[Training Epoch 2] Batch 1117, Loss 0.5076292753219604\n","[Training Epoch 2] Batch 1118, Loss 0.5026395320892334\n","[Training Epoch 2] Batch 1119, Loss 0.4988512396812439\n","[Training Epoch 2] Batch 1120, Loss 0.476043164730072\n","[Training Epoch 2] Batch 1121, Loss 0.4605610966682434\n","[Training Epoch 2] Batch 1122, Loss 0.5077736377716064\n","[Training Epoch 2] Batch 1123, Loss 0.5073379278182983\n","[Training Epoch 2] Batch 1124, Loss 0.4919682741165161\n","[Training Epoch 2] Batch 1125, Loss 0.4962514042854309\n","[Training Epoch 2] Batch 1126, Loss 0.4753555655479431\n","[Training Epoch 2] Batch 1127, Loss 0.5218703746795654\n","[Training Epoch 2] Batch 1128, Loss 0.5438835024833679\n","[Training Epoch 2] Batch 1129, Loss 0.47095319628715515\n","[Training Epoch 2] Batch 1130, Loss 0.4988269805908203\n","[Training Epoch 2] Batch 1131, Loss 0.5051917433738708\n","[Training Epoch 2] Batch 1132, Loss 0.5048088431358337\n","[Training Epoch 2] Batch 1133, Loss 0.4715227782726288\n","[Training Epoch 2] Batch 1134, Loss 0.5077756643295288\n","[Training Epoch 2] Batch 1135, Loss 0.453164279460907\n","[Training Epoch 2] Batch 1136, Loss 0.49459031224250793\n","[Training Epoch 2] Batch 1137, Loss 0.4539964199066162\n","[Training Epoch 2] Batch 1138, Loss 0.4787159562110901\n","[Training Epoch 2] Batch 1139, Loss 0.517271101474762\n","[Training Epoch 2] Batch 1140, Loss 0.5111696720123291\n","[Training Epoch 2] Batch 1141, Loss 0.46437177062034607\n","[Training Epoch 2] Batch 1142, Loss 0.5268351435661316\n","[Training Epoch 2] Batch 1143, Loss 0.5161796808242798\n","[Training Epoch 2] Batch 1144, Loss 0.4845256209373474\n","[Training Epoch 2] Batch 1145, Loss 0.5003992319107056\n","[Training Epoch 2] Batch 1146, Loss 0.49619680643081665\n","[Training Epoch 2] Batch 1147, Loss 0.48259007930755615\n","[Training Epoch 2] Batch 1148, Loss 0.4944622218608856\n","[Training Epoch 2] Batch 1149, Loss 0.5107104778289795\n","[Training Epoch 2] Batch 1150, Loss 0.4979953169822693\n","[Training Epoch 2] Batch 1151, Loss 0.4833400547504425\n","[Training Epoch 2] Batch 1152, Loss 0.49043604731559753\n","[Training Epoch 2] Batch 1153, Loss 0.4820600152015686\n","[Training Epoch 2] Batch 1154, Loss 0.4918586015701294\n","[Training Epoch 2] Batch 1155, Loss 0.5039580464363098\n","[Training Epoch 2] Batch 1156, Loss 0.5205473899841309\n","[Training Epoch 2] Batch 1157, Loss 0.48509490489959717\n","[Training Epoch 2] Batch 1158, Loss 0.5136030912399292\n","[Training Epoch 2] Batch 1159, Loss 0.4910889267921448\n","[Training Epoch 2] Batch 1160, Loss 0.5038349628448486\n","[Training Epoch 2] Batch 1161, Loss 0.5093446969985962\n","[Training Epoch 2] Batch 1162, Loss 0.5070444941520691\n","[Training Epoch 2] Batch 1163, Loss 0.5001824498176575\n","[Training Epoch 2] Batch 1164, Loss 0.5025497078895569\n","[Training Epoch 2] Batch 1165, Loss 0.5180184841156006\n","[Training Epoch 2] Batch 1166, Loss 0.5066321492195129\n","[Training Epoch 2] Batch 1167, Loss 0.4937930703163147\n","[Training Epoch 2] Batch 1168, Loss 0.49687904119491577\n","[Training Epoch 2] Batch 1169, Loss 0.5101561546325684\n","[Training Epoch 2] Batch 1170, Loss 0.48166149854660034\n","[Training Epoch 2] Batch 1171, Loss 0.49709728360176086\n","[Training Epoch 2] Batch 1172, Loss 0.50401771068573\n","[Training Epoch 2] Batch 1173, Loss 0.5017126798629761\n","[Training Epoch 2] Batch 1174, Loss 0.5050991773605347\n","[Training Epoch 2] Batch 1175, Loss 0.48614323139190674\n","[Training Epoch 2] Batch 1176, Loss 0.5248914957046509\n","[Training Epoch 2] Batch 1177, Loss 0.5139356255531311\n","[Training Epoch 2] Batch 1178, Loss 0.5142199993133545\n","[Training Epoch 2] Batch 1179, Loss 0.5189764499664307\n","[Training Epoch 2] Batch 1180, Loss 0.5087627172470093\n","[Training Epoch 2] Batch 1181, Loss 0.4936850666999817\n","[Training Epoch 2] Batch 1182, Loss 0.49507448077201843\n","[Training Epoch 2] Batch 1183, Loss 0.473174512386322\n","[Training Epoch 2] Batch 1184, Loss 0.5008846521377563\n","[Training Epoch 2] Batch 1185, Loss 0.4701579213142395\n","[Training Epoch 2] Batch 1186, Loss 0.5247307419776917\n","[Training Epoch 2] Batch 1187, Loss 0.5003305077552795\n","[Training Epoch 2] Batch 1188, Loss 0.45385122299194336\n","[Training Epoch 2] Batch 1189, Loss 0.5024925470352173\n","[Training Epoch 2] Batch 1190, Loss 0.4641433358192444\n","[Training Epoch 2] Batch 1191, Loss 0.5107991695404053\n","[Training Epoch 2] Batch 1192, Loss 0.5115232467651367\n","[Training Epoch 2] Batch 1193, Loss 0.5016785860061646\n","[Training Epoch 2] Batch 1194, Loss 0.4881274700164795\n","[Training Epoch 2] Batch 1195, Loss 0.4795943796634674\n","[Training Epoch 2] Batch 1196, Loss 0.5162038803100586\n","[Training Epoch 2] Batch 1197, Loss 0.5035049915313721\n","[Training Epoch 2] Batch 1198, Loss 0.497608482837677\n","[Training Epoch 2] Batch 1199, Loss 0.4970877766609192\n","[Training Epoch 2] Batch 1200, Loss 0.47674429416656494\n","[Training Epoch 2] Batch 1201, Loss 0.502306342124939\n","[Training Epoch 2] Batch 1202, Loss 0.49855005741119385\n","[Training Epoch 2] Batch 1203, Loss 0.5036134719848633\n","[Training Epoch 2] Batch 1204, Loss 0.48519474267959595\n","[Training Epoch 2] Batch 1205, Loss 0.47514253854751587\n","[Training Epoch 2] Batch 1206, Loss 0.5061379671096802\n","[Training Epoch 2] Batch 1207, Loss 0.47475388646125793\n","[Training Epoch 2] Batch 1208, Loss 0.5073834657669067\n","[Training Epoch 2] Batch 1209, Loss 0.5088819265365601\n","[Training Epoch 2] Batch 1210, Loss 0.5089544057846069\n","[Training Epoch 2] Batch 1211, Loss 0.5032215118408203\n","[Training Epoch 2] Batch 1212, Loss 0.5191137790679932\n","[Training Epoch 2] Batch 1213, Loss 0.4805857241153717\n","[Training Epoch 2] Batch 1214, Loss 0.48982253670692444\n","[Training Epoch 2] Batch 1215, Loss 0.5078599452972412\n","[Training Epoch 2] Batch 1216, Loss 0.5179305672645569\n","[Training Epoch 2] Batch 1217, Loss 0.48131728172302246\n","[Training Epoch 2] Batch 1218, Loss 0.5093415379524231\n","[Training Epoch 2] Batch 1219, Loss 0.518366277217865\n","[Training Epoch 2] Batch 1220, Loss 0.5037418603897095\n","[Training Epoch 2] Batch 1221, Loss 0.5197519659996033\n","[Training Epoch 2] Batch 1222, Loss 0.49726223945617676\n","[Training Epoch 2] Batch 1223, Loss 0.48963165283203125\n","[Training Epoch 2] Batch 1224, Loss 0.4796401858329773\n","[Training Epoch 2] Batch 1225, Loss 0.4982890486717224\n","[Training Epoch 2] Batch 1226, Loss 0.5032429695129395\n","[Training Epoch 2] Batch 1227, Loss 0.47186851501464844\n","[Training Epoch 2] Batch 1228, Loss 0.4544792175292969\n","[Training Epoch 2] Batch 1229, Loss 0.5320041179656982\n","[Training Epoch 2] Batch 1230, Loss 0.48455941677093506\n","[Training Epoch 2] Batch 1231, Loss 0.5124709606170654\n","[Training Epoch 2] Batch 1232, Loss 0.4721754491329193\n","[Training Epoch 2] Batch 1233, Loss 0.5117875337600708\n","[Training Epoch 2] Batch 1234, Loss 0.5462957620620728\n","[Training Epoch 2] Batch 1235, Loss 0.4856278896331787\n","[Training Epoch 2] Batch 1236, Loss 0.45620274543762207\n","[Training Epoch 2] Batch 1237, Loss 0.48872119188308716\n","[Training Epoch 2] Batch 1238, Loss 0.5007842779159546\n","[Training Epoch 2] Batch 1239, Loss 0.48542141914367676\n","[Training Epoch 2] Batch 1240, Loss 0.46038782596588135\n","[Training Epoch 2] Batch 1241, Loss 0.5230021476745605\n","[Training Epoch 2] Batch 1242, Loss 0.47441112995147705\n","[Training Epoch 2] Batch 1243, Loss 0.49414801597595215\n","[Training Epoch 2] Batch 1244, Loss 0.5210625529289246\n","[Training Epoch 2] Batch 1245, Loss 0.4981951117515564\n","[Training Epoch 2] Batch 1246, Loss 0.5005013942718506\n","[Training Epoch 2] Batch 1247, Loss 0.5205239057540894\n","[Training Epoch 2] Batch 1248, Loss 0.4956331253051758\n","[Training Epoch 2] Batch 1249, Loss 0.4929159879684448\n","[Training Epoch 2] Batch 1250, Loss 0.47448378801345825\n","[Training Epoch 2] Batch 1251, Loss 0.5199708938598633\n","[Training Epoch 2] Batch 1252, Loss 0.5051926374435425\n","[Training Epoch 2] Batch 1253, Loss 0.514346182346344\n","[Training Epoch 2] Batch 1254, Loss 0.5056376457214355\n","[Training Epoch 2] Batch 1255, Loss 0.48457783460617065\n","[Training Epoch 2] Batch 1256, Loss 0.4843682646751404\n","[Training Epoch 2] Batch 1257, Loss 0.49799907207489014\n","[Training Epoch 2] Batch 1258, Loss 0.49393701553344727\n","[Training Epoch 2] Batch 1259, Loss 0.5064260363578796\n","[Training Epoch 2] Batch 1260, Loss 0.48379868268966675\n","[Training Epoch 2] Batch 1261, Loss 0.5134183168411255\n","[Training Epoch 2] Batch 1262, Loss 0.5035792589187622\n","[Training Epoch 2] Batch 1263, Loss 0.490556925535202\n","[Training Epoch 2] Batch 1264, Loss 0.4921133816242218\n","[Training Epoch 2] Batch 1265, Loss 0.5125759840011597\n","[Training Epoch 2] Batch 1266, Loss 0.5432704091072083\n","[Training Epoch 2] Batch 1267, Loss 0.5130299925804138\n","[Training Epoch 2] Batch 1268, Loss 0.5256120562553406\n","[Training Epoch 2] Batch 1269, Loss 0.4781624972820282\n","[Training Epoch 2] Batch 1270, Loss 0.5105593204498291\n","[Training Epoch 2] Batch 1271, Loss 0.5068672895431519\n","[Training Epoch 2] Batch 1272, Loss 0.5049092173576355\n","[Training Epoch 2] Batch 1273, Loss 0.4874645173549652\n","[Training Epoch 2] Batch 1274, Loss 0.5212948322296143\n","[Training Epoch 2] Batch 1275, Loss 0.5067456960678101\n","[Training Epoch 2] Batch 1276, Loss 0.5285809636116028\n","[Training Epoch 2] Batch 1277, Loss 0.48498696088790894\n","[Training Epoch 2] Batch 1278, Loss 0.512771487236023\n","[Training Epoch 2] Batch 1279, Loss 0.5093754529953003\n","[Training Epoch 2] Batch 1280, Loss 0.5129204392433167\n","[Training Epoch 2] Batch 1281, Loss 0.5223405361175537\n","[Training Epoch 2] Batch 1282, Loss 0.5300232172012329\n","[Training Epoch 2] Batch 1283, Loss 0.4944554567337036\n","[Training Epoch 2] Batch 1284, Loss 0.4907429814338684\n","[Training Epoch 2] Batch 1285, Loss 0.5134354829788208\n","[Training Epoch 2] Batch 1286, Loss 0.5126498937606812\n","[Training Epoch 2] Batch 1287, Loss 0.5002884864807129\n","[Training Epoch 2] Batch 1288, Loss 0.4859273135662079\n","[Training Epoch 2] Batch 1289, Loss 0.49202170968055725\n","[Training Epoch 2] Batch 1290, Loss 0.503183901309967\n","[Training Epoch 2] Batch 1291, Loss 0.4936007261276245\n","[Training Epoch 2] Batch 1292, Loss 0.4869672656059265\n","[Training Epoch 2] Batch 1293, Loss 0.4943908452987671\n","[Training Epoch 2] Batch 1294, Loss 0.511760950088501\n","[Training Epoch 2] Batch 1295, Loss 0.48481690883636475\n","[Training Epoch 2] Batch 1296, Loss 0.5184254050254822\n","[Training Epoch 2] Batch 1297, Loss 0.49098819494247437\n","[Training Epoch 2] Batch 1298, Loss 0.468891441822052\n","[Training Epoch 2] Batch 1299, Loss 0.48065102100372314\n","[Training Epoch 2] Batch 1300, Loss 0.47797319293022156\n","[Training Epoch 2] Batch 1301, Loss 0.5065358281135559\n","[Training Epoch 2] Batch 1302, Loss 0.4777947664260864\n","[Training Epoch 2] Batch 1303, Loss 0.513548731803894\n","[Training Epoch 2] Batch 1304, Loss 0.49527615308761597\n","[Training Epoch 2] Batch 1305, Loss 0.4817063808441162\n","[Training Epoch 2] Batch 1306, Loss 0.48127949237823486\n","[Training Epoch 2] Batch 1307, Loss 0.5111687183380127\n","[Training Epoch 2] Batch 1308, Loss 0.5059834718704224\n","[Training Epoch 2] Batch 1309, Loss 0.4898863434791565\n","[Training Epoch 2] Batch 1310, Loss 0.4847792387008667\n","[Training Epoch 2] Batch 1311, Loss 0.49871826171875\n","[Training Epoch 2] Batch 1312, Loss 0.4904441833496094\n","[Training Epoch 2] Batch 1313, Loss 0.48253166675567627\n","[Training Epoch 2] Batch 1314, Loss 0.5105839371681213\n","[Training Epoch 2] Batch 1315, Loss 0.48823660612106323\n","[Training Epoch 2] Batch 1316, Loss 0.5013214349746704\n","[Training Epoch 2] Batch 1317, Loss 0.5211102366447449\n","[Training Epoch 2] Batch 1318, Loss 0.4997013211250305\n","[Training Epoch 2] Batch 1319, Loss 0.475205659866333\n","[Training Epoch 2] Batch 1320, Loss 0.4840744733810425\n","[Training Epoch 2] Batch 1321, Loss 0.4879874885082245\n","[Training Epoch 2] Batch 1322, Loss 0.5176738500595093\n","[Training Epoch 2] Batch 1323, Loss 0.46387943625450134\n","[Training Epoch 2] Batch 1324, Loss 0.4719225764274597\n","[Training Epoch 2] Batch 1325, Loss 0.49517112970352173\n","[Training Epoch 2] Batch 1326, Loss 0.46940433979034424\n","[Training Epoch 2] Batch 1327, Loss 0.4885231852531433\n","[Training Epoch 2] Batch 1328, Loss 0.48206189274787903\n","[Training Epoch 2] Batch 1329, Loss 0.5096830129623413\n","[Training Epoch 2] Batch 1330, Loss 0.4891943335533142\n","[Training Epoch 2] Batch 1331, Loss 0.4961787164211273\n","[Training Epoch 2] Batch 1332, Loss 0.5067557096481323\n","[Training Epoch 2] Batch 1333, Loss 0.5005309581756592\n","[Training Epoch 2] Batch 1334, Loss 0.5229175090789795\n","[Training Epoch 2] Batch 1335, Loss 0.4521765112876892\n","[Training Epoch 2] Batch 1336, Loss 0.495199978351593\n","[Training Epoch 2] Batch 1337, Loss 0.5006702542304993\n","[Training Epoch 2] Batch 1338, Loss 0.4933171272277832\n","[Training Epoch 2] Batch 1339, Loss 0.4976266026496887\n","[Training Epoch 2] Batch 1340, Loss 0.4707590639591217\n","[Training Epoch 2] Batch 1341, Loss 0.5255091190338135\n","[Training Epoch 2] Batch 1342, Loss 0.5061039924621582\n","[Training Epoch 2] Batch 1343, Loss 0.4881780743598938\n","[Training Epoch 2] Batch 1344, Loss 0.5228618383407593\n","[Training Epoch 2] Batch 1345, Loss 0.4934844672679901\n","[Training Epoch 2] Batch 1346, Loss 0.5136803984642029\n","[Training Epoch 2] Batch 1347, Loss 0.4965847432613373\n","[Training Epoch 2] Batch 1348, Loss 0.47563356161117554\n","[Training Epoch 2] Batch 1349, Loss 0.47720351815223694\n","[Training Epoch 2] Batch 1350, Loss 0.4787103533744812\n","[Training Epoch 2] Batch 1351, Loss 0.5185757279396057\n","[Training Epoch 2] Batch 1352, Loss 0.5171358585357666\n","[Training Epoch 2] Batch 1353, Loss 0.5025105476379395\n","[Training Epoch 2] Batch 1354, Loss 0.5381971597671509\n","[Training Epoch 2] Batch 1355, Loss 0.499550998210907\n","[Training Epoch 2] Batch 1356, Loss 0.503167986869812\n","[Training Epoch 2] Batch 1357, Loss 0.5063208341598511\n","[Training Epoch 2] Batch 1358, Loss 0.488506942987442\n","[Training Epoch 2] Batch 1359, Loss 0.48542726039886475\n","[Training Epoch 2] Batch 1360, Loss 0.48496583104133606\n","[Training Epoch 2] Batch 1361, Loss 0.49352675676345825\n","[Training Epoch 2] Batch 1362, Loss 0.48243221640586853\n","[Training Epoch 2] Batch 1363, Loss 0.5185695290565491\n","[Training Epoch 2] Batch 1364, Loss 0.5230467915534973\n","[Training Epoch 2] Batch 1365, Loss 0.4783656895160675\n","[Training Epoch 2] Batch 1366, Loss 0.47939345240592957\n","[Training Epoch 2] Batch 1367, Loss 0.5029057860374451\n","[Training Epoch 2] Batch 1368, Loss 0.5009845495223999\n","[Training Epoch 2] Batch 1369, Loss 0.5051130056381226\n","[Training Epoch 2] Batch 1370, Loss 0.5171254873275757\n","[Training Epoch 2] Batch 1371, Loss 0.5231717228889465\n","[Training Epoch 2] Batch 1372, Loss 0.5139776468276978\n","[Training Epoch 2] Batch 1373, Loss 0.48143303394317627\n","[Training Epoch 2] Batch 1374, Loss 0.5006855726242065\n","[Training Epoch 2] Batch 1375, Loss 0.5102041363716125\n","[Training Epoch 2] Batch 1376, Loss 0.4918561279773712\n","[Training Epoch 2] Batch 1377, Loss 0.5126255750656128\n","[Training Epoch 2] Batch 1378, Loss 0.5018771886825562\n","[Training Epoch 2] Batch 1379, Loss 0.5024230480194092\n","[Training Epoch 2] Batch 1380, Loss 0.5225664377212524\n","[Training Epoch 2] Batch 1381, Loss 0.47125136852264404\n","[Training Epoch 2] Batch 1382, Loss 0.47041356563568115\n","[Training Epoch 2] Batch 1383, Loss 0.4933008551597595\n","[Training Epoch 2] Batch 1384, Loss 0.5336164236068726\n","[Training Epoch 2] Batch 1385, Loss 0.4829370379447937\n","[Training Epoch 2] Batch 1386, Loss 0.5004197359085083\n","[Training Epoch 2] Batch 1387, Loss 0.4887496829032898\n","[Training Epoch 2] Batch 1388, Loss 0.5002219676971436\n","[Training Epoch 2] Batch 1389, Loss 0.515810489654541\n","[Training Epoch 2] Batch 1390, Loss 0.49382245540618896\n","[Training Epoch 2] Batch 1391, Loss 0.5097007155418396\n","[Training Epoch 2] Batch 1392, Loss 0.4965771436691284\n","[Training Epoch 2] Batch 1393, Loss 0.5023477077484131\n","[Training Epoch 2] Batch 1394, Loss 0.5226343274116516\n","[Training Epoch 2] Batch 1395, Loss 0.4712149202823639\n","[Training Epoch 2] Batch 1396, Loss 0.495969295501709\n","[Training Epoch 2] Batch 1397, Loss 0.48295891284942627\n","[Training Epoch 2] Batch 1398, Loss 0.4981782138347626\n","[Training Epoch 2] Batch 1399, Loss 0.4858424663543701\n","[Training Epoch 2] Batch 1400, Loss 0.45696893334388733\n","[Training Epoch 2] Batch 1401, Loss 0.5155307054519653\n","[Training Epoch 2] Batch 1402, Loss 0.5143414735794067\n","[Training Epoch 2] Batch 1403, Loss 0.4925149083137512\n","[Training Epoch 2] Batch 1404, Loss 0.5064629316329956\n","[Training Epoch 2] Batch 1405, Loss 0.4920191764831543\n","[Training Epoch 2] Batch 1406, Loss 0.49611616134643555\n","[Training Epoch 2] Batch 1407, Loss 0.48737332224845886\n","[Training Epoch 2] Batch 1408, Loss 0.5026993751525879\n","[Training Epoch 2] Batch 1409, Loss 0.4720411002635956\n","[Training Epoch 2] Batch 1410, Loss 0.4877350330352783\n","[Training Epoch 2] Batch 1411, Loss 0.48214730620384216\n","[Training Epoch 2] Batch 1412, Loss 0.5006752014160156\n","[Training Epoch 2] Batch 1413, Loss 0.48590272665023804\n","[Training Epoch 2] Batch 1414, Loss 0.4923803210258484\n","[Training Epoch 2] Batch 1415, Loss 0.4815634489059448\n","[Training Epoch 2] Batch 1416, Loss 0.52808678150177\n","[Training Epoch 2] Batch 1417, Loss 0.4676239490509033\n","[Training Epoch 2] Batch 1418, Loss 0.49717527627944946\n","[Training Epoch 2] Batch 1419, Loss 0.4924614727497101\n","[Training Epoch 2] Batch 1420, Loss 0.5107586979866028\n","[Training Epoch 2] Batch 1421, Loss 0.5170520544052124\n","[Training Epoch 2] Batch 1422, Loss 0.4779757261276245\n","[Training Epoch 2] Batch 1423, Loss 0.502668023109436\n","[Training Epoch 2] Batch 1424, Loss 0.5095969438552856\n","[Training Epoch 2] Batch 1425, Loss 0.48445820808410645\n","[Training Epoch 2] Batch 1426, Loss 0.5147004127502441\n","[Training Epoch 2] Batch 1427, Loss 0.5057257413864136\n","[Training Epoch 2] Batch 1428, Loss 0.4864310622215271\n","[Training Epoch 2] Batch 1429, Loss 0.5074138641357422\n","[Training Epoch 2] Batch 1430, Loss 0.49802419543266296\n","[Training Epoch 2] Batch 1431, Loss 0.4779035151004791\n","[Training Epoch 2] Batch 1432, Loss 0.5139480829238892\n","[Training Epoch 2] Batch 1433, Loss 0.5026525259017944\n","[Training Epoch 2] Batch 1434, Loss 0.5003060698509216\n","[Training Epoch 2] Batch 1435, Loss 0.5054100751876831\n","[Training Epoch 2] Batch 1436, Loss 0.4738413691520691\n","[Training Epoch 2] Batch 1437, Loss 0.4803226590156555\n","[Training Epoch 2] Batch 1438, Loss 0.5046245455741882\n","[Training Epoch 2] Batch 1439, Loss 0.5377515554428101\n","[Training Epoch 2] Batch 1440, Loss 0.49036774039268494\n","[Training Epoch 2] Batch 1441, Loss 0.5139898061752319\n","[Training Epoch 2] Batch 1442, Loss 0.5135948657989502\n","[Training Epoch 2] Batch 1443, Loss 0.48370397090911865\n","[Training Epoch 2] Batch 1444, Loss 0.4804399013519287\n","[Training Epoch 2] Batch 1445, Loss 0.510848879814148\n","[Training Epoch 2] Batch 1446, Loss 0.4814518094062805\n","[Training Epoch 2] Batch 1447, Loss 0.5088902711868286\n","[Training Epoch 2] Batch 1448, Loss 0.5146209001541138\n","[Training Epoch 2] Batch 1449, Loss 0.4778810739517212\n","[Training Epoch 2] Batch 1450, Loss 0.5193945169448853\n","[Training Epoch 2] Batch 1451, Loss 0.5178754329681396\n","[Training Epoch 2] Batch 1452, Loss 0.5097771883010864\n","[Training Epoch 2] Batch 1453, Loss 0.49982213973999023\n","[Training Epoch 2] Batch 1454, Loss 0.5028667449951172\n","[Training Epoch 2] Batch 1455, Loss 0.4916711449623108\n","[Training Epoch 2] Batch 1456, Loss 0.5145827531814575\n","[Training Epoch 2] Batch 1457, Loss 0.492794930934906\n","[Training Epoch 2] Batch 1458, Loss 0.48571544885635376\n","[Training Epoch 2] Batch 1459, Loss 0.5195105075836182\n","[Training Epoch 2] Batch 1460, Loss 0.5113987922668457\n","[Training Epoch 2] Batch 1461, Loss 0.48543253540992737\n","[Training Epoch 2] Batch 1462, Loss 0.49940425157546997\n","[Training Epoch 2] Batch 1463, Loss 0.4686152935028076\n","[Training Epoch 2] Batch 1464, Loss 0.5023989081382751\n","[Training Epoch 2] Batch 1465, Loss 0.5133110284805298\n","[Training Epoch 2] Batch 1466, Loss 0.4800134599208832\n","[Training Epoch 2] Batch 1467, Loss 0.5135575532913208\n","[Training Epoch 2] Batch 1468, Loss 0.5063250064849854\n","[Training Epoch 2] Batch 1469, Loss 0.5216865539550781\n","[Training Epoch 2] Batch 1470, Loss 0.5042757987976074\n","[Training Epoch 2] Batch 1471, Loss 0.4970306158065796\n","[Training Epoch 2] Batch 1472, Loss 0.48532959818840027\n","[Training Epoch 2] Batch 1473, Loss 0.5037920475006104\n","[Training Epoch 2] Batch 1474, Loss 0.46918362379074097\n","[Training Epoch 2] Batch 1475, Loss 0.5241423845291138\n","[Training Epoch 2] Batch 1476, Loss 0.46246838569641113\n","[Training Epoch 2] Batch 1477, Loss 0.46826228499412537\n","[Training Epoch 2] Batch 1478, Loss 0.5110738277435303\n","[Training Epoch 2] Batch 1479, Loss 0.4935005307197571\n","[Training Epoch 2] Batch 1480, Loss 0.4705786406993866\n","[Training Epoch 2] Batch 1481, Loss 0.4832703173160553\n","[Training Epoch 2] Batch 1482, Loss 0.5209689140319824\n","[Training Epoch 2] Batch 1483, Loss 0.5007234811782837\n","[Training Epoch 2] Batch 1484, Loss 0.5114960670471191\n","[Training Epoch 2] Batch 1485, Loss 0.5224665999412537\n","[Training Epoch 2] Batch 1486, Loss 0.5138692855834961\n","[Training Epoch 2] Batch 1487, Loss 0.48965197801589966\n","[Training Epoch 2] Batch 1488, Loss 0.48755958676338196\n","[Training Epoch 2] Batch 1489, Loss 0.5106558799743652\n","[Training Epoch 2] Batch 1490, Loss 0.4922081232070923\n","[Training Epoch 2] Batch 1491, Loss 0.49984636902809143\n","[Training Epoch 2] Batch 1492, Loss 0.5015977621078491\n","[Training Epoch 2] Batch 1493, Loss 0.5050640106201172\n","[Training Epoch 2] Batch 1494, Loss 0.46322697401046753\n","[Training Epoch 2] Batch 1495, Loss 0.5224717855453491\n","[Training Epoch 2] Batch 1496, Loss 0.47750771045684814\n","[Training Epoch 2] Batch 1497, Loss 0.49092990159988403\n","[Training Epoch 2] Batch 1498, Loss 0.5129423141479492\n","[Training Epoch 2] Batch 1499, Loss 0.5099043846130371\n","[Training Epoch 2] Batch 1500, Loss 0.46843230724334717\n","[Training Epoch 2] Batch 1501, Loss 0.4893905520439148\n","[Training Epoch 2] Batch 1502, Loss 0.5442893505096436\n","[Training Epoch 2] Batch 1503, Loss 0.46808090806007385\n","[Training Epoch 2] Batch 1504, Loss 0.4897007346153259\n","[Training Epoch 2] Batch 1505, Loss 0.4973123073577881\n","[Training Epoch 2] Batch 1506, Loss 0.49646079540252686\n","[Training Epoch 2] Batch 1507, Loss 0.48255959153175354\n","[Training Epoch 2] Batch 1508, Loss 0.5031651258468628\n","[Training Epoch 2] Batch 1509, Loss 0.5231871604919434\n","[Training Epoch 2] Batch 1510, Loss 0.45517343282699585\n","[Training Epoch 2] Batch 1511, Loss 0.49666744470596313\n","[Training Epoch 2] Batch 1512, Loss 0.5011591911315918\n","[Training Epoch 2] Batch 1513, Loss 0.5050525069236755\n","[Training Epoch 2] Batch 1514, Loss 0.5199844837188721\n","[Training Epoch 2] Batch 1515, Loss 0.5014255046844482\n","[Training Epoch 2] Batch 1516, Loss 0.5034000873565674\n","[Training Epoch 2] Batch 1517, Loss 0.511172890663147\n","[Training Epoch 2] Batch 1518, Loss 0.5002923011779785\n","[Training Epoch 2] Batch 1519, Loss 0.5016133785247803\n","[Training Epoch 2] Batch 1520, Loss 0.5221719145774841\n","[Training Epoch 2] Batch 1521, Loss 0.5216179490089417\n","[Training Epoch 2] Batch 1522, Loss 0.498261034488678\n","[Training Epoch 2] Batch 1523, Loss 0.47757619619369507\n","[Training Epoch 2] Batch 1524, Loss 0.5350998640060425\n","[Training Epoch 2] Batch 1525, Loss 0.48918288946151733\n","[Training Epoch 2] Batch 1526, Loss 0.4907505512237549\n","[Training Epoch 2] Batch 1527, Loss 0.4954231083393097\n","[Training Epoch 2] Batch 1528, Loss 0.4864400625228882\n","[Training Epoch 2] Batch 1529, Loss 0.47950834035873413\n","[Training Epoch 2] Batch 1530, Loss 0.48978593945503235\n","[Training Epoch 2] Batch 1531, Loss 0.5038126111030579\n","[Training Epoch 2] Batch 1532, Loss 0.5098270177841187\n","[Training Epoch 2] Batch 1533, Loss 0.478119432926178\n","[Training Epoch 2] Batch 1534, Loss 0.5045036673545837\n","[Training Epoch 2] Batch 1535, Loss 0.4802328944206238\n","[Training Epoch 2] Batch 1536, Loss 0.4955246150493622\n","[Training Epoch 2] Batch 1537, Loss 0.5076143145561218\n","[Training Epoch 2] Batch 1538, Loss 0.4959856867790222\n","[Training Epoch 2] Batch 1539, Loss 0.4898216724395752\n","[Training Epoch 2] Batch 1540, Loss 0.4856930077075958\n","[Training Epoch 2] Batch 1541, Loss 0.5101439952850342\n","[Training Epoch 2] Batch 1542, Loss 0.49900102615356445\n","[Training Epoch 2] Batch 1543, Loss 0.49352991580963135\n","[Training Epoch 2] Batch 1544, Loss 0.513219952583313\n","[Training Epoch 2] Batch 1545, Loss 0.5031415224075317\n","[Training Epoch 2] Batch 1546, Loss 0.4896218776702881\n","[Training Epoch 2] Batch 1547, Loss 0.4900355041027069\n","[Training Epoch 2] Batch 1548, Loss 0.5163227319717407\n","[Training Epoch 2] Batch 1549, Loss 0.49591076374053955\n","[Training Epoch 2] Batch 1550, Loss 0.4823206663131714\n","[Training Epoch 2] Batch 1551, Loss 0.5351043939590454\n","[Training Epoch 2] Batch 1552, Loss 0.493804007768631\n","[Training Epoch 2] Batch 1553, Loss 0.5019785165786743\n","[Training Epoch 2] Batch 1554, Loss 0.5099833607673645\n","[Training Epoch 2] Batch 1555, Loss 0.49957776069641113\n","[Training Epoch 2] Batch 1556, Loss 0.5091731548309326\n","[Training Epoch 2] Batch 1557, Loss 0.5354786515235901\n","[Training Epoch 2] Batch 1558, Loss 0.5179105401039124\n","[Training Epoch 2] Batch 1559, Loss 0.4811380207538605\n","[Training Epoch 2] Batch 1560, Loss 0.49143093824386597\n","[Training Epoch 2] Batch 1561, Loss 0.5399452447891235\n","[Training Epoch 2] Batch 1562, Loss 0.5234361886978149\n","[Training Epoch 2] Batch 1563, Loss 0.4950087070465088\n","[Training Epoch 2] Batch 1564, Loss 0.44930389523506165\n","[Training Epoch 2] Batch 1565, Loss 0.4928373098373413\n","[Training Epoch 2] Batch 1566, Loss 0.4836391806602478\n","[Training Epoch 2] Batch 1567, Loss 0.49042460322380066\n","[Training Epoch 2] Batch 1568, Loss 0.5075293183326721\n","[Training Epoch 2] Batch 1569, Loss 0.4692115783691406\n","[Training Epoch 2] Batch 1570, Loss 0.46279177069664\n","[Training Epoch 2] Batch 1571, Loss 0.4708492159843445\n","[Training Epoch 2] Batch 1572, Loss 0.4865230917930603\n","[Training Epoch 2] Batch 1573, Loss 0.5197652578353882\n","[Training Epoch 2] Batch 1574, Loss 0.4771844446659088\n","[Training Epoch 2] Batch 1575, Loss 0.5097009539604187\n","[Training Epoch 2] Batch 1576, Loss 0.48649173974990845\n","[Training Epoch 2] Batch 1577, Loss 0.5074319839477539\n","[Training Epoch 2] Batch 1578, Loss 0.4889121651649475\n","[Training Epoch 2] Batch 1579, Loss 0.509067952632904\n","[Training Epoch 2] Batch 1580, Loss 0.5056342482566833\n","[Training Epoch 2] Batch 1581, Loss 0.4962148368358612\n","[Training Epoch 2] Batch 1582, Loss 0.5008561015129089\n","[Training Epoch 2] Batch 1583, Loss 0.5223358273506165\n","[Training Epoch 2] Batch 1584, Loss 0.5195060968399048\n","[Training Epoch 2] Batch 1585, Loss 0.5048683285713196\n","[Training Epoch 2] Batch 1586, Loss 0.5111132860183716\n","[Training Epoch 2] Batch 1587, Loss 0.4859858453273773\n","[Training Epoch 2] Batch 1588, Loss 0.4797512888908386\n","[Training Epoch 2] Batch 1589, Loss 0.4961439371109009\n","[Training Epoch 2] Batch 1590, Loss 0.5140486359596252\n","[Training Epoch 2] Batch 1591, Loss 0.5032737255096436\n","[Training Epoch 2] Batch 1592, Loss 0.4850524067878723\n","[Training Epoch 2] Batch 1593, Loss 0.4566316306591034\n","[Training Epoch 2] Batch 1594, Loss 0.5085204243659973\n","[Training Epoch 2] Batch 1595, Loss 0.48050451278686523\n","[Training Epoch 2] Batch 1596, Loss 0.5139753818511963\n","[Training Epoch 2] Batch 1597, Loss 0.46787798404693604\n","[Training Epoch 2] Batch 1598, Loss 0.48221540451049805\n","[Training Epoch 2] Batch 1599, Loss 0.46756869554519653\n","[Training Epoch 2] Batch 1600, Loss 0.48140713572502136\n","[Training Epoch 2] Batch 1601, Loss 0.4974687993526459\n","[Training Epoch 2] Batch 1602, Loss 0.49170705676078796\n","[Training Epoch 2] Batch 1603, Loss 0.5200039148330688\n","[Training Epoch 2] Batch 1604, Loss 0.50963294506073\n","[Training Epoch 2] Batch 1605, Loss 0.5157909393310547\n","[Training Epoch 2] Batch 1606, Loss 0.5211660861968994\n","[Training Epoch 2] Batch 1607, Loss 0.5128715634346008\n","[Training Epoch 2] Batch 1608, Loss 0.49779582023620605\n","[Training Epoch 2] Batch 1609, Loss 0.4962128698825836\n","[Training Epoch 2] Batch 1610, Loss 0.5119985342025757\n","[Training Epoch 2] Batch 1611, Loss 0.5143164396286011\n","[Training Epoch 2] Batch 1612, Loss 0.5185643434524536\n","[Training Epoch 2] Batch 1613, Loss 0.4631582498550415\n","[Training Epoch 2] Batch 1614, Loss 0.4778389036655426\n","[Training Epoch 2] Batch 1615, Loss 0.47882887721061707\n","[Training Epoch 2] Batch 1616, Loss 0.5055112838745117\n","[Training Epoch 2] Batch 1617, Loss 0.4975835680961609\n","[Training Epoch 2] Batch 1618, Loss 0.4948762059211731\n","[Training Epoch 2] Batch 1619, Loss 0.477353572845459\n","[Training Epoch 2] Batch 1620, Loss 0.49754536151885986\n","[Training Epoch 2] Batch 1621, Loss 0.4949032664299011\n","[Training Epoch 2] Batch 1622, Loss 0.5146289467811584\n","[Training Epoch 2] Batch 1623, Loss 0.4974602460861206\n","[Training Epoch 2] Batch 1624, Loss 0.4900512993335724\n","[Training Epoch 2] Batch 1625, Loss 0.46586549282073975\n","[Training Epoch 2] Batch 1626, Loss 0.477540522813797\n","[Training Epoch 2] Batch 1627, Loss 0.499418705701828\n","[Training Epoch 2] Batch 1628, Loss 0.503115713596344\n","[Training Epoch 2] Batch 1629, Loss 0.5184012651443481\n","[Training Epoch 2] Batch 1630, Loss 0.48699814081192017\n","[Training Epoch 2] Batch 1631, Loss 0.4850493371486664\n","[Training Epoch 2] Batch 1632, Loss 0.510050892829895\n","[Training Epoch 2] Batch 1633, Loss 0.49177175760269165\n","[Training Epoch 2] Batch 1634, Loss 0.5086308121681213\n","[Training Epoch 2] Batch 1635, Loss 0.5233898758888245\n","[Training Epoch 2] Batch 1636, Loss 0.47820091247558594\n","[Training Epoch 2] Batch 1637, Loss 0.5077784061431885\n","[Training Epoch 2] Batch 1638, Loss 0.47932618856430054\n","[Training Epoch 2] Batch 1639, Loss 0.4920959770679474\n","[Training Epoch 2] Batch 1640, Loss 0.45132434368133545\n","[Training Epoch 2] Batch 1641, Loss 0.501585841178894\n","[Training Epoch 2] Batch 1642, Loss 0.5021438598632812\n","[Training Epoch 2] Batch 1643, Loss 0.5056201219558716\n","[Training Epoch 2] Batch 1644, Loss 0.4853731691837311\n","[Training Epoch 2] Batch 1645, Loss 0.47524720430374146\n","[Training Epoch 2] Batch 1646, Loss 0.47605764865875244\n","[Training Epoch 2] Batch 1647, Loss 0.5187929272651672\n","[Training Epoch 2] Batch 1648, Loss 0.4952062964439392\n","[Training Epoch 2] Batch 1649, Loss 0.5078939199447632\n","[Training Epoch 2] Batch 1650, Loss 0.4856587052345276\n","[Training Epoch 2] Batch 1651, Loss 0.500235915184021\n","[Training Epoch 2] Batch 1652, Loss 0.458284854888916\n","[Training Epoch 2] Batch 1653, Loss 0.5065625905990601\n","[Training Epoch 2] Batch 1654, Loss 0.4986361563205719\n","[Training Epoch 2] Batch 1655, Loss 0.4856610894203186\n","[Training Epoch 2] Batch 1656, Loss 0.5188878178596497\n","[Training Epoch 2] Batch 1657, Loss 0.4729209244251251\n","[Training Epoch 2] Batch 1658, Loss 0.5204387307167053\n","[Training Epoch 2] Batch 1659, Loss 0.4857802391052246\n","[Training Epoch 2] Batch 1660, Loss 0.4964149594306946\n","[Training Epoch 2] Batch 1661, Loss 0.4888734817504883\n","[Training Epoch 2] Batch 1662, Loss 0.4873720407485962\n","[Training Epoch 2] Batch 1663, Loss 0.5112618803977966\n","[Training Epoch 2] Batch 1664, Loss 0.5160893797874451\n","[Training Epoch 2] Batch 1665, Loss 0.5339343547821045\n","[Training Epoch 2] Batch 1666, Loss 0.4846099615097046\n","[Training Epoch 2] Batch 1667, Loss 0.48763352632522583\n","[Training Epoch 2] Batch 1668, Loss 0.4923655688762665\n","[Training Epoch 2] Batch 1669, Loss 0.49980610609054565\n","[Training Epoch 2] Batch 1670, Loss 0.4961349070072174\n","[Training Epoch 2] Batch 1671, Loss 0.49474406242370605\n","[Training Epoch 2] Batch 1672, Loss 0.5117853879928589\n","[Training Epoch 2] Batch 1673, Loss 0.5008355379104614\n","[Training Epoch 2] Batch 1674, Loss 0.509084165096283\n","[Training Epoch 2] Batch 1675, Loss 0.5001398324966431\n","[Training Epoch 2] Batch 1676, Loss 0.4967919886112213\n","[Training Epoch 2] Batch 1677, Loss 0.49284037947654724\n","[Training Epoch 2] Batch 1678, Loss 0.5065325498580933\n","[Training Epoch 2] Batch 1679, Loss 0.48892033100128174\n","[Training Epoch 2] Batch 1680, Loss 0.5039395093917847\n","[Training Epoch 2] Batch 1681, Loss 0.4713751971721649\n","[Training Epoch 2] Batch 1682, Loss 0.4845616817474365\n","[Training Epoch 2] Batch 1683, Loss 0.514466404914856\n","[Training Epoch 2] Batch 1684, Loss 0.4781809449195862\n","[Training Epoch 2] Batch 1685, Loss 0.4881554841995239\n","[Training Epoch 2] Batch 1686, Loss 0.49288302659988403\n","[Training Epoch 2] Batch 1687, Loss 0.5089131593704224\n","[Training Epoch 2] Batch 1688, Loss 0.4962393343448639\n","[Training Epoch 2] Batch 1689, Loss 0.5203482508659363\n","[Training Epoch 2] Batch 1690, Loss 0.49452340602874756\n","[Training Epoch 2] Batch 1691, Loss 0.47617107629776\n","[Training Epoch 2] Batch 1692, Loss 0.5269688367843628\n","[Training Epoch 2] Batch 1693, Loss 0.5232824087142944\n","[Training Epoch 2] Batch 1694, Loss 0.4895945191383362\n","[Training Epoch 2] Batch 1695, Loss 0.516579806804657\n","[Training Epoch 2] Batch 1696, Loss 0.4825206398963928\n","[Training Epoch 2] Batch 1697, Loss 0.4760863482952118\n","[Training Epoch 2] Batch 1698, Loss 0.485032320022583\n","[Training Epoch 2] Batch 1699, Loss 0.48990127444267273\n","[Training Epoch 2] Batch 1700, Loss 0.4820291996002197\n","[Training Epoch 2] Batch 1701, Loss 0.485726535320282\n","[Training Epoch 2] Batch 1702, Loss 0.49257490038871765\n","[Training Epoch 2] Batch 1703, Loss 0.4695691168308258\n","[Training Epoch 2] Batch 1704, Loss 0.5239847898483276\n","[Training Epoch 2] Batch 1705, Loss 0.5021389722824097\n","[Training Epoch 2] Batch 1706, Loss 0.47871068120002747\n","[Training Epoch 2] Batch 1707, Loss 0.512004554271698\n","[Training Epoch 2] Batch 1708, Loss 0.5425766110420227\n","[Training Epoch 2] Batch 1709, Loss 0.5120830535888672\n","[Training Epoch 2] Batch 1710, Loss 0.47607162594795227\n","[Training Epoch 2] Batch 1711, Loss 0.48801344633102417\n","[Training Epoch 2] Batch 1712, Loss 0.4999498724937439\n","[Training Epoch 2] Batch 1713, Loss 0.47770851850509644\n","[Training Epoch 2] Batch 1714, Loss 0.4712051749229431\n","[Training Epoch 2] Batch 1715, Loss 0.4797399044036865\n","[Training Epoch 2] Batch 1716, Loss 0.5093264579772949\n","[Training Epoch 2] Batch 1717, Loss 0.46528834104537964\n","[Training Epoch 2] Batch 1718, Loss 0.49929550290107727\n","[Training Epoch 2] Batch 1719, Loss 0.49754372239112854\n","[Training Epoch 2] Batch 1720, Loss 0.5148022770881653\n","[Training Epoch 2] Batch 1721, Loss 0.5150506496429443\n","[Training Epoch 2] Batch 1722, Loss 0.46781057119369507\n","[Training Epoch 2] Batch 1723, Loss 0.5071302652359009\n","[Training Epoch 2] Batch 1724, Loss 0.47696638107299805\n","[Training Epoch 2] Batch 1725, Loss 0.46021759510040283\n","[Training Epoch 2] Batch 1726, Loss 0.529076337814331\n","[Training Epoch 2] Batch 1727, Loss 0.5168431997299194\n","[Training Epoch 2] Batch 1728, Loss 0.4919300377368927\n","[Training Epoch 2] Batch 1729, Loss 0.5235366821289062\n","[Training Epoch 2] Batch 1730, Loss 0.4710184931755066\n","[Training Epoch 2] Batch 1731, Loss 0.4609643816947937\n","[Training Epoch 2] Batch 1732, Loss 0.46739524602890015\n","[Training Epoch 2] Batch 1733, Loss 0.5078473091125488\n","[Training Epoch 2] Batch 1734, Loss 0.502427339553833\n","[Training Epoch 2] Batch 1735, Loss 0.5043244361877441\n","[Training Epoch 2] Batch 1736, Loss 0.48634809255599976\n","[Training Epoch 2] Batch 1737, Loss 0.4754195809364319\n","[Training Epoch 2] Batch 1738, Loss 0.5080863237380981\n","[Training Epoch 2] Batch 1739, Loss 0.49922293424606323\n","[Training Epoch 2] Batch 1740, Loss 0.5255975723266602\n","[Training Epoch 2] Batch 1741, Loss 0.468303918838501\n","[Training Epoch 2] Batch 1742, Loss 0.47730571031570435\n","[Training Epoch 2] Batch 1743, Loss 0.4935643970966339\n","[Training Epoch 2] Batch 1744, Loss 0.5104356408119202\n","[Training Epoch 2] Batch 1745, Loss 0.48652130365371704\n","[Training Epoch 2] Batch 1746, Loss 0.5077849626541138\n","[Training Epoch 2] Batch 1747, Loss 0.5034959316253662\n","[Training Epoch 2] Batch 1748, Loss 0.4663774371147156\n","[Training Epoch 2] Batch 1749, Loss 0.4953951835632324\n","[Training Epoch 2] Batch 1750, Loss 0.48307156562805176\n","[Training Epoch 2] Batch 1751, Loss 0.5132644176483154\n","[Training Epoch 2] Batch 1752, Loss 0.4923431873321533\n","[Training Epoch 2] Batch 1753, Loss 0.487018346786499\n","[Training Epoch 2] Batch 1754, Loss 0.46721792221069336\n","[Training Epoch 2] Batch 1755, Loss 0.4792298674583435\n","[Training Epoch 2] Batch 1756, Loss 0.4981074929237366\n","[Training Epoch 2] Batch 1757, Loss 0.469608873128891\n","[Training Epoch 2] Batch 1758, Loss 0.5104257464408875\n","[Training Epoch 2] Batch 1759, Loss 0.501402735710144\n","[Training Epoch 2] Batch 1760, Loss 0.49414584040641785\n","[Training Epoch 2] Batch 1761, Loss 0.5071914792060852\n","[Training Epoch 2] Batch 1762, Loss 0.5217294692993164\n","[Training Epoch 2] Batch 1763, Loss 0.5087698698043823\n","[Training Epoch 2] Batch 1764, Loss 0.4518042206764221\n","[Training Epoch 2] Batch 1765, Loss 0.48412472009658813\n","[Training Epoch 2] Batch 1766, Loss 0.4966883659362793\n","[Training Epoch 2] Batch 1767, Loss 0.49420464038848877\n","[Training Epoch 2] Batch 1768, Loss 0.4988134503364563\n","[Training Epoch 2] Batch 1769, Loss 0.5181646943092346\n","[Training Epoch 2] Batch 1770, Loss 0.5065921545028687\n","[Training Epoch 2] Batch 1771, Loss 0.501013994216919\n","[Training Epoch 2] Batch 1772, Loss 0.47412773966789246\n","[Training Epoch 2] Batch 1773, Loss 0.5058164000511169\n","[Training Epoch 2] Batch 1774, Loss 0.4902750253677368\n","[Training Epoch 2] Batch 1775, Loss 0.4697796106338501\n","[Training Epoch 2] Batch 1776, Loss 0.5007871389389038\n","[Training Epoch 2] Batch 1777, Loss 0.48063164949417114\n","[Training Epoch 2] Batch 1778, Loss 0.511227548122406\n","[Training Epoch 2] Batch 1779, Loss 0.49671047925949097\n","[Training Epoch 2] Batch 1780, Loss 0.5089774131774902\n","[Training Epoch 2] Batch 1781, Loss 0.46033650636672974\n","[Training Epoch 2] Batch 1782, Loss 0.47522875666618347\n","[Training Epoch 2] Batch 1783, Loss 0.49632418155670166\n","[Training Epoch 2] Batch 1784, Loss 0.49615439772605896\n","[Training Epoch 2] Batch 1785, Loss 0.49090662598609924\n","[Training Epoch 2] Batch 1786, Loss 0.47405803203582764\n","[Training Epoch 2] Batch 1787, Loss 0.45538952946662903\n","[Training Epoch 2] Batch 1788, Loss 0.47348445653915405\n","[Training Epoch 2] Batch 1789, Loss 0.49031296372413635\n","[Training Epoch 2] Batch 1790, Loss 0.5152548551559448\n","[Training Epoch 2] Batch 1791, Loss 0.5110684037208557\n","[Training Epoch 2] Batch 1792, Loss 0.47477108240127563\n","[Training Epoch 2] Batch 1793, Loss 0.5102627873420715\n","[Training Epoch 2] Batch 1794, Loss 0.4894922077655792\n","[Training Epoch 2] Batch 1795, Loss 0.4608260691165924\n","[Training Epoch 2] Batch 1796, Loss 0.5258561372756958\n","[Training Epoch 2] Batch 1797, Loss 0.4969967007637024\n","[Training Epoch 2] Batch 1798, Loss 0.4881460666656494\n","[Training Epoch 2] Batch 1799, Loss 0.47483041882514954\n","[Training Epoch 2] Batch 1800, Loss 0.46994417905807495\n","[Training Epoch 2] Batch 1801, Loss 0.501116156578064\n","[Training Epoch 2] Batch 1802, Loss 0.503452718257904\n","[Training Epoch 2] Batch 1803, Loss 0.4965752363204956\n","[Training Epoch 2] Batch 1804, Loss 0.49687322974205017\n","[Training Epoch 2] Batch 1805, Loss 0.4898199737071991\n","[Training Epoch 2] Batch 1806, Loss 0.47534388303756714\n","[Training Epoch 2] Batch 1807, Loss 0.49770909547805786\n","[Training Epoch 2] Batch 1808, Loss 0.4865526556968689\n","[Training Epoch 2] Batch 1809, Loss 0.4975029230117798\n","[Training Epoch 2] Batch 1810, Loss 0.46814122796058655\n","[Training Epoch 2] Batch 1811, Loss 0.5360040068626404\n","[Training Epoch 2] Batch 1812, Loss 0.4923417866230011\n","[Training Epoch 2] Batch 1813, Loss 0.5055860280990601\n","[Training Epoch 2] Batch 1814, Loss 0.5219931602478027\n","[Training Epoch 2] Batch 1815, Loss 0.4881335198879242\n","[Training Epoch 2] Batch 1816, Loss 0.515842616558075\n","[Training Epoch 2] Batch 1817, Loss 0.47529539465904236\n","[Training Epoch 2] Batch 1818, Loss 0.482975572347641\n","[Training Epoch 2] Batch 1819, Loss 0.4749915301799774\n","[Training Epoch 2] Batch 1820, Loss 0.502920389175415\n","[Training Epoch 2] Batch 1821, Loss 0.48095202445983887\n","[Training Epoch 2] Batch 1822, Loss 0.48442986607551575\n","[Training Epoch 2] Batch 1823, Loss 0.4917117953300476\n","[Training Epoch 2] Batch 1824, Loss 0.5072847604751587\n","[Training Epoch 2] Batch 1825, Loss 0.5164644718170166\n","[Training Epoch 2] Batch 1826, Loss 0.48548227548599243\n","[Training Epoch 2] Batch 1827, Loss 0.47427794337272644\n","[Training Epoch 2] Batch 1828, Loss 0.48926112055778503\n","[Training Epoch 2] Batch 1829, Loss 0.4933485984802246\n","[Training Epoch 2] Batch 1830, Loss 0.5035356879234314\n","[Training Epoch 2] Batch 1831, Loss 0.5035355091094971\n","[Training Epoch 2] Batch 1832, Loss 0.5147756338119507\n","[Training Epoch 2] Batch 1833, Loss 0.5117805600166321\n","[Training Epoch 2] Batch 1834, Loss 0.4945797622203827\n","[Training Epoch 2] Batch 1835, Loss 0.48931875824928284\n","[Training Epoch 2] Batch 1836, Loss 0.4896804392337799\n","[Training Epoch 2] Batch 1837, Loss 0.4565879702568054\n","[Training Epoch 2] Batch 1838, Loss 0.4920080602169037\n","[Training Epoch 2] Batch 1839, Loss 0.4747392535209656\n","[Training Epoch 2] Batch 1840, Loss 0.49118703603744507\n","[Training Epoch 2] Batch 1841, Loss 0.5072282552719116\n","[Training Epoch 2] Batch 1842, Loss 0.4868869185447693\n","[Training Epoch 2] Batch 1843, Loss 0.4826517105102539\n","[Training Epoch 2] Batch 1844, Loss 0.4971470236778259\n","[Training Epoch 2] Batch 1845, Loss 0.5006129741668701\n","[Training Epoch 2] Batch 1846, Loss 0.46610546112060547\n","[Training Epoch 2] Batch 1847, Loss 0.4889308214187622\n","[Training Epoch 2] Batch 1848, Loss 0.4994586706161499\n","[Training Epoch 2] Batch 1849, Loss 0.5293272137641907\n","[Training Epoch 2] Batch 1850, Loss 0.5335495471954346\n","[Training Epoch 2] Batch 1851, Loss 0.5178511738777161\n","[Training Epoch 2] Batch 1852, Loss 0.4981250762939453\n","[Training Epoch 2] Batch 1853, Loss 0.5053009986877441\n","[Training Epoch 2] Batch 1854, Loss 0.5069006085395813\n","[Training Epoch 2] Batch 1855, Loss 0.5026825070381165\n","[Training Epoch 2] Batch 1856, Loss 0.49844038486480713\n","[Training Epoch 2] Batch 1857, Loss 0.5070613026618958\n","[Training Epoch 2] Batch 1858, Loss 0.48410552740097046\n","[Training Epoch 2] Batch 1859, Loss 0.5090357661247253\n","[Training Epoch 2] Batch 1860, Loss 0.5218396782875061\n","[Training Epoch 2] Batch 1861, Loss 0.5226187109947205\n","[Training Epoch 2] Batch 1862, Loss 0.5048050880432129\n","[Training Epoch 2] Batch 1863, Loss 0.4589357078075409\n","[Training Epoch 2] Batch 1864, Loss 0.5271635055541992\n","[Training Epoch 2] Batch 1865, Loss 0.4607851505279541\n","[Training Epoch 2] Batch 1866, Loss 0.4952707588672638\n","[Training Epoch 2] Batch 1867, Loss 0.49791988730430603\n","[Training Epoch 2] Batch 1868, Loss 0.4978552758693695\n","[Training Epoch 2] Batch 1869, Loss 0.4995373785495758\n","[Training Epoch 2] Batch 1870, Loss 0.4942803382873535\n","[Training Epoch 2] Batch 1871, Loss 0.48369550704956055\n","[Training Epoch 2] Batch 1872, Loss 0.4810903072357178\n","[Training Epoch 2] Batch 1873, Loss 0.4862285256385803\n","[Training Epoch 2] Batch 1874, Loss 0.4824727177619934\n","[Training Epoch 2] Batch 1875, Loss 0.4976022243499756\n","[Training Epoch 2] Batch 1876, Loss 0.5243977904319763\n","[Training Epoch 2] Batch 1877, Loss 0.49964073300361633\n","[Training Epoch 2] Batch 1878, Loss 0.4808892607688904\n","[Training Epoch 2] Batch 1879, Loss 0.5034999251365662\n","[Training Epoch 2] Batch 1880, Loss 0.5275077819824219\n","[Training Epoch 2] Batch 1881, Loss 0.4850388169288635\n","[Training Epoch 2] Batch 1882, Loss 0.5183479189872742\n","[Training Epoch 2] Batch 1883, Loss 0.48232242465019226\n","[Training Epoch 2] Batch 1884, Loss 0.5124125480651855\n","[Training Epoch 2] Batch 1885, Loss 0.48752859234809875\n","[Training Epoch 2] Batch 1886, Loss 0.510324239730835\n","[Training Epoch 2] Batch 1887, Loss 0.49734336137771606\n","[Training Epoch 2] Batch 1888, Loss 0.5216278433799744\n","[Training Epoch 2] Batch 1889, Loss 0.5190823078155518\n","[Training Epoch 2] Batch 1890, Loss 0.4889090061187744\n","[Training Epoch 2] Batch 1891, Loss 0.4827379882335663\n","[Training Epoch 2] Batch 1892, Loss 0.50384920835495\n","[Training Epoch 2] Batch 1893, Loss 0.5179769992828369\n","[Training Epoch 2] Batch 1894, Loss 0.4865410625934601\n","[Training Epoch 2] Batch 1895, Loss 0.489208459854126\n","[Training Epoch 2] Batch 1896, Loss 0.47031259536743164\n","[Training Epoch 2] Batch 1897, Loss 0.48423999547958374\n","[Training Epoch 2] Batch 1898, Loss 0.5291563272476196\n","[Training Epoch 2] Batch 1899, Loss 0.4878655672073364\n","[Training Epoch 2] Batch 1900, Loss 0.5039248466491699\n","[Training Epoch 2] Batch 1901, Loss 0.4671320617198944\n","[Training Epoch 2] Batch 1902, Loss 0.49366697669029236\n","[Training Epoch 2] Batch 1903, Loss 0.4771103262901306\n","[Training Epoch 2] Batch 1904, Loss 0.4918951988220215\n","[Training Epoch 2] Batch 1905, Loss 0.48940005898475647\n","[Training Epoch 2] Batch 1906, Loss 0.5040912628173828\n","[Training Epoch 2] Batch 1907, Loss 0.46773093938827515\n","[Training Epoch 2] Batch 1908, Loss 0.5143989324569702\n","[Training Epoch 2] Batch 1909, Loss 0.4859575629234314\n","[Training Epoch 2] Batch 1910, Loss 0.4993785619735718\n","[Training Epoch 2] Batch 1911, Loss 0.5127757787704468\n","[Training Epoch 2] Batch 1912, Loss 0.46121954917907715\n","[Training Epoch 2] Batch 1913, Loss 0.5004730224609375\n","[Training Epoch 2] Batch 1914, Loss 0.5152389407157898\n","[Training Epoch 2] Batch 1915, Loss 0.4689179062843323\n","[Training Epoch 2] Batch 1916, Loss 0.49446308612823486\n","[Training Epoch 2] Batch 1917, Loss 0.48791372776031494\n","[Training Epoch 2] Batch 1918, Loss 0.4887157380580902\n","[Training Epoch 2] Batch 1919, Loss 0.48878684639930725\n","[Training Epoch 2] Batch 1920, Loss 0.4713195562362671\n","[Training Epoch 2] Batch 1921, Loss 0.48707324266433716\n","[Training Epoch 2] Batch 1922, Loss 0.5011377930641174\n","[Training Epoch 2] Batch 1923, Loss 0.5115194320678711\n","[Training Epoch 2] Batch 1924, Loss 0.509687066078186\n","[Training Epoch 2] Batch 1925, Loss 0.47035378217697144\n","[Training Epoch 2] Batch 1926, Loss 0.4988047182559967\n","[Training Epoch 2] Batch 1927, Loss 0.4973701238632202\n","[Training Epoch 2] Batch 1928, Loss 0.5140224695205688\n","[Training Epoch 2] Batch 1929, Loss 0.5314960479736328\n","[Training Epoch 2] Batch 1930, Loss 0.4929370582103729\n","[Training Epoch 2] Batch 1931, Loss 0.49746936559677124\n","[Training Epoch 2] Batch 1932, Loss 0.5112044811248779\n","[Training Epoch 2] Batch 1933, Loss 0.49457526206970215\n","[Training Epoch 2] Batch 1934, Loss 0.5172450542449951\n","[Training Epoch 2] Batch 1935, Loss 0.5093883275985718\n","[Training Epoch 2] Batch 1936, Loss 0.5110687017440796\n","[Training Epoch 2] Batch 1937, Loss 0.4870210886001587\n","[Training Epoch 2] Batch 1938, Loss 0.4757658839225769\n","[Training Epoch 2] Batch 1939, Loss 0.4857873320579529\n","[Training Epoch 2] Batch 1940, Loss 0.47664737701416016\n","[Training Epoch 2] Batch 1941, Loss 0.5075023770332336\n","[Training Epoch 2] Batch 1942, Loss 0.48927557468414307\n","[Training Epoch 2] Batch 1943, Loss 0.48246365785598755\n","[Training Epoch 2] Batch 1944, Loss 0.49218571186065674\n","[Training Epoch 2] Batch 1945, Loss 0.489310085773468\n","[Training Epoch 2] Batch 1946, Loss 0.517133891582489\n","[Training Epoch 2] Batch 1947, Loss 0.5080621242523193\n","[Training Epoch 2] Batch 1948, Loss 0.49840253591537476\n","[Training Epoch 2] Batch 1949, Loss 0.48212680220603943\n","[Training Epoch 2] Batch 1950, Loss 0.509888768196106\n","[Training Epoch 2] Batch 1951, Loss 0.47018879652023315\n","[Training Epoch 2] Batch 1952, Loss 0.4747455418109894\n","[Training Epoch 2] Batch 1953, Loss 0.5008684396743774\n","[Training Epoch 2] Batch 1954, Loss 0.4966024160385132\n","[Training Epoch 2] Batch 1955, Loss 0.4789040982723236\n","[Training Epoch 2] Batch 1956, Loss 0.5043811798095703\n","[Training Epoch 2] Batch 1957, Loss 0.5185779333114624\n","[Training Epoch 2] Batch 1958, Loss 0.48849421739578247\n","[Training Epoch 2] Batch 1959, Loss 0.523942232131958\n","[Training Epoch 2] Batch 1960, Loss 0.501814603805542\n","[Training Epoch 2] Batch 1961, Loss 0.4952356815338135\n","[Training Epoch 2] Batch 1962, Loss 0.5112828612327576\n","[Training Epoch 2] Batch 1963, Loss 0.509463369846344\n","[Training Epoch 2] Batch 1964, Loss 0.47177809476852417\n","[Training Epoch 2] Batch 1965, Loss 0.47959864139556885\n","[Training Epoch 2] Batch 1966, Loss 0.4870491623878479\n","[Training Epoch 2] Batch 1967, Loss 0.5063886642456055\n","[Training Epoch 2] Batch 1968, Loss 0.47117704153060913\n","[Training Epoch 2] Batch 1969, Loss 0.5084150433540344\n","[Training Epoch 2] Batch 1970, Loss 0.5052364468574524\n","[Training Epoch 2] Batch 1971, Loss 0.49727708101272583\n","[Training Epoch 2] Batch 1972, Loss 0.506332278251648\n","[Training Epoch 2] Batch 1973, Loss 0.48841744661331177\n","[Training Epoch 2] Batch 1974, Loss 0.5018764734268188\n","[Training Epoch 2] Batch 1975, Loss 0.4805436432361603\n","[Training Epoch 2] Batch 1976, Loss 0.4943561553955078\n","[Training Epoch 2] Batch 1977, Loss 0.5019277334213257\n","[Training Epoch 2] Batch 1978, Loss 0.46523067355155945\n","[Training Epoch 2] Batch 1979, Loss 0.496475487947464\n","[Training Epoch 2] Batch 1980, Loss 0.4988722503185272\n","[Training Epoch 2] Batch 1981, Loss 0.4961038827896118\n","[Training Epoch 2] Batch 1982, Loss 0.46862247586250305\n","[Training Epoch 2] Batch 1983, Loss 0.5075364708900452\n","[Training Epoch 2] Batch 1984, Loss 0.49261125922203064\n","[Training Epoch 2] Batch 1985, Loss 0.5202493071556091\n","[Training Epoch 2] Batch 1986, Loss 0.49596700072288513\n","[Training Epoch 2] Batch 1987, Loss 0.4746796488761902\n","[Training Epoch 2] Batch 1988, Loss 0.5056207180023193\n","[Training Epoch 2] Batch 1989, Loss 0.5139788389205933\n","[Training Epoch 2] Batch 1990, Loss 0.49212780594825745\n","[Training Epoch 2] Batch 1991, Loss 0.5367450714111328\n","[Training Epoch 2] Batch 1992, Loss 0.5297844409942627\n","[Training Epoch 2] Batch 1993, Loss 0.49543002247810364\n","[Training Epoch 2] Batch 1994, Loss 0.49662068486213684\n","[Training Epoch 2] Batch 1995, Loss 0.48361751437187195\n","[Training Epoch 2] Batch 1996, Loss 0.4989669620990753\n","[Training Epoch 2] Batch 1997, Loss 0.4675242304801941\n","[Training Epoch 2] Batch 1998, Loss 0.4665614366531372\n","[Training Epoch 2] Batch 1999, Loss 0.5457978844642639\n","[Training Epoch 2] Batch 2000, Loss 0.511681318283081\n","[Training Epoch 2] Batch 2001, Loss 0.5002807974815369\n","[Training Epoch 2] Batch 2002, Loss 0.49924349784851074\n","[Training Epoch 2] Batch 2003, Loss 0.47544869780540466\n","[Training Epoch 2] Batch 2004, Loss 0.4963127374649048\n","[Training Epoch 2] Batch 2005, Loss 0.4785163998603821\n","[Training Epoch 2] Batch 2006, Loss 0.5031168460845947\n","[Training Epoch 2] Batch 2007, Loss 0.4928922951221466\n","[Training Epoch 2] Batch 2008, Loss 0.4626263976097107\n","[Training Epoch 2] Batch 2009, Loss 0.5051119923591614\n","[Training Epoch 2] Batch 2010, Loss 0.4586838483810425\n","[Training Epoch 2] Batch 2011, Loss 0.5263822078704834\n","[Training Epoch 2] Batch 2012, Loss 0.4703975319862366\n","[Training Epoch 2] Batch 2013, Loss 0.48194918036460876\n","[Training Epoch 2] Batch 2014, Loss 0.4974372684955597\n","[Training Epoch 2] Batch 2015, Loss 0.49965599179267883\n","[Training Epoch 2] Batch 2016, Loss 0.4690003991127014\n","[Training Epoch 2] Batch 2017, Loss 0.5190869569778442\n","[Training Epoch 2] Batch 2018, Loss 0.47387611865997314\n","[Training Epoch 2] Batch 2019, Loss 0.47475913166999817\n","[Training Epoch 2] Batch 2020, Loss 0.46567535400390625\n","[Training Epoch 2] Batch 2021, Loss 0.4791625142097473\n","[Training Epoch 2] Batch 2022, Loss 0.5167875289916992\n","[Training Epoch 2] Batch 2023, Loss 0.500927209854126\n","[Training Epoch 2] Batch 2024, Loss 0.47470563650131226\n","[Training Epoch 2] Batch 2025, Loss 0.4957242012023926\n","[Training Epoch 2] Batch 2026, Loss 0.5000084638595581\n","[Training Epoch 2] Batch 2027, Loss 0.5009274482727051\n","[Training Epoch 2] Batch 2028, Loss 0.5074635744094849\n","[Training Epoch 2] Batch 2029, Loss 0.4938185214996338\n","[Training Epoch 2] Batch 2030, Loss 0.49379849433898926\n","[Training Epoch 2] Batch 2031, Loss 0.4845973551273346\n","[Training Epoch 2] Batch 2032, Loss 0.5124549865722656\n","[Training Epoch 2] Batch 2033, Loss 0.49621281027793884\n","[Training Epoch 2] Batch 2034, Loss 0.5033166408538818\n","[Training Epoch 2] Batch 2035, Loss 0.48717907071113586\n","[Training Epoch 2] Batch 2036, Loss 0.5024828314781189\n","[Training Epoch 2] Batch 2037, Loss 0.5012009739875793\n","[Training Epoch 2] Batch 2038, Loss 0.47866272926330566\n","[Training Epoch 2] Batch 2039, Loss 0.520068883895874\n","[Training Epoch 2] Batch 2040, Loss 0.46858930587768555\n","[Training Epoch 2] Batch 2041, Loss 0.5143028497695923\n","[Training Epoch 2] Batch 2042, Loss 0.47997304797172546\n","[Training Epoch 2] Batch 2043, Loss 0.5186116099357605\n","[Training Epoch 2] Batch 2044, Loss 0.47039878368377686\n","[Training Epoch 2] Batch 2045, Loss 0.5095662474632263\n","[Training Epoch 2] Batch 2046, Loss 0.5061848163604736\n","[Training Epoch 2] Batch 2047, Loss 0.48149046301841736\n","[Training Epoch 2] Batch 2048, Loss 0.5041722059249878\n","[Training Epoch 2] Batch 2049, Loss 0.5051736831665039\n","[Training Epoch 2] Batch 2050, Loss 0.5165733695030212\n","[Training Epoch 2] Batch 2051, Loss 0.500690221786499\n","[Training Epoch 2] Batch 2052, Loss 0.5139223337173462\n","[Training Epoch 2] Batch 2053, Loss 0.5122419595718384\n","[Training Epoch 2] Batch 2054, Loss 0.46457618474960327\n","[Training Epoch 2] Batch 2055, Loss 0.5042933225631714\n","[Training Epoch 2] Batch 2056, Loss 0.4847860336303711\n","[Training Epoch 2] Batch 2057, Loss 0.4893530011177063\n","[Training Epoch 2] Batch 2058, Loss 0.4966689348220825\n","[Training Epoch 2] Batch 2059, Loss 0.4726279675960541\n","[Training Epoch 2] Batch 2060, Loss 0.4778611660003662\n","[Training Epoch 2] Batch 2061, Loss 0.5014697313308716\n","[Training Epoch 2] Batch 2062, Loss 0.46188825368881226\n","[Training Epoch 2] Batch 2063, Loss 0.5000784993171692\n","[Training Epoch 2] Batch 2064, Loss 0.49364006519317627\n","[Training Epoch 2] Batch 2065, Loss 0.5149866342544556\n","[Training Epoch 2] Batch 2066, Loss 0.48970454931259155\n","[Training Epoch 2] Batch 2067, Loss 0.5083977580070496\n","[Training Epoch 2] Batch 2068, Loss 0.46738651394844055\n","[Training Epoch 2] Batch 2069, Loss 0.48588699102401733\n","[Training Epoch 2] Batch 2070, Loss 0.4714595675468445\n","[Training Epoch 2] Batch 2071, Loss 0.49833086133003235\n","[Training Epoch 2] Batch 2072, Loss 0.5083867311477661\n","[Training Epoch 2] Batch 2073, Loss 0.5091904401779175\n","[Training Epoch 2] Batch 2074, Loss 0.5120760202407837\n","[Training Epoch 2] Batch 2075, Loss 0.4846110939979553\n","[Training Epoch 2] Batch 2076, Loss 0.4788445830345154\n","[Training Epoch 2] Batch 2077, Loss 0.5003344416618347\n","[Training Epoch 2] Batch 2078, Loss 0.4955323338508606\n","[Training Epoch 2] Batch 2079, Loss 0.4489705562591553\n","[Training Epoch 2] Batch 2080, Loss 0.49099642038345337\n","[Training Epoch 2] Batch 2081, Loss 0.48531296849250793\n","[Training Epoch 2] Batch 2082, Loss 0.505037248134613\n","[Training Epoch 2] Batch 2083, Loss 0.501792311668396\n","[Training Epoch 2] Batch 2084, Loss 0.459321528673172\n","[Training Epoch 2] Batch 2085, Loss 0.5081133842468262\n","[Training Epoch 2] Batch 2086, Loss 0.5067678689956665\n","[Training Epoch 2] Batch 2087, Loss 0.4956505298614502\n","[Training Epoch 2] Batch 2088, Loss 0.5027642846107483\n","[Training Epoch 2] Batch 2089, Loss 0.5060170888900757\n","[Training Epoch 2] Batch 2090, Loss 0.4691793620586395\n","[Training Epoch 2] Batch 2091, Loss 0.48089492321014404\n","[Training Epoch 2] Batch 2092, Loss 0.4821934103965759\n","[Training Epoch 2] Batch 2093, Loss 0.46889764070510864\n","[Training Epoch 2] Batch 2094, Loss 0.49092137813568115\n","[Training Epoch 2] Batch 2095, Loss 0.492881715297699\n","[Training Epoch 2] Batch 2096, Loss 0.48851674795150757\n","[Training Epoch 2] Batch 2097, Loss 0.4962693452835083\n","[Training Epoch 2] Batch 2098, Loss 0.49723660945892334\n","[Training Epoch 2] Batch 2099, Loss 0.4877626299858093\n","[Training Epoch 2] Batch 2100, Loss 0.4809461236000061\n","[Training Epoch 2] Batch 2101, Loss 0.49999237060546875\n","[Training Epoch 2] Batch 2102, Loss 0.4996722638607025\n","[Training Epoch 2] Batch 2103, Loss 0.4956931471824646\n","[Training Epoch 2] Batch 2104, Loss 0.4831455945968628\n","[Training Epoch 2] Batch 2105, Loss 0.5139023661613464\n","[Training Epoch 2] Batch 2106, Loss 0.50778728723526\n","[Training Epoch 2] Batch 2107, Loss 0.5355844497680664\n","[Training Epoch 2] Batch 2108, Loss 0.47921913862228394\n","[Training Epoch 2] Batch 2109, Loss 0.5359683632850647\n","[Training Epoch 2] Batch 2110, Loss 0.503739058971405\n","[Training Epoch 2] Batch 2111, Loss 0.4827095568180084\n","[Training Epoch 2] Batch 2112, Loss 0.5071681141853333\n","[Training Epoch 2] Batch 2113, Loss 0.4820089638233185\n","[Training Epoch 2] Batch 2114, Loss 0.4881899058818817\n","[Training Epoch 2] Batch 2115, Loss 0.49596837162971497\n","[Training Epoch 2] Batch 2116, Loss 0.487457811832428\n","[Training Epoch 2] Batch 2117, Loss 0.492090106010437\n","[Training Epoch 2] Batch 2118, Loss 0.49918121099472046\n","[Training Epoch 2] Batch 2119, Loss 0.5009542107582092\n","[Training Epoch 2] Batch 2120, Loss 0.5110955238342285\n","[Training Epoch 2] Batch 2121, Loss 0.5021979808807373\n","[Training Epoch 2] Batch 2122, Loss 0.5003221035003662\n","[Training Epoch 2] Batch 2123, Loss 0.5063765645027161\n","[Training Epoch 2] Batch 2124, Loss 0.49613216519355774\n","[Training Epoch 2] Batch 2125, Loss 0.500030517578125\n","[Training Epoch 2] Batch 2126, Loss 0.4821122884750366\n","[Training Epoch 2] Batch 2127, Loss 0.4919354319572449\n","[Training Epoch 2] Batch 2128, Loss 0.49250680208206177\n","[Training Epoch 2] Batch 2129, Loss 0.477230429649353\n","[Training Epoch 2] Batch 2130, Loss 0.49587106704711914\n","[Training Epoch 2] Batch 2131, Loss 0.4916139245033264\n","[Training Epoch 2] Batch 2132, Loss 0.4955230951309204\n","[Training Epoch 2] Batch 2133, Loss 0.5076429843902588\n","[Training Epoch 2] Batch 2134, Loss 0.485042542219162\n","[Training Epoch 2] Batch 2135, Loss 0.5137048959732056\n","[Training Epoch 2] Batch 2136, Loss 0.45617756247520447\n","[Training Epoch 2] Batch 2137, Loss 0.5060834884643555\n","[Training Epoch 2] Batch 2138, Loss 0.48798590898513794\n","[Training Epoch 2] Batch 2139, Loss 0.4970345199108124\n","[Training Epoch 2] Batch 2140, Loss 0.5206865072250366\n","[Training Epoch 2] Batch 2141, Loss 0.5038292407989502\n","[Training Epoch 2] Batch 2142, Loss 0.49823370575904846\n","[Training Epoch 2] Batch 2143, Loss 0.5136672258377075\n","[Training Epoch 2] Batch 2144, Loss 0.5044984817504883\n","[Training Epoch 2] Batch 2145, Loss 0.49371665716171265\n","[Training Epoch 2] Batch 2146, Loss 0.5245881080627441\n","[Training Epoch 2] Batch 2147, Loss 0.5104207396507263\n","[Training Epoch 2] Batch 2148, Loss 0.527884840965271\n","[Training Epoch 2] Batch 2149, Loss 0.49980777502059937\n","[Training Epoch 2] Batch 2150, Loss 0.48298248648643494\n","[Training Epoch 2] Batch 2151, Loss 0.48876920342445374\n","[Training Epoch 2] Batch 2152, Loss 0.4917193651199341\n","[Training Epoch 2] Batch 2153, Loss 0.5008987188339233\n","[Training Epoch 2] Batch 2154, Loss 0.4746597111225128\n","[Training Epoch 2] Batch 2155, Loss 0.4994644522666931\n","[Training Epoch 2] Batch 2156, Loss 0.47282081842422485\n","[Training Epoch 2] Batch 2157, Loss 0.5166816711425781\n","[Training Epoch 2] Batch 2158, Loss 0.4897809326648712\n","[Training Epoch 2] Batch 2159, Loss 0.4950205087661743\n","[Training Epoch 2] Batch 2160, Loss 0.4605744481086731\n","[Training Epoch 2] Batch 2161, Loss 0.4842475652694702\n","[Training Epoch 2] Batch 2162, Loss 0.4940922260284424\n","[Training Epoch 2] Batch 2163, Loss 0.4697416424751282\n","[Training Epoch 2] Batch 2164, Loss 0.4816981554031372\n","[Training Epoch 2] Batch 2165, Loss 0.4921099841594696\n","[Training Epoch 2] Batch 2166, Loss 0.4892690181732178\n","[Training Epoch 2] Batch 2167, Loss 0.5079543590545654\n","[Training Epoch 2] Batch 2168, Loss 0.4772864878177643\n","[Training Epoch 2] Batch 2169, Loss 0.4909853935241699\n","[Training Epoch 2] Batch 2170, Loss 0.4789060354232788\n","[Training Epoch 2] Batch 2171, Loss 0.49508076906204224\n","[Training Epoch 2] Batch 2172, Loss 0.45269253849983215\n","[Training Epoch 2] Batch 2173, Loss 0.47541242837905884\n","[Training Epoch 2] Batch 2174, Loss 0.47475266456604004\n","[Training Epoch 2] Batch 2175, Loss 0.49742376804351807\n","[Training Epoch 2] Batch 2176, Loss 0.5215257406234741\n","[Training Epoch 2] Batch 2177, Loss 0.4888033866882324\n","[Training Epoch 2] Batch 2178, Loss 0.5085179209709167\n","[Training Epoch 2] Batch 2179, Loss 0.4570227563381195\n","[Training Epoch 2] Batch 2180, Loss 0.4871600568294525\n","[Training Epoch 2] Batch 2181, Loss 0.5105072259902954\n","[Training Epoch 2] Batch 2182, Loss 0.49082446098327637\n","[Training Epoch 2] Batch 2183, Loss 0.5100499987602234\n","[Training Epoch 2] Batch 2184, Loss 0.49104952812194824\n","[Training Epoch 2] Batch 2185, Loss 0.5124319195747375\n","[Training Epoch 2] Batch 2186, Loss 0.4778191149234772\n","[Training Epoch 2] Batch 2187, Loss 0.45994025468826294\n","[Training Epoch 2] Batch 2188, Loss 0.4996313750743866\n","[Training Epoch 2] Batch 2189, Loss 0.4910307228565216\n","[Training Epoch 2] Batch 2190, Loss 0.4773505926132202\n","[Training Epoch 2] Batch 2191, Loss 0.4756452441215515\n","[Training Epoch 2] Batch 2192, Loss 0.5060529708862305\n","[Training Epoch 2] Batch 2193, Loss 0.4625195860862732\n","[Training Epoch 2] Batch 2194, Loss 0.5103353261947632\n","[Training Epoch 2] Batch 2195, Loss 0.4917759299278259\n","[Training Epoch 2] Batch 2196, Loss 0.5090884566307068\n","[Training Epoch 2] Batch 2197, Loss 0.4824947714805603\n","[Training Epoch 2] Batch 2198, Loss 0.4732540249824524\n","[Training Epoch 2] Batch 2199, Loss 0.5060750246047974\n","[Training Epoch 2] Batch 2200, Loss 0.5072987079620361\n","[Training Epoch 2] Batch 2201, Loss 0.5333255529403687\n","[Training Epoch 2] Batch 2202, Loss 0.48391032218933105\n","[Training Epoch 2] Batch 2203, Loss 0.49445825815200806\n","[Training Epoch 2] Batch 2204, Loss 0.5075342655181885\n","[Training Epoch 2] Batch 2205, Loss 0.489075630903244\n","[Training Epoch 2] Batch 2206, Loss 0.49864381551742554\n","[Training Epoch 2] Batch 2207, Loss 0.47336748242378235\n","[Training Epoch 2] Batch 2208, Loss 0.5024296045303345\n","[Training Epoch 2] Batch 2209, Loss 0.48869532346725464\n","[Training Epoch 2] Batch 2210, Loss 0.5201899409294128\n","[Training Epoch 2] Batch 2211, Loss 0.5038844347000122\n","[Training Epoch 2] Batch 2212, Loss 0.5121488571166992\n","[Training Epoch 2] Batch 2213, Loss 0.4929124712944031\n","[Training Epoch 2] Batch 2214, Loss 0.47466808557510376\n","[Training Epoch 2] Batch 2215, Loss 0.49959835410118103\n","[Training Epoch 2] Batch 2216, Loss 0.5290091037750244\n","[Training Epoch 2] Batch 2217, Loss 0.5525334477424622\n","[Training Epoch 2] Batch 2218, Loss 0.4701925218105316\n","[Training Epoch 2] Batch 2219, Loss 0.5175943374633789\n","[Training Epoch 2] Batch 2220, Loss 0.5244196653366089\n","[Training Epoch 2] Batch 2221, Loss 0.4752737581729889\n","[Training Epoch 2] Batch 2222, Loss 0.4965707063674927\n","[Training Epoch 2] Batch 2223, Loss 0.4826698303222656\n","[Training Epoch 2] Batch 2224, Loss 0.49681442975997925\n","[Training Epoch 2] Batch 2225, Loss 0.5004086494445801\n","[Training Epoch 2] Batch 2226, Loss 0.5032637119293213\n","[Training Epoch 2] Batch 2227, Loss 0.5242998600006104\n","[Training Epoch 2] Batch 2228, Loss 0.5035187005996704\n","[Training Epoch 2] Batch 2229, Loss 0.5030328035354614\n","[Training Epoch 2] Batch 2230, Loss 0.48135653138160706\n","[Training Epoch 2] Batch 2231, Loss 0.5100238919258118\n","[Training Epoch 2] Batch 2232, Loss 0.48991629481315613\n","[Training Epoch 2] Batch 2233, Loss 0.5120023488998413\n","[Training Epoch 2] Batch 2234, Loss 0.5237125158309937\n","[Training Epoch 2] Batch 2235, Loss 0.48809707164764404\n","[Training Epoch 2] Batch 2236, Loss 0.5104727149009705\n","[Training Epoch 2] Batch 2237, Loss 0.5258287191390991\n","[Training Epoch 2] Batch 2238, Loss 0.5033355355262756\n","[Training Epoch 2] Batch 2239, Loss 0.4652796983718872\n","[Training Epoch 2] Batch 2240, Loss 0.48729759454727173\n","[Training Epoch 2] Batch 2241, Loss 0.529940664768219\n","[Training Epoch 2] Batch 2242, Loss 0.5090969204902649\n","[Training Epoch 2] Batch 2243, Loss 0.5448015928268433\n","[Training Epoch 2] Batch 2244, Loss 0.5081849694252014\n","[Training Epoch 2] Batch 2245, Loss 0.5136008262634277\n","[Training Epoch 2] Batch 2246, Loss 0.46147167682647705\n","[Training Epoch 2] Batch 2247, Loss 0.5193067789077759\n","[Training Epoch 2] Batch 2248, Loss 0.532250165939331\n","[Training Epoch 2] Batch 2249, Loss 0.5116565227508545\n","[Training Epoch 2] Batch 2250, Loss 0.5248346328735352\n","[Training Epoch 2] Batch 2251, Loss 0.5131460428237915\n","[Training Epoch 2] Batch 2252, Loss 0.4629686772823334\n","[Training Epoch 2] Batch 2253, Loss 0.4606853127479553\n","[Training Epoch 2] Batch 2254, Loss 0.48772481083869934\n","[Training Epoch 2] Batch 2255, Loss 0.4926837086677551\n","[Training Epoch 2] Batch 2256, Loss 0.5202535390853882\n","[Training Epoch 2] Batch 2257, Loss 0.5034053325653076\n","[Training Epoch 2] Batch 2258, Loss 0.49654102325439453\n","[Training Epoch 2] Batch 2259, Loss 0.4913155734539032\n","[Training Epoch 2] Batch 2260, Loss 0.47875937819480896\n","[Training Epoch 2] Batch 2261, Loss 0.505852460861206\n","[Training Epoch 2] Batch 2262, Loss 0.46575164794921875\n","[Training Epoch 2] Batch 2263, Loss 0.48594820499420166\n","[Training Epoch 2] Batch 2264, Loss 0.493492066860199\n","[Training Epoch 2] Batch 2265, Loss 0.4934990406036377\n","[Training Epoch 2] Batch 2266, Loss 0.4930591881275177\n","[Training Epoch 2] Batch 2267, Loss 0.5115436911582947\n","[Training Epoch 2] Batch 2268, Loss 0.4855560064315796\n","[Training Epoch 2] Batch 2269, Loss 0.5047675967216492\n","[Training Epoch 2] Batch 2270, Loss 0.5172966718673706\n","[Training Epoch 2] Batch 2271, Loss 0.5173141956329346\n","[Training Epoch 2] Batch 2272, Loss 0.4851522445678711\n","[Training Epoch 2] Batch 2273, Loss 0.4692584276199341\n","[Training Epoch 2] Batch 2274, Loss 0.5146421790122986\n","[Training Epoch 2] Batch 2275, Loss 0.4838780164718628\n","[Training Epoch 2] Batch 2276, Loss 0.4719698429107666\n","[Training Epoch 2] Batch 2277, Loss 0.49339714646339417\n","[Training Epoch 2] Batch 2278, Loss 0.5037996768951416\n","[Training Epoch 2] Batch 2279, Loss 0.49078133702278137\n","[Training Epoch 2] Batch 2280, Loss 0.49458011984825134\n","[Training Epoch 2] Batch 2281, Loss 0.4656347632408142\n","[Training Epoch 2] Batch 2282, Loss 0.5139989852905273\n","[Training Epoch 2] Batch 2283, Loss 0.499432772397995\n","[Training Epoch 2] Batch 2284, Loss 0.49889764189720154\n","[Training Epoch 2] Batch 2285, Loss 0.45879387855529785\n","[Training Epoch 2] Batch 2286, Loss 0.4979011118412018\n","[Training Epoch 2] Batch 2287, Loss 0.5014472007751465\n","[Training Epoch 2] Batch 2288, Loss 0.5420982837677002\n","[Training Epoch 2] Batch 2289, Loss 0.5013872385025024\n","[Training Epoch 2] Batch 2290, Loss 0.4733549952507019\n","[Training Epoch 2] Batch 2291, Loss 0.5045799612998962\n","[Training Epoch 2] Batch 2292, Loss 0.5173894166946411\n","[Training Epoch 2] Batch 2293, Loss 0.4896989166736603\n","[Training Epoch 2] Batch 2294, Loss 0.5137628316879272\n","[Training Epoch 2] Batch 2295, Loss 0.5005229711532593\n","[Training Epoch 2] Batch 2296, Loss 0.5073328018188477\n","[Training Epoch 2] Batch 2297, Loss 0.49797824025154114\n","[Training Epoch 2] Batch 2298, Loss 0.5333606600761414\n","[Training Epoch 2] Batch 2299, Loss 0.4956255555152893\n","[Training Epoch 2] Batch 2300, Loss 0.500956654548645\n","[Training Epoch 2] Batch 2301, Loss 0.4953311085700989\n","[Training Epoch 2] Batch 2302, Loss 0.5244027376174927\n","[Training Epoch 2] Batch 2303, Loss 0.48137885332107544\n","[Training Epoch 2] Batch 2304, Loss 0.4901759922504425\n","[Training Epoch 2] Batch 2305, Loss 0.47398948669433594\n","[Training Epoch 2] Batch 2306, Loss 0.5150101184844971\n","[Training Epoch 2] Batch 2307, Loss 0.4920078217983246\n","[Training Epoch 2] Batch 2308, Loss 0.4832213819026947\n","[Training Epoch 2] Batch 2309, Loss 0.5163372755050659\n","[Training Epoch 2] Batch 2310, Loss 0.5171042680740356\n","[Training Epoch 2] Batch 2311, Loss 0.49000677466392517\n","[Training Epoch 2] Batch 2312, Loss 0.4664672315120697\n","[Training Epoch 2] Batch 2313, Loss 0.5019029974937439\n","[Training Epoch 2] Batch 2314, Loss 0.4651651382446289\n","[Training Epoch 2] Batch 2315, Loss 0.5037003755569458\n","[Training Epoch 2] Batch 2316, Loss 0.46167707443237305\n","[Training Epoch 2] Batch 2317, Loss 0.49317383766174316\n","[Training Epoch 2] Batch 2318, Loss 0.4909074306488037\n","[Training Epoch 2] Batch 2319, Loss 0.471121609210968\n","[Training Epoch 2] Batch 2320, Loss 0.4863673448562622\n","[Training Epoch 2] Batch 2321, Loss 0.49325570464134216\n","[Training Epoch 2] Batch 2322, Loss 0.475447416305542\n","[Training Epoch 2] Batch 2323, Loss 0.5247776508331299\n","[Training Epoch 2] Batch 2324, Loss 0.49618446826934814\n","[Training Epoch 2] Batch 2325, Loss 0.5223915576934814\n","[Training Epoch 2] Batch 2326, Loss 0.493335098028183\n","[Training Epoch 2] Batch 2327, Loss 0.4854298233985901\n","[Training Epoch 2] Batch 2328, Loss 0.499986469745636\n","[Training Epoch 2] Batch 2329, Loss 0.4730950593948364\n","[Training Epoch 2] Batch 2330, Loss 0.49599090218544006\n","[Training Epoch 2] Batch 2331, Loss 0.5098490715026855\n","[Training Epoch 2] Batch 2332, Loss 0.5328078269958496\n","[Training Epoch 2] Batch 2333, Loss 0.5042586922645569\n","[Training Epoch 2] Batch 2334, Loss 0.5077309012413025\n","[Training Epoch 2] Batch 2335, Loss 0.5019367337226868\n","[Training Epoch 2] Batch 2336, Loss 0.49420756101608276\n","[Training Epoch 2] Batch 2337, Loss 0.5010911226272583\n","[Training Epoch 2] Batch 2338, Loss 0.4904589354991913\n","[Training Epoch 2] Batch 2339, Loss 0.47318872809410095\n","[Training Epoch 2] Batch 2340, Loss 0.5008746981620789\n","[Training Epoch 2] Batch 2341, Loss 0.46546298265457153\n","[Training Epoch 2] Batch 2342, Loss 0.5289909839630127\n","[Training Epoch 2] Batch 2343, Loss 0.4934868812561035\n","[Training Epoch 2] Batch 2344, Loss 0.46865028142929077\n","[Training Epoch 2] Batch 2345, Loss 0.4721151888370514\n","[Training Epoch 2] Batch 2346, Loss 0.47752881050109863\n","[Training Epoch 2] Batch 2347, Loss 0.5125288963317871\n","[Training Epoch 2] Batch 2348, Loss 0.5020066499710083\n","[Training Epoch 2] Batch 2349, Loss 0.5163275599479675\n","[Training Epoch 2] Batch 2350, Loss 0.44998374581336975\n","[Training Epoch 2] Batch 2351, Loss 0.504828929901123\n","[Training Epoch 2] Batch 2352, Loss 0.49360668659210205\n","[Training Epoch 2] Batch 2353, Loss 0.4582967162132263\n","[Training Epoch 2] Batch 2354, Loss 0.4987025856971741\n","[Training Epoch 2] Batch 2355, Loss 0.47094210982322693\n","[Training Epoch 2] Batch 2356, Loss 0.4975647032260895\n","[Training Epoch 2] Batch 2357, Loss 0.4839361310005188\n","[Training Epoch 2] Batch 2358, Loss 0.4682135581970215\n","[Training Epoch 2] Batch 2359, Loss 0.5033614635467529\n","[Training Epoch 2] Batch 2360, Loss 0.5015987157821655\n","[Training Epoch 2] Batch 2361, Loss 0.47718560695648193\n","[Training Epoch 2] Batch 2362, Loss 0.4963773488998413\n","[Training Epoch 2] Batch 2363, Loss 0.49777016043663025\n","[Training Epoch 2] Batch 2364, Loss 0.47122594714164734\n","[Training Epoch 2] Batch 2365, Loss 0.4882544279098511\n","[Training Epoch 2] Batch 2366, Loss 0.46213361620903015\n","[Training Epoch 2] Batch 2367, Loss 0.5121603608131409\n","[Training Epoch 2] Batch 2368, Loss 0.5036221742630005\n","[Training Epoch 2] Batch 2369, Loss 0.514392614364624\n","[Training Epoch 2] Batch 2370, Loss 0.4947627782821655\n","[Training Epoch 2] Batch 2371, Loss 0.4854140877723694\n","[Training Epoch 2] Batch 2372, Loss 0.5241724848747253\n","[Training Epoch 2] Batch 2373, Loss 0.47761771082878113\n","[Training Epoch 2] Batch 2374, Loss 0.48696762323379517\n","[Training Epoch 2] Batch 2375, Loss 0.4898645579814911\n","[Training Epoch 2] Batch 2376, Loss 0.46987682580947876\n","[Training Epoch 2] Batch 2377, Loss 0.47784000635147095\n","[Training Epoch 2] Batch 2378, Loss 0.47099390625953674\n","[Training Epoch 2] Batch 2379, Loss 0.4968227744102478\n","[Training Epoch 2] Batch 2380, Loss 0.4870339035987854\n","[Training Epoch 2] Batch 2381, Loss 0.4659247398376465\n","[Training Epoch 2] Batch 2382, Loss 0.49501746892929077\n","[Training Epoch 2] Batch 2383, Loss 0.4683080315589905\n","[Training Epoch 2] Batch 2384, Loss 0.48602497577667236\n","[Training Epoch 2] Batch 2385, Loss 0.5028238892555237\n","[Training Epoch 2] Batch 2386, Loss 0.47857218980789185\n","[Training Epoch 2] Batch 2387, Loss 0.47817760705947876\n","[Training Epoch 2] Batch 2388, Loss 0.5225468277931213\n","[Training Epoch 2] Batch 2389, Loss 0.4578942656517029\n","[Training Epoch 2] Batch 2390, Loss 0.5127315521240234\n","[Training Epoch 2] Batch 2391, Loss 0.4837973415851593\n","[Training Epoch 2] Batch 2392, Loss 0.49228841066360474\n","[Training Epoch 2] Batch 2393, Loss 0.47333967685699463\n","[Training Epoch 2] Batch 2394, Loss 0.49098044633865356\n","[Training Epoch 2] Batch 2395, Loss 0.48743367195129395\n","[Training Epoch 2] Batch 2396, Loss 0.5142391920089722\n","[Training Epoch 2] Batch 2397, Loss 0.47571873664855957\n","[Training Epoch 2] Batch 2398, Loss 0.46194833517074585\n","[Training Epoch 2] Batch 2399, Loss 0.5023766756057739\n","[Training Epoch 2] Batch 2400, Loss 0.47200459241867065\n","[Training Epoch 2] Batch 2401, Loss 0.4869391918182373\n","[Training Epoch 2] Batch 2402, Loss 0.5210608243942261\n","[Training Epoch 2] Batch 2403, Loss 0.49390551447868347\n","[Training Epoch 2] Batch 2404, Loss 0.4970146417617798\n","[Training Epoch 2] Batch 2405, Loss 0.48091834783554077\n","[Training Epoch 2] Batch 2406, Loss 0.45805516839027405\n","[Training Epoch 2] Batch 2407, Loss 0.4865839183330536\n","[Training Epoch 2] Batch 2408, Loss 0.49279552698135376\n","[Training Epoch 2] Batch 2409, Loss 0.4941670000553131\n","[Training Epoch 2] Batch 2410, Loss 0.5267668962478638\n","[Training Epoch 2] Batch 2411, Loss 0.5033619403839111\n","[Training Epoch 2] Batch 2412, Loss 0.4740713834762573\n","[Training Epoch 2] Batch 2413, Loss 0.45852553844451904\n","[Training Epoch 2] Batch 2414, Loss 0.49625664949417114\n","[Training Epoch 2] Batch 2415, Loss 0.49459534883499146\n","[Training Epoch 2] Batch 2416, Loss 0.4693222641944885\n","[Training Epoch 2] Batch 2417, Loss 0.4972180724143982\n","[Training Epoch 2] Batch 2418, Loss 0.48728296160697937\n","[Training Epoch 2] Batch 2419, Loss 0.49961546063423157\n","[Training Epoch 2] Batch 2420, Loss 0.49048930406570435\n","[Training Epoch 2] Batch 2421, Loss 0.4808744192123413\n","[Training Epoch 2] Batch 2422, Loss 0.48926153779029846\n","[Training Epoch 2] Batch 2423, Loss 0.48422935605049133\n","[Training Epoch 2] Batch 2424, Loss 0.48393306136131287\n","[Training Epoch 2] Batch 2425, Loss 0.44821521639823914\n","[Training Epoch 2] Batch 2426, Loss 0.4857883155345917\n","[Training Epoch 2] Batch 2427, Loss 0.5052533745765686\n","[Training Epoch 2] Batch 2428, Loss 0.5046486854553223\n","[Training Epoch 2] Batch 2429, Loss 0.4614575505256653\n","[Training Epoch 2] Batch 2430, Loss 0.5046916007995605\n","[Training Epoch 2] Batch 2431, Loss 0.4754621088504791\n","[Training Epoch 2] Batch 2432, Loss 0.5037792325019836\n","[Training Epoch 2] Batch 2433, Loss 0.4741660952568054\n","[Training Epoch 2] Batch 2434, Loss 0.499310165643692\n","[Training Epoch 2] Batch 2435, Loss 0.5232658386230469\n","[Training Epoch 2] Batch 2436, Loss 0.519022524356842\n","[Training Epoch 2] Batch 2437, Loss 0.47562962770462036\n","[Training Epoch 2] Batch 2438, Loss 0.4763161540031433\n","[Training Epoch 2] Batch 2439, Loss 0.47168731689453125\n","[Training Epoch 2] Batch 2440, Loss 0.4750174880027771\n","[Training Epoch 2] Batch 2441, Loss 0.44773542881011963\n","[Training Epoch 2] Batch 2442, Loss 0.5047885179519653\n","[Training Epoch 2] Batch 2443, Loss 0.5059658288955688\n","[Training Epoch 2] Batch 2444, Loss 0.5303125977516174\n","[Training Epoch 2] Batch 2445, Loss 0.4942132234573364\n","[Training Epoch 2] Batch 2446, Loss 0.4857039749622345\n","[Training Epoch 2] Batch 2447, Loss 0.5028551816940308\n","[Training Epoch 2] Batch 2448, Loss 0.493219256401062\n","[Training Epoch 2] Batch 2449, Loss 0.46852147579193115\n","[Training Epoch 2] Batch 2450, Loss 0.5129696726799011\n","[Training Epoch 2] Batch 2451, Loss 0.5054166316986084\n","[Training Epoch 2] Batch 2452, Loss 0.5056352019309998\n","[Training Epoch 2] Batch 2453, Loss 0.48121047019958496\n","[Training Epoch 2] Batch 2454, Loss 0.4778308570384979\n","[Training Epoch 2] Batch 2455, Loss 0.4754685163497925\n","[Training Epoch 2] Batch 2456, Loss 0.5010517835617065\n","[Training Epoch 2] Batch 2457, Loss 0.4876713156700134\n","[Training Epoch 2] Batch 2458, Loss 0.47633254528045654\n","[Training Epoch 2] Batch 2459, Loss 0.503095269203186\n","[Training Epoch 2] Batch 2460, Loss 0.4893215298652649\n","[Training Epoch 2] Batch 2461, Loss 0.4930103123188019\n","[Training Epoch 2] Batch 2462, Loss 0.4531831443309784\n","[Training Epoch 2] Batch 2463, Loss 0.4851072430610657\n","[Training Epoch 2] Batch 2464, Loss 0.5020824670791626\n","[Training Epoch 2] Batch 2465, Loss 0.4767288267612457\n","[Training Epoch 2] Batch 2466, Loss 0.5109777450561523\n","[Training Epoch 2] Batch 2467, Loss 0.4860699772834778\n","[Training Epoch 2] Batch 2468, Loss 0.5106561779975891\n","[Training Epoch 2] Batch 2469, Loss 0.5389747023582458\n","[Training Epoch 2] Batch 2470, Loss 0.48773664236068726\n","[Training Epoch 2] Batch 2471, Loss 0.5153709650039673\n","[Training Epoch 2] Batch 2472, Loss 0.47986775636672974\n","[Training Epoch 2] Batch 2473, Loss 0.5068067312240601\n","[Training Epoch 2] Batch 2474, Loss 0.4976010024547577\n","[Training Epoch 2] Batch 2475, Loss 0.47937047481536865\n","[Training Epoch 2] Batch 2476, Loss 0.5027297735214233\n","[Training Epoch 2] Batch 2477, Loss 0.5010292530059814\n","[Training Epoch 2] Batch 2478, Loss 0.5008856058120728\n","[Training Epoch 2] Batch 2479, Loss 0.49795064330101013\n","[Training Epoch 2] Batch 2480, Loss 0.5127521753311157\n","[Training Epoch 2] Batch 2481, Loss 0.4907699525356293\n","[Training Epoch 2] Batch 2482, Loss 0.4901806712150574\n","[Training Epoch 2] Batch 2483, Loss 0.4873310327529907\n","[Training Epoch 2] Batch 2484, Loss 0.5084690451622009\n","[Training Epoch 2] Batch 2485, Loss 0.46409788727760315\n","[Training Epoch 2] Batch 2486, Loss 0.48538875579833984\n","[Training Epoch 2] Batch 2487, Loss 0.4591579735279083\n","[Training Epoch 2] Batch 2488, Loss 0.49621647596359253\n","[Training Epoch 2] Batch 2489, Loss 0.48148003220558167\n","[Training Epoch 2] Batch 2490, Loss 0.5204282999038696\n","[Training Epoch 2] Batch 2491, Loss 0.5281804800033569\n","[Training Epoch 2] Batch 2492, Loss 0.4916136860847473\n","[Training Epoch 2] Batch 2493, Loss 0.496378630399704\n","[Training Epoch 2] Batch 2494, Loss 0.5011735558509827\n","[Training Epoch 2] Batch 2495, Loss 0.4848936200141907\n","[Training Epoch 2] Batch 2496, Loss 0.46627748012542725\n","[Training Epoch 2] Batch 2497, Loss 0.490092396736145\n","[Training Epoch 2] Batch 2498, Loss 0.488945871591568\n","[Training Epoch 2] Batch 2499, Loss 0.48377013206481934\n","[Training Epoch 2] Batch 2500, Loss 0.510083794593811\n","[Training Epoch 2] Batch 2501, Loss 0.5005594491958618\n","[Training Epoch 2] Batch 2502, Loss 0.4742674231529236\n","[Training Epoch 2] Batch 2503, Loss 0.5040898323059082\n","[Training Epoch 2] Batch 2504, Loss 0.5167206525802612\n","[Training Epoch 2] Batch 2505, Loss 0.5020712614059448\n","[Training Epoch 2] Batch 2506, Loss 0.4878012537956238\n","[Training Epoch 2] Batch 2507, Loss 0.49239808320999146\n","[Training Epoch 2] Batch 2508, Loss 0.5106768608093262\n","[Training Epoch 2] Batch 2509, Loss 0.4867928624153137\n","[Training Epoch 2] Batch 2510, Loss 0.48675593733787537\n","[Training Epoch 2] Batch 2511, Loss 0.49769458174705505\n","[Training Epoch 2] Batch 2512, Loss 0.5104345083236694\n","[Training Epoch 2] Batch 2513, Loss 0.46661633253097534\n","[Training Epoch 2] Batch 2514, Loss 0.5081089735031128\n","[Training Epoch 2] Batch 2515, Loss 0.47463029623031616\n","[Training Epoch 2] Batch 2516, Loss 0.481994092464447\n","[Training Epoch 2] Batch 2517, Loss 0.49621886014938354\n","[Training Epoch 2] Batch 2518, Loss 0.4855819344520569\n","[Training Epoch 2] Batch 2519, Loss 0.47380632162094116\n","[Training Epoch 2] Batch 2520, Loss 0.4937463700771332\n","[Training Epoch 2] Batch 2521, Loss 0.5056384801864624\n","[Training Epoch 2] Batch 2522, Loss 0.49921759963035583\n","[Training Epoch 2] Batch 2523, Loss 0.45412150025367737\n","[Training Epoch 2] Batch 2524, Loss 0.47803521156311035\n","[Training Epoch 2] Batch 2525, Loss 0.48679444193840027\n","[Training Epoch 2] Batch 2526, Loss 0.46845996379852295\n","[Training Epoch 2] Batch 2527, Loss 0.47976526618003845\n","[Training Epoch 2] Batch 2528, Loss 0.5117849707603455\n","[Training Epoch 2] Batch 2529, Loss 0.49180763959884644\n","[Training Epoch 2] Batch 2530, Loss 0.5144749879837036\n","[Training Epoch 2] Batch 2531, Loss 0.5213602781295776\n","[Training Epoch 2] Batch 2532, Loss 0.5113288760185242\n","[Training Epoch 2] Batch 2533, Loss 0.47345513105392456\n","[Training Epoch 2] Batch 2534, Loss 0.48093631863594055\n","[Training Epoch 2] Batch 2535, Loss 0.4915921986103058\n","[Training Epoch 2] Batch 2536, Loss 0.5089824795722961\n","[Training Epoch 2] Batch 2537, Loss 0.499197393655777\n","[Training Epoch 2] Batch 2538, Loss 0.5195938348770142\n","[Training Epoch 2] Batch 2539, Loss 0.5094990134239197\n","[Training Epoch 2] Batch 2540, Loss 0.5083043575286865\n","[Training Epoch 2] Batch 2541, Loss 0.5175455212593079\n","[Training Epoch 2] Batch 2542, Loss 0.5016360282897949\n","[Training Epoch 2] Batch 2543, Loss 0.5223426818847656\n","[Training Epoch 2] Batch 2544, Loss 0.45543110370635986\n","[Training Epoch 2] Batch 2545, Loss 0.497353196144104\n","[Training Epoch 2] Batch 2546, Loss 0.48316752910614014\n","[Training Epoch 2] Batch 2547, Loss 0.4745826721191406\n","[Training Epoch 2] Batch 2548, Loss 0.4722437560558319\n","[Training Epoch 2] Batch 2549, Loss 0.4630237817764282\n","[Training Epoch 2] Batch 2550, Loss 0.5030734539031982\n","[Training Epoch 2] Batch 2551, Loss 0.4855240285396576\n","[Training Epoch 2] Batch 2552, Loss 0.5008276700973511\n","[Training Epoch 2] Batch 2553, Loss 0.4969242513179779\n","[Training Epoch 2] Batch 2554, Loss 0.487316370010376\n","[Training Epoch 2] Batch 2555, Loss 0.4910910725593567\n","[Training Epoch 2] Batch 2556, Loss 0.47223007678985596\n","[Training Epoch 2] Batch 2557, Loss 0.48222559690475464\n","[Training Epoch 2] Batch 2558, Loss 0.4791713356971741\n","[Training Epoch 2] Batch 2559, Loss 0.47504234313964844\n","[Training Epoch 2] Batch 2560, Loss 0.5011117458343506\n","[Training Epoch 2] Batch 2561, Loss 0.47840163111686707\n","[Training Epoch 2] Batch 2562, Loss 0.48549386858940125\n","[Training Epoch 2] Batch 2563, Loss 0.48248571157455444\n","[Training Epoch 2] Batch 2564, Loss 0.46040529012680054\n","[Training Epoch 2] Batch 2565, Loss 0.5084841847419739\n","[Training Epoch 2] Batch 2566, Loss 0.4803276062011719\n","[Training Epoch 2] Batch 2567, Loss 0.4934820234775543\n","[Training Epoch 2] Batch 2568, Loss 0.4943777620792389\n","[Training Epoch 2] Batch 2569, Loss 0.4910619258880615\n","[Training Epoch 2] Batch 2570, Loss 0.5031628608703613\n","[Training Epoch 2] Batch 2571, Loss 0.5170507431030273\n","[Training Epoch 2] Batch 2572, Loss 0.5023605823516846\n","[Training Epoch 2] Batch 2573, Loss 0.4958309233188629\n","[Training Epoch 2] Batch 2574, Loss 0.4959269165992737\n","[Training Epoch 2] Batch 2575, Loss 0.45921972393989563\n","[Training Epoch 2] Batch 2576, Loss 0.4957515597343445\n","[Training Epoch 2] Batch 2577, Loss 0.4577251076698303\n","[Training Epoch 2] Batch 2578, Loss 0.4943901300430298\n","[Training Epoch 2] Batch 2579, Loss 0.481434166431427\n","[Training Epoch 2] Batch 2580, Loss 0.501895010471344\n","[Training Epoch 2] Batch 2581, Loss 0.5184273719787598\n","[Training Epoch 2] Batch 2582, Loss 0.4700847864151001\n","[Training Epoch 2] Batch 2583, Loss 0.4814132750034332\n","[Training Epoch 2] Batch 2584, Loss 0.48628488183021545\n","[Training Epoch 2] Batch 2585, Loss 0.45407676696777344\n","[Training Epoch 2] Batch 2586, Loss 0.4878087639808655\n","[Training Epoch 2] Batch 2587, Loss 0.49978089332580566\n","[Training Epoch 2] Batch 2588, Loss 0.4997364282608032\n","[Training Epoch 2] Batch 2589, Loss 0.4796667993068695\n","[Training Epoch 2] Batch 2590, Loss 0.4959064722061157\n","[Training Epoch 2] Batch 2591, Loss 0.5225681066513062\n","[Training Epoch 2] Batch 2592, Loss 0.4713621437549591\n","[Training Epoch 2] Batch 2593, Loss 0.4755867123603821\n","[Training Epoch 2] Batch 2594, Loss 0.4737843871116638\n","[Training Epoch 2] Batch 2595, Loss 0.47774752974510193\n","[Training Epoch 2] Batch 2596, Loss 0.4871683418750763\n","[Training Epoch 2] Batch 2597, Loss 0.4966126084327698\n","[Training Epoch 2] Batch 2598, Loss 0.49802064895629883\n","[Training Epoch 2] Batch 2599, Loss 0.5076470375061035\n","[Training Epoch 2] Batch 2600, Loss 0.4960298240184784\n","[Training Epoch 2] Batch 2601, Loss 0.4571602940559387\n","[Training Epoch 2] Batch 2602, Loss 0.49263301491737366\n","[Training Epoch 2] Batch 2603, Loss 0.5077541470527649\n","[Training Epoch 2] Batch 2604, Loss 0.4939810037612915\n","[Training Epoch 2] Batch 2605, Loss 0.48807191848754883\n","[Training Epoch 2] Batch 2606, Loss 0.5089613199234009\n","[Training Epoch 2] Batch 2607, Loss 0.5202393531799316\n","[Training Epoch 2] Batch 2608, Loss 0.4854182004928589\n","[Training Epoch 2] Batch 2609, Loss 0.5076786279678345\n","[Training Epoch 2] Batch 2610, Loss 0.45653772354125977\n","[Training Epoch 2] Batch 2611, Loss 0.4969937801361084\n","[Training Epoch 2] Batch 2612, Loss 0.4785574972629547\n","[Training Epoch 2] Batch 2613, Loss 0.49183619022369385\n","[Training Epoch 2] Batch 2614, Loss 0.5225821733474731\n","[Training Epoch 2] Batch 2615, Loss 0.4551469087600708\n","[Training Epoch 2] Batch 2616, Loss 0.4732152819633484\n","[Training Epoch 2] Batch 2617, Loss 0.5174221396446228\n","[Training Epoch 2] Batch 2618, Loss 0.4969549775123596\n","[Training Epoch 2] Batch 2619, Loss 0.4831081032752991\n","[Training Epoch 2] Batch 2620, Loss 0.4661802351474762\n","[Training Epoch 2] Batch 2621, Loss 0.4905344843864441\n","[Training Epoch 2] Batch 2622, Loss 0.5261698961257935\n","[Training Epoch 2] Batch 2623, Loss 0.4514070153236389\n","[Training Epoch 2] Batch 2624, Loss 0.5081910490989685\n","[Training Epoch 2] Batch 2625, Loss 0.4797932505607605\n","[Training Epoch 2] Batch 2626, Loss 0.5024416446685791\n","[Training Epoch 2] Batch 2627, Loss 0.5157320499420166\n","[Training Epoch 2] Batch 2628, Loss 0.4892685115337372\n","[Training Epoch 2] Batch 2629, Loss 0.4758569002151489\n","[Training Epoch 2] Batch 2630, Loss 0.49271464347839355\n","[Training Epoch 2] Batch 2631, Loss 0.47893092036247253\n","[Training Epoch 2] Batch 2632, Loss 0.5069707632064819\n","[Training Epoch 2] Batch 2633, Loss 0.4649079442024231\n","[Training Epoch 2] Batch 2634, Loss 0.49485108256340027\n","[Training Epoch 2] Batch 2635, Loss 0.5021896958351135\n","[Training Epoch 2] Batch 2636, Loss 0.5033446550369263\n","[Training Epoch 2] Batch 2637, Loss 0.49623602628707886\n","[Training Epoch 2] Batch 2638, Loss 0.506376326084137\n","[Training Epoch 2] Batch 2639, Loss 0.5050817131996155\n","[Training Epoch 2] Batch 2640, Loss 0.5094459056854248\n","[Training Epoch 2] Batch 2641, Loss 0.4942425787448883\n","[Training Epoch 2] Batch 2642, Loss 0.4847615361213684\n","[Training Epoch 2] Batch 2643, Loss 0.5110772848129272\n","[Training Epoch 2] Batch 2644, Loss 0.4798407554626465\n","[Training Epoch 2] Batch 2645, Loss 0.4725331664085388\n","[Training Epoch 2] Batch 2646, Loss 0.488080769777298\n","[Training Epoch 2] Batch 2647, Loss 0.4721750319004059\n","[Training Epoch 2] Batch 2648, Loss 0.45737969875335693\n","[Training Epoch 2] Batch 2649, Loss 0.4670478105545044\n","[Training Epoch 2] Batch 2650, Loss 0.4963247776031494\n","[Training Epoch 2] Batch 2651, Loss 0.49409568309783936\n","[Training Epoch 2] Batch 2652, Loss 0.489955335855484\n","[Training Epoch 2] Batch 2653, Loss 0.4356103539466858\n","[Training Epoch 2] Batch 2654, Loss 0.49270322918891907\n","[Training Epoch 2] Batch 2655, Loss 0.48373839259147644\n","[Training Epoch 2] Batch 2656, Loss 0.49470841884613037\n","[Training Epoch 2] Batch 2657, Loss 0.483478307723999\n","[Training Epoch 2] Batch 2658, Loss 0.4936642646789551\n","[Training Epoch 2] Batch 2659, Loss 0.5027550458908081\n","[Training Epoch 2] Batch 2660, Loss 0.5041540861129761\n","[Training Epoch 2] Batch 2661, Loss 0.5050647258758545\n","[Training Epoch 2] Batch 2662, Loss 0.4799501895904541\n","[Training Epoch 2] Batch 2663, Loss 0.46719759702682495\n","[Training Epoch 2] Batch 2664, Loss 0.49332478642463684\n","[Training Epoch 2] Batch 2665, Loss 0.49003565311431885\n","[Training Epoch 2] Batch 2666, Loss 0.46474671363830566\n","[Training Epoch 2] Batch 2667, Loss 0.499794602394104\n","[Training Epoch 2] Batch 2668, Loss 0.48536741733551025\n","[Training Epoch 2] Batch 2669, Loss 0.5075037479400635\n","[Training Epoch 2] Batch 2670, Loss 0.5127061605453491\n","[Training Epoch 2] Batch 2671, Loss 0.5366461277008057\n","[Training Epoch 2] Batch 2672, Loss 0.4874313473701477\n","[Training Epoch 2] Batch 2673, Loss 0.49736401438713074\n","[Training Epoch 2] Batch 2674, Loss 0.5003628134727478\n","[Training Epoch 2] Batch 2675, Loss 0.48094555735588074\n","[Training Epoch 2] Batch 2676, Loss 0.4802096486091614\n","[Training Epoch 2] Batch 2677, Loss 0.45352691411972046\n","[Training Epoch 2] Batch 2678, Loss 0.5024711489677429\n","[Training Epoch 2] Batch 2679, Loss 0.5384566783905029\n","[Training Epoch 2] Batch 2680, Loss 0.48308348655700684\n","[Training Epoch 2] Batch 2681, Loss 0.4699380397796631\n","[Training Epoch 2] Batch 2682, Loss 0.4508463740348816\n","[Training Epoch 2] Batch 2683, Loss 0.47964906692504883\n","[Training Epoch 2] Batch 2684, Loss 0.4860379099845886\n","[Training Epoch 2] Batch 2685, Loss 0.46321696043014526\n","[Training Epoch 2] Batch 2686, Loss 0.49162644147872925\n","[Training Epoch 2] Batch 2687, Loss 0.4916497468948364\n","[Training Epoch 2] Batch 2688, Loss 0.47036540508270264\n","[Training Epoch 2] Batch 2689, Loss 0.504265308380127\n","[Training Epoch 2] Batch 2690, Loss 0.475490927696228\n","[Training Epoch 2] Batch 2691, Loss 0.48677924275398254\n","[Training Epoch 2] Batch 2692, Loss 0.49437427520751953\n","[Training Epoch 2] Batch 2693, Loss 0.49006104469299316\n","[Training Epoch 2] Batch 2694, Loss 0.4883066415786743\n","[Training Epoch 2] Batch 2695, Loss 0.47845175862312317\n","[Training Epoch 2] Batch 2696, Loss 0.5171898603439331\n","[Training Epoch 2] Batch 2697, Loss 0.4919106960296631\n","[Training Epoch 2] Batch 2698, Loss 0.4647044539451599\n","[Training Epoch 2] Batch 2699, Loss 0.4931638240814209\n","[Training Epoch 2] Batch 2700, Loss 0.5045112371444702\n","[Training Epoch 2] Batch 2701, Loss 0.5019757747650146\n","[Training Epoch 2] Batch 2702, Loss 0.4848325848579407\n","[Training Epoch 2] Batch 2703, Loss 0.4836542010307312\n","[Training Epoch 2] Batch 2704, Loss 0.47941434383392334\n","[Training Epoch 2] Batch 2705, Loss 0.5005699396133423\n","[Training Epoch 2] Batch 2706, Loss 0.5057271122932434\n","[Training Epoch 2] Batch 2707, Loss 0.4759584665298462\n","[Training Epoch 2] Batch 2708, Loss 0.48529091477394104\n","[Training Epoch 2] Batch 2709, Loss 0.5158103108406067\n","[Training Epoch 2] Batch 2710, Loss 0.46940910816192627\n","[Training Epoch 2] Batch 2711, Loss 0.5051237344741821\n","[Training Epoch 2] Batch 2712, Loss 0.4786730706691742\n","[Training Epoch 2] Batch 2713, Loss 0.48590826988220215\n","[Training Epoch 2] Batch 2714, Loss 0.4867047667503357\n","[Training Epoch 2] Batch 2715, Loss 0.4662158787250519\n","[Training Epoch 2] Batch 2716, Loss 0.4712013900279999\n","[Training Epoch 2] Batch 2717, Loss 0.4918840229511261\n","[Training Epoch 2] Batch 2718, Loss 0.5118176937103271\n","[Training Epoch 2] Batch 2719, Loss 0.4784187078475952\n","[Training Epoch 2] Batch 2720, Loss 0.4785757064819336\n","[Training Epoch 2] Batch 2721, Loss 0.4964541792869568\n","[Training Epoch 2] Batch 2722, Loss 0.4808494746685028\n","[Training Epoch 2] Batch 2723, Loss 0.47888118028640747\n","[Training Epoch 2] Batch 2724, Loss 0.4678133726119995\n","[Training Epoch 2] Batch 2725, Loss 0.5249608755111694\n","[Training Epoch 2] Batch 2726, Loss 0.4935646057128906\n","[Training Epoch 2] Batch 2727, Loss 0.46757596731185913\n","[Training Epoch 2] Batch 2728, Loss 0.4941900074481964\n","[Training Epoch 2] Batch 2729, Loss 0.5030245780944824\n","[Training Epoch 2] Batch 2730, Loss 0.46721577644348145\n","[Training Epoch 2] Batch 2731, Loss 0.5065670013427734\n","[Training Epoch 2] Batch 2732, Loss 0.44489866495132446\n","[Training Epoch 2] Batch 2733, Loss 0.4924865961074829\n","[Training Epoch 2] Batch 2734, Loss 0.4818602204322815\n","[Training Epoch 2] Batch 2735, Loss 0.49169498682022095\n","[Training Epoch 2] Batch 2736, Loss 0.4743596911430359\n","[Training Epoch 2] Batch 2737, Loss 0.4827740788459778\n","[Training Epoch 2] Batch 2738, Loss 0.5260205864906311\n","[Training Epoch 2] Batch 2739, Loss 0.4792780876159668\n","[Training Epoch 2] Batch 2740, Loss 0.47123032808303833\n","[Training Epoch 2] Batch 2741, Loss 0.4686111807823181\n","[Training Epoch 2] Batch 2742, Loss 0.4804569482803345\n","[Training Epoch 2] Batch 2743, Loss 0.49193018674850464\n","[Training Epoch 2] Batch 2744, Loss 0.5097177028656006\n","[Training Epoch 2] Batch 2745, Loss 0.46852195262908936\n","[Training Epoch 2] Batch 2746, Loss 0.48985493183135986\n","[Training Epoch 2] Batch 2747, Loss 0.49533504247665405\n","[Training Epoch 2] Batch 2748, Loss 0.48184794187545776\n","[Training Epoch 2] Batch 2749, Loss 0.47432130575180054\n","[Training Epoch 2] Batch 2750, Loss 0.48430782556533813\n","[Training Epoch 2] Batch 2751, Loss 0.458117812871933\n","[Training Epoch 2] Batch 2752, Loss 0.5140687227249146\n","[Training Epoch 2] Batch 2753, Loss 0.5110833048820496\n","[Training Epoch 2] Batch 2754, Loss 0.4521936774253845\n","[Training Epoch 2] Batch 2755, Loss 0.49203646183013916\n","[Training Epoch 2] Batch 2756, Loss 0.4605450928211212\n","[Training Epoch 2] Batch 2757, Loss 0.4924011528491974\n","[Training Epoch 2] Batch 2758, Loss 0.4694860577583313\n","[Training Epoch 2] Batch 2759, Loss 0.49656081199645996\n","[Training Epoch 2] Batch 2760, Loss 0.4718208312988281\n","[Training Epoch 2] Batch 2761, Loss 0.4709625244140625\n","[Training Epoch 2] Batch 2762, Loss 0.46219366788864136\n","[Training Epoch 2] Batch 2763, Loss 0.49276623129844666\n","[Training Epoch 2] Batch 2764, Loss 0.4803128242492676\n","[Training Epoch 2] Batch 2765, Loss 0.49736180901527405\n","[Training Epoch 2] Batch 2766, Loss 0.47258198261260986\n","[Training Epoch 2] Batch 2767, Loss 0.4975302219390869\n","[Training Epoch 2] Batch 2768, Loss 0.5000198483467102\n","[Training Epoch 2] Batch 2769, Loss 0.5079053044319153\n","[Training Epoch 2] Batch 2770, Loss 0.48329728841781616\n","[Training Epoch 2] Batch 2771, Loss 0.4775429368019104\n","[Training Epoch 2] Batch 2772, Loss 0.49661728739738464\n","[Training Epoch 2] Batch 2773, Loss 0.48669344186782837\n","[Training Epoch 2] Batch 2774, Loss 0.4753640294075012\n","[Training Epoch 2] Batch 2775, Loss 0.5078149437904358\n","[Training Epoch 2] Batch 2776, Loss 0.5054218769073486\n","[Training Epoch 2] Batch 2777, Loss 0.46377336978912354\n","[Training Epoch 2] Batch 2778, Loss 0.4891034662723541\n","[Training Epoch 2] Batch 2779, Loss 0.500957727432251\n","[Training Epoch 2] Batch 2780, Loss 0.522079348564148\n","[Training Epoch 2] Batch 2781, Loss 0.4810001254081726\n","[Training Epoch 2] Batch 2782, Loss 0.5046939849853516\n","[Training Epoch 2] Batch 2783, Loss 0.4893806576728821\n","[Training Epoch 2] Batch 2784, Loss 0.48654890060424805\n","[Training Epoch 2] Batch 2785, Loss 0.48105350136756897\n","[Training Epoch 2] Batch 2786, Loss 0.4748576283454895\n","[Training Epoch 2] Batch 2787, Loss 0.4803144335746765\n","[Training Epoch 2] Batch 2788, Loss 0.48488402366638184\n","[Training Epoch 2] Batch 2789, Loss 0.47900789976119995\n","[Training Epoch 2] Batch 2790, Loss 0.49548372626304626\n","[Training Epoch 2] Batch 2791, Loss 0.5091978311538696\n","[Training Epoch 2] Batch 2792, Loss 0.49737706780433655\n","[Training Epoch 2] Batch 2793, Loss 0.48395201563835144\n","[Training Epoch 2] Batch 2794, Loss 0.5311731100082397\n","[Training Epoch 2] Batch 2795, Loss 0.48508793115615845\n","[Training Epoch 2] Batch 2796, Loss 0.49597224593162537\n","[Training Epoch 2] Batch 2797, Loss 0.4686570167541504\n","[Training Epoch 2] Batch 2798, Loss 0.48301613330841064\n","[Training Epoch 2] Batch 2799, Loss 0.5158570408821106\n","[Training Epoch 2] Batch 2800, Loss 0.5127723217010498\n","[Training Epoch 2] Batch 2801, Loss 0.48634037375450134\n","[Training Epoch 2] Batch 2802, Loss 0.4751962125301361\n","[Training Epoch 2] Batch 2803, Loss 0.5269116759300232\n","[Training Epoch 2] Batch 2804, Loss 0.5000208616256714\n","[Training Epoch 2] Batch 2805, Loss 0.47995129227638245\n","[Training Epoch 2] Batch 2806, Loss 0.4793664813041687\n","[Training Epoch 2] Batch 2807, Loss 0.4715006351470947\n","[Training Epoch 2] Batch 2808, Loss 0.5124673247337341\n","[Training Epoch 2] Batch 2809, Loss 0.47407853603363037\n","[Training Epoch 2] Batch 2810, Loss 0.47726231813430786\n","[Training Epoch 2] Batch 2811, Loss 0.4876987338066101\n","[Training Epoch 2] Batch 2812, Loss 0.5132992267608643\n","[Training Epoch 2] Batch 2813, Loss 0.5063091516494751\n","[Training Epoch 2] Batch 2814, Loss 0.5244011282920837\n","[Training Epoch 2] Batch 2815, Loss 0.4807954728603363\n","[Training Epoch 2] Batch 2816, Loss 0.4986633360385895\n","[Training Epoch 2] Batch 2817, Loss 0.48529958724975586\n","[Training Epoch 2] Batch 2818, Loss 0.47442102432250977\n","[Training Epoch 2] Batch 2819, Loss 0.467367947101593\n","[Training Epoch 2] Batch 2820, Loss 0.5002857446670532\n","[Training Epoch 2] Batch 2821, Loss 0.47957903146743774\n","[Training Epoch 2] Batch 2822, Loss 0.4763922393321991\n","[Training Epoch 2] Batch 2823, Loss 0.4642864167690277\n","[Training Epoch 2] Batch 2824, Loss 0.47777682542800903\n","[Training Epoch 2] Batch 2825, Loss 0.46013301610946655\n","[Training Epoch 2] Batch 2826, Loss 0.46429622173309326\n","[Training Epoch 2] Batch 2827, Loss 0.5092410445213318\n","[Training Epoch 2] Batch 2828, Loss 0.47298479080200195\n","[Training Epoch 2] Batch 2829, Loss 0.4707632064819336\n","[Training Epoch 2] Batch 2830, Loss 0.49427488446235657\n","[Training Epoch 2] Batch 2831, Loss 0.48321640491485596\n","[Training Epoch 2] Batch 2832, Loss 0.46174025535583496\n","[Training Epoch 2] Batch 2833, Loss 0.46951740980148315\n","[Training Epoch 2] Batch 2834, Loss 0.4880834221839905\n","[Training Epoch 2] Batch 2835, Loss 0.48320242762565613\n","[Training Epoch 2] Batch 2836, Loss 0.4839280843734741\n","[Training Epoch 2] Batch 2837, Loss 0.4603492021560669\n","[Training Epoch 2] Batch 2838, Loss 0.4983408451080322\n","[Training Epoch 2] Batch 2839, Loss 0.4907371401786804\n","[Training Epoch 2] Batch 2840, Loss 0.4688333570957184\n","[Training Epoch 2] Batch 2841, Loss 0.47851982712745667\n","[Training Epoch 2] Batch 2842, Loss 0.4948631823062897\n","[Training Epoch 2] Batch 2843, Loss 0.4739544987678528\n","[Training Epoch 2] Batch 2844, Loss 0.4577277898788452\n","[Training Epoch 2] Batch 2845, Loss 0.4907570481300354\n","[Training Epoch 2] Batch 2846, Loss 0.4771214723587036\n","[Training Epoch 2] Batch 2847, Loss 0.4737648367881775\n","[Training Epoch 2] Batch 2848, Loss 0.4650815427303314\n","[Training Epoch 2] Batch 2849, Loss 0.45286887884140015\n","[Training Epoch 2] Batch 2850, Loss 0.4516019821166992\n","[Training Epoch 2] Batch 2851, Loss 0.4792570471763611\n","[Training Epoch 2] Batch 2852, Loss 0.49401816725730896\n","[Training Epoch 2] Batch 2853, Loss 0.5049566626548767\n","[Training Epoch 2] Batch 2854, Loss 0.4942614436149597\n","[Training Epoch 2] Batch 2855, Loss 0.48725590109825134\n","[Training Epoch 2] Batch 2856, Loss 0.4860415458679199\n","[Training Epoch 2] Batch 2857, Loss 0.48883870244026184\n","[Training Epoch 2] Batch 2858, Loss 0.47367769479751587\n","[Training Epoch 2] Batch 2859, Loss 0.4506833851337433\n","[Training Epoch 2] Batch 2860, Loss 0.5008844137191772\n","[Training Epoch 2] Batch 2861, Loss 0.48573049902915955\n","[Training Epoch 2] Batch 2862, Loss 0.4945918917655945\n","[Training Epoch 2] Batch 2863, Loss 0.46644890308380127\n","[Training Epoch 2] Batch 2864, Loss 0.494717538356781\n","[Training Epoch 2] Batch 2865, Loss 0.5217329263687134\n","[Training Epoch 2] Batch 2866, Loss 0.4835814833641052\n","[Training Epoch 2] Batch 2867, Loss 0.48771506547927856\n","[Training Epoch 2] Batch 2868, Loss 0.4689204692840576\n","[Training Epoch 2] Batch 2869, Loss 0.49406731128692627\n","[Training Epoch 2] Batch 2870, Loss 0.46607476472854614\n","[Training Epoch 2] Batch 2871, Loss 0.4707493185997009\n","[Training Epoch 2] Batch 2872, Loss 0.4858083724975586\n","[Training Epoch 2] Batch 2873, Loss 0.4973914325237274\n","[Training Epoch 2] Batch 2874, Loss 0.48424550890922546\n","[Training Epoch 2] Batch 2875, Loss 0.47966259717941284\n","[Training Epoch 2] Batch 2876, Loss 0.4765503406524658\n","[Training Epoch 2] Batch 2877, Loss 0.5087645053863525\n","[Training Epoch 2] Batch 2878, Loss 0.4795849919319153\n","[Training Epoch 2] Batch 2879, Loss 0.5047277808189392\n","[Training Epoch 2] Batch 2880, Loss 0.4539888799190521\n","[Training Epoch 2] Batch 2881, Loss 0.4955143928527832\n","[Training Epoch 2] Batch 2882, Loss 0.48634234070777893\n","[Training Epoch 2] Batch 2883, Loss 0.47970300912857056\n","[Training Epoch 2] Batch 2884, Loss 0.48831287026405334\n","[Training Epoch 2] Batch 2885, Loss 0.503937840461731\n","[Training Epoch 2] Batch 2886, Loss 0.4614694118499756\n","[Training Epoch 2] Batch 2887, Loss 0.48375916481018066\n","[Training Epoch 2] Batch 2888, Loss 0.4806828200817108\n","[Training Epoch 2] Batch 2889, Loss 0.49678486585617065\n","[Training Epoch 2] Batch 2890, Loss 0.517298698425293\n","[Training Epoch 2] Batch 2891, Loss 0.4779247045516968\n","[Training Epoch 2] Batch 2892, Loss 0.45971059799194336\n","[Training Epoch 2] Batch 2893, Loss 0.48505181074142456\n","[Training Epoch 2] Batch 2894, Loss 0.4775780439376831\n","[Training Epoch 2] Batch 2895, Loss 0.4838917851448059\n","[Training Epoch 2] Batch 2896, Loss 0.48551812767982483\n","[Training Epoch 2] Batch 2897, Loss 0.47900664806365967\n","[Training Epoch 2] Batch 2898, Loss 0.4960487186908722\n","[Training Epoch 2] Batch 2899, Loss 0.4680253267288208\n","[Training Epoch 2] Batch 2900, Loss 0.5216253995895386\n","[Training Epoch 2] Batch 2901, Loss 0.49445444345474243\n","[Training Epoch 2] Batch 2902, Loss 0.5124086141586304\n","[Training Epoch 2] Batch 2903, Loss 0.48636001348495483\n","[Training Epoch 2] Batch 2904, Loss 0.4941399097442627\n","[Training Epoch 2] Batch 2905, Loss 0.4816216826438904\n","[Training Epoch 2] Batch 2906, Loss 0.46745240688323975\n","[Training Epoch 2] Batch 2907, Loss 0.5244063138961792\n","[Training Epoch 2] Batch 2908, Loss 0.4876447021961212\n","[Training Epoch 2] Batch 2909, Loss 0.49698102474212646\n","[Training Epoch 2] Batch 2910, Loss 0.5040210485458374\n","[Training Epoch 2] Batch 2911, Loss 0.4508407711982727\n","[Training Epoch 2] Batch 2912, Loss 0.4703053832054138\n","[Training Epoch 2] Batch 2913, Loss 0.4224681556224823\n","[Training Epoch 2] Batch 2914, Loss 0.5037587881088257\n","[Training Epoch 2] Batch 2915, Loss 0.4793680012226105\n","[Training Epoch 2] Batch 2916, Loss 0.4780092239379883\n","[Training Epoch 2] Batch 2917, Loss 0.4492509365081787\n","[Training Epoch 2] Batch 2918, Loss 0.4724464416503906\n","[Training Epoch 2] Batch 2919, Loss 0.5031425952911377\n","[Training Epoch 2] Batch 2920, Loss 0.49072226881980896\n","[Training Epoch 2] Batch 2921, Loss 0.5077225565910339\n","[Training Epoch 2] Batch 2922, Loss 0.503148078918457\n","[Training Epoch 2] Batch 2923, Loss 0.47534066438674927\n","[Training Epoch 2] Batch 2924, Loss 0.47363871335983276\n","[Training Epoch 2] Batch 2925, Loss 0.48734620213508606\n","[Training Epoch 2] Batch 2926, Loss 0.49844515323638916\n","[Training Epoch 2] Batch 2927, Loss 0.49422407150268555\n","[Training Epoch 2] Batch 2928, Loss 0.4974296987056732\n","[Training Epoch 2] Batch 2929, Loss 0.49986404180526733\n","[Training Epoch 2] Batch 2930, Loss 0.4873715043067932\n","[Training Epoch 2] Batch 2931, Loss 0.4749125838279724\n","[Training Epoch 2] Batch 2932, Loss 0.5216007232666016\n","[Training Epoch 2] Batch 2933, Loss 0.4635930061340332\n","[Training Epoch 2] Batch 2934, Loss 0.45666101574897766\n","[Training Epoch 2] Batch 2935, Loss 0.4890529215335846\n","[Training Epoch 2] Batch 2936, Loss 0.49269983172416687\n","[Training Epoch 2] Batch 2937, Loss 0.5090806484222412\n","[Training Epoch 2] Batch 2938, Loss 0.49217817187309265\n","[Training Epoch 2] Batch 2939, Loss 0.45638880133628845\n","[Training Epoch 2] Batch 2940, Loss 0.49530476331710815\n","[Training Epoch 2] Batch 2941, Loss 0.5045071244239807\n","[Training Epoch 2] Batch 2942, Loss 0.49116671085357666\n","[Training Epoch 2] Batch 2943, Loss 0.5034740567207336\n","[Training Epoch 2] Batch 2944, Loss 0.48790907859802246\n","[Training Epoch 2] Batch 2945, Loss 0.486136257648468\n","[Training Epoch 2] Batch 2946, Loss 0.4751465916633606\n","[Training Epoch 2] Batch 2947, Loss 0.5033941864967346\n","[Training Epoch 2] Batch 2948, Loss 0.5050540566444397\n","[Training Epoch 2] Batch 2949, Loss 0.4823593497276306\n","[Training Epoch 2] Batch 2950, Loss 0.4765896797180176\n","[Training Epoch 2] Batch 2951, Loss 0.44049379229545593\n","[Training Epoch 2] Batch 2952, Loss 0.46431031823158264\n","[Training Epoch 2] Batch 2953, Loss 0.5042796730995178\n","[Training Epoch 2] Batch 2954, Loss 0.4845004081726074\n","[Training Epoch 2] Batch 2955, Loss 0.47956743836402893\n","[Training Epoch 2] Batch 2956, Loss 0.4907427728176117\n","[Training Epoch 2] Batch 2957, Loss 0.493113249540329\n","[Training Epoch 2] Batch 2958, Loss 0.4905225336551666\n","[Training Epoch 2] Batch 2959, Loss 0.5140335559844971\n","[Training Epoch 2] Batch 2960, Loss 0.5068067312240601\n","[Training Epoch 2] Batch 2961, Loss 0.48545271158218384\n","[Training Epoch 2] Batch 2962, Loss 0.4980424642562866\n","[Training Epoch 2] Batch 2963, Loss 0.4743784964084625\n","[Training Epoch 2] Batch 2964, Loss 0.45465701818466187\n","[Training Epoch 2] Batch 2965, Loss 0.45630982518196106\n","[Training Epoch 2] Batch 2966, Loss 0.5140493512153625\n","[Training Epoch 2] Batch 2967, Loss 0.487930029630661\n","[Training Epoch 2] Batch 2968, Loss 0.4869818091392517\n","[Training Epoch 2] Batch 2969, Loss 0.4619867205619812\n","[Training Epoch 2] Batch 2970, Loss 0.4993804097175598\n","[Training Epoch 2] Batch 2971, Loss 0.47635018825531006\n","[Training Epoch 2] Batch 2972, Loss 0.4755209982395172\n","[Training Epoch 2] Batch 2973, Loss 0.46006152033805847\n","[Training Epoch 2] Batch 2974, Loss 0.48527759313583374\n","[Training Epoch 2] Batch 2975, Loss 0.5236701965332031\n","[Training Epoch 2] Batch 2976, Loss 0.4712066948413849\n","[Training Epoch 2] Batch 2977, Loss 0.47311514616012573\n","[Training Epoch 2] Batch 2978, Loss 0.4757496118545532\n","[Training Epoch 2] Batch 2979, Loss 0.4793609380722046\n","[Training Epoch 2] Batch 2980, Loss 0.48510023951530457\n","[Training Epoch 2] Batch 2981, Loss 0.48249348998069763\n","[Training Epoch 2] Batch 2982, Loss 0.5033543705940247\n","[Training Epoch 2] Batch 2983, Loss 0.4985870122909546\n","[Training Epoch 2] Batch 2984, Loss 0.4993048906326294\n","[Training Epoch 2] Batch 2985, Loss 0.4585800766944885\n","[Training Epoch 2] Batch 2986, Loss 0.47789594531059265\n","[Training Epoch 2] Batch 2987, Loss 0.48122984170913696\n","[Training Epoch 2] Batch 2988, Loss 0.492364764213562\n","[Training Epoch 2] Batch 2989, Loss 0.4951198399066925\n","[Training Epoch 2] Batch 2990, Loss 0.4860447645187378\n","[Training Epoch 2] Batch 2991, Loss 0.5086684823036194\n","[Training Epoch 2] Batch 2992, Loss 0.5048796534538269\n","[Training Epoch 2] Batch 2993, Loss 0.48640334606170654\n","[Training Epoch 2] Batch 2994, Loss 0.500432014465332\n","[Training Epoch 2] Batch 2995, Loss 0.4879917502403259\n","[Training Epoch 2] Batch 2996, Loss 0.4607597887516022\n","[Training Epoch 2] Batch 2997, Loss 0.45768189430236816\n","[Training Epoch 2] Batch 2998, Loss 0.45842769742012024\n","[Training Epoch 2] Batch 2999, Loss 0.4852142333984375\n","[Training Epoch 2] Batch 3000, Loss 0.4895015358924866\n","[Training Epoch 2] Batch 3001, Loss 0.5213114023208618\n","[Training Epoch 2] Batch 3002, Loss 0.4884915053844452\n","[Training Epoch 2] Batch 3003, Loss 0.49962231516838074\n","[Training Epoch 2] Batch 3004, Loss 0.514369547367096\n","[Training Epoch 2] Batch 3005, Loss 0.4891270101070404\n","[Training Epoch 2] Batch 3006, Loss 0.4639246463775635\n","[Training Epoch 2] Batch 3007, Loss 0.5058976411819458\n","[Training Epoch 2] Batch 3008, Loss 0.5000067353248596\n","[Training Epoch 2] Batch 3009, Loss 0.47132325172424316\n","[Training Epoch 2] Batch 3010, Loss 0.46560996770858765\n","[Training Epoch 2] Batch 3011, Loss 0.5217216610908508\n","[Training Epoch 2] Batch 3012, Loss 0.4826184809207916\n","[Training Epoch 2] Batch 3013, Loss 0.4564330279827118\n","[Training Epoch 2] Batch 3014, Loss 0.4907636046409607\n","[Training Epoch 2] Batch 3015, Loss 0.4725419282913208\n","[Training Epoch 2] Batch 3016, Loss 0.47168585658073425\n","[Training Epoch 2] Batch 3017, Loss 0.4942222833633423\n","[Training Epoch 2] Batch 3018, Loss 0.49792972207069397\n","[Training Epoch 2] Batch 3019, Loss 0.4872531592845917\n","[Training Epoch 2] Batch 3020, Loss 0.4823169410228729\n","[Training Epoch 2] Batch 3021, Loss 0.518791675567627\n","[Training Epoch 2] Batch 3022, Loss 0.4931734502315521\n","[Training Epoch 2] Batch 3023, Loss 0.4840559661388397\n","[Training Epoch 2] Batch 3024, Loss 0.4806367754936218\n","[Training Epoch 2] Batch 3025, Loss 0.47460973262786865\n","[Training Epoch 2] Batch 3026, Loss 0.47401368618011475\n","[Training Epoch 2] Batch 3027, Loss 0.48263898491859436\n","[Training Epoch 2] Batch 3028, Loss 0.48738688230514526\n","[Training Epoch 2] Batch 3029, Loss 0.4575013518333435\n","[Training Epoch 2] Batch 3030, Loss 0.5057228207588196\n","[Training Epoch 2] Batch 3031, Loss 0.47028976678848267\n","[Training Epoch 2] Batch 3032, Loss 0.49536004662513733\n","[Training Epoch 2] Batch 3033, Loss 0.4626772999763489\n","[Training Epoch 2] Batch 3034, Loss 0.46436798572540283\n","[Training Epoch 2] Batch 3035, Loss 0.46241870522499084\n","[Training Epoch 2] Batch 3036, Loss 0.4891203045845032\n","[Training Epoch 2] Batch 3037, Loss 0.4716625213623047\n","[Training Epoch 2] Batch 3038, Loss 0.49358460307121277\n","[Training Epoch 2] Batch 3039, Loss 0.4822065234184265\n","[Training Epoch 2] Batch 3040, Loss 0.5043768286705017\n","[Training Epoch 2] Batch 3041, Loss 0.5137678384780884\n","[Training Epoch 2] Batch 3042, Loss 0.5123242139816284\n","[Training Epoch 2] Batch 3043, Loss 0.47073498368263245\n","[Training Epoch 2] Batch 3044, Loss 0.47463759779930115\n","[Training Epoch 2] Batch 3045, Loss 0.47691720724105835\n","[Training Epoch 2] Batch 3046, Loss 0.4725162088871002\n","[Training Epoch 2] Batch 3047, Loss 0.4889938235282898\n","[Training Epoch 2] Batch 3048, Loss 0.4626535177230835\n","[Training Epoch 2] Batch 3049, Loss 0.51568603515625\n","[Training Epoch 2] Batch 3050, Loss 0.4980255961418152\n","[Training Epoch 2] Batch 3051, Loss 0.5175573229789734\n","[Training Epoch 2] Batch 3052, Loss 0.47727662324905396\n","[Training Epoch 2] Batch 3053, Loss 0.5068332552909851\n","[Training Epoch 2] Batch 3054, Loss 0.4913705587387085\n","[Training Epoch 2] Batch 3055, Loss 0.48171764612197876\n","[Training Epoch 2] Batch 3056, Loss 0.48060497641563416\n","[Training Epoch 2] Batch 3057, Loss 0.5004326105117798\n","[Training Epoch 2] Batch 3058, Loss 0.4796737730503082\n","[Training Epoch 2] Batch 3059, Loss 0.4807109236717224\n","[Training Epoch 2] Batch 3060, Loss 0.4820379614830017\n","[Training Epoch 2] Batch 3061, Loss 0.4751160740852356\n","[Training Epoch 2] Batch 3062, Loss 0.4530337154865265\n","[Training Epoch 2] Batch 3063, Loss 0.4664008319377899\n","[Training Epoch 2] Batch 3064, Loss 0.4843086302280426\n","[Training Epoch 2] Batch 3065, Loss 0.5007926225662231\n","[Training Epoch 2] Batch 3066, Loss 0.44238942861557007\n","[Training Epoch 2] Batch 3067, Loss 0.4893152713775635\n","[Training Epoch 2] Batch 3068, Loss 0.4961486756801605\n","[Training Epoch 2] Batch 3069, Loss 0.4712717831134796\n","[Training Epoch 2] Batch 3070, Loss 0.49795129895210266\n","[Training Epoch 2] Batch 3071, Loss 0.5182294249534607\n","[Training Epoch 2] Batch 3072, Loss 0.47579386830329895\n","[Training Epoch 2] Batch 3073, Loss 0.452689528465271\n","[Training Epoch 2] Batch 3074, Loss 0.48078036308288574\n","[Training Epoch 2] Batch 3075, Loss 0.4798531234264374\n","[Training Epoch 2] Batch 3076, Loss 0.47554612159729004\n","[Training Epoch 2] Batch 3077, Loss 0.4635177254676819\n","[Training Epoch 2] Batch 3078, Loss 0.497803270816803\n","[Training Epoch 2] Batch 3079, Loss 0.47484877705574036\n","[Training Epoch 2] Batch 3080, Loss 0.4949825406074524\n","[Training Epoch 2] Batch 3081, Loss 0.4728674292564392\n","[Training Epoch 2] Batch 3082, Loss 0.49574771523475647\n","[Training Epoch 2] Batch 3083, Loss 0.4869077205657959\n","[Training Epoch 2] Batch 3084, Loss 0.4791901707649231\n","[Training Epoch 2] Batch 3085, Loss 0.530729353427887\n","[Training Epoch 2] Batch 3086, Loss 0.5030748844146729\n","[Training Epoch 2] Batch 3087, Loss 0.4765017032623291\n","[Training Epoch 2] Batch 3088, Loss 0.4605841636657715\n","[Training Epoch 2] Batch 3089, Loss 0.4622836112976074\n","[Training Epoch 2] Batch 3090, Loss 0.4783986806869507\n","[Training Epoch 2] Batch 3091, Loss 0.5091961622238159\n","[Training Epoch 2] Batch 3092, Loss 0.4797416627407074\n","[Training Epoch 2] Batch 3093, Loss 0.4425696134567261\n","[Training Epoch 2] Batch 3094, Loss 0.4813728928565979\n","[Training Epoch 2] Batch 3095, Loss 0.4527019262313843\n","[Training Epoch 2] Batch 3096, Loss 0.4786773920059204\n","[Training Epoch 2] Batch 3097, Loss 0.4466322064399719\n","[Training Epoch 2] Batch 3098, Loss 0.4963206648826599\n","[Training Epoch 2] Batch 3099, Loss 0.4879728853702545\n","[Training Epoch 2] Batch 3100, Loss 0.4860627055168152\n","[Training Epoch 2] Batch 3101, Loss 0.4737604856491089\n","[Training Epoch 2] Batch 3102, Loss 0.47893190383911133\n","[Training Epoch 2] Batch 3103, Loss 0.4593135714530945\n","[Training Epoch 2] Batch 3104, Loss 0.4925643801689148\n","[Training Epoch 2] Batch 3105, Loss 0.465609610080719\n","[Training Epoch 2] Batch 3106, Loss 0.4608471989631653\n","[Training Epoch 2] Batch 3107, Loss 0.4859382212162018\n","[Training Epoch 2] Batch 3108, Loss 0.4860926866531372\n","[Training Epoch 2] Batch 3109, Loss 0.4774569869041443\n","[Training Epoch 2] Batch 3110, Loss 0.4492068290710449\n","[Training Epoch 2] Batch 3111, Loss 0.47654134035110474\n","[Training Epoch 2] Batch 3112, Loss 0.4755912125110626\n","[Training Epoch 2] Batch 3113, Loss 0.47478771209716797\n","[Training Epoch 2] Batch 3114, Loss 0.47699132561683655\n","[Training Epoch 2] Batch 3115, Loss 0.4590955972671509\n","[Training Epoch 2] Batch 3116, Loss 0.46922802925109863\n","[Training Epoch 2] Batch 3117, Loss 0.4729933440685272\n","[Training Epoch 2] Batch 3118, Loss 0.4693065285682678\n","[Training Epoch 2] Batch 3119, Loss 0.4908525347709656\n","[Training Epoch 2] Batch 3120, Loss 0.4716593325138092\n","[Training Epoch 2] Batch 3121, Loss 0.482568621635437\n","[Training Epoch 2] Batch 3122, Loss 0.4732862710952759\n","[Training Epoch 2] Batch 3123, Loss 0.4766935408115387\n","[Training Epoch 2] Batch 3124, Loss 0.45755884051322937\n","[Training Epoch 2] Batch 3125, Loss 0.47973689436912537\n","[Training Epoch 2] Batch 3126, Loss 0.4597061574459076\n","[Training Epoch 2] Batch 3127, Loss 0.4719838500022888\n","[Training Epoch 2] Batch 3128, Loss 0.5004807114601135\n","[Training Epoch 2] Batch 3129, Loss 0.48501908779144287\n","[Training Epoch 2] Batch 3130, Loss 0.481987327337265\n","[Training Epoch 2] Batch 3131, Loss 0.4793570935726166\n","[Training Epoch 2] Batch 3132, Loss 0.5110079646110535\n","[Training Epoch 2] Batch 3133, Loss 0.4515795111656189\n","[Training Epoch 2] Batch 3134, Loss 0.5007923245429993\n","[Training Epoch 2] Batch 3135, Loss 0.4904898405075073\n","[Training Epoch 2] Batch 3136, Loss 0.5087229609489441\n","[Training Epoch 2] Batch 3137, Loss 0.45353788137435913\n","[Training Epoch 2] Batch 3138, Loss 0.46927475929260254\n","[Training Epoch 2] Batch 3139, Loss 0.4749155044555664\n","[Training Epoch 2] Batch 3140, Loss 0.4786284565925598\n","[Training Epoch 2] Batch 3141, Loss 0.4998162090778351\n","[Training Epoch 2] Batch 3142, Loss 0.4600597321987152\n","[Training Epoch 2] Batch 3143, Loss 0.491530179977417\n","[Training Epoch 2] Batch 3144, Loss 0.4904019236564636\n","[Training Epoch 2] Batch 3145, Loss 0.4773188829421997\n","[Training Epoch 2] Batch 3146, Loss 0.48626017570495605\n","[Training Epoch 2] Batch 3147, Loss 0.4794173240661621\n","[Training Epoch 2] Batch 3148, Loss 0.48247823119163513\n","[Training Epoch 2] Batch 3149, Loss 0.5065126419067383\n","[Training Epoch 2] Batch 3150, Loss 0.5044867396354675\n","[Training Epoch 2] Batch 3151, Loss 0.49819526076316833\n","[Training Epoch 2] Batch 3152, Loss 0.4838116466999054\n","[Training Epoch 2] Batch 3153, Loss 0.4611491560935974\n","[Training Epoch 2] Batch 3154, Loss 0.5035934448242188\n","[Training Epoch 2] Batch 3155, Loss 0.4713793992996216\n","[Training Epoch 2] Batch 3156, Loss 0.4862479865550995\n","[Training Epoch 2] Batch 3157, Loss 0.48336008191108704\n","[Training Epoch 2] Batch 3158, Loss 0.4848078489303589\n","[Training Epoch 2] Batch 3159, Loss 0.5039969086647034\n","[Training Epoch 2] Batch 3160, Loss 0.46722590923309326\n","[Training Epoch 2] Batch 3161, Loss 0.4709776043891907\n","[Training Epoch 2] Batch 3162, Loss 0.4670564830303192\n","[Training Epoch 2] Batch 3163, Loss 0.46695566177368164\n","[Training Epoch 2] Batch 3164, Loss 0.493691086769104\n","[Training Epoch 2] Batch 3165, Loss 0.46053797006607056\n","[Training Epoch 2] Batch 3166, Loss 0.4787444472312927\n","[Training Epoch 2] Batch 3167, Loss 0.44736626744270325\n","[Training Epoch 2] Batch 3168, Loss 0.4935181736946106\n","[Training Epoch 2] Batch 3169, Loss 0.49955299496650696\n","[Training Epoch 2] Batch 3170, Loss 0.48776671290397644\n","[Training Epoch 2] Batch 3171, Loss 0.4763963520526886\n","[Training Epoch 2] Batch 3172, Loss 0.4744059443473816\n","[Training Epoch 2] Batch 3173, Loss 0.5020544528961182\n","[Training Epoch 2] Batch 3174, Loss 0.4725790023803711\n","[Training Epoch 2] Batch 3175, Loss 0.4896238446235657\n","[Training Epoch 2] Batch 3176, Loss 0.45269712805747986\n","[Training Epoch 2] Batch 3177, Loss 0.4884657859802246\n","[Training Epoch 2] Batch 3178, Loss 0.4881283640861511\n","[Training Epoch 2] Batch 3179, Loss 0.48761510848999023\n","[Training Epoch 2] Batch 3180, Loss 0.47445887327194214\n","[Training Epoch 2] Batch 3181, Loss 0.525474488735199\n","[Training Epoch 2] Batch 3182, Loss 0.45928478240966797\n","[Training Epoch 2] Batch 3183, Loss 0.5083961486816406\n","[Training Epoch 2] Batch 3184, Loss 0.47603753209114075\n","[Training Epoch 2] Batch 3185, Loss 0.45903271436691284\n","[Training Epoch 2] Batch 3186, Loss 0.4777352511882782\n","[Training Epoch 2] Batch 3187, Loss 0.4721151888370514\n","[Training Epoch 2] Batch 3188, Loss 0.5154000520706177\n","[Training Epoch 2] Batch 3189, Loss 0.470734566450119\n","[Training Epoch 2] Batch 3190, Loss 0.4893966615200043\n","[Training Epoch 2] Batch 3191, Loss 0.4615734815597534\n","[Training Epoch 2] Batch 3192, Loss 0.47214990854263306\n","[Training Epoch 2] Batch 3193, Loss 0.4641118049621582\n","[Training Epoch 2] Batch 3194, Loss 0.5074524283409119\n","[Training Epoch 2] Batch 3195, Loss 0.47538578510284424\n","[Training Epoch 2] Batch 3196, Loss 0.5025655031204224\n","[Training Epoch 2] Batch 3197, Loss 0.4799708127975464\n","[Training Epoch 2] Batch 3198, Loss 0.47266316413879395\n","[Training Epoch 2] Batch 3199, Loss 0.4530618190765381\n","[Training Epoch 2] Batch 3200, Loss 0.49709099531173706\n","[Training Epoch 2] Batch 3201, Loss 0.47190284729003906\n","[Training Epoch 2] Batch 3202, Loss 0.4801199734210968\n","[Training Epoch 2] Batch 3203, Loss 0.4670770466327667\n","[Training Epoch 2] Batch 3204, Loss 0.49858638644218445\n","[Training Epoch 2] Batch 3205, Loss 0.4861879050731659\n","[Training Epoch 2] Batch 3206, Loss 0.4904777407646179\n","[Training Epoch 2] Batch 3207, Loss 0.4433911442756653\n","[Training Epoch 2] Batch 3208, Loss 0.49137189984321594\n","[Training Epoch 2] Batch 3209, Loss 0.460934042930603\n","[Training Epoch 2] Batch 3210, Loss 0.5043617486953735\n","[Training Epoch 2] Batch 3211, Loss 0.49524247646331787\n","[Training Epoch 2] Batch 3212, Loss 0.4690665602684021\n","[Training Epoch 2] Batch 3213, Loss 0.442149817943573\n","[Training Epoch 2] Batch 3214, Loss 0.48290133476257324\n","[Training Epoch 2] Batch 3215, Loss 0.47026532888412476\n","[Training Epoch 2] Batch 3216, Loss 0.48152709007263184\n","[Training Epoch 2] Batch 3217, Loss 0.502021074295044\n","[Training Epoch 2] Batch 3218, Loss 0.4903801679611206\n","[Training Epoch 2] Batch 3219, Loss 0.48769906163215637\n","[Training Epoch 2] Batch 3220, Loss 0.472597599029541\n","[Training Epoch 2] Batch 3221, Loss 0.4609355330467224\n","[Training Epoch 2] Batch 3222, Loss 0.4788878858089447\n","[Training Epoch 2] Batch 3223, Loss 0.49693387746810913\n","[Training Epoch 2] Batch 3224, Loss 0.4712027907371521\n","[Training Epoch 2] Batch 3225, Loss 0.48773860931396484\n","[Training Epoch 2] Batch 3226, Loss 0.47259438037872314\n","[Training Epoch 2] Batch 3227, Loss 0.4749126136302948\n","[Training Epoch 2] Batch 3228, Loss 0.49697545170783997\n","[Training Epoch 2] Batch 3229, Loss 0.4840981364250183\n","[Training Epoch 2] Batch 3230, Loss 0.4683482050895691\n","[Training Epoch 2] Batch 3231, Loss 0.4618620276451111\n","[Training Epoch 2] Batch 3232, Loss 0.46545758843421936\n","[Training Epoch 2] Batch 3233, Loss 0.5039139986038208\n","[Training Epoch 2] Batch 3234, Loss 0.47914212942123413\n","[Training Epoch 2] Batch 3235, Loss 0.47465431690216064\n","[Training Epoch 2] Batch 3236, Loss 0.5034182071685791\n","[Training Epoch 2] Batch 3237, Loss 0.48107752203941345\n","[Training Epoch 2] Batch 3238, Loss 0.5061680674552917\n","[Training Epoch 2] Batch 3239, Loss 0.4623763859272003\n","[Training Epoch 2] Batch 3240, Loss 0.47758129239082336\n","[Training Epoch 2] Batch 3241, Loss 0.4917895793914795\n","[Training Epoch 2] Batch 3242, Loss 0.4727538824081421\n","[Training Epoch 2] Batch 3243, Loss 0.49604377150535583\n","[Training Epoch 2] Batch 3244, Loss 0.43841272592544556\n","[Training Epoch 2] Batch 3245, Loss 0.476409375667572\n","[Training Epoch 2] Batch 3246, Loss 0.45545196533203125\n","[Training Epoch 2] Batch 3247, Loss 0.46366459131240845\n","[Training Epoch 2] Batch 3248, Loss 0.4630221128463745\n","[Training Epoch 2] Batch 3249, Loss 0.48311305046081543\n","[Training Epoch 2] Batch 3250, Loss 0.4690118432044983\n","[Training Epoch 2] Batch 3251, Loss 0.4906529188156128\n","[Training Epoch 2] Batch 3252, Loss 0.48396003246307373\n","[Training Epoch 2] Batch 3253, Loss 0.4771411418914795\n","[Training Epoch 2] Batch 3254, Loss 0.4688676595687866\n","[Training Epoch 2] Batch 3255, Loss 0.4460034668445587\n","[Training Epoch 2] Batch 3256, Loss 0.49512040615081787\n","[Training Epoch 2] Batch 3257, Loss 0.46455487608909607\n","[Training Epoch 2] Batch 3258, Loss 0.48303765058517456\n","[Training Epoch 2] Batch 3259, Loss 0.48088037967681885\n","[Training Epoch 2] Batch 3260, Loss 0.4877277612686157\n","[Training Epoch 2] Batch 3261, Loss 0.4771505296230316\n","[Training Epoch 2] Batch 3262, Loss 0.4589254856109619\n","[Training Epoch 2] Batch 3263, Loss 0.46936339139938354\n","[Training Epoch 2] Batch 3264, Loss 0.4854443669319153\n","[Training Epoch 2] Batch 3265, Loss 0.4718828499317169\n","[Training Epoch 2] Batch 3266, Loss 0.49821728467941284\n","[Training Epoch 2] Batch 3267, Loss 0.5093488693237305\n","[Training Epoch 2] Batch 3268, Loss 0.4929591715335846\n","[Training Epoch 2] Batch 3269, Loss 0.507641077041626\n","[Training Epoch 2] Batch 3270, Loss 0.498139888048172\n","[Training Epoch 2] Batch 3271, Loss 0.4811651110649109\n","[Training Epoch 2] Batch 3272, Loss 0.4670484662055969\n","[Training Epoch 2] Batch 3273, Loss 0.4805018901824951\n","[Training Epoch 2] Batch 3274, Loss 0.47054344415664673\n","[Training Epoch 2] Batch 3275, Loss 0.47572702169418335\n","[Training Epoch 2] Batch 3276, Loss 0.44876566529273987\n","[Training Epoch 2] Batch 3277, Loss 0.46445906162261963\n","[Training Epoch 2] Batch 3278, Loss 0.47512173652648926\n","[Training Epoch 2] Batch 3279, Loss 0.48199939727783203\n","[Training Epoch 2] Batch 3280, Loss 0.4575841426849365\n","[Training Epoch 2] Batch 3281, Loss 0.5028427243232727\n","[Training Epoch 2] Batch 3282, Loss 0.5138329267501831\n","[Training Epoch 2] Batch 3283, Loss 0.49070629477500916\n","[Training Epoch 2] Batch 3284, Loss 0.4798944890499115\n","[Training Epoch 2] Batch 3285, Loss 0.4730460047721863\n","[Training Epoch 2] Batch 3286, Loss 0.49258261919021606\n","[Training Epoch 2] Batch 3287, Loss 0.49798598885536194\n","[Training Epoch 2] Batch 3288, Loss 0.4672423005104065\n","[Training Epoch 2] Batch 3289, Loss 0.4576435983181\n","[Training Epoch 2] Batch 3290, Loss 0.4916556477546692\n","[Training Epoch 2] Batch 3291, Loss 0.49223679304122925\n","[Training Epoch 2] Batch 3292, Loss 0.4872532784938812\n","[Training Epoch 2] Batch 3293, Loss 0.4876839816570282\n","[Training Epoch 2] Batch 3294, Loss 0.5028006434440613\n","[Training Epoch 2] Batch 3295, Loss 0.5009052157402039\n","[Training Epoch 2] Batch 3296, Loss 0.5176002979278564\n","[Training Epoch 2] Batch 3297, Loss 0.47805628180503845\n","[Training Epoch 2] Batch 3298, Loss 0.5046451091766357\n","[Training Epoch 2] Batch 3299, Loss 0.4825744926929474\n","[Training Epoch 2] Batch 3300, Loss 0.47673624753952026\n","[Training Epoch 2] Batch 3301, Loss 0.46201324462890625\n","[Training Epoch 2] Batch 3302, Loss 0.45254766941070557\n","[Training Epoch 2] Batch 3303, Loss 0.48090463876724243\n","[Training Epoch 2] Batch 3304, Loss 0.4828355610370636\n","[Training Epoch 2] Batch 3305, Loss 0.4739871025085449\n","[Training Epoch 2] Batch 3306, Loss 0.4763262867927551\n","[Training Epoch 2] Batch 3307, Loss 0.5137273073196411\n","[Training Epoch 2] Batch 3308, Loss 0.476051390171051\n","[Training Epoch 2] Batch 3309, Loss 0.46076494455337524\n","[Training Epoch 2] Batch 3310, Loss 0.45011401176452637\n","[Training Epoch 2] Batch 3311, Loss 0.46714484691619873\n","[Training Epoch 2] Batch 3312, Loss 0.47311925888061523\n","[Training Epoch 2] Batch 3313, Loss 0.479853093624115\n","[Training Epoch 2] Batch 3314, Loss 0.49250680208206177\n","[Training Epoch 2] Batch 3315, Loss 0.4900800585746765\n","[Training Epoch 2] Batch 3316, Loss 0.4802514910697937\n","[Training Epoch 2] Batch 3317, Loss 0.4522795081138611\n","[Training Epoch 2] Batch 3318, Loss 0.46156126260757446\n","[Training Epoch 2] Batch 3319, Loss 0.4910961985588074\n","[Training Epoch 2] Batch 3320, Loss 0.46407443284988403\n","[Training Epoch 2] Batch 3321, Loss 0.47536927461624146\n","[Training Epoch 2] Batch 3322, Loss 0.48576819896698\n","[Training Epoch 2] Batch 3323, Loss 0.4913730025291443\n","[Training Epoch 2] Batch 3324, Loss 0.49382832646369934\n","[Training Epoch 2] Batch 3325, Loss 0.4825797379016876\n","[Training Epoch 2] Batch 3326, Loss 0.4557695984840393\n","[Training Epoch 2] Batch 3327, Loss 0.45120829343795776\n","[Training Epoch 2] Batch 3328, Loss 0.4554790258407593\n","[Training Epoch 2] Batch 3329, Loss 0.48228198289871216\n","[Training Epoch 2] Batch 3330, Loss 0.5079452991485596\n","[Training Epoch 2] Batch 3331, Loss 0.45157578587532043\n","[Training Epoch 2] Batch 3332, Loss 0.48629891872406006\n","[Training Epoch 2] Batch 3333, Loss 0.5037475824356079\n","[Training Epoch 2] Batch 3334, Loss 0.47204190492630005\n","[Training Epoch 2] Batch 3335, Loss 0.4559409022331238\n","[Training Epoch 2] Batch 3336, Loss 0.47549664974212646\n","[Training Epoch 2] Batch 3337, Loss 0.4841903746128082\n","[Training Epoch 2] Batch 3338, Loss 0.47307202219963074\n","[Training Epoch 2] Batch 3339, Loss 0.4852411150932312\n","[Training Epoch 2] Batch 3340, Loss 0.4736994206905365\n","[Training Epoch 2] Batch 3341, Loss 0.4918564558029175\n","[Training Epoch 2] Batch 3342, Loss 0.4862651228904724\n","[Training Epoch 2] Batch 3343, Loss 0.48221278190612793\n","[Training Epoch 2] Batch 3344, Loss 0.4945662021636963\n","[Training Epoch 2] Batch 3345, Loss 0.47643738985061646\n","[Training Epoch 2] Batch 3346, Loss 0.45773401856422424\n","[Training Epoch 2] Batch 3347, Loss 0.4770145118236542\n","[Training Epoch 2] Batch 3348, Loss 0.5003371238708496\n","[Training Epoch 2] Batch 3349, Loss 0.49359938502311707\n","[Training Epoch 2] Batch 3350, Loss 0.46764448285102844\n","[Training Epoch 2] Batch 3351, Loss 0.4450110197067261\n","[Training Epoch 2] Batch 3352, Loss 0.5077991485595703\n","[Training Epoch 2] Batch 3353, Loss 0.4817637801170349\n","[Training Epoch 2] Batch 3354, Loss 0.48200350999832153\n","[Training Epoch 2] Batch 3355, Loss 0.5068209767341614\n","[Training Epoch 2] Batch 3356, Loss 0.4604053795337677\n","[Training Epoch 2] Batch 3357, Loss 0.48793381452560425\n","[Training Epoch 2] Batch 3358, Loss 0.49105575680732727\n","[Training Epoch 2] Batch 3359, Loss 0.47376519441604614\n","[Training Epoch 2] Batch 3360, Loss 0.5024698376655579\n","[Training Epoch 2] Batch 3361, Loss 0.47859257459640503\n","[Training Epoch 2] Batch 3362, Loss 0.4774353504180908\n","[Training Epoch 2] Batch 3363, Loss 0.46184664964675903\n","[Training Epoch 2] Batch 3364, Loss 0.4767168462276459\n","[Training Epoch 2] Batch 3365, Loss 0.4860655665397644\n","[Training Epoch 2] Batch 3366, Loss 0.49597153067588806\n","[Training Epoch 2] Batch 3367, Loss 0.49136853218078613\n","[Training Epoch 2] Batch 3368, Loss 0.48116350173950195\n","[Training Epoch 2] Batch 3369, Loss 0.4828808307647705\n","[Training Epoch 2] Batch 3370, Loss 0.47318506240844727\n","[Training Epoch 2] Batch 3371, Loss 0.483307808637619\n","[Training Epoch 2] Batch 3372, Loss 0.5082369446754456\n","[Training Epoch 2] Batch 3373, Loss 0.5059500932693481\n","[Training Epoch 2] Batch 3374, Loss 0.4903086721897125\n","[Training Epoch 2] Batch 3375, Loss 0.504153847694397\n","[Training Epoch 2] Batch 3376, Loss 0.44517433643341064\n","[Training Epoch 2] Batch 3377, Loss 0.47910767793655396\n","[Training Epoch 2] Batch 3378, Loss 0.48298758268356323\n","[Training Epoch 2] Batch 3379, Loss 0.5147340297698975\n","[Training Epoch 2] Batch 3380, Loss 0.4981568455696106\n","[Training Epoch 2] Batch 3381, Loss 0.46405941247940063\n","[Training Epoch 2] Batch 3382, Loss 0.48476308584213257\n","[Training Epoch 2] Batch 3383, Loss 0.4782141149044037\n","[Training Epoch 2] Batch 3384, Loss 0.4684017300605774\n","[Training Epoch 2] Batch 3385, Loss 0.4911193251609802\n","[Training Epoch 2] Batch 3386, Loss 0.48587095737457275\n","[Training Epoch 2] Batch 3387, Loss 0.4648514986038208\n","[Training Epoch 2] Batch 3388, Loss 0.4894914925098419\n","[Training Epoch 2] Batch 3389, Loss 0.4680226445198059\n","[Training Epoch 2] Batch 3390, Loss 0.44728341698646545\n","[Training Epoch 2] Batch 3391, Loss 0.4981767535209656\n","[Training Epoch 2] Batch 3392, Loss 0.49867990612983704\n","[Training Epoch 2] Batch 3393, Loss 0.4828609228134155\n","[Training Epoch 2] Batch 3394, Loss 0.4739782512187958\n","[Training Epoch 2] Batch 3395, Loss 0.47379934787750244\n","[Training Epoch 2] Batch 3396, Loss 0.4892219305038452\n","[Training Epoch 2] Batch 3397, Loss 0.4516184329986572\n","[Training Epoch 2] Batch 3398, Loss 0.4709492027759552\n","[Training Epoch 2] Batch 3399, Loss 0.49200132489204407\n","[Training Epoch 2] Batch 3400, Loss 0.4931485950946808\n","[Training Epoch 2] Batch 3401, Loss 0.4601108431816101\n","[Training Epoch 2] Batch 3402, Loss 0.4565469026565552\n","[Training Epoch 2] Batch 3403, Loss 0.47483572363853455\n","[Training Epoch 2] Batch 3404, Loss 0.48141199350357056\n","[Training Epoch 2] Batch 3405, Loss 0.46952104568481445\n","[Training Epoch 2] Batch 3406, Loss 0.4695267677307129\n","[Training Epoch 2] Batch 3407, Loss 0.48128271102905273\n","[Training Epoch 2] Batch 3408, Loss 0.4485475420951843\n","[Training Epoch 2] Batch 3409, Loss 0.4857209324836731\n","[Training Epoch 2] Batch 3410, Loss 0.4703420102596283\n","[Training Epoch 2] Batch 3411, Loss 0.4379362463951111\n","[Training Epoch 2] Batch 3412, Loss 0.4618130922317505\n","[Training Epoch 2] Batch 3413, Loss 0.4822109639644623\n","[Training Epoch 2] Batch 3414, Loss 0.506664514541626\n","[Training Epoch 2] Batch 3415, Loss 0.48737606406211853\n","[Training Epoch 2] Batch 3416, Loss 0.4543774127960205\n","[Training Epoch 2] Batch 3417, Loss 0.4585404098033905\n","[Training Epoch 2] Batch 3418, Loss 0.45313915610313416\n","[Training Epoch 2] Batch 3419, Loss 0.4909856915473938\n","[Training Epoch 2] Batch 3420, Loss 0.4669550955295563\n","[Training Epoch 2] Batch 3421, Loss 0.4884216785430908\n","[Training Epoch 2] Batch 3422, Loss 0.49202990531921387\n","[Training Epoch 2] Batch 3423, Loss 0.4731760025024414\n","[Training Epoch 2] Batch 3424, Loss 0.4645005464553833\n","[Training Epoch 2] Batch 3425, Loss 0.48228153586387634\n","[Training Epoch 2] Batch 3426, Loss 0.44523823261260986\n","[Training Epoch 2] Batch 3427, Loss 0.44543254375457764\n","[Training Epoch 2] Batch 3428, Loss 0.48915159702301025\n","[Training Epoch 2] Batch 3429, Loss 0.5165673494338989\n","[Training Epoch 2] Batch 3430, Loss 0.4782003164291382\n","[Training Epoch 2] Batch 3431, Loss 0.4859950542449951\n","[Training Epoch 2] Batch 3432, Loss 0.46035468578338623\n","[Training Epoch 2] Batch 3433, Loss 0.5087875127792358\n","[Training Epoch 2] Batch 3434, Loss 0.4833758473396301\n","[Training Epoch 2] Batch 3435, Loss 0.47068944573402405\n","[Training Epoch 2] Batch 3436, Loss 0.47284018993377686\n","[Training Epoch 2] Batch 3437, Loss 0.4793734550476074\n","[Training Epoch 2] Batch 3438, Loss 0.4775189757347107\n","[Training Epoch 2] Batch 3439, Loss 0.4612257480621338\n","[Training Epoch 2] Batch 3440, Loss 0.4592999219894409\n","[Training Epoch 2] Batch 3441, Loss 0.47087690234184265\n","[Training Epoch 2] Batch 3442, Loss 0.46683594584465027\n","[Training Epoch 2] Batch 3443, Loss 0.47054100036621094\n","[Training Epoch 2] Batch 3444, Loss 0.4979569911956787\n","[Training Epoch 2] Batch 3445, Loss 0.47999492287635803\n","[Training Epoch 2] Batch 3446, Loss 0.48687565326690674\n","[Training Epoch 2] Batch 3447, Loss 0.46947747468948364\n","[Training Epoch 2] Batch 3448, Loss 0.4814414381980896\n","[Training Epoch 2] Batch 3449, Loss 0.4926680326461792\n","[Training Epoch 2] Batch 3450, Loss 0.4965510666370392\n","[Training Epoch 2] Batch 3451, Loss 0.47565457224845886\n","[Training Epoch 2] Batch 3452, Loss 0.47233352065086365\n","[Training Epoch 2] Batch 3453, Loss 0.5083320736885071\n","[Training Epoch 2] Batch 3454, Loss 0.46731364727020264\n","[Training Epoch 2] Batch 3455, Loss 0.45335716009140015\n","[Training Epoch 2] Batch 3456, Loss 0.4488099217414856\n","[Training Epoch 2] Batch 3457, Loss 0.45909640192985535\n","[Training Epoch 2] Batch 3458, Loss 0.5039178729057312\n","[Training Epoch 2] Batch 3459, Loss 0.4440358281135559\n","[Training Epoch 2] Batch 3460, Loss 0.48189759254455566\n","[Training Epoch 2] Batch 3461, Loss 0.4480854272842407\n","[Training Epoch 2] Batch 3462, Loss 0.46602773666381836\n","[Training Epoch 2] Batch 3463, Loss 0.493962824344635\n","[Training Epoch 2] Batch 3464, Loss 0.5128263235092163\n","[Training Epoch 2] Batch 3465, Loss 0.47318312525749207\n","[Training Epoch 2] Batch 3466, Loss 0.50542151927948\n","[Training Epoch 2] Batch 3467, Loss 0.46105778217315674\n","[Training Epoch 2] Batch 3468, Loss 0.5288211107254028\n","[Training Epoch 2] Batch 3469, Loss 0.4674954414367676\n","[Training Epoch 2] Batch 3470, Loss 0.4752406179904938\n","[Training Epoch 2] Batch 3471, Loss 0.4623682498931885\n","[Training Epoch 2] Batch 3472, Loss 0.4662196636199951\n","[Training Epoch 2] Batch 3473, Loss 0.4797142446041107\n","[Training Epoch 2] Batch 3474, Loss 0.4691724181175232\n","[Training Epoch 2] Batch 3475, Loss 0.5017110705375671\n","[Training Epoch 2] Batch 3476, Loss 0.4991823732852936\n","[Training Epoch 2] Batch 3477, Loss 0.48250553011894226\n","[Training Epoch 2] Batch 3478, Loss 0.4846978485584259\n","[Training Epoch 2] Batch 3479, Loss 0.4783080816268921\n","[Training Epoch 2] Batch 3480, Loss 0.4878700375556946\n","[Training Epoch 2] Batch 3481, Loss 0.4720297157764435\n","[Training Epoch 2] Batch 3482, Loss 0.4608692526817322\n","[Training Epoch 2] Batch 3483, Loss 0.4778572618961334\n","[Training Epoch 2] Batch 3484, Loss 0.44581806659698486\n","[Training Epoch 2] Batch 3485, Loss 0.4877389371395111\n","[Training Epoch 2] Batch 3486, Loss 0.4608578085899353\n","[Training Epoch 2] Batch 3487, Loss 0.4782717525959015\n","[Training Epoch 2] Batch 3488, Loss 0.4855380654335022\n","[Training Epoch 2] Batch 3489, Loss 0.48646479845046997\n","[Training Epoch 2] Batch 3490, Loss 0.4768625795841217\n","[Training Epoch 2] Batch 3491, Loss 0.48213687539100647\n","[Training Epoch 2] Batch 3492, Loss 0.4975404143333435\n","[Training Epoch 2] Batch 3493, Loss 0.5034030675888062\n","[Training Epoch 2] Batch 3494, Loss 0.4540380835533142\n","[Training Epoch 2] Batch 3495, Loss 0.4938890039920807\n","[Training Epoch 2] Batch 3496, Loss 0.45754557847976685\n","[Training Epoch 2] Batch 3497, Loss 0.4581439197063446\n","[Training Epoch 2] Batch 3498, Loss 0.4589499533176422\n","[Training Epoch 2] Batch 3499, Loss 0.48083749413490295\n","[Training Epoch 2] Batch 3500, Loss 0.4746953248977661\n","[Training Epoch 2] Batch 3501, Loss 0.47098273038864136\n","[Training Epoch 2] Batch 3502, Loss 0.5034648180007935\n","[Training Epoch 2] Batch 3503, Loss 0.4944411814212799\n","[Training Epoch 2] Batch 3504, Loss 0.4461001455783844\n","[Training Epoch 2] Batch 3505, Loss 0.46844446659088135\n","[Training Epoch 2] Batch 3506, Loss 0.4725410044193268\n","[Training Epoch 2] Batch 3507, Loss 0.4284653663635254\n","[Training Epoch 2] Batch 3508, Loss 0.49420055747032166\n","[Training Epoch 2] Batch 3509, Loss 0.48583394289016724\n","[Training Epoch 2] Batch 3510, Loss 0.45036110281944275\n","[Training Epoch 2] Batch 3511, Loss 0.49718743562698364\n","[Training Epoch 2] Batch 3512, Loss 0.4805615246295929\n","[Training Epoch 2] Batch 3513, Loss 0.5035198926925659\n","[Training Epoch 2] Batch 3514, Loss 0.4452728033065796\n","[Training Epoch 2] Batch 3515, Loss 0.4810786843299866\n","[Training Epoch 2] Batch 3516, Loss 0.47811809182167053\n","[Training Epoch 2] Batch 3517, Loss 0.5155754685401917\n","[Training Epoch 2] Batch 3518, Loss 0.454368531703949\n","[Training Epoch 2] Batch 3519, Loss 0.48438113927841187\n","[Training Epoch 2] Batch 3520, Loss 0.45497632026672363\n","[Training Epoch 2] Batch 3521, Loss 0.481479287147522\n","[Training Epoch 2] Batch 3522, Loss 0.4969300329685211\n","[Training Epoch 2] Batch 3523, Loss 0.46400049328804016\n","[Training Epoch 2] Batch 3524, Loss 0.47533929347991943\n","[Training Epoch 2] Batch 3525, Loss 0.4854227900505066\n","[Training Epoch 2] Batch 3526, Loss 0.45804235339164734\n","[Training Epoch 2] Batch 3527, Loss 0.46098440885543823\n","[Training Epoch 2] Batch 3528, Loss 0.48566293716430664\n","[Training Epoch 2] Batch 3529, Loss 0.46201860904693604\n","[Training Epoch 2] Batch 3530, Loss 0.4586045742034912\n","[Training Epoch 2] Batch 3531, Loss 0.4833102822303772\n","[Training Epoch 2] Batch 3532, Loss 0.47131288051605225\n","[Training Epoch 2] Batch 3533, Loss 0.4400062561035156\n","[Training Epoch 2] Batch 3534, Loss 0.468132346868515\n","[Training Epoch 2] Batch 3535, Loss 0.4649237394332886\n","[Training Epoch 2] Batch 3536, Loss 0.4744546711444855\n","[Training Epoch 2] Batch 3537, Loss 0.4455474019050598\n","[Training Epoch 2] Batch 3538, Loss 0.47866255044937134\n","[Training Epoch 2] Batch 3539, Loss 0.504004955291748\n","[Training Epoch 2] Batch 3540, Loss 0.46641138195991516\n","[Training Epoch 2] Batch 3541, Loss 0.46989643573760986\n","[Training Epoch 2] Batch 3542, Loss 0.49260467290878296\n","[Training Epoch 2] Batch 3543, Loss 0.504997730255127\n","[Training Epoch 2] Batch 3544, Loss 0.48301616311073303\n","[Training Epoch 2] Batch 3545, Loss 0.44970211386680603\n","[Training Epoch 2] Batch 3546, Loss 0.4889127016067505\n","[Training Epoch 2] Batch 3547, Loss 0.4765424430370331\n","[Training Epoch 2] Batch 3548, Loss 0.47051888704299927\n","[Training Epoch 2] Batch 3549, Loss 0.4740157127380371\n","[Training Epoch 2] Batch 3550, Loss 0.48491474986076355\n","[Training Epoch 2] Batch 3551, Loss 0.49809902906417847\n","[Training Epoch 2] Batch 3552, Loss 0.4772157371044159\n","[Training Epoch 2] Batch 3553, Loss 0.49046790599823\n","[Training Epoch 2] Batch 3554, Loss 0.46159589290618896\n","[Training Epoch 2] Batch 3555, Loss 0.4701448380947113\n","[Training Epoch 2] Batch 3556, Loss 0.5143228769302368\n","[Training Epoch 2] Batch 3557, Loss 0.47044020891189575\n","[Training Epoch 2] Batch 3558, Loss 0.4728972911834717\n","[Training Epoch 2] Batch 3559, Loss 0.46110183000564575\n","[Training Epoch 2] Batch 3560, Loss 0.46729207038879395\n","[Training Epoch 2] Batch 3561, Loss 0.47571396827697754\n","[Training Epoch 2] Batch 3562, Loss 0.4556378722190857\n","[Training Epoch 2] Batch 3563, Loss 0.49080556631088257\n","[Training Epoch 2] Batch 3564, Loss 0.49226638674736023\n","[Training Epoch 2] Batch 3565, Loss 0.4516946077346802\n","[Training Epoch 2] Batch 3566, Loss 0.4679148495197296\n","[Training Epoch 2] Batch 3567, Loss 0.44593706727027893\n","[Training Epoch 2] Batch 3568, Loss 0.49176836013793945\n","[Training Epoch 2] Batch 3569, Loss 0.4623471796512604\n","[Training Epoch 2] Batch 3570, Loss 0.4825868010520935\n","[Training Epoch 2] Batch 3571, Loss 0.4794137477874756\n","[Training Epoch 2] Batch 3572, Loss 0.4449912905693054\n","[Training Epoch 2] Batch 3573, Loss 0.4953601062297821\n","[Training Epoch 2] Batch 3574, Loss 0.45452815294265747\n","[Training Epoch 2] Batch 3575, Loss 0.4772821068763733\n","[Training Epoch 2] Batch 3576, Loss 0.4826819896697998\n","[Training Epoch 2] Batch 3577, Loss 0.4880497455596924\n","[Training Epoch 2] Batch 3578, Loss 0.4626268148422241\n","[Training Epoch 2] Batch 3579, Loss 0.4626002907752991\n","[Training Epoch 2] Batch 3580, Loss 0.4507567882537842\n","[Training Epoch 2] Batch 3581, Loss 0.502708911895752\n","[Training Epoch 2] Batch 3582, Loss 0.5117318630218506\n","[Training Epoch 2] Batch 3583, Loss 0.46452534198760986\n","[Training Epoch 2] Batch 3584, Loss 0.4680616855621338\n","[Training Epoch 2] Batch 3585, Loss 0.46895748376846313\n","[Training Epoch 2] Batch 3586, Loss 0.46042582392692566\n","[Training Epoch 2] Batch 3587, Loss 0.4478853642940521\n","[Training Epoch 2] Batch 3588, Loss 0.4901953637599945\n","[Training Epoch 2] Batch 3589, Loss 0.4489620327949524\n","[Training Epoch 2] Batch 3590, Loss 0.4815070629119873\n","[Training Epoch 2] Batch 3591, Loss 0.47370606660842896\n","[Training Epoch 2] Batch 3592, Loss 0.5042486190795898\n","[Training Epoch 2] Batch 3593, Loss 0.4646250307559967\n","[Training Epoch 2] Batch 3594, Loss 0.4985123872756958\n","[Training Epoch 2] Batch 3595, Loss 0.4728931188583374\n","[Training Epoch 2] Batch 3596, Loss 0.4567035138607025\n","[Training Epoch 2] Batch 3597, Loss 0.43235498666763306\n","[Training Epoch 2] Batch 3598, Loss 0.47927576303482056\n","[Training Epoch 2] Batch 3599, Loss 0.46946901082992554\n","[Training Epoch 2] Batch 3600, Loss 0.4788080155849457\n","[Training Epoch 2] Batch 3601, Loss 0.4793699085712433\n","[Training Epoch 2] Batch 3602, Loss 0.4775782525539398\n","[Training Epoch 2] Batch 3603, Loss 0.45389896631240845\n","[Training Epoch 2] Batch 3604, Loss 0.4758836328983307\n","[Training Epoch 2] Batch 3605, Loss 0.42802900075912476\n","[Training Epoch 2] Batch 3606, Loss 0.46219807863235474\n","[Training Epoch 2] Batch 3607, Loss 0.4724453389644623\n","[Training Epoch 2] Batch 3608, Loss 0.4759342670440674\n","[Training Epoch 2] Batch 3609, Loss 0.43466025590896606\n","[Training Epoch 2] Batch 3610, Loss 0.4895942807197571\n","[Training Epoch 2] Batch 3611, Loss 0.4565369784832001\n","[Training Epoch 2] Batch 3612, Loss 0.4633329510688782\n","[Training Epoch 2] Batch 3613, Loss 0.4745350182056427\n","[Training Epoch 2] Batch 3614, Loss 0.45801347494125366\n","[Training Epoch 2] Batch 3615, Loss 0.4684304893016815\n","[Training Epoch 2] Batch 3616, Loss 0.4947760999202728\n","[Training Epoch 2] Batch 3617, Loss 0.4921606183052063\n","[Training Epoch 2] Batch 3618, Loss 0.4981684386730194\n","[Training Epoch 2] Batch 3619, Loss 0.4535270035266876\n","[Training Epoch 2] Batch 3620, Loss 0.4661322236061096\n","[Training Epoch 2] Batch 3621, Loss 0.48350340127944946\n","[Training Epoch 2] Batch 3622, Loss 0.4499303698539734\n","[Training Epoch 2] Batch 3623, Loss 0.4851405620574951\n","[Training Epoch 2] Batch 3624, Loss 0.4749857783317566\n","[Training Epoch 2] Batch 3625, Loss 0.45351070165634155\n","[Training Epoch 2] Batch 3626, Loss 0.47897353768348694\n","[Training Epoch 2] Batch 3627, Loss 0.4555276036262512\n","[Training Epoch 2] Batch 3628, Loss 0.46233701705932617\n","[Training Epoch 2] Batch 3629, Loss 0.4783576726913452\n","[Training Epoch 2] Batch 3630, Loss 0.49309250712394714\n","[Training Epoch 2] Batch 3631, Loss 0.4750586152076721\n","[Training Epoch 2] Batch 3632, Loss 0.47052493691444397\n","[Training Epoch 2] Batch 3633, Loss 0.4665451943874359\n","[Training Epoch 2] Batch 3634, Loss 0.5085545182228088\n","[Training Epoch 2] Batch 3635, Loss 0.4666854739189148\n","[Training Epoch 2] Batch 3636, Loss 0.4653814136981964\n","[Training Epoch 2] Batch 3637, Loss 0.4705994427204132\n","[Training Epoch 2] Batch 3638, Loss 0.44319671392440796\n","[Training Epoch 2] Batch 3639, Loss 0.4657896161079407\n","[Training Epoch 2] Batch 3640, Loss 0.4657171964645386\n","[Training Epoch 2] Batch 3641, Loss 0.44429972767829895\n","[Training Epoch 2] Batch 3642, Loss 0.47900712490081787\n","[Training Epoch 2] Batch 3643, Loss 0.4591785669326782\n","[Training Epoch 2] Batch 3644, Loss 0.4606913626194\n","[Training Epoch 2] Batch 3645, Loss 0.45847946405410767\n","[Training Epoch 2] Batch 3646, Loss 0.4576803147792816\n","[Training Epoch 2] Batch 3647, Loss 0.4777156412601471\n","[Training Epoch 2] Batch 3648, Loss 0.46025222539901733\n","[Training Epoch 2] Batch 3649, Loss 0.45460307598114014\n","[Training Epoch 2] Batch 3650, Loss 0.4674144685268402\n","[Training Epoch 2] Batch 3651, Loss 0.4599580466747284\n","[Training Epoch 2] Batch 3652, Loss 0.4701877236366272\n","[Training Epoch 2] Batch 3653, Loss 0.47311440110206604\n","[Training Epoch 2] Batch 3654, Loss 0.45641380548477173\n","[Training Epoch 2] Batch 3655, Loss 0.47279980778694153\n","[Training Epoch 2] Batch 3656, Loss 0.5158571004867554\n","[Training Epoch 2] Batch 3657, Loss 0.45734351873397827\n","[Training Epoch 2] Batch 3658, Loss 0.4502551555633545\n","[Training Epoch 2] Batch 3659, Loss 0.46961766481399536\n","[Training Epoch 2] Batch 3660, Loss 0.44365501403808594\n","[Training Epoch 2] Batch 3661, Loss 0.4650423526763916\n","[Training Epoch 2] Batch 3662, Loss 0.46122390031814575\n","[Training Epoch 2] Batch 3663, Loss 0.48768138885498047\n","[Training Epoch 2] Batch 3664, Loss 0.5022706985473633\n","[Training Epoch 2] Batch 3665, Loss 0.5022098422050476\n","[Training Epoch 2] Batch 3666, Loss 0.44794291257858276\n","[Training Epoch 2] Batch 3667, Loss 0.45963191986083984\n","[Training Epoch 2] Batch 3668, Loss 0.4725375473499298\n","[Training Epoch 2] Batch 3669, Loss 0.476645827293396\n","[Training Epoch 2] Batch 3670, Loss 0.506679892539978\n","[Training Epoch 2] Batch 3671, Loss 0.4804680049419403\n","[Training Epoch 2] Batch 3672, Loss 0.49455690383911133\n","[Training Epoch 2] Batch 3673, Loss 0.4928940534591675\n","[Training Epoch 2] Batch 3674, Loss 0.4453084170818329\n","[Training Epoch 2] Batch 3675, Loss 0.4434320330619812\n","[Training Epoch 2] Batch 3676, Loss 0.4853820204734802\n","[Training Epoch 2] Batch 3677, Loss 0.5044225454330444\n","[Training Epoch 2] Batch 3678, Loss 0.48348891735076904\n","[Training Epoch 2] Batch 3679, Loss 0.4886740446090698\n","[Training Epoch 2] Batch 3680, Loss 0.47008955478668213\n","[Training Epoch 2] Batch 3681, Loss 0.43817591667175293\n","[Training Epoch 2] Batch 3682, Loss 0.46205103397369385\n","[Training Epoch 2] Batch 3683, Loss 0.45585232973098755\n","[Training Epoch 2] Batch 3684, Loss 0.4792884588241577\n","[Training Epoch 2] Batch 3685, Loss 0.4734530448913574\n","[Training Epoch 2] Batch 3686, Loss 0.450717955827713\n","[Training Epoch 2] Batch 3687, Loss 0.46559274196624756\n","[Training Epoch 2] Batch 3688, Loss 0.4511314034461975\n","[Training Epoch 2] Batch 3689, Loss 0.4654221534729004\n","[Training Epoch 2] Batch 3690, Loss 0.4659488797187805\n","[Training Epoch 2] Batch 3691, Loss 0.48707473278045654\n","[Training Epoch 2] Batch 3692, Loss 0.49678492546081543\n","[Training Epoch 2] Batch 3693, Loss 0.47548535466194153\n","[Training Epoch 2] Batch 3694, Loss 0.47090137004852295\n","[Training Epoch 2] Batch 3695, Loss 0.46417364478111267\n","[Training Epoch 2] Batch 3696, Loss 0.48035112023353577\n","[Training Epoch 2] Batch 3697, Loss 0.47548192739486694\n","[Training Epoch 2] Batch 3698, Loss 0.46357452869415283\n","[Training Epoch 2] Batch 3699, Loss 0.4552878737449646\n","[Training Epoch 2] Batch 3700, Loss 0.47223764657974243\n","[Training Epoch 2] Batch 3701, Loss 0.47354522347450256\n","[Training Epoch 2] Batch 3702, Loss 0.47735002636909485\n","[Training Epoch 2] Batch 3703, Loss 0.5009080767631531\n","[Training Epoch 2] Batch 3704, Loss 0.44714486598968506\n","[Training Epoch 2] Batch 3705, Loss 0.4627465605735779\n","[Training Epoch 2] Batch 3706, Loss 0.4745902717113495\n","[Training Epoch 2] Batch 3707, Loss 0.4616854190826416\n","[Training Epoch 2] Batch 3708, Loss 0.47884130477905273\n","[Training Epoch 2] Batch 3709, Loss 0.4889017939567566\n","[Training Epoch 2] Batch 3710, Loss 0.48063409328460693\n","[Training Epoch 2] Batch 3711, Loss 0.4753270745277405\n","[Training Epoch 2] Batch 3712, Loss 0.4485330581665039\n","[Training Epoch 2] Batch 3713, Loss 0.4510761499404907\n","[Training Epoch 2] Batch 3714, Loss 0.44952502846717834\n","[Training Epoch 2] Batch 3715, Loss 0.4732619524002075\n","[Training Epoch 2] Batch 3716, Loss 0.48921069502830505\n","[Training Epoch 2] Batch 3717, Loss 0.48942044377326965\n","[Training Epoch 2] Batch 3718, Loss 0.4960050582885742\n","[Training Epoch 2] Batch 3719, Loss 0.47100430727005005\n","[Training Epoch 2] Batch 3720, Loss 0.4884515106678009\n","[Training Epoch 2] Batch 3721, Loss 0.4762738049030304\n","[Training Epoch 2] Batch 3722, Loss 0.5022262334823608\n","[Training Epoch 2] Batch 3723, Loss 0.4792476296424866\n","[Training Epoch 2] Batch 3724, Loss 0.47615206241607666\n","[Training Epoch 2] Batch 3725, Loss 0.45788657665252686\n","[Training Epoch 2] Batch 3726, Loss 0.5020073652267456\n","[Training Epoch 2] Batch 3727, Loss 0.5150384902954102\n","[Training Epoch 2] Batch 3728, Loss 0.4816657304763794\n","[Training Epoch 2] Batch 3729, Loss 0.449082612991333\n","[Training Epoch 2] Batch 3730, Loss 0.45527613162994385\n","[Training Epoch 2] Batch 3731, Loss 0.46896207332611084\n","[Training Epoch 2] Batch 3732, Loss 0.4815657436847687\n","[Training Epoch 2] Batch 3733, Loss 0.4597703814506531\n","[Training Epoch 2] Batch 3734, Loss 0.4573149085044861\n","[Training Epoch 2] Batch 3735, Loss 0.4691592752933502\n","[Training Epoch 2] Batch 3736, Loss 0.46404480934143066\n","[Training Epoch 2] Batch 3737, Loss 0.512322187423706\n","[Training Epoch 2] Batch 3738, Loss 0.4516317844390869\n","[Training Epoch 2] Batch 3739, Loss 0.4725652039051056\n","[Training Epoch 2] Batch 3740, Loss 0.46153318881988525\n","[Training Epoch 2] Batch 3741, Loss 0.4599129855632782\n","[Training Epoch 2] Batch 3742, Loss 0.4817226529121399\n","[Training Epoch 2] Batch 3743, Loss 0.46624597907066345\n","[Training Epoch 2] Batch 3744, Loss 0.48514777421951294\n","[Training Epoch 2] Batch 3745, Loss 0.48037946224212646\n","[Training Epoch 2] Batch 3746, Loss 0.44868332147598267\n","[Training Epoch 2] Batch 3747, Loss 0.47335827350616455\n","[Training Epoch 2] Batch 3748, Loss 0.48987120389938354\n","[Training Epoch 2] Batch 3749, Loss 0.4485436677932739\n","[Training Epoch 2] Batch 3750, Loss 0.4647819399833679\n","[Training Epoch 2] Batch 3751, Loss 0.49300485849380493\n","[Training Epoch 2] Batch 3752, Loss 0.459532767534256\n","[Training Epoch 2] Batch 3753, Loss 0.48419392108917236\n","[Training Epoch 2] Batch 3754, Loss 0.48017454147338867\n","[Training Epoch 2] Batch 3755, Loss 0.494229793548584\n","[Training Epoch 2] Batch 3756, Loss 0.4638636112213135\n","[Training Epoch 2] Batch 3757, Loss 0.45494943857192993\n","[Training Epoch 2] Batch 3758, Loss 0.4479559063911438\n","[Training Epoch 2] Batch 3759, Loss 0.5036196708679199\n","[Training Epoch 2] Batch 3760, Loss 0.46010711789131165\n","[Training Epoch 2] Batch 3761, Loss 0.4697405695915222\n","[Training Epoch 2] Batch 3762, Loss 0.5023263692855835\n","[Training Epoch 2] Batch 3763, Loss 0.45492464303970337\n","[Training Epoch 2] Batch 3764, Loss 0.4572281837463379\n","[Training Epoch 2] Batch 3765, Loss 0.47594940662384033\n","[Training Epoch 2] Batch 3766, Loss 0.46694687008857727\n","[Training Epoch 2] Batch 3767, Loss 0.47381389141082764\n","[Training Epoch 2] Batch 3768, Loss 0.4880463480949402\n","[Training Epoch 2] Batch 3769, Loss 0.47522926330566406\n","[Training Epoch 2] Batch 3770, Loss 0.48710185289382935\n","[Training Epoch 2] Batch 3771, Loss 0.4485782980918884\n","[Training Epoch 2] Batch 3772, Loss 0.5023155808448792\n","[Training Epoch 2] Batch 3773, Loss 0.4857374429702759\n","[Training Epoch 2] Batch 3774, Loss 0.4645133316516876\n","[Training Epoch 2] Batch 3775, Loss 0.45473748445510864\n","[Training Epoch 2] Batch 3776, Loss 0.4680187404155731\n","[Training Epoch 2] Batch 3777, Loss 0.4857408106327057\n","[Training Epoch 2] Batch 3778, Loss 0.49628350138664246\n","[Training Epoch 2] Batch 3779, Loss 0.4386175274848938\n","[Training Epoch 2] Batch 3780, Loss 0.49214857816696167\n","[Training Epoch 2] Batch 3781, Loss 0.4806548058986664\n","[Training Epoch 2] Batch 3782, Loss 0.47674912214279175\n","[Training Epoch 2] Batch 3783, Loss 0.47849130630493164\n","[Training Epoch 2] Batch 3784, Loss 0.470302015542984\n","[Training Epoch 2] Batch 3785, Loss 0.4678972363471985\n","[Training Epoch 2] Batch 3786, Loss 0.4477096199989319\n","[Training Epoch 2] Batch 3787, Loss 0.46794408559799194\n","[Training Epoch 2] Batch 3788, Loss 0.4948440492153168\n","[Training Epoch 2] Batch 3789, Loss 0.46230268478393555\n","[Training Epoch 2] Batch 3790, Loss 0.4592529833316803\n","[Training Epoch 2] Batch 3791, Loss 0.46095338463783264\n","[Training Epoch 2] Batch 3792, Loss 0.4787866473197937\n","[Training Epoch 2] Batch 3793, Loss 0.45206785202026367\n","[Training Epoch 2] Batch 3794, Loss 0.4662798345088959\n","[Training Epoch 2] Batch 3795, Loss 0.45456549525260925\n","[Training Epoch 2] Batch 3796, Loss 0.483212411403656\n","[Training Epoch 2] Batch 3797, Loss 0.4746500253677368\n","[Training Epoch 2] Batch 3798, Loss 0.47015058994293213\n","[Training Epoch 2] Batch 3799, Loss 0.44579023122787476\n","[Training Epoch 2] Batch 3800, Loss 0.44086217880249023\n","[Training Epoch 2] Batch 3801, Loss 0.44279956817626953\n","[Training Epoch 2] Batch 3802, Loss 0.47047215700149536\n","[Training Epoch 2] Batch 3803, Loss 0.49310600757598877\n","[Training Epoch 2] Batch 3804, Loss 0.49960455298423767\n","[Training Epoch 2] Batch 3805, Loss 0.4794378876686096\n","[Training Epoch 2] Batch 3806, Loss 0.4752911925315857\n","[Training Epoch 2] Batch 3807, Loss 0.47432368993759155\n","[Training Epoch 2] Batch 3808, Loss 0.4689658284187317\n","[Training Epoch 2] Batch 3809, Loss 0.4293859601020813\n","[Training Epoch 2] Batch 3810, Loss 0.4765738546848297\n","[Training Epoch 2] Batch 3811, Loss 0.4397357106208801\n","[Training Epoch 2] Batch 3812, Loss 0.47505682706832886\n","[Training Epoch 2] Batch 3813, Loss 0.4932214021682739\n","[Training Epoch 2] Batch 3814, Loss 0.49056127667427063\n","[Training Epoch 2] Batch 3815, Loss 0.47660207748413086\n","[Training Epoch 2] Batch 3816, Loss 0.48549339175224304\n","[Training Epoch 2] Batch 3817, Loss 0.5061103105545044\n","[Training Epoch 2] Batch 3818, Loss 0.4806528091430664\n","[Training Epoch 2] Batch 3819, Loss 0.4778470993041992\n","[Training Epoch 2] Batch 3820, Loss 0.44650349020957947\n","[Training Epoch 2] Batch 3821, Loss 0.49675506353378296\n","[Training Epoch 2] Batch 3822, Loss 0.4690861701965332\n","[Training Epoch 2] Batch 3823, Loss 0.47029948234558105\n","[Training Epoch 2] Batch 3824, Loss 0.4875670075416565\n","[Training Epoch 2] Batch 3825, Loss 0.46283698081970215\n","[Training Epoch 2] Batch 3826, Loss 0.46307438611984253\n","[Training Epoch 2] Batch 3827, Loss 0.45868754386901855\n","[Training Epoch 2] Batch 3828, Loss 0.4328848123550415\n","[Training Epoch 2] Batch 3829, Loss 0.4491345286369324\n","[Training Epoch 2] Batch 3830, Loss 0.47249120473861694\n","[Training Epoch 2] Batch 3831, Loss 0.4352000653743744\n","[Training Epoch 2] Batch 3832, Loss 0.4515193998813629\n","[Training Epoch 2] Batch 3833, Loss 0.49095261096954346\n","[Training Epoch 2] Batch 3834, Loss 0.4373617470264435\n","[Training Epoch 2] Batch 3835, Loss 0.4640198051929474\n","[Training Epoch 2] Batch 3836, Loss 0.4711923897266388\n","[Training Epoch 2] Batch 3837, Loss 0.46542397141456604\n","[Training Epoch 2] Batch 3838, Loss 0.4653671979904175\n","[Training Epoch 2] Batch 3839, Loss 0.45627671480178833\n","[Training Epoch 2] Batch 3840, Loss 0.48975950479507446\n","[Training Epoch 2] Batch 3841, Loss 0.4721764922142029\n","[Training Epoch 2] Batch 3842, Loss 0.46043241024017334\n","[Training Epoch 2] Batch 3843, Loss 0.4850815534591675\n","[Training Epoch 2] Batch 3844, Loss 0.4976106882095337\n","[Training Epoch 2] Batch 3845, Loss 0.4726465046405792\n","[Training Epoch 2] Batch 3846, Loss 0.47758281230926514\n","[Training Epoch 2] Batch 3847, Loss 0.44300374388694763\n","[Training Epoch 2] Batch 3848, Loss 0.4674151539802551\n","[Training Epoch 2] Batch 3849, Loss 0.4564235806465149\n","[Training Epoch 2] Batch 3850, Loss 0.44219520688056946\n","[Training Epoch 2] Batch 3851, Loss 0.4877207279205322\n","[Training Epoch 2] Batch 3852, Loss 0.46140867471694946\n","[Training Epoch 2] Batch 3853, Loss 0.46388983726501465\n","[Training Epoch 2] Batch 3854, Loss 0.4588302671909332\n","[Training Epoch 2] Batch 3855, Loss 0.45741620659828186\n","[Training Epoch 2] Batch 3856, Loss 0.5045036673545837\n","[Training Epoch 2] Batch 3857, Loss 0.45485085248947144\n","[Training Epoch 2] Batch 3858, Loss 0.46133726835250854\n","[Training Epoch 2] Batch 3859, Loss 0.48050597310066223\n","[Training Epoch 2] Batch 3860, Loss 0.49845024943351746\n","[Training Epoch 2] Batch 3861, Loss 0.47095268964767456\n","[Training Epoch 2] Batch 3862, Loss 0.4491193890571594\n","[Training Epoch 2] Batch 3863, Loss 0.4825901687145233\n","[Training Epoch 2] Batch 3864, Loss 0.4669080376625061\n","[Training Epoch 2] Batch 3865, Loss 0.45799610018730164\n","[Training Epoch 2] Batch 3866, Loss 0.4939923584461212\n","[Training Epoch 2] Batch 3867, Loss 0.45640748739242554\n","[Training Epoch 2] Batch 3868, Loss 0.47812992334365845\n","[Training Epoch 2] Batch 3869, Loss 0.46230506896972656\n","[Training Epoch 2] Batch 3870, Loss 0.45816054940223694\n","[Training Epoch 2] Batch 3871, Loss 0.4406129717826843\n","[Training Epoch 2] Batch 3872, Loss 0.48629891872406006\n","[Training Epoch 2] Batch 3873, Loss 0.43068164587020874\n","[Training Epoch 2] Batch 3874, Loss 0.47672009468078613\n","[Training Epoch 2] Batch 3875, Loss 0.4551355242729187\n","[Training Epoch 2] Batch 3876, Loss 0.45705875754356384\n","[Training Epoch 2] Batch 3877, Loss 0.4877309799194336\n","[Training Epoch 2] Batch 3878, Loss 0.4862857460975647\n","[Training Epoch 2] Batch 3879, Loss 0.449055552482605\n","[Training Epoch 2] Batch 3880, Loss 0.48671168088912964\n","[Training Epoch 2] Batch 3881, Loss 0.4502682685852051\n","[Training Epoch 2] Batch 3882, Loss 0.5038412809371948\n","[Training Epoch 2] Batch 3883, Loss 0.452254980802536\n","[Training Epoch 2] Batch 3884, Loss 0.4903537631034851\n","[Training Epoch 2] Batch 3885, Loss 0.4519108533859253\n","[Training Epoch 2] Batch 3886, Loss 0.4879782497882843\n","[Training Epoch 2] Batch 3887, Loss 0.4759719967842102\n","[Training Epoch 2] Batch 3888, Loss 0.44673675298690796\n","[Training Epoch 2] Batch 3889, Loss 0.4863569140434265\n","[Training Epoch 2] Batch 3890, Loss 0.4831695556640625\n","[Training Epoch 2] Batch 3891, Loss 0.46832799911499023\n","[Training Epoch 2] Batch 3892, Loss 0.487474262714386\n","[Training Epoch 2] Batch 3893, Loss 0.4710952639579773\n","[Training Epoch 2] Batch 3894, Loss 0.45489490032196045\n","[Training Epoch 2] Batch 3895, Loss 0.4556897282600403\n","[Training Epoch 2] Batch 3896, Loss 0.4449233412742615\n","[Training Epoch 2] Batch 3897, Loss 0.4741332530975342\n","[Training Epoch 2] Batch 3898, Loss 0.4723382890224457\n","[Training Epoch 2] Batch 3899, Loss 0.48245930671691895\n","[Training Epoch 2] Batch 3900, Loss 0.452896386384964\n","[Training Epoch 2] Batch 3901, Loss 0.46027815341949463\n","[Training Epoch 2] Batch 3902, Loss 0.45693743228912354\n","[Training Epoch 2] Batch 3903, Loss 0.4834026098251343\n","[Training Epoch 2] Batch 3904, Loss 0.4820448160171509\n","[Training Epoch 2] Batch 3905, Loss 0.4327053129673004\n","[Training Epoch 2] Batch 3906, Loss 0.49239277839660645\n","[Training Epoch 2] Batch 3907, Loss 0.49083080887794495\n","[Training Epoch 2] Batch 3908, Loss 0.45255112648010254\n","[Training Epoch 2] Batch 3909, Loss 0.4374309778213501\n","[Training Epoch 2] Batch 3910, Loss 0.5011709332466125\n","[Training Epoch 2] Batch 3911, Loss 0.44964030385017395\n","[Training Epoch 2] Batch 3912, Loss 0.46781936287879944\n","[Training Epoch 2] Batch 3913, Loss 0.462880402803421\n","[Training Epoch 2] Batch 3914, Loss 0.441435307264328\n","[Training Epoch 2] Batch 3915, Loss 0.477230429649353\n","[Training Epoch 2] Batch 3916, Loss 0.4641282856464386\n","[Training Epoch 2] Batch 3917, Loss 0.46371209621429443\n","[Training Epoch 2] Batch 3918, Loss 0.4672356843948364\n","[Training Epoch 2] Batch 3919, Loss 0.4578357934951782\n","[Training Epoch 2] Batch 3920, Loss 0.4489198923110962\n","[Training Epoch 2] Batch 3921, Loss 0.46174925565719604\n","[Training Epoch 2] Batch 3922, Loss 0.4632149338722229\n","[Training Epoch 2] Batch 3923, Loss 0.4490881562232971\n","[Training Epoch 2] Batch 3924, Loss 0.49154555797576904\n","[Training Epoch 2] Batch 3925, Loss 0.47753703594207764\n","[Training Epoch 2] Batch 3926, Loss 0.4691429138183594\n","[Training Epoch 2] Batch 3927, Loss 0.4783521294593811\n","[Training Epoch 2] Batch 3928, Loss 0.4874650537967682\n","[Training Epoch 2] Batch 3929, Loss 0.47320830821990967\n","[Training Epoch 2] Batch 3930, Loss 0.47079402208328247\n","[Training Epoch 2] Batch 3931, Loss 0.47028931975364685\n","[Training Epoch 2] Batch 3932, Loss 0.463478147983551\n","[Training Epoch 2] Batch 3933, Loss 0.46362173557281494\n","[Training Epoch 2] Batch 3934, Loss 0.44238823652267456\n","[Training Epoch 2] Batch 3935, Loss 0.43469685316085815\n","[Training Epoch 2] Batch 3936, Loss 0.46849000453948975\n","[Training Epoch 2] Batch 3937, Loss 0.45670321583747864\n","[Training Epoch 2] Batch 3938, Loss 0.4614676833152771\n","[Training Epoch 2] Batch 3939, Loss 0.4713011384010315\n","[Training Epoch 2] Batch 3940, Loss 0.48700690269470215\n","[Training Epoch 2] Batch 3941, Loss 0.4582670331001282\n","[Training Epoch 2] Batch 3942, Loss 0.45353585481643677\n","[Training Epoch 2] Batch 3943, Loss 0.44499415159225464\n","[Training Epoch 2] Batch 3944, Loss 0.44810086488723755\n","[Training Epoch 2] Batch 3945, Loss 0.46629804372787476\n","[Training Epoch 2] Batch 3946, Loss 0.45394963026046753\n","[Training Epoch 2] Batch 3947, Loss 0.4741995334625244\n","[Training Epoch 2] Batch 3948, Loss 0.47390657663345337\n","[Training Epoch 2] Batch 3949, Loss 0.4617060720920563\n","[Training Epoch 2] Batch 3950, Loss 0.4670901894569397\n","[Training Epoch 2] Batch 3951, Loss 0.4720876216888428\n","[Training Epoch 2] Batch 3952, Loss 0.48515748977661133\n","[Training Epoch 2] Batch 3953, Loss 0.44267475605010986\n","[Training Epoch 2] Batch 3954, Loss 0.46221232414245605\n","[Training Epoch 2] Batch 3955, Loss 0.4662446081638336\n","[Training Epoch 2] Batch 3956, Loss 0.5276527404785156\n","[Training Epoch 2] Batch 3957, Loss 0.4630599617958069\n","[Training Epoch 2] Batch 3958, Loss 0.48634201288223267\n","[Training Epoch 2] Batch 3959, Loss 0.442899227142334\n","[Training Epoch 2] Batch 3960, Loss 0.4527343511581421\n","[Training Epoch 2] Batch 3961, Loss 0.4737687110900879\n","[Training Epoch 2] Batch 3962, Loss 0.46973717212677\n","[Training Epoch 2] Batch 3963, Loss 0.5068143606185913\n","[Training Epoch 2] Batch 3964, Loss 0.45223915576934814\n","[Training Epoch 2] Batch 3965, Loss 0.47824379801750183\n","[Training Epoch 2] Batch 3966, Loss 0.4775000214576721\n","[Training Epoch 2] Batch 3967, Loss 0.45076990127563477\n","[Training Epoch 2] Batch 3968, Loss 0.5032622218132019\n","[Training Epoch 2] Batch 3969, Loss 0.42428773641586304\n","[Training Epoch 2] Batch 3970, Loss 0.4513787031173706\n","[Training Epoch 2] Batch 3971, Loss 0.49067428708076477\n","[Training Epoch 2] Batch 3972, Loss 0.47971874475479126\n","[Training Epoch 2] Batch 3973, Loss 0.5083791613578796\n","[Training Epoch 2] Batch 3974, Loss 0.45486634969711304\n","[Training Epoch 2] Batch 3975, Loss 0.4853575527667999\n","[Training Epoch 2] Batch 3976, Loss 0.472334086894989\n","[Training Epoch 2] Batch 3977, Loss 0.5099868774414062\n","[Training Epoch 2] Batch 3978, Loss 0.45787370204925537\n","[Training Epoch 2] Batch 3979, Loss 0.4557327628135681\n","[Training Epoch 2] Batch 3980, Loss 0.5007010698318481\n","[Training Epoch 2] Batch 3981, Loss 0.43653208017349243\n","[Training Epoch 2] Batch 3982, Loss 0.47416651248931885\n","[Training Epoch 2] Batch 3983, Loss 0.4813842177391052\n","[Training Epoch 2] Batch 3984, Loss 0.4524194300174713\n","[Training Epoch 2] Batch 3985, Loss 0.4224196672439575\n","[Training Epoch 2] Batch 3986, Loss 0.47214919328689575\n","[Training Epoch 2] Batch 3987, Loss 0.46559980511665344\n","[Training Epoch 2] Batch 3988, Loss 0.43409717082977295\n","[Training Epoch 2] Batch 3989, Loss 0.4701681435108185\n","[Training Epoch 2] Batch 3990, Loss 0.4354255795478821\n","[Training Epoch 2] Batch 3991, Loss 0.4690968096256256\n","[Training Epoch 2] Batch 3992, Loss 0.45527327060699463\n","[Training Epoch 2] Batch 3993, Loss 0.47955062985420227\n","[Training Epoch 2] Batch 3994, Loss 0.48503074049949646\n","[Training Epoch 2] Batch 3995, Loss 0.4701097905635834\n","[Training Epoch 2] Batch 3996, Loss 0.4497261345386505\n","[Training Epoch 2] Batch 3997, Loss 0.4924693703651428\n","[Training Epoch 2] Batch 3998, Loss 0.4532369375228882\n","[Training Epoch 2] Batch 3999, Loss 0.4604347348213196\n","[Training Epoch 2] Batch 4000, Loss 0.4687758684158325\n","[Training Epoch 2] Batch 4001, Loss 0.45247307419776917\n","[Training Epoch 2] Batch 4002, Loss 0.4442060887813568\n","[Training Epoch 2] Batch 4003, Loss 0.45938992500305176\n","[Training Epoch 2] Batch 4004, Loss 0.4466426968574524\n","[Training Epoch 2] Batch 4005, Loss 0.4609188437461853\n","[Training Epoch 2] Batch 4006, Loss 0.46076321601867676\n","[Training Epoch 2] Batch 4007, Loss 0.470203161239624\n","[Training Epoch 2] Batch 4008, Loss 0.46355700492858887\n","[Training Epoch 2] Batch 4009, Loss 0.49136117100715637\n","[Training Epoch 2] Batch 4010, Loss 0.4659816026687622\n","[Training Epoch 2] Batch 4011, Loss 0.49294161796569824\n","[Training Epoch 2] Batch 4012, Loss 0.44461143016815186\n","[Training Epoch 2] Batch 4013, Loss 0.4600943922996521\n","[Training Epoch 2] Batch 4014, Loss 0.47598403692245483\n","[Training Epoch 2] Batch 4015, Loss 0.43740540742874146\n","[Training Epoch 2] Batch 4016, Loss 0.4401647448539734\n","[Training Epoch 2] Batch 4017, Loss 0.4771510660648346\n","[Training Epoch 2] Batch 4018, Loss 0.46704691648483276\n","[Training Epoch 2] Batch 4019, Loss 0.46817243099212646\n","[Training Epoch 2] Batch 4020, Loss 0.477057546377182\n","[Training Epoch 2] Batch 4021, Loss 0.4553864002227783\n","[Training Epoch 2] Batch 4022, Loss 0.4572884440422058\n","[Training Epoch 2] Batch 4023, Loss 0.4564395844936371\n","[Training Epoch 2] Batch 4024, Loss 0.4997592866420746\n","[Training Epoch 2] Batch 4025, Loss 0.46260690689086914\n","[Training Epoch 2] Batch 4026, Loss 0.4629281461238861\n","[Training Epoch 2] Batch 4027, Loss 0.471879780292511\n","[Training Epoch 2] Batch 4028, Loss 0.465953528881073\n","[Training Epoch 2] Batch 4029, Loss 0.4619184732437134\n","[Training Epoch 2] Batch 4030, Loss 0.4645228981971741\n","[Training Epoch 2] Batch 4031, Loss 0.509052038192749\n","[Training Epoch 2] Batch 4032, Loss 0.46988746523857117\n","[Training Epoch 2] Batch 4033, Loss 0.5153491497039795\n","[Training Epoch 2] Batch 4034, Loss 0.46161359548568726\n","[Training Epoch 2] Batch 4035, Loss 0.48389366269111633\n","[Training Epoch 2] Batch 4036, Loss 0.4882233142852783\n","[Training Epoch 2] Batch 4037, Loss 0.4728998839855194\n","[Training Epoch 2] Batch 4038, Loss 0.4715813994407654\n","[Training Epoch 2] Batch 4039, Loss 0.4695996046066284\n","[Training Epoch 2] Batch 4040, Loss 0.45558470487594604\n","[Training Epoch 2] Batch 4041, Loss 0.4475691318511963\n","[Training Epoch 2] Batch 4042, Loss 0.47722047567367554\n","[Training Epoch 2] Batch 4043, Loss 0.4657350778579712\n","[Training Epoch 2] Batch 4044, Loss 0.466558575630188\n","[Training Epoch 2] Batch 4045, Loss 0.4490596652030945\n","[Training Epoch 2] Batch 4046, Loss 0.5041601061820984\n","[Training Epoch 2] Batch 4047, Loss 0.46677032113075256\n","[Training Epoch 2] Batch 4048, Loss 0.4622732996940613\n","[Training Epoch 2] Batch 4049, Loss 0.4600084125995636\n","[Training Epoch 2] Batch 4050, Loss 0.44722646474838257\n","[Training Epoch 2] Batch 4051, Loss 0.4463379979133606\n","[Training Epoch 2] Batch 4052, Loss 0.4616299867630005\n","[Training Epoch 2] Batch 4053, Loss 0.45617684721946716\n","[Training Epoch 2] Batch 4054, Loss 0.47458404302597046\n","[Training Epoch 2] Batch 4055, Loss 0.47957542538642883\n","[Training Epoch 2] Batch 4056, Loss 0.4526362717151642\n","[Training Epoch 2] Batch 4057, Loss 0.46357840299606323\n","[Training Epoch 2] Batch 4058, Loss 0.4716774821281433\n","[Training Epoch 2] Batch 4059, Loss 0.4623393714427948\n","[Training Epoch 2] Batch 4060, Loss 0.4596925675868988\n","[Training Epoch 2] Batch 4061, Loss 0.4486945867538452\n","[Training Epoch 2] Batch 4062, Loss 0.4531232714653015\n","[Training Epoch 2] Batch 4063, Loss 0.46984559297561646\n","[Training Epoch 2] Batch 4064, Loss 0.4507240951061249\n","[Training Epoch 2] Batch 4065, Loss 0.46437525749206543\n","[Training Epoch 2] Batch 4066, Loss 0.4560461640357971\n","[Training Epoch 2] Batch 4067, Loss 0.47048985958099365\n","[Training Epoch 2] Batch 4068, Loss 0.46340444684028625\n","[Training Epoch 2] Batch 4069, Loss 0.48270100355148315\n","[Training Epoch 2] Batch 4070, Loss 0.4833005964756012\n","[Training Epoch 2] Batch 4071, Loss 0.4539855122566223\n","[Training Epoch 2] Batch 4072, Loss 0.46652349829673767\n","[Training Epoch 2] Batch 4073, Loss 0.46891528367996216\n","[Training Epoch 2] Batch 4074, Loss 0.4690715968608856\n","[Training Epoch 2] Batch 4075, Loss 0.45798084139823914\n","[Training Epoch 2] Batch 4076, Loss 0.5036166906356812\n","[Training Epoch 2] Batch 4077, Loss 0.4512988328933716\n","[Training Epoch 2] Batch 4078, Loss 0.47264355421066284\n","[Training Epoch 2] Batch 4079, Loss 0.4782198667526245\n","[Training Epoch 2] Batch 4080, Loss 0.4475669264793396\n","[Training Epoch 2] Batch 4081, Loss 0.43743041157722473\n","[Training Epoch 2] Batch 4082, Loss 0.46912363171577454\n","[Training Epoch 2] Batch 4083, Loss 0.47127455472946167\n","[Training Epoch 2] Batch 4084, Loss 0.4881756603717804\n","[Training Epoch 2] Batch 4085, Loss 0.45207661390304565\n","[Training Epoch 2] Batch 4086, Loss 0.459051251411438\n","[Training Epoch 2] Batch 4087, Loss 0.46793466806411743\n","[Training Epoch 2] Batch 4088, Loss 0.4559568166732788\n","[Training Epoch 2] Batch 4089, Loss 0.45077911019325256\n","[Training Epoch 2] Batch 4090, Loss 0.48509323596954346\n","[Training Epoch 2] Batch 4091, Loss 0.4847014546394348\n","[Training Epoch 2] Batch 4092, Loss 0.46699392795562744\n","[Training Epoch 2] Batch 4093, Loss 0.47822120785713196\n","[Training Epoch 2] Batch 4094, Loss 0.4828317165374756\n","[Training Epoch 2] Batch 4095, Loss 0.45484861731529236\n","[Training Epoch 2] Batch 4096, Loss 0.4777674078941345\n","[Training Epoch 2] Batch 4097, Loss 0.4671962261199951\n","[Training Epoch 2] Batch 4098, Loss 0.45991379022598267\n","[Training Epoch 2] Batch 4099, Loss 0.46258264780044556\n","[Training Epoch 2] Batch 4100, Loss 0.4756661057472229\n","[Training Epoch 2] Batch 4101, Loss 0.44367319345474243\n","[Training Epoch 2] Batch 4102, Loss 0.49814528226852417\n","[Training Epoch 2] Batch 4103, Loss 0.501714825630188\n","[Training Epoch 2] Batch 4104, Loss 0.4659733772277832\n","[Training Epoch 2] Batch 4105, Loss 0.4779471755027771\n","[Training Epoch 2] Batch 4106, Loss 0.4334449768066406\n","[Training Epoch 2] Batch 4107, Loss 0.47193413972854614\n","[Training Epoch 2] Batch 4108, Loss 0.4381447434425354\n","[Training Epoch 2] Batch 4109, Loss 0.46148228645324707\n","[Training Epoch 2] Batch 4110, Loss 0.47020894289016724\n","[Training Epoch 2] Batch 4111, Loss 0.4779812693595886\n","[Training Epoch 2] Batch 4112, Loss 0.44419214129447937\n","[Training Epoch 2] Batch 4113, Loss 0.46819108724594116\n","[Training Epoch 2] Batch 4114, Loss 0.4722776412963867\n","[Training Epoch 2] Batch 4115, Loss 0.45081809163093567\n","[Training Epoch 2] Batch 4116, Loss 0.4886570870876312\n","[Training Epoch 2] Batch 4117, Loss 0.4485575258731842\n","[Training Epoch 2] Batch 4118, Loss 0.4588603973388672\n","[Training Epoch 2] Batch 4119, Loss 0.526930570602417\n","[Training Epoch 2] Batch 4120, Loss 0.4889765977859497\n","[Training Epoch 2] Batch 4121, Loss 0.4259077310562134\n","[Training Epoch 2] Batch 4122, Loss 0.4610392451286316\n","[Training Epoch 2] Batch 4123, Loss 0.43852710723876953\n","[Training Epoch 2] Batch 4124, Loss 0.44671082496643066\n","[Training Epoch 2] Batch 4125, Loss 0.476484090089798\n","[Training Epoch 2] Batch 4126, Loss 0.4652596712112427\n","[Training Epoch 2] Batch 4127, Loss 0.4516269564628601\n","[Training Epoch 2] Batch 4128, Loss 0.4778023362159729\n","[Training Epoch 2] Batch 4129, Loss 0.41803428530693054\n","[Training Epoch 2] Batch 4130, Loss 0.46620357036590576\n","[Training Epoch 2] Batch 4131, Loss 0.4576084613800049\n","[Training Epoch 2] Batch 4132, Loss 0.4764719605445862\n","[Training Epoch 2] Batch 4133, Loss 0.48285990953445435\n","[Training Epoch 2] Batch 4134, Loss 0.4711478352546692\n","[Training Epoch 2] Batch 4135, Loss 0.43386176228523254\n","[Training Epoch 2] Batch 4136, Loss 0.44939765334129333\n","[Training Epoch 2] Batch 4137, Loss 0.4659380316734314\n","[Training Epoch 2] Batch 4138, Loss 0.48683667182922363\n","[Training Epoch 2] Batch 4139, Loss 0.4688647985458374\n","[Training Epoch 2] Batch 4140, Loss 0.4554165005683899\n","[Training Epoch 2] Batch 4141, Loss 0.47879305481910706\n","[Training Epoch 2] Batch 4142, Loss 0.4807790517807007\n","[Training Epoch 2] Batch 4143, Loss 0.47411787509918213\n","[Training Epoch 2] Batch 4144, Loss 0.4704218804836273\n","[Training Epoch 2] Batch 4145, Loss 0.47161248326301575\n","[Training Epoch 2] Batch 4146, Loss 0.4809483587741852\n","[Training Epoch 2] Batch 4147, Loss 0.48842504620552063\n","[Training Epoch 2] Batch 4148, Loss 0.45635247230529785\n","[Training Epoch 2] Batch 4149, Loss 0.4788939356803894\n","[Training Epoch 2] Batch 4150, Loss 0.4685386121273041\n","[Training Epoch 2] Batch 4151, Loss 0.461484432220459\n","[Training Epoch 2] Batch 4152, Loss 0.46959343552589417\n","[Training Epoch 2] Batch 4153, Loss 0.4408104121685028\n","[Training Epoch 2] Batch 4154, Loss 0.4420880675315857\n","[Training Epoch 2] Batch 4155, Loss 0.44919896125793457\n","[Training Epoch 2] Batch 4156, Loss 0.47470200061798096\n","[Training Epoch 2] Batch 4157, Loss 0.4363223910331726\n","[Training Epoch 2] Batch 4158, Loss 0.4725481867790222\n","[Training Epoch 2] Batch 4159, Loss 0.43798184394836426\n","[Training Epoch 2] Batch 4160, Loss 0.4306684136390686\n","[Training Epoch 2] Batch 4161, Loss 0.43564802408218384\n","[Training Epoch 2] Batch 4162, Loss 0.46416330337524414\n","[Training Epoch 2] Batch 4163, Loss 0.46025699377059937\n","[Training Epoch 2] Batch 4164, Loss 0.4529029130935669\n","[Training Epoch 2] Batch 4165, Loss 0.4745139479637146\n","[Training Epoch 2] Batch 4166, Loss 0.505528450012207\n","[Training Epoch 2] Batch 4167, Loss 0.45817312598228455\n","[Training Epoch 2] Batch 4168, Loss 0.47094982862472534\n","[Training Epoch 2] Batch 4169, Loss 0.47450393438339233\n","[Training Epoch 2] Batch 4170, Loss 0.45955199003219604\n","[Training Epoch 2] Batch 4171, Loss 0.47442060708999634\n","[Training Epoch 2] Batch 4172, Loss 0.5011727213859558\n","[Training Epoch 2] Batch 4173, Loss 0.4692687392234802\n","[Training Epoch 2] Batch 4174, Loss 0.46266448497772217\n","[Training Epoch 2] Batch 4175, Loss 0.47258853912353516\n","[Training Epoch 2] Batch 4176, Loss 0.45588892698287964\n","[Training Epoch 2] Batch 4177, Loss 0.46818405389785767\n","[Training Epoch 2] Batch 4178, Loss 0.4421626031398773\n","[Training Epoch 2] Batch 4179, Loss 0.4758843779563904\n","[Training Epoch 2] Batch 4180, Loss 0.4557170271873474\n","[Training Epoch 2] Batch 4181, Loss 0.4752848744392395\n","[Training Epoch 2] Batch 4182, Loss 0.45575031638145447\n","[Training Epoch 2] Batch 4183, Loss 0.441036581993103\n","[Training Epoch 2] Batch 4184, Loss 0.47214919328689575\n","[Training Epoch 2] Batch 4185, Loss 0.46624723076820374\n","[Training Epoch 2] Batch 4186, Loss 0.4263782501220703\n","[Training Epoch 2] Batch 4187, Loss 0.43879109621047974\n","[Training Epoch 2] Batch 4188, Loss 0.4239775836467743\n","[Training Epoch 2] Batch 4189, Loss 0.42317208647727966\n","[Training Epoch 2] Batch 4190, Loss 0.4516814947128296\n","[Training Epoch 2] Batch 4191, Loss 0.4613367021083832\n","[Training Epoch 2] Batch 4192, Loss 0.4545680284500122\n","[Training Epoch 2] Batch 4193, Loss 0.4663025736808777\n","[Training Epoch 2] Batch 4194, Loss 0.4840009808540344\n","[Training Epoch 2] Batch 4195, Loss 0.4885982275009155\n","[Training Epoch 2] Batch 4196, Loss 0.4520450234413147\n","[Training Epoch 2] Batch 4197, Loss 0.4893682599067688\n","[Training Epoch 2] Batch 4198, Loss 0.49347397685050964\n","[Training Epoch 2] Batch 4199, Loss 0.4711235463619232\n","[Training Epoch 2] Batch 4200, Loss 0.43769538402557373\n","[Training Epoch 2] Batch 4201, Loss 0.47108763456344604\n","[Training Epoch 2] Batch 4202, Loss 0.4518762230873108\n","[Training Epoch 2] Batch 4203, Loss 0.4838047921657562\n","[Training Epoch 2] Batch 4204, Loss 0.48241397738456726\n","[Training Epoch 2] Batch 4205, Loss 0.44166475534439087\n","[Training Epoch 2] Batch 4206, Loss 0.4305427074432373\n","[Training Epoch 2] Batch 4207, Loss 0.45269912481307983\n","[Training Epoch 2] Batch 4208, Loss 0.4664731025695801\n","[Training Epoch 2] Batch 4209, Loss 0.4461061358451843\n","[Training Epoch 2] Batch 4210, Loss 0.46867698431015015\n","[Training Epoch 2] Batch 4211, Loss 0.44001808762550354\n","[Training Epoch 2] Batch 4212, Loss 0.48718518018722534\n","[Training Epoch 2] Batch 4213, Loss 0.45999911427497864\n","[Training Epoch 2] Batch 4214, Loss 0.4492124319076538\n","[Training Epoch 2] Batch 4215, Loss 0.4504631757736206\n","[Training Epoch 2] Batch 4216, Loss 0.4746417999267578\n","[Training Epoch 2] Batch 4217, Loss 0.4642676115036011\n","[Training Epoch 2] Batch 4218, Loss 0.44406190514564514\n","[Training Epoch 2] Batch 4219, Loss 0.4558258354663849\n","[Training Epoch 2] Batch 4220, Loss 0.4561699330806732\n","[Training Epoch 2] Batch 4221, Loss 0.4547450542449951\n","[Training Epoch 2] Batch 4222, Loss 0.45281100273132324\n","[Training Epoch 2] Batch 4223, Loss 0.4293493628501892\n","[Training Epoch 2] Batch 4224, Loss 0.48448044061660767\n","[Training Epoch 2] Batch 4225, Loss 0.4613494277000427\n","[Training Epoch 2] Batch 4226, Loss 0.48257994651794434\n","[Training Epoch 2] Batch 4227, Loss 0.4385993182659149\n","[Training Epoch 2] Batch 4228, Loss 0.48131927847862244\n","[Training Epoch 2] Batch 4229, Loss 0.46822530031204224\n","[Training Epoch 2] Batch 4230, Loss 0.47400522232055664\n","[Training Epoch 2] Batch 4231, Loss 0.41837966442108154\n","[Training Epoch 2] Batch 4232, Loss 0.4568477272987366\n","[Training Epoch 2] Batch 4233, Loss 0.4557421803474426\n","[Training Epoch 2] Batch 4234, Loss 0.4661795198917389\n","[Training Epoch 2] Batch 4235, Loss 0.45302659273147583\n","[Training Epoch 2] Batch 4236, Loss 0.45132967829704285\n","[Training Epoch 2] Batch 4237, Loss 0.43200254440307617\n","[Training Epoch 2] Batch 4238, Loss 0.44981011748313904\n","[Training Epoch 2] Batch 4239, Loss 0.48117733001708984\n","[Training Epoch 2] Batch 4240, Loss 0.45523297786712646\n","[Training Epoch 2] Batch 4241, Loss 0.4769899547100067\n","[Training Epoch 2] Batch 4242, Loss 0.44522690773010254\n","[Training Epoch 2] Batch 4243, Loss 0.4361627399921417\n","[Training Epoch 2] Batch 4244, Loss 0.4254794716835022\n","[Training Epoch 2] Batch 4245, Loss 0.4277017116546631\n","[Training Epoch 2] Batch 4246, Loss 0.45750150084495544\n","[Training Epoch 2] Batch 4247, Loss 0.500872015953064\n","[Training Epoch 2] Batch 4248, Loss 0.4642782509326935\n","[Training Epoch 2] Batch 4249, Loss 0.42882704734802246\n","[Training Epoch 2] Batch 4250, Loss 0.4747820496559143\n","[Training Epoch 2] Batch 4251, Loss 0.48176103830337524\n","[Training Epoch 2] Batch 4252, Loss 0.43207722902297974\n","[Training Epoch 2] Batch 4253, Loss 0.4581252634525299\n","[Training Epoch 2] Batch 4254, Loss 0.45036935806274414\n","[Training Epoch 2] Batch 4255, Loss 0.4693114161491394\n","[Training Epoch 2] Batch 4256, Loss 0.4265720844268799\n","[Training Epoch 2] Batch 4257, Loss 0.44556522369384766\n","[Training Epoch 2] Batch 4258, Loss 0.48618146777153015\n","[Training Epoch 2] Batch 4259, Loss 0.45823612809181213\n","[Training Epoch 2] Batch 4260, Loss 0.42784231901168823\n","[Training Epoch 2] Batch 4261, Loss 0.47492271661758423\n","[Training Epoch 2] Batch 4262, Loss 0.42827948927879333\n","[Training Epoch 2] Batch 4263, Loss 0.4268190860748291\n","[Training Epoch 2] Batch 4264, Loss 0.46840524673461914\n","[Training Epoch 2] Batch 4265, Loss 0.4627382755279541\n","[Training Epoch 2] Batch 4266, Loss 0.4816648066043854\n","[Training Epoch 2] Batch 4267, Loss 0.4580919146537781\n","[Training Epoch 2] Batch 4268, Loss 0.48622751235961914\n","[Training Epoch 2] Batch 4269, Loss 0.46891582012176514\n","[Training Epoch 2] Batch 4270, Loss 0.4762357473373413\n","[Training Epoch 2] Batch 4271, Loss 0.48075199127197266\n","[Training Epoch 2] Batch 4272, Loss 0.4283461272716522\n","[Training Epoch 2] Batch 4273, Loss 0.4849298894405365\n","[Training Epoch 2] Batch 4274, Loss 0.4767622947692871\n","[Training Epoch 2] Batch 4275, Loss 0.451820969581604\n","[Training Epoch 2] Batch 4276, Loss 0.4497421383857727\n","[Training Epoch 2] Batch 4277, Loss 0.46502935886383057\n","[Training Epoch 2] Batch 4278, Loss 0.4705258011817932\n","[Training Epoch 2] Batch 4279, Loss 0.47372856736183167\n","[Training Epoch 2] Batch 4280, Loss 0.43406975269317627\n","[Training Epoch 2] Batch 4281, Loss 0.4311846196651459\n","[Training Epoch 2] Batch 4282, Loss 0.45620274543762207\n","[Training Epoch 2] Batch 4283, Loss 0.4807528257369995\n","[Training Epoch 2] Batch 4284, Loss 0.4573409855365753\n","[Training Epoch 2] Batch 4285, Loss 0.4463731646537781\n","[Training Epoch 2] Batch 4286, Loss 0.4791984260082245\n","[Training Epoch 2] Batch 4287, Loss 0.46582305431365967\n","[Training Epoch 2] Batch 4288, Loss 0.4694022536277771\n","[Training Epoch 2] Batch 4289, Loss 0.4686020016670227\n","[Training Epoch 2] Batch 4290, Loss 0.4703943133354187\n","[Training Epoch 2] Batch 4291, Loss 0.431729793548584\n","[Training Epoch 2] Batch 4292, Loss 0.4571484327316284\n","[Training Epoch 2] Batch 4293, Loss 0.455691933631897\n","[Training Epoch 2] Batch 4294, Loss 0.5021076798439026\n","[Training Epoch 2] Batch 4295, Loss 0.46600574254989624\n","[Training Epoch 2] Batch 4296, Loss 0.48833367228507996\n","[Training Epoch 2] Batch 4297, Loss 0.4389549493789673\n","[Training Epoch 2] Batch 4298, Loss 0.4733228087425232\n","[Training Epoch 2] Batch 4299, Loss 0.46875739097595215\n","[Training Epoch 2] Batch 4300, Loss 0.46337273716926575\n","[Training Epoch 2] Batch 4301, Loss 0.47853025794029236\n","[Training Epoch 2] Batch 4302, Loss 0.4611654281616211\n","[Training Epoch 2] Batch 4303, Loss 0.4602314829826355\n","[Training Epoch 2] Batch 4304, Loss 0.44631198048591614\n","[Training Epoch 2] Batch 4305, Loss 0.4648560881614685\n","[Training Epoch 2] Batch 4306, Loss 0.4433537721633911\n","[Training Epoch 2] Batch 4307, Loss 0.4594931900501251\n","[Training Epoch 2] Batch 4308, Loss 0.4752015471458435\n","[Training Epoch 2] Batch 4309, Loss 0.4959007501602173\n","[Training Epoch 2] Batch 4310, Loss 0.466144859790802\n","[Training Epoch 2] Batch 4311, Loss 0.45590269565582275\n","[Training Epoch 2] Batch 4312, Loss 0.4239192605018616\n","[Training Epoch 2] Batch 4313, Loss 0.4565925598144531\n","[Training Epoch 2] Batch 4314, Loss 0.43633949756622314\n","[Training Epoch 2] Batch 4315, Loss 0.43433070182800293\n","[Training Epoch 2] Batch 4316, Loss 0.44471386075019836\n","[Training Epoch 2] Batch 4317, Loss 0.49112939834594727\n","[Training Epoch 2] Batch 4318, Loss 0.4783347249031067\n","[Training Epoch 2] Batch 4319, Loss 0.46928703784942627\n","[Training Epoch 2] Batch 4320, Loss 0.47196394205093384\n","[Training Epoch 2] Batch 4321, Loss 0.44807425141334534\n","[Training Epoch 2] Batch 4322, Loss 0.45511260628700256\n","[Training Epoch 2] Batch 4323, Loss 0.45492953062057495\n","[Training Epoch 2] Batch 4324, Loss 0.4533224105834961\n","[Training Epoch 2] Batch 4325, Loss 0.45468607544898987\n","[Training Epoch 2] Batch 4326, Loss 0.45137789845466614\n","[Training Epoch 2] Batch 4327, Loss 0.43953758478164673\n","[Training Epoch 2] Batch 4328, Loss 0.45013999938964844\n","[Training Epoch 2] Batch 4329, Loss 0.42525672912597656\n","[Training Epoch 2] Batch 4330, Loss 0.4180653691291809\n","[Training Epoch 2] Batch 4331, Loss 0.45859241485595703\n","[Training Epoch 2] Batch 4332, Loss 0.4479686915874481\n","[Training Epoch 2] Batch 4333, Loss 0.45379638671875\n","[Training Epoch 2] Batch 4334, Loss 0.45357459783554077\n","[Training Epoch 2] Batch 4335, Loss 0.44292616844177246\n","[Training Epoch 2] Batch 4336, Loss 0.4753490090370178\n","[Training Epoch 2] Batch 4337, Loss 0.4723137617111206\n","[Training Epoch 2] Batch 4338, Loss 0.4333781599998474\n","[Training Epoch 2] Batch 4339, Loss 0.4446125328540802\n","[Training Epoch 2] Batch 4340, Loss 0.45493367314338684\n","[Training Epoch 2] Batch 4341, Loss 0.4652402997016907\n","[Training Epoch 2] Batch 4342, Loss 0.42451536655426025\n","[Training Epoch 2] Batch 4343, Loss 0.4628302752971649\n","[Training Epoch 2] Batch 4344, Loss 0.4446415901184082\n","[Training Epoch 2] Batch 4345, Loss 0.43609848618507385\n","[Training Epoch 2] Batch 4346, Loss 0.47829318046569824\n","[Training Epoch 2] Batch 4347, Loss 0.43654558062553406\n","[Training Epoch 2] Batch 4348, Loss 0.460906982421875\n","[Training Epoch 2] Batch 4349, Loss 0.4518333375453949\n","[Training Epoch 2] Batch 4350, Loss 0.4549512267112732\n","[Training Epoch 2] Batch 4351, Loss 0.44000357389450073\n","[Training Epoch 2] Batch 4352, Loss 0.4640498757362366\n","[Training Epoch 2] Batch 4353, Loss 0.452184796333313\n","[Training Epoch 2] Batch 4354, Loss 0.4487765431404114\n","[Training Epoch 2] Batch 4355, Loss 0.4489850699901581\n","[Training Epoch 2] Batch 4356, Loss 0.4517974257469177\n","[Training Epoch 2] Batch 4357, Loss 0.481112003326416\n","[Training Epoch 2] Batch 4358, Loss 0.4620860517024994\n","[Training Epoch 2] Batch 4359, Loss 0.4409734606742859\n","[Training Epoch 2] Batch 4360, Loss 0.4435292184352875\n","[Training Epoch 2] Batch 4361, Loss 0.47348305583000183\n","[Training Epoch 2] Batch 4362, Loss 0.46492093801498413\n","[Training Epoch 2] Batch 4363, Loss 0.44210267066955566\n","[Training Epoch 2] Batch 4364, Loss 0.49978139996528625\n","[Training Epoch 2] Batch 4365, Loss 0.44040119647979736\n","[Training Epoch 2] Batch 4366, Loss 0.4722277522087097\n","[Training Epoch 2] Batch 4367, Loss 0.4587084949016571\n","[Training Epoch 2] Batch 4368, Loss 0.48812854290008545\n","[Training Epoch 2] Batch 4369, Loss 0.48015475273132324\n","[Training Epoch 2] Batch 4370, Loss 0.42712000012397766\n","[Training Epoch 2] Batch 4371, Loss 0.4323066473007202\n","[Training Epoch 2] Batch 4372, Loss 0.44494616985321045\n","[Training Epoch 2] Batch 4373, Loss 0.4530099034309387\n","[Training Epoch 2] Batch 4374, Loss 0.4757165312767029\n","[Training Epoch 2] Batch 4375, Loss 0.45107001066207886\n","[Training Epoch 2] Batch 4376, Loss 0.49160853028297424\n","[Training Epoch 2] Batch 4377, Loss 0.44212454557418823\n","[Training Epoch 2] Batch 4378, Loss 0.4733756482601166\n","[Training Epoch 2] Batch 4379, Loss 0.4185471534729004\n","[Training Epoch 2] Batch 4380, Loss 0.45740073919296265\n","[Training Epoch 2] Batch 4381, Loss 0.42632973194122314\n","[Training Epoch 2] Batch 4382, Loss 0.45191749930381775\n","[Training Epoch 2] Batch 4383, Loss 0.4695003032684326\n","[Training Epoch 2] Batch 4384, Loss 0.45978042483329773\n","[Training Epoch 2] Batch 4385, Loss 0.4536469578742981\n","[Training Epoch 2] Batch 4386, Loss 0.44431641697883606\n","[Training Epoch 2] Batch 4387, Loss 0.46072331070899963\n","[Training Epoch 2] Batch 4388, Loss 0.42867788672447205\n","[Training Epoch 2] Batch 4389, Loss 0.4298299551010132\n","[Training Epoch 2] Batch 4390, Loss 0.414730966091156\n","[Training Epoch 2] Batch 4391, Loss 0.4514414668083191\n","[Training Epoch 2] Batch 4392, Loss 0.45666593313217163\n","[Training Epoch 2] Batch 4393, Loss 0.44738081097602844\n","[Training Epoch 2] Batch 4394, Loss 0.4460916519165039\n","[Training Epoch 2] Batch 4395, Loss 0.44753298163414\n","[Training Epoch 2] Batch 4396, Loss 0.4676007926464081\n","[Training Epoch 2] Batch 4397, Loss 0.4545590281486511\n","[Training Epoch 2] Batch 4398, Loss 0.47277480363845825\n","[Training Epoch 2] Batch 4399, Loss 0.45194143056869507\n","[Training Epoch 2] Batch 4400, Loss 0.476283997297287\n","[Training Epoch 2] Batch 4401, Loss 0.4417920410633087\n","[Training Epoch 2] Batch 4402, Loss 0.44016698002815247\n","[Training Epoch 2] Batch 4403, Loss 0.44746318459510803\n","[Training Epoch 2] Batch 4404, Loss 0.444653183221817\n","[Training Epoch 2] Batch 4405, Loss 0.4559285640716553\n","[Training Epoch 2] Batch 4406, Loss 0.4584420919418335\n","[Training Epoch 2] Batch 4407, Loss 0.46155765652656555\n","[Training Epoch 2] Batch 4408, Loss 0.46352052688598633\n","[Training Epoch 2] Batch 4409, Loss 0.4867590069770813\n","[Training Epoch 2] Batch 4410, Loss 0.45056986808776855\n","[Training Epoch 2] Batch 4411, Loss 0.4479246437549591\n","[Training Epoch 2] Batch 4412, Loss 0.46001213788986206\n","[Training Epoch 2] Batch 4413, Loss 0.4455386996269226\n","[Training Epoch 2] Batch 4414, Loss 0.4394909143447876\n","[Training Epoch 2] Batch 4415, Loss 0.44059300422668457\n","[Training Epoch 2] Batch 4416, Loss 0.4638440012931824\n","[Training Epoch 2] Batch 4417, Loss 0.4589075446128845\n","[Training Epoch 2] Batch 4418, Loss 0.4663134813308716\n","[Training Epoch 2] Batch 4419, Loss 0.44421032071113586\n","[Training Epoch 2] Batch 4420, Loss 0.47023332118988037\n","[Training Epoch 2] Batch 4421, Loss 0.44433367252349854\n","[Training Epoch 2] Batch 4422, Loss 0.45663267374038696\n","[Training Epoch 2] Batch 4423, Loss 0.4621417820453644\n","[Training Epoch 2] Batch 4424, Loss 0.47805649042129517\n","[Training Epoch 2] Batch 4425, Loss 0.471005380153656\n","[Training Epoch 2] Batch 4426, Loss 0.4634581208229065\n","[Training Epoch 2] Batch 4427, Loss 0.4317450523376465\n","[Training Epoch 2] Batch 4428, Loss 0.478719025850296\n","[Training Epoch 2] Batch 4429, Loss 0.45019182562828064\n","[Training Epoch 2] Batch 4430, Loss 0.47341951727867126\n","[Training Epoch 2] Batch 4431, Loss 0.4444553852081299\n","[Training Epoch 2] Batch 4432, Loss 0.4561331272125244\n","[Training Epoch 2] Batch 4433, Loss 0.4564919173717499\n","[Training Epoch 2] Batch 4434, Loss 0.4898063838481903\n","[Training Epoch 2] Batch 4435, Loss 0.46014657616615295\n","[Training Epoch 2] Batch 4436, Loss 0.4538940191268921\n","[Training Epoch 2] Batch 4437, Loss 0.4552918076515198\n","[Training Epoch 2] Batch 4438, Loss 0.4618186950683594\n","[Training Epoch 2] Batch 4439, Loss 0.43597763776779175\n","[Training Epoch 2] Batch 4440, Loss 0.45046672224998474\n","[Training Epoch 2] Batch 4441, Loss 0.46383723616600037\n","[Training Epoch 2] Batch 4442, Loss 0.4604905843734741\n","[Training Epoch 2] Batch 4443, Loss 0.4781847596168518\n","[Training Epoch 2] Batch 4444, Loss 0.4441636800765991\n","[Training Epoch 2] Batch 4445, Loss 0.4289335012435913\n","[Training Epoch 2] Batch 4446, Loss 0.4904576539993286\n","[Training Epoch 2] Batch 4447, Loss 0.4732015132904053\n","[Training Epoch 2] Batch 4448, Loss 0.47210434079170227\n","[Training Epoch 2] Batch 4449, Loss 0.46149733662605286\n","[Training Epoch 2] Batch 4450, Loss 0.4608883261680603\n","[Training Epoch 2] Batch 4451, Loss 0.4856923520565033\n","[Training Epoch 2] Batch 4452, Loss 0.45949018001556396\n","[Training Epoch 2] Batch 4453, Loss 0.4266946315765381\n","[Training Epoch 2] Batch 4454, Loss 0.4297502040863037\n","[Training Epoch 2] Batch 4455, Loss 0.4482208490371704\n","[Training Epoch 2] Batch 4456, Loss 0.42914754152297974\n","[Training Epoch 2] Batch 4457, Loss 0.48045969009399414\n","[Training Epoch 2] Batch 4458, Loss 0.4533316493034363\n","[Training Epoch 2] Batch 4459, Loss 0.44554436206817627\n","[Training Epoch 2] Batch 4460, Loss 0.4568940997123718\n","[Training Epoch 2] Batch 4461, Loss 0.4525146484375\n","[Training Epoch 2] Batch 4462, Loss 0.46944236755371094\n","[Training Epoch 2] Batch 4463, Loss 0.4418110251426697\n","[Training Epoch 2] Batch 4464, Loss 0.4298825263977051\n","[Training Epoch 2] Batch 4465, Loss 0.4545660614967346\n","[Training Epoch 2] Batch 4466, Loss 0.4569920301437378\n","[Training Epoch 2] Batch 4467, Loss 0.4460070729255676\n","[Training Epoch 2] Batch 4468, Loss 0.43101370334625244\n","[Training Epoch 2] Batch 4469, Loss 0.45763128995895386\n","[Training Epoch 2] Batch 4470, Loss 0.47654247283935547\n","[Training Epoch 2] Batch 4471, Loss 0.438072144985199\n","[Training Epoch 2] Batch 4472, Loss 0.47376298904418945\n","[Training Epoch 2] Batch 4473, Loss 0.41423940658569336\n","[Training Epoch 2] Batch 4474, Loss 0.438260018825531\n","[Training Epoch 2] Batch 4475, Loss 0.4679951071739197\n","[Training Epoch 2] Batch 4476, Loss 0.47215327620506287\n","[Training Epoch 2] Batch 4477, Loss 0.4517519474029541\n","[Training Epoch 2] Batch 4478, Loss 0.40784531831741333\n","[Training Epoch 2] Batch 4479, Loss 0.44679996371269226\n","[Training Epoch 2] Batch 4480, Loss 0.4427434802055359\n","[Training Epoch 2] Batch 4481, Loss 0.4267367124557495\n","[Training Epoch 2] Batch 4482, Loss 0.4488162398338318\n","[Training Epoch 2] Batch 4483, Loss 0.4250180125236511\n","[Training Epoch 2] Batch 4484, Loss 0.4371987581253052\n","[Training Epoch 2] Batch 4485, Loss 0.46965456008911133\n","[Training Epoch 2] Batch 4486, Loss 0.46029698848724365\n","[Training Epoch 2] Batch 4487, Loss 0.47037768363952637\n","[Training Epoch 2] Batch 4488, Loss 0.48705628514289856\n","[Training Epoch 2] Batch 4489, Loss 0.4701371490955353\n","[Training Epoch 2] Batch 4490, Loss 0.44520509243011475\n","[Training Epoch 2] Batch 4491, Loss 0.4388025999069214\n","[Training Epoch 2] Batch 4492, Loss 0.45608651638031006\n","[Training Epoch 2] Batch 4493, Loss 0.46429747343063354\n","[Training Epoch 2] Batch 4494, Loss 0.4640456736087799\n","[Training Epoch 2] Batch 4495, Loss 0.4324083924293518\n","[Training Epoch 2] Batch 4496, Loss 0.46634340286254883\n","[Training Epoch 2] Batch 4497, Loss 0.4774412512779236\n","[Training Epoch 2] Batch 4498, Loss 0.4609113335609436\n","[Training Epoch 2] Batch 4499, Loss 0.45507577061653137\n","[Training Epoch 2] Batch 4500, Loss 0.4513195753097534\n","[Training Epoch 2] Batch 4501, Loss 0.4429522752761841\n","[Training Epoch 2] Batch 4502, Loss 0.43175560235977173\n","[Training Epoch 2] Batch 4503, Loss 0.4482955038547516\n","[Training Epoch 2] Batch 4504, Loss 0.45443618297576904\n","[Training Epoch 2] Batch 4505, Loss 0.4784688353538513\n","[Training Epoch 2] Batch 4506, Loss 0.45973116159439087\n","[Training Epoch 2] Batch 4507, Loss 0.47369492053985596\n","[Training Epoch 2] Batch 4508, Loss 0.44654178619384766\n","[Training Epoch 2] Batch 4509, Loss 0.42722252011299133\n","[Training Epoch 2] Batch 4510, Loss 0.4442257285118103\n","[Training Epoch 2] Batch 4511, Loss 0.4370343089103699\n","[Training Epoch 2] Batch 4512, Loss 0.45510002970695496\n","[Training Epoch 2] Batch 4513, Loss 0.444845050573349\n","[Training Epoch 2] Batch 4514, Loss 0.4429226517677307\n","[Training Epoch 2] Batch 4515, Loss 0.460629403591156\n","[Training Epoch 2] Batch 4516, Loss 0.4633225202560425\n","[Training Epoch 2] Batch 4517, Loss 0.4503154754638672\n","[Training Epoch 2] Batch 4518, Loss 0.4233449101448059\n","[Training Epoch 2] Batch 4519, Loss 0.4742748737335205\n","[Training Epoch 2] Batch 4520, Loss 0.4769037365913391\n","[Training Epoch 2] Batch 4521, Loss 0.48869508504867554\n","[Training Epoch 2] Batch 4522, Loss 0.43031075596809387\n","[Training Epoch 2] Batch 4523, Loss 0.4291602075099945\n","[Training Epoch 2] Batch 4524, Loss 0.4287700951099396\n","[Training Epoch 2] Batch 4525, Loss 0.4605954885482788\n","[Training Epoch 2] Batch 4526, Loss 0.4283287525177002\n","[Training Epoch 2] Batch 4527, Loss 0.4417288899421692\n","[Training Epoch 2] Batch 4528, Loss 0.4446296989917755\n","[Training Epoch 2] Batch 4529, Loss 0.45009031891822815\n","[Training Epoch 2] Batch 4530, Loss 0.47808128595352173\n","[Training Epoch 2] Batch 4531, Loss 0.4699283242225647\n","[Training Epoch 2] Batch 4532, Loss 0.4370579123497009\n","[Training Epoch 2] Batch 4533, Loss 0.4394210875034332\n","[Training Epoch 2] Batch 4534, Loss 0.4455745816230774\n","[Training Epoch 2] Batch 4535, Loss 0.4507521092891693\n","[Training Epoch 2] Batch 4536, Loss 0.4463956952095032\n","[Training Epoch 2] Batch 4537, Loss 0.47778084874153137\n","[Training Epoch 2] Batch 4538, Loss 0.46602100133895874\n","[Training Epoch 2] Batch 4539, Loss 0.4744163453578949\n","[Training Epoch 2] Batch 4540, Loss 0.46687623858451843\n","[Training Epoch 2] Batch 4541, Loss 0.4296551048755646\n","[Training Epoch 2] Batch 4542, Loss 0.44532883167266846\n","[Training Epoch 2] Batch 4543, Loss 0.4670153856277466\n","[Training Epoch 2] Batch 4544, Loss 0.4311588704586029\n","[Training Epoch 2] Batch 4545, Loss 0.4552172124385834\n","[Training Epoch 2] Batch 4546, Loss 0.45385974645614624\n","[Training Epoch 2] Batch 4547, Loss 0.4432944059371948\n","[Training Epoch 2] Batch 4548, Loss 0.4412127733230591\n","[Training Epoch 2] Batch 4549, Loss 0.4738157391548157\n","[Training Epoch 2] Batch 4550, Loss 0.46833017468452454\n","[Training Epoch 2] Batch 4551, Loss 0.4190462827682495\n","[Training Epoch 2] Batch 4552, Loss 0.4512920081615448\n","[Training Epoch 2] Batch 4553, Loss 0.46714526414871216\n","[Training Epoch 2] Batch 4554, Loss 0.4694116711616516\n","[Training Epoch 2] Batch 4555, Loss 0.4411267340183258\n","[Training Epoch 2] Batch 4556, Loss 0.43160921335220337\n","[Training Epoch 2] Batch 4557, Loss 0.47914546728134155\n","[Training Epoch 2] Batch 4558, Loss 0.4649001955986023\n","[Training Epoch 2] Batch 4559, Loss 0.4773566424846649\n","[Training Epoch 2] Batch 4560, Loss 0.48518800735473633\n","[Training Epoch 2] Batch 4561, Loss 0.4674893021583557\n","[Training Epoch 2] Batch 4562, Loss 0.4631349444389343\n","[Training Epoch 2] Batch 4563, Loss 0.48093515634536743\n","[Training Epoch 2] Batch 4564, Loss 0.4528961777687073\n","[Training Epoch 2] Batch 4565, Loss 0.44153010845184326\n","[Training Epoch 2] Batch 4566, Loss 0.4541811943054199\n","[Training Epoch 2] Batch 4567, Loss 0.44925743341445923\n","[Training Epoch 2] Batch 4568, Loss 0.42562925815582275\n","[Training Epoch 2] Batch 4569, Loss 0.46313127875328064\n","[Training Epoch 2] Batch 4570, Loss 0.4599854350090027\n","[Training Epoch 2] Batch 4571, Loss 0.4554547667503357\n","[Training Epoch 2] Batch 4572, Loss 0.42368975281715393\n","[Training Epoch 2] Batch 4573, Loss 0.4505607485771179\n","[Training Epoch 2] Batch 4574, Loss 0.4811345338821411\n","[Training Epoch 2] Batch 4575, Loss 0.4361081123352051\n","[Training Epoch 2] Batch 4576, Loss 0.4451833963394165\n","[Training Epoch 2] Batch 4577, Loss 0.4576040506362915\n","[Training Epoch 2] Batch 4578, Loss 0.45307692885398865\n","[Training Epoch 2] Batch 4579, Loss 0.44525474309921265\n","[Training Epoch 2] Batch 4580, Loss 0.47900259494781494\n","[Training Epoch 2] Batch 4581, Loss 0.41725218296051025\n","[Training Epoch 2] Batch 4582, Loss 0.4736247956752777\n","[Training Epoch 2] Batch 4583, Loss 0.4166256785392761\n","[Training Epoch 2] Batch 4584, Loss 0.44723936915397644\n","[Training Epoch 2] Batch 4585, Loss 0.48563653230667114\n","[Training Epoch 2] Batch 4586, Loss 0.4548320174217224\n","[Training Epoch 2] Batch 4587, Loss 0.40357133746147156\n","[Training Epoch 2] Batch 4588, Loss 0.461553692817688\n","[Training Epoch 2] Batch 4589, Loss 0.43365710973739624\n","[Training Epoch 2] Batch 4590, Loss 0.48713454604148865\n","[Training Epoch 2] Batch 4591, Loss 0.45634064078330994\n","[Training Epoch 2] Batch 4592, Loss 0.42621710896492004\n","[Training Epoch 2] Batch 4593, Loss 0.43371719121932983\n","[Training Epoch 2] Batch 4594, Loss 0.4628843069076538\n","[Training Epoch 2] Batch 4595, Loss 0.47172093391418457\n","[Training Epoch 2] Batch 4596, Loss 0.44439008831977844\n","[Training Epoch 2] Batch 4597, Loss 0.4423198103904724\n","[Training Epoch 2] Batch 4598, Loss 0.45462825894355774\n","[Training Epoch 2] Batch 4599, Loss 0.4663638174533844\n","[Training Epoch 2] Batch 4600, Loss 0.44633808732032776\n","[Training Epoch 2] Batch 4601, Loss 0.4703931510448456\n","[Training Epoch 2] Batch 4602, Loss 0.45049822330474854\n","[Training Epoch 2] Batch 4603, Loss 0.43214377760887146\n","[Training Epoch 2] Batch 4604, Loss 0.4875856339931488\n","[Training Epoch 2] Batch 4605, Loss 0.4630963206291199\n","[Training Epoch 2] Batch 4606, Loss 0.45862895250320435\n","[Training Epoch 2] Batch 4607, Loss 0.45958811044692993\n","[Training Epoch 2] Batch 4608, Loss 0.4384698271751404\n","[Training Epoch 2] Batch 4609, Loss 0.4583207964897156\n","[Training Epoch 2] Batch 4610, Loss 0.46817052364349365\n","[Training Epoch 2] Batch 4611, Loss 0.47027787566185\n","[Training Epoch 2] Batch 4612, Loss 0.43846672773361206\n","[Training Epoch 2] Batch 4613, Loss 0.4383091628551483\n","[Training Epoch 2] Batch 4614, Loss 0.43020695447921753\n","[Training Epoch 2] Batch 4615, Loss 0.44131264090538025\n","[Training Epoch 2] Batch 4616, Loss 0.43517595529556274\n","[Training Epoch 2] Batch 4617, Loss 0.45709294080734253\n","[Training Epoch 2] Batch 4618, Loss 0.4339888095855713\n","[Training Epoch 2] Batch 4619, Loss 0.4399694800376892\n","[Training Epoch 2] Batch 4620, Loss 0.4836329519748688\n","[Training Epoch 2] Batch 4621, Loss 0.47100478410720825\n","[Training Epoch 2] Batch 4622, Loss 0.441348135471344\n","[Training Epoch 2] Batch 4623, Loss 0.3975341320037842\n","[Training Epoch 2] Batch 4624, Loss 0.4700087308883667\n","[Training Epoch 2] Batch 4625, Loss 0.45492303371429443\n","[Training Epoch 2] Batch 4626, Loss 0.42050954699516296\n","[Training Epoch 2] Batch 4627, Loss 0.4272787570953369\n","[Training Epoch 2] Batch 4628, Loss 0.4681866765022278\n","[Training Epoch 2] Batch 4629, Loss 0.4418472349643707\n","[Training Epoch 2] Batch 4630, Loss 0.4774767756462097\n","[Training Epoch 2] Batch 4631, Loss 0.456792950630188\n","[Training Epoch 2] Batch 4632, Loss 0.4491448402404785\n","[Training Epoch 2] Batch 4633, Loss 0.44193553924560547\n","[Training Epoch 2] Batch 4634, Loss 0.461234986782074\n","[Training Epoch 2] Batch 4635, Loss 0.47623980045318604\n","[Training Epoch 2] Batch 4636, Loss 0.45115095376968384\n","[Training Epoch 2] Batch 4637, Loss 0.43531811237335205\n","[Training Epoch 2] Batch 4638, Loss 0.44249239563941956\n","[Training Epoch 2] Batch 4639, Loss 0.4720169007778168\n","[Training Epoch 2] Batch 4640, Loss 0.45346295833587646\n","[Training Epoch 2] Batch 4641, Loss 0.4499816298484802\n","[Training Epoch 2] Batch 4642, Loss 0.48636195063591003\n","[Training Epoch 2] Batch 4643, Loss 0.43849509954452515\n","[Training Epoch 2] Batch 4644, Loss 0.4344332814216614\n","[Training Epoch 2] Batch 4645, Loss 0.4559311866760254\n","[Training Epoch 2] Batch 4646, Loss 0.4396388530731201\n","[Training Epoch 2] Batch 4647, Loss 0.4462392330169678\n","[Training Epoch 2] Batch 4648, Loss 0.4282595217227936\n","[Training Epoch 2] Batch 4649, Loss 0.4677186906337738\n","[Training Epoch 2] Batch 4650, Loss 0.46015629172325134\n","[Training Epoch 2] Batch 4651, Loss 0.4367423355579376\n","[Training Epoch 2] Batch 4652, Loss 0.4252834916114807\n","[Training Epoch 2] Batch 4653, Loss 0.4416933059692383\n","[Training Epoch 2] Batch 4654, Loss 0.45182183384895325\n","[Training Epoch 2] Batch 4655, Loss 0.4768292307853699\n","[Training Epoch 2] Batch 4656, Loss 0.40873801708221436\n","[Training Epoch 2] Batch 4657, Loss 0.45582354068756104\n","[Training Epoch 2] Batch 4658, Loss 0.4799823462963104\n","[Training Epoch 2] Batch 4659, Loss 0.4662281274795532\n","[Training Epoch 2] Batch 4660, Loss 0.46673113107681274\n","[Training Epoch 2] Batch 4661, Loss 0.43585366010665894\n","[Training Epoch 2] Batch 4662, Loss 0.4725797772407532\n","[Training Epoch 2] Batch 4663, Loss 0.44179800152778625\n","[Training Epoch 2] Batch 4664, Loss 0.4638229012489319\n","[Training Epoch 2] Batch 4665, Loss 0.4696102738380432\n","[Training Epoch 2] Batch 4666, Loss 0.44167590141296387\n","[Training Epoch 2] Batch 4667, Loss 0.4522989094257355\n","[Training Epoch 2] Batch 4668, Loss 0.44615209102630615\n","[Training Epoch 2] Batch 4669, Loss 0.4446102976799011\n","[Training Epoch 2] Batch 4670, Loss 0.4533557891845703\n","[Training Epoch 2] Batch 4671, Loss 0.45240578055381775\n","[Training Epoch 2] Batch 4672, Loss 0.44548603892326355\n","[Training Epoch 2] Batch 4673, Loss 0.46517378091812134\n","[Training Epoch 2] Batch 4674, Loss 0.42755454778671265\n","[Training Epoch 2] Batch 4675, Loss 0.43920648097991943\n","[Training Epoch 2] Batch 4676, Loss 0.4427364766597748\n","[Training Epoch 2] Batch 4677, Loss 0.4464161694049835\n","[Training Epoch 2] Batch 4678, Loss 0.4548138380050659\n","[Training Epoch 2] Batch 4679, Loss 0.45853477716445923\n","[Training Epoch 2] Batch 4680, Loss 0.46057283878326416\n","[Training Epoch 2] Batch 4681, Loss 0.4481041729450226\n","[Training Epoch 2] Batch 4682, Loss 0.4755712151527405\n","[Training Epoch 2] Batch 4683, Loss 0.44858518242836\n","[Training Epoch 2] Batch 4684, Loss 0.434332013130188\n","[Training Epoch 2] Batch 4685, Loss 0.4494172930717468\n","[Training Epoch 2] Batch 4686, Loss 0.44152283668518066\n","[Training Epoch 2] Batch 4687, Loss 0.4346086382865906\n","[Training Epoch 2] Batch 4688, Loss 0.45127004384994507\n","[Training Epoch 2] Batch 4689, Loss 0.4392010569572449\n","[Training Epoch 2] Batch 4690, Loss 0.42491188645362854\n","[Training Epoch 2] Batch 4691, Loss 0.4570103883743286\n","[Training Epoch 2] Batch 4692, Loss 0.4474780261516571\n","[Training Epoch 2] Batch 4693, Loss 0.48213526606559753\n","[Training Epoch 2] Batch 4694, Loss 0.43487754464149475\n","[Training Epoch 2] Batch 4695, Loss 0.43838629126548767\n","[Training Epoch 2] Batch 4696, Loss 0.45222142338752747\n","[Training Epoch 2] Batch 4697, Loss 0.4429248869419098\n","[Training Epoch 2] Batch 4698, Loss 0.46901679039001465\n","[Training Epoch 2] Batch 4699, Loss 0.4523361921310425\n","[Training Epoch 2] Batch 4700, Loss 0.44972407817840576\n","[Training Epoch 2] Batch 4701, Loss 0.4523929953575134\n","[Training Epoch 2] Batch 4702, Loss 0.4577677249908447\n","[Training Epoch 2] Batch 4703, Loss 0.43964406847953796\n","[Training Epoch 2] Batch 4704, Loss 0.43290966749191284\n","[Training Epoch 2] Batch 4705, Loss 0.4473245143890381\n","[Training Epoch 2] Batch 4706, Loss 0.4550219774246216\n","[Training Epoch 2] Batch 4707, Loss 0.45922791957855225\n","[Training Epoch 2] Batch 4708, Loss 0.4768034517765045\n","[Training Epoch 2] Batch 4709, Loss 0.45428144931793213\n","[Training Epoch 2] Batch 4710, Loss 0.42220285534858704\n","[Training Epoch 2] Batch 4711, Loss 0.4551315903663635\n","[Training Epoch 2] Batch 4712, Loss 0.4665408730506897\n","[Training Epoch 2] Batch 4713, Loss 0.46383243799209595\n","[Training Epoch 2] Batch 4714, Loss 0.4659222662448883\n","[Training Epoch 2] Batch 4715, Loss 0.483323335647583\n","[Training Epoch 2] Batch 4716, Loss 0.4541134238243103\n","[Training Epoch 2] Batch 4717, Loss 0.4271886348724365\n","[Training Epoch 2] Batch 4718, Loss 0.4695417881011963\n","[Training Epoch 2] Batch 4719, Loss 0.48133161664009094\n","[Training Epoch 2] Batch 4720, Loss 0.4396857023239136\n","[Training Epoch 2] Batch 4721, Loss 0.462371826171875\n","[Training Epoch 2] Batch 4722, Loss 0.44397151470184326\n","[Training Epoch 2] Batch 4723, Loss 0.47179365158081055\n","[Training Epoch 2] Batch 4724, Loss 0.4749409556388855\n","[Training Epoch 2] Batch 4725, Loss 0.4668300449848175\n","[Training Epoch 2] Batch 4726, Loss 0.4481196403503418\n","[Training Epoch 2] Batch 4727, Loss 0.4318147301673889\n","[Training Epoch 2] Batch 4728, Loss 0.43881189823150635\n","[Training Epoch 2] Batch 4729, Loss 0.45517539978027344\n","[Training Epoch 2] Batch 4730, Loss 0.4801163375377655\n","[Training Epoch 2] Batch 4731, Loss 0.45328307151794434\n","[Training Epoch 2] Batch 4732, Loss 0.4444164037704468\n","[Training Epoch 2] Batch 4733, Loss 0.4328255355358124\n","[Training Epoch 2] Batch 4734, Loss 0.42698541283607483\n","[Training Epoch 2] Batch 4735, Loss 0.45492231845855713\n","[Training Epoch 2] Batch 4736, Loss 0.4621008634567261\n","[Training Epoch 2] Batch 4737, Loss 0.4544718861579895\n","[Training Epoch 2] Batch 4738, Loss 0.40559887886047363\n","[Training Epoch 2] Batch 4739, Loss 0.4377308189868927\n","[Training Epoch 2] Batch 4740, Loss 0.44739967584609985\n","[Training Epoch 2] Batch 4741, Loss 0.4783438742160797\n","[Training Epoch 2] Batch 4742, Loss 0.43708449602127075\n","[Training Epoch 2] Batch 4743, Loss 0.4467759132385254\n","[Training Epoch 2] Batch 4744, Loss 0.4410954713821411\n","[Training Epoch 2] Batch 4745, Loss 0.4532325863838196\n","[Training Epoch 2] Batch 4746, Loss 0.4469074308872223\n","[Training Epoch 2] Batch 4747, Loss 0.4307185113430023\n","[Training Epoch 2] Batch 4748, Loss 0.4516790807247162\n","[Training Epoch 2] Batch 4749, Loss 0.4358155429363251\n","[Training Epoch 2] Batch 4750, Loss 0.4518999755382538\n","[Training Epoch 2] Batch 4751, Loss 0.4737803339958191\n","[Training Epoch 2] Batch 4752, Loss 0.46407580375671387\n","[Training Epoch 2] Batch 4753, Loss 0.44686388969421387\n","[Training Epoch 2] Batch 4754, Loss 0.45400503277778625\n","[Training Epoch 2] Batch 4755, Loss 0.45129743218421936\n","[Training Epoch 2] Batch 4756, Loss 0.48163914680480957\n","[Training Epoch 2] Batch 4757, Loss 0.4757947325706482\n","[Training Epoch 2] Batch 4758, Loss 0.4395560026168823\n","[Training Epoch 2] Batch 4759, Loss 0.4486578106880188\n","[Training Epoch 2] Batch 4760, Loss 0.4435638189315796\n","[Training Epoch 2] Batch 4761, Loss 0.4587876498699188\n","[Training Epoch 2] Batch 4762, Loss 0.4729780852794647\n","[Training Epoch 2] Batch 4763, Loss 0.4133811891078949\n","[Training Epoch 2] Batch 4764, Loss 0.44582220911979675\n","[Training Epoch 2] Batch 4765, Loss 0.4454159736633301\n","[Training Epoch 2] Batch 4766, Loss 0.4361443519592285\n","[Training Epoch 2] Batch 4767, Loss 0.43849772214889526\n","[Training Epoch 2] Batch 4768, Loss 0.47217610478401184\n","[Training Epoch 2] Batch 4769, Loss 0.4595784544944763\n","[Training Epoch 2] Batch 4770, Loss 0.4408118724822998\n","[Training Epoch 2] Batch 4771, Loss 0.4270339608192444\n","[Training Epoch 2] Batch 4772, Loss 0.4756348729133606\n","[Training Epoch 2] Batch 4773, Loss 0.4702625870704651\n","[Training Epoch 2] Batch 4774, Loss 0.4509636163711548\n","[Training Epoch 2] Batch 4775, Loss 0.4525183439254761\n","[Training Epoch 2] Batch 4776, Loss 0.41936179995536804\n","[Training Epoch 2] Batch 4777, Loss 0.42570585012435913\n","[Training Epoch 2] Batch 4778, Loss 0.419736385345459\n","[Training Epoch 2] Batch 4779, Loss 0.43257075548171997\n","[Training Epoch 2] Batch 4780, Loss 0.44185352325439453\n","[Training Epoch 2] Batch 4781, Loss 0.44339457154273987\n","[Training Epoch 2] Batch 4782, Loss 0.4467342495918274\n","[Training Epoch 2] Batch 4783, Loss 0.4497997760772705\n","[Training Epoch 2] Batch 4784, Loss 0.4590073227882385\n","[Training Epoch 2] Batch 4785, Loss 0.43041685223579407\n","[Training Epoch 2] Batch 4786, Loss 0.43494829535484314\n","[Training Epoch 2] Batch 4787, Loss 0.42805567383766174\n","[Training Epoch 2] Batch 4788, Loss 0.4433070123195648\n","[Training Epoch 2] Batch 4789, Loss 0.4413987994194031\n","[Training Epoch 2] Batch 4790, Loss 0.46379566192626953\n","[Training Epoch 2] Batch 4791, Loss 0.4226328134536743\n","[Training Epoch 2] Batch 4792, Loss 0.4466363191604614\n","[Training Epoch 2] Batch 4793, Loss 0.4309641420841217\n","[Training Epoch 2] Batch 4794, Loss 0.4644071161746979\n","[Training Epoch 2] Batch 4795, Loss 0.45529109239578247\n","[Training Epoch 2] Batch 4796, Loss 0.46862977743148804\n","[Training Epoch 2] Batch 4797, Loss 0.46986404061317444\n","[Training Epoch 2] Batch 4798, Loss 0.4289500117301941\n","[Training Epoch 2] Batch 4799, Loss 0.45563915371894836\n","[Training Epoch 2] Batch 4800, Loss 0.42824405431747437\n","[Training Epoch 2] Batch 4801, Loss 0.4469712972640991\n","[Training Epoch 2] Batch 4802, Loss 0.4370349049568176\n","[Training Epoch 2] Batch 4803, Loss 0.4888625741004944\n","[Training Epoch 2] Batch 4804, Loss 0.44695553183555603\n","[Training Epoch 2] Batch 4805, Loss 0.43167752027511597\n","[Training Epoch 2] Batch 4806, Loss 0.4234490394592285\n","[Training Epoch 2] Batch 4807, Loss 0.4417259097099304\n","[Training Epoch 2] Batch 4808, Loss 0.43624433875083923\n","[Training Epoch 2] Batch 4809, Loss 0.4838087260723114\n","[Training Epoch 2] Batch 4810, Loss 0.44793611764907837\n","[Training Epoch 2] Batch 4811, Loss 0.44714152812957764\n","[Training Epoch 2] Batch 4812, Loss 0.4167254567146301\n","[Training Epoch 2] Batch 4813, Loss 0.46266186237335205\n","[Training Epoch 2] Batch 4814, Loss 0.4480314254760742\n","[Training Epoch 2] Batch 4815, Loss 0.4338008463382721\n","[Training Epoch 2] Batch 4816, Loss 0.4371848702430725\n","[Training Epoch 2] Batch 4817, Loss 0.45030152797698975\n","[Training Epoch 2] Batch 4818, Loss 0.46454402804374695\n","[Training Epoch 2] Batch 4819, Loss 0.41327205300331116\n","[Training Epoch 2] Batch 4820, Loss 0.43947523832321167\n","[Training Epoch 2] Batch 4821, Loss 0.46232712268829346\n","[Training Epoch 2] Batch 4822, Loss 0.45498862862586975\n","[Training Epoch 2] Batch 4823, Loss 0.480803906917572\n","[Training Epoch 2] Batch 4824, Loss 0.43304526805877686\n","[Training Epoch 2] Batch 4825, Loss 0.45857301354408264\n","[Training Epoch 2] Batch 4826, Loss 0.4516831040382385\n","[Training Epoch 2] Batch 4827, Loss 0.46072012186050415\n","[Training Epoch 2] Batch 4828, Loss 0.43284595012664795\n","[Training Epoch 2] Batch 4829, Loss 0.44923537969589233\n","[Training Epoch 2] Batch 4830, Loss 0.4289621114730835\n","[Training Epoch 2] Batch 4831, Loss 0.4472491145133972\n","[Training Epoch 2] Batch 4832, Loss 0.4312530755996704\n","[Training Epoch 2] Batch 4833, Loss 0.44436630606651306\n","[Training Epoch 2] Batch 4834, Loss 0.4782087802886963\n","[Training Epoch 2] Batch 4835, Loss 0.44403544068336487\n","[Training Epoch 2] Batch 4836, Loss 0.41889688372612\n","[Training Epoch 2] Batch 4837, Loss 0.4541613459587097\n","[Training Epoch 2] Batch 4838, Loss 0.4580079913139343\n","[Training Epoch 2] Batch 4839, Loss 0.46491432189941406\n","[Training Epoch 2] Batch 4840, Loss 0.448623925447464\n","[Training Epoch 2] Batch 4841, Loss 0.45002543926239014\n","[Training Epoch 2] Batch 4842, Loss 0.4599812626838684\n","[Training Epoch 2] Batch 4843, Loss 0.4246175289154053\n","[Training Epoch 2] Batch 4844, Loss 0.44550830125808716\n","[Training Epoch 2] Batch 4845, Loss 0.44857528805732727\n","[Training Epoch 2] Batch 4846, Loss 0.44853341579437256\n","[Training Epoch 2] Batch 4847, Loss 0.4455670118331909\n","[Training Epoch 2] Batch 4848, Loss 0.44916626811027527\n","[Training Epoch 2] Batch 4849, Loss 0.418743759393692\n","[Training Epoch 2] Batch 4850, Loss 0.43811190128326416\n","[Training Epoch 2] Batch 4851, Loss 0.4426446557044983\n","[Training Epoch 2] Batch 4852, Loss 0.483419269323349\n","[Training Epoch 2] Batch 4853, Loss 0.4326799809932709\n","[Training Epoch 2] Batch 4854, Loss 0.4318644404411316\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:52: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame.\n","Try using .loc[row_indexer,col_indexer] = value instead\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"]},{"name":"stdout","output_type":"stream","text":["[Evluating Epoch 2] HR = 0.2795, NDCG = 0.1455\n","Epoch 3 starts !\n","--------------------------------------------------------------------------------\n","[Training Epoch 3] Batch 0, Loss 0.43240225315093994\n","[Training Epoch 3] Batch 1, Loss 0.4462946355342865\n","[Training Epoch 3] Batch 2, Loss 0.4275852143764496\n","[Training Epoch 3] Batch 3, Loss 0.42780163884162903\n","[Training Epoch 3] Batch 4, Loss 0.44714266061782837\n","[Training Epoch 3] Batch 5, Loss 0.4121231138706207\n","[Training Epoch 3] Batch 6, Loss 0.44347262382507324\n","[Training Epoch 3] Batch 7, Loss 0.46766671538352966\n","[Training Epoch 3] Batch 8, Loss 0.4059411883354187\n","[Training Epoch 3] Batch 9, Loss 0.4120780825614929\n","[Training Epoch 3] Batch 10, Loss 0.4247094988822937\n","[Training Epoch 3] Batch 11, Loss 0.43614524602890015\n","[Training Epoch 3] Batch 12, Loss 0.45783835649490356\n","[Training Epoch 3] Batch 13, Loss 0.45321759581565857\n","[Training Epoch 3] Batch 14, Loss 0.42523181438446045\n","[Training Epoch 3] Batch 15, Loss 0.4335278868675232\n","[Training Epoch 3] Batch 16, Loss 0.4386427700519562\n","[Training Epoch 3] Batch 17, Loss 0.43098652362823486\n","[Training Epoch 3] Batch 18, Loss 0.3905821442604065\n","[Training Epoch 3] Batch 19, Loss 0.4420393407344818\n","[Training Epoch 3] Batch 20, Loss 0.4353855848312378\n","[Training Epoch 3] Batch 21, Loss 0.44701719284057617\n","[Training Epoch 3] Batch 22, Loss 0.46020200848579407\n","[Training Epoch 3] Batch 23, Loss 0.46708109974861145\n","[Training Epoch 3] Batch 24, Loss 0.4386499524116516\n","[Training Epoch 3] Batch 25, Loss 0.42525559663772583\n","[Training Epoch 3] Batch 26, Loss 0.43042218685150146\n","[Training Epoch 3] Batch 27, Loss 0.4189275801181793\n","[Training Epoch 3] Batch 28, Loss 0.422647088766098\n","[Training Epoch 3] Batch 29, Loss 0.42273619771003723\n","[Training Epoch 3] Batch 30, Loss 0.45584115386009216\n","[Training Epoch 3] Batch 31, Loss 0.45071178674697876\n","[Training Epoch 3] Batch 32, Loss 0.446657657623291\n","[Training Epoch 3] Batch 33, Loss 0.4563104510307312\n","[Training Epoch 3] Batch 34, Loss 0.41340112686157227\n","[Training Epoch 3] Batch 35, Loss 0.4555915296077728\n","[Training Epoch 3] Batch 36, Loss 0.446369469165802\n","[Training Epoch 3] Batch 37, Loss 0.4458494782447815\n","[Training Epoch 3] Batch 38, Loss 0.448697566986084\n","[Training Epoch 3] Batch 39, Loss 0.4293822944164276\n","[Training Epoch 3] Batch 40, Loss 0.4376620054244995\n","[Training Epoch 3] Batch 41, Loss 0.45460817217826843\n","[Training Epoch 3] Batch 42, Loss 0.4625457525253296\n","[Training Epoch 3] Batch 43, Loss 0.44595789909362793\n","[Training Epoch 3] Batch 44, Loss 0.43348824977874756\n","[Training Epoch 3] Batch 45, Loss 0.46367883682250977\n","[Training Epoch 3] Batch 46, Loss 0.43955233693122864\n","[Training Epoch 3] Batch 47, Loss 0.4274420440196991\n","[Training Epoch 3] Batch 48, Loss 0.4691302478313446\n","[Training Epoch 3] Batch 49, Loss 0.4461362957954407\n","[Training Epoch 3] Batch 50, Loss 0.43977832794189453\n","[Training Epoch 3] Batch 51, Loss 0.4423469603061676\n","[Training Epoch 3] Batch 52, Loss 0.45219695568084717\n","[Training Epoch 3] Batch 53, Loss 0.43659961223602295\n","[Training Epoch 3] Batch 54, Loss 0.45843571424484253\n","[Training Epoch 3] Batch 55, Loss 0.43743428587913513\n","[Training Epoch 3] Batch 56, Loss 0.42909812927246094\n","[Training Epoch 3] Batch 57, Loss 0.4523456394672394\n","[Training Epoch 3] Batch 58, Loss 0.4467204213142395\n","[Training Epoch 3] Batch 59, Loss 0.4200724959373474\n","[Training Epoch 3] Batch 60, Loss 0.4747644066810608\n","[Training Epoch 3] Batch 61, Loss 0.49580132961273193\n","[Training Epoch 3] Batch 62, Loss 0.45920610427856445\n","[Training Epoch 3] Batch 63, Loss 0.45478981733322144\n","[Training Epoch 3] Batch 64, Loss 0.4537166357040405\n","[Training Epoch 3] Batch 65, Loss 0.4551491141319275\n","[Training Epoch 3] Batch 66, Loss 0.4550919532775879\n","[Training Epoch 3] Batch 67, Loss 0.47435328364372253\n","[Training Epoch 3] Batch 68, Loss 0.43944957852363586\n","[Training Epoch 3] Batch 69, Loss 0.43380051851272583\n","[Training Epoch 3] Batch 70, Loss 0.4311419129371643\n","[Training Epoch 3] Batch 71, Loss 0.45750856399536133\n","[Training Epoch 3] Batch 72, Loss 0.44631117582321167\n","[Training Epoch 3] Batch 73, Loss 0.4468390941619873\n","[Training Epoch 3] Batch 74, Loss 0.42675966024398804\n","[Training Epoch 3] Batch 75, Loss 0.44260525703430176\n","[Training Epoch 3] Batch 76, Loss 0.46095025539398193\n","[Training Epoch 3] Batch 77, Loss 0.48690563440322876\n","[Training Epoch 3] Batch 78, Loss 0.4493098556995392\n","[Training Epoch 3] Batch 79, Loss 0.4415628910064697\n","[Training Epoch 3] Batch 80, Loss 0.43020257353782654\n","[Training Epoch 3] Batch 81, Loss 0.45137226581573486\n","[Training Epoch 3] Batch 82, Loss 0.44639354944229126\n","[Training Epoch 3] Batch 83, Loss 0.42473751306533813\n","[Training Epoch 3] Batch 84, Loss 0.4332764446735382\n","[Training Epoch 3] Batch 85, Loss 0.43743494153022766\n","[Training Epoch 3] Batch 86, Loss 0.4537116587162018\n","[Training Epoch 3] Batch 87, Loss 0.4270733892917633\n","[Training Epoch 3] Batch 88, Loss 0.4333640933036804\n","[Training Epoch 3] Batch 89, Loss 0.4401542544364929\n","[Training Epoch 3] Batch 90, Loss 0.429720014333725\n","[Training Epoch 3] Batch 91, Loss 0.42538851499557495\n","[Training Epoch 3] Batch 92, Loss 0.4546634256839752\n","[Training Epoch 3] Batch 93, Loss 0.44900137186050415\n","[Training Epoch 3] Batch 94, Loss 0.43760934472084045\n","[Training Epoch 3] Batch 95, Loss 0.4395136535167694\n","[Training Epoch 3] Batch 96, Loss 0.45906248688697815\n","[Training Epoch 3] Batch 97, Loss 0.4687420725822449\n","[Training Epoch 3] Batch 98, Loss 0.4648282825946808\n","[Training Epoch 3] Batch 99, Loss 0.46206074953079224\n","[Training Epoch 3] Batch 100, Loss 0.43525606393814087\n","[Training Epoch 3] Batch 101, Loss 0.41877031326293945\n","[Training Epoch 3] Batch 102, Loss 0.4269762933254242\n","[Training Epoch 3] Batch 103, Loss 0.42303717136383057\n","[Training Epoch 3] Batch 104, Loss 0.4499167203903198\n","[Training Epoch 3] Batch 105, Loss 0.4521939754486084\n","[Training Epoch 3] Batch 106, Loss 0.4511181116104126\n","[Training Epoch 3] Batch 107, Loss 0.47189512848854065\n","[Training Epoch 3] Batch 108, Loss 0.4469743072986603\n","[Training Epoch 3] Batch 109, Loss 0.4379485249519348\n","[Training Epoch 3] Batch 110, Loss 0.4264494776725769\n","[Training Epoch 3] Batch 111, Loss 0.42722415924072266\n","[Training Epoch 3] Batch 112, Loss 0.44884800910949707\n","[Training Epoch 3] Batch 113, Loss 0.457691490650177\n","[Training Epoch 3] Batch 114, Loss 0.4375847578048706\n","[Training Epoch 3] Batch 115, Loss 0.45141440629959106\n","[Training Epoch 3] Batch 116, Loss 0.4589853882789612\n","[Training Epoch 3] Batch 117, Loss 0.46418848633766174\n","[Training Epoch 3] Batch 118, Loss 0.42696085572242737\n","[Training Epoch 3] Batch 119, Loss 0.411885142326355\n","[Training Epoch 3] Batch 120, Loss 0.435968816280365\n","[Training Epoch 3] Batch 121, Loss 0.44105345010757446\n","[Training Epoch 3] Batch 122, Loss 0.4324193000793457\n","[Training Epoch 3] Batch 123, Loss 0.4823857843875885\n","[Training Epoch 3] Batch 124, Loss 0.4363919198513031\n","[Training Epoch 3] Batch 125, Loss 0.4107967019081116\n","[Training Epoch 3] Batch 126, Loss 0.453301340341568\n","[Training Epoch 3] Batch 127, Loss 0.4717140793800354\n","[Training Epoch 3] Batch 128, Loss 0.4435974061489105\n","[Training Epoch 3] Batch 129, Loss 0.41516005992889404\n","[Training Epoch 3] Batch 130, Loss 0.4529758095741272\n","[Training Epoch 3] Batch 131, Loss 0.4415305256843567\n","[Training Epoch 3] Batch 132, Loss 0.44240307807922363\n","[Training Epoch 3] Batch 133, Loss 0.44140854477882385\n","[Training Epoch 3] Batch 134, Loss 0.4413629174232483\n","[Training Epoch 3] Batch 135, Loss 0.45578935742378235\n","[Training Epoch 3] Batch 136, Loss 0.43067097663879395\n","[Training Epoch 3] Batch 137, Loss 0.4491683840751648\n","[Training Epoch 3] Batch 138, Loss 0.4068642556667328\n","[Training Epoch 3] Batch 139, Loss 0.4316517114639282\n","[Training Epoch 3] Batch 140, Loss 0.42209771275520325\n","[Training Epoch 3] Batch 141, Loss 0.45130375027656555\n","[Training Epoch 3] Batch 142, Loss 0.4348410964012146\n","[Training Epoch 3] Batch 143, Loss 0.4107167720794678\n","[Training Epoch 3] Batch 144, Loss 0.4372892677783966\n","[Training Epoch 3] Batch 145, Loss 0.4329378604888916\n","[Training Epoch 3] Batch 146, Loss 0.4478546380996704\n","[Training Epoch 3] Batch 147, Loss 0.4469316005706787\n","[Training Epoch 3] Batch 148, Loss 0.4458805322647095\n","[Training Epoch 3] Batch 149, Loss 0.42593348026275635\n","[Training Epoch 3] Batch 150, Loss 0.43260976672172546\n","[Training Epoch 3] Batch 151, Loss 0.4427669644355774\n","[Training Epoch 3] Batch 152, Loss 0.44602662324905396\n","[Training Epoch 3] Batch 153, Loss 0.430724561214447\n","[Training Epoch 3] Batch 154, Loss 0.46286046504974365\n","[Training Epoch 3] Batch 155, Loss 0.4359617233276367\n","[Training Epoch 3] Batch 156, Loss 0.4424470067024231\n","[Training Epoch 3] Batch 157, Loss 0.3913843631744385\n","[Training Epoch 3] Batch 158, Loss 0.4385397732257843\n","[Training Epoch 3] Batch 159, Loss 0.4309019446372986\n","[Training Epoch 3] Batch 160, Loss 0.4510439336299896\n","[Training Epoch 3] Batch 161, Loss 0.44268637895584106\n","[Training Epoch 3] Batch 162, Loss 0.4151448607444763\n","[Training Epoch 3] Batch 163, Loss 0.41788744926452637\n","[Training Epoch 3] Batch 164, Loss 0.4113665819168091\n","[Training Epoch 3] Batch 165, Loss 0.46980759501457214\n","[Training Epoch 3] Batch 166, Loss 0.41408708691596985\n","[Training Epoch 3] Batch 167, Loss 0.41013389825820923\n","[Training Epoch 3] Batch 168, Loss 0.4208880662918091\n","[Training Epoch 3] Batch 169, Loss 0.4357016682624817\n","[Training Epoch 3] Batch 170, Loss 0.41313648223876953\n","[Training Epoch 3] Batch 171, Loss 0.4289913475513458\n","[Training Epoch 3] Batch 172, Loss 0.4310724139213562\n","[Training Epoch 3] Batch 173, Loss 0.4074597954750061\n","[Training Epoch 3] Batch 174, Loss 0.44694215059280396\n","[Training Epoch 3] Batch 175, Loss 0.44491076469421387\n","[Training Epoch 3] Batch 176, Loss 0.40838152170181274\n","[Training Epoch 3] Batch 177, Loss 0.44931328296661377\n","[Training Epoch 3] Batch 178, Loss 0.4576457738876343\n","[Training Epoch 3] Batch 179, Loss 0.44212770462036133\n","[Training Epoch 3] Batch 180, Loss 0.42791444063186646\n","[Training Epoch 3] Batch 181, Loss 0.43522125482559204\n","[Training Epoch 3] Batch 182, Loss 0.4431701600551605\n","[Training Epoch 3] Batch 183, Loss 0.4146961569786072\n","[Training Epoch 3] Batch 184, Loss 0.45267969369888306\n","[Training Epoch 3] Batch 185, Loss 0.4371047616004944\n","[Training Epoch 3] Batch 186, Loss 0.4184268116950989\n","[Training Epoch 3] Batch 187, Loss 0.45380017161369324\n","[Training Epoch 3] Batch 188, Loss 0.4621017575263977\n","[Training Epoch 3] Batch 189, Loss 0.4311367869377136\n","[Training Epoch 3] Batch 190, Loss 0.41573870182037354\n","[Training Epoch 3] Batch 191, Loss 0.4488341808319092\n","[Training Epoch 3] Batch 192, Loss 0.4385615587234497\n","[Training Epoch 3] Batch 193, Loss 0.4230440855026245\n","[Training Epoch 3] Batch 194, Loss 0.45494192838668823\n","[Training Epoch 3] Batch 195, Loss 0.4156720042228699\n","[Training Epoch 3] Batch 196, Loss 0.4510989189147949\n","[Training Epoch 3] Batch 197, Loss 0.47084301710128784\n","[Training Epoch 3] Batch 198, Loss 0.4539490342140198\n","[Training Epoch 3] Batch 199, Loss 0.4311707019805908\n","[Training Epoch 3] Batch 200, Loss 0.4787081480026245\n","[Training Epoch 3] Batch 201, Loss 0.4331549406051636\n","[Training Epoch 3] Batch 202, Loss 0.44345343112945557\n","[Training Epoch 3] Batch 203, Loss 0.4135024845600128\n","[Training Epoch 3] Batch 204, Loss 0.44918960332870483\n","[Training Epoch 3] Batch 205, Loss 0.4231932759284973\n","[Training Epoch 3] Batch 206, Loss 0.4267929196357727\n","[Training Epoch 3] Batch 207, Loss 0.4388255476951599\n","[Training Epoch 3] Batch 208, Loss 0.44341009855270386\n","[Training Epoch 3] Batch 209, Loss 0.45812129974365234\n","[Training Epoch 3] Batch 210, Loss 0.44386669993400574\n","[Training Epoch 3] Batch 211, Loss 0.4547211825847626\n","[Training Epoch 3] Batch 212, Loss 0.4446524381637573\n","[Training Epoch 3] Batch 213, Loss 0.46567201614379883\n","[Training Epoch 3] Batch 214, Loss 0.4268510341644287\n","[Training Epoch 3] Batch 215, Loss 0.3958238661289215\n","[Training Epoch 3] Batch 216, Loss 0.4802396297454834\n","[Training Epoch 3] Batch 217, Loss 0.45322471857070923\n","[Training Epoch 3] Batch 218, Loss 0.4648844003677368\n","[Training Epoch 3] Batch 219, Loss 0.42730721831321716\n","[Training Epoch 3] Batch 220, Loss 0.4386882781982422\n","[Training Epoch 3] Batch 221, Loss 0.44877851009368896\n","[Training Epoch 3] Batch 222, Loss 0.42991751432418823\n","[Training Epoch 3] Batch 223, Loss 0.4283621907234192\n","[Training Epoch 3] Batch 224, Loss 0.4595475196838379\n","[Training Epoch 3] Batch 225, Loss 0.447483628988266\n","[Training Epoch 3] Batch 226, Loss 0.42474228143692017\n","[Training Epoch 3] Batch 227, Loss 0.41323286294937134\n","[Training Epoch 3] Batch 228, Loss 0.42718109488487244\n","[Training Epoch 3] Batch 229, Loss 0.42202019691467285\n","[Training Epoch 3] Batch 230, Loss 0.44613784551620483\n","[Training Epoch 3] Batch 231, Loss 0.4608059525489807\n","[Training Epoch 3] Batch 232, Loss 0.4140015244483948\n","[Training Epoch 3] Batch 233, Loss 0.45254528522491455\n","[Training Epoch 3] Batch 234, Loss 0.4208686947822571\n","[Training Epoch 3] Batch 235, Loss 0.40140050649642944\n","[Training Epoch 3] Batch 236, Loss 0.41213884949684143\n","[Training Epoch 3] Batch 237, Loss 0.42230403423309326\n","[Training Epoch 3] Batch 238, Loss 0.4651032090187073\n","[Training Epoch 3] Batch 239, Loss 0.44165822863578796\n","[Training Epoch 3] Batch 240, Loss 0.4474469721317291\n","[Training Epoch 3] Batch 241, Loss 0.4085533320903778\n","[Training Epoch 3] Batch 242, Loss 0.44914042949676514\n","[Training Epoch 3] Batch 243, Loss 0.43013882637023926\n","[Training Epoch 3] Batch 244, Loss 0.4556901454925537\n","[Training Epoch 3] Batch 245, Loss 0.46046480536460876\n","[Training Epoch 3] Batch 246, Loss 0.4330212473869324\n","[Training Epoch 3] Batch 247, Loss 0.44142669439315796\n","[Training Epoch 3] Batch 248, Loss 0.4210197329521179\n","[Training Epoch 3] Batch 249, Loss 0.43201616406440735\n","[Training Epoch 3] Batch 250, Loss 0.42772287130355835\n","[Training Epoch 3] Batch 251, Loss 0.4331355690956116\n","[Training Epoch 3] Batch 252, Loss 0.4356052279472351\n","[Training Epoch 3] Batch 253, Loss 0.43165913224220276\n","[Training Epoch 3] Batch 254, Loss 0.41968995332717896\n","[Training Epoch 3] Batch 255, Loss 0.41015300154685974\n","[Training Epoch 3] Batch 256, Loss 0.4208064377307892\n","[Training Epoch 3] Batch 257, Loss 0.4447118639945984\n","[Training Epoch 3] Batch 258, Loss 0.4263424873352051\n","[Training Epoch 3] Batch 259, Loss 0.4501928985118866\n","[Training Epoch 3] Batch 260, Loss 0.43170881271362305\n","[Training Epoch 3] Batch 261, Loss 0.443707674741745\n","[Training Epoch 3] Batch 262, Loss 0.4532459080219269\n","[Training Epoch 3] Batch 263, Loss 0.4504767656326294\n","[Training Epoch 3] Batch 264, Loss 0.44619348645210266\n","[Training Epoch 3] Batch 265, Loss 0.4349454641342163\n","[Training Epoch 3] Batch 266, Loss 0.40517207980155945\n","[Training Epoch 3] Batch 267, Loss 0.42195892333984375\n","[Training Epoch 3] Batch 268, Loss 0.43578219413757324\n","[Training Epoch 3] Batch 269, Loss 0.43573814630508423\n","[Training Epoch 3] Batch 270, Loss 0.3911953568458557\n","[Training Epoch 3] Batch 271, Loss 0.4437476396560669\n","[Training Epoch 3] Batch 272, Loss 0.4613592028617859\n","[Training Epoch 3] Batch 273, Loss 0.44234979152679443\n","[Training Epoch 3] Batch 274, Loss 0.4654413163661957\n","[Training Epoch 3] Batch 275, Loss 0.4481373429298401\n","[Training Epoch 3] Batch 276, Loss 0.41473710536956787\n","[Training Epoch 3] Batch 277, Loss 0.4366846978664398\n","[Training Epoch 3] Batch 278, Loss 0.410722553730011\n","[Training Epoch 3] Batch 279, Loss 0.4278377890586853\n","[Training Epoch 3] Batch 280, Loss 0.4473181366920471\n","[Training Epoch 3] Batch 281, Loss 0.45491325855255127\n","[Training Epoch 3] Batch 282, Loss 0.44707298278808594\n","[Training Epoch 3] Batch 283, Loss 0.44682377576828003\n","[Training Epoch 3] Batch 284, Loss 0.43082377314567566\n","[Training Epoch 3] Batch 285, Loss 0.4438122510910034\n","[Training Epoch 3] Batch 286, Loss 0.4427841305732727\n","[Training Epoch 3] Batch 287, Loss 0.40473172068595886\n","[Training Epoch 3] Batch 288, Loss 0.43978798389434814\n","[Training Epoch 3] Batch 289, Loss 0.43077215552330017\n","[Training Epoch 3] Batch 290, Loss 0.44716548919677734\n","[Training Epoch 3] Batch 291, Loss 0.4126872420310974\n","[Training Epoch 3] Batch 292, Loss 0.42647087574005127\n","[Training Epoch 3] Batch 293, Loss 0.4317193031311035\n","[Training Epoch 3] Batch 294, Loss 0.43213802576065063\n","[Training Epoch 3] Batch 295, Loss 0.44516944885253906\n","[Training Epoch 3] Batch 296, Loss 0.44816848635673523\n","[Training Epoch 3] Batch 297, Loss 0.4501461982727051\n","[Training Epoch 3] Batch 298, Loss 0.4595796465873718\n","[Training Epoch 3] Batch 299, Loss 0.4353891611099243\n","[Training Epoch 3] Batch 300, Loss 0.4564313292503357\n","[Training Epoch 3] Batch 301, Loss 0.4708630442619324\n","[Training Epoch 3] Batch 302, Loss 0.4394919276237488\n","[Training Epoch 3] Batch 303, Loss 0.43965500593185425\n","[Training Epoch 3] Batch 304, Loss 0.4441472291946411\n","[Training Epoch 3] Batch 305, Loss 0.40897977352142334\n","[Training Epoch 3] Batch 306, Loss 0.43532419204711914\n","[Training Epoch 3] Batch 307, Loss 0.43851935863494873\n","[Training Epoch 3] Batch 308, Loss 0.42670971155166626\n","[Training Epoch 3] Batch 309, Loss 0.43250179290771484\n","[Training Epoch 3] Batch 310, Loss 0.43232858180999756\n","[Training Epoch 3] Batch 311, Loss 0.45039793848991394\n","[Training Epoch 3] Batch 312, Loss 0.4475209414958954\n","[Training Epoch 3] Batch 313, Loss 0.46317699551582336\n","[Training Epoch 3] Batch 314, Loss 0.4450804591178894\n","[Training Epoch 3] Batch 315, Loss 0.4258654713630676\n","[Training Epoch 3] Batch 316, Loss 0.4265686869621277\n","[Training Epoch 3] Batch 317, Loss 0.458609402179718\n","[Training Epoch 3] Batch 318, Loss 0.4584553837776184\n","[Training Epoch 3] Batch 319, Loss 0.42647722363471985\n","[Training Epoch 3] Batch 320, Loss 0.46845322847366333\n","[Training Epoch 3] Batch 321, Loss 0.4268651604652405\n","[Training Epoch 3] Batch 322, Loss 0.41950851678848267\n","[Training Epoch 3] Batch 323, Loss 0.43768948316574097\n","[Training Epoch 3] Batch 324, Loss 0.4281694293022156\n","[Training Epoch 3] Batch 325, Loss 0.38874688744544983\n","[Training Epoch 3] Batch 326, Loss 0.45757538080215454\n","[Training Epoch 3] Batch 327, Loss 0.4222606420516968\n","[Training Epoch 3] Batch 328, Loss 0.4242632985115051\n","[Training Epoch 3] Batch 329, Loss 0.4455134868621826\n","[Training Epoch 3] Batch 330, Loss 0.4140804409980774\n","[Training Epoch 3] Batch 331, Loss 0.42748844623565674\n","[Training Epoch 3] Batch 332, Loss 0.4355396032333374\n","[Training Epoch 3] Batch 333, Loss 0.42816853523254395\n","[Training Epoch 3] Batch 334, Loss 0.4397760331630707\n","[Training Epoch 3] Batch 335, Loss 0.45628821849823\n","[Training Epoch 3] Batch 336, Loss 0.43773722648620605\n","[Training Epoch 3] Batch 337, Loss 0.4710056781768799\n","[Training Epoch 3] Batch 338, Loss 0.45425963401794434\n","[Training Epoch 3] Batch 339, Loss 0.4071800112724304\n","[Training Epoch 3] Batch 340, Loss 0.43470990657806396\n","[Training Epoch 3] Batch 341, Loss 0.41249608993530273\n","[Training Epoch 3] Batch 342, Loss 0.4171275496482849\n","[Training Epoch 3] Batch 343, Loss 0.4232293963432312\n","[Training Epoch 3] Batch 344, Loss 0.4177009165287018\n","[Training Epoch 3] Batch 345, Loss 0.4202132821083069\n","[Training Epoch 3] Batch 346, Loss 0.4798269271850586\n","[Training Epoch 3] Batch 347, Loss 0.4297921657562256\n","[Training Epoch 3] Batch 348, Loss 0.43589839339256287\n","[Training Epoch 3] Batch 349, Loss 0.4245423674583435\n","[Training Epoch 3] Batch 350, Loss 0.45279520750045776\n","[Training Epoch 3] Batch 351, Loss 0.46693605184555054\n","[Training Epoch 3] Batch 352, Loss 0.42561981081962585\n","[Training Epoch 3] Batch 353, Loss 0.434747576713562\n","[Training Epoch 3] Batch 354, Loss 0.4373081922531128\n","[Training Epoch 3] Batch 355, Loss 0.41195064783096313\n","[Training Epoch 3] Batch 356, Loss 0.4146181344985962\n","[Training Epoch 3] Batch 357, Loss 0.4793010950088501\n","[Training Epoch 3] Batch 358, Loss 0.4425894320011139\n","[Training Epoch 3] Batch 359, Loss 0.44345563650131226\n","[Training Epoch 3] Batch 360, Loss 0.41925686597824097\n","[Training Epoch 3] Batch 361, Loss 0.420327365398407\n","[Training Epoch 3] Batch 362, Loss 0.415975958108902\n","[Training Epoch 3] Batch 363, Loss 0.4465274214744568\n","[Training Epoch 3] Batch 364, Loss 0.45717477798461914\n","[Training Epoch 3] Batch 365, Loss 0.4216799736022949\n","[Training Epoch 3] Batch 366, Loss 0.40064874291419983\n","[Training Epoch 3] Batch 367, Loss 0.42300093173980713\n","[Training Epoch 3] Batch 368, Loss 0.40878382325172424\n","[Training Epoch 3] Batch 369, Loss 0.4546200931072235\n","[Training Epoch 3] Batch 370, Loss 0.4411444365978241\n","[Training Epoch 3] Batch 371, Loss 0.44051119685173035\n","[Training Epoch 3] Batch 372, Loss 0.44534629583358765\n","[Training Epoch 3] Batch 373, Loss 0.3830889165401459\n","[Training Epoch 3] Batch 374, Loss 0.45828714966773987\n","[Training Epoch 3] Batch 375, Loss 0.43418747186660767\n","[Training Epoch 3] Batch 376, Loss 0.44542062282562256\n","[Training Epoch 3] Batch 377, Loss 0.4326222240924835\n","[Training Epoch 3] Batch 378, Loss 0.4488565921783447\n","[Training Epoch 3] Batch 379, Loss 0.4625086486339569\n","[Training Epoch 3] Batch 380, Loss 0.44159242510795593\n","[Training Epoch 3] Batch 381, Loss 0.45272064208984375\n","[Training Epoch 3] Batch 382, Loss 0.4141446352005005\n","[Training Epoch 3] Batch 383, Loss 0.4219171702861786\n","[Training Epoch 3] Batch 384, Loss 0.4212770462036133\n","[Training Epoch 3] Batch 385, Loss 0.44329172372817993\n","[Training Epoch 3] Batch 386, Loss 0.4177149534225464\n","[Training Epoch 3] Batch 387, Loss 0.45793041586875916\n","[Training Epoch 3] Batch 388, Loss 0.42629697918891907\n","[Training Epoch 3] Batch 389, Loss 0.4379746913909912\n","[Training Epoch 3] Batch 390, Loss 0.46288251876831055\n","[Training Epoch 3] Batch 391, Loss 0.4215129613876343\n","[Training Epoch 3] Batch 392, Loss 0.47505927085876465\n","[Training Epoch 3] Batch 393, Loss 0.43130049109458923\n","[Training Epoch 3] Batch 394, Loss 0.45147794485092163\n","[Training Epoch 3] Batch 395, Loss 0.46138957142829895\n","[Training Epoch 3] Batch 396, Loss 0.39725813269615173\n","[Training Epoch 3] Batch 397, Loss 0.46194276213645935\n","[Training Epoch 3] Batch 398, Loss 0.423347532749176\n","[Training Epoch 3] Batch 399, Loss 0.4496089816093445\n","[Training Epoch 3] Batch 400, Loss 0.412163645029068\n","[Training Epoch 3] Batch 401, Loss 0.47559428215026855\n","[Training Epoch 3] Batch 402, Loss 0.4389232397079468\n","[Training Epoch 3] Batch 403, Loss 0.4376269578933716\n","[Training Epoch 3] Batch 404, Loss 0.41038778424263\n","[Training Epoch 3] Batch 405, Loss 0.43558788299560547\n","[Training Epoch 3] Batch 406, Loss 0.46464359760284424\n","[Training Epoch 3] Batch 407, Loss 0.44468656182289124\n","[Training Epoch 3] Batch 408, Loss 0.41332316398620605\n","[Training Epoch 3] Batch 409, Loss 0.47458475828170776\n","[Training Epoch 3] Batch 410, Loss 0.4550396800041199\n","[Training Epoch 3] Batch 411, Loss 0.4412342309951782\n","[Training Epoch 3] Batch 412, Loss 0.42596882581710815\n","[Training Epoch 3] Batch 413, Loss 0.45577067136764526\n","[Training Epoch 3] Batch 414, Loss 0.43009477853775024\n","[Training Epoch 3] Batch 415, Loss 0.4631316661834717\n","[Training Epoch 3] Batch 416, Loss 0.4120200276374817\n","[Training Epoch 3] Batch 417, Loss 0.42540472745895386\n","[Training Epoch 3] Batch 418, Loss 0.4349481165409088\n","[Training Epoch 3] Batch 419, Loss 0.45054036378860474\n","[Training Epoch 3] Batch 420, Loss 0.4290207028388977\n","[Training Epoch 3] Batch 421, Loss 0.4244306683540344\n","[Training Epoch 3] Batch 422, Loss 0.46823883056640625\n","[Training Epoch 3] Batch 423, Loss 0.44526201486587524\n","[Training Epoch 3] Batch 424, Loss 0.4334108829498291\n","[Training Epoch 3] Batch 425, Loss 0.4279155135154724\n","[Training Epoch 3] Batch 426, Loss 0.4654959440231323\n","[Training Epoch 3] Batch 427, Loss 0.4556620121002197\n","[Training Epoch 3] Batch 428, Loss 0.4363706111907959\n","[Training Epoch 3] Batch 429, Loss 0.4474126100540161\n","[Training Epoch 3] Batch 430, Loss 0.4238225221633911\n","[Training Epoch 3] Batch 431, Loss 0.44547709822654724\n","[Training Epoch 3] Batch 432, Loss 0.43020638823509216\n","[Training Epoch 3] Batch 433, Loss 0.41482990980148315\n","[Training Epoch 3] Batch 434, Loss 0.43216651678085327\n","[Training Epoch 3] Batch 435, Loss 0.4554961621761322\n","[Training Epoch 3] Batch 436, Loss 0.4347938299179077\n","[Training Epoch 3] Batch 437, Loss 0.42615801095962524\n","[Training Epoch 3] Batch 438, Loss 0.4447479248046875\n","[Training Epoch 3] Batch 439, Loss 0.42506372928619385\n","[Training Epoch 3] Batch 440, Loss 0.4617866277694702\n","[Training Epoch 3] Batch 441, Loss 0.42573827505111694\n","[Training Epoch 3] Batch 442, Loss 0.42753851413726807\n","[Training Epoch 3] Batch 443, Loss 0.4466491937637329\n","[Training Epoch 3] Batch 444, Loss 0.4482547342777252\n","[Training Epoch 3] Batch 445, Loss 0.48589351773262024\n","[Training Epoch 3] Batch 446, Loss 0.4449375867843628\n","[Training Epoch 3] Batch 447, Loss 0.437924861907959\n","[Training Epoch 3] Batch 448, Loss 0.4169391691684723\n","[Training Epoch 3] Batch 449, Loss 0.4505698084831238\n","[Training Epoch 3] Batch 450, Loss 0.4193190038204193\n","[Training Epoch 3] Batch 451, Loss 0.41903167963027954\n","[Training Epoch 3] Batch 452, Loss 0.4105311930179596\n","[Training Epoch 3] Batch 453, Loss 0.4210383892059326\n","[Training Epoch 3] Batch 454, Loss 0.40990275144577026\n","[Training Epoch 3] Batch 455, Loss 0.4290626049041748\n","[Training Epoch 3] Batch 456, Loss 0.4113994836807251\n","[Training Epoch 3] Batch 457, Loss 0.4384004473686218\n","[Training Epoch 3] Batch 458, Loss 0.40538516640663147\n","[Training Epoch 3] Batch 459, Loss 0.4517390727996826\n","[Training Epoch 3] Batch 460, Loss 0.42214715480804443\n","[Training Epoch 3] Batch 461, Loss 0.43255141377449036\n","[Training Epoch 3] Batch 462, Loss 0.4509086012840271\n","[Training Epoch 3] Batch 463, Loss 0.4212995767593384\n","[Training Epoch 3] Batch 464, Loss 0.4161826968193054\n","[Training Epoch 3] Batch 465, Loss 0.44772130250930786\n","[Training Epoch 3] Batch 466, Loss 0.44643300771713257\n","[Training Epoch 3] Batch 467, Loss 0.44458699226379395\n","[Training Epoch 3] Batch 468, Loss 0.447884738445282\n","[Training Epoch 3] Batch 469, Loss 0.42960798740386963\n","[Training Epoch 3] Batch 470, Loss 0.43180087208747864\n","[Training Epoch 3] Batch 471, Loss 0.4405228793621063\n","[Training Epoch 3] Batch 472, Loss 0.46100127696990967\n","[Training Epoch 3] Batch 473, Loss 0.404727965593338\n","[Training Epoch 3] Batch 474, Loss 0.4466935992240906\n","[Training Epoch 3] Batch 475, Loss 0.4572886526584625\n","[Training Epoch 3] Batch 476, Loss 0.4334896504878998\n","[Training Epoch 3] Batch 477, Loss 0.4174700379371643\n","[Training Epoch 3] Batch 478, Loss 0.4172161817550659\n","[Training Epoch 3] Batch 479, Loss 0.422926664352417\n","[Training Epoch 3] Batch 480, Loss 0.41331714391708374\n","[Training Epoch 3] Batch 481, Loss 0.4215735197067261\n","[Training Epoch 3] Batch 482, Loss 0.4316682815551758\n","[Training Epoch 3] Batch 483, Loss 0.4485633969306946\n","[Training Epoch 3] Batch 484, Loss 0.42742830514907837\n","[Training Epoch 3] Batch 485, Loss 0.44151607155799866\n","[Training Epoch 3] Batch 486, Loss 0.44390374422073364\n","[Training Epoch 3] Batch 487, Loss 0.4204939007759094\n","[Training Epoch 3] Batch 488, Loss 0.47072917222976685\n","[Training Epoch 3] Batch 489, Loss 0.4590260982513428\n","[Training Epoch 3] Batch 490, Loss 0.4205160439014435\n","[Training Epoch 3] Batch 491, Loss 0.44553714990615845\n","[Training Epoch 3] Batch 492, Loss 0.4217279255390167\n","[Training Epoch 3] Batch 493, Loss 0.46188947558403015\n","[Training Epoch 3] Batch 494, Loss 0.44099950790405273\n","[Training Epoch 3] Batch 495, Loss 0.45625102519989014\n","[Training Epoch 3] Batch 496, Loss 0.43484702706336975\n","[Training Epoch 3] Batch 497, Loss 0.41960644721984863\n","[Training Epoch 3] Batch 498, Loss 0.46475011110305786\n","[Training Epoch 3] Batch 499, Loss 0.4072341322898865\n","[Training Epoch 3] Batch 500, Loss 0.45503777265548706\n","[Training Epoch 3] Batch 501, Loss 0.43898439407348633\n","[Training Epoch 3] Batch 502, Loss 0.44030022621154785\n","[Training Epoch 3] Batch 503, Loss 0.43107470870018005\n","[Training Epoch 3] Batch 504, Loss 0.4001712203025818\n","[Training Epoch 3] Batch 505, Loss 0.4149250388145447\n","[Training Epoch 3] Batch 506, Loss 0.46605563163757324\n","[Training Epoch 3] Batch 507, Loss 0.43400105834007263\n","[Training Epoch 3] Batch 508, Loss 0.42336389422416687\n","[Training Epoch 3] Batch 509, Loss 0.4258720576763153\n","[Training Epoch 3] Batch 510, Loss 0.46679699420928955\n","[Training Epoch 3] Batch 511, Loss 0.44863975048065186\n","[Training Epoch 3] Batch 512, Loss 0.4239054322242737\n","[Training Epoch 3] Batch 513, Loss 0.4335711896419525\n","[Training Epoch 3] Batch 514, Loss 0.44574999809265137\n","[Training Epoch 3] Batch 515, Loss 0.40960973501205444\n","[Training Epoch 3] Batch 516, Loss 0.44780099391937256\n","[Training Epoch 3] Batch 517, Loss 0.4294176697731018\n","[Training Epoch 3] Batch 518, Loss 0.42071986198425293\n","[Training Epoch 3] Batch 519, Loss 0.4608498513698578\n","[Training Epoch 3] Batch 520, Loss 0.4553629457950592\n","[Training Epoch 3] Batch 521, Loss 0.4296466112136841\n","[Training Epoch 3] Batch 522, Loss 0.4335228204727173\n","[Training Epoch 3] Batch 523, Loss 0.44436413049697876\n","[Training Epoch 3] Batch 524, Loss 0.4380439519882202\n","[Training Epoch 3] Batch 525, Loss 0.4462072253227234\n","[Training Epoch 3] Batch 526, Loss 0.45800673961639404\n","[Training Epoch 3] Batch 527, Loss 0.433851420879364\n","[Training Epoch 3] Batch 528, Loss 0.4163275957107544\n","[Training Epoch 3] Batch 529, Loss 0.4174398183822632\n","[Training Epoch 3] Batch 530, Loss 0.419435054063797\n","[Training Epoch 3] Batch 531, Loss 0.43940046429634094\n","[Training Epoch 3] Batch 532, Loss 0.42854219675064087\n","[Training Epoch 3] Batch 533, Loss 0.46275126934051514\n","[Training Epoch 3] Batch 534, Loss 0.4354771375656128\n","[Training Epoch 3] Batch 535, Loss 0.4173581600189209\n","[Training Epoch 3] Batch 536, Loss 0.41894766688346863\n","[Training Epoch 3] Batch 537, Loss 0.4278874695301056\n","[Training Epoch 3] Batch 538, Loss 0.43371859192848206\n","[Training Epoch 3] Batch 539, Loss 0.4274362325668335\n","[Training Epoch 3] Batch 540, Loss 0.43155741691589355\n","[Training Epoch 3] Batch 541, Loss 0.4541611671447754\n","[Training Epoch 3] Batch 542, Loss 0.4308968186378479\n","[Training Epoch 3] Batch 543, Loss 0.4015728831291199\n","[Training Epoch 3] Batch 544, Loss 0.41723352670669556\n","[Training Epoch 3] Batch 545, Loss 0.46114635467529297\n","[Training Epoch 3] Batch 546, Loss 0.4640094041824341\n","[Training Epoch 3] Batch 547, Loss 0.4177777171134949\n","[Training Epoch 3] Batch 548, Loss 0.40952762961387634\n","[Training Epoch 3] Batch 549, Loss 0.43966883420944214\n","[Training Epoch 3] Batch 550, Loss 0.44824832677841187\n","[Training Epoch 3] Batch 551, Loss 0.4266325533390045\n","[Training Epoch 3] Batch 552, Loss 0.4330251216888428\n","[Training Epoch 3] Batch 553, Loss 0.4388657808303833\n","[Training Epoch 3] Batch 554, Loss 0.44041335582733154\n","[Training Epoch 3] Batch 555, Loss 0.41021502017974854\n","[Training Epoch 3] Batch 556, Loss 0.4234880805015564\n","[Training Epoch 3] Batch 557, Loss 0.42361095547676086\n","[Training Epoch 3] Batch 558, Loss 0.46274781227111816\n","[Training Epoch 3] Batch 559, Loss 0.43354707956314087\n","[Training Epoch 3] Batch 560, Loss 0.4333586096763611\n","[Training Epoch 3] Batch 561, Loss 0.4580335021018982\n","[Training Epoch 3] Batch 562, Loss 0.4170285165309906\n","[Training Epoch 3] Batch 563, Loss 0.4369649887084961\n","[Training Epoch 3] Batch 564, Loss 0.4237629771232605\n","[Training Epoch 3] Batch 565, Loss 0.4227645695209503\n","[Training Epoch 3] Batch 566, Loss 0.44721075892448425\n","[Training Epoch 3] Batch 567, Loss 0.4284728765487671\n","[Training Epoch 3] Batch 568, Loss 0.41251352429389954\n","[Training Epoch 3] Batch 569, Loss 0.4328625798225403\n","[Training Epoch 3] Batch 570, Loss 0.428117960691452\n","[Training Epoch 3] Batch 571, Loss 0.4276083707809448\n","[Training Epoch 3] Batch 572, Loss 0.416998028755188\n","[Training Epoch 3] Batch 573, Loss 0.46393507719039917\n","[Training Epoch 3] Batch 574, Loss 0.4230926036834717\n","[Training Epoch 3] Batch 575, Loss 0.4420839548110962\n","[Training Epoch 3] Batch 576, Loss 0.4184414744377136\n","[Training Epoch 3] Batch 577, Loss 0.44578760862350464\n","[Training Epoch 3] Batch 578, Loss 0.40950602293014526\n","[Training Epoch 3] Batch 579, Loss 0.47187912464141846\n","[Training Epoch 3] Batch 580, Loss 0.4463217258453369\n","[Training Epoch 3] Batch 581, Loss 0.4485630989074707\n","[Training Epoch 3] Batch 582, Loss 0.40031248331069946\n","[Training Epoch 3] Batch 583, Loss 0.4116572141647339\n","[Training Epoch 3] Batch 584, Loss 0.4164465665817261\n","[Training Epoch 3] Batch 585, Loss 0.433535635471344\n","[Training Epoch 3] Batch 586, Loss 0.4288073778152466\n","[Training Epoch 3] Batch 587, Loss 0.41846561431884766\n","[Training Epoch 3] Batch 588, Loss 0.4275115728378296\n","[Training Epoch 3] Batch 589, Loss 0.44180336594581604\n","[Training Epoch 3] Batch 590, Loss 0.44870924949645996\n","[Training Epoch 3] Batch 591, Loss 0.42433005571365356\n","[Training Epoch 3] Batch 592, Loss 0.41523101925849915\n","[Training Epoch 3] Batch 593, Loss 0.4146733283996582\n","[Training Epoch 3] Batch 594, Loss 0.42863404750823975\n","[Training Epoch 3] Batch 595, Loss 0.5007076263427734\n","[Training Epoch 3] Batch 596, Loss 0.4280785024166107\n","[Training Epoch 3] Batch 597, Loss 0.41400182247161865\n","[Training Epoch 3] Batch 598, Loss 0.3922410011291504\n","[Training Epoch 3] Batch 599, Loss 0.4175044894218445\n","[Training Epoch 3] Batch 600, Loss 0.38315021991729736\n","[Training Epoch 3] Batch 601, Loss 0.443323016166687\n","[Training Epoch 3] Batch 602, Loss 0.4325694143772125\n","[Training Epoch 3] Batch 603, Loss 0.41892898082733154\n","[Training Epoch 3] Batch 604, Loss 0.43915605545043945\n","[Training Epoch 3] Batch 605, Loss 0.4505072832107544\n","[Training Epoch 3] Batch 606, Loss 0.4458075761795044\n","[Training Epoch 3] Batch 607, Loss 0.4293598234653473\n","[Training Epoch 3] Batch 608, Loss 0.4292089343070984\n","[Training Epoch 3] Batch 609, Loss 0.42407843470573425\n","[Training Epoch 3] Batch 610, Loss 0.41911977529525757\n","[Training Epoch 3] Batch 611, Loss 0.42532601952552795\n","[Training Epoch 3] Batch 612, Loss 0.44899874925613403\n","[Training Epoch 3] Batch 613, Loss 0.4200161099433899\n","[Training Epoch 3] Batch 614, Loss 0.40334057807922363\n","[Training Epoch 3] Batch 615, Loss 0.4384051561355591\n","[Training Epoch 3] Batch 616, Loss 0.45536312460899353\n","[Training Epoch 3] Batch 617, Loss 0.41833943128585815\n","[Training Epoch 3] Batch 618, Loss 0.4538072943687439\n","[Training Epoch 3] Batch 619, Loss 0.4410915970802307\n","[Training Epoch 3] Batch 620, Loss 0.4508207142353058\n","[Training Epoch 3] Batch 621, Loss 0.4312761723995209\n","[Training Epoch 3] Batch 622, Loss 0.4115097224712372\n","[Training Epoch 3] Batch 623, Loss 0.4452839493751526\n","[Training Epoch 3] Batch 624, Loss 0.45831745862960815\n","[Training Epoch 3] Batch 625, Loss 0.46938517689704895\n","[Training Epoch 3] Batch 626, Loss 0.45972856879234314\n","[Training Epoch 3] Batch 627, Loss 0.40568065643310547\n","[Training Epoch 3] Batch 628, Loss 0.43954360485076904\n","[Training Epoch 3] Batch 629, Loss 0.44489067792892456\n","[Training Epoch 3] Batch 630, Loss 0.4608045220375061\n","[Training Epoch 3] Batch 631, Loss 0.4092385768890381\n","[Training Epoch 3] Batch 632, Loss 0.41683876514434814\n","[Training Epoch 3] Batch 633, Loss 0.4193079471588135\n","[Training Epoch 3] Batch 634, Loss 0.43577420711517334\n","[Training Epoch 3] Batch 635, Loss 0.42877069115638733\n","[Training Epoch 3] Batch 636, Loss 0.4082847833633423\n","[Training Epoch 3] Batch 637, Loss 0.4011728763580322\n","[Training Epoch 3] Batch 638, Loss 0.4116308093070984\n","[Training Epoch 3] Batch 639, Loss 0.41612106561660767\n","[Training Epoch 3] Batch 640, Loss 0.42172878980636597\n","[Training Epoch 3] Batch 641, Loss 0.42952829599380493\n","[Training Epoch 3] Batch 642, Loss 0.39933449029922485\n","[Training Epoch 3] Batch 643, Loss 0.4150204658508301\n","[Training Epoch 3] Batch 644, Loss 0.439622700214386\n","[Training Epoch 3] Batch 645, Loss 0.43593886494636536\n","[Training Epoch 3] Batch 646, Loss 0.4064479172229767\n","[Training Epoch 3] Batch 647, Loss 0.45723098516464233\n","[Training Epoch 3] Batch 648, Loss 0.41626402735710144\n","[Training Epoch 3] Batch 649, Loss 0.4804881513118744\n","[Training Epoch 3] Batch 650, Loss 0.43999117612838745\n","[Training Epoch 3] Batch 651, Loss 0.4291732609272003\n","[Training Epoch 3] Batch 652, Loss 0.43012410402297974\n","[Training Epoch 3] Batch 653, Loss 0.4154764413833618\n","[Training Epoch 3] Batch 654, Loss 0.4605150818824768\n","[Training Epoch 3] Batch 655, Loss 0.4000520706176758\n","[Training Epoch 3] Batch 656, Loss 0.4231872260570526\n","[Training Epoch 3] Batch 657, Loss 0.4255944788455963\n","[Training Epoch 3] Batch 658, Loss 0.43616753816604614\n","[Training Epoch 3] Batch 659, Loss 0.43375319242477417\n","[Training Epoch 3] Batch 660, Loss 0.4278881549835205\n","[Training Epoch 3] Batch 661, Loss 0.41983357071876526\n","[Training Epoch 3] Batch 662, Loss 0.4342230558395386\n","[Training Epoch 3] Batch 663, Loss 0.43622899055480957\n","[Training Epoch 3] Batch 664, Loss 0.44215816259384155\n","[Training Epoch 3] Batch 665, Loss 0.4228200316429138\n","[Training Epoch 3] Batch 666, Loss 0.4419613778591156\n","[Training Epoch 3] Batch 667, Loss 0.4286765456199646\n","[Training Epoch 3] Batch 668, Loss 0.4392874836921692\n","[Training Epoch 3] Batch 669, Loss 0.4343678653240204\n","[Training Epoch 3] Batch 670, Loss 0.39688095450401306\n","[Training Epoch 3] Batch 671, Loss 0.42851024866104126\n","[Training Epoch 3] Batch 672, Loss 0.38854092359542847\n","[Training Epoch 3] Batch 673, Loss 0.42903274297714233\n","[Training Epoch 3] Batch 674, Loss 0.42181697487831116\n","[Training Epoch 3] Batch 675, Loss 0.4289494752883911\n","[Training Epoch 3] Batch 676, Loss 0.45176708698272705\n","[Training Epoch 3] Batch 677, Loss 0.42814159393310547\n","[Training Epoch 3] Batch 678, Loss 0.43633097410202026\n","[Training Epoch 3] Batch 679, Loss 0.43537473678588867\n","[Training Epoch 3] Batch 680, Loss 0.43964338302612305\n","[Training Epoch 3] Batch 681, Loss 0.4465460181236267\n","[Training Epoch 3] Batch 682, Loss 0.4080210030078888\n","[Training Epoch 3] Batch 683, Loss 0.3823072612285614\n","[Training Epoch 3] Batch 684, Loss 0.42535364627838135\n","[Training Epoch 3] Batch 685, Loss 0.45099377632141113\n","[Training Epoch 3] Batch 686, Loss 0.42837217450141907\n","[Training Epoch 3] Batch 687, Loss 0.44114312529563904\n","[Training Epoch 3] Batch 688, Loss 0.4176924228668213\n","[Training Epoch 3] Batch 689, Loss 0.44514432549476624\n","[Training Epoch 3] Batch 690, Loss 0.4507351219654083\n","[Training Epoch 3] Batch 691, Loss 0.42354509234428406\n","[Training Epoch 3] Batch 692, Loss 0.3969569504261017\n","[Training Epoch 3] Batch 693, Loss 0.40691182017326355\n","[Training Epoch 3] Batch 694, Loss 0.4117627739906311\n","[Training Epoch 3] Batch 695, Loss 0.42034754157066345\n","[Training Epoch 3] Batch 696, Loss 0.419783353805542\n","[Training Epoch 3] Batch 697, Loss 0.39896711707115173\n","[Training Epoch 3] Batch 698, Loss 0.44721779227256775\n","[Training Epoch 3] Batch 699, Loss 0.4412407875061035\n","[Training Epoch 3] Batch 700, Loss 0.43049073219299316\n","[Training Epoch 3] Batch 701, Loss 0.44234198331832886\n","[Training Epoch 3] Batch 702, Loss 0.39747700095176697\n","[Training Epoch 3] Batch 703, Loss 0.4310937523841858\n","[Training Epoch 3] Batch 704, Loss 0.4257962107658386\n","[Training Epoch 3] Batch 705, Loss 0.4077438712120056\n","[Training Epoch 3] Batch 706, Loss 0.4655301868915558\n","[Training Epoch 3] Batch 707, Loss 0.4174453616142273\n","[Training Epoch 3] Batch 708, Loss 0.4221276044845581\n","[Training Epoch 3] Batch 709, Loss 0.4476361572742462\n","[Training Epoch 3] Batch 710, Loss 0.4159797728061676\n","[Training Epoch 3] Batch 711, Loss 0.434693843126297\n","[Training Epoch 3] Batch 712, Loss 0.39544540643692017\n","[Training Epoch 3] Batch 713, Loss 0.42981547117233276\n","[Training Epoch 3] Batch 714, Loss 0.4279823899269104\n","[Training Epoch 3] Batch 715, Loss 0.4255971312522888\n","[Training Epoch 3] Batch 716, Loss 0.4382169246673584\n","[Training Epoch 3] Batch 717, Loss 0.4713110327720642\n","[Training Epoch 3] Batch 718, Loss 0.45604217052459717\n","[Training Epoch 3] Batch 719, Loss 0.4332142472267151\n","[Training Epoch 3] Batch 720, Loss 0.4491623640060425\n","[Training Epoch 3] Batch 721, Loss 0.4246335029602051\n","[Training Epoch 3] Batch 722, Loss 0.4343215227127075\n","[Training Epoch 3] Batch 723, Loss 0.39750808477401733\n","[Training Epoch 3] Batch 724, Loss 0.4158649146556854\n","[Training Epoch 3] Batch 725, Loss 0.4608413577079773\n","[Training Epoch 3] Batch 726, Loss 0.4219248294830322\n","[Training Epoch 3] Batch 727, Loss 0.43049442768096924\n","[Training Epoch 3] Batch 728, Loss 0.42622920870780945\n","[Training Epoch 3] Batch 729, Loss 0.43824177980422974\n","[Training Epoch 3] Batch 730, Loss 0.44125446677207947\n","[Training Epoch 3] Batch 731, Loss 0.4145278036594391\n","[Training Epoch 3] Batch 732, Loss 0.4639627933502197\n","[Training Epoch 3] Batch 733, Loss 0.4405784010887146\n","[Training Epoch 3] Batch 734, Loss 0.42512190341949463\n","[Training Epoch 3] Batch 735, Loss 0.42943552136421204\n","[Training Epoch 3] Batch 736, Loss 0.4134872853755951\n","[Training Epoch 3] Batch 737, Loss 0.46718472242355347\n","[Training Epoch 3] Batch 738, Loss 0.45182546973228455\n","[Training Epoch 3] Batch 739, Loss 0.43955618143081665\n","[Training Epoch 3] Batch 740, Loss 0.4526161551475525\n","[Training Epoch 3] Batch 741, Loss 0.42376336455345154\n","[Training Epoch 3] Batch 742, Loss 0.4060444235801697\n","[Training Epoch 3] Batch 743, Loss 0.4159236550331116\n","[Training Epoch 3] Batch 744, Loss 0.4210550785064697\n","[Training Epoch 3] Batch 745, Loss 0.43003472685813904\n","[Training Epoch 3] Batch 746, Loss 0.3985265791416168\n","[Training Epoch 3] Batch 747, Loss 0.4101756513118744\n","[Training Epoch 3] Batch 748, Loss 0.39801955223083496\n","[Training Epoch 3] Batch 749, Loss 0.42821502685546875\n","[Training Epoch 3] Batch 750, Loss 0.41585901379585266\n","[Training Epoch 3] Batch 751, Loss 0.4294852912425995\n","[Training Epoch 3] Batch 752, Loss 0.43213149905204773\n","[Training Epoch 3] Batch 753, Loss 0.4524981379508972\n","[Training Epoch 3] Batch 754, Loss 0.4131770730018616\n","[Training Epoch 3] Batch 755, Loss 0.42345094680786133\n","[Training Epoch 3] Batch 756, Loss 0.45388829708099365\n","[Training Epoch 3] Batch 757, Loss 0.41157034039497375\n","[Training Epoch 3] Batch 758, Loss 0.43908727169036865\n","[Training Epoch 3] Batch 759, Loss 0.41762053966522217\n","[Training Epoch 3] Batch 760, Loss 0.4003090262413025\n","[Training Epoch 3] Batch 761, Loss 0.4418109655380249\n","[Training Epoch 3] Batch 762, Loss 0.40767502784729004\n","[Training Epoch 3] Batch 763, Loss 0.4414435029029846\n","[Training Epoch 3] Batch 764, Loss 0.44677186012268066\n","[Training Epoch 3] Batch 765, Loss 0.4555414617061615\n","[Training Epoch 3] Batch 766, Loss 0.42397814989089966\n","[Training Epoch 3] Batch 767, Loss 0.4354235529899597\n","[Training Epoch 3] Batch 768, Loss 0.4341047704219818\n","[Training Epoch 3] Batch 769, Loss 0.4124738872051239\n","[Training Epoch 3] Batch 770, Loss 0.4214172065258026\n","[Training Epoch 3] Batch 771, Loss 0.42939499020576477\n","[Training Epoch 3] Batch 772, Loss 0.4185102581977844\n","[Training Epoch 3] Batch 773, Loss 0.4203004240989685\n","[Training Epoch 3] Batch 774, Loss 0.4078451991081238\n","[Training Epoch 3] Batch 775, Loss 0.4330412745475769\n","[Training Epoch 3] Batch 776, Loss 0.42667800188064575\n","[Training Epoch 3] Batch 777, Loss 0.42013415694236755\n","[Training Epoch 3] Batch 778, Loss 0.43911534547805786\n","[Training Epoch 3] Batch 779, Loss 0.41426578164100647\n","[Training Epoch 3] Batch 780, Loss 0.40583646297454834\n","[Training Epoch 3] Batch 781, Loss 0.4428219199180603\n","[Training Epoch 3] Batch 782, Loss 0.4450181722640991\n","[Training Epoch 3] Batch 783, Loss 0.42906075716018677\n","[Training Epoch 3] Batch 784, Loss 0.4243152141571045\n","[Training Epoch 3] Batch 785, Loss 0.4166492223739624\n","[Training Epoch 3] Batch 786, Loss 0.43319177627563477\n","[Training Epoch 3] Batch 787, Loss 0.44165489077568054\n","[Training Epoch 3] Batch 788, Loss 0.42759042978286743\n","[Training Epoch 3] Batch 789, Loss 0.4194920063018799\n","[Training Epoch 3] Batch 790, Loss 0.44619080424308777\n","[Training Epoch 3] Batch 791, Loss 0.45347344875335693\n","[Training Epoch 3] Batch 792, Loss 0.4167154133319855\n","[Training Epoch 3] Batch 793, Loss 0.42283275723457336\n","[Training Epoch 3] Batch 794, Loss 0.44389083981513977\n","[Training Epoch 3] Batch 795, Loss 0.4193045496940613\n","[Training Epoch 3] Batch 796, Loss 0.4354783296585083\n","[Training Epoch 3] Batch 797, Loss 0.4021409749984741\n","[Training Epoch 3] Batch 798, Loss 0.4101490378379822\n","[Training Epoch 3] Batch 799, Loss 0.4209968149662018\n","[Training Epoch 3] Batch 800, Loss 0.45215490460395813\n","[Training Epoch 3] Batch 801, Loss 0.43048295378685\n","[Training Epoch 3] Batch 802, Loss 0.4761750102043152\n","[Training Epoch 3] Batch 803, Loss 0.44483932852745056\n","[Training Epoch 3] Batch 804, Loss 0.40639811754226685\n","[Training Epoch 3] Batch 805, Loss 0.43338096141815186\n","[Training Epoch 3] Batch 806, Loss 0.4154716730117798\n","[Training Epoch 3] Batch 807, Loss 0.416374534368515\n","[Training Epoch 3] Batch 808, Loss 0.40903952717781067\n","[Training Epoch 3] Batch 809, Loss 0.39844465255737305\n","[Training Epoch 3] Batch 810, Loss 0.43405720591545105\n","[Training Epoch 3] Batch 811, Loss 0.4476887583732605\n","[Training Epoch 3] Batch 812, Loss 0.431913822889328\n","[Training Epoch 3] Batch 813, Loss 0.4308236837387085\n","[Training Epoch 3] Batch 814, Loss 0.43819284439086914\n","[Training Epoch 3] Batch 815, Loss 0.40685832500457764\n","[Training Epoch 3] Batch 816, Loss 0.41764581203460693\n","[Training Epoch 3] Batch 817, Loss 0.40539318323135376\n","[Training Epoch 3] Batch 818, Loss 0.4159320890903473\n","[Training Epoch 3] Batch 819, Loss 0.43556416034698486\n","[Training Epoch 3] Batch 820, Loss 0.439973920583725\n","[Training Epoch 3] Batch 821, Loss 0.43677788972854614\n","[Training Epoch 3] Batch 822, Loss 0.44122788310050964\n","[Training Epoch 3] Batch 823, Loss 0.43006059527397156\n","[Training Epoch 3] Batch 824, Loss 0.4375098645687103\n","[Training Epoch 3] Batch 825, Loss 0.44388338923454285\n","[Training Epoch 3] Batch 826, Loss 0.42636096477508545\n","[Training Epoch 3] Batch 827, Loss 0.40956878662109375\n","[Training Epoch 3] Batch 828, Loss 0.4182122051715851\n","[Training Epoch 3] Batch 829, Loss 0.396316796541214\n","[Training Epoch 3] Batch 830, Loss 0.38581156730651855\n","[Training Epoch 3] Batch 831, Loss 0.41855910420417786\n","[Training Epoch 3] Batch 832, Loss 0.4267507493495941\n","[Training Epoch 3] Batch 833, Loss 0.4403632581233978\n","[Training Epoch 3] Batch 834, Loss 0.41695934534072876\n","[Training Epoch 3] Batch 835, Loss 0.4494450092315674\n","[Training Epoch 3] Batch 836, Loss 0.4244145154953003\n","[Training Epoch 3] Batch 837, Loss 0.41023778915405273\n","[Training Epoch 3] Batch 838, Loss 0.4674988389015198\n","[Training Epoch 3] Batch 839, Loss 0.4087652564048767\n","[Training Epoch 3] Batch 840, Loss 0.4350200891494751\n","[Training Epoch 3] Batch 841, Loss 0.4208637475967407\n","[Training Epoch 3] Batch 842, Loss 0.4470270872116089\n","[Training Epoch 3] Batch 843, Loss 0.437226265668869\n","[Training Epoch 3] Batch 844, Loss 0.4430345892906189\n","[Training Epoch 3] Batch 845, Loss 0.42089980840682983\n","[Training Epoch 3] Batch 846, Loss 0.4499354958534241\n","[Training Epoch 3] Batch 847, Loss 0.4211410880088806\n","[Training Epoch 3] Batch 848, Loss 0.44673776626586914\n","[Training Epoch 3] Batch 849, Loss 0.4026867747306824\n","[Training Epoch 3] Batch 850, Loss 0.44713592529296875\n","[Training Epoch 3] Batch 851, Loss 0.4614194929599762\n","[Training Epoch 3] Batch 852, Loss 0.41345199942588806\n","[Training Epoch 3] Batch 853, Loss 0.4376643896102905\n","[Training Epoch 3] Batch 854, Loss 0.4241814613342285\n","[Training Epoch 3] Batch 855, Loss 0.4103938341140747\n","[Training Epoch 3] Batch 856, Loss 0.40634214878082275\n","[Training Epoch 3] Batch 857, Loss 0.4121655821800232\n","[Training Epoch 3] Batch 858, Loss 0.42849719524383545\n","[Training Epoch 3] Batch 859, Loss 0.43564438819885254\n","[Training Epoch 3] Batch 860, Loss 0.4212818443775177\n","[Training Epoch 3] Batch 861, Loss 0.4234426021575928\n","[Training Epoch 3] Batch 862, Loss 0.4024876058101654\n","[Training Epoch 3] Batch 863, Loss 0.42968663573265076\n","[Training Epoch 3] Batch 864, Loss 0.39015358686447144\n","[Training Epoch 3] Batch 865, Loss 0.39638304710388184\n","[Training Epoch 3] Batch 866, Loss 0.38429492712020874\n","[Training Epoch 3] Batch 867, Loss 0.4296698272228241\n","[Training Epoch 3] Batch 868, Loss 0.4252166152000427\n","[Training Epoch 3] Batch 869, Loss 0.47458720207214355\n","[Training Epoch 3] Batch 870, Loss 0.43054813146591187\n","[Training Epoch 3] Batch 871, Loss 0.4242549240589142\n","[Training Epoch 3] Batch 872, Loss 0.4029521346092224\n","[Training Epoch 3] Batch 873, Loss 0.4129863977432251\n","[Training Epoch 3] Batch 874, Loss 0.3940005898475647\n","[Training Epoch 3] Batch 875, Loss 0.43762534856796265\n","[Training Epoch 3] Batch 876, Loss 0.45147719979286194\n","[Training Epoch 3] Batch 877, Loss 0.4038310945034027\n","[Training Epoch 3] Batch 878, Loss 0.4206089675426483\n","[Training Epoch 3] Batch 879, Loss 0.4471818208694458\n","[Training Epoch 3] Batch 880, Loss 0.44449150562286377\n","[Training Epoch 3] Batch 881, Loss 0.40688034892082214\n","[Training Epoch 3] Batch 882, Loss 0.42209067940711975\n","[Training Epoch 3] Batch 883, Loss 0.41534173488616943\n","[Training Epoch 3] Batch 884, Loss 0.4442007839679718\n","[Training Epoch 3] Batch 885, Loss 0.432574987411499\n","[Training Epoch 3] Batch 886, Loss 0.4566318392753601\n","[Training Epoch 3] Batch 887, Loss 0.4328700304031372\n","[Training Epoch 3] Batch 888, Loss 0.41369515657424927\n","[Training Epoch 3] Batch 889, Loss 0.4172258973121643\n","[Training Epoch 3] Batch 890, Loss 0.44916999340057373\n","[Training Epoch 3] Batch 891, Loss 0.4191069006919861\n","[Training Epoch 3] Batch 892, Loss 0.447123259305954\n","[Training Epoch 3] Batch 893, Loss 0.42342567443847656\n","[Training Epoch 3] Batch 894, Loss 0.395744264125824\n","[Training Epoch 3] Batch 895, Loss 0.4336087107658386\n","[Training Epoch 3] Batch 896, Loss 0.4288199543952942\n","[Training Epoch 3] Batch 897, Loss 0.4069947302341461\n","[Training Epoch 3] Batch 898, Loss 0.4366931915283203\n","[Training Epoch 3] Batch 899, Loss 0.3851272463798523\n","[Training Epoch 3] Batch 900, Loss 0.4445510804653168\n","[Training Epoch 3] Batch 901, Loss 0.4405073821544647\n","[Training Epoch 3] Batch 902, Loss 0.43260902166366577\n","[Training Epoch 3] Batch 903, Loss 0.4072358012199402\n","[Training Epoch 3] Batch 904, Loss 0.44203925132751465\n","[Training Epoch 3] Batch 905, Loss 0.44795048236846924\n","[Training Epoch 3] Batch 906, Loss 0.3941320478916168\n","[Training Epoch 3] Batch 907, Loss 0.4335222840309143\n","[Training Epoch 3] Batch 908, Loss 0.43443620204925537\n","[Training Epoch 3] Batch 909, Loss 0.4477033019065857\n","[Training Epoch 3] Batch 910, Loss 0.4344865679740906\n","[Training Epoch 3] Batch 911, Loss 0.4048771858215332\n","[Training Epoch 3] Batch 912, Loss 0.4296581745147705\n","[Training Epoch 3] Batch 913, Loss 0.4241974949836731\n","[Training Epoch 3] Batch 914, Loss 0.4299848973751068\n","[Training Epoch 3] Batch 915, Loss 0.43496644496917725\n","[Training Epoch 3] Batch 916, Loss 0.4426345229148865\n","[Training Epoch 3] Batch 917, Loss 0.4092101454734802\n","[Training Epoch 3] Batch 918, Loss 0.4009951949119568\n","[Training Epoch 3] Batch 919, Loss 0.4397115111351013\n","[Training Epoch 3] Batch 920, Loss 0.42329007387161255\n","[Training Epoch 3] Batch 921, Loss 0.4521309733390808\n","[Training Epoch 3] Batch 922, Loss 0.4013689160346985\n","[Training Epoch 3] Batch 923, Loss 0.42077288031578064\n","[Training Epoch 3] Batch 924, Loss 0.42980116605758667\n","[Training Epoch 3] Batch 925, Loss 0.41115742921829224\n","[Training Epoch 3] Batch 926, Loss 0.42875850200653076\n","[Training Epoch 3] Batch 927, Loss 0.4200950264930725\n","[Training Epoch 3] Batch 928, Loss 0.44870150089263916\n","[Training Epoch 3] Batch 929, Loss 0.42902708053588867\n","[Training Epoch 3] Batch 930, Loss 0.45492416620254517\n","[Training Epoch 3] Batch 931, Loss 0.4302353262901306\n","[Training Epoch 3] Batch 932, Loss 0.41948747634887695\n","[Training Epoch 3] Batch 933, Loss 0.43744760751724243\n","[Training Epoch 3] Batch 934, Loss 0.4255847632884979\n","[Training Epoch 3] Batch 935, Loss 0.44352519512176514\n","[Training Epoch 3] Batch 936, Loss 0.44555574655532837\n","[Training Epoch 3] Batch 937, Loss 0.4174482524394989\n","[Training Epoch 3] Batch 938, Loss 0.41370099782943726\n","[Training Epoch 3] Batch 939, Loss 0.40962034463882446\n","[Training Epoch 3] Batch 940, Loss 0.43307173252105713\n","[Training Epoch 3] Batch 941, Loss 0.4124280512332916\n","[Training Epoch 3] Batch 942, Loss 0.42356011271476746\n","[Training Epoch 3] Batch 943, Loss 0.4056469202041626\n","[Training Epoch 3] Batch 944, Loss 0.4222046136856079\n","[Training Epoch 3] Batch 945, Loss 0.42380622029304504\n","[Training Epoch 3] Batch 946, Loss 0.425025999546051\n","[Training Epoch 3] Batch 947, Loss 0.417235791683197\n","[Training Epoch 3] Batch 948, Loss 0.4169469475746155\n","[Training Epoch 3] Batch 949, Loss 0.42120224237442017\n","[Training Epoch 3] Batch 950, Loss 0.4565609097480774\n","[Training Epoch 3] Batch 951, Loss 0.41275250911712646\n","[Training Epoch 3] Batch 952, Loss 0.44661277532577515\n","[Training Epoch 3] Batch 953, Loss 0.4262595474720001\n","[Training Epoch 3] Batch 954, Loss 0.4275546371936798\n","[Training Epoch 3] Batch 955, Loss 0.4372803568840027\n","[Training Epoch 3] Batch 956, Loss 0.42579472064971924\n","[Training Epoch 3] Batch 957, Loss 0.4455462098121643\n","[Training Epoch 3] Batch 958, Loss 0.38932767510414124\n","[Training Epoch 3] Batch 959, Loss 0.4265121817588806\n","[Training Epoch 3] Batch 960, Loss 0.4333287477493286\n","[Training Epoch 3] Batch 961, Loss 0.39210045337677\n","[Training Epoch 3] Batch 962, Loss 0.4256123900413513\n","[Training Epoch 3] Batch 963, Loss 0.40198081731796265\n","[Training Epoch 3] Batch 964, Loss 0.4380398988723755\n","[Training Epoch 3] Batch 965, Loss 0.41574519872665405\n","[Training Epoch 3] Batch 966, Loss 0.4213641881942749\n","[Training Epoch 3] Batch 967, Loss 0.45500707626342773\n","[Training Epoch 3] Batch 968, Loss 0.4051637053489685\n","[Training Epoch 3] Batch 969, Loss 0.43213531374931335\n","[Training Epoch 3] Batch 970, Loss 0.4219840466976166\n","[Training Epoch 3] Batch 971, Loss 0.42172178626060486\n","[Training Epoch 3] Batch 972, Loss 0.42233896255493164\n","[Training Epoch 3] Batch 973, Loss 0.37737327814102173\n","[Training Epoch 3] Batch 974, Loss 0.44905734062194824\n","[Training Epoch 3] Batch 975, Loss 0.44767212867736816\n","[Training Epoch 3] Batch 976, Loss 0.40712273120880127\n","[Training Epoch 3] Batch 977, Loss 0.40514785051345825\n","[Training Epoch 3] Batch 978, Loss 0.40199175477027893\n","[Training Epoch 3] Batch 979, Loss 0.4038066267967224\n","[Training Epoch 3] Batch 980, Loss 0.4323062002658844\n","[Training Epoch 3] Batch 981, Loss 0.40181583166122437\n","[Training Epoch 3] Batch 982, Loss 0.4374019503593445\n","[Training Epoch 3] Batch 983, Loss 0.43193185329437256\n","[Training Epoch 3] Batch 984, Loss 0.4115245044231415\n","[Training Epoch 3] Batch 985, Loss 0.41832995414733887\n","[Training Epoch 3] Batch 986, Loss 0.41565388441085815\n","[Training Epoch 3] Batch 987, Loss 0.4006807208061218\n","[Training Epoch 3] Batch 988, Loss 0.44989365339279175\n","[Training Epoch 3] Batch 989, Loss 0.44404029846191406\n","[Training Epoch 3] Batch 990, Loss 0.42033708095550537\n","[Training Epoch 3] Batch 991, Loss 0.4212762713432312\n","[Training Epoch 3] Batch 992, Loss 0.4459208548069\n","[Training Epoch 3] Batch 993, Loss 0.4153362512588501\n","[Training Epoch 3] Batch 994, Loss 0.4613206386566162\n","[Training Epoch 3] Batch 995, Loss 0.4184837341308594\n","[Training Epoch 3] Batch 996, Loss 0.41531187295913696\n","[Training Epoch 3] Batch 997, Loss 0.4255317449569702\n","[Training Epoch 3] Batch 998, Loss 0.40737563371658325\n","[Training Epoch 3] Batch 999, Loss 0.41986462473869324\n","[Training Epoch 3] Batch 1000, Loss 0.41273564100265503\n","[Training Epoch 3] Batch 1001, Loss 0.4195656478404999\n","[Training Epoch 3] Batch 1002, Loss 0.44632771611213684\n","[Training Epoch 3] Batch 1003, Loss 0.42245128750801086\n","[Training Epoch 3] Batch 1004, Loss 0.4350903034210205\n","[Training Epoch 3] Batch 1005, Loss 0.3847290277481079\n","[Training Epoch 3] Batch 1006, Loss 0.4251680374145508\n","[Training Epoch 3] Batch 1007, Loss 0.43058672547340393\n","[Training Epoch 3] Batch 1008, Loss 0.4216967821121216\n","[Training Epoch 3] Batch 1009, Loss 0.4230579137802124\n","[Training Epoch 3] Batch 1010, Loss 0.41528165340423584\n","[Training Epoch 3] Batch 1011, Loss 0.40382832288742065\n","[Training Epoch 3] Batch 1012, Loss 0.4403650760650635\n","[Training Epoch 3] Batch 1013, Loss 0.4139687716960907\n","[Training Epoch 3] Batch 1014, Loss 0.4283076822757721\n","[Training Epoch 3] Batch 1015, Loss 0.4251011312007904\n","[Training Epoch 3] Batch 1016, Loss 0.3975820541381836\n","[Training Epoch 3] Batch 1017, Loss 0.40174221992492676\n","[Training Epoch 3] Batch 1018, Loss 0.4043601453304291\n","[Training Epoch 3] Batch 1019, Loss 0.4247739315032959\n","[Training Epoch 3] Batch 1020, Loss 0.45916658639907837\n","[Training Epoch 3] Batch 1021, Loss 0.4172472059726715\n","[Training Epoch 3] Batch 1022, Loss 0.42714041471481323\n","[Training Epoch 3] Batch 1023, Loss 0.4173317551612854\n","[Training Epoch 3] Batch 1024, Loss 0.42272770404815674\n","[Training Epoch 3] Batch 1025, Loss 0.44084131717681885\n","[Training Epoch 3] Batch 1026, Loss 0.3983156979084015\n","[Training Epoch 3] Batch 1027, Loss 0.4517320692539215\n","[Training Epoch 3] Batch 1028, Loss 0.4292544722557068\n","[Training Epoch 3] Batch 1029, Loss 0.4129815101623535\n","[Training Epoch 3] Batch 1030, Loss 0.4257737696170807\n","[Training Epoch 3] Batch 1031, Loss 0.4391923248767853\n","[Training Epoch 3] Batch 1032, Loss 0.40705758333206177\n","[Training Epoch 3] Batch 1033, Loss 0.436525821685791\n","[Training Epoch 3] Batch 1034, Loss 0.43698304891586304\n","[Training Epoch 3] Batch 1035, Loss 0.40247154235839844\n","[Training Epoch 3] Batch 1036, Loss 0.43807870149612427\n","[Training Epoch 3] Batch 1037, Loss 0.4180835485458374\n","[Training Epoch 3] Batch 1038, Loss 0.41408249735832214\n","[Training Epoch 3] Batch 1039, Loss 0.42900753021240234\n","[Training Epoch 3] Batch 1040, Loss 0.4324336051940918\n","[Training Epoch 3] Batch 1041, Loss 0.44202715158462524\n","[Training Epoch 3] Batch 1042, Loss 0.43160128593444824\n","[Training Epoch 3] Batch 1043, Loss 0.41289716958999634\n","[Training Epoch 3] Batch 1044, Loss 0.42691755294799805\n","[Training Epoch 3] Batch 1045, Loss 0.4588735103607178\n","[Training Epoch 3] Batch 1046, Loss 0.43118777871131897\n","[Training Epoch 3] Batch 1047, Loss 0.40444642305374146\n","[Training Epoch 3] Batch 1048, Loss 0.42035341262817383\n","[Training Epoch 3] Batch 1049, Loss 0.44079574942588806\n","[Training Epoch 3] Batch 1050, Loss 0.4315283000469208\n","[Training Epoch 3] Batch 1051, Loss 0.42413872480392456\n","[Training Epoch 3] Batch 1052, Loss 0.4076281189918518\n","[Training Epoch 3] Batch 1053, Loss 0.4169408977031708\n","[Training Epoch 3] Batch 1054, Loss 0.4231787919998169\n","[Training Epoch 3] Batch 1055, Loss 0.4160997271537781\n","[Training Epoch 3] Batch 1056, Loss 0.4377380609512329\n","[Training Epoch 3] Batch 1057, Loss 0.4508155584335327\n","[Training Epoch 3] Batch 1058, Loss 0.4429151117801666\n","[Training Epoch 3] Batch 1059, Loss 0.4365333616733551\n","[Training Epoch 3] Batch 1060, Loss 0.42742934823036194\n","[Training Epoch 3] Batch 1061, Loss 0.4244723916053772\n","[Training Epoch 3] Batch 1062, Loss 0.42512398958206177\n","[Training Epoch 3] Batch 1063, Loss 0.3971850872039795\n","[Training Epoch 3] Batch 1064, Loss 0.4161737263202667\n","[Training Epoch 3] Batch 1065, Loss 0.3962629437446594\n","[Training Epoch 3] Batch 1066, Loss 0.38621246814727783\n","[Training Epoch 3] Batch 1067, Loss 0.426052987575531\n","[Training Epoch 3] Batch 1068, Loss 0.42509716749191284\n","[Training Epoch 3] Batch 1069, Loss 0.4086947441101074\n","[Training Epoch 3] Batch 1070, Loss 0.479819118976593\n","[Training Epoch 3] Batch 1071, Loss 0.43273940682411194\n","[Training Epoch 3] Batch 1072, Loss 0.45148923993110657\n","[Training Epoch 3] Batch 1073, Loss 0.4291086792945862\n","[Training Epoch 3] Batch 1074, Loss 0.4104955792427063\n","[Training Epoch 3] Batch 1075, Loss 0.4285475015640259\n","[Training Epoch 3] Batch 1076, Loss 0.42841774225234985\n","[Training Epoch 3] Batch 1077, Loss 0.4046778380870819\n","[Training Epoch 3] Batch 1078, Loss 0.40143710374832153\n","[Training Epoch 3] Batch 1079, Loss 0.4210425019264221\n","[Training Epoch 3] Batch 1080, Loss 0.42653924226760864\n","[Training Epoch 3] Batch 1081, Loss 0.45730283856391907\n","[Training Epoch 3] Batch 1082, Loss 0.42577484250068665\n","[Training Epoch 3] Batch 1083, Loss 0.42626047134399414\n","[Training Epoch 3] Batch 1084, Loss 0.3953768014907837\n","[Training Epoch 3] Batch 1085, Loss 0.424051433801651\n","[Training Epoch 3] Batch 1086, Loss 0.41373348236083984\n","[Training Epoch 3] Batch 1087, Loss 0.42499297857284546\n","[Training Epoch 3] Batch 1088, Loss 0.4736057221889496\n","[Training Epoch 3] Batch 1089, Loss 0.423881471157074\n","[Training Epoch 3] Batch 1090, Loss 0.4487228989601135\n","[Training Epoch 3] Batch 1091, Loss 0.4126715064048767\n","[Training Epoch 3] Batch 1092, Loss 0.43406155705451965\n","[Training Epoch 3] Batch 1093, Loss 0.38313281536102295\n","[Training Epoch 3] Batch 1094, Loss 0.42821770906448364\n","[Training Epoch 3] Batch 1095, Loss 0.41748547554016113\n","[Training Epoch 3] Batch 1096, Loss 0.43407130241394043\n","[Training Epoch 3] Batch 1097, Loss 0.4052008390426636\n","[Training Epoch 3] Batch 1098, Loss 0.42027801275253296\n","[Training Epoch 3] Batch 1099, Loss 0.4139690697193146\n","[Training Epoch 3] Batch 1100, Loss 0.4408026337623596\n","[Training Epoch 3] Batch 1101, Loss 0.4251812696456909\n","[Training Epoch 3] Batch 1102, Loss 0.3988366723060608\n","[Training Epoch 3] Batch 1103, Loss 0.41004490852355957\n","[Training Epoch 3] Batch 1104, Loss 0.41249778866767883\n","[Training Epoch 3] Batch 1105, Loss 0.44107556343078613\n","[Training Epoch 3] Batch 1106, Loss 0.4117588698863983\n","[Training Epoch 3] Batch 1107, Loss 0.43189895153045654\n","[Training Epoch 3] Batch 1108, Loss 0.4464074969291687\n","[Training Epoch 3] Batch 1109, Loss 0.41421937942504883\n","[Training Epoch 3] Batch 1110, Loss 0.4454140365123749\n","[Training Epoch 3] Batch 1111, Loss 0.42118167877197266\n","[Training Epoch 3] Batch 1112, Loss 0.401604562997818\n","[Training Epoch 3] Batch 1113, Loss 0.4159550070762634\n","[Training Epoch 3] Batch 1114, Loss 0.4367518723011017\n","[Training Epoch 3] Batch 1115, Loss 0.43089085817337036\n","[Training Epoch 3] Batch 1116, Loss 0.3872641921043396\n","[Training Epoch 3] Batch 1117, Loss 0.42173588275909424\n","[Training Epoch 3] Batch 1118, Loss 0.4057745635509491\n","[Training Epoch 3] Batch 1119, Loss 0.4319494068622589\n","[Training Epoch 3] Batch 1120, Loss 0.4268621802330017\n","[Training Epoch 3] Batch 1121, Loss 0.4218023121356964\n","[Training Epoch 3] Batch 1122, Loss 0.4235871434211731\n","[Training Epoch 3] Batch 1123, Loss 0.3920365571975708\n","[Training Epoch 3] Batch 1124, Loss 0.43067479133605957\n","[Training Epoch 3] Batch 1125, Loss 0.42134588956832886\n","[Training Epoch 3] Batch 1126, Loss 0.4230141341686249\n","[Training Epoch 3] Batch 1127, Loss 0.40713775157928467\n","[Training Epoch 3] Batch 1128, Loss 0.40379413962364197\n","[Training Epoch 3] Batch 1129, Loss 0.41652047634124756\n","[Training Epoch 3] Batch 1130, Loss 0.4092469811439514\n","[Training Epoch 3] Batch 1131, Loss 0.46548303961753845\n","[Training Epoch 3] Batch 1132, Loss 0.43970787525177\n","[Training Epoch 3] Batch 1133, Loss 0.4342159032821655\n","[Training Epoch 3] Batch 1134, Loss 0.4253362715244293\n","[Training Epoch 3] Batch 1135, Loss 0.4123811721801758\n","[Training Epoch 3] Batch 1136, Loss 0.43010514974594116\n","[Training Epoch 3] Batch 1137, Loss 0.416003555059433\n","[Training Epoch 3] Batch 1138, Loss 0.4052238166332245\n","[Training Epoch 3] Batch 1139, Loss 0.4215675890445709\n","[Training Epoch 3] Batch 1140, Loss 0.4336020052433014\n","[Training Epoch 3] Batch 1141, Loss 0.4156012535095215\n","[Training Epoch 3] Batch 1142, Loss 0.4589676260948181\n","[Training Epoch 3] Batch 1143, Loss 0.4260696768760681\n","[Training Epoch 3] Batch 1144, Loss 0.41418159008026123\n","[Training Epoch 3] Batch 1145, Loss 0.4015662968158722\n","[Training Epoch 3] Batch 1146, Loss 0.4490213096141815\n","[Training Epoch 3] Batch 1147, Loss 0.4013933837413788\n","[Training Epoch 3] Batch 1148, Loss 0.41266798973083496\n","[Training Epoch 3] Batch 1149, Loss 0.40985679626464844\n","[Training Epoch 3] Batch 1150, Loss 0.4168717861175537\n","[Training Epoch 3] Batch 1151, Loss 0.4160449504852295\n","[Training Epoch 3] Batch 1152, Loss 0.4236978590488434\n","[Training Epoch 3] Batch 1153, Loss 0.39557379484176636\n","[Training Epoch 3] Batch 1154, Loss 0.4165618419647217\n","[Training Epoch 3] Batch 1155, Loss 0.425252765417099\n","[Training Epoch 3] Batch 1156, Loss 0.3851572871208191\n","[Training Epoch 3] Batch 1157, Loss 0.40802112221717834\n","[Training Epoch 3] Batch 1158, Loss 0.384890615940094\n","[Training Epoch 3] Batch 1159, Loss 0.4349909722805023\n","[Training Epoch 3] Batch 1160, Loss 0.42351704835891724\n","[Training Epoch 3] Batch 1161, Loss 0.3997325301170349\n","[Training Epoch 3] Batch 1162, Loss 0.40766191482543945\n","[Training Epoch 3] Batch 1163, Loss 0.4331247806549072\n","[Training Epoch 3] Batch 1164, Loss 0.4351160526275635\n","[Training Epoch 3] Batch 1165, Loss 0.451832115650177\n","[Training Epoch 3] Batch 1166, Loss 0.437082439661026\n","[Training Epoch 3] Batch 1167, Loss 0.42484050989151\n","[Training Epoch 3] Batch 1168, Loss 0.443390429019928\n","[Training Epoch 3] Batch 1169, Loss 0.4200882613658905\n","[Training Epoch 3] Batch 1170, Loss 0.4379100501537323\n","[Training Epoch 3] Batch 1171, Loss 0.4048071801662445\n","[Training Epoch 3] Batch 1172, Loss 0.41017356514930725\n","[Training Epoch 3] Batch 1173, Loss 0.4172435998916626\n","[Training Epoch 3] Batch 1174, Loss 0.4203089475631714\n","[Training Epoch 3] Batch 1175, Loss 0.4339456558227539\n","[Training Epoch 3] Batch 1176, Loss 0.4048207998275757\n","[Training Epoch 3] Batch 1177, Loss 0.433320552110672\n","[Training Epoch 3] Batch 1178, Loss 0.43605971336364746\n","[Training Epoch 3] Batch 1179, Loss 0.4483678340911865\n","[Training Epoch 3] Batch 1180, Loss 0.42512381076812744\n","[Training Epoch 3] Batch 1181, Loss 0.41043537855148315\n","[Training Epoch 3] Batch 1182, Loss 0.43505996465682983\n","[Training Epoch 3] Batch 1183, Loss 0.40222832560539246\n","[Training Epoch 3] Batch 1184, Loss 0.41522303223609924\n","[Training Epoch 3] Batch 1185, Loss 0.4240277111530304\n","[Training Epoch 3] Batch 1186, Loss 0.4081065058708191\n","[Training Epoch 3] Batch 1187, Loss 0.4193863868713379\n","[Training Epoch 3] Batch 1188, Loss 0.4247244596481323\n","[Training Epoch 3] Batch 1189, Loss 0.41518914699554443\n","[Training Epoch 3] Batch 1190, Loss 0.4005834460258484\n","[Training Epoch 3] Batch 1191, Loss 0.44009047746658325\n","[Training Epoch 3] Batch 1192, Loss 0.4425511062145233\n","[Training Epoch 3] Batch 1193, Loss 0.43422016501426697\n","[Training Epoch 3] Batch 1194, Loss 0.4228328764438629\n","[Training Epoch 3] Batch 1195, Loss 0.39824309945106506\n","[Training Epoch 3] Batch 1196, Loss 0.4350510835647583\n","[Training Epoch 3] Batch 1197, Loss 0.42502129077911377\n","[Training Epoch 3] Batch 1198, Loss 0.4275703430175781\n","[Training Epoch 3] Batch 1199, Loss 0.43277284502983093\n","[Training Epoch 3] Batch 1200, Loss 0.41125303506851196\n","[Training Epoch 3] Batch 1201, Loss 0.42506492137908936\n","[Training Epoch 3] Batch 1202, Loss 0.39067330956459045\n","[Training Epoch 3] Batch 1203, Loss 0.41689983010292053\n","[Training Epoch 3] Batch 1204, Loss 0.40087389945983887\n","[Training Epoch 3] Batch 1205, Loss 0.43132200837135315\n","[Training Epoch 3] Batch 1206, Loss 0.40633395314216614\n","[Training Epoch 3] Batch 1207, Loss 0.4140746593475342\n","[Training Epoch 3] Batch 1208, Loss 0.41737890243530273\n","[Training Epoch 3] Batch 1209, Loss 0.40707463026046753\n","[Training Epoch 3] Batch 1210, Loss 0.44681796431541443\n","[Training Epoch 3] Batch 1211, Loss 0.4577034115791321\n","[Training Epoch 3] Batch 1212, Loss 0.4347323477268219\n","[Training Epoch 3] Batch 1213, Loss 0.41098499298095703\n","[Training Epoch 3] Batch 1214, Loss 0.4200780987739563\n","[Training Epoch 3] Batch 1215, Loss 0.39145195484161377\n","[Training Epoch 3] Batch 1216, Loss 0.42449435591697693\n","[Training Epoch 3] Batch 1217, Loss 0.44605323672294617\n","[Training Epoch 3] Batch 1218, Loss 0.4231081008911133\n","[Training Epoch 3] Batch 1219, Loss 0.41305482387542725\n","[Training Epoch 3] Batch 1220, Loss 0.4098432660102844\n","[Training Epoch 3] Batch 1221, Loss 0.43636614084243774\n","[Training Epoch 3] Batch 1222, Loss 0.38263237476348877\n","[Training Epoch 3] Batch 1223, Loss 0.44292396306991577\n","[Training Epoch 3] Batch 1224, Loss 0.41110682487487793\n","[Training Epoch 3] Batch 1225, Loss 0.4135432839393616\n","[Training Epoch 3] Batch 1226, Loss 0.4094361662864685\n","[Training Epoch 3] Batch 1227, Loss 0.3872964382171631\n","[Training Epoch 3] Batch 1228, Loss 0.4256530702114105\n","[Training Epoch 3] Batch 1229, Loss 0.4012075364589691\n","[Training Epoch 3] Batch 1230, Loss 0.4333913326263428\n","[Training Epoch 3] Batch 1231, Loss 0.42406392097473145\n","[Training Epoch 3] Batch 1232, Loss 0.4185907542705536\n","[Training Epoch 3] Batch 1233, Loss 0.39450564980506897\n","[Training Epoch 3] Batch 1234, Loss 0.4545934796333313\n","[Training Epoch 3] Batch 1235, Loss 0.4444468915462494\n","[Training Epoch 3] Batch 1236, Loss 0.42014604806900024\n","[Training Epoch 3] Batch 1237, Loss 0.42742228507995605\n","[Training Epoch 3] Batch 1238, Loss 0.4442107677459717\n","[Training Epoch 3] Batch 1239, Loss 0.4112550616264343\n","[Training Epoch 3] Batch 1240, Loss 0.40690773725509644\n","[Training Epoch 3] Batch 1241, Loss 0.4161043167114258\n","[Training Epoch 3] Batch 1242, Loss 0.44458940625190735\n","[Training Epoch 3] Batch 1243, Loss 0.39763325452804565\n","[Training Epoch 3] Batch 1244, Loss 0.430223286151886\n","[Training Epoch 3] Batch 1245, Loss 0.41547662019729614\n","[Training Epoch 3] Batch 1246, Loss 0.4395488500595093\n","[Training Epoch 3] Batch 1247, Loss 0.4231465756893158\n","[Training Epoch 3] Batch 1248, Loss 0.4441922605037689\n","[Training Epoch 3] Batch 1249, Loss 0.42663708329200745\n","[Training Epoch 3] Batch 1250, Loss 0.4151393175125122\n","[Training Epoch 3] Batch 1251, Loss 0.43316879868507385\n","[Training Epoch 3] Batch 1252, Loss 0.41142168641090393\n","[Training Epoch 3] Batch 1253, Loss 0.424344539642334\n","[Training Epoch 3] Batch 1254, Loss 0.4280441403388977\n","[Training Epoch 3] Batch 1255, Loss 0.4212385416030884\n","[Training Epoch 3] Batch 1256, Loss 0.44033411145210266\n","[Training Epoch 3] Batch 1257, Loss 0.4168257713317871\n","[Training Epoch 3] Batch 1258, Loss 0.44826942682266235\n","[Training Epoch 3] Batch 1259, Loss 0.4079815745353699\n","[Training Epoch 3] Batch 1260, Loss 0.40175682306289673\n","[Training Epoch 3] Batch 1261, Loss 0.42954498529434204\n","[Training Epoch 3] Batch 1262, Loss 0.41130226850509644\n","[Training Epoch 3] Batch 1263, Loss 0.403509259223938\n","[Training Epoch 3] Batch 1264, Loss 0.423419713973999\n","[Training Epoch 3] Batch 1265, Loss 0.42740118503570557\n","[Training Epoch 3] Batch 1266, Loss 0.3768680989742279\n","[Training Epoch 3] Batch 1267, Loss 0.40706026554107666\n","[Training Epoch 3] Batch 1268, Loss 0.4078304171562195\n","[Training Epoch 3] Batch 1269, Loss 0.4364084005355835\n","[Training Epoch 3] Batch 1270, Loss 0.43087124824523926\n","[Training Epoch 3] Batch 1271, Loss 0.43244558572769165\n","[Training Epoch 3] Batch 1272, Loss 0.41080182790756226\n","[Training Epoch 3] Batch 1273, Loss 0.42641299962997437\n","[Training Epoch 3] Batch 1274, Loss 0.4534842073917389\n","[Training Epoch 3] Batch 1275, Loss 0.3792012929916382\n","[Training Epoch 3] Batch 1276, Loss 0.42567723989486694\n","[Training Epoch 3] Batch 1277, Loss 0.45391198992729187\n","[Training Epoch 3] Batch 1278, Loss 0.44351714849472046\n","[Training Epoch 3] Batch 1279, Loss 0.40135443210601807\n","[Training Epoch 3] Batch 1280, Loss 0.4214075803756714\n","[Training Epoch 3] Batch 1281, Loss 0.42914876341819763\n","[Training Epoch 3] Batch 1282, Loss 0.41775062680244446\n","[Training Epoch 3] Batch 1283, Loss 0.3938506841659546\n","[Training Epoch 3] Batch 1284, Loss 0.4010298252105713\n","[Training Epoch 3] Batch 1285, Loss 0.4095305800437927\n","[Training Epoch 3] Batch 1286, Loss 0.39151790738105774\n","[Training Epoch 3] Batch 1287, Loss 0.41825997829437256\n","[Training Epoch 3] Batch 1288, Loss 0.430209755897522\n","[Training Epoch 3] Batch 1289, Loss 0.4386768341064453\n","[Training Epoch 3] Batch 1290, Loss 0.42030173540115356\n","[Training Epoch 3] Batch 1291, Loss 0.4502013325691223\n","[Training Epoch 3] Batch 1292, Loss 0.4372023344039917\n","[Training Epoch 3] Batch 1293, Loss 0.3676995038986206\n","[Training Epoch 3] Batch 1294, Loss 0.42787808179855347\n","[Training Epoch 3] Batch 1295, Loss 0.41609007120132446\n","[Training Epoch 3] Batch 1296, Loss 0.4331628382205963\n","[Training Epoch 3] Batch 1297, Loss 0.4168502688407898\n","[Training Epoch 3] Batch 1298, Loss 0.437345027923584\n","[Training Epoch 3] Batch 1299, Loss 0.3963674306869507\n","[Training Epoch 3] Batch 1300, Loss 0.4020976424217224\n","[Training Epoch 3] Batch 1301, Loss 0.4087482690811157\n","[Training Epoch 3] Batch 1302, Loss 0.4125567078590393\n","[Training Epoch 3] Batch 1303, Loss 0.397482305765152\n","[Training Epoch 3] Batch 1304, Loss 0.44156116247177124\n","[Training Epoch 3] Batch 1305, Loss 0.42975372076034546\n","[Training Epoch 3] Batch 1306, Loss 0.41972631216049194\n","[Training Epoch 3] Batch 1307, Loss 0.39970511198043823\n","[Training Epoch 3] Batch 1308, Loss 0.41493725776672363\n","[Training Epoch 3] Batch 1309, Loss 0.43349123001098633\n","[Training Epoch 3] Batch 1310, Loss 0.44057437777519226\n","[Training Epoch 3] Batch 1311, Loss 0.4058743417263031\n","[Training Epoch 3] Batch 1312, Loss 0.42476925253868103\n","[Training Epoch 3] Batch 1313, Loss 0.37270504236221313\n","[Training Epoch 3] Batch 1314, Loss 0.4148460626602173\n","[Training Epoch 3] Batch 1315, Loss 0.38673675060272217\n","[Training Epoch 3] Batch 1316, Loss 0.41942739486694336\n","[Training Epoch 3] Batch 1317, Loss 0.4222661256790161\n","[Training Epoch 3] Batch 1318, Loss 0.36012253165245056\n","[Training Epoch 3] Batch 1319, Loss 0.42536240816116333\n","[Training Epoch 3] Batch 1320, Loss 0.4199473559856415\n","[Training Epoch 3] Batch 1321, Loss 0.43195009231567383\n","[Training Epoch 3] Batch 1322, Loss 0.43158280849456787\n","[Training Epoch 3] Batch 1323, Loss 0.4186323583126068\n","[Training Epoch 3] Batch 1324, Loss 0.38839349150657654\n","[Training Epoch 3] Batch 1325, Loss 0.3920065462589264\n","[Training Epoch 3] Batch 1326, Loss 0.4390092194080353\n","[Training Epoch 3] Batch 1327, Loss 0.42571941018104553\n","[Training Epoch 3] Batch 1328, Loss 0.3609120845794678\n","[Training Epoch 3] Batch 1329, Loss 0.3975747227668762\n","[Training Epoch 3] Batch 1330, Loss 0.39094871282577515\n","[Training Epoch 3] Batch 1331, Loss 0.43912890553474426\n","[Training Epoch 3] Batch 1332, Loss 0.4072291851043701\n","[Training Epoch 3] Batch 1333, Loss 0.42228367924690247\n","[Training Epoch 3] Batch 1334, Loss 0.4559106230735779\n","[Training Epoch 3] Batch 1335, Loss 0.4435760974884033\n","[Training Epoch 3] Batch 1336, Loss 0.4114569425582886\n","[Training Epoch 3] Batch 1337, Loss 0.41487058997154236\n","[Training Epoch 3] Batch 1338, Loss 0.4241507947444916\n","[Training Epoch 3] Batch 1339, Loss 0.38337963819503784\n","[Training Epoch 3] Batch 1340, Loss 0.42502743005752563\n","[Training Epoch 3] Batch 1341, Loss 0.43068334460258484\n","[Training Epoch 3] Batch 1342, Loss 0.4100886285305023\n","[Training Epoch 3] Batch 1343, Loss 0.4526748061180115\n","[Training Epoch 3] Batch 1344, Loss 0.4155503511428833\n","[Training Epoch 3] Batch 1345, Loss 0.42236629128456116\n","[Training Epoch 3] Batch 1346, Loss 0.4140201807022095\n","[Training Epoch 3] Batch 1347, Loss 0.42933398485183716\n","[Training Epoch 3] Batch 1348, Loss 0.41388028860092163\n","[Training Epoch 3] Batch 1349, Loss 0.4407072365283966\n","[Training Epoch 3] Batch 1350, Loss 0.4381749927997589\n","[Training Epoch 3] Batch 1351, Loss 0.41836512088775635\n","[Training Epoch 3] Batch 1352, Loss 0.39073482155799866\n","[Training Epoch 3] Batch 1353, Loss 0.3958071768283844\n","[Training Epoch 3] Batch 1354, Loss 0.44161689281463623\n","[Training Epoch 3] Batch 1355, Loss 0.4103524088859558\n","[Training Epoch 3] Batch 1356, Loss 0.4106650650501251\n","[Training Epoch 3] Batch 1357, Loss 0.4226174056529999\n","[Training Epoch 3] Batch 1358, Loss 0.41480737924575806\n","[Training Epoch 3] Batch 1359, Loss 0.41643333435058594\n","[Training Epoch 3] Batch 1360, Loss 0.41066449880599976\n","[Training Epoch 3] Batch 1361, Loss 0.4277383089065552\n","[Training Epoch 3] Batch 1362, Loss 0.42787712812423706\n","[Training Epoch 3] Batch 1363, Loss 0.4100739657878876\n","[Training Epoch 3] Batch 1364, Loss 0.4355286955833435\n","[Training Epoch 3] Batch 1365, Loss 0.3948653042316437\n","[Training Epoch 3] Batch 1366, Loss 0.4621660113334656\n","[Training Epoch 3] Batch 1367, Loss 0.40760546922683716\n","[Training Epoch 3] Batch 1368, Loss 0.43932920694351196\n","[Training Epoch 3] Batch 1369, Loss 0.43778544664382935\n","[Training Epoch 3] Batch 1370, Loss 0.42121022939682007\n","[Training Epoch 3] Batch 1371, Loss 0.41779255867004395\n","[Training Epoch 3] Batch 1372, Loss 0.40398263931274414\n","[Training Epoch 3] Batch 1373, Loss 0.40867355465888977\n","[Training Epoch 3] Batch 1374, Loss 0.4260954260826111\n","[Training Epoch 3] Batch 1375, Loss 0.40782952308654785\n","[Training Epoch 3] Batch 1376, Loss 0.4168120324611664\n","[Training Epoch 3] Batch 1377, Loss 0.4322205185890198\n","[Training Epoch 3] Batch 1378, Loss 0.43394312262535095\n","[Training Epoch 3] Batch 1379, Loss 0.42381179332733154\n","[Training Epoch 3] Batch 1380, Loss 0.4295193552970886\n","[Training Epoch 3] Batch 1381, Loss 0.4183638095855713\n","[Training Epoch 3] Batch 1382, Loss 0.41706719994544983\n","[Training Epoch 3] Batch 1383, Loss 0.40837720036506653\n","[Training Epoch 3] Batch 1384, Loss 0.43307146430015564\n","[Training Epoch 3] Batch 1385, Loss 0.4233464002609253\n","[Training Epoch 3] Batch 1386, Loss 0.41890639066696167\n","[Training Epoch 3] Batch 1387, Loss 0.40741485357284546\n","[Training Epoch 3] Batch 1388, Loss 0.42497432231903076\n","[Training Epoch 3] Batch 1389, Loss 0.4181586503982544\n","[Training Epoch 3] Batch 1390, Loss 0.436134934425354\n","[Training Epoch 3] Batch 1391, Loss 0.4092615842819214\n","[Training Epoch 3] Batch 1392, Loss 0.429810494184494\n","[Training Epoch 3] Batch 1393, Loss 0.4198181629180908\n","[Training Epoch 3] Batch 1394, Loss 0.39903610944747925\n","[Training Epoch 3] Batch 1395, Loss 0.40672218799591064\n","[Training Epoch 3] Batch 1396, Loss 0.39387667179107666\n","[Training Epoch 3] Batch 1397, Loss 0.39045384526252747\n","[Training Epoch 3] Batch 1398, Loss 0.41101109981536865\n","[Training Epoch 3] Batch 1399, Loss 0.4181016683578491\n","[Training Epoch 3] Batch 1400, Loss 0.4118935465812683\n","[Training Epoch 3] Batch 1401, Loss 0.4583062529563904\n","[Training Epoch 3] Batch 1402, Loss 0.420692503452301\n","[Training Epoch 3] Batch 1403, Loss 0.4282774031162262\n","[Training Epoch 3] Batch 1404, Loss 0.42099785804748535\n","[Training Epoch 3] Batch 1405, Loss 0.4233638644218445\n","[Training Epoch 3] Batch 1406, Loss 0.3992258906364441\n","[Training Epoch 3] Batch 1407, Loss 0.39435988664627075\n","[Training Epoch 3] Batch 1408, Loss 0.41173285245895386\n","[Training Epoch 3] Batch 1409, Loss 0.411899209022522\n","[Training Epoch 3] Batch 1410, Loss 0.41986602544784546\n","[Training Epoch 3] Batch 1411, Loss 0.4252561330795288\n","[Training Epoch 3] Batch 1412, Loss 0.42587611079216003\n","[Training Epoch 3] Batch 1413, Loss 0.41486650705337524\n","[Training Epoch 3] Batch 1414, Loss 0.3910068869590759\n","[Training Epoch 3] Batch 1415, Loss 0.43095001578330994\n","[Training Epoch 3] Batch 1416, Loss 0.39679235219955444\n","[Training Epoch 3] Batch 1417, Loss 0.45337843894958496\n","[Training Epoch 3] Batch 1418, Loss 0.4159379005432129\n","[Training Epoch 3] Batch 1419, Loss 0.4421561360359192\n","[Training Epoch 3] Batch 1420, Loss 0.38500189781188965\n","[Training Epoch 3] Batch 1421, Loss 0.4171213209629059\n","[Training Epoch 3] Batch 1422, Loss 0.44213634729385376\n","[Training Epoch 3] Batch 1423, Loss 0.4243311285972595\n","[Training Epoch 3] Batch 1424, Loss 0.4469515085220337\n","[Training Epoch 3] Batch 1425, Loss 0.41272905468940735\n","[Training Epoch 3] Batch 1426, Loss 0.42617666721343994\n","[Training Epoch 3] Batch 1427, Loss 0.4051654040813446\n","[Training Epoch 3] Batch 1428, Loss 0.40144139528274536\n","[Training Epoch 3] Batch 1429, Loss 0.4273780584335327\n","[Training Epoch 3] Batch 1430, Loss 0.44531357288360596\n","[Training Epoch 3] Batch 1431, Loss 0.41773393750190735\n","[Training Epoch 3] Batch 1432, Loss 0.43271851539611816\n","[Training Epoch 3] Batch 1433, Loss 0.4045587182044983\n","[Training Epoch 3] Batch 1434, Loss 0.401710569858551\n","[Training Epoch 3] Batch 1435, Loss 0.41554397344589233\n","[Training Epoch 3] Batch 1436, Loss 0.42868274450302124\n","[Training Epoch 3] Batch 1437, Loss 0.44104185700416565\n","[Training Epoch 3] Batch 1438, Loss 0.40331605076789856\n","[Training Epoch 3] Batch 1439, Loss 0.4182535707950592\n","[Training Epoch 3] Batch 1440, Loss 0.41915303468704224\n","[Training Epoch 3] Batch 1441, Loss 0.4075092375278473\n","[Training Epoch 3] Batch 1442, Loss 0.4222491979598999\n","[Training Epoch 3] Batch 1443, Loss 0.40489503741264343\n","[Training Epoch 3] Batch 1444, Loss 0.40976327657699585\n","[Training Epoch 3] Batch 1445, Loss 0.44637972116470337\n","[Training Epoch 3] Batch 1446, Loss 0.423326313495636\n","[Training Epoch 3] Batch 1447, Loss 0.3988490104675293\n","[Training Epoch 3] Batch 1448, Loss 0.40555447340011597\n","[Training Epoch 3] Batch 1449, Loss 0.4088525176048279\n","[Training Epoch 3] Batch 1450, Loss 0.415644109249115\n","[Training Epoch 3] Batch 1451, Loss 0.40526169538497925\n","[Training Epoch 3] Batch 1452, Loss 0.40506985783576965\n","[Training Epoch 3] Batch 1453, Loss 0.38968604803085327\n","[Training Epoch 3] Batch 1454, Loss 0.43237149715423584\n","[Training Epoch 3] Batch 1455, Loss 0.43155843019485474\n","[Training Epoch 3] Batch 1456, Loss 0.39568454027175903\n","[Training Epoch 3] Batch 1457, Loss 0.39171043038368225\n","[Training Epoch 3] Batch 1458, Loss 0.41195493936538696\n","[Training Epoch 3] Batch 1459, Loss 0.3987088203430176\n","[Training Epoch 3] Batch 1460, Loss 0.3918185234069824\n","[Training Epoch 3] Batch 1461, Loss 0.4146372079849243\n","[Training Epoch 3] Batch 1462, Loss 0.42310577630996704\n","[Training Epoch 3] Batch 1463, Loss 0.41984960436820984\n","[Training Epoch 3] Batch 1464, Loss 0.41231659054756165\n","[Training Epoch 3] Batch 1465, Loss 0.43790721893310547\n","[Training Epoch 3] Batch 1466, Loss 0.4089944660663605\n","[Training Epoch 3] Batch 1467, Loss 0.43707555532455444\n","[Training Epoch 3] Batch 1468, Loss 0.4276125431060791\n","[Training Epoch 3] Batch 1469, Loss 0.41067662835121155\n","[Training Epoch 3] Batch 1470, Loss 0.4208652675151825\n","[Training Epoch 3] Batch 1471, Loss 0.41804271936416626\n","[Training Epoch 3] Batch 1472, Loss 0.43444985151290894\n","[Training Epoch 3] Batch 1473, Loss 0.3981788158416748\n","[Training Epoch 3] Batch 1474, Loss 0.41767069697380066\n","[Training Epoch 3] Batch 1475, Loss 0.4356328248977661\n","[Training Epoch 3] Batch 1476, Loss 0.4139423966407776\n","[Training Epoch 3] Batch 1477, Loss 0.41980284452438354\n","[Training Epoch 3] Batch 1478, Loss 0.40295159816741943\n","[Training Epoch 3] Batch 1479, Loss 0.39766252040863037\n","[Training Epoch 3] Batch 1480, Loss 0.4192066192626953\n","[Training Epoch 3] Batch 1481, Loss 0.4094427525997162\n","[Training Epoch 3] Batch 1482, Loss 0.4586567282676697\n","[Training Epoch 3] Batch 1483, Loss 0.4123997092247009\n","[Training Epoch 3] Batch 1484, Loss 0.40183839201927185\n","[Training Epoch 3] Batch 1485, Loss 0.39023536443710327\n","[Training Epoch 3] Batch 1486, Loss 0.38231566548347473\n","[Training Epoch 3] Batch 1487, Loss 0.4180985391139984\n","[Training Epoch 3] Batch 1488, Loss 0.42073148488998413\n","[Training Epoch 3] Batch 1489, Loss 0.417609304189682\n","[Training Epoch 3] Batch 1490, Loss 0.4231210947036743\n","[Training Epoch 3] Batch 1491, Loss 0.40776270627975464\n","[Training Epoch 3] Batch 1492, Loss 0.40799185633659363\n","[Training Epoch 3] Batch 1493, Loss 0.4213210642337799\n","[Training Epoch 3] Batch 1494, Loss 0.429506778717041\n","[Training Epoch 3] Batch 1495, Loss 0.39915359020233154\n","[Training Epoch 3] Batch 1496, Loss 0.40689000487327576\n","[Training Epoch 3] Batch 1497, Loss 0.40418505668640137\n","[Training Epoch 3] Batch 1498, Loss 0.422953724861145\n","[Training Epoch 3] Batch 1499, Loss 0.39857548475265503\n","[Training Epoch 3] Batch 1500, Loss 0.4409390091896057\n","[Training Epoch 3] Batch 1501, Loss 0.4181445837020874\n","[Training Epoch 3] Batch 1502, Loss 0.4329228401184082\n","[Training Epoch 3] Batch 1503, Loss 0.4202463626861572\n","[Training Epoch 3] Batch 1504, Loss 0.41608327627182007\n","[Training Epoch 3] Batch 1505, Loss 0.42034998536109924\n","[Training Epoch 3] Batch 1506, Loss 0.4146313965320587\n","[Training Epoch 3] Batch 1507, Loss 0.4273533821105957\n","[Training Epoch 3] Batch 1508, Loss 0.3897610902786255\n","[Training Epoch 3] Batch 1509, Loss 0.41541093587875366\n","[Training Epoch 3] Batch 1510, Loss 0.40836435556411743\n","[Training Epoch 3] Batch 1511, Loss 0.433207631111145\n","[Training Epoch 3] Batch 1512, Loss 0.42418181896209717\n","[Training Epoch 3] Batch 1513, Loss 0.4457557201385498\n","[Training Epoch 3] Batch 1514, Loss 0.42689990997314453\n","[Training Epoch 3] Batch 1515, Loss 0.4354955554008484\n","[Training Epoch 3] Batch 1516, Loss 0.37995946407318115\n","[Training Epoch 3] Batch 1517, Loss 0.3752789795398712\n","[Training Epoch 3] Batch 1518, Loss 0.4289829134941101\n","[Training Epoch 3] Batch 1519, Loss 0.4128253161907196\n","[Training Epoch 3] Batch 1520, Loss 0.4091944992542267\n","[Training Epoch 3] Batch 1521, Loss 0.3980027735233307\n","[Training Epoch 3] Batch 1522, Loss 0.4455717206001282\n","[Training Epoch 3] Batch 1523, Loss 0.4286152124404907\n","[Training Epoch 3] Batch 1524, Loss 0.3983091115951538\n","[Training Epoch 3] Batch 1525, Loss 0.4039168953895569\n","[Training Epoch 3] Batch 1526, Loss 0.4033157527446747\n","[Training Epoch 3] Batch 1527, Loss 0.4102039933204651\n","[Training Epoch 3] Batch 1528, Loss 0.4093325138092041\n","[Training Epoch 3] Batch 1529, Loss 0.3962894380092621\n","[Training Epoch 3] Batch 1530, Loss 0.4249470829963684\n","[Training Epoch 3] Batch 1531, Loss 0.39956632256507874\n","[Training Epoch 3] Batch 1532, Loss 0.40821465849876404\n","[Training Epoch 3] Batch 1533, Loss 0.4115861654281616\n","[Training Epoch 3] Batch 1534, Loss 0.40743374824523926\n","[Training Epoch 3] Batch 1535, Loss 0.4193771779537201\n","[Training Epoch 3] Batch 1536, Loss 0.3894228935241699\n","[Training Epoch 3] Batch 1537, Loss 0.3859919607639313\n","[Training Epoch 3] Batch 1538, Loss 0.39331692457199097\n","[Training Epoch 3] Batch 1539, Loss 0.4195965528488159\n","[Training Epoch 3] Batch 1540, Loss 0.3916895389556885\n","[Training Epoch 3] Batch 1541, Loss 0.4120734632015228\n","[Training Epoch 3] Batch 1542, Loss 0.40610581636428833\n","[Training Epoch 3] Batch 1543, Loss 0.4084363579750061\n","[Training Epoch 3] Batch 1544, Loss 0.40367886424064636\n","[Training Epoch 3] Batch 1545, Loss 0.38842785358428955\n","[Training Epoch 3] Batch 1546, Loss 0.4137951135635376\n","[Training Epoch 3] Batch 1547, Loss 0.4091692566871643\n","[Training Epoch 3] Batch 1548, Loss 0.3945566415786743\n","[Training Epoch 3] Batch 1549, Loss 0.3936767578125\n","[Training Epoch 3] Batch 1550, Loss 0.41233742237091064\n","[Training Epoch 3] Batch 1551, Loss 0.38218292593955994\n","[Training Epoch 3] Batch 1552, Loss 0.43174418807029724\n","[Training Epoch 3] Batch 1553, Loss 0.4033077359199524\n","[Training Epoch 3] Batch 1554, Loss 0.41790682077407837\n","[Training Epoch 3] Batch 1555, Loss 0.4410848617553711\n","[Training Epoch 3] Batch 1556, Loss 0.40827614068984985\n","[Training Epoch 3] Batch 1557, Loss 0.4290808141231537\n","[Training Epoch 3] Batch 1558, Loss 0.4169688820838928\n","[Training Epoch 3] Batch 1559, Loss 0.3989160656929016\n","[Training Epoch 3] Batch 1560, Loss 0.4012318253517151\n","[Training Epoch 3] Batch 1561, Loss 0.4261173605918884\n","[Training Epoch 3] Batch 1562, Loss 0.4041118919849396\n","[Training Epoch 3] Batch 1563, Loss 0.4078953266143799\n","[Training Epoch 3] Batch 1564, Loss 0.42313700914382935\n","[Training Epoch 3] Batch 1565, Loss 0.44200950860977173\n","[Training Epoch 3] Batch 1566, Loss 0.42085397243499756\n","[Training Epoch 3] Batch 1567, Loss 0.439465194940567\n","[Training Epoch 3] Batch 1568, Loss 0.4425657391548157\n","[Training Epoch 3] Batch 1569, Loss 0.41230496764183044\n","[Training Epoch 3] Batch 1570, Loss 0.384111225605011\n","[Training Epoch 3] Batch 1571, Loss 0.3934916853904724\n","[Training Epoch 3] Batch 1572, Loss 0.4200846254825592\n","[Training Epoch 3] Batch 1573, Loss 0.40709081292152405\n","[Training Epoch 3] Batch 1574, Loss 0.4268947243690491\n","[Training Epoch 3] Batch 1575, Loss 0.458437442779541\n","[Training Epoch 3] Batch 1576, Loss 0.3902430534362793\n","[Training Epoch 3] Batch 1577, Loss 0.42626625299453735\n","[Training Epoch 3] Batch 1578, Loss 0.4028209447860718\n","[Training Epoch 3] Batch 1579, Loss 0.4079941511154175\n","[Training Epoch 3] Batch 1580, Loss 0.4182162582874298\n","[Training Epoch 3] Batch 1581, Loss 0.3995715379714966\n","[Training Epoch 3] Batch 1582, Loss 0.43034642934799194\n","[Training Epoch 3] Batch 1583, Loss 0.41389602422714233\n","[Training Epoch 3] Batch 1584, Loss 0.4331154227256775\n","[Training Epoch 3] Batch 1585, Loss 0.4217677116394043\n","[Training Epoch 3] Batch 1586, Loss 0.42518311738967896\n","[Training Epoch 3] Batch 1587, Loss 0.4283517003059387\n","[Training Epoch 3] Batch 1588, Loss 0.39618003368377686\n","[Training Epoch 3] Batch 1589, Loss 0.39106181263923645\n","[Training Epoch 3] Batch 1590, Loss 0.38100194931030273\n","[Training Epoch 3] Batch 1591, Loss 0.4091760218143463\n","[Training Epoch 3] Batch 1592, Loss 0.3715229630470276\n","[Training Epoch 3] Batch 1593, Loss 0.3940451145172119\n","[Training Epoch 3] Batch 1594, Loss 0.39341044425964355\n","[Training Epoch 3] Batch 1595, Loss 0.41803133487701416\n","[Training Epoch 3] Batch 1596, Loss 0.4162905216217041\n","[Training Epoch 3] Batch 1597, Loss 0.3984413146972656\n","[Training Epoch 3] Batch 1598, Loss 0.4136744737625122\n","[Training Epoch 3] Batch 1599, Loss 0.4130256772041321\n","[Training Epoch 3] Batch 1600, Loss 0.4160734713077545\n","[Training Epoch 3] Batch 1601, Loss 0.4370887279510498\n","[Training Epoch 3] Batch 1602, Loss 0.39792731404304504\n","[Training Epoch 3] Batch 1603, Loss 0.39796778559684753\n","[Training Epoch 3] Batch 1604, Loss 0.4248492121696472\n","[Training Epoch 3] Batch 1605, Loss 0.41121238470077515\n","[Training Epoch 3] Batch 1606, Loss 0.404344379901886\n","[Training Epoch 3] Batch 1607, Loss 0.4477223753929138\n","[Training Epoch 3] Batch 1608, Loss 0.4373369812965393\n","[Training Epoch 3] Batch 1609, Loss 0.3970046937465668\n","[Training Epoch 3] Batch 1610, Loss 0.419346421957016\n","[Training Epoch 3] Batch 1611, Loss 0.4153922498226166\n","[Training Epoch 3] Batch 1612, Loss 0.38476434350013733\n","[Training Epoch 3] Batch 1613, Loss 0.38937079906463623\n","[Training Epoch 3] Batch 1614, Loss 0.40225136280059814\n","[Training Epoch 3] Batch 1615, Loss 0.37265974283218384\n","[Training Epoch 3] Batch 1616, Loss 0.39201295375823975\n","[Training Epoch 3] Batch 1617, Loss 0.397830992937088\n","[Training Epoch 3] Batch 1618, Loss 0.39982306957244873\n","[Training Epoch 3] Batch 1619, Loss 0.40478062629699707\n","[Training Epoch 3] Batch 1620, Loss 0.3875550329685211\n","[Training Epoch 3] Batch 1621, Loss 0.44038161635398865\n","[Training Epoch 3] Batch 1622, Loss 0.43894416093826294\n","[Training Epoch 3] Batch 1623, Loss 0.43136557936668396\n","[Training Epoch 3] Batch 1624, Loss 0.41550272703170776\n","[Training Epoch 3] Batch 1625, Loss 0.40053653717041016\n","[Training Epoch 3] Batch 1626, Loss 0.42603421211242676\n","[Training Epoch 3] Batch 1627, Loss 0.3920512795448303\n","[Training Epoch 3] Batch 1628, Loss 0.4148685932159424\n","[Training Epoch 3] Batch 1629, Loss 0.41920721530914307\n","[Training Epoch 3] Batch 1630, Loss 0.464931845664978\n","[Training Epoch 3] Batch 1631, Loss 0.396109938621521\n","[Training Epoch 3] Batch 1632, Loss 0.41973280906677246\n","[Training Epoch 3] Batch 1633, Loss 0.381447970867157\n","[Training Epoch 3] Batch 1634, Loss 0.40863755345344543\n","[Training Epoch 3] Batch 1635, Loss 0.3977768123149872\n","[Training Epoch 3] Batch 1636, Loss 0.3859270215034485\n","[Training Epoch 3] Batch 1637, Loss 0.39232537150382996\n","[Training Epoch 3] Batch 1638, Loss 0.39095523953437805\n","[Training Epoch 3] Batch 1639, Loss 0.35554224252700806\n","[Training Epoch 3] Batch 1640, Loss 0.40686535835266113\n","[Training Epoch 3] Batch 1641, Loss 0.38417479395866394\n","[Training Epoch 3] Batch 1642, Loss 0.3896508812904358\n","[Training Epoch 3] Batch 1643, Loss 0.41312476992607117\n","[Training Epoch 3] Batch 1644, Loss 0.4144294261932373\n","[Training Epoch 3] Batch 1645, Loss 0.4096832871437073\n","[Training Epoch 3] Batch 1646, Loss 0.41687968373298645\n","[Training Epoch 3] Batch 1647, Loss 0.4189704656600952\n","[Training Epoch 3] Batch 1648, Loss 0.4204515814781189\n","[Training Epoch 3] Batch 1649, Loss 0.4287447929382324\n","[Training Epoch 3] Batch 1650, Loss 0.45099061727523804\n","[Training Epoch 3] Batch 1651, Loss 0.3783336877822876\n","[Training Epoch 3] Batch 1652, Loss 0.39219242334365845\n","[Training Epoch 3] Batch 1653, Loss 0.41069674491882324\n","[Training Epoch 3] Batch 1654, Loss 0.4240207374095917\n","[Training Epoch 3] Batch 1655, Loss 0.40687865018844604\n","[Training Epoch 3] Batch 1656, Loss 0.39896154403686523\n","[Training Epoch 3] Batch 1657, Loss 0.4001156687736511\n","[Training Epoch 3] Batch 1658, Loss 0.411221444606781\n","[Training Epoch 3] Batch 1659, Loss 0.39255794882774353\n","[Training Epoch 3] Batch 1660, Loss 0.3754410147666931\n","[Training Epoch 3] Batch 1661, Loss 0.43296143412590027\n","[Training Epoch 3] Batch 1662, Loss 0.4371475279331207\n","[Training Epoch 3] Batch 1663, Loss 0.441872775554657\n","[Training Epoch 3] Batch 1664, Loss 0.4310016930103302\n","[Training Epoch 3] Batch 1665, Loss 0.3822667598724365\n","[Training Epoch 3] Batch 1666, Loss 0.37834036350250244\n","[Training Epoch 3] Batch 1667, Loss 0.41091930866241455\n","[Training Epoch 3] Batch 1668, Loss 0.410295307636261\n","[Training Epoch 3] Batch 1669, Loss 0.40828144550323486\n","[Training Epoch 3] Batch 1670, Loss 0.41235971450805664\n","[Training Epoch 3] Batch 1671, Loss 0.40340256690979004\n","[Training Epoch 3] Batch 1672, Loss 0.4212280511856079\n","[Training Epoch 3] Batch 1673, Loss 0.4231291115283966\n","[Training Epoch 3] Batch 1674, Loss 0.3802418112754822\n","[Training Epoch 3] Batch 1675, Loss 0.42293432354927063\n","[Training Epoch 3] Batch 1676, Loss 0.4187913239002228\n","[Training Epoch 3] Batch 1677, Loss 0.4301070272922516\n","[Training Epoch 3] Batch 1678, Loss 0.40441587567329407\n","[Training Epoch 3] Batch 1679, Loss 0.4221270680427551\n","[Training Epoch 3] Batch 1680, Loss 0.41333091259002686\n","[Training Epoch 3] Batch 1681, Loss 0.4251066744327545\n","[Training Epoch 3] Batch 1682, Loss 0.40198028087615967\n","[Training Epoch 3] Batch 1683, Loss 0.39425158500671387\n","[Training Epoch 3] Batch 1684, Loss 0.3838861584663391\n","[Training Epoch 3] Batch 1685, Loss 0.4064136743545532\n","[Training Epoch 3] Batch 1686, Loss 0.38782998919487\n","[Training Epoch 3] Batch 1687, Loss 0.4350176453590393\n","[Training Epoch 3] Batch 1688, Loss 0.4119580388069153\n","[Training Epoch 3] Batch 1689, Loss 0.41128987073898315\n","[Training Epoch 3] Batch 1690, Loss 0.37803709506988525\n","[Training Epoch 3] Batch 1691, Loss 0.4052238166332245\n","[Training Epoch 3] Batch 1692, Loss 0.39957985281944275\n","[Training Epoch 3] Batch 1693, Loss 0.4057547152042389\n","[Training Epoch 3] Batch 1694, Loss 0.3966531753540039\n","[Training Epoch 3] Batch 1695, Loss 0.42045411467552185\n","[Training Epoch 3] Batch 1696, Loss 0.40581172704696655\n","[Training Epoch 3] Batch 1697, Loss 0.434347540140152\n","[Training Epoch 3] Batch 1698, Loss 0.41266098618507385\n","[Training Epoch 3] Batch 1699, Loss 0.3829587996006012\n","[Training Epoch 3] Batch 1700, Loss 0.3647909164428711\n","[Training Epoch 3] Batch 1701, Loss 0.4256956875324249\n","[Training Epoch 3] Batch 1702, Loss 0.41852009296417236\n","[Training Epoch 3] Batch 1703, Loss 0.40539759397506714\n","[Training Epoch 3] Batch 1704, Loss 0.4004610776901245\n","[Training Epoch 3] Batch 1705, Loss 0.38398319482803345\n","[Training Epoch 3] Batch 1706, Loss 0.3962281346321106\n","[Training Epoch 3] Batch 1707, Loss 0.3874526619911194\n","[Training Epoch 3] Batch 1708, Loss 0.4064042568206787\n","[Training Epoch 3] Batch 1709, Loss 0.3971492052078247\n","[Training Epoch 3] Batch 1710, Loss 0.4498612880706787\n","[Training Epoch 3] Batch 1711, Loss 0.41257911920547485\n","[Training Epoch 3] Batch 1712, Loss 0.4042060971260071\n","[Training Epoch 3] Batch 1713, Loss 0.4164555072784424\n","[Training Epoch 3] Batch 1714, Loss 0.41833245754241943\n","[Training Epoch 3] Batch 1715, Loss 0.41695982217788696\n","[Training Epoch 3] Batch 1716, Loss 0.4328423738479614\n","[Training Epoch 3] Batch 1717, Loss 0.4149685502052307\n","[Training Epoch 3] Batch 1718, Loss 0.41265779733657837\n","[Training Epoch 3] Batch 1719, Loss 0.42877647280693054\n","[Training Epoch 3] Batch 1720, Loss 0.37143540382385254\n","[Training Epoch 3] Batch 1721, Loss 0.4042729139328003\n","[Training Epoch 3] Batch 1722, Loss 0.4101034998893738\n","[Training Epoch 3] Batch 1723, Loss 0.40722668170928955\n","[Training Epoch 3] Batch 1724, Loss 0.3862300515174866\n","[Training Epoch 3] Batch 1725, Loss 0.4295833706855774\n","[Training Epoch 3] Batch 1726, Loss 0.45098528265953064\n","[Training Epoch 3] Batch 1727, Loss 0.3959730267524719\n","[Training Epoch 3] Batch 1728, Loss 0.40465348958969116\n","[Training Epoch 3] Batch 1729, Loss 0.4355684518814087\n","[Training Epoch 3] Batch 1730, Loss 0.40086811780929565\n","[Training Epoch 3] Batch 1731, Loss 0.41833624243736267\n","[Training Epoch 3] Batch 1732, Loss 0.41873958706855774\n","[Training Epoch 3] Batch 1733, Loss 0.4011264145374298\n","[Training Epoch 3] Batch 1734, Loss 0.41269826889038086\n","[Training Epoch 3] Batch 1735, Loss 0.3944028615951538\n","[Training Epoch 3] Batch 1736, Loss 0.40122461318969727\n","[Training Epoch 3] Batch 1737, Loss 0.38113898038864136\n","[Training Epoch 3] Batch 1738, Loss 0.400734543800354\n","[Training Epoch 3] Batch 1739, Loss 0.3966744542121887\n","[Training Epoch 3] Batch 1740, Loss 0.369997501373291\n","[Training Epoch 3] Batch 1741, Loss 0.4025033116340637\n","[Training Epoch 3] Batch 1742, Loss 0.3932640254497528\n","[Training Epoch 3] Batch 1743, Loss 0.3845852017402649\n","[Training Epoch 3] Batch 1744, Loss 0.400385320186615\n","[Training Epoch 3] Batch 1745, Loss 0.4041159152984619\n","[Training Epoch 3] Batch 1746, Loss 0.41895920038223267\n","[Training Epoch 3] Batch 1747, Loss 0.4189267158508301\n","[Training Epoch 3] Batch 1748, Loss 0.4027467668056488\n","[Training Epoch 3] Batch 1749, Loss 0.40508896112442017\n","[Training Epoch 3] Batch 1750, Loss 0.3733745813369751\n","[Training Epoch 3] Batch 1751, Loss 0.4096798002719879\n","[Training Epoch 3] Batch 1752, Loss 0.3887273371219635\n","[Training Epoch 3] Batch 1753, Loss 0.42608994245529175\n","[Training Epoch 3] Batch 1754, Loss 0.4302726984024048\n","[Training Epoch 3] Batch 1755, Loss 0.4561125636100769\n","[Training Epoch 3] Batch 1756, Loss 0.4278809428215027\n","[Training Epoch 3] Batch 1757, Loss 0.3982054591178894\n","[Training Epoch 3] Batch 1758, Loss 0.4107089936733246\n","[Training Epoch 3] Batch 1759, Loss 0.4145309329032898\n","[Training Epoch 3] Batch 1760, Loss 0.4130750894546509\n","[Training Epoch 3] Batch 1761, Loss 0.3910793364048004\n","[Training Epoch 3] Batch 1762, Loss 0.38794293999671936\n","[Training Epoch 3] Batch 1763, Loss 0.4033742845058441\n","[Training Epoch 3] Batch 1764, Loss 0.4288698434829712\n","[Training Epoch 3] Batch 1765, Loss 0.4230945110321045\n","[Training Epoch 3] Batch 1766, Loss 0.4057290256023407\n","[Training Epoch 3] Batch 1767, Loss 0.4144943952560425\n","[Training Epoch 3] Batch 1768, Loss 0.421853631734848\n","[Training Epoch 3] Batch 1769, Loss 0.39265739917755127\n","[Training Epoch 3] Batch 1770, Loss 0.40446221828460693\n","[Training Epoch 3] Batch 1771, Loss 0.4064044952392578\n","[Training Epoch 3] Batch 1772, Loss 0.35132259130477905\n","[Training Epoch 3] Batch 1773, Loss 0.3910979628562927\n","[Training Epoch 3] Batch 1774, Loss 0.41075628995895386\n","[Training Epoch 3] Batch 1775, Loss 0.4103860557079315\n","[Training Epoch 3] Batch 1776, Loss 0.4258486032485962\n","[Training Epoch 3] Batch 1777, Loss 0.4058522880077362\n","[Training Epoch 3] Batch 1778, Loss 0.3876492381095886\n","[Training Epoch 3] Batch 1779, Loss 0.4180169999599457\n","[Training Epoch 3] Batch 1780, Loss 0.41557833552360535\n","[Training Epoch 3] Batch 1781, Loss 0.39914989471435547\n","[Training Epoch 3] Batch 1782, Loss 0.4021047353744507\n","[Training Epoch 3] Batch 1783, Loss 0.40460404753685\n","[Training Epoch 3] Batch 1784, Loss 0.4486960172653198\n","[Training Epoch 3] Batch 1785, Loss 0.4530520737171173\n","[Training Epoch 3] Batch 1786, Loss 0.38842451572418213\n","[Training Epoch 3] Batch 1787, Loss 0.3724537491798401\n","[Training Epoch 3] Batch 1788, Loss 0.39459335803985596\n","[Training Epoch 3] Batch 1789, Loss 0.44977867603302\n","[Training Epoch 3] Batch 1790, Loss 0.4009084701538086\n","[Training Epoch 3] Batch 1791, Loss 0.40989232063293457\n","[Training Epoch 3] Batch 1792, Loss 0.40422773361206055\n","[Training Epoch 3] Batch 1793, Loss 0.39035433530807495\n","[Training Epoch 3] Batch 1794, Loss 0.3829687237739563\n","[Training Epoch 3] Batch 1795, Loss 0.41620147228240967\n","[Training Epoch 3] Batch 1796, Loss 0.41817784309387207\n","[Training Epoch 3] Batch 1797, Loss 0.4395252764225006\n","[Training Epoch 3] Batch 1798, Loss 0.4243120849132538\n","[Training Epoch 3] Batch 1799, Loss 0.43918395042419434\n","[Training Epoch 3] Batch 1800, Loss 0.4061185419559479\n","[Training Epoch 3] Batch 1801, Loss 0.40752172470092773\n","[Training Epoch 3] Batch 1802, Loss 0.43095073103904724\n","[Training Epoch 3] Batch 1803, Loss 0.39858055114746094\n","[Training Epoch 3] Batch 1804, Loss 0.39832842350006104\n","[Training Epoch 3] Batch 1805, Loss 0.43276211619377136\n","[Training Epoch 3] Batch 1806, Loss 0.4134010076522827\n","[Training Epoch 3] Batch 1807, Loss 0.387584924697876\n","[Training Epoch 3] Batch 1808, Loss 0.4118272066116333\n","[Training Epoch 3] Batch 1809, Loss 0.4160767197608948\n","[Training Epoch 3] Batch 1810, Loss 0.4045564532279968\n","[Training Epoch 3] Batch 1811, Loss 0.42985260486602783\n","[Training Epoch 3] Batch 1812, Loss 0.3980858027935028\n","[Training Epoch 3] Batch 1813, Loss 0.4249012768268585\n","[Training Epoch 3] Batch 1814, Loss 0.4016871154308319\n","[Training Epoch 3] Batch 1815, Loss 0.3866618871688843\n","[Training Epoch 3] Batch 1816, Loss 0.3634703755378723\n","[Training Epoch 3] Batch 1817, Loss 0.4159683287143707\n","[Training Epoch 3] Batch 1818, Loss 0.4196926951408386\n","[Training Epoch 3] Batch 1819, Loss 0.4286510646343231\n","[Training Epoch 3] Batch 1820, Loss 0.4101976752281189\n","[Training Epoch 3] Batch 1821, Loss 0.3768905997276306\n","[Training Epoch 3] Batch 1822, Loss 0.4287475049495697\n","[Training Epoch 3] Batch 1823, Loss 0.39358800649642944\n","[Training Epoch 3] Batch 1824, Loss 0.425417959690094\n","[Training Epoch 3] Batch 1825, Loss 0.38968947529792786\n","[Training Epoch 3] Batch 1826, Loss 0.39795178174972534\n","[Training Epoch 3] Batch 1827, Loss 0.4104861617088318\n","[Training Epoch 3] Batch 1828, Loss 0.418136328458786\n","[Training Epoch 3] Batch 1829, Loss 0.398248553276062\n","[Training Epoch 3] Batch 1830, Loss 0.4140603542327881\n","[Training Epoch 3] Batch 1831, Loss 0.4100356996059418\n","[Training Epoch 3] Batch 1832, Loss 0.4206289052963257\n","[Training Epoch 3] Batch 1833, Loss 0.39414891600608826\n","[Training Epoch 3] Batch 1834, Loss 0.409793496131897\n","[Training Epoch 3] Batch 1835, Loss 0.38001036643981934\n","[Training Epoch 3] Batch 1836, Loss 0.38446885347366333\n","[Training Epoch 3] Batch 1837, Loss 0.390910804271698\n","[Training Epoch 3] Batch 1838, Loss 0.40372776985168457\n","[Training Epoch 3] Batch 1839, Loss 0.3798900842666626\n","[Training Epoch 3] Batch 1840, Loss 0.40757372975349426\n","[Training Epoch 3] Batch 1841, Loss 0.40251046419143677\n","[Training Epoch 3] Batch 1842, Loss 0.4119238257408142\n","[Training Epoch 3] Batch 1843, Loss 0.39188310503959656\n","[Training Epoch 3] Batch 1844, Loss 0.40819910168647766\n","[Training Epoch 3] Batch 1845, Loss 0.360897421836853\n","[Training Epoch 3] Batch 1846, Loss 0.39588719606399536\n","[Training Epoch 3] Batch 1847, Loss 0.44036781787872314\n","[Training Epoch 3] Batch 1848, Loss 0.4342260956764221\n","[Training Epoch 3] Batch 1849, Loss 0.428946852684021\n","[Training Epoch 3] Batch 1850, Loss 0.37908056378364563\n","[Training Epoch 3] Batch 1851, Loss 0.40327775478363037\n","[Training Epoch 3] Batch 1852, Loss 0.40687066316604614\n","[Training Epoch 3] Batch 1853, Loss 0.37120988965034485\n","[Training Epoch 3] Batch 1854, Loss 0.459294855594635\n","[Training Epoch 3] Batch 1855, Loss 0.3805546164512634\n","[Training Epoch 3] Batch 1856, Loss 0.38862913846969604\n","[Training Epoch 3] Batch 1857, Loss 0.39148828387260437\n","[Training Epoch 3] Batch 1858, Loss 0.41281116008758545\n","[Training Epoch 3] Batch 1859, Loss 0.3843100666999817\n","[Training Epoch 3] Batch 1860, Loss 0.3879736065864563\n","[Training Epoch 3] Batch 1861, Loss 0.4029034674167633\n","[Training Epoch 3] Batch 1862, Loss 0.4013437032699585\n","[Training Epoch 3] Batch 1863, Loss 0.431956946849823\n","[Training Epoch 3] Batch 1864, Loss 0.3933541774749756\n","[Training Epoch 3] Batch 1865, Loss 0.40134531259536743\n","[Training Epoch 3] Batch 1866, Loss 0.3991278409957886\n","[Training Epoch 3] Batch 1867, Loss 0.4117107391357422\n","[Training Epoch 3] Batch 1868, Loss 0.40909600257873535\n","[Training Epoch 3] Batch 1869, Loss 0.41123688220977783\n","[Training Epoch 3] Batch 1870, Loss 0.37817126512527466\n","[Training Epoch 3] Batch 1871, Loss 0.4153486490249634\n","[Training Epoch 3] Batch 1872, Loss 0.4330882430076599\n","[Training Epoch 3] Batch 1873, Loss 0.4209376871585846\n","[Training Epoch 3] Batch 1874, Loss 0.42423105239868164\n","[Training Epoch 3] Batch 1875, Loss 0.41769659519195557\n","[Training Epoch 3] Batch 1876, Loss 0.4223509728908539\n","[Training Epoch 3] Batch 1877, Loss 0.4039740562438965\n","[Training Epoch 3] Batch 1878, Loss 0.39545634388923645\n","[Training Epoch 3] Batch 1879, Loss 0.38870522379875183\n","[Training Epoch 3] Batch 1880, Loss 0.41878002882003784\n","[Training Epoch 3] Batch 1881, Loss 0.3860657513141632\n","[Training Epoch 3] Batch 1882, Loss 0.39666441082954407\n","[Training Epoch 3] Batch 1883, Loss 0.3977474570274353\n","[Training Epoch 3] Batch 1884, Loss 0.4180123805999756\n","[Training Epoch 3] Batch 1885, Loss 0.408336877822876\n","[Training Epoch 3] Batch 1886, Loss 0.40675777196884155\n","[Training Epoch 3] Batch 1887, Loss 0.4057185649871826\n","[Training Epoch 3] Batch 1888, Loss 0.4203560948371887\n","[Training Epoch 3] Batch 1889, Loss 0.4129575192928314\n","[Training Epoch 3] Batch 1890, Loss 0.40793219208717346\n","[Training Epoch 3] Batch 1891, Loss 0.44652920961380005\n","[Training Epoch 3] Batch 1892, Loss 0.40177851915359497\n","[Training Epoch 3] Batch 1893, Loss 0.40635693073272705\n","[Training Epoch 3] Batch 1894, Loss 0.38308197259902954\n","[Training Epoch 3] Batch 1895, Loss 0.4161056578159332\n","[Training Epoch 3] Batch 1896, Loss 0.40300798416137695\n","[Training Epoch 3] Batch 1897, Loss 0.3763464689254761\n","[Training Epoch 3] Batch 1898, Loss 0.3950846195220947\n","[Training Epoch 3] Batch 1899, Loss 0.40597400069236755\n","[Training Epoch 3] Batch 1900, Loss 0.3877241909503937\n","[Training Epoch 3] Batch 1901, Loss 0.41737598180770874\n","[Training Epoch 3] Batch 1902, Loss 0.3888441324234009\n","[Training Epoch 3] Batch 1903, Loss 0.3935045599937439\n","[Training Epoch 3] Batch 1904, Loss 0.4030153155326843\n","[Training Epoch 3] Batch 1905, Loss 0.3946917653083801\n","[Training Epoch 3] Batch 1906, Loss 0.37256741523742676\n","[Training Epoch 3] Batch 1907, Loss 0.39351433515548706\n","[Training Epoch 3] Batch 1908, Loss 0.3690802752971649\n","[Training Epoch 3] Batch 1909, Loss 0.42052820324897766\n","[Training Epoch 3] Batch 1910, Loss 0.41011136770248413\n","[Training Epoch 3] Batch 1911, Loss 0.37933388352394104\n","[Training Epoch 3] Batch 1912, Loss 0.3904761075973511\n","[Training Epoch 3] Batch 1913, Loss 0.4008077383041382\n","[Training Epoch 3] Batch 1914, Loss 0.4103878140449524\n","[Training Epoch 3] Batch 1915, Loss 0.4375980794429779\n","[Training Epoch 3] Batch 1916, Loss 0.4053952395915985\n","[Training Epoch 3] Batch 1917, Loss 0.4408957362174988\n","[Training Epoch 3] Batch 1918, Loss 0.3972405791282654\n","[Training Epoch 3] Batch 1919, Loss 0.40287095308303833\n","[Training Epoch 3] Batch 1920, Loss 0.4234638214111328\n","[Training Epoch 3] Batch 1921, Loss 0.3994017243385315\n","[Training Epoch 3] Batch 1922, Loss 0.42018309235572815\n","[Training Epoch 3] Batch 1923, Loss 0.38345590233802795\n","[Training Epoch 3] Batch 1924, Loss 0.41830191016197205\n","[Training Epoch 3] Batch 1925, Loss 0.40802037715911865\n","[Training Epoch 3] Batch 1926, Loss 0.42196547985076904\n","[Training Epoch 3] Batch 1927, Loss 0.420230507850647\n","[Training Epoch 3] Batch 1928, Loss 0.3799933195114136\n","[Training Epoch 3] Batch 1929, Loss 0.438862144947052\n","[Training Epoch 3] Batch 1930, Loss 0.38225507736206055\n","[Training Epoch 3] Batch 1931, Loss 0.4311929941177368\n","[Training Epoch 3] Batch 1932, Loss 0.4093070924282074\n","[Training Epoch 3] Batch 1933, Loss 0.3892694413661957\n","[Training Epoch 3] Batch 1934, Loss 0.39097344875335693\n","[Training Epoch 3] Batch 1935, Loss 0.38372451066970825\n","[Training Epoch 3] Batch 1936, Loss 0.41752904653549194\n","[Training Epoch 3] Batch 1937, Loss 0.3757915496826172\n","[Training Epoch 3] Batch 1938, Loss 0.4110316038131714\n","[Training Epoch 3] Batch 1939, Loss 0.37228718400001526\n","[Training Epoch 3] Batch 1940, Loss 0.394140362739563\n","[Training Epoch 3] Batch 1941, Loss 0.41955965757369995\n","[Training Epoch 3] Batch 1942, Loss 0.39792686700820923\n","[Training Epoch 3] Batch 1943, Loss 0.42421817779541016\n","[Training Epoch 3] Batch 1944, Loss 0.42072224617004395\n","[Training Epoch 3] Batch 1945, Loss 0.4301527142524719\n","[Training Epoch 3] Batch 1946, Loss 0.4049953818321228\n","[Training Epoch 3] Batch 1947, Loss 0.40607911348342896\n","[Training Epoch 3] Batch 1948, Loss 0.36891669034957886\n","[Training Epoch 3] Batch 1949, Loss 0.3849610388278961\n","[Training Epoch 3] Batch 1950, Loss 0.413201242685318\n","[Training Epoch 3] Batch 1951, Loss 0.4101909399032593\n","[Training Epoch 3] Batch 1952, Loss 0.4191592335700989\n","[Training Epoch 3] Batch 1953, Loss 0.4068395495414734\n","[Training Epoch 3] Batch 1954, Loss 0.38300520181655884\n","[Training Epoch 3] Batch 1955, Loss 0.4215521514415741\n","[Training Epoch 3] Batch 1956, Loss 0.37597137689590454\n","[Training Epoch 3] Batch 1957, Loss 0.43929773569107056\n","[Training Epoch 3] Batch 1958, Loss 0.38125258684158325\n","[Training Epoch 3] Batch 1959, Loss 0.39725321531295776\n","[Training Epoch 3] Batch 1960, Loss 0.39014264941215515\n","[Training Epoch 3] Batch 1961, Loss 0.4178711175918579\n","[Training Epoch 3] Batch 1962, Loss 0.408727765083313\n","[Training Epoch 3] Batch 1963, Loss 0.42215079069137573\n","[Training Epoch 3] Batch 1964, Loss 0.39069485664367676\n","[Training Epoch 3] Batch 1965, Loss 0.38592296838760376\n","[Training Epoch 3] Batch 1966, Loss 0.42730745673179626\n","[Training Epoch 3] Batch 1967, Loss 0.4032202959060669\n","[Training Epoch 3] Batch 1968, Loss 0.4319949150085449\n","[Training Epoch 3] Batch 1969, Loss 0.4090096354484558\n","[Training Epoch 3] Batch 1970, Loss 0.40139949321746826\n","[Training Epoch 3] Batch 1971, Loss 0.41279181838035583\n","[Training Epoch 3] Batch 1972, Loss 0.43801814317703247\n","[Training Epoch 3] Batch 1973, Loss 0.3829004466533661\n","[Training Epoch 3] Batch 1974, Loss 0.40207844972610474\n","[Training Epoch 3] Batch 1975, Loss 0.42525219917297363\n","[Training Epoch 3] Batch 1976, Loss 0.4001895785331726\n","[Training Epoch 3] Batch 1977, Loss 0.43596792221069336\n","[Training Epoch 3] Batch 1978, Loss 0.40839144587516785\n","[Training Epoch 3] Batch 1979, Loss 0.41277575492858887\n","[Training Epoch 3] Batch 1980, Loss 0.421670138835907\n","[Training Epoch 3] Batch 1981, Loss 0.4005257487297058\n","[Training Epoch 3] Batch 1982, Loss 0.4166138172149658\n","[Training Epoch 3] Batch 1983, Loss 0.40639424324035645\n","[Training Epoch 3] Batch 1984, Loss 0.3708133101463318\n","[Training Epoch 3] Batch 1985, Loss 0.37870901823043823\n","[Training Epoch 3] Batch 1986, Loss 0.41890662908554077\n","[Training Epoch 3] Batch 1987, Loss 0.4277452230453491\n","[Training Epoch 3] Batch 1988, Loss 0.41588717699050903\n","[Training Epoch 3] Batch 1989, Loss 0.39894184470176697\n","[Training Epoch 3] Batch 1990, Loss 0.37473928928375244\n","[Training Epoch 3] Batch 1991, Loss 0.38296353816986084\n","[Training Epoch 3] Batch 1992, Loss 0.4158417880535126\n","[Training Epoch 3] Batch 1993, Loss 0.37970560789108276\n","[Training Epoch 3] Batch 1994, Loss 0.4192012548446655\n","[Training Epoch 3] Batch 1995, Loss 0.4041334390640259\n","[Training Epoch 3] Batch 1996, Loss 0.4169769287109375\n","[Training Epoch 3] Batch 1997, Loss 0.4098150432109833\n","[Training Epoch 3] Batch 1998, Loss 0.38814157247543335\n","[Training Epoch 3] Batch 1999, Loss 0.430946946144104\n","[Training Epoch 3] Batch 2000, Loss 0.38971835374832153\n","[Training Epoch 3] Batch 2001, Loss 0.37577009201049805\n","[Training Epoch 3] Batch 2002, Loss 0.41326212882995605\n","[Training Epoch 3] Batch 2003, Loss 0.407551646232605\n","[Training Epoch 3] Batch 2004, Loss 0.393037885427475\n","[Training Epoch 3] Batch 2005, Loss 0.3904235363006592\n","[Training Epoch 3] Batch 2006, Loss 0.4106987714767456\n","[Training Epoch 3] Batch 2007, Loss 0.41761142015457153\n","[Training Epoch 3] Batch 2008, Loss 0.41317039728164673\n","[Training Epoch 3] Batch 2009, Loss 0.43974798917770386\n","[Training Epoch 3] Batch 2010, Loss 0.39756089448928833\n","[Training Epoch 3] Batch 2011, Loss 0.4049188792705536\n","[Training Epoch 3] Batch 2012, Loss 0.4266538619995117\n","[Training Epoch 3] Batch 2013, Loss 0.38336873054504395\n","[Training Epoch 3] Batch 2014, Loss 0.40835148096084595\n","[Training Epoch 3] Batch 2015, Loss 0.3967980742454529\n","[Training Epoch 3] Batch 2016, Loss 0.4091414511203766\n","[Training Epoch 3] Batch 2017, Loss 0.4015049338340759\n","[Training Epoch 3] Batch 2018, Loss 0.4243365526199341\n","[Training Epoch 3] Batch 2019, Loss 0.4146179258823395\n","[Training Epoch 3] Batch 2020, Loss 0.40298426151275635\n","[Training Epoch 3] Batch 2021, Loss 0.3885282278060913\n","[Training Epoch 3] Batch 2022, Loss 0.40810781717300415\n","[Training Epoch 3] Batch 2023, Loss 0.38770195841789246\n","[Training Epoch 3] Batch 2024, Loss 0.41413870453834534\n","[Training Epoch 3] Batch 2025, Loss 0.4145759642124176\n","[Training Epoch 3] Batch 2026, Loss 0.36321133375167847\n","[Training Epoch 3] Batch 2027, Loss 0.4005184769630432\n","[Training Epoch 3] Batch 2028, Loss 0.4329847991466522\n","[Training Epoch 3] Batch 2029, Loss 0.42217135429382324\n","[Training Epoch 3] Batch 2030, Loss 0.42153385281562805\n","[Training Epoch 3] Batch 2031, Loss 0.40781649947166443\n","[Training Epoch 3] Batch 2032, Loss 0.3812428116798401\n","[Training Epoch 3] Batch 2033, Loss 0.3920273184776306\n","[Training Epoch 3] Batch 2034, Loss 0.40770813822746277\n","[Training Epoch 3] Batch 2035, Loss 0.40375393629074097\n","[Training Epoch 3] Batch 2036, Loss 0.42272287607192993\n","[Training Epoch 3] Batch 2037, Loss 0.4145039916038513\n","[Training Epoch 3] Batch 2038, Loss 0.43329888582229614\n","[Training Epoch 3] Batch 2039, Loss 0.3958837687969208\n","[Training Epoch 3] Batch 2040, Loss 0.4187399744987488\n","[Training Epoch 3] Batch 2041, Loss 0.4342304766178131\n","[Training Epoch 3] Batch 2042, Loss 0.3994980454444885\n","[Training Epoch 3] Batch 2043, Loss 0.4020576477050781\n","[Training Epoch 3] Batch 2044, Loss 0.3860923945903778\n","[Training Epoch 3] Batch 2045, Loss 0.3922150731086731\n","[Training Epoch 3] Batch 2046, Loss 0.38792118430137634\n","[Training Epoch 3] Batch 2047, Loss 0.4032425284385681\n","[Training Epoch 3] Batch 2048, Loss 0.4073928892612457\n","[Training Epoch 3] Batch 2049, Loss 0.4184543192386627\n","[Training Epoch 3] Batch 2050, Loss 0.42151689529418945\n","[Training Epoch 3] Batch 2051, Loss 0.408825159072876\n","[Training Epoch 3] Batch 2052, Loss 0.4132125973701477\n","[Training Epoch 3] Batch 2053, Loss 0.38087108731269836\n","[Training Epoch 3] Batch 2054, Loss 0.3832092881202698\n","[Training Epoch 3] Batch 2055, Loss 0.453818142414093\n","[Training Epoch 3] Batch 2056, Loss 0.4020889103412628\n","[Training Epoch 3] Batch 2057, Loss 0.4388618469238281\n","[Training Epoch 3] Batch 2058, Loss 0.4192523956298828\n","[Training Epoch 3] Batch 2059, Loss 0.4079166650772095\n","[Training Epoch 3] Batch 2060, Loss 0.39636775851249695\n","[Training Epoch 3] Batch 2061, Loss 0.40378355979919434\n","[Training Epoch 3] Batch 2062, Loss 0.400848388671875\n","[Training Epoch 3] Batch 2063, Loss 0.41136085987091064\n","[Training Epoch 3] Batch 2064, Loss 0.42942535877227783\n","[Training Epoch 3] Batch 2065, Loss 0.4064139723777771\n","[Training Epoch 3] Batch 2066, Loss 0.40892264246940613\n","[Training Epoch 3] Batch 2067, Loss 0.41024690866470337\n","[Training Epoch 3] Batch 2068, Loss 0.40476590394973755\n","[Training Epoch 3] Batch 2069, Loss 0.4028550982475281\n","[Training Epoch 3] Batch 2070, Loss 0.3784613013267517\n","[Training Epoch 3] Batch 2071, Loss 0.4069194495677948\n","[Training Epoch 3] Batch 2072, Loss 0.40075767040252686\n","[Training Epoch 3] Batch 2073, Loss 0.43729448318481445\n","[Training Epoch 3] Batch 2074, Loss 0.43208634853363037\n","[Training Epoch 3] Batch 2075, Loss 0.42492809891700745\n","[Training Epoch 3] Batch 2076, Loss 0.3908708691596985\n","[Training Epoch 3] Batch 2077, Loss 0.39558929204940796\n","[Training Epoch 3] Batch 2078, Loss 0.3822348713874817\n","[Training Epoch 3] Batch 2079, Loss 0.39985233545303345\n","[Training Epoch 3] Batch 2080, Loss 0.3664984107017517\n","[Training Epoch 3] Batch 2081, Loss 0.38139617443084717\n","[Training Epoch 3] Batch 2082, Loss 0.40503573417663574\n","[Training Epoch 3] Batch 2083, Loss 0.4053313434123993\n","[Training Epoch 3] Batch 2084, Loss 0.4136669635772705\n","[Training Epoch 3] Batch 2085, Loss 0.42715853452682495\n","[Training Epoch 3] Batch 2086, Loss 0.3829149603843689\n","[Training Epoch 3] Batch 2087, Loss 0.43378084897994995\n","[Training Epoch 3] Batch 2088, Loss 0.3645171821117401\n","[Training Epoch 3] Batch 2089, Loss 0.4040932357311249\n","[Training Epoch 3] Batch 2090, Loss 0.4364113211631775\n","[Training Epoch 3] Batch 2091, Loss 0.39096128940582275\n","[Training Epoch 3] Batch 2092, Loss 0.4377879798412323\n","[Training Epoch 3] Batch 2093, Loss 0.4190610349178314\n","[Training Epoch 3] Batch 2094, Loss 0.39839833974838257\n","[Training Epoch 3] Batch 2095, Loss 0.3815124034881592\n","[Training Epoch 3] Batch 2096, Loss 0.4107016324996948\n","[Training Epoch 3] Batch 2097, Loss 0.3783673942089081\n","[Training Epoch 3] Batch 2098, Loss 0.4123838245868683\n","[Training Epoch 3] Batch 2099, Loss 0.40363919734954834\n","[Training Epoch 3] Batch 2100, Loss 0.40997931361198425\n","[Training Epoch 3] Batch 2101, Loss 0.3942744731903076\n","[Training Epoch 3] Batch 2102, Loss 0.4052450656890869\n","[Training Epoch 3] Batch 2103, Loss 0.39513006806373596\n","[Training Epoch 3] Batch 2104, Loss 0.3998429775238037\n","[Training Epoch 3] Batch 2105, Loss 0.3917379677295685\n","[Training Epoch 3] Batch 2106, Loss 0.41160905361175537\n","[Training Epoch 3] Batch 2107, Loss 0.38183578848838806\n","[Training Epoch 3] Batch 2108, Loss 0.4092745780944824\n","[Training Epoch 3] Batch 2109, Loss 0.40181052684783936\n","[Training Epoch 3] Batch 2110, Loss 0.417936772108078\n","[Training Epoch 3] Batch 2111, Loss 0.40447142720222473\n","[Training Epoch 3] Batch 2112, Loss 0.4044715464115143\n","[Training Epoch 3] Batch 2113, Loss 0.39134645462036133\n","[Training Epoch 3] Batch 2114, Loss 0.3956732153892517\n","[Training Epoch 3] Batch 2115, Loss 0.42744094133377075\n","[Training Epoch 3] Batch 2116, Loss 0.3774119019508362\n","[Training Epoch 3] Batch 2117, Loss 0.4238138496875763\n","[Training Epoch 3] Batch 2118, Loss 0.417031854391098\n","[Training Epoch 3] Batch 2119, Loss 0.42346736788749695\n","[Training Epoch 3] Batch 2120, Loss 0.39252397418022156\n","[Training Epoch 3] Batch 2121, Loss 0.416629821062088\n","[Training Epoch 3] Batch 2122, Loss 0.4252810478210449\n","[Training Epoch 3] Batch 2123, Loss 0.4064767360687256\n","[Training Epoch 3] Batch 2124, Loss 0.3681964874267578\n","[Training Epoch 3] Batch 2125, Loss 0.41257572174072266\n","[Training Epoch 3] Batch 2126, Loss 0.37634193897247314\n","[Training Epoch 3] Batch 2127, Loss 0.4156729578971863\n","[Training Epoch 3] Batch 2128, Loss 0.41825205087661743\n","[Training Epoch 3] Batch 2129, Loss 0.41347095370292664\n","[Training Epoch 3] Batch 2130, Loss 0.4030676484107971\n","[Training Epoch 3] Batch 2131, Loss 0.4341956377029419\n","[Training Epoch 3] Batch 2132, Loss 0.40383702516555786\n","[Training Epoch 3] Batch 2133, Loss 0.40021777153015137\n","[Training Epoch 3] Batch 2134, Loss 0.40235447883605957\n","[Training Epoch 3] Batch 2135, Loss 0.3937293291091919\n","[Training Epoch 3] Batch 2136, Loss 0.3963407874107361\n","[Training Epoch 3] Batch 2137, Loss 0.4134131073951721\n","[Training Epoch 3] Batch 2138, Loss 0.44522398710250854\n","[Training Epoch 3] Batch 2139, Loss 0.4118213951587677\n","[Training Epoch 3] Batch 2140, Loss 0.40977323055267334\n","[Training Epoch 3] Batch 2141, Loss 0.399181604385376\n","[Training Epoch 3] Batch 2142, Loss 0.42138662934303284\n","[Training Epoch 3] Batch 2143, Loss 0.4068892002105713\n","[Training Epoch 3] Batch 2144, Loss 0.4181641936302185\n","[Training Epoch 3] Batch 2145, Loss 0.38468796014785767\n","[Training Epoch 3] Batch 2146, Loss 0.39526045322418213\n","[Training Epoch 3] Batch 2147, Loss 0.40240591764450073\n","[Training Epoch 3] Batch 2148, Loss 0.4324721097946167\n","[Training Epoch 3] Batch 2149, Loss 0.38112473487854004\n","[Training Epoch 3] Batch 2150, Loss 0.3879348039627075\n","[Training Epoch 3] Batch 2151, Loss 0.42853114008903503\n","[Training Epoch 3] Batch 2152, Loss 0.40206390619277954\n","[Training Epoch 3] Batch 2153, Loss 0.39031654596328735\n","[Training Epoch 3] Batch 2154, Loss 0.3962523341178894\n","[Training Epoch 3] Batch 2155, Loss 0.4059690833091736\n","[Training Epoch 3] Batch 2156, Loss 0.3872605264186859\n","[Training Epoch 3] Batch 2157, Loss 0.3967113792896271\n","[Training Epoch 3] Batch 2158, Loss 0.43051302433013916\n","[Training Epoch 3] Batch 2159, Loss 0.3793598413467407\n","[Training Epoch 3] Batch 2160, Loss 0.4000217914581299\n","[Training Epoch 3] Batch 2161, Loss 0.3930593430995941\n","[Training Epoch 3] Batch 2162, Loss 0.43413984775543213\n","[Training Epoch 3] Batch 2163, Loss 0.40073472261428833\n","[Training Epoch 3] Batch 2164, Loss 0.3988019824028015\n","[Training Epoch 3] Batch 2165, Loss 0.4446597099304199\n","[Training Epoch 3] Batch 2166, Loss 0.3789597153663635\n","[Training Epoch 3] Batch 2167, Loss 0.4032444655895233\n","[Training Epoch 3] Batch 2168, Loss 0.41077545285224915\n","[Training Epoch 3] Batch 2169, Loss 0.4153137803077698\n","[Training Epoch 3] Batch 2170, Loss 0.41938748955726624\n","[Training Epoch 3] Batch 2171, Loss 0.39583489298820496\n","[Training Epoch 3] Batch 2172, Loss 0.3941460847854614\n","[Training Epoch 3] Batch 2173, Loss 0.4286090135574341\n","[Training Epoch 3] Batch 2174, Loss 0.38171377778053284\n","[Training Epoch 3] Batch 2175, Loss 0.4183600842952728\n","[Training Epoch 3] Batch 2176, Loss 0.41004014015197754\n","[Training Epoch 3] Batch 2177, Loss 0.37314438819885254\n","[Training Epoch 3] Batch 2178, Loss 0.4188689589500427\n","[Training Epoch 3] Batch 2179, Loss 0.3907586336135864\n","[Training Epoch 3] Batch 2180, Loss 0.41223016381263733\n","[Training Epoch 3] Batch 2181, Loss 0.4297347366809845\n","[Training Epoch 3] Batch 2182, Loss 0.3988579511642456\n","[Training Epoch 3] Batch 2183, Loss 0.4225093126296997\n","[Training Epoch 3] Batch 2184, Loss 0.42326998710632324\n","[Training Epoch 3] Batch 2185, Loss 0.40014082193374634\n","[Training Epoch 3] Batch 2186, Loss 0.41043388843536377\n","[Training Epoch 3] Batch 2187, Loss 0.39094334840774536\n","[Training Epoch 3] Batch 2188, Loss 0.41558516025543213\n","[Training Epoch 3] Batch 2189, Loss 0.3894651532173157\n","[Training Epoch 3] Batch 2190, Loss 0.4049127995967865\n","[Training Epoch 3] Batch 2191, Loss 0.37617817521095276\n","[Training Epoch 3] Batch 2192, Loss 0.4023873209953308\n","[Training Epoch 3] Batch 2193, Loss 0.42277300357818604\n","[Training Epoch 3] Batch 2194, Loss 0.4093421697616577\n","[Training Epoch 3] Batch 2195, Loss 0.38066866993904114\n","[Training Epoch 3] Batch 2196, Loss 0.4275321960449219\n","[Training Epoch 3] Batch 2197, Loss 0.3900917172431946\n","[Training Epoch 3] Batch 2198, Loss 0.4051095247268677\n","[Training Epoch 3] Batch 2199, Loss 0.42658472061157227\n","[Training Epoch 3] Batch 2200, Loss 0.4046097993850708\n","[Training Epoch 3] Batch 2201, Loss 0.3753443956375122\n","[Training Epoch 3] Batch 2202, Loss 0.40952807664871216\n","[Training Epoch 3] Batch 2203, Loss 0.4147202968597412\n","[Training Epoch 3] Batch 2204, Loss 0.4195481836795807\n","[Training Epoch 3] Batch 2205, Loss 0.4047250747680664\n","[Training Epoch 3] Batch 2206, Loss 0.4030047655105591\n","[Training Epoch 3] Batch 2207, Loss 0.4025039076805115\n","[Training Epoch 3] Batch 2208, Loss 0.4167397916316986\n","[Training Epoch 3] Batch 2209, Loss 0.41811612248420715\n","[Training Epoch 3] Batch 2210, Loss 0.3877529799938202\n","[Training Epoch 3] Batch 2211, Loss 0.4165472090244293\n","[Training Epoch 3] Batch 2212, Loss 0.40953129529953003\n","[Training Epoch 3] Batch 2213, Loss 0.4184536635875702\n","[Training Epoch 3] Batch 2214, Loss 0.39547690749168396\n","[Training Epoch 3] Batch 2215, Loss 0.4136422872543335\n","[Training Epoch 3] Batch 2216, Loss 0.39584437012672424\n","[Training Epoch 3] Batch 2217, Loss 0.38170236349105835\n","[Training Epoch 3] Batch 2218, Loss 0.39723503589630127\n","[Training Epoch 3] Batch 2219, Loss 0.414594441652298\n","[Training Epoch 3] Batch 2220, Loss 0.3937208950519562\n","[Training Epoch 3] Batch 2221, Loss 0.4051545262336731\n","[Training Epoch 3] Batch 2222, Loss 0.4100658893585205\n","[Training Epoch 3] Batch 2223, Loss 0.36218005418777466\n","[Training Epoch 3] Batch 2224, Loss 0.4286608397960663\n","[Training Epoch 3] Batch 2225, Loss 0.3932117223739624\n","[Training Epoch 3] Batch 2226, Loss 0.4013006389141083\n","[Training Epoch 3] Batch 2227, Loss 0.3844449520111084\n","[Training Epoch 3] Batch 2228, Loss 0.34720200300216675\n","[Training Epoch 3] Batch 2229, Loss 0.404802531003952\n","[Training Epoch 3] Batch 2230, Loss 0.4141383171081543\n","[Training Epoch 3] Batch 2231, Loss 0.4195426106452942\n","[Training Epoch 3] Batch 2232, Loss 0.4109388291835785\n","[Training Epoch 3] Batch 2233, Loss 0.38969552516937256\n","[Training Epoch 3] Batch 2234, Loss 0.39126986265182495\n","[Training Epoch 3] Batch 2235, Loss 0.39164942502975464\n","[Training Epoch 3] Batch 2236, Loss 0.4001452922821045\n","[Training Epoch 3] Batch 2237, Loss 0.3973265588283539\n","[Training Epoch 3] Batch 2238, Loss 0.42510122060775757\n","[Training Epoch 3] Batch 2239, Loss 0.4115222990512848\n","[Training Epoch 3] Batch 2240, Loss 0.4183032512664795\n","[Training Epoch 3] Batch 2241, Loss 0.42637842893600464\n","[Training Epoch 3] Batch 2242, Loss 0.3916783928871155\n","[Training Epoch 3] Batch 2243, Loss 0.4125705361366272\n","[Training Epoch 3] Batch 2244, Loss 0.44749316573143005\n","[Training Epoch 3] Batch 2245, Loss 0.3546903133392334\n","[Training Epoch 3] Batch 2246, Loss 0.4124433994293213\n","[Training Epoch 3] Batch 2247, Loss 0.40584152936935425\n","[Training Epoch 3] Batch 2248, Loss 0.4116183817386627\n","[Training Epoch 3] Batch 2249, Loss 0.39960166811943054\n","[Training Epoch 3] Batch 2250, Loss 0.38855206966400146\n","[Training Epoch 3] Batch 2251, Loss 0.38332730531692505\n","[Training Epoch 3] Batch 2252, Loss 0.40533626079559326\n","[Training Epoch 3] Batch 2253, Loss 0.4071025848388672\n","[Training Epoch 3] Batch 2254, Loss 0.4155591130256653\n","[Training Epoch 3] Batch 2255, Loss 0.40310198068618774\n","[Training Epoch 3] Batch 2256, Loss 0.420845091342926\n","[Training Epoch 3] Batch 2257, Loss 0.4411807656288147\n","[Training Epoch 3] Batch 2258, Loss 0.42841804027557373\n","[Training Epoch 3] Batch 2259, Loss 0.4161522388458252\n","[Training Epoch 3] Batch 2260, Loss 0.40037834644317627\n","[Training Epoch 3] Batch 2261, Loss 0.3911574184894562\n","[Training Epoch 3] Batch 2262, Loss 0.3987809121608734\n","[Training Epoch 3] Batch 2263, Loss 0.40695175528526306\n","[Training Epoch 3] Batch 2264, Loss 0.4183131456375122\n","[Training Epoch 3] Batch 2265, Loss 0.4020429849624634\n","[Training Epoch 3] Batch 2266, Loss 0.3793666362762451\n","[Training Epoch 3] Batch 2267, Loss 0.39536604285240173\n","[Training Epoch 3] Batch 2268, Loss 0.3824968934059143\n","[Training Epoch 3] Batch 2269, Loss 0.4081055819988251\n","[Training Epoch 3] Batch 2270, Loss 0.3842109739780426\n","[Training Epoch 3] Batch 2271, Loss 0.3588276505470276\n","[Training Epoch 3] Batch 2272, Loss 0.4262051582336426\n","[Training Epoch 3] Batch 2273, Loss 0.38031476736068726\n","[Training Epoch 3] Batch 2274, Loss 0.3883500397205353\n","[Training Epoch 3] Batch 2275, Loss 0.3715716600418091\n","[Training Epoch 3] Batch 2276, Loss 0.4149412512779236\n","[Training Epoch 3] Batch 2277, Loss 0.40738415718078613\n","[Training Epoch 3] Batch 2278, Loss 0.4169793426990509\n","[Training Epoch 3] Batch 2279, Loss 0.3912227153778076\n","[Training Epoch 3] Batch 2280, Loss 0.41472679376602173\n","[Training Epoch 3] Batch 2281, Loss 0.3550047278404236\n","[Training Epoch 3] Batch 2282, Loss 0.44673264026641846\n","[Training Epoch 3] Batch 2283, Loss 0.3879494071006775\n","[Training Epoch 3] Batch 2284, Loss 0.40096724033355713\n","[Training Epoch 3] Batch 2285, Loss 0.41447150707244873\n","[Training Epoch 3] Batch 2286, Loss 0.4168162941932678\n","[Training Epoch 3] Batch 2287, Loss 0.38900813460350037\n","[Training Epoch 3] Batch 2288, Loss 0.43468010425567627\n","[Training Epoch 3] Batch 2289, Loss 0.39444175362586975\n","[Training Epoch 3] Batch 2290, Loss 0.41348356008529663\n","[Training Epoch 3] Batch 2291, Loss 0.4142843782901764\n","[Training Epoch 3] Batch 2292, Loss 0.42999267578125\n","[Training Epoch 3] Batch 2293, Loss 0.4009043276309967\n","[Training Epoch 3] Batch 2294, Loss 0.42082899808883667\n","[Training Epoch 3] Batch 2295, Loss 0.42758744955062866\n","[Training Epoch 3] Batch 2296, Loss 0.39902907609939575\n","[Training Epoch 3] Batch 2297, Loss 0.3977992534637451\n","[Training Epoch 3] Batch 2298, Loss 0.4112304449081421\n","[Training Epoch 3] Batch 2299, Loss 0.4065462350845337\n","[Training Epoch 3] Batch 2300, Loss 0.3846237063407898\n","[Training Epoch 3] Batch 2301, Loss 0.3974437713623047\n","[Training Epoch 3] Batch 2302, Loss 0.3965799808502197\n","[Training Epoch 3] Batch 2303, Loss 0.4181002974510193\n","[Training Epoch 3] Batch 2304, Loss 0.39497363567352295\n","[Training Epoch 3] Batch 2305, Loss 0.4081096351146698\n","[Training Epoch 3] Batch 2306, Loss 0.3925732970237732\n","[Training Epoch 3] Batch 2307, Loss 0.43254730105400085\n","[Training Epoch 3] Batch 2308, Loss 0.3770011067390442\n","[Training Epoch 3] Batch 2309, Loss 0.3514544367790222\n","[Training Epoch 3] Batch 2310, Loss 0.411535382270813\n","[Training Epoch 3] Batch 2311, Loss 0.40984785556793213\n","[Training Epoch 3] Batch 2312, Loss 0.41155290603637695\n","[Training Epoch 3] Batch 2313, Loss 0.37357306480407715\n","[Training Epoch 3] Batch 2314, Loss 0.4028627276420593\n","[Training Epoch 3] Batch 2315, Loss 0.38437384366989136\n","[Training Epoch 3] Batch 2316, Loss 0.42690977454185486\n","[Training Epoch 3] Batch 2317, Loss 0.4066494107246399\n","[Training Epoch 3] Batch 2318, Loss 0.4194861948490143\n","[Training Epoch 3] Batch 2319, Loss 0.4379432499408722\n","[Training Epoch 3] Batch 2320, Loss 0.3805341124534607\n","[Training Epoch 3] Batch 2321, Loss 0.3966027796268463\n","[Training Epoch 3] Batch 2322, Loss 0.40320292115211487\n","[Training Epoch 3] Batch 2323, Loss 0.3988979756832123\n","[Training Epoch 3] Batch 2324, Loss 0.41540026664733887\n","[Training Epoch 3] Batch 2325, Loss 0.3991672396659851\n","[Training Epoch 3] Batch 2326, Loss 0.4197966456413269\n","[Training Epoch 3] Batch 2327, Loss 0.41460660099983215\n","[Training Epoch 3] Batch 2328, Loss 0.3869178295135498\n","[Training Epoch 3] Batch 2329, Loss 0.43452417850494385\n","[Training Epoch 3] Batch 2330, Loss 0.4458499252796173\n","[Training Epoch 3] Batch 2331, Loss 0.4060439467430115\n","[Training Epoch 3] Batch 2332, Loss 0.3850286602973938\n","[Training Epoch 3] Batch 2333, Loss 0.4057408571243286\n","[Training Epoch 3] Batch 2334, Loss 0.4150846600532532\n","[Training Epoch 3] Batch 2335, Loss 0.4242800772190094\n","[Training Epoch 3] Batch 2336, Loss 0.42631667852401733\n","[Training Epoch 3] Batch 2337, Loss 0.41660642623901367\n","[Training Epoch 3] Batch 2338, Loss 0.41951924562454224\n","[Training Epoch 3] Batch 2339, Loss 0.38932985067367554\n","[Training Epoch 3] Batch 2340, Loss 0.4119583070278168\n","[Training Epoch 3] Batch 2341, Loss 0.3757949769496918\n","[Training Epoch 3] Batch 2342, Loss 0.38670870661735535\n","[Training Epoch 3] Batch 2343, Loss 0.40326595306396484\n","[Training Epoch 3] Batch 2344, Loss 0.422522634267807\n","[Training Epoch 3] Batch 2345, Loss 0.39785826206207275\n","[Training Epoch 3] Batch 2346, Loss 0.4138931632041931\n","[Training Epoch 3] Batch 2347, Loss 0.4221929609775543\n","[Training Epoch 3] Batch 2348, Loss 0.3939390778541565\n","[Training Epoch 3] Batch 2349, Loss 0.39413756132125854\n","[Training Epoch 3] Batch 2350, Loss 0.3905869424343109\n","[Training Epoch 3] Batch 2351, Loss 0.4147665500640869\n","[Training Epoch 3] Batch 2352, Loss 0.4102429151535034\n","[Training Epoch 3] Batch 2353, Loss 0.4096416234970093\n","[Training Epoch 3] Batch 2354, Loss 0.39871323108673096\n","[Training Epoch 3] Batch 2355, Loss 0.4022808074951172\n","[Training Epoch 3] Batch 2356, Loss 0.408092737197876\n","[Training Epoch 3] Batch 2357, Loss 0.3785642981529236\n","[Training Epoch 3] Batch 2358, Loss 0.41389161348342896\n","[Training Epoch 3] Batch 2359, Loss 0.4092254638671875\n","[Training Epoch 3] Batch 2360, Loss 0.3855935335159302\n","[Training Epoch 3] Batch 2361, Loss 0.3959978222846985\n","[Training Epoch 3] Batch 2362, Loss 0.3972025513648987\n","[Training Epoch 3] Batch 2363, Loss 0.4058361053466797\n","[Training Epoch 3] Batch 2364, Loss 0.4082005023956299\n","[Training Epoch 3] Batch 2365, Loss 0.386346697807312\n","[Training Epoch 3] Batch 2366, Loss 0.4005393385887146\n","[Training Epoch 3] Batch 2367, Loss 0.45126888155937195\n","[Training Epoch 3] Batch 2368, Loss 0.3902396559715271\n","[Training Epoch 3] Batch 2369, Loss 0.38852331042289734\n","[Training Epoch 3] Batch 2370, Loss 0.42918968200683594\n","[Training Epoch 3] Batch 2371, Loss 0.4029082655906677\n","[Training Epoch 3] Batch 2372, Loss 0.4022912383079529\n","[Training Epoch 3] Batch 2373, Loss 0.39051496982574463\n","[Training Epoch 3] Batch 2374, Loss 0.39721012115478516\n","[Training Epoch 3] Batch 2375, Loss 0.40452662110328674\n","[Training Epoch 3] Batch 2376, Loss 0.3854566812515259\n","[Training Epoch 3] Batch 2377, Loss 0.3914676606655121\n","[Training Epoch 3] Batch 2378, Loss 0.4187091886997223\n","[Training Epoch 3] Batch 2379, Loss 0.39823490381240845\n","[Training Epoch 3] Batch 2380, Loss 0.3987780809402466\n","[Training Epoch 3] Batch 2381, Loss 0.4344925284385681\n","[Training Epoch 3] Batch 2382, Loss 0.3647039532661438\n","[Training Epoch 3] Batch 2383, Loss 0.3979525566101074\n","[Training Epoch 3] Batch 2384, Loss 0.3717583119869232\n","[Training Epoch 3] Batch 2385, Loss 0.39598548412323\n","[Training Epoch 3] Batch 2386, Loss 0.3892362713813782\n","[Training Epoch 3] Batch 2387, Loss 0.42099660634994507\n","[Training Epoch 3] Batch 2388, Loss 0.393280029296875\n","[Training Epoch 3] Batch 2389, Loss 0.3785020112991333\n","[Training Epoch 3] Batch 2390, Loss 0.402628630399704\n","[Training Epoch 3] Batch 2391, Loss 0.3958264887332916\n","[Training Epoch 3] Batch 2392, Loss 0.4137953817844391\n","[Training Epoch 3] Batch 2393, Loss 0.41750597953796387\n","[Training Epoch 3] Batch 2394, Loss 0.4181782305240631\n","[Training Epoch 3] Batch 2395, Loss 0.3924713134765625\n","[Training Epoch 3] Batch 2396, Loss 0.40060651302337646\n","[Training Epoch 3] Batch 2397, Loss 0.37974080443382263\n","[Training Epoch 3] Batch 2398, Loss 0.4223066568374634\n","[Training Epoch 3] Batch 2399, Loss 0.3724379539489746\n","[Training Epoch 3] Batch 2400, Loss 0.41376546025276184\n","[Training Epoch 3] Batch 2401, Loss 0.41024288535118103\n","[Training Epoch 3] Batch 2402, Loss 0.44149118661880493\n","[Training Epoch 3] Batch 2403, Loss 0.39456355571746826\n","[Training Epoch 3] Batch 2404, Loss 0.3949486017227173\n","[Training Epoch 3] Batch 2405, Loss 0.41134825348854065\n","[Training Epoch 3] Batch 2406, Loss 0.44355547428131104\n","[Training Epoch 3] Batch 2407, Loss 0.38902899622917175\n","[Training Epoch 3] Batch 2408, Loss 0.3863401710987091\n","[Training Epoch 3] Batch 2409, Loss 0.3851432800292969\n","[Training Epoch 3] Batch 2410, Loss 0.40156763792037964\n","[Training Epoch 3] Batch 2411, Loss 0.4053739905357361\n","[Training Epoch 3] Batch 2412, Loss 0.4089977443218231\n","[Training Epoch 3] Batch 2413, Loss 0.36325040459632874\n","[Training Epoch 3] Batch 2414, Loss 0.4115545451641083\n","[Training Epoch 3] Batch 2415, Loss 0.4208106994628906\n","[Training Epoch 3] Batch 2416, Loss 0.40337705612182617\n","[Training Epoch 3] Batch 2417, Loss 0.4222152531147003\n","[Training Epoch 3] Batch 2418, Loss 0.40541279315948486\n","[Training Epoch 3] Batch 2419, Loss 0.41156506538391113\n","[Training Epoch 3] Batch 2420, Loss 0.3933073580265045\n","[Training Epoch 3] Batch 2421, Loss 0.41066521406173706\n","[Training Epoch 3] Batch 2422, Loss 0.390196293592453\n","[Training Epoch 3] Batch 2423, Loss 0.4099641740322113\n","[Training Epoch 3] Batch 2424, Loss 0.391658216714859\n","[Training Epoch 3] Batch 2425, Loss 0.3930971026420593\n","[Training Epoch 3] Batch 2426, Loss 0.41717517375946045\n","[Training Epoch 3] Batch 2427, Loss 0.3954309821128845\n","[Training Epoch 3] Batch 2428, Loss 0.3977380692958832\n","[Training Epoch 3] Batch 2429, Loss 0.38729894161224365\n","[Training Epoch 3] Batch 2430, Loss 0.3966735899448395\n","[Training Epoch 3] Batch 2431, Loss 0.4148111343383789\n","[Training Epoch 3] Batch 2432, Loss 0.41560518741607666\n","[Training Epoch 3] Batch 2433, Loss 0.4154798984527588\n","[Training Epoch 3] Batch 2434, Loss 0.42404311895370483\n","[Training Epoch 3] Batch 2435, Loss 0.40027669072151184\n","[Training Epoch 3] Batch 2436, Loss 0.39626985788345337\n","[Training Epoch 3] Batch 2437, Loss 0.3855041563510895\n","[Training Epoch 3] Batch 2438, Loss 0.3865485191345215\n","[Training Epoch 3] Batch 2439, Loss 0.3849905729293823\n","[Training Epoch 3] Batch 2440, Loss 0.3648556172847748\n","[Training Epoch 3] Batch 2441, Loss 0.39177006483078003\n","[Training Epoch 3] Batch 2442, Loss 0.3709077835083008\n","[Training Epoch 3] Batch 2443, Loss 0.40284600853919983\n","[Training Epoch 3] Batch 2444, Loss 0.3928249776363373\n","[Training Epoch 3] Batch 2445, Loss 0.41300079226493835\n","[Training Epoch 3] Batch 2446, Loss 0.42929619550704956\n","[Training Epoch 3] Batch 2447, Loss 0.35838523507118225\n","[Training Epoch 3] Batch 2448, Loss 0.3826667070388794\n","[Training Epoch 3] Batch 2449, Loss 0.410902202129364\n","[Training Epoch 3] Batch 2450, Loss 0.3744993805885315\n","[Training Epoch 3] Batch 2451, Loss 0.3895145058631897\n","[Training Epoch 3] Batch 2452, Loss 0.3878215253353119\n","[Training Epoch 3] Batch 2453, Loss 0.3926059305667877\n","[Training Epoch 3] Batch 2454, Loss 0.4084988236427307\n","[Training Epoch 3] Batch 2455, Loss 0.3978831171989441\n","[Training Epoch 3] Batch 2456, Loss 0.39876803755760193\n","[Training Epoch 3] Batch 2457, Loss 0.38677385449409485\n","[Training Epoch 3] Batch 2458, Loss 0.43118956685066223\n","[Training Epoch 3] Batch 2459, Loss 0.397687166929245\n","[Training Epoch 3] Batch 2460, Loss 0.40994590520858765\n","[Training Epoch 3] Batch 2461, Loss 0.38341960310935974\n","[Training Epoch 3] Batch 2462, Loss 0.3760477900505066\n","[Training Epoch 3] Batch 2463, Loss 0.4090406894683838\n","[Training Epoch 3] Batch 2464, Loss 0.3970668315887451\n","[Training Epoch 3] Batch 2465, Loss 0.39065104722976685\n","[Training Epoch 3] Batch 2466, Loss 0.37271255254745483\n","[Training Epoch 3] Batch 2467, Loss 0.38383209705352783\n","[Training Epoch 3] Batch 2468, Loss 0.3987191319465637\n","[Training Epoch 3] Batch 2469, Loss 0.4108784794807434\n","[Training Epoch 3] Batch 2470, Loss 0.406732439994812\n","[Training Epoch 3] Batch 2471, Loss 0.41282933950424194\n","[Training Epoch 3] Batch 2472, Loss 0.4032564163208008\n","[Training Epoch 3] Batch 2473, Loss 0.3991425037384033\n","[Training Epoch 3] Batch 2474, Loss 0.4101938009262085\n","[Training Epoch 3] Batch 2475, Loss 0.3520950973033905\n","[Training Epoch 3] Batch 2476, Loss 0.39427071809768677\n","[Training Epoch 3] Batch 2477, Loss 0.3989306092262268\n","[Training Epoch 3] Batch 2478, Loss 0.40951961278915405\n","[Training Epoch 3] Batch 2479, Loss 0.4142983853816986\n","[Training Epoch 3] Batch 2480, Loss 0.37879884243011475\n","[Training Epoch 3] Batch 2481, Loss 0.396984338760376\n","[Training Epoch 3] Batch 2482, Loss 0.41478294134140015\n","[Training Epoch 3] Batch 2483, Loss 0.40813231468200684\n","[Training Epoch 3] Batch 2484, Loss 0.395725280046463\n","[Training Epoch 3] Batch 2485, Loss 0.3932531774044037\n","[Training Epoch 3] Batch 2486, Loss 0.40314507484436035\n","[Training Epoch 3] Batch 2487, Loss 0.37784892320632935\n","[Training Epoch 3] Batch 2488, Loss 0.39272743463516235\n","[Training Epoch 3] Batch 2489, Loss 0.40705543756484985\n","[Training Epoch 3] Batch 2490, Loss 0.4033803939819336\n","[Training Epoch 3] Batch 2491, Loss 0.38180363178253174\n","[Training Epoch 3] Batch 2492, Loss 0.38333553075790405\n","[Training Epoch 3] Batch 2493, Loss 0.3799280524253845\n","[Training Epoch 3] Batch 2494, Loss 0.3945915102958679\n","[Training Epoch 3] Batch 2495, Loss 0.390478253364563\n","[Training Epoch 3] Batch 2496, Loss 0.40437984466552734\n","[Training Epoch 3] Batch 2497, Loss 0.4206954836845398\n","[Training Epoch 3] Batch 2498, Loss 0.37320226430892944\n","[Training Epoch 3] Batch 2499, Loss 0.40436461567878723\n","[Training Epoch 3] Batch 2500, Loss 0.39320695400238037\n","[Training Epoch 3] Batch 2501, Loss 0.3832854926586151\n","[Training Epoch 3] Batch 2502, Loss 0.3881784677505493\n","[Training Epoch 3] Batch 2503, Loss 0.4494801163673401\n","[Training Epoch 3] Batch 2504, Loss 0.3741651475429535\n","[Training Epoch 3] Batch 2505, Loss 0.3999842405319214\n","[Training Epoch 3] Batch 2506, Loss 0.45575207471847534\n","[Training Epoch 3] Batch 2507, Loss 0.41920989751815796\n","[Training Epoch 3] Batch 2508, Loss 0.38859283924102783\n","[Training Epoch 3] Batch 2509, Loss 0.39005333185195923\n","[Training Epoch 3] Batch 2510, Loss 0.4151822626590729\n","[Training Epoch 3] Batch 2511, Loss 0.4194730222225189\n","[Training Epoch 3] Batch 2512, Loss 0.3815455436706543\n","[Training Epoch 3] Batch 2513, Loss 0.3862377405166626\n","[Training Epoch 3] Batch 2514, Loss 0.40130317211151123\n","[Training Epoch 3] Batch 2515, Loss 0.3902079164981842\n","[Training Epoch 3] Batch 2516, Loss 0.4283669888973236\n","[Training Epoch 3] Batch 2517, Loss 0.38419267535209656\n","[Training Epoch 3] Batch 2518, Loss 0.37453073263168335\n","[Training Epoch 3] Batch 2519, Loss 0.4228067994117737\n","[Training Epoch 3] Batch 2520, Loss 0.3884979486465454\n","[Training Epoch 3] Batch 2521, Loss 0.41775020956993103\n","[Training Epoch 3] Batch 2522, Loss 0.39000973105430603\n","[Training Epoch 3] Batch 2523, Loss 0.39509764313697815\n","[Training Epoch 3] Batch 2524, Loss 0.41286659240722656\n","[Training Epoch 3] Batch 2525, Loss 0.4300447106361389\n","[Training Epoch 3] Batch 2526, Loss 0.41492414474487305\n","[Training Epoch 3] Batch 2527, Loss 0.3991062641143799\n","[Training Epoch 3] Batch 2528, Loss 0.39102303981781006\n","[Training Epoch 3] Batch 2529, Loss 0.4159424901008606\n","[Training Epoch 3] Batch 2530, Loss 0.3921327590942383\n","[Training Epoch 3] Batch 2531, Loss 0.38120394945144653\n","[Training Epoch 3] Batch 2532, Loss 0.41220834851264954\n","[Training Epoch 3] Batch 2533, Loss 0.42773622274398804\n","[Training Epoch 3] Batch 2534, Loss 0.4137876033782959\n","[Training Epoch 3] Batch 2535, Loss 0.4439471960067749\n","[Training Epoch 3] Batch 2536, Loss 0.39304304122924805\n","[Training Epoch 3] Batch 2537, Loss 0.4110569357872009\n","[Training Epoch 3] Batch 2538, Loss 0.45025473833084106\n","[Training Epoch 3] Batch 2539, Loss 0.4092261493206024\n","[Training Epoch 3] Batch 2540, Loss 0.4405248761177063\n","[Training Epoch 3] Batch 2541, Loss 0.42248988151550293\n","[Training Epoch 3] Batch 2542, Loss 0.38068678975105286\n","[Training Epoch 3] Batch 2543, Loss 0.3931758999824524\n","[Training Epoch 3] Batch 2544, Loss 0.40761056542396545\n","[Training Epoch 3] Batch 2545, Loss 0.38101503252983093\n","[Training Epoch 3] Batch 2546, Loss 0.40749478340148926\n","[Training Epoch 3] Batch 2547, Loss 0.3872222304344177\n","[Training Epoch 3] Batch 2548, Loss 0.37093257904052734\n","[Training Epoch 3] Batch 2549, Loss 0.38971516489982605\n","[Training Epoch 3] Batch 2550, Loss 0.3978821635246277\n","[Training Epoch 3] Batch 2551, Loss 0.3812599778175354\n","[Training Epoch 3] Batch 2552, Loss 0.40902775526046753\n","[Training Epoch 3] Batch 2553, Loss 0.42721542716026306\n","[Training Epoch 3] Batch 2554, Loss 0.3977324962615967\n","[Training Epoch 3] Batch 2555, Loss 0.38606321811676025\n","[Training Epoch 3] Batch 2556, Loss 0.35964643955230713\n","[Training Epoch 3] Batch 2557, Loss 0.4318220317363739\n","[Training Epoch 3] Batch 2558, Loss 0.4105372428894043\n","[Training Epoch 3] Batch 2559, Loss 0.4117893576622009\n","[Training Epoch 3] Batch 2560, Loss 0.39072728157043457\n","[Training Epoch 3] Batch 2561, Loss 0.3615027070045471\n","[Training Epoch 3] Batch 2562, Loss 0.36453956365585327\n","[Training Epoch 3] Batch 2563, Loss 0.3841087520122528\n","[Training Epoch 3] Batch 2564, Loss 0.39685243368148804\n","[Training Epoch 3] Batch 2565, Loss 0.39284855127334595\n","[Training Epoch 3] Batch 2566, Loss 0.41229048371315\n","[Training Epoch 3] Batch 2567, Loss 0.3860688805580139\n","[Training Epoch 3] Batch 2568, Loss 0.4080148935317993\n","[Training Epoch 3] Batch 2569, Loss 0.40776899456977844\n","[Training Epoch 3] Batch 2570, Loss 0.4216783344745636\n","[Training Epoch 3] Batch 2571, Loss 0.38509052991867065\n","[Training Epoch 3] Batch 2572, Loss 0.40075814723968506\n","[Training Epoch 3] Batch 2573, Loss 0.3789059519767761\n","[Training Epoch 3] Batch 2574, Loss 0.40174123644828796\n","[Training Epoch 3] Batch 2575, Loss 0.4244002103805542\n","[Training Epoch 3] Batch 2576, Loss 0.42356395721435547\n","[Training Epoch 3] Batch 2577, Loss 0.3984208106994629\n","[Training Epoch 3] Batch 2578, Loss 0.4258093535900116\n","[Training Epoch 3] Batch 2579, Loss 0.3961673378944397\n","[Training Epoch 3] Batch 2580, Loss 0.39400988817214966\n","[Training Epoch 3] Batch 2581, Loss 0.35733726620674133\n","[Training Epoch 3] Batch 2582, Loss 0.375468909740448\n","[Training Epoch 3] Batch 2583, Loss 0.37739098072052\n","[Training Epoch 3] Batch 2584, Loss 0.3897297978401184\n","[Training Epoch 3] Batch 2585, Loss 0.40377023816108704\n","[Training Epoch 3] Batch 2586, Loss 0.38200074434280396\n","[Training Epoch 3] Batch 2587, Loss 0.40430188179016113\n","[Training Epoch 3] Batch 2588, Loss 0.41792032122612\n","[Training Epoch 3] Batch 2589, Loss 0.4183330833911896\n","[Training Epoch 3] Batch 2590, Loss 0.3959072530269623\n","[Training Epoch 3] Batch 2591, Loss 0.4129815101623535\n","[Training Epoch 3] Batch 2592, Loss 0.41021275520324707\n","[Training Epoch 3] Batch 2593, Loss 0.39168304204940796\n","[Training Epoch 3] Batch 2594, Loss 0.38792651891708374\n","[Training Epoch 3] Batch 2595, Loss 0.4041632413864136\n","[Training Epoch 3] Batch 2596, Loss 0.3928670287132263\n","[Training Epoch 3] Batch 2597, Loss 0.3622246980667114\n","[Training Epoch 3] Batch 2598, Loss 0.39527493715286255\n","[Training Epoch 3] Batch 2599, Loss 0.384507954120636\n","[Training Epoch 3] Batch 2600, Loss 0.41312864422798157\n","[Training Epoch 3] Batch 2601, Loss 0.356137216091156\n","[Training Epoch 3] Batch 2602, Loss 0.3925437927246094\n","[Training Epoch 3] Batch 2603, Loss 0.3920350670814514\n","[Training Epoch 3] Batch 2604, Loss 0.40921854972839355\n","[Training Epoch 3] Batch 2605, Loss 0.40410780906677246\n","[Training Epoch 3] Batch 2606, Loss 0.4149286448955536\n","[Training Epoch 3] Batch 2607, Loss 0.42496418952941895\n","[Training Epoch 3] Batch 2608, Loss 0.37750813364982605\n","[Training Epoch 3] Batch 2609, Loss 0.388828307390213\n","[Training Epoch 3] Batch 2610, Loss 0.35837259888648987\n","[Training Epoch 3] Batch 2611, Loss 0.37639808654785156\n","[Training Epoch 3] Batch 2612, Loss 0.4144619107246399\n","[Training Epoch 3] Batch 2613, Loss 0.4205671548843384\n","[Training Epoch 3] Batch 2614, Loss 0.39387667179107666\n","[Training Epoch 3] Batch 2615, Loss 0.3873542845249176\n","[Training Epoch 3] Batch 2616, Loss 0.37365639209747314\n","[Training Epoch 3] Batch 2617, Loss 0.39100879430770874\n","[Training Epoch 3] Batch 2618, Loss 0.35214200615882874\n","[Training Epoch 3] Batch 2619, Loss 0.393039345741272\n","[Training Epoch 3] Batch 2620, Loss 0.38428980112075806\n","[Training Epoch 3] Batch 2621, Loss 0.37611842155456543\n","[Training Epoch 3] Batch 2622, Loss 0.3681655526161194\n","[Training Epoch 3] Batch 2623, Loss 0.4075166881084442\n","[Training Epoch 3] Batch 2624, Loss 0.388491690158844\n","[Training Epoch 3] Batch 2625, Loss 0.4076734781265259\n","[Training Epoch 3] Batch 2626, Loss 0.3861485421657562\n","[Training Epoch 3] Batch 2627, Loss 0.40429002046585083\n","[Training Epoch 3] Batch 2628, Loss 0.3820880055427551\n","[Training Epoch 3] Batch 2629, Loss 0.36975955963134766\n","[Training Epoch 3] Batch 2630, Loss 0.3829469084739685\n","[Training Epoch 3] Batch 2631, Loss 0.40652996301651\n","[Training Epoch 3] Batch 2632, Loss 0.40510085225105286\n","[Training Epoch 3] Batch 2633, Loss 0.40656745433807373\n","[Training Epoch 3] Batch 2634, Loss 0.40948227047920227\n","[Training Epoch 3] Batch 2635, Loss 0.39201343059539795\n","[Training Epoch 3] Batch 2636, Loss 0.4009377360343933\n","[Training Epoch 3] Batch 2637, Loss 0.40890297293663025\n","[Training Epoch 3] Batch 2638, Loss 0.35466858744621277\n","[Training Epoch 3] Batch 2639, Loss 0.3793637156486511\n","[Training Epoch 3] Batch 2640, Loss 0.3963537812232971\n","[Training Epoch 3] Batch 2641, Loss 0.3841938376426697\n","[Training Epoch 3] Batch 2642, Loss 0.41474810242652893\n","[Training Epoch 3] Batch 2643, Loss 0.44181376695632935\n","[Training Epoch 3] Batch 2644, Loss 0.3943421542644501\n","[Training Epoch 3] Batch 2645, Loss 0.3983346223831177\n","[Training Epoch 3] Batch 2646, Loss 0.38173240423202515\n","[Training Epoch 3] Batch 2647, Loss 0.41803741455078125\n","[Training Epoch 3] Batch 2648, Loss 0.39222878217697144\n","[Training Epoch 3] Batch 2649, Loss 0.3977036476135254\n","[Training Epoch 3] Batch 2650, Loss 0.35874632000923157\n","[Training Epoch 3] Batch 2651, Loss 0.41694754362106323\n","[Training Epoch 3] Batch 2652, Loss 0.38404616713523865\n","[Training Epoch 3] Batch 2653, Loss 0.3959707021713257\n","[Training Epoch 3] Batch 2654, Loss 0.4082011282444\n","[Training Epoch 3] Batch 2655, Loss 0.3754286766052246\n","[Training Epoch 3] Batch 2656, Loss 0.40949589014053345\n","[Training Epoch 3] Batch 2657, Loss 0.42528456449508667\n","[Training Epoch 3] Batch 2658, Loss 0.3843133747577667\n","[Training Epoch 3] Batch 2659, Loss 0.38257136940956116\n","[Training Epoch 3] Batch 2660, Loss 0.3951154947280884\n","[Training Epoch 3] Batch 2661, Loss 0.4029226005077362\n","[Training Epoch 3] Batch 2662, Loss 0.39230743050575256\n","[Training Epoch 3] Batch 2663, Loss 0.3978828489780426\n","[Training Epoch 3] Batch 2664, Loss 0.3640766441822052\n","[Training Epoch 3] Batch 2665, Loss 0.36541181802749634\n","[Training Epoch 3] Batch 2666, Loss 0.40192681550979614\n","[Training Epoch 3] Batch 2667, Loss 0.3991611897945404\n","[Training Epoch 3] Batch 2668, Loss 0.3793402910232544\n","[Training Epoch 3] Batch 2669, Loss 0.4094952642917633\n","[Training Epoch 3] Batch 2670, Loss 0.36310797929763794\n","[Training Epoch 3] Batch 2671, Loss 0.40339839458465576\n","[Training Epoch 3] Batch 2672, Loss 0.42433297634124756\n","[Training Epoch 3] Batch 2673, Loss 0.3901592791080475\n","[Training Epoch 3] Batch 2674, Loss 0.39682692289352417\n","[Training Epoch 3] Batch 2675, Loss 0.3913593888282776\n","[Training Epoch 3] Batch 2676, Loss 0.41999077796936035\n","[Training Epoch 3] Batch 2677, Loss 0.382647842168808\n","[Training Epoch 3] Batch 2678, Loss 0.44339680671691895\n","[Training Epoch 3] Batch 2679, Loss 0.4001360535621643\n","[Training Epoch 3] Batch 2680, Loss 0.4163825511932373\n","[Training Epoch 3] Batch 2681, Loss 0.39329689741134644\n","[Training Epoch 3] Batch 2682, Loss 0.38054385781288147\n","[Training Epoch 3] Batch 2683, Loss 0.38441604375839233\n","[Training Epoch 3] Batch 2684, Loss 0.3874984085559845\n","[Training Epoch 3] Batch 2685, Loss 0.41749852895736694\n","[Training Epoch 3] Batch 2686, Loss 0.3662095069885254\n","[Training Epoch 3] Batch 2687, Loss 0.40207383036613464\n","[Training Epoch 3] Batch 2688, Loss 0.3867136836051941\n","[Training Epoch 3] Batch 2689, Loss 0.3929764926433563\n","[Training Epoch 3] Batch 2690, Loss 0.3790932595729828\n","[Training Epoch 3] Batch 2691, Loss 0.4113001823425293\n","[Training Epoch 3] Batch 2692, Loss 0.3933908939361572\n","[Training Epoch 3] Batch 2693, Loss 0.3845539093017578\n","[Training Epoch 3] Batch 2694, Loss 0.4194428324699402\n","[Training Epoch 3] Batch 2695, Loss 0.40447938442230225\n","[Training Epoch 3] Batch 2696, Loss 0.39544564485549927\n","[Training Epoch 3] Batch 2697, Loss 0.406085729598999\n","[Training Epoch 3] Batch 2698, Loss 0.39227500557899475\n","[Training Epoch 3] Batch 2699, Loss 0.35364609956741333\n","[Training Epoch 3] Batch 2700, Loss 0.3856502175331116\n","[Training Epoch 3] Batch 2701, Loss 0.38432204723358154\n","[Training Epoch 3] Batch 2702, Loss 0.42062997817993164\n","[Training Epoch 3] Batch 2703, Loss 0.38756033778190613\n","[Training Epoch 3] Batch 2704, Loss 0.41541314125061035\n","[Training Epoch 3] Batch 2705, Loss 0.3686322271823883\n","[Training Epoch 3] Batch 2706, Loss 0.3880031704902649\n","[Training Epoch 3] Batch 2707, Loss 0.4010622501373291\n","[Training Epoch 3] Batch 2708, Loss 0.3714812695980072\n","[Training Epoch 3] Batch 2709, Loss 0.3819425702095032\n","[Training Epoch 3] Batch 2710, Loss 0.40281420946121216\n","[Training Epoch 3] Batch 2711, Loss 0.40115877985954285\n","[Training Epoch 3] Batch 2712, Loss 0.4037749171257019\n","[Training Epoch 3] Batch 2713, Loss 0.3754580616950989\n","[Training Epoch 3] Batch 2714, Loss 0.38776153326034546\n","[Training Epoch 3] Batch 2715, Loss 0.4415584206581116\n","[Training Epoch 3] Batch 2716, Loss 0.3924182057380676\n","[Training Epoch 3] Batch 2717, Loss 0.4202498197555542\n","[Training Epoch 3] Batch 2718, Loss 0.41965946555137634\n","[Training Epoch 3] Batch 2719, Loss 0.40180888772010803\n","[Training Epoch 3] Batch 2720, Loss 0.38249891996383667\n","[Training Epoch 3] Batch 2721, Loss 0.39766183495521545\n","[Training Epoch 3] Batch 2722, Loss 0.3625059723854065\n","[Training Epoch 3] Batch 2723, Loss 0.397126704454422\n","[Training Epoch 3] Batch 2724, Loss 0.3953096866607666\n","[Training Epoch 3] Batch 2725, Loss 0.3999289274215698\n","[Training Epoch 3] Batch 2726, Loss 0.41067156195640564\n","[Training Epoch 3] Batch 2727, Loss 0.4073812961578369\n","[Training Epoch 3] Batch 2728, Loss 0.42024242877960205\n","[Training Epoch 3] Batch 2729, Loss 0.4023202657699585\n","[Training Epoch 3] Batch 2730, Loss 0.3963567912578583\n","[Training Epoch 3] Batch 2731, Loss 0.3974212408065796\n","[Training Epoch 3] Batch 2732, Loss 0.39924392104148865\n","[Training Epoch 3] Batch 2733, Loss 0.38738858699798584\n","[Training Epoch 3] Batch 2734, Loss 0.3793295621871948\n","[Training Epoch 3] Batch 2735, Loss 0.3719872236251831\n","[Training Epoch 3] Batch 2736, Loss 0.3635317087173462\n","[Training Epoch 3] Batch 2737, Loss 0.4041866958141327\n","[Training Epoch 3] Batch 2738, Loss 0.3658287525177002\n","[Training Epoch 3] Batch 2739, Loss 0.4072965383529663\n","[Training Epoch 3] Batch 2740, Loss 0.4252655506134033\n","[Training Epoch 3] Batch 2741, Loss 0.4175114333629608\n","[Training Epoch 3] Batch 2742, Loss 0.37575364112854004\n","[Training Epoch 3] Batch 2743, Loss 0.404113233089447\n","[Training Epoch 3] Batch 2744, Loss 0.39185631275177\n","[Training Epoch 3] Batch 2745, Loss 0.3731614947319031\n","[Training Epoch 3] Batch 2746, Loss 0.36146003007888794\n","[Training Epoch 3] Batch 2747, Loss 0.37277865409851074\n","[Training Epoch 3] Batch 2748, Loss 0.41092079877853394\n","[Training Epoch 3] Batch 2749, Loss 0.38588041067123413\n","[Training Epoch 3] Batch 2750, Loss 0.3817300796508789\n","[Training Epoch 3] Batch 2751, Loss 0.3925950825214386\n","[Training Epoch 3] Batch 2752, Loss 0.4276712238788605\n","[Training Epoch 3] Batch 2753, Loss 0.4286266267299652\n","[Training Epoch 3] Batch 2754, Loss 0.4175679683685303\n","[Training Epoch 3] Batch 2755, Loss 0.3821249008178711\n","[Training Epoch 3] Batch 2756, Loss 0.4161970019340515\n","[Training Epoch 3] Batch 2757, Loss 0.4012450873851776\n","[Training Epoch 3] Batch 2758, Loss 0.39416593313217163\n","[Training Epoch 3] Batch 2759, Loss 0.3949335515499115\n","[Training Epoch 3] Batch 2760, Loss 0.38350969552993774\n","[Training Epoch 3] Batch 2761, Loss 0.38429415225982666\n","[Training Epoch 3] Batch 2762, Loss 0.3735279440879822\n","[Training Epoch 3] Batch 2763, Loss 0.37529438734054565\n","[Training Epoch 3] Batch 2764, Loss 0.3830353617668152\n","[Training Epoch 3] Batch 2765, Loss 0.3857879638671875\n","[Training Epoch 3] Batch 2766, Loss 0.3978455066680908\n","[Training Epoch 3] Batch 2767, Loss 0.373349666595459\n","[Training Epoch 3] Batch 2768, Loss 0.3731839060783386\n","[Training Epoch 3] Batch 2769, Loss 0.39804965257644653\n","[Training Epoch 3] Batch 2770, Loss 0.38996732234954834\n","[Training Epoch 3] Batch 2771, Loss 0.39487722516059875\n","[Training Epoch 3] Batch 2772, Loss 0.39624130725860596\n","[Training Epoch 3] Batch 2773, Loss 0.39197760820388794\n","[Training Epoch 3] Batch 2774, Loss 0.38351988792419434\n","[Training Epoch 3] Batch 2775, Loss 0.407360702753067\n","[Training Epoch 3] Batch 2776, Loss 0.40229395031929016\n","[Training Epoch 3] Batch 2777, Loss 0.4103965759277344\n","[Training Epoch 3] Batch 2778, Loss 0.3742462396621704\n","[Training Epoch 3] Batch 2779, Loss 0.41527748107910156\n","[Training Epoch 3] Batch 2780, Loss 0.38829493522644043\n","[Training Epoch 3] Batch 2781, Loss 0.3716798424720764\n","[Training Epoch 3] Batch 2782, Loss 0.39407622814178467\n","[Training Epoch 3] Batch 2783, Loss 0.3867793083190918\n","[Training Epoch 3] Batch 2784, Loss 0.3971610963344574\n","[Training Epoch 3] Batch 2785, Loss 0.4027206599712372\n","[Training Epoch 3] Batch 2786, Loss 0.41165757179260254\n","[Training Epoch 3] Batch 2787, Loss 0.39591097831726074\n","[Training Epoch 3] Batch 2788, Loss 0.40648549795150757\n","[Training Epoch 3] Batch 2789, Loss 0.39394277334213257\n","[Training Epoch 3] Batch 2790, Loss 0.40346571803092957\n","[Training Epoch 3] Batch 2791, Loss 0.38679781556129456\n","[Training Epoch 3] Batch 2792, Loss 0.40279650688171387\n","[Training Epoch 3] Batch 2793, Loss 0.3965265452861786\n","[Training Epoch 3] Batch 2794, Loss 0.3954153060913086\n","[Training Epoch 3] Batch 2795, Loss 0.3804413676261902\n","[Training Epoch 3] Batch 2796, Loss 0.3964764475822449\n","[Training Epoch 3] Batch 2797, Loss 0.39258480072021484\n","[Training Epoch 3] Batch 2798, Loss 0.4109014570713043\n","[Training Epoch 3] Batch 2799, Loss 0.3920929431915283\n","[Training Epoch 3] Batch 2800, Loss 0.4121027886867523\n","[Training Epoch 3] Batch 2801, Loss 0.3590207099914551\n","[Training Epoch 3] Batch 2802, Loss 0.3808474540710449\n","[Training Epoch 3] Batch 2803, Loss 0.3903844952583313\n","[Training Epoch 3] Batch 2804, Loss 0.38112613558769226\n","[Training Epoch 3] Batch 2805, Loss 0.3852039575576782\n","[Training Epoch 3] Batch 2806, Loss 0.412841796875\n","[Training Epoch 3] Batch 2807, Loss 0.39084604382514954\n","[Training Epoch 3] Batch 2808, Loss 0.37823325395584106\n","[Training Epoch 3] Batch 2809, Loss 0.38213232159614563\n","[Training Epoch 3] Batch 2810, Loss 0.39971667528152466\n","[Training Epoch 3] Batch 2811, Loss 0.43483322858810425\n","[Training Epoch 3] Batch 2812, Loss 0.40990039706230164\n","[Training Epoch 3] Batch 2813, Loss 0.4008418321609497\n","[Training Epoch 3] Batch 2814, Loss 0.3687130808830261\n","[Training Epoch 3] Batch 2815, Loss 0.3839793801307678\n","[Training Epoch 3] Batch 2816, Loss 0.38959985971450806\n","[Training Epoch 3] Batch 2817, Loss 0.367980420589447\n","[Training Epoch 3] Batch 2818, Loss 0.38029593229293823\n","[Training Epoch 3] Batch 2819, Loss 0.40198975801467896\n","[Training Epoch 3] Batch 2820, Loss 0.38654035329818726\n","[Training Epoch 3] Batch 2821, Loss 0.395364373922348\n","[Training Epoch 3] Batch 2822, Loss 0.384543776512146\n","[Training Epoch 3] Batch 2823, Loss 0.41639819741249084\n","[Training Epoch 3] Batch 2824, Loss 0.391063392162323\n","[Training Epoch 3] Batch 2825, Loss 0.3960055112838745\n","[Training Epoch 3] Batch 2826, Loss 0.40311145782470703\n","[Training Epoch 3] Batch 2827, Loss 0.3911246359348297\n","[Training Epoch 3] Batch 2828, Loss 0.3730279803276062\n","[Training Epoch 3] Batch 2829, Loss 0.4301033914089203\n","[Training Epoch 3] Batch 2830, Loss 0.4001733064651489\n","[Training Epoch 3] Batch 2831, Loss 0.39579150080680847\n","[Training Epoch 3] Batch 2832, Loss 0.4095093905925751\n","[Training Epoch 3] Batch 2833, Loss 0.3689391314983368\n","[Training Epoch 3] Batch 2834, Loss 0.3918485641479492\n","[Training Epoch 3] Batch 2835, Loss 0.39833611249923706\n","[Training Epoch 3] Batch 2836, Loss 0.411570280790329\n","[Training Epoch 3] Batch 2837, Loss 0.4127122461795807\n","[Training Epoch 3] Batch 2838, Loss 0.39326488971710205\n","[Training Epoch 3] Batch 2839, Loss 0.3799225986003876\n","[Training Epoch 3] Batch 2840, Loss 0.4086288809776306\n","[Training Epoch 3] Batch 2841, Loss 0.3857899308204651\n","[Training Epoch 3] Batch 2842, Loss 0.39750123023986816\n","[Training Epoch 3] Batch 2843, Loss 0.4049953818321228\n","[Training Epoch 3] Batch 2844, Loss 0.4023020267486572\n","[Training Epoch 3] Batch 2845, Loss 0.3871217668056488\n","[Training Epoch 3] Batch 2846, Loss 0.3806673586368561\n","[Training Epoch 3] Batch 2847, Loss 0.37163764238357544\n","[Training Epoch 3] Batch 2848, Loss 0.3960866928100586\n","[Training Epoch 3] Batch 2849, Loss 0.42262983322143555\n","[Training Epoch 3] Batch 2850, Loss 0.37154853343963623\n","[Training Epoch 3] Batch 2851, Loss 0.40095627307891846\n","[Training Epoch 3] Batch 2852, Loss 0.41306400299072266\n","[Training Epoch 3] Batch 2853, Loss 0.4125843048095703\n","[Training Epoch 3] Batch 2854, Loss 0.3849412798881531\n","[Training Epoch 3] Batch 2855, Loss 0.370220422744751\n","[Training Epoch 3] Batch 2856, Loss 0.3592996597290039\n","[Training Epoch 3] Batch 2857, Loss 0.3653717637062073\n","[Training Epoch 3] Batch 2858, Loss 0.39418867230415344\n","[Training Epoch 3] Batch 2859, Loss 0.38441959023475647\n","[Training Epoch 3] Batch 2860, Loss 0.3991762399673462\n","[Training Epoch 3] Batch 2861, Loss 0.40743207931518555\n","[Training Epoch 3] Batch 2862, Loss 0.35404902696609497\n","[Training Epoch 3] Batch 2863, Loss 0.38619929552078247\n","[Training Epoch 3] Batch 2864, Loss 0.3614814281463623\n","[Training Epoch 3] Batch 2865, Loss 0.396449476480484\n","[Training Epoch 3] Batch 2866, Loss 0.372050940990448\n","[Training Epoch 3] Batch 2867, Loss 0.38617783784866333\n","[Training Epoch 3] Batch 2868, Loss 0.4320208430290222\n","[Training Epoch 3] Batch 2869, Loss 0.41215258836746216\n","[Training Epoch 3] Batch 2870, Loss 0.40814724564552307\n","[Training Epoch 3] Batch 2871, Loss 0.42204955220222473\n","[Training Epoch 3] Batch 2872, Loss 0.38956499099731445\n","[Training Epoch 3] Batch 2873, Loss 0.3739790916442871\n","[Training Epoch 3] Batch 2874, Loss 0.39919614791870117\n","[Training Epoch 3] Batch 2875, Loss 0.4232846796512604\n","[Training Epoch 3] Batch 2876, Loss 0.4195883572101593\n","[Training Epoch 3] Batch 2877, Loss 0.39723870158195496\n","[Training Epoch 3] Batch 2878, Loss 0.4074346125125885\n","[Training Epoch 3] Batch 2879, Loss 0.37992918491363525\n","[Training Epoch 3] Batch 2880, Loss 0.3756615221500397\n","[Training Epoch 3] Batch 2881, Loss 0.4223281741142273\n","[Training Epoch 3] Batch 2882, Loss 0.3896057605743408\n","[Training Epoch 3] Batch 2883, Loss 0.3869696855545044\n","[Training Epoch 3] Batch 2884, Loss 0.37098580598831177\n","[Training Epoch 3] Batch 2885, Loss 0.4156978130340576\n","[Training Epoch 3] Batch 2886, Loss 0.397908478975296\n","[Training Epoch 3] Batch 2887, Loss 0.374006986618042\n","[Training Epoch 3] Batch 2888, Loss 0.4070974588394165\n","[Training Epoch 3] Batch 2889, Loss 0.4246295094490051\n","[Training Epoch 3] Batch 2890, Loss 0.3822039067745209\n","[Training Epoch 3] Batch 2891, Loss 0.36936983466148376\n","[Training Epoch 3] Batch 2892, Loss 0.40462103486061096\n","[Training Epoch 3] Batch 2893, Loss 0.40483003854751587\n","[Training Epoch 3] Batch 2894, Loss 0.40416112542152405\n","[Training Epoch 3] Batch 2895, Loss 0.40579649806022644\n","[Training Epoch 3] Batch 2896, Loss 0.4106242060661316\n","[Training Epoch 3] Batch 2897, Loss 0.4433234632015228\n","[Training Epoch 3] Batch 2898, Loss 0.3872383236885071\n","[Training Epoch 3] Batch 2899, Loss 0.4149665832519531\n","[Training Epoch 3] Batch 2900, Loss 0.34904128313064575\n","[Training Epoch 3] Batch 2901, Loss 0.3973701000213623\n","[Training Epoch 3] Batch 2902, Loss 0.380179762840271\n","[Training Epoch 3] Batch 2903, Loss 0.38544875383377075\n","[Training Epoch 3] Batch 2904, Loss 0.41014426946640015\n","[Training Epoch 3] Batch 2905, Loss 0.4111142158508301\n","[Training Epoch 3] Batch 2906, Loss 0.37892359495162964\n","[Training Epoch 3] Batch 2907, Loss 0.36076346039772034\n","[Training Epoch 3] Batch 2908, Loss 0.37394699454307556\n","[Training Epoch 3] Batch 2909, Loss 0.4005717635154724\n","[Training Epoch 3] Batch 2910, Loss 0.40101802349090576\n","[Training Epoch 3] Batch 2911, Loss 0.37690088152885437\n","[Training Epoch 3] Batch 2912, Loss 0.3864569664001465\n","[Training Epoch 3] Batch 2913, Loss 0.35718175768852234\n","[Training Epoch 3] Batch 2914, Loss 0.3899730443954468\n","[Training Epoch 3] Batch 2915, Loss 0.37930941581726074\n","[Training Epoch 3] Batch 2916, Loss 0.37442708015441895\n","[Training Epoch 3] Batch 2917, Loss 0.41563498973846436\n","[Training Epoch 3] Batch 2918, Loss 0.40886324644088745\n","[Training Epoch 3] Batch 2919, Loss 0.39868852496147156\n","[Training Epoch 3] Batch 2920, Loss 0.3632162809371948\n","[Training Epoch 3] Batch 2921, Loss 0.4188697934150696\n","[Training Epoch 3] Batch 2922, Loss 0.36117154359817505\n","[Training Epoch 3] Batch 2923, Loss 0.4022921919822693\n","[Training Epoch 3] Batch 2924, Loss 0.3810434639453888\n","[Training Epoch 3] Batch 2925, Loss 0.3858965337276459\n","[Training Epoch 3] Batch 2926, Loss 0.39979100227355957\n","[Training Epoch 3] Batch 2927, Loss 0.4224535822868347\n","[Training Epoch 3] Batch 2928, Loss 0.40572237968444824\n","[Training Epoch 3] Batch 2929, Loss 0.38928142189979553\n","[Training Epoch 3] Batch 2930, Loss 0.3903324604034424\n","[Training Epoch 3] Batch 2931, Loss 0.388872891664505\n","[Training Epoch 3] Batch 2932, Loss 0.3921438157558441\n","[Training Epoch 3] Batch 2933, Loss 0.3615630567073822\n","[Training Epoch 3] Batch 2934, Loss 0.4052371382713318\n","[Training Epoch 3] Batch 2935, Loss 0.43877944350242615\n","[Training Epoch 3] Batch 2936, Loss 0.3840649127960205\n","[Training Epoch 3] Batch 2937, Loss 0.4029994010925293\n","[Training Epoch 3] Batch 2938, Loss 0.3998144268989563\n","[Training Epoch 3] Batch 2939, Loss 0.4097152352333069\n","[Training Epoch 3] Batch 2940, Loss 0.39033713936805725\n","[Training Epoch 3] Batch 2941, Loss 0.38160595297813416\n","[Training Epoch 3] Batch 2942, Loss 0.37154632806777954\n","[Training Epoch 3] Batch 2943, Loss 0.3883662223815918\n","[Training Epoch 3] Batch 2944, Loss 0.40642887353897095\n","[Training Epoch 3] Batch 2945, Loss 0.40686294436454773\n","[Training Epoch 3] Batch 2946, Loss 0.3917328715324402\n","[Training Epoch 3] Batch 2947, Loss 0.41417375206947327\n","[Training Epoch 3] Batch 2948, Loss 0.3595384955406189\n","[Training Epoch 3] Batch 2949, Loss 0.38626617193222046\n","[Training Epoch 3] Batch 2950, Loss 0.3987245559692383\n","[Training Epoch 3] Batch 2951, Loss 0.39578157663345337\n","[Training Epoch 3] Batch 2952, Loss 0.42735180258750916\n","[Training Epoch 3] Batch 2953, Loss 0.4042069911956787\n","[Training Epoch 3] Batch 2954, Loss 0.3948633074760437\n","[Training Epoch 3] Batch 2955, Loss 0.39024728536605835\n","[Training Epoch 3] Batch 2956, Loss 0.39110267162323\n","[Training Epoch 3] Batch 2957, Loss 0.43766337633132935\n","[Training Epoch 3] Batch 2958, Loss 0.3872221112251282\n","[Training Epoch 3] Batch 2959, Loss 0.36230385303497314\n","[Training Epoch 3] Batch 2960, Loss 0.4231349229812622\n","[Training Epoch 3] Batch 2961, Loss 0.4074894189834595\n","[Training Epoch 3] Batch 2962, Loss 0.3905729651451111\n","[Training Epoch 3] Batch 2963, Loss 0.4001620411872864\n","[Training Epoch 3] Batch 2964, Loss 0.38286104798316956\n","[Training Epoch 3] Batch 2965, Loss 0.3834177851676941\n","[Training Epoch 3] Batch 2966, Loss 0.36639511585235596\n","[Training Epoch 3] Batch 2967, Loss 0.38823941349983215\n","[Training Epoch 3] Batch 2968, Loss 0.34149837493896484\n","[Training Epoch 3] Batch 2969, Loss 0.37321609258651733\n","[Training Epoch 3] Batch 2970, Loss 0.3859439492225647\n","[Training Epoch 3] Batch 2971, Loss 0.4055308997631073\n","[Training Epoch 3] Batch 2972, Loss 0.3669341802597046\n","[Training Epoch 3] Batch 2973, Loss 0.4225054681301117\n","[Training Epoch 3] Batch 2974, Loss 0.36911919713020325\n","[Training Epoch 3] Batch 2975, Loss 0.38844215869903564\n","[Training Epoch 3] Batch 2976, Loss 0.3974936008453369\n","[Training Epoch 3] Batch 2977, Loss 0.36436283588409424\n","[Training Epoch 3] Batch 2978, Loss 0.3834230601787567\n","[Training Epoch 3] Batch 2979, Loss 0.38037365674972534\n","[Training Epoch 3] Batch 2980, Loss 0.4136676788330078\n","[Training Epoch 3] Batch 2981, Loss 0.3787459433078766\n","[Training Epoch 3] Batch 2982, Loss 0.36306801438331604\n","[Training Epoch 3] Batch 2983, Loss 0.36087241768836975\n","[Training Epoch 3] Batch 2984, Loss 0.403056263923645\n","[Training Epoch 3] Batch 2985, Loss 0.35357216000556946\n","[Training Epoch 3] Batch 2986, Loss 0.4020155668258667\n","[Training Epoch 3] Batch 2987, Loss 0.3721787631511688\n","[Training Epoch 3] Batch 2988, Loss 0.3817753791809082\n","[Training Epoch 3] Batch 2989, Loss 0.38403159379959106\n","[Training Epoch 3] Batch 2990, Loss 0.3717608153820038\n","[Training Epoch 3] Batch 2991, Loss 0.37511157989501953\n","[Training Epoch 3] Batch 2992, Loss 0.38066959381103516\n","[Training Epoch 3] Batch 2993, Loss 0.39257752895355225\n","[Training Epoch 3] Batch 2994, Loss 0.3900264799594879\n","[Training Epoch 3] Batch 2995, Loss 0.3605923652648926\n","[Training Epoch 3] Batch 2996, Loss 0.40564849972724915\n","[Training Epoch 3] Batch 2997, Loss 0.3837737739086151\n","[Training Epoch 3] Batch 2998, Loss 0.38043323159217834\n","[Training Epoch 3] Batch 2999, Loss 0.40496695041656494\n","[Training Epoch 3] Batch 3000, Loss 0.38565510511398315\n","[Training Epoch 3] Batch 3001, Loss 0.4316856265068054\n","[Training Epoch 3] Batch 3002, Loss 0.3897721469402313\n","[Training Epoch 3] Batch 3003, Loss 0.41662687063217163\n","[Training Epoch 3] Batch 3004, Loss 0.4301814138889313\n","[Training Epoch 3] Batch 3005, Loss 0.4077463448047638\n","[Training Epoch 3] Batch 3006, Loss 0.38363659381866455\n","[Training Epoch 3] Batch 3007, Loss 0.3657855987548828\n","[Training Epoch 3] Batch 3008, Loss 0.39684709906578064\n","[Training Epoch 3] Batch 3009, Loss 0.43345680832862854\n","[Training Epoch 3] Batch 3010, Loss 0.3799282908439636\n","[Training Epoch 3] Batch 3011, Loss 0.3789595365524292\n","[Training Epoch 3] Batch 3012, Loss 0.36309078335762024\n","[Training Epoch 3] Batch 3013, Loss 0.3901315927505493\n","[Training Epoch 3] Batch 3014, Loss 0.37995076179504395\n","[Training Epoch 3] Batch 3015, Loss 0.3695172667503357\n","[Training Epoch 3] Batch 3016, Loss 0.4070395827293396\n","[Training Epoch 3] Batch 3017, Loss 0.40470123291015625\n","[Training Epoch 3] Batch 3018, Loss 0.394243985414505\n","[Training Epoch 3] Batch 3019, Loss 0.39450913667678833\n","[Training Epoch 3] Batch 3020, Loss 0.37917953729629517\n","[Training Epoch 3] Batch 3021, Loss 0.4157610237598419\n","[Training Epoch 3] Batch 3022, Loss 0.3960563540458679\n","[Training Epoch 3] Batch 3023, Loss 0.40101030468940735\n","[Training Epoch 3] Batch 3024, Loss 0.38186904788017273\n","[Training Epoch 3] Batch 3025, Loss 0.3898623585700989\n","[Training Epoch 3] Batch 3026, Loss 0.4027371406555176\n","[Training Epoch 3] Batch 3027, Loss 0.4086534380912781\n","[Training Epoch 3] Batch 3028, Loss 0.4307621121406555\n","[Training Epoch 3] Batch 3029, Loss 0.38903433084487915\n","[Training Epoch 3] Batch 3030, Loss 0.36901068687438965\n","[Training Epoch 3] Batch 3031, Loss 0.37613990902900696\n","[Training Epoch 3] Batch 3032, Loss 0.3886147141456604\n","[Training Epoch 3] Batch 3033, Loss 0.3881211578845978\n","[Training Epoch 3] Batch 3034, Loss 0.3881862163543701\n","[Training Epoch 3] Batch 3035, Loss 0.3644270896911621\n","[Training Epoch 3] Batch 3036, Loss 0.39102113246917725\n","[Training Epoch 3] Batch 3037, Loss 0.41467541456222534\n","[Training Epoch 3] Batch 3038, Loss 0.3738687038421631\n","[Training Epoch 3] Batch 3039, Loss 0.38554710149765015\n","[Training Epoch 3] Batch 3040, Loss 0.3629772961139679\n","[Training Epoch 3] Batch 3041, Loss 0.4159987270832062\n","[Training Epoch 3] Batch 3042, Loss 0.41411709785461426\n","[Training Epoch 3] Batch 3043, Loss 0.376209557056427\n","[Training Epoch 3] Batch 3044, Loss 0.4379618763923645\n","[Training Epoch 3] Batch 3045, Loss 0.3636910915374756\n","[Training Epoch 3] Batch 3046, Loss 0.41267913579940796\n","[Training Epoch 3] Batch 3047, Loss 0.3908732235431671\n","[Training Epoch 3] Batch 3048, Loss 0.3780912160873413\n","[Training Epoch 3] Batch 3049, Loss 0.40452367067337036\n","[Training Epoch 3] Batch 3050, Loss 0.3621361255645752\n","[Training Epoch 3] Batch 3051, Loss 0.410537987947464\n","[Training Epoch 3] Batch 3052, Loss 0.3836005926132202\n","[Training Epoch 3] Batch 3053, Loss 0.41192057728767395\n","[Training Epoch 3] Batch 3054, Loss 0.39750567078590393\n","[Training Epoch 3] Batch 3055, Loss 0.4056057333946228\n","[Training Epoch 3] Batch 3056, Loss 0.37009933590888977\n","[Training Epoch 3] Batch 3057, Loss 0.40141594409942627\n","[Training Epoch 3] Batch 3058, Loss 0.40297114849090576\n","[Training Epoch 3] Batch 3059, Loss 0.3750031590461731\n","[Training Epoch 3] Batch 3060, Loss 0.4047276973724365\n","[Training Epoch 3] Batch 3061, Loss 0.401519238948822\n","[Training Epoch 3] Batch 3062, Loss 0.40229734778404236\n","[Training Epoch 3] Batch 3063, Loss 0.3750949501991272\n","[Training Epoch 3] Batch 3064, Loss 0.3964175581932068\n","[Training Epoch 3] Batch 3065, Loss 0.4120253622531891\n","[Training Epoch 3] Batch 3066, Loss 0.3764682412147522\n","[Training Epoch 3] Batch 3067, Loss 0.37794244289398193\n","[Training Epoch 3] Batch 3068, Loss 0.381023108959198\n","[Training Epoch 3] Batch 3069, Loss 0.4244917035102844\n","[Training Epoch 3] Batch 3070, Loss 0.36176684498786926\n","[Training Epoch 3] Batch 3071, Loss 0.4133686125278473\n","[Training Epoch 3] Batch 3072, Loss 0.3872152268886566\n","[Training Epoch 3] Batch 3073, Loss 0.3924480378627777\n","[Training Epoch 3] Batch 3074, Loss 0.37205880880355835\n","[Training Epoch 3] Batch 3075, Loss 0.3798207640647888\n","[Training Epoch 3] Batch 3076, Loss 0.391431599855423\n","[Training Epoch 3] Batch 3077, Loss 0.38632267713546753\n","[Training Epoch 3] Batch 3078, Loss 0.39954861998558044\n","[Training Epoch 3] Batch 3079, Loss 0.3595166802406311\n","[Training Epoch 3] Batch 3080, Loss 0.3529057204723358\n","[Training Epoch 3] Batch 3081, Loss 0.39301928877830505\n","[Training Epoch 3] Batch 3082, Loss 0.36678487062454224\n","[Training Epoch 3] Batch 3083, Loss 0.3809489607810974\n","[Training Epoch 3] Batch 3084, Loss 0.3747295141220093\n","[Training Epoch 3] Batch 3085, Loss 0.3654118776321411\n","[Training Epoch 3] Batch 3086, Loss 0.41630324721336365\n","[Training Epoch 3] Batch 3087, Loss 0.39368587732315063\n","[Training Epoch 3] Batch 3088, Loss 0.3944361209869385\n","[Training Epoch 3] Batch 3089, Loss 0.39197278022766113\n","[Training Epoch 3] Batch 3090, Loss 0.3753397464752197\n","[Training Epoch 3] Batch 3091, Loss 0.40247511863708496\n","[Training Epoch 3] Batch 3092, Loss 0.4332374334335327\n","[Training Epoch 3] Batch 3093, Loss 0.3651992678642273\n","[Training Epoch 3] Batch 3094, Loss 0.40503305196762085\n","[Training Epoch 3] Batch 3095, Loss 0.4275674819946289\n","[Training Epoch 3] Batch 3096, Loss 0.36750051379203796\n","[Training Epoch 3] Batch 3097, Loss 0.3872021436691284\n","[Training Epoch 3] Batch 3098, Loss 0.3929442763328552\n","[Training Epoch 3] Batch 3099, Loss 0.40166208148002625\n","[Training Epoch 3] Batch 3100, Loss 0.3542344570159912\n","[Training Epoch 3] Batch 3101, Loss 0.3726433515548706\n","[Training Epoch 3] Batch 3102, Loss 0.4106079339981079\n","[Training Epoch 3] Batch 3103, Loss 0.42119842767715454\n","[Training Epoch 3] Batch 3104, Loss 0.39060112833976746\n","[Training Epoch 3] Batch 3105, Loss 0.39007413387298584\n","[Training Epoch 3] Batch 3106, Loss 0.3906553387641907\n","[Training Epoch 3] Batch 3107, Loss 0.3742808699607849\n","[Training Epoch 3] Batch 3108, Loss 0.3989000916481018\n","[Training Epoch 3] Batch 3109, Loss 0.3800511658191681\n","[Training Epoch 3] Batch 3110, Loss 0.3771023750305176\n","[Training Epoch 3] Batch 3111, Loss 0.3881933391094208\n","[Training Epoch 3] Batch 3112, Loss 0.3611759543418884\n","[Training Epoch 3] Batch 3113, Loss 0.39157143235206604\n","[Training Epoch 3] Batch 3114, Loss 0.3872055411338806\n","[Training Epoch 3] Batch 3115, Loss 0.3939407467842102\n","[Training Epoch 3] Batch 3116, Loss 0.4037752151489258\n","[Training Epoch 3] Batch 3117, Loss 0.3632797598838806\n","[Training Epoch 3] Batch 3118, Loss 0.396823525428772\n","[Training Epoch 3] Batch 3119, Loss 0.39084476232528687\n","[Training Epoch 3] Batch 3120, Loss 0.38342052698135376\n","[Training Epoch 3] Batch 3121, Loss 0.42571723461151123\n","[Training Epoch 3] Batch 3122, Loss 0.3782076835632324\n","[Training Epoch 3] Batch 3123, Loss 0.3760673999786377\n","[Training Epoch 3] Batch 3124, Loss 0.38021475076675415\n","[Training Epoch 3] Batch 3125, Loss 0.3806700110435486\n","[Training Epoch 3] Batch 3126, Loss 0.3750683069229126\n","[Training Epoch 3] Batch 3127, Loss 0.4286247491836548\n","[Training Epoch 3] Batch 3128, Loss 0.4163356125354767\n","[Training Epoch 3] Batch 3129, Loss 0.38688042759895325\n","[Training Epoch 3] Batch 3130, Loss 0.3991876244544983\n","[Training Epoch 3] Batch 3131, Loss 0.4006505012512207\n","[Training Epoch 3] Batch 3132, Loss 0.38878288865089417\n","[Training Epoch 3] Batch 3133, Loss 0.4016895890235901\n","[Training Epoch 3] Batch 3134, Loss 0.4075081944465637\n","[Training Epoch 3] Batch 3135, Loss 0.4027049243450165\n","[Training Epoch 3] Batch 3136, Loss 0.4136730432510376\n","[Training Epoch 3] Batch 3137, Loss 0.4020826518535614\n","[Training Epoch 3] Batch 3138, Loss 0.3759914040565491\n","[Training Epoch 3] Batch 3139, Loss 0.39817100763320923\n","[Training Epoch 3] Batch 3140, Loss 0.3706991672515869\n","[Training Epoch 3] Batch 3141, Loss 0.3964514136314392\n","[Training Epoch 3] Batch 3142, Loss 0.40623247623443604\n","[Training Epoch 3] Batch 3143, Loss 0.393919974565506\n","[Training Epoch 3] Batch 3144, Loss 0.3959261178970337\n","[Training Epoch 3] Batch 3145, Loss 0.4120584726333618\n","[Training Epoch 3] Batch 3146, Loss 0.3775559663772583\n","[Training Epoch 3] Batch 3147, Loss 0.3962763249874115\n","[Training Epoch 3] Batch 3148, Loss 0.3688293695449829\n","[Training Epoch 3] Batch 3149, Loss 0.4199070334434509\n","[Training Epoch 3] Batch 3150, Loss 0.38904398679733276\n","[Training Epoch 3] Batch 3151, Loss 0.39567166566848755\n","[Training Epoch 3] Batch 3152, Loss 0.38315701484680176\n","[Training Epoch 3] Batch 3153, Loss 0.4129176735877991\n","[Training Epoch 3] Batch 3154, Loss 0.41242069005966187\n","[Training Epoch 3] Batch 3155, Loss 0.41732001304626465\n","[Training Epoch 3] Batch 3156, Loss 0.4330967366695404\n","[Training Epoch 3] Batch 3157, Loss 0.3775940537452698\n","[Training Epoch 3] Batch 3158, Loss 0.41328322887420654\n","[Training Epoch 3] Batch 3159, Loss 0.40286579728126526\n","[Training Epoch 3] Batch 3160, Loss 0.3841594457626343\n","[Training Epoch 3] Batch 3161, Loss 0.38003337383270264\n","[Training Epoch 3] Batch 3162, Loss 0.39167216420173645\n","[Training Epoch 3] Batch 3163, Loss 0.37880533933639526\n","[Training Epoch 3] Batch 3164, Loss 0.43578416109085083\n","[Training Epoch 3] Batch 3165, Loss 0.40964555740356445\n","[Training Epoch 3] Batch 3166, Loss 0.4267072081565857\n","[Training Epoch 3] Batch 3167, Loss 0.37179866433143616\n","[Training Epoch 3] Batch 3168, Loss 0.41279661655426025\n","[Training Epoch 3] Batch 3169, Loss 0.38746121525764465\n","[Training Epoch 3] Batch 3170, Loss 0.3888617753982544\n","[Training Epoch 3] Batch 3171, Loss 0.39448118209838867\n","[Training Epoch 3] Batch 3172, Loss 0.35239648818969727\n","[Training Epoch 3] Batch 3173, Loss 0.39544957876205444\n","[Training Epoch 3] Batch 3174, Loss 0.37431600689888\n","[Training Epoch 3] Batch 3175, Loss 0.3746829628944397\n","[Training Epoch 3] Batch 3176, Loss 0.3796024024486542\n","[Training Epoch 3] Batch 3177, Loss 0.36985814571380615\n","[Training Epoch 3] Batch 3178, Loss 0.39441734552383423\n","[Training Epoch 3] Batch 3179, Loss 0.3781582713127136\n","[Training Epoch 3] Batch 3180, Loss 0.40014415979385376\n","[Training Epoch 3] Batch 3181, Loss 0.37592947483062744\n","[Training Epoch 3] Batch 3182, Loss 0.38196492195129395\n","[Training Epoch 3] Batch 3183, Loss 0.3805517852306366\n","[Training Epoch 3] Batch 3184, Loss 0.39992713928222656\n","[Training Epoch 3] Batch 3185, Loss 0.401279091835022\n","[Training Epoch 3] Batch 3186, Loss 0.37617820501327515\n","[Training Epoch 3] Batch 3187, Loss 0.3950343132019043\n","[Training Epoch 3] Batch 3188, Loss 0.4189528524875641\n","[Training Epoch 3] Batch 3189, Loss 0.3832061290740967\n","[Training Epoch 3] Batch 3190, Loss 0.4047588109970093\n","[Training Epoch 3] Batch 3191, Loss 0.37138816714286804\n","[Training Epoch 3] Batch 3192, Loss 0.4053073525428772\n","[Training Epoch 3] Batch 3193, Loss 0.3495938777923584\n","[Training Epoch 3] Batch 3194, Loss 0.3934704065322876\n","[Training Epoch 3] Batch 3195, Loss 0.3888724446296692\n","[Training Epoch 3] Batch 3196, Loss 0.40314561128616333\n","[Training Epoch 3] Batch 3197, Loss 0.37088218331336975\n","[Training Epoch 3] Batch 3198, Loss 0.4069659113883972\n","[Training Epoch 3] Batch 3199, Loss 0.39507365226745605\n","[Training Epoch 3] Batch 3200, Loss 0.3912876844406128\n","[Training Epoch 3] Batch 3201, Loss 0.39902690052986145\n","[Training Epoch 3] Batch 3202, Loss 0.39665716886520386\n","[Training Epoch 3] Batch 3203, Loss 0.4107435345649719\n","[Training Epoch 3] Batch 3204, Loss 0.3794028162956238\n","[Training Epoch 3] Batch 3205, Loss 0.3664521872997284\n","[Training Epoch 3] Batch 3206, Loss 0.373818039894104\n","[Training Epoch 3] Batch 3207, Loss 0.39021337032318115\n","[Training Epoch 3] Batch 3208, Loss 0.4215748906135559\n","[Training Epoch 3] Batch 3209, Loss 0.37460869550704956\n","[Training Epoch 3] Batch 3210, Loss 0.40081340074539185\n","[Training Epoch 3] Batch 3211, Loss 0.39373576641082764\n","[Training Epoch 3] Batch 3212, Loss 0.3821234107017517\n","[Training Epoch 3] Batch 3213, Loss 0.3697783946990967\n","[Training Epoch 3] Batch 3214, Loss 0.41700470447540283\n","[Training Epoch 3] Batch 3215, Loss 0.3608720600605011\n","[Training Epoch 3] Batch 3216, Loss 0.3776823878288269\n","[Training Epoch 3] Batch 3217, Loss 0.3934696912765503\n","[Training Epoch 3] Batch 3218, Loss 0.3853628635406494\n","[Training Epoch 3] Batch 3219, Loss 0.39506518840789795\n","[Training Epoch 3] Batch 3220, Loss 0.36259546875953674\n","[Training Epoch 3] Batch 3221, Loss 0.39577168226242065\n","[Training Epoch 3] Batch 3222, Loss 0.4010182321071625\n","[Training Epoch 3] Batch 3223, Loss 0.3759417235851288\n","[Training Epoch 3] Batch 3224, Loss 0.38691434264183044\n","[Training Epoch 3] Batch 3225, Loss 0.3882133960723877\n","[Training Epoch 3] Batch 3226, Loss 0.39814621210098267\n","[Training Epoch 3] Batch 3227, Loss 0.3775659501552582\n","[Training Epoch 3] Batch 3228, Loss 0.3953387439250946\n","[Training Epoch 3] Batch 3229, Loss 0.38942280411720276\n","[Training Epoch 3] Batch 3230, Loss 0.38281023502349854\n","[Training Epoch 3] Batch 3231, Loss 0.383023202419281\n","[Training Epoch 3] Batch 3232, Loss 0.37877142429351807\n","[Training Epoch 3] Batch 3233, Loss 0.38868218660354614\n","[Training Epoch 3] Batch 3234, Loss 0.39207923412323\n","[Training Epoch 3] Batch 3235, Loss 0.37996429204940796\n","[Training Epoch 3] Batch 3236, Loss 0.36983489990234375\n","[Training Epoch 3] Batch 3237, Loss 0.3702296018600464\n","[Training Epoch 3] Batch 3238, Loss 0.39961880445480347\n","[Training Epoch 3] Batch 3239, Loss 0.39442336559295654\n","[Training Epoch 3] Batch 3240, Loss 0.40199869871139526\n","[Training Epoch 3] Batch 3241, Loss 0.37495771050453186\n","[Training Epoch 3] Batch 3242, Loss 0.394835889339447\n","[Training Epoch 3] Batch 3243, Loss 0.39879241585731506\n","[Training Epoch 3] Batch 3244, Loss 0.3848787546157837\n","[Training Epoch 3] Batch 3245, Loss 0.3648614287376404\n","[Training Epoch 3] Batch 3246, Loss 0.35700899362564087\n","[Training Epoch 3] Batch 3247, Loss 0.3809860348701477\n","[Training Epoch 3] Batch 3248, Loss 0.38927221298217773\n","[Training Epoch 3] Batch 3249, Loss 0.3913414180278778\n","[Training Epoch 3] Batch 3250, Loss 0.37528541684150696\n","[Training Epoch 3] Batch 3251, Loss 0.3821260333061218\n","[Training Epoch 3] Batch 3252, Loss 0.37028613686561584\n","[Training Epoch 3] Batch 3253, Loss 0.4086128771305084\n","[Training Epoch 3] Batch 3254, Loss 0.37375378608703613\n","[Training Epoch 3] Batch 3255, Loss 0.389853835105896\n","[Training Epoch 3] Batch 3256, Loss 0.4092670679092407\n","[Training Epoch 3] Batch 3257, Loss 0.40651845932006836\n","[Training Epoch 3] Batch 3258, Loss 0.38812464475631714\n","[Training Epoch 3] Batch 3259, Loss 0.4001579284667969\n","[Training Epoch 3] Batch 3260, Loss 0.3611951172351837\n","[Training Epoch 3] Batch 3261, Loss 0.387371301651001\n","[Training Epoch 3] Batch 3262, Loss 0.3963434100151062\n","[Training Epoch 3] Batch 3263, Loss 0.3823493719100952\n","[Training Epoch 3] Batch 3264, Loss 0.3965192139148712\n","[Training Epoch 3] Batch 3265, Loss 0.40174978971481323\n","[Training Epoch 3] Batch 3266, Loss 0.40622174739837646\n","[Training Epoch 3] Batch 3267, Loss 0.36912792921066284\n","[Training Epoch 3] Batch 3268, Loss 0.38214248418807983\n","[Training Epoch 3] Batch 3269, Loss 0.39129507541656494\n","[Training Epoch 3] Batch 3270, Loss 0.40557757019996643\n","[Training Epoch 3] Batch 3271, Loss 0.3979025185108185\n","[Training Epoch 3] Batch 3272, Loss 0.40321946144104004\n","[Training Epoch 3] Batch 3273, Loss 0.37999677658081055\n","[Training Epoch 3] Batch 3274, Loss 0.40981364250183105\n","[Training Epoch 3] Batch 3275, Loss 0.42182958126068115\n","[Training Epoch 3] Batch 3276, Loss 0.43170881271362305\n","[Training Epoch 3] Batch 3277, Loss 0.3831384479999542\n","[Training Epoch 3] Batch 3278, Loss 0.3771616816520691\n","[Training Epoch 3] Batch 3279, Loss 0.40698838233947754\n","[Training Epoch 3] Batch 3280, Loss 0.3960455060005188\n","[Training Epoch 3] Batch 3281, Loss 0.3775905966758728\n","[Training Epoch 3] Batch 3282, Loss 0.3921108841896057\n","[Training Epoch 3] Batch 3283, Loss 0.38261133432388306\n","[Training Epoch 3] Batch 3284, Loss 0.3836503028869629\n","[Training Epoch 3] Batch 3285, Loss 0.4152737259864807\n","[Training Epoch 3] Batch 3286, Loss 0.3684765696525574\n","[Training Epoch 3] Batch 3287, Loss 0.37656015157699585\n","[Training Epoch 3] Batch 3288, Loss 0.37811967730522156\n","[Training Epoch 3] Batch 3289, Loss 0.39095205068588257\n","[Training Epoch 3] Batch 3290, Loss 0.38728979229927063\n","[Training Epoch 3] Batch 3291, Loss 0.36350297927856445\n","[Training Epoch 3] Batch 3292, Loss 0.39228495955467224\n","[Training Epoch 3] Batch 3293, Loss 0.4021078944206238\n","[Training Epoch 3] Batch 3294, Loss 0.38603082299232483\n","[Training Epoch 3] Batch 3295, Loss 0.3861464858055115\n","[Training Epoch 3] Batch 3296, Loss 0.37853968143463135\n","[Training Epoch 3] Batch 3297, Loss 0.3934326767921448\n","[Training Epoch 3] Batch 3298, Loss 0.36561235785484314\n","[Training Epoch 3] Batch 3299, Loss 0.38550248742103577\n","[Training Epoch 3] Batch 3300, Loss 0.383242666721344\n","[Training Epoch 3] Batch 3301, Loss 0.4191898703575134\n","[Training Epoch 3] Batch 3302, Loss 0.37509816884994507\n","[Training Epoch 3] Batch 3303, Loss 0.37057799100875854\n","[Training Epoch 3] Batch 3304, Loss 0.3829738199710846\n","[Training Epoch 3] Batch 3305, Loss 0.39575958251953125\n","[Training Epoch 3] Batch 3306, Loss 0.3993859887123108\n","[Training Epoch 3] Batch 3307, Loss 0.39626139402389526\n","[Training Epoch 3] Batch 3308, Loss 0.39338400959968567\n","[Training Epoch 3] Batch 3309, Loss 0.39832639694213867\n","[Training Epoch 3] Batch 3310, Loss 0.35424336791038513\n","[Training Epoch 3] Batch 3311, Loss 0.3943018913269043\n","[Training Epoch 3] Batch 3312, Loss 0.4236118793487549\n","[Training Epoch 3] Batch 3313, Loss 0.37425777316093445\n","[Training Epoch 3] Batch 3314, Loss 0.389045387506485\n","[Training Epoch 3] Batch 3315, Loss 0.38169074058532715\n","[Training Epoch 3] Batch 3316, Loss 0.39162296056747437\n","[Training Epoch 3] Batch 3317, Loss 0.3776415288448334\n","[Training Epoch 3] Batch 3318, Loss 0.3845363259315491\n","[Training Epoch 3] Batch 3319, Loss 0.41884952783584595\n","[Training Epoch 3] Batch 3320, Loss 0.3884202837944031\n","[Training Epoch 3] Batch 3321, Loss 0.3953375518321991\n","[Training Epoch 3] Batch 3322, Loss 0.3795003890991211\n","[Training Epoch 3] Batch 3323, Loss 0.37266331911087036\n","[Training Epoch 3] Batch 3324, Loss 0.3801361322402954\n","[Training Epoch 3] Batch 3325, Loss 0.3829115033149719\n","[Training Epoch 3] Batch 3326, Loss 0.3933245539665222\n","[Training Epoch 3] Batch 3327, Loss 0.3720550537109375\n","[Training Epoch 3] Batch 3328, Loss 0.4027526378631592\n","[Training Epoch 3] Batch 3329, Loss 0.38767746090888977\n","[Training Epoch 3] Batch 3330, Loss 0.3928171396255493\n","[Training Epoch 3] Batch 3331, Loss 0.39078080654144287\n","[Training Epoch 3] Batch 3332, Loss 0.36201030015945435\n","[Training Epoch 3] Batch 3333, Loss 0.37741583585739136\n","[Training Epoch 3] Batch 3334, Loss 0.39489707350730896\n","[Training Epoch 3] Batch 3335, Loss 0.42525890469551086\n","[Training Epoch 3] Batch 3336, Loss 0.4082223176956177\n","[Training Epoch 3] Batch 3337, Loss 0.3888593018054962\n","[Training Epoch 3] Batch 3338, Loss 0.38676396012306213\n","[Training Epoch 3] Batch 3339, Loss 0.4010397791862488\n","[Training Epoch 3] Batch 3340, Loss 0.3766383230686188\n","[Training Epoch 3] Batch 3341, Loss 0.4286147356033325\n","[Training Epoch 3] Batch 3342, Loss 0.3799886107444763\n","[Training Epoch 3] Batch 3343, Loss 0.3993971347808838\n","[Training Epoch 3] Batch 3344, Loss 0.38333258032798767\n","[Training Epoch 3] Batch 3345, Loss 0.39459484815597534\n","[Training Epoch 3] Batch 3346, Loss 0.3696252107620239\n","[Training Epoch 3] Batch 3347, Loss 0.376054048538208\n","[Training Epoch 3] Batch 3348, Loss 0.40109437704086304\n","[Training Epoch 3] Batch 3349, Loss 0.38774120807647705\n","[Training Epoch 3] Batch 3350, Loss 0.3742291033267975\n","[Training Epoch 3] Batch 3351, Loss 0.35044795274734497\n","[Training Epoch 3] Batch 3352, Loss 0.36622026562690735\n","[Training Epoch 3] Batch 3353, Loss 0.3897867798805237\n","[Training Epoch 3] Batch 3354, Loss 0.4087035357952118\n","[Training Epoch 3] Batch 3355, Loss 0.40268146991729736\n","[Training Epoch 3] Batch 3356, Loss 0.39396777749061584\n","[Training Epoch 3] Batch 3357, Loss 0.39004936814308167\n","[Training Epoch 3] Batch 3358, Loss 0.40598198771476746\n","[Training Epoch 3] Batch 3359, Loss 0.3632804751396179\n","[Training Epoch 3] Batch 3360, Loss 0.40484434366226196\n","[Training Epoch 3] Batch 3361, Loss 0.39048752188682556\n","[Training Epoch 3] Batch 3362, Loss 0.3670135736465454\n","[Training Epoch 3] Batch 3363, Loss 0.41450223326683044\n","[Training Epoch 3] Batch 3364, Loss 0.3808179497718811\n","[Training Epoch 3] Batch 3365, Loss 0.36492326855659485\n","[Training Epoch 3] Batch 3366, Loss 0.36401960253715515\n","[Training Epoch 3] Batch 3367, Loss 0.38978374004364014\n","[Training Epoch 3] Batch 3368, Loss 0.3689131736755371\n","[Training Epoch 3] Batch 3369, Loss 0.4036112427711487\n","[Training Epoch 3] Batch 3370, Loss 0.4110869765281677\n","[Training Epoch 3] Batch 3371, Loss 0.4084206819534302\n","[Training Epoch 3] Batch 3372, Loss 0.3979596495628357\n","[Training Epoch 3] Batch 3373, Loss 0.389681875705719\n","[Training Epoch 3] Batch 3374, Loss 0.37695246934890747\n","[Training Epoch 3] Batch 3375, Loss 0.39703696966171265\n","[Training Epoch 3] Batch 3376, Loss 0.3668960928916931\n","[Training Epoch 3] Batch 3377, Loss 0.4190146327018738\n","[Training Epoch 3] Batch 3378, Loss 0.36711251735687256\n","[Training Epoch 3] Batch 3379, Loss 0.395940899848938\n","[Training Epoch 3] Batch 3380, Loss 0.4058929979801178\n","[Training Epoch 3] Batch 3381, Loss 0.3841322958469391\n","[Training Epoch 3] Batch 3382, Loss 0.4387568533420563\n","[Training Epoch 3] Batch 3383, Loss 0.37324321269989014\n","[Training Epoch 3] Batch 3384, Loss 0.41309207677841187\n","[Training Epoch 3] Batch 3385, Loss 0.3945204019546509\n","[Training Epoch 3] Batch 3386, Loss 0.39786845445632935\n","[Training Epoch 3] Batch 3387, Loss 0.37557369470596313\n","[Training Epoch 3] Batch 3388, Loss 0.3771292567253113\n","[Training Epoch 3] Batch 3389, Loss 0.3851938545703888\n","[Training Epoch 3] Batch 3390, Loss 0.42218154668807983\n","[Training Epoch 3] Batch 3391, Loss 0.3998400568962097\n","[Training Epoch 3] Batch 3392, Loss 0.3614446520805359\n","[Training Epoch 3] Batch 3393, Loss 0.3965702950954437\n","[Training Epoch 3] Batch 3394, Loss 0.3945637345314026\n","[Training Epoch 3] Batch 3395, Loss 0.3481817841529846\n","[Training Epoch 3] Batch 3396, Loss 0.38022148609161377\n","[Training Epoch 3] Batch 3397, Loss 0.3823898434638977\n","[Training Epoch 3] Batch 3398, Loss 0.3718453645706177\n","[Training Epoch 3] Batch 3399, Loss 0.39760828018188477\n","[Training Epoch 3] Batch 3400, Loss 0.4201437532901764\n","[Training Epoch 3] Batch 3401, Loss 0.3790268898010254\n","[Training Epoch 3] Batch 3402, Loss 0.35909759998321533\n","[Training Epoch 3] Batch 3403, Loss 0.37314265966415405\n","[Training Epoch 3] Batch 3404, Loss 0.3696756660938263\n","[Training Epoch 3] Batch 3405, Loss 0.4001830220222473\n","[Training Epoch 3] Batch 3406, Loss 0.398751437664032\n","[Training Epoch 3] Batch 3407, Loss 0.3940425515174866\n","[Training Epoch 3] Batch 3408, Loss 0.3820737600326538\n","[Training Epoch 3] Batch 3409, Loss 0.3906136751174927\n","[Training Epoch 3] Batch 3410, Loss 0.37392380833625793\n","[Training Epoch 3] Batch 3411, Loss 0.3675851821899414\n","[Training Epoch 3] Batch 3412, Loss 0.36506107449531555\n","[Training Epoch 3] Batch 3413, Loss 0.38568055629730225\n","[Training Epoch 3] Batch 3414, Loss 0.3667530417442322\n","[Training Epoch 3] Batch 3415, Loss 0.4004307985305786\n","[Training Epoch 3] Batch 3416, Loss 0.3811810612678528\n","[Training Epoch 3] Batch 3417, Loss 0.38940057158470154\n","[Training Epoch 3] Batch 3418, Loss 0.38371139764785767\n","[Training Epoch 3] Batch 3419, Loss 0.37947878241539\n","[Training Epoch 3] Batch 3420, Loss 0.41470852494239807\n","[Training Epoch 3] Batch 3421, Loss 0.40422672033309937\n","[Training Epoch 3] Batch 3422, Loss 0.36002302169799805\n","[Training Epoch 3] Batch 3423, Loss 0.3766665458679199\n","[Training Epoch 3] Batch 3424, Loss 0.394977867603302\n","[Training Epoch 3] Batch 3425, Loss 0.3645273447036743\n","[Training Epoch 3] Batch 3426, Loss 0.35178342461586\n","[Training Epoch 3] Batch 3427, Loss 0.35098665952682495\n","[Training Epoch 3] Batch 3428, Loss 0.39120256900787354\n","[Training Epoch 3] Batch 3429, Loss 0.3599395751953125\n","[Training Epoch 3] Batch 3430, Loss 0.3920011520385742\n","[Training Epoch 3] Batch 3431, Loss 0.39146292209625244\n","[Training Epoch 3] Batch 3432, Loss 0.366727352142334\n","[Training Epoch 3] Batch 3433, Loss 0.39276283979415894\n","[Training Epoch 3] Batch 3434, Loss 0.38050252199172974\n","[Training Epoch 3] Batch 3435, Loss 0.37605252861976624\n","[Training Epoch 3] Batch 3436, Loss 0.3862748444080353\n","[Training Epoch 3] Batch 3437, Loss 0.38069891929626465\n","[Training Epoch 3] Batch 3438, Loss 0.3956727385520935\n","[Training Epoch 3] Batch 3439, Loss 0.4006354510784149\n","[Training Epoch 3] Batch 3440, Loss 0.39329421520233154\n","[Training Epoch 3] Batch 3441, Loss 0.37617960572242737\n","[Training Epoch 3] Batch 3442, Loss 0.3990674912929535\n","[Training Epoch 3] Batch 3443, Loss 0.37918996810913086\n","[Training Epoch 3] Batch 3444, Loss 0.36831825971603394\n","[Training Epoch 3] Batch 3445, Loss 0.3730213940143585\n","[Training Epoch 3] Batch 3446, Loss 0.38951432704925537\n","[Training Epoch 3] Batch 3447, Loss 0.39169934391975403\n","[Training Epoch 3] Batch 3448, Loss 0.3772166669368744\n","[Training Epoch 3] Batch 3449, Loss 0.4098559319972992\n","[Training Epoch 3] Batch 3450, Loss 0.3849179148674011\n","[Training Epoch 3] Batch 3451, Loss 0.3848283588886261\n","[Training Epoch 3] Batch 3452, Loss 0.39380067586898804\n","[Training Epoch 3] Batch 3453, Loss 0.3942224681377411\n","[Training Epoch 3] Batch 3454, Loss 0.3646888732910156\n","[Training Epoch 3] Batch 3455, Loss 0.36888769268989563\n","[Training Epoch 3] Batch 3456, Loss 0.40081286430358887\n","[Training Epoch 3] Batch 3457, Loss 0.3719838857650757\n","[Training Epoch 3] Batch 3458, Loss 0.39067357778549194\n","[Training Epoch 3] Batch 3459, Loss 0.389213502407074\n","[Training Epoch 3] Batch 3460, Loss 0.33051133155822754\n","[Training Epoch 3] Batch 3461, Loss 0.37983614206314087\n","[Training Epoch 3] Batch 3462, Loss 0.38132378458976746\n","[Training Epoch 3] Batch 3463, Loss 0.3898715376853943\n","[Training Epoch 3] Batch 3464, Loss 0.3959241509437561\n","[Training Epoch 3] Batch 3465, Loss 0.3610970973968506\n","[Training Epoch 3] Batch 3466, Loss 0.39745599031448364\n","[Training Epoch 3] Batch 3467, Loss 0.3480421006679535\n","[Training Epoch 3] Batch 3468, Loss 0.374172180891037\n","[Training Epoch 3] Batch 3469, Loss 0.40018391609191895\n","[Training Epoch 3] Batch 3470, Loss 0.3938848674297333\n","[Training Epoch 3] Batch 3471, Loss 0.37569984793663025\n","[Training Epoch 3] Batch 3472, Loss 0.4022173285484314\n","[Training Epoch 3] Batch 3473, Loss 0.37522053718566895\n","[Training Epoch 3] Batch 3474, Loss 0.3876119554042816\n","[Training Epoch 3] Batch 3475, Loss 0.35930219292640686\n","[Training Epoch 3] Batch 3476, Loss 0.343097448348999\n","[Training Epoch 3] Batch 3477, Loss 0.4192394018173218\n","[Training Epoch 3] Batch 3478, Loss 0.40965211391448975\n","[Training Epoch 3] Batch 3479, Loss 0.4101423919200897\n","[Training Epoch 3] Batch 3480, Loss 0.41588446497917175\n","[Training Epoch 3] Batch 3481, Loss 0.36214929819107056\n","[Training Epoch 3] Batch 3482, Loss 0.37468862533569336\n","[Training Epoch 3] Batch 3483, Loss 0.40089356899261475\n","[Training Epoch 3] Batch 3484, Loss 0.3786970376968384\n","[Training Epoch 3] Batch 3485, Loss 0.3966110348701477\n","[Training Epoch 3] Batch 3486, Loss 0.36521780490875244\n","[Training Epoch 3] Batch 3487, Loss 0.3900114595890045\n","[Training Epoch 3] Batch 3488, Loss 0.3866605758666992\n","[Training Epoch 3] Batch 3489, Loss 0.4075435400009155\n","[Training Epoch 3] Batch 3490, Loss 0.37787118554115295\n","[Training Epoch 3] Batch 3491, Loss 0.38554006814956665\n","[Training Epoch 3] Batch 3492, Loss 0.3752896785736084\n","[Training Epoch 3] Batch 3493, Loss 0.36405524611473083\n","[Training Epoch 3] Batch 3494, Loss 0.4011804163455963\n","[Training Epoch 3] Batch 3495, Loss 0.3969026505947113\n","[Training Epoch 3] Batch 3496, Loss 0.3720201551914215\n","[Training Epoch 3] Batch 3497, Loss 0.372519314289093\n","[Training Epoch 3] Batch 3498, Loss 0.3387494385242462\n","[Training Epoch 3] Batch 3499, Loss 0.3774406313896179\n","[Training Epoch 3] Batch 3500, Loss 0.3742150068283081\n","[Training Epoch 3] Batch 3501, Loss 0.35921794176101685\n","[Training Epoch 3] Batch 3502, Loss 0.3857545852661133\n","[Training Epoch 3] Batch 3503, Loss 0.37235304713249207\n","[Training Epoch 3] Batch 3504, Loss 0.3812143802642822\n","[Training Epoch 3] Batch 3505, Loss 0.358852356672287\n","[Training Epoch 3] Batch 3506, Loss 0.3972463607788086\n","[Training Epoch 3] Batch 3507, Loss 0.37569671869277954\n","[Training Epoch 3] Batch 3508, Loss 0.35403305292129517\n","[Training Epoch 3] Batch 3509, Loss 0.3873789608478546\n","[Training Epoch 3] Batch 3510, Loss 0.3823363184928894\n","[Training Epoch 3] Batch 3511, Loss 0.3755423426628113\n","[Training Epoch 3] Batch 3512, Loss 0.3811170756816864\n","[Training Epoch 3] Batch 3513, Loss 0.392045795917511\n","[Training Epoch 3] Batch 3514, Loss 0.40771785378456116\n","[Training Epoch 3] Batch 3515, Loss 0.3917159140110016\n","[Training Epoch 3] Batch 3516, Loss 0.37729108333587646\n","[Training Epoch 3] Batch 3517, Loss 0.3931281566619873\n","[Training Epoch 3] Batch 3518, Loss 0.39184805750846863\n","[Training Epoch 3] Batch 3519, Loss 0.4135153591632843\n","[Training Epoch 3] Batch 3520, Loss 0.39275553822517395\n","[Training Epoch 3] Batch 3521, Loss 0.4064810276031494\n","[Training Epoch 3] Batch 3522, Loss 0.37401387095451355\n","[Training Epoch 3] Batch 3523, Loss 0.362574964761734\n","[Training Epoch 3] Batch 3524, Loss 0.37545621395111084\n","[Training Epoch 3] Batch 3525, Loss 0.3580043613910675\n","[Training Epoch 3] Batch 3526, Loss 0.3695675730705261\n","[Training Epoch 3] Batch 3527, Loss 0.40863025188446045\n","[Training Epoch 3] Batch 3528, Loss 0.4018753170967102\n","[Training Epoch 3] Batch 3529, Loss 0.3944150507450104\n","[Training Epoch 3] Batch 3530, Loss 0.39355963468551636\n","[Training Epoch 3] Batch 3531, Loss 0.39201825857162476\n","[Training Epoch 3] Batch 3532, Loss 0.40861988067626953\n","[Training Epoch 3] Batch 3533, Loss 0.38614892959594727\n","[Training Epoch 3] Batch 3534, Loss 0.37462031841278076\n","[Training Epoch 3] Batch 3535, Loss 0.36118805408477783\n","[Training Epoch 3] Batch 3536, Loss 0.3797135353088379\n","[Training Epoch 3] Batch 3537, Loss 0.40834957361221313\n","[Training Epoch 3] Batch 3538, Loss 0.40747153759002686\n","[Training Epoch 3] Batch 3539, Loss 0.40584003925323486\n","[Training Epoch 3] Batch 3540, Loss 0.3769494891166687\n","[Training Epoch 3] Batch 3541, Loss 0.37729692459106445\n","[Training Epoch 3] Batch 3542, Loss 0.3738916516304016\n","[Training Epoch 3] Batch 3543, Loss 0.3926284611225128\n","[Training Epoch 3] Batch 3544, Loss 0.37083280086517334\n","[Training Epoch 3] Batch 3545, Loss 0.3887880742549896\n","[Training Epoch 3] Batch 3546, Loss 0.37639889121055603\n","[Training Epoch 3] Batch 3547, Loss 0.3824111223220825\n","[Training Epoch 3] Batch 3548, Loss 0.4003404676914215\n","[Training Epoch 3] Batch 3549, Loss 0.3851083219051361\n","[Training Epoch 3] Batch 3550, Loss 0.3835669159889221\n","[Training Epoch 3] Batch 3551, Loss 0.3728143572807312\n","[Training Epoch 3] Batch 3552, Loss 0.41725724935531616\n","[Training Epoch 3] Batch 3553, Loss 0.35161611437797546\n","[Training Epoch 3] Batch 3554, Loss 0.3819090723991394\n","[Training Epoch 3] Batch 3555, Loss 0.3961731791496277\n","[Training Epoch 3] Batch 3556, Loss 0.359987735748291\n","[Training Epoch 3] Batch 3557, Loss 0.3716817796230316\n","[Training Epoch 3] Batch 3558, Loss 0.4119267463684082\n","[Training Epoch 3] Batch 3559, Loss 0.3880341649055481\n","[Training Epoch 3] Batch 3560, Loss 0.37933915853500366\n","[Training Epoch 3] Batch 3561, Loss 0.36964935064315796\n","[Training Epoch 3] Batch 3562, Loss 0.36743462085723877\n","[Training Epoch 3] Batch 3563, Loss 0.365660160779953\n","[Training Epoch 3] Batch 3564, Loss 0.39400699734687805\n","[Training Epoch 3] Batch 3565, Loss 0.3952852785587311\n","[Training Epoch 3] Batch 3566, Loss 0.37551212310791016\n","[Training Epoch 3] Batch 3567, Loss 0.3468106985092163\n","[Training Epoch 3] Batch 3568, Loss 0.3864595592021942\n","[Training Epoch 3] Batch 3569, Loss 0.3745591938495636\n","[Training Epoch 3] Batch 3570, Loss 0.3853835165500641\n","[Training Epoch 3] Batch 3571, Loss 0.39962852001190186\n","[Training Epoch 3] Batch 3572, Loss 0.41711124777793884\n","[Training Epoch 3] Batch 3573, Loss 0.3760034441947937\n","[Training Epoch 3] Batch 3574, Loss 0.40532395243644714\n","[Training Epoch 3] Batch 3575, Loss 0.40142402052879333\n","[Training Epoch 3] Batch 3576, Loss 0.4160488247871399\n","[Training Epoch 3] Batch 3577, Loss 0.39883309602737427\n","[Training Epoch 3] Batch 3578, Loss 0.37315765023231506\n","[Training Epoch 3] Batch 3579, Loss 0.40494006872177124\n","[Training Epoch 3] Batch 3580, Loss 0.3597102761268616\n","[Training Epoch 3] Batch 3581, Loss 0.3847680985927582\n","[Training Epoch 3] Batch 3582, Loss 0.3742126524448395\n","[Training Epoch 3] Batch 3583, Loss 0.4083715081214905\n","[Training Epoch 3] Batch 3584, Loss 0.4011365473270416\n","[Training Epoch 3] Batch 3585, Loss 0.3865981101989746\n","[Training Epoch 3] Batch 3586, Loss 0.3940853476524353\n","[Training Epoch 3] Batch 3587, Loss 0.3736344575881958\n","[Training Epoch 3] Batch 3588, Loss 0.3604550361633301\n","[Training Epoch 3] Batch 3589, Loss 0.3845433294773102\n","[Training Epoch 3] Batch 3590, Loss 0.3906171917915344\n","[Training Epoch 3] Batch 3591, Loss 0.3812692165374756\n","[Training Epoch 3] Batch 3592, Loss 0.3844701051712036\n","[Training Epoch 3] Batch 3593, Loss 0.36951780319213867\n","[Training Epoch 3] Batch 3594, Loss 0.39973968267440796\n","[Training Epoch 3] Batch 3595, Loss 0.41170549392700195\n","[Training Epoch 3] Batch 3596, Loss 0.370214581489563\n","[Training Epoch 3] Batch 3597, Loss 0.38998016715049744\n","[Training Epoch 3] Batch 3598, Loss 0.37309783697128296\n","[Training Epoch 3] Batch 3599, Loss 0.4198528528213501\n","[Training Epoch 3] Batch 3600, Loss 0.38457536697387695\n","[Training Epoch 3] Batch 3601, Loss 0.33936354517936707\n","[Training Epoch 3] Batch 3602, Loss 0.3933528661727905\n","[Training Epoch 3] Batch 3603, Loss 0.3846502900123596\n","[Training Epoch 3] Batch 3604, Loss 0.38839125633239746\n","[Training Epoch 3] Batch 3605, Loss 0.39859539270401\n","[Training Epoch 3] Batch 3606, Loss 0.3799465298652649\n","[Training Epoch 3] Batch 3607, Loss 0.3761386275291443\n","[Training Epoch 3] Batch 3608, Loss 0.381900817155838\n","[Training Epoch 3] Batch 3609, Loss 0.4060577154159546\n","[Training Epoch 3] Batch 3610, Loss 0.37334558367729187\n","[Training Epoch 3] Batch 3611, Loss 0.38987571001052856\n","[Training Epoch 3] Batch 3612, Loss 0.4132460355758667\n","[Training Epoch 3] Batch 3613, Loss 0.3836648464202881\n","[Training Epoch 3] Batch 3614, Loss 0.41187232732772827\n","[Training Epoch 3] Batch 3615, Loss 0.3885763883590698\n","[Training Epoch 3] Batch 3616, Loss 0.3718477487564087\n","[Training Epoch 3] Batch 3617, Loss 0.3661174178123474\n","[Training Epoch 3] Batch 3618, Loss 0.4109048843383789\n","[Training Epoch 3] Batch 3619, Loss 0.3976779878139496\n","[Training Epoch 3] Batch 3620, Loss 0.4090399146080017\n","[Training Epoch 3] Batch 3621, Loss 0.37427037954330444\n","[Training Epoch 3] Batch 3622, Loss 0.3435792028903961\n","[Training Epoch 3] Batch 3623, Loss 0.4095585346221924\n","[Training Epoch 3] Batch 3624, Loss 0.3691326379776001\n","[Training Epoch 3] Batch 3625, Loss 0.3841856122016907\n","[Training Epoch 3] Batch 3626, Loss 0.4071429669857025\n","[Training Epoch 3] Batch 3627, Loss 0.4063153862953186\n","[Training Epoch 3] Batch 3628, Loss 0.356247216463089\n","[Training Epoch 3] Batch 3629, Loss 0.3999982178211212\n","[Training Epoch 3] Batch 3630, Loss 0.3774804174900055\n","[Training Epoch 3] Batch 3631, Loss 0.37263840436935425\n","[Training Epoch 3] Batch 3632, Loss 0.39719974994659424\n","[Training Epoch 3] Batch 3633, Loss 0.39056164026260376\n","[Training Epoch 3] Batch 3634, Loss 0.38863393664360046\n","[Training Epoch 3] Batch 3635, Loss 0.38570666313171387\n","[Training Epoch 3] Batch 3636, Loss 0.3978445529937744\n","[Training Epoch 3] Batch 3637, Loss 0.35077762603759766\n","[Training Epoch 3] Batch 3638, Loss 0.38631027936935425\n","[Training Epoch 3] Batch 3639, Loss 0.36521440744400024\n","[Training Epoch 3] Batch 3640, Loss 0.3871293067932129\n","[Training Epoch 3] Batch 3641, Loss 0.4141651690006256\n","[Training Epoch 3] Batch 3642, Loss 0.39836713671684265\n","[Training Epoch 3] Batch 3643, Loss 0.3778838813304901\n","[Training Epoch 3] Batch 3644, Loss 0.3874686658382416\n","[Training Epoch 3] Batch 3645, Loss 0.3518228530883789\n","[Training Epoch 3] Batch 3646, Loss 0.37373268604278564\n","[Training Epoch 3] Batch 3647, Loss 0.36973243951797485\n","[Training Epoch 3] Batch 3648, Loss 0.37126266956329346\n","[Training Epoch 3] Batch 3649, Loss 0.4139900803565979\n","[Training Epoch 3] Batch 3650, Loss 0.3510151505470276\n","[Training Epoch 3] Batch 3651, Loss 0.40854412317276\n","[Training Epoch 3] Batch 3652, Loss 0.3878176808357239\n","[Training Epoch 3] Batch 3653, Loss 0.39446404576301575\n","[Training Epoch 3] Batch 3654, Loss 0.39951014518737793\n","[Training Epoch 3] Batch 3655, Loss 0.35491690039634705\n","[Training Epoch 3] Batch 3656, Loss 0.37542006373405457\n","[Training Epoch 3] Batch 3657, Loss 0.38722261786460876\n","[Training Epoch 3] Batch 3658, Loss 0.41176265478134155\n","[Training Epoch 3] Batch 3659, Loss 0.36950069665908813\n","[Training Epoch 3] Batch 3660, Loss 0.3648339509963989\n","[Training Epoch 3] Batch 3661, Loss 0.36994338035583496\n","[Training Epoch 3] Batch 3662, Loss 0.36921799182891846\n","[Training Epoch 3] Batch 3663, Loss 0.4007130265235901\n","[Training Epoch 3] Batch 3664, Loss 0.3849533200263977\n","[Training Epoch 3] Batch 3665, Loss 0.401540607213974\n","[Training Epoch 3] Batch 3666, Loss 0.35507702827453613\n","[Training Epoch 3] Batch 3667, Loss 0.36266544461250305\n","[Training Epoch 3] Batch 3668, Loss 0.3755529522895813\n","[Training Epoch 3] Batch 3669, Loss 0.4002259373664856\n","[Training Epoch 3] Batch 3670, Loss 0.35830122232437134\n","[Training Epoch 3] Batch 3671, Loss 0.4028812348842621\n","[Training Epoch 3] Batch 3672, Loss 0.38737753033638\n","[Training Epoch 3] Batch 3673, Loss 0.3731783330440521\n","[Training Epoch 3] Batch 3674, Loss 0.4096079468727112\n","[Training Epoch 3] Batch 3675, Loss 0.37445610761642456\n","[Training Epoch 3] Batch 3676, Loss 0.39262011647224426\n","[Training Epoch 3] Batch 3677, Loss 0.38723039627075195\n","[Training Epoch 3] Batch 3678, Loss 0.38720715045928955\n","[Training Epoch 3] Batch 3679, Loss 0.3961789608001709\n","[Training Epoch 3] Batch 3680, Loss 0.4125843048095703\n","[Training Epoch 3] Batch 3681, Loss 0.38083282113075256\n","[Training Epoch 3] Batch 3682, Loss 0.3846540153026581\n","[Training Epoch 3] Batch 3683, Loss 0.38033854961395264\n","[Training Epoch 3] Batch 3684, Loss 0.3978710472583771\n","[Training Epoch 3] Batch 3685, Loss 0.3621014952659607\n","[Training Epoch 3] Batch 3686, Loss 0.3874993324279785\n","[Training Epoch 3] Batch 3687, Loss 0.37750983238220215\n","[Training Epoch 3] Batch 3688, Loss 0.3707956075668335\n","[Training Epoch 3] Batch 3689, Loss 0.372155100107193\n","[Training Epoch 3] Batch 3690, Loss 0.41918468475341797\n","[Training Epoch 3] Batch 3691, Loss 0.37629082798957825\n","[Training Epoch 3] Batch 3692, Loss 0.39120548963546753\n","[Training Epoch 3] Batch 3693, Loss 0.3821974992752075\n","[Training Epoch 3] Batch 3694, Loss 0.40939784049987793\n","[Training Epoch 3] Batch 3695, Loss 0.3718861937522888\n","[Training Epoch 3] Batch 3696, Loss 0.3835487961769104\n","[Training Epoch 3] Batch 3697, Loss 0.3993479013442993\n","[Training Epoch 3] Batch 3698, Loss 0.40332093834877014\n","[Training Epoch 3] Batch 3699, Loss 0.3657495975494385\n","[Training Epoch 3] Batch 3700, Loss 0.38274693489074707\n","[Training Epoch 3] Batch 3701, Loss 0.40395793318748474\n","[Training Epoch 3] Batch 3702, Loss 0.41716694831848145\n","[Training Epoch 3] Batch 3703, Loss 0.39554452896118164\n","[Training Epoch 3] Batch 3704, Loss 0.4068652391433716\n","[Training Epoch 3] Batch 3705, Loss 0.36141887307167053\n","[Training Epoch 3] Batch 3706, Loss 0.39144936203956604\n","[Training Epoch 3] Batch 3707, Loss 0.395961195230484\n","[Training Epoch 3] Batch 3708, Loss 0.40317070484161377\n","[Training Epoch 3] Batch 3709, Loss 0.391702264547348\n","[Training Epoch 3] Batch 3710, Loss 0.39779412746429443\n","[Training Epoch 3] Batch 3711, Loss 0.39076751470565796\n","[Training Epoch 3] Batch 3712, Loss 0.40380316972732544\n","[Training Epoch 3] Batch 3713, Loss 0.3800623416900635\n","[Training Epoch 3] Batch 3714, Loss 0.3816099166870117\n","[Training Epoch 3] Batch 3715, Loss 0.39907652139663696\n","[Training Epoch 3] Batch 3716, Loss 0.40265512466430664\n","[Training Epoch 3] Batch 3717, Loss 0.394304096698761\n","[Training Epoch 3] Batch 3718, Loss 0.35982662439346313\n","[Training Epoch 3] Batch 3719, Loss 0.4011904001235962\n","[Training Epoch 3] Batch 3720, Loss 0.3901107907295227\n","[Training Epoch 3] Batch 3721, Loss 0.37009909749031067\n","[Training Epoch 3] Batch 3722, Loss 0.3808295726776123\n","[Training Epoch 3] Batch 3723, Loss 0.38652777671813965\n","[Training Epoch 3] Batch 3724, Loss 0.366809606552124\n","[Training Epoch 3] Batch 3725, Loss 0.3871864080429077\n","[Training Epoch 3] Batch 3726, Loss 0.3764580190181732\n","[Training Epoch 3] Batch 3727, Loss 0.3721182346343994\n","[Training Epoch 3] Batch 3728, Loss 0.3969394564628601\n","[Training Epoch 3] Batch 3729, Loss 0.37333133816719055\n","[Training Epoch 3] Batch 3730, Loss 0.36905524134635925\n","[Training Epoch 3] Batch 3731, Loss 0.4176753759384155\n","[Training Epoch 3] Batch 3732, Loss 0.4067520499229431\n","[Training Epoch 3] Batch 3733, Loss 0.39142143726348877\n","[Training Epoch 3] Batch 3734, Loss 0.3865993022918701\n","[Training Epoch 3] Batch 3735, Loss 0.36259445548057556\n","[Training Epoch 3] Batch 3736, Loss 0.3747238516807556\n","[Training Epoch 3] Batch 3737, Loss 0.3903672993183136\n","[Training Epoch 3] Batch 3738, Loss 0.39249640703201294\n","[Training Epoch 3] Batch 3739, Loss 0.3999561071395874\n","[Training Epoch 3] Batch 3740, Loss 0.39898279309272766\n","[Training Epoch 3] Batch 3741, Loss 0.391620934009552\n","[Training Epoch 3] Batch 3742, Loss 0.3813345432281494\n","[Training Epoch 3] Batch 3743, Loss 0.3951246440410614\n","[Training Epoch 3] Batch 3744, Loss 0.4196604788303375\n","[Training Epoch 3] Batch 3745, Loss 0.3886420726776123\n","[Training Epoch 3] Batch 3746, Loss 0.3726045489311218\n","[Training Epoch 3] Batch 3747, Loss 0.36466583609580994\n","[Training Epoch 3] Batch 3748, Loss 0.3812900185585022\n","[Training Epoch 3] Batch 3749, Loss 0.38549870252609253\n","[Training Epoch 3] Batch 3750, Loss 0.35707101225852966\n","[Training Epoch 3] Batch 3751, Loss 0.4219350814819336\n","[Training Epoch 3] Batch 3752, Loss 0.3534056544303894\n","[Training Epoch 3] Batch 3753, Loss 0.37391892075538635\n","[Training Epoch 3] Batch 3754, Loss 0.38766157627105713\n","[Training Epoch 3] Batch 3755, Loss 0.4090649485588074\n","[Training Epoch 3] Batch 3756, Loss 0.39240527153015137\n","[Training Epoch 3] Batch 3757, Loss 0.3886072635650635\n","[Training Epoch 3] Batch 3758, Loss 0.38028833270072937\n","[Training Epoch 3] Batch 3759, Loss 0.39634817838668823\n","[Training Epoch 3] Batch 3760, Loss 0.3874138295650482\n","[Training Epoch 3] Batch 3761, Loss 0.3542553186416626\n","[Training Epoch 3] Batch 3762, Loss 0.3797275424003601\n","[Training Epoch 3] Batch 3763, Loss 0.4096924662590027\n","[Training Epoch 3] Batch 3764, Loss 0.42760568857192993\n","[Training Epoch 3] Batch 3765, Loss 0.35995811223983765\n","[Training Epoch 3] Batch 3766, Loss 0.39568567276000977\n","[Training Epoch 3] Batch 3767, Loss 0.41566959023475647\n","[Training Epoch 3] Batch 3768, Loss 0.4008750915527344\n","[Training Epoch 3] Batch 3769, Loss 0.35883957147598267\n","[Training Epoch 3] Batch 3770, Loss 0.37655022740364075\n","[Training Epoch 3] Batch 3771, Loss 0.3922274112701416\n","[Training Epoch 3] Batch 3772, Loss 0.36108142137527466\n","[Training Epoch 3] Batch 3773, Loss 0.38380587100982666\n","[Training Epoch 3] Batch 3774, Loss 0.36854538321495056\n","[Training Epoch 3] Batch 3775, Loss 0.39120325446128845\n","[Training Epoch 3] Batch 3776, Loss 0.3938083052635193\n","[Training Epoch 3] Batch 3777, Loss 0.3780721426010132\n","[Training Epoch 3] Batch 3778, Loss 0.3731288015842438\n","[Training Epoch 3] Batch 3779, Loss 0.3714574873447418\n","[Training Epoch 3] Batch 3780, Loss 0.34832698106765747\n","[Training Epoch 3] Batch 3781, Loss 0.40918803215026855\n","[Training Epoch 3] Batch 3782, Loss 0.3718678951263428\n","[Training Epoch 3] Batch 3783, Loss 0.3912367820739746\n","[Training Epoch 3] Batch 3784, Loss 0.3405298590660095\n","[Training Epoch 3] Batch 3785, Loss 0.35883891582489014\n","[Training Epoch 3] Batch 3786, Loss 0.3697893023490906\n","[Training Epoch 3] Batch 3787, Loss 0.3802505135536194\n","[Training Epoch 3] Batch 3788, Loss 0.3743332624435425\n","[Training Epoch 3] Batch 3789, Loss 0.4023182988166809\n","[Training Epoch 3] Batch 3790, Loss 0.38478901982307434\n","[Training Epoch 3] Batch 3791, Loss 0.37568676471710205\n","[Training Epoch 3] Batch 3792, Loss 0.3876279294490814\n","[Training Epoch 3] Batch 3793, Loss 0.4020841717720032\n","[Training Epoch 3] Batch 3794, Loss 0.4183776378631592\n","[Training Epoch 3] Batch 3795, Loss 0.3805866539478302\n","[Training Epoch 3] Batch 3796, Loss 0.3873487710952759\n","[Training Epoch 3] Batch 3797, Loss 0.3725680112838745\n","[Training Epoch 3] Batch 3798, Loss 0.357510507106781\n","[Training Epoch 3] Batch 3799, Loss 0.3889126777648926\n","[Training Epoch 3] Batch 3800, Loss 0.3897539973258972\n","[Training Epoch 3] Batch 3801, Loss 0.4226933717727661\n","[Training Epoch 3] Batch 3802, Loss 0.3577667772769928\n","[Training Epoch 3] Batch 3803, Loss 0.406872034072876\n","[Training Epoch 3] Batch 3804, Loss 0.39050158858299255\n","[Training Epoch 3] Batch 3805, Loss 0.36931872367858887\n","[Training Epoch 3] Batch 3806, Loss 0.36591851711273193\n","[Training Epoch 3] Batch 3807, Loss 0.3908061385154724\n","[Training Epoch 3] Batch 3808, Loss 0.3792823255062103\n","[Training Epoch 3] Batch 3809, Loss 0.38345760107040405\n","[Training Epoch 3] Batch 3810, Loss 0.3833252191543579\n","[Training Epoch 3] Batch 3811, Loss 0.36229732632637024\n","[Training Epoch 3] Batch 3812, Loss 0.3741097152233124\n","[Training Epoch 3] Batch 3813, Loss 0.3829069137573242\n","[Training Epoch 3] Batch 3814, Loss 0.3868405818939209\n","[Training Epoch 3] Batch 3815, Loss 0.37825965881347656\n","[Training Epoch 3] Batch 3816, Loss 0.40226975083351135\n","[Training Epoch 3] Batch 3817, Loss 0.3851645588874817\n","[Training Epoch 3] Batch 3818, Loss 0.3776630759239197\n","[Training Epoch 3] Batch 3819, Loss 0.39595407247543335\n","[Training Epoch 3] Batch 3820, Loss 0.37766891717910767\n","[Training Epoch 3] Batch 3821, Loss 0.35560542345046997\n","[Training Epoch 3] Batch 3822, Loss 0.39721032977104187\n","[Training Epoch 3] Batch 3823, Loss 0.4348607659339905\n","[Training Epoch 3] Batch 3824, Loss 0.3679080307483673\n","[Training Epoch 3] Batch 3825, Loss 0.40128183364868164\n","[Training Epoch 3] Batch 3826, Loss 0.3823610246181488\n","[Training Epoch 3] Batch 3827, Loss 0.40099021792411804\n","[Training Epoch 3] Batch 3828, Loss 0.43097978830337524\n","[Training Epoch 3] Batch 3829, Loss 0.3673836588859558\n","[Training Epoch 3] Batch 3830, Loss 0.40396156907081604\n","[Training Epoch 3] Batch 3831, Loss 0.3814011216163635\n","[Training Epoch 3] Batch 3832, Loss 0.38248130679130554\n","[Training Epoch 3] Batch 3833, Loss 0.41178640723228455\n","[Training Epoch 3] Batch 3834, Loss 0.3870369791984558\n","[Training Epoch 3] Batch 3835, Loss 0.40410780906677246\n","[Training Epoch 3] Batch 3836, Loss 0.3880547285079956\n","[Training Epoch 3] Batch 3837, Loss 0.3628131151199341\n","[Training Epoch 3] Batch 3838, Loss 0.3955838084220886\n","[Training Epoch 3] Batch 3839, Loss 0.40959978103637695\n","[Training Epoch 3] Batch 3840, Loss 0.3836832046508789\n","[Training Epoch 3] Batch 3841, Loss 0.40027064085006714\n","[Training Epoch 3] Batch 3842, Loss 0.3758898377418518\n","[Training Epoch 3] Batch 3843, Loss 0.3784816265106201\n","[Training Epoch 3] Batch 3844, Loss 0.3815063536167145\n","[Training Epoch 3] Batch 3845, Loss 0.3892715275287628\n","[Training Epoch 3] Batch 3846, Loss 0.39626115560531616\n","[Training Epoch 3] Batch 3847, Loss 0.42439281940460205\n","[Training Epoch 3] Batch 3848, Loss 0.38335490226745605\n","[Training Epoch 3] Batch 3849, Loss 0.3705100119113922\n","[Training Epoch 3] Batch 3850, Loss 0.402595579624176\n","[Training Epoch 3] Batch 3851, Loss 0.36846840381622314\n","[Training Epoch 3] Batch 3852, Loss 0.3653642237186432\n","[Training Epoch 3] Batch 3853, Loss 0.3749392628669739\n","[Training Epoch 3] Batch 3854, Loss 0.32897332310676575\n","[Training Epoch 3] Batch 3855, Loss 0.4028139114379883\n","[Training Epoch 3] Batch 3856, Loss 0.3882552683353424\n","[Training Epoch 3] Batch 3857, Loss 0.3946996331214905\n","[Training Epoch 3] Batch 3858, Loss 0.35543113946914673\n","[Training Epoch 3] Batch 3859, Loss 0.38316217064857483\n","[Training Epoch 3] Batch 3860, Loss 0.3568017780780792\n","[Training Epoch 3] Batch 3861, Loss 0.3705477714538574\n","[Training Epoch 3] Batch 3862, Loss 0.3998457193374634\n","[Training Epoch 3] Batch 3863, Loss 0.3928585648536682\n","[Training Epoch 3] Batch 3864, Loss 0.39190685749053955\n","[Training Epoch 3] Batch 3865, Loss 0.3751043975353241\n","[Training Epoch 3] Batch 3866, Loss 0.40251559019088745\n","[Training Epoch 3] Batch 3867, Loss 0.4193143844604492\n","[Training Epoch 3] Batch 3868, Loss 0.38177788257598877\n","[Training Epoch 3] Batch 3869, Loss 0.402759850025177\n","[Training Epoch 3] Batch 3870, Loss 0.40861842036247253\n","[Training Epoch 3] Batch 3871, Loss 0.3628036677837372\n","[Training Epoch 3] Batch 3872, Loss 0.39480313658714294\n","[Training Epoch 3] Batch 3873, Loss 0.3937181234359741\n","[Training Epoch 3] Batch 3874, Loss 0.37935978174209595\n","[Training Epoch 3] Batch 3875, Loss 0.3807544708251953\n","[Training Epoch 3] Batch 3876, Loss 0.38375264406204224\n","[Training Epoch 3] Batch 3877, Loss 0.3686036467552185\n","[Training Epoch 3] Batch 3878, Loss 0.378295361995697\n","[Training Epoch 3] Batch 3879, Loss 0.3891379237174988\n","[Training Epoch 3] Batch 3880, Loss 0.39397433400154114\n","[Training Epoch 3] Batch 3881, Loss 0.3749265670776367\n","[Training Epoch 3] Batch 3882, Loss 0.3954203128814697\n","[Training Epoch 3] Batch 3883, Loss 0.39359498023986816\n","[Training Epoch 3] Batch 3884, Loss 0.36704596877098083\n","[Training Epoch 3] Batch 3885, Loss 0.39481669664382935\n","[Training Epoch 3] Batch 3886, Loss 0.3676711916923523\n","[Training Epoch 3] Batch 3887, Loss 0.37084710597991943\n","[Training Epoch 3] Batch 3888, Loss 0.39545971155166626\n","[Training Epoch 3] Batch 3889, Loss 0.3684660494327545\n","[Training Epoch 3] Batch 3890, Loss 0.35145971179008484\n","[Training Epoch 3] Batch 3891, Loss 0.4089803993701935\n","[Training Epoch 3] Batch 3892, Loss 0.37766164541244507\n","[Training Epoch 3] Batch 3893, Loss 0.35554230213165283\n","[Training Epoch 3] Batch 3894, Loss 0.395209938287735\n","[Training Epoch 3] Batch 3895, Loss 0.37432652711868286\n","[Training Epoch 3] Batch 3896, Loss 0.37279945611953735\n","[Training Epoch 3] Batch 3897, Loss 0.41794878244400024\n","[Training Epoch 3] Batch 3898, Loss 0.4071062207221985\n","[Training Epoch 3] Batch 3899, Loss 0.4143717288970947\n","[Training Epoch 3] Batch 3900, Loss 0.36561599373817444\n","[Training Epoch 3] Batch 3901, Loss 0.36381569504737854\n","[Training Epoch 3] Batch 3902, Loss 0.4063273072242737\n","[Training Epoch 3] Batch 3903, Loss 0.3626146614551544\n","[Training Epoch 3] Batch 3904, Loss 0.36128711700439453\n","[Training Epoch 3] Batch 3905, Loss 0.388604074716568\n","[Training Epoch 3] Batch 3906, Loss 0.37541502714157104\n","[Training Epoch 3] Batch 3907, Loss 0.3945111930370331\n","[Training Epoch 3] Batch 3908, Loss 0.3962368071079254\n","[Training Epoch 3] Batch 3909, Loss 0.38415470719337463\n","[Training Epoch 3] Batch 3910, Loss 0.36622095108032227\n","[Training Epoch 3] Batch 3911, Loss 0.4035971760749817\n","[Training Epoch 3] Batch 3912, Loss 0.3795577585697174\n","[Training Epoch 3] Batch 3913, Loss 0.3730342984199524\n","[Training Epoch 3] Batch 3914, Loss 0.37026655673980713\n","[Training Epoch 3] Batch 3915, Loss 0.38616377115249634\n","[Training Epoch 3] Batch 3916, Loss 0.3991052806377411\n","[Training Epoch 3] Batch 3917, Loss 0.38308849930763245\n","[Training Epoch 3] Batch 3918, Loss 0.3833264708518982\n","[Training Epoch 3] Batch 3919, Loss 0.3964536786079407\n","[Training Epoch 3] Batch 3920, Loss 0.3774992823600769\n","[Training Epoch 3] Batch 3921, Loss 0.3519114553928375\n","[Training Epoch 3] Batch 3922, Loss 0.380819708108902\n","[Training Epoch 3] Batch 3923, Loss 0.4312378168106079\n","[Training Epoch 3] Batch 3924, Loss 0.39060038328170776\n","[Training Epoch 3] Batch 3925, Loss 0.3671188950538635\n","[Training Epoch 3] Batch 3926, Loss 0.3819381892681122\n","[Training Epoch 3] Batch 3927, Loss 0.3978133499622345\n","[Training Epoch 3] Batch 3928, Loss 0.4262941777706146\n","[Training Epoch 3] Batch 3929, Loss 0.3761887550354004\n","[Training Epoch 3] Batch 3930, Loss 0.38161420822143555\n","[Training Epoch 3] Batch 3931, Loss 0.37229859828948975\n","[Training Epoch 3] Batch 3932, Loss 0.3961755633354187\n","[Training Epoch 3] Batch 3933, Loss 0.3724583387374878\n","[Training Epoch 3] Batch 3934, Loss 0.38238802552223206\n","[Training Epoch 3] Batch 3935, Loss 0.3819361925125122\n","[Training Epoch 3] Batch 3936, Loss 0.37817656993865967\n","[Training Epoch 3] Batch 3937, Loss 0.3827318549156189\n","[Training Epoch 3] Batch 3938, Loss 0.40137016773223877\n","[Training Epoch 3] Batch 3939, Loss 0.3725457191467285\n","[Training Epoch 3] Batch 3940, Loss 0.3903089761734009\n","[Training Epoch 3] Batch 3941, Loss 0.38179612159729004\n","[Training Epoch 3] Batch 3942, Loss 0.4058822989463806\n","[Training Epoch 3] Batch 3943, Loss 0.35017111897468567\n","[Training Epoch 3] Batch 3944, Loss 0.35238945484161377\n","[Training Epoch 3] Batch 3945, Loss 0.37648719549179077\n","[Training Epoch 3] Batch 3946, Loss 0.3930855989456177\n","[Training Epoch 3] Batch 3947, Loss 0.40548670291900635\n","[Training Epoch 3] Batch 3948, Loss 0.363050639629364\n","[Training Epoch 3] Batch 3949, Loss 0.3855990171432495\n","[Training Epoch 3] Batch 3950, Loss 0.40038639307022095\n","[Training Epoch 3] Batch 3951, Loss 0.3795131742954254\n","[Training Epoch 3] Batch 3952, Loss 0.3689868152141571\n","[Training Epoch 3] Batch 3953, Loss 0.3709070682525635\n","[Training Epoch 3] Batch 3954, Loss 0.40976446866989136\n","[Training Epoch 3] Batch 3955, Loss 0.38983339071273804\n","[Training Epoch 3] Batch 3956, Loss 0.3764965832233429\n","[Training Epoch 3] Batch 3957, Loss 0.37760043144226074\n","[Training Epoch 3] Batch 3958, Loss 0.39832717180252075\n","[Training Epoch 3] Batch 3959, Loss 0.41601866483688354\n","[Training Epoch 3] Batch 3960, Loss 0.3757191300392151\n","[Training Epoch 3] Batch 3961, Loss 0.3800719380378723\n","[Training Epoch 3] Batch 3962, Loss 0.3990754783153534\n","[Training Epoch 3] Batch 3963, Loss 0.3931412100791931\n","[Training Epoch 3] Batch 3964, Loss 0.38681018352508545\n","[Training Epoch 3] Batch 3965, Loss 0.3753284811973572\n","[Training Epoch 3] Batch 3966, Loss 0.3865184783935547\n","[Training Epoch 3] Batch 3967, Loss 0.3888319730758667\n","[Training Epoch 3] Batch 3968, Loss 0.36883431673049927\n","[Training Epoch 3] Batch 3969, Loss 0.38276588916778564\n","[Training Epoch 3] Batch 3970, Loss 0.3831498622894287\n","[Training Epoch 3] Batch 3971, Loss 0.3761608898639679\n","[Training Epoch 3] Batch 3972, Loss 0.3949185311794281\n","[Training Epoch 3] Batch 3973, Loss 0.35821768641471863\n","[Training Epoch 3] Batch 3974, Loss 0.3846169114112854\n","[Training Epoch 3] Batch 3975, Loss 0.3698440194129944\n","[Training Epoch 3] Batch 3976, Loss 0.3783561885356903\n","[Training Epoch 3] Batch 3977, Loss 0.401828795671463\n","[Training Epoch 3] Batch 3978, Loss 0.3810611963272095\n","[Training Epoch 3] Batch 3979, Loss 0.3836127519607544\n","[Training Epoch 3] Batch 3980, Loss 0.4137977361679077\n","[Training Epoch 3] Batch 3981, Loss 0.37203437089920044\n","[Training Epoch 3] Batch 3982, Loss 0.369587779045105\n","[Training Epoch 3] Batch 3983, Loss 0.39090079069137573\n","[Training Epoch 3] Batch 3984, Loss 0.40109896659851074\n","[Training Epoch 3] Batch 3985, Loss 0.38906222581863403\n","[Training Epoch 3] Batch 3986, Loss 0.38001585006713867\n","[Training Epoch 3] Batch 3987, Loss 0.40245792269706726\n","[Training Epoch 3] Batch 3988, Loss 0.3924712538719177\n","[Training Epoch 3] Batch 3989, Loss 0.38972973823547363\n","[Training Epoch 3] Batch 3990, Loss 0.37739136815071106\n","[Training Epoch 3] Batch 3991, Loss 0.3935452699661255\n","[Training Epoch 3] Batch 3992, Loss 0.3754522502422333\n","[Training Epoch 3] Batch 3993, Loss 0.3839966952800751\n","[Training Epoch 3] Batch 3994, Loss 0.3890646994113922\n","[Training Epoch 3] Batch 3995, Loss 0.38427191972732544\n","[Training Epoch 3] Batch 3996, Loss 0.4016815423965454\n","[Training Epoch 3] Batch 3997, Loss 0.40282702445983887\n","[Training Epoch 3] Batch 3998, Loss 0.3766399621963501\n","[Training Epoch 3] Batch 3999, Loss 0.3761129379272461\n","[Training Epoch 3] Batch 4000, Loss 0.39096498489379883\n","[Training Epoch 3] Batch 4001, Loss 0.3457760214805603\n","[Training Epoch 3] Batch 4002, Loss 0.4240752160549164\n","[Training Epoch 3] Batch 4003, Loss 0.40715309977531433\n","[Training Epoch 3] Batch 4004, Loss 0.3756324052810669\n","[Training Epoch 3] Batch 4005, Loss 0.39760440587997437\n","[Training Epoch 3] Batch 4006, Loss 0.3817733824253082\n","[Training Epoch 3] Batch 4007, Loss 0.36938750743865967\n","[Training Epoch 3] Batch 4008, Loss 0.3942868411540985\n","[Training Epoch 3] Batch 4009, Loss 0.380437433719635\n","[Training Epoch 3] Batch 4010, Loss 0.4006933569908142\n","[Training Epoch 3] Batch 4011, Loss 0.40495073795318604\n","[Training Epoch 3] Batch 4012, Loss 0.3568747639656067\n","[Training Epoch 3] Batch 4013, Loss 0.375044047832489\n","[Training Epoch 3] Batch 4014, Loss 0.36028411984443665\n","[Training Epoch 3] Batch 4015, Loss 0.37281250953674316\n","[Training Epoch 3] Batch 4016, Loss 0.3472079336643219\n","[Training Epoch 3] Batch 4017, Loss 0.40432649850845337\n","[Training Epoch 3] Batch 4018, Loss 0.3682482838630676\n","[Training Epoch 3] Batch 4019, Loss 0.36233121156692505\n","[Training Epoch 3] Batch 4020, Loss 0.39990025758743286\n","[Training Epoch 3] Batch 4021, Loss 0.3980224132537842\n","[Training Epoch 3] Batch 4022, Loss 0.40302956104278564\n","[Training Epoch 3] Batch 4023, Loss 0.39448556303977966\n","[Training Epoch 3] Batch 4024, Loss 0.39127713441848755\n","[Training Epoch 3] Batch 4025, Loss 0.3957480788230896\n","[Training Epoch 3] Batch 4026, Loss 0.372132807970047\n","[Training Epoch 3] Batch 4027, Loss 0.3510013222694397\n","[Training Epoch 3] Batch 4028, Loss 0.3988480567932129\n","[Training Epoch 3] Batch 4029, Loss 0.389553040266037\n","[Training Epoch 3] Batch 4030, Loss 0.35845947265625\n","[Training Epoch 3] Batch 4031, Loss 0.36780568957328796\n","[Training Epoch 3] Batch 4032, Loss 0.38043731451034546\n","[Training Epoch 3] Batch 4033, Loss 0.37091636657714844\n","[Training Epoch 3] Batch 4034, Loss 0.36303824186325073\n","[Training Epoch 3] Batch 4035, Loss 0.36211222410202026\n","[Training Epoch 3] Batch 4036, Loss 0.3693118691444397\n","[Training Epoch 3] Batch 4037, Loss 0.37110209465026855\n","[Training Epoch 3] Batch 4038, Loss 0.42426085472106934\n","[Training Epoch 3] Batch 4039, Loss 0.4063565731048584\n","[Training Epoch 3] Batch 4040, Loss 0.40024256706237793\n","[Training Epoch 3] Batch 4041, Loss 0.36429518461227417\n","[Training Epoch 3] Batch 4042, Loss 0.40920746326446533\n","[Training Epoch 3] Batch 4043, Loss 0.3797627091407776\n","[Training Epoch 3] Batch 4044, Loss 0.4067723751068115\n","[Training Epoch 3] Batch 4045, Loss 0.4032151997089386\n","[Training Epoch 3] Batch 4046, Loss 0.3712269067764282\n","[Training Epoch 3] Batch 4047, Loss 0.3905538320541382\n","[Training Epoch 3] Batch 4048, Loss 0.39095285534858704\n","[Training Epoch 3] Batch 4049, Loss 0.35818392038345337\n","[Training Epoch 3] Batch 4050, Loss 0.39574527740478516\n","[Training Epoch 3] Batch 4051, Loss 0.36735737323760986\n","[Training Epoch 3] Batch 4052, Loss 0.34768837690353394\n","[Training Epoch 3] Batch 4053, Loss 0.3558918535709381\n","[Training Epoch 3] Batch 4054, Loss 0.3809698820114136\n","[Training Epoch 3] Batch 4055, Loss 0.4206215739250183\n","[Training Epoch 3] Batch 4056, Loss 0.3913676142692566\n","[Training Epoch 3] Batch 4057, Loss 0.37730804085731506\n","[Training Epoch 3] Batch 4058, Loss 0.38007837533950806\n","[Training Epoch 3] Batch 4059, Loss 0.3537745177745819\n","[Training Epoch 3] Batch 4060, Loss 0.40050920844078064\n","[Training Epoch 3] Batch 4061, Loss 0.35895437002182007\n","[Training Epoch 3] Batch 4062, Loss 0.38214072585105896\n","[Training Epoch 3] Batch 4063, Loss 0.4069899916648865\n","[Training Epoch 3] Batch 4064, Loss 0.34459930658340454\n","[Training Epoch 3] Batch 4065, Loss 0.40328365564346313\n","[Training Epoch 3] Batch 4066, Loss 0.37164372205734253\n","[Training Epoch 3] Batch 4067, Loss 0.36254698038101196\n","[Training Epoch 3] Batch 4068, Loss 0.40509337186813354\n","[Training Epoch 3] Batch 4069, Loss 0.41278815269470215\n","[Training Epoch 3] Batch 4070, Loss 0.37675023078918457\n","[Training Epoch 3] Batch 4071, Loss 0.3710014224052429\n","[Training Epoch 3] Batch 4072, Loss 0.3739868402481079\n","[Training Epoch 3] Batch 4073, Loss 0.3677409291267395\n","[Training Epoch 3] Batch 4074, Loss 0.38978928327560425\n","[Training Epoch 3] Batch 4075, Loss 0.37951675057411194\n","[Training Epoch 3] Batch 4076, Loss 0.36618247628211975\n","[Training Epoch 3] Batch 4077, Loss 0.3792763650417328\n","[Training Epoch 3] Batch 4078, Loss 0.3986985981464386\n","[Training Epoch 3] Batch 4079, Loss 0.38488173484802246\n","[Training Epoch 3] Batch 4080, Loss 0.39138370752334595\n","[Training Epoch 3] Batch 4081, Loss 0.41085654497146606\n","[Training Epoch 3] Batch 4082, Loss 0.413327157497406\n","[Training Epoch 3] Batch 4083, Loss 0.3698098659515381\n","[Training Epoch 3] Batch 4084, Loss 0.36639484763145447\n","[Training Epoch 3] Batch 4085, Loss 0.4104127883911133\n","[Training Epoch 3] Batch 4086, Loss 0.38892585039138794\n","[Training Epoch 3] Batch 4087, Loss 0.3760277032852173\n","[Training Epoch 3] Batch 4088, Loss 0.3706076741218567\n","[Training Epoch 3] Batch 4089, Loss 0.36361241340637207\n","[Training Epoch 3] Batch 4090, Loss 0.34895408153533936\n","[Training Epoch 3] Batch 4091, Loss 0.36814790964126587\n","[Training Epoch 3] Batch 4092, Loss 0.36940324306488037\n","[Training Epoch 3] Batch 4093, Loss 0.3940877914428711\n","[Training Epoch 3] Batch 4094, Loss 0.40782856941223145\n","[Training Epoch 3] Batch 4095, Loss 0.37248021364212036\n","[Training Epoch 3] Batch 4096, Loss 0.3802432417869568\n","[Training Epoch 3] Batch 4097, Loss 0.39313942193984985\n","[Training Epoch 3] Batch 4098, Loss 0.37847793102264404\n","[Training Epoch 3] Batch 4099, Loss 0.3437119126319885\n","[Training Epoch 3] Batch 4100, Loss 0.3896215558052063\n","[Training Epoch 3] Batch 4101, Loss 0.37857019901275635\n","[Training Epoch 3] Batch 4102, Loss 0.37978601455688477\n","[Training Epoch 3] Batch 4103, Loss 0.3635519742965698\n","[Training Epoch 3] Batch 4104, Loss 0.3927120268344879\n","[Training Epoch 3] Batch 4105, Loss 0.39362961053848267\n","[Training Epoch 3] Batch 4106, Loss 0.36389315128326416\n","[Training Epoch 3] Batch 4107, Loss 0.39022713899612427\n","[Training Epoch 3] Batch 4108, Loss 0.34532448649406433\n","[Training Epoch 3] Batch 4109, Loss 0.4043863117694855\n","[Training Epoch 3] Batch 4110, Loss 0.4005768895149231\n","[Training Epoch 3] Batch 4111, Loss 0.39454448223114014\n","[Training Epoch 3] Batch 4112, Loss 0.3741160035133362\n","[Training Epoch 3] Batch 4113, Loss 0.4387245774269104\n","[Training Epoch 3] Batch 4114, Loss 0.38693657517433167\n","[Training Epoch 3] Batch 4115, Loss 0.39131075143814087\n","[Training Epoch 3] Batch 4116, Loss 0.3819199800491333\n","[Training Epoch 3] Batch 4117, Loss 0.3998193144798279\n","[Training Epoch 3] Batch 4118, Loss 0.3742310702800751\n","[Training Epoch 3] Batch 4119, Loss 0.3660328686237335\n","[Training Epoch 3] Batch 4120, Loss 0.3567292094230652\n","[Training Epoch 3] Batch 4121, Loss 0.3722628355026245\n","[Training Epoch 3] Batch 4122, Loss 0.419455885887146\n","[Training Epoch 3] Batch 4123, Loss 0.3974238932132721\n","[Training Epoch 3] Batch 4124, Loss 0.3937559425830841\n","[Training Epoch 3] Batch 4125, Loss 0.36247700452804565\n","[Training Epoch 3] Batch 4126, Loss 0.3925899863243103\n","[Training Epoch 3] Batch 4127, Loss 0.4154432415962219\n","[Training Epoch 3] Batch 4128, Loss 0.37947168946266174\n","[Training Epoch 3] Batch 4129, Loss 0.3675142228603363\n","[Training Epoch 3] Batch 4130, Loss 0.39256352186203003\n","[Training Epoch 3] Batch 4131, Loss 0.3844902515411377\n","[Training Epoch 3] Batch 4132, Loss 0.3680088222026825\n","[Training Epoch 3] Batch 4133, Loss 0.37762218713760376\n","[Training Epoch 3] Batch 4134, Loss 0.36467960476875305\n","[Training Epoch 3] Batch 4135, Loss 0.41298365592956543\n","[Training Epoch 3] Batch 4136, Loss 0.39825260639190674\n","[Training Epoch 3] Batch 4137, Loss 0.3652544319629669\n","[Training Epoch 3] Batch 4138, Loss 0.39713960886001587\n","[Training Epoch 3] Batch 4139, Loss 0.36816686391830444\n","[Training Epoch 3] Batch 4140, Loss 0.40017592906951904\n","[Training Epoch 3] Batch 4141, Loss 0.3932151198387146\n","[Training Epoch 3] Batch 4142, Loss 0.4077243506908417\n","[Training Epoch 3] Batch 4143, Loss 0.4164871573448181\n","[Training Epoch 3] Batch 4144, Loss 0.4343634247779846\n","[Training Epoch 3] Batch 4145, Loss 0.3864303231239319\n","[Training Epoch 3] Batch 4146, Loss 0.3620676100254059\n","[Training Epoch 3] Batch 4147, Loss 0.3744131326675415\n","[Training Epoch 3] Batch 4148, Loss 0.3836258053779602\n","[Training Epoch 3] Batch 4149, Loss 0.3663037419319153\n","[Training Epoch 3] Batch 4150, Loss 0.38773638010025024\n","[Training Epoch 3] Batch 4151, Loss 0.3858632445335388\n","[Training Epoch 3] Batch 4152, Loss 0.3702769875526428\n","[Training Epoch 3] Batch 4153, Loss 0.3951742649078369\n","[Training Epoch 3] Batch 4154, Loss 0.37511783838272095\n","[Training Epoch 3] Batch 4155, Loss 0.37724703550338745\n","[Training Epoch 3] Batch 4156, Loss 0.33818089962005615\n","[Training Epoch 3] Batch 4157, Loss 0.3845025599002838\n","[Training Epoch 3] Batch 4158, Loss 0.4028264880180359\n","[Training Epoch 3] Batch 4159, Loss 0.38565772771835327\n","[Training Epoch 3] Batch 4160, Loss 0.38049328327178955\n","[Training Epoch 3] Batch 4161, Loss 0.3982415795326233\n","[Training Epoch 3] Batch 4162, Loss 0.3814919590950012\n","[Training Epoch 3] Batch 4163, Loss 0.3754315972328186\n","[Training Epoch 3] Batch 4164, Loss 0.3911800682544708\n","[Training Epoch 3] Batch 4165, Loss 0.36008456349372864\n","[Training Epoch 3] Batch 4166, Loss 0.3893367350101471\n","[Training Epoch 3] Batch 4167, Loss 0.37345343828201294\n","[Training Epoch 3] Batch 4168, Loss 0.3968676030635834\n","[Training Epoch 3] Batch 4169, Loss 0.41972213983535767\n","[Training Epoch 3] Batch 4170, Loss 0.3882753252983093\n","[Training Epoch 3] Batch 4171, Loss 0.3706977367401123\n","[Training Epoch 3] Batch 4172, Loss 0.40709811449050903\n","[Training Epoch 3] Batch 4173, Loss 0.3774956464767456\n","[Training Epoch 3] Batch 4174, Loss 0.3895624577999115\n","[Training Epoch 3] Batch 4175, Loss 0.34387272596359253\n","[Training Epoch 3] Batch 4176, Loss 0.4120047986507416\n","[Training Epoch 3] Batch 4177, Loss 0.4210672974586487\n","[Training Epoch 3] Batch 4178, Loss 0.39752936363220215\n","[Training Epoch 3] Batch 4179, Loss 0.38931217789649963\n","[Training Epoch 3] Batch 4180, Loss 0.38005369901657104\n","[Training Epoch 3] Batch 4181, Loss 0.39514410495758057\n","[Training Epoch 3] Batch 4182, Loss 0.39158985018730164\n","[Training Epoch 3] Batch 4183, Loss 0.38551250100135803\n","[Training Epoch 3] Batch 4184, Loss 0.3894365429878235\n","[Training Epoch 3] Batch 4185, Loss 0.4150763750076294\n","[Training Epoch 3] Batch 4186, Loss 0.3598388433456421\n","[Training Epoch 3] Batch 4187, Loss 0.398280531167984\n","[Training Epoch 3] Batch 4188, Loss 0.38539308309555054\n","[Training Epoch 3] Batch 4189, Loss 0.3735816776752472\n","[Training Epoch 3] Batch 4190, Loss 0.3950371742248535\n","[Training Epoch 3] Batch 4191, Loss 0.390342652797699\n","[Training Epoch 3] Batch 4192, Loss 0.4037046432495117\n","[Training Epoch 3] Batch 4193, Loss 0.3998258113861084\n","[Training Epoch 3] Batch 4194, Loss 0.3859606385231018\n","[Training Epoch 3] Batch 4195, Loss 0.38078802824020386\n","[Training Epoch 3] Batch 4196, Loss 0.378404438495636\n","[Training Epoch 3] Batch 4197, Loss 0.4004972577095032\n","[Training Epoch 3] Batch 4198, Loss 0.39984363317489624\n","[Training Epoch 3] Batch 4199, Loss 0.39811426401138306\n","[Training Epoch 3] Batch 4200, Loss 0.4015403091907501\n","[Training Epoch 3] Batch 4201, Loss 0.37348616123199463\n","[Training Epoch 3] Batch 4202, Loss 0.3975788950920105\n","[Training Epoch 3] Batch 4203, Loss 0.3854473829269409\n","[Training Epoch 3] Batch 4204, Loss 0.38601154088974\n","[Training Epoch 3] Batch 4205, Loss 0.40223416686058044\n","[Training Epoch 3] Batch 4206, Loss 0.3530256748199463\n","[Training Epoch 3] Batch 4207, Loss 0.3958362340927124\n","[Training Epoch 3] Batch 4208, Loss 0.3969116806983948\n","[Training Epoch 3] Batch 4209, Loss 0.37093788385391235\n","[Training Epoch 3] Batch 4210, Loss 0.3899359107017517\n","[Training Epoch 3] Batch 4211, Loss 0.3827950656414032\n","[Training Epoch 3] Batch 4212, Loss 0.3892151117324829\n","[Training Epoch 3] Batch 4213, Loss 0.3741939663887024\n","[Training Epoch 3] Batch 4214, Loss 0.3851095736026764\n","[Training Epoch 3] Batch 4215, Loss 0.3810370862483978\n","[Training Epoch 3] Batch 4216, Loss 0.39306947588920593\n","[Training Epoch 3] Batch 4217, Loss 0.3828931748867035\n","[Training Epoch 3] Batch 4218, Loss 0.3751457631587982\n","[Training Epoch 3] Batch 4219, Loss 0.37355613708496094\n","[Training Epoch 3] Batch 4220, Loss 0.39300933480262756\n","[Training Epoch 3] Batch 4221, Loss 0.40122997760772705\n","[Training Epoch 3] Batch 4222, Loss 0.3818589150905609\n","[Training Epoch 3] Batch 4223, Loss 0.3737791180610657\n","[Training Epoch 3] Batch 4224, Loss 0.3629792630672455\n","[Training Epoch 3] Batch 4225, Loss 0.3572935461997986\n","[Training Epoch 3] Batch 4226, Loss 0.3953365087509155\n","[Training Epoch 3] Batch 4227, Loss 0.3770979642868042\n","[Training Epoch 3] Batch 4228, Loss 0.41418641805648804\n","[Training Epoch 3] Batch 4229, Loss 0.3467100262641907\n","[Training Epoch 3] Batch 4230, Loss 0.36816126108169556\n","[Training Epoch 3] Batch 4231, Loss 0.3792206645011902\n","[Training Epoch 3] Batch 4232, Loss 0.3597169518470764\n","[Training Epoch 3] Batch 4233, Loss 0.3932836651802063\n","[Training Epoch 3] Batch 4234, Loss 0.3628256022930145\n","[Training Epoch 3] Batch 4235, Loss 0.3673645257949829\n","[Training Epoch 3] Batch 4236, Loss 0.38570842146873474\n","[Training Epoch 3] Batch 4237, Loss 0.3832588791847229\n","[Training Epoch 3] Batch 4238, Loss 0.3803638219833374\n","[Training Epoch 3] Batch 4239, Loss 0.3902113437652588\n","[Training Epoch 3] Batch 4240, Loss 0.4098988175392151\n","[Training Epoch 3] Batch 4241, Loss 0.36549508571624756\n","[Training Epoch 3] Batch 4242, Loss 0.3265771269798279\n","[Training Epoch 3] Batch 4243, Loss 0.3674561083316803\n","[Training Epoch 3] Batch 4244, Loss 0.36374467611312866\n","[Training Epoch 3] Batch 4245, Loss 0.3590103089809418\n","[Training Epoch 3] Batch 4246, Loss 0.3896624445915222\n","[Training Epoch 3] Batch 4247, Loss 0.3844407796859741\n","[Training Epoch 3] Batch 4248, Loss 0.3521709442138672\n","[Training Epoch 3] Batch 4249, Loss 0.37840384244918823\n","[Training Epoch 3] Batch 4250, Loss 0.37994518876075745\n","[Training Epoch 3] Batch 4251, Loss 0.3995118737220764\n","[Training Epoch 3] Batch 4252, Loss 0.4014889597892761\n","[Training Epoch 3] Batch 4253, Loss 0.378156840801239\n","[Training Epoch 3] Batch 4254, Loss 0.40277183055877686\n","[Training Epoch 3] Batch 4255, Loss 0.36961400508880615\n","[Training Epoch 3] Batch 4256, Loss 0.3546789586544037\n","[Training Epoch 3] Batch 4257, Loss 0.40038806200027466\n","[Training Epoch 3] Batch 4258, Loss 0.38358110189437866\n","[Training Epoch 3] Batch 4259, Loss 0.37568870186805725\n","[Training Epoch 3] Batch 4260, Loss 0.3737587332725525\n","[Training Epoch 3] Batch 4261, Loss 0.378476083278656\n","[Training Epoch 3] Batch 4262, Loss 0.40909069776535034\n","[Training Epoch 3] Batch 4263, Loss 0.392205148935318\n","[Training Epoch 3] Batch 4264, Loss 0.3753979504108429\n","[Training Epoch 3] Batch 4265, Loss 0.40789633989334106\n","[Training Epoch 3] Batch 4266, Loss 0.3874611258506775\n","[Training Epoch 3] Batch 4267, Loss 0.37084078788757324\n","[Training Epoch 3] Batch 4268, Loss 0.40499147772789\n","[Training Epoch 3] Batch 4269, Loss 0.4078003466129303\n","[Training Epoch 3] Batch 4270, Loss 0.3859046399593353\n","[Training Epoch 3] Batch 4271, Loss 0.3757321834564209\n","[Training Epoch 3] Batch 4272, Loss 0.36828678846359253\n","[Training Epoch 3] Batch 4273, Loss 0.33805012702941895\n","[Training Epoch 3] Batch 4274, Loss 0.3694099485874176\n","[Training Epoch 3] Batch 4275, Loss 0.41328489780426025\n","[Training Epoch 3] Batch 4276, Loss 0.36286407709121704\n","[Training Epoch 3] Batch 4277, Loss 0.37211179733276367\n","[Training Epoch 3] Batch 4278, Loss 0.4067232608795166\n","[Training Epoch 3] Batch 4279, Loss 0.3835722804069519\n","[Training Epoch 3] Batch 4280, Loss 0.3890354633331299\n","[Training Epoch 3] Batch 4281, Loss 0.37924376130104065\n","[Training Epoch 3] Batch 4282, Loss 0.3829517960548401\n","[Training Epoch 3] Batch 4283, Loss 0.3840157985687256\n","[Training Epoch 3] Batch 4284, Loss 0.398693323135376\n","[Training Epoch 3] Batch 4285, Loss 0.3748736083507538\n","[Training Epoch 3] Batch 4286, Loss 0.35938698053359985\n","[Training Epoch 3] Batch 4287, Loss 0.3899419903755188\n","[Training Epoch 3] Batch 4288, Loss 0.37091201543807983\n","[Training Epoch 3] Batch 4289, Loss 0.39932990074157715\n","[Training Epoch 3] Batch 4290, Loss 0.36549705266952515\n","[Training Epoch 3] Batch 4291, Loss 0.39436596632003784\n","[Training Epoch 3] Batch 4292, Loss 0.35874879360198975\n","[Training Epoch 3] Batch 4293, Loss 0.4069331884384155\n","[Training Epoch 3] Batch 4294, Loss 0.3924848735332489\n","[Training Epoch 3] Batch 4295, Loss 0.37656569480895996\n","[Training Epoch 3] Batch 4296, Loss 0.381202757358551\n","[Training Epoch 3] Batch 4297, Loss 0.39475545287132263\n","[Training Epoch 3] Batch 4298, Loss 0.3643440306186676\n","[Training Epoch 3] Batch 4299, Loss 0.39412355422973633\n","[Training Epoch 3] Batch 4300, Loss 0.3841537535190582\n","[Training Epoch 3] Batch 4301, Loss 0.3905980587005615\n","[Training Epoch 3] Batch 4302, Loss 0.4025167226791382\n","[Training Epoch 3] Batch 4303, Loss 0.4106827974319458\n","[Training Epoch 3] Batch 4304, Loss 0.35556042194366455\n","[Training Epoch 3] Batch 4305, Loss 0.3917105197906494\n","[Training Epoch 3] Batch 4306, Loss 0.3644271194934845\n","[Training Epoch 3] Batch 4307, Loss 0.3619663119316101\n","[Training Epoch 3] Batch 4308, Loss 0.3658989667892456\n","[Training Epoch 3] Batch 4309, Loss 0.37650981545448303\n","[Training Epoch 3] Batch 4310, Loss 0.38277140259742737\n","[Training Epoch 3] Batch 4311, Loss 0.3820597529411316\n","[Training Epoch 3] Batch 4312, Loss 0.3775968849658966\n","[Training Epoch 3] Batch 4313, Loss 0.37755411863327026\n","[Training Epoch 3] Batch 4314, Loss 0.3945329487323761\n","[Training Epoch 3] Batch 4315, Loss 0.3687530755996704\n","[Training Epoch 3] Batch 4316, Loss 0.3805250823497772\n","[Training Epoch 3] Batch 4317, Loss 0.4140869379043579\n","[Training Epoch 3] Batch 4318, Loss 0.3787573277950287\n","[Training Epoch 3] Batch 4319, Loss 0.3587082028388977\n","[Training Epoch 3] Batch 4320, Loss 0.3512369990348816\n","[Training Epoch 3] Batch 4321, Loss 0.37841829657554626\n","[Training Epoch 3] Batch 4322, Loss 0.38587692379951477\n","[Training Epoch 3] Batch 4323, Loss 0.36093515157699585\n","[Training Epoch 3] Batch 4324, Loss 0.3762268126010895\n","[Training Epoch 3] Batch 4325, Loss 0.3622855842113495\n","[Training Epoch 3] Batch 4326, Loss 0.401347815990448\n","[Training Epoch 3] Batch 4327, Loss 0.36845654249191284\n","[Training Epoch 3] Batch 4328, Loss 0.379693865776062\n","[Training Epoch 3] Batch 4329, Loss 0.38260558247566223\n","[Training Epoch 3] Batch 4330, Loss 0.38621246814727783\n","[Training Epoch 3] Batch 4331, Loss 0.37204766273498535\n","[Training Epoch 3] Batch 4332, Loss 0.35981032252311707\n","[Training Epoch 3] Batch 4333, Loss 0.3951740562915802\n","[Training Epoch 3] Batch 4334, Loss 0.3863985538482666\n","[Training Epoch 3] Batch 4335, Loss 0.39341598749160767\n","[Training Epoch 3] Batch 4336, Loss 0.3775270879268646\n","[Training Epoch 3] Batch 4337, Loss 0.36904779076576233\n","[Training Epoch 3] Batch 4338, Loss 0.37095773220062256\n","[Training Epoch 3] Batch 4339, Loss 0.35034677386283875\n","[Training Epoch 3] Batch 4340, Loss 0.3995753824710846\n","[Training Epoch 3] Batch 4341, Loss 0.3914318084716797\n","[Training Epoch 3] Batch 4342, Loss 0.3723336160182953\n","[Training Epoch 3] Batch 4343, Loss 0.3838367164134979\n","[Training Epoch 3] Batch 4344, Loss 0.40265482664108276\n","[Training Epoch 3] Batch 4345, Loss 0.4044182300567627\n","[Training Epoch 3] Batch 4346, Loss 0.35710451006889343\n","[Training Epoch 3] Batch 4347, Loss 0.36670559644699097\n","[Training Epoch 3] Batch 4348, Loss 0.3710017502307892\n","[Training Epoch 3] Batch 4349, Loss 0.37061235308647156\n","[Training Epoch 3] Batch 4350, Loss 0.3867936432361603\n","[Training Epoch 3] Batch 4351, Loss 0.39673417806625366\n","[Training Epoch 3] Batch 4352, Loss 0.36835846304893494\n","[Training Epoch 3] Batch 4353, Loss 0.37659454345703125\n","[Training Epoch 3] Batch 4354, Loss 0.3920168876647949\n","[Training Epoch 3] Batch 4355, Loss 0.3929060697555542\n","[Training Epoch 3] Batch 4356, Loss 0.3857639729976654\n","[Training Epoch 3] Batch 4357, Loss 0.3733280897140503\n","[Training Epoch 3] Batch 4358, Loss 0.3673976957798004\n","[Training Epoch 3] Batch 4359, Loss 0.3547283709049225\n","[Training Epoch 3] Batch 4360, Loss 0.3886564075946808\n","[Training Epoch 3] Batch 4361, Loss 0.38664114475250244\n","[Training Epoch 3] Batch 4362, Loss 0.3716675043106079\n","[Training Epoch 3] Batch 4363, Loss 0.36607518792152405\n","[Training Epoch 3] Batch 4364, Loss 0.358612060546875\n","[Training Epoch 3] Batch 4365, Loss 0.4041406512260437\n","[Training Epoch 3] Batch 4366, Loss 0.34499168395996094\n","[Training Epoch 3] Batch 4367, Loss 0.3934897184371948\n","[Training Epoch 3] Batch 4368, Loss 0.38523975014686584\n","[Training Epoch 3] Batch 4369, Loss 0.3900420069694519\n","[Training Epoch 3] Batch 4370, Loss 0.35826772451400757\n","[Training Epoch 3] Batch 4371, Loss 0.37337154150009155\n","[Training Epoch 3] Batch 4372, Loss 0.39474767446517944\n","[Training Epoch 3] Batch 4373, Loss 0.3794172406196594\n","[Training Epoch 3] Batch 4374, Loss 0.38925185799598694\n","[Training Epoch 3] Batch 4375, Loss 0.3690890371799469\n","[Training Epoch 3] Batch 4376, Loss 0.384876012802124\n","[Training Epoch 3] Batch 4377, Loss 0.35349106788635254\n","[Training Epoch 3] Batch 4378, Loss 0.37273508310317993\n","[Training Epoch 3] Batch 4379, Loss 0.35100552439689636\n","[Training Epoch 3] Batch 4380, Loss 0.37613651156425476\n","[Training Epoch 3] Batch 4381, Loss 0.3809490501880646\n","[Training Epoch 3] Batch 4382, Loss 0.4075087010860443\n","[Training Epoch 3] Batch 4383, Loss 0.3729470372200012\n","[Training Epoch 3] Batch 4384, Loss 0.3700496554374695\n","[Training Epoch 3] Batch 4385, Loss 0.38386011123657227\n","[Training Epoch 3] Batch 4386, Loss 0.3832238018512726\n","[Training Epoch 3] Batch 4387, Loss 0.3914576768875122\n","[Training Epoch 3] Batch 4388, Loss 0.35173940658569336\n","[Training Epoch 3] Batch 4389, Loss 0.3749360144138336\n","[Training Epoch 3] Batch 4390, Loss 0.3873775601387024\n","[Training Epoch 3] Batch 4391, Loss 0.3535939157009125\n","[Training Epoch 3] Batch 4392, Loss 0.3661446273326874\n","[Training Epoch 3] Batch 4393, Loss 0.3935883641242981\n","[Training Epoch 3] Batch 4394, Loss 0.369137704372406\n","[Training Epoch 3] Batch 4395, Loss 0.34967491030693054\n","[Training Epoch 3] Batch 4396, Loss 0.38978511095046997\n","[Training Epoch 3] Batch 4397, Loss 0.3853955864906311\n","[Training Epoch 3] Batch 4398, Loss 0.36176109313964844\n","[Training Epoch 3] Batch 4399, Loss 0.3898404836654663\n","[Training Epoch 3] Batch 4400, Loss 0.38047224283218384\n","[Training Epoch 3] Batch 4401, Loss 0.36850088834762573\n","[Training Epoch 3] Batch 4402, Loss 0.3747785687446594\n","[Training Epoch 3] Batch 4403, Loss 0.38718271255493164\n","[Training Epoch 3] Batch 4404, Loss 0.39474576711654663\n","[Training Epoch 3] Batch 4405, Loss 0.3933297097682953\n","[Training Epoch 3] Batch 4406, Loss 0.37342581152915955\n","[Training Epoch 3] Batch 4407, Loss 0.3521597981452942\n","[Training Epoch 3] Batch 4408, Loss 0.3492891788482666\n","[Training Epoch 3] Batch 4409, Loss 0.43957430124282837\n","[Training Epoch 3] Batch 4410, Loss 0.3787609934806824\n","[Training Epoch 3] Batch 4411, Loss 0.4197365939617157\n","[Training Epoch 3] Batch 4412, Loss 0.3496270775794983\n","[Training Epoch 3] Batch 4413, Loss 0.3892841637134552\n","[Training Epoch 3] Batch 4414, Loss 0.3764914274215698\n","[Training Epoch 3] Batch 4415, Loss 0.38037997484207153\n","[Training Epoch 3] Batch 4416, Loss 0.3725784718990326\n","[Training Epoch 3] Batch 4417, Loss 0.3882402181625366\n","[Training Epoch 3] Batch 4418, Loss 0.3467797040939331\n","[Training Epoch 3] Batch 4419, Loss 0.3623010516166687\n","[Training Epoch 3] Batch 4420, Loss 0.34179919958114624\n","[Training Epoch 3] Batch 4421, Loss 0.3961493968963623\n","[Training Epoch 3] Batch 4422, Loss 0.3630122244358063\n","[Training Epoch 3] Batch 4423, Loss 0.37148550152778625\n","[Training Epoch 3] Batch 4424, Loss 0.3740338683128357\n","[Training Epoch 3] Batch 4425, Loss 0.3727248013019562\n","[Training Epoch 3] Batch 4426, Loss 0.3680471181869507\n","[Training Epoch 3] Batch 4427, Loss 0.36022475361824036\n","[Training Epoch 3] Batch 4428, Loss 0.3897246718406677\n","[Training Epoch 3] Batch 4429, Loss 0.3335246443748474\n","[Training Epoch 3] Batch 4430, Loss 0.38477975130081177\n","[Training Epoch 3] Batch 4431, Loss 0.3858509361743927\n","[Training Epoch 3] Batch 4432, Loss 0.3722078204154968\n","[Training Epoch 3] Batch 4433, Loss 0.3810329735279083\n","[Training Epoch 3] Batch 4434, Loss 0.3659919202327728\n","[Training Epoch 3] Batch 4435, Loss 0.3590378165245056\n","[Training Epoch 3] Batch 4436, Loss 0.3938671052455902\n","[Training Epoch 3] Batch 4437, Loss 0.368411123752594\n","[Training Epoch 3] Batch 4438, Loss 0.3855588734149933\n","[Training Epoch 3] Batch 4439, Loss 0.36129969358444214\n","[Training Epoch 3] Batch 4440, Loss 0.38003677129745483\n","[Training Epoch 3] Batch 4441, Loss 0.3615100383758545\n","[Training Epoch 3] Batch 4442, Loss 0.34992721676826477\n","[Training Epoch 3] Batch 4443, Loss 0.36541634798049927\n","[Training Epoch 3] Batch 4444, Loss 0.37576478719711304\n","[Training Epoch 3] Batch 4445, Loss 0.3644622266292572\n","[Training Epoch 3] Batch 4446, Loss 0.37904730439186096\n","[Training Epoch 3] Batch 4447, Loss 0.3463372588157654\n","[Training Epoch 3] Batch 4448, Loss 0.3949374556541443\n","[Training Epoch 3] Batch 4449, Loss 0.3886636793613434\n","[Training Epoch 3] Batch 4450, Loss 0.4000498056411743\n","[Training Epoch 3] Batch 4451, Loss 0.37401261925697327\n","[Training Epoch 3] Batch 4452, Loss 0.39364543557167053\n","[Training Epoch 3] Batch 4453, Loss 0.38536393642425537\n","[Training Epoch 3] Batch 4454, Loss 0.3626527190208435\n","[Training Epoch 3] Batch 4455, Loss 0.36559614539146423\n","[Training Epoch 3] Batch 4456, Loss 0.3129189610481262\n","[Training Epoch 3] Batch 4457, Loss 0.3645240366458893\n","[Training Epoch 3] Batch 4458, Loss 0.38623905181884766\n","[Training Epoch 3] Batch 4459, Loss 0.4056565463542938\n","[Training Epoch 3] Batch 4460, Loss 0.392024964094162\n","[Training Epoch 3] Batch 4461, Loss 0.3952784836292267\n","[Training Epoch 3] Batch 4462, Loss 0.36896729469299316\n","[Training Epoch 3] Batch 4463, Loss 0.3545229732990265\n","[Training Epoch 3] Batch 4464, Loss 0.419636607170105\n","[Training Epoch 3] Batch 4465, Loss 0.3798002600669861\n","[Training Epoch 3] Batch 4466, Loss 0.40018680691719055\n","[Training Epoch 3] Batch 4467, Loss 0.38942059874534607\n","[Training Epoch 3] Batch 4468, Loss 0.37973371148109436\n","[Training Epoch 3] Batch 4469, Loss 0.3903319537639618\n","[Training Epoch 3] Batch 4470, Loss 0.3714976906776428\n","[Training Epoch 3] Batch 4471, Loss 0.3706924021244049\n","[Training Epoch 3] Batch 4472, Loss 0.41119611263275146\n","[Training Epoch 3] Batch 4473, Loss 0.3619299530982971\n","[Training Epoch 3] Batch 4474, Loss 0.3792285919189453\n","[Training Epoch 3] Batch 4475, Loss 0.3696489930152893\n","[Training Epoch 3] Batch 4476, Loss 0.35677871108055115\n","[Training Epoch 3] Batch 4477, Loss 0.38960638642311096\n","[Training Epoch 3] Batch 4478, Loss 0.42032456398010254\n","[Training Epoch 3] Batch 4479, Loss 0.4008733630180359\n","[Training Epoch 3] Batch 4480, Loss 0.3924461603164673\n","[Training Epoch 3] Batch 4481, Loss 0.41968950629234314\n","[Training Epoch 3] Batch 4482, Loss 0.3884021043777466\n","[Training Epoch 3] Batch 4483, Loss 0.3896951675415039\n","[Training Epoch 3] Batch 4484, Loss 0.391269713640213\n","[Training Epoch 3] Batch 4485, Loss 0.3740301728248596\n","[Training Epoch 3] Batch 4486, Loss 0.3998976945877075\n","[Training Epoch 3] Batch 4487, Loss 0.3997957110404968\n","[Training Epoch 3] Batch 4488, Loss 0.37902045249938965\n","[Training Epoch 3] Batch 4489, Loss 0.3698410987854004\n","[Training Epoch 3] Batch 4490, Loss 0.36324796080589294\n","[Training Epoch 3] Batch 4491, Loss 0.3690617084503174\n","[Training Epoch 3] Batch 4492, Loss 0.3999062776565552\n","[Training Epoch 3] Batch 4493, Loss 0.38505804538726807\n","[Training Epoch 3] Batch 4494, Loss 0.3795951306819916\n","[Training Epoch 3] Batch 4495, Loss 0.3942702114582062\n","[Training Epoch 3] Batch 4496, Loss 0.3579716086387634\n","[Training Epoch 3] Batch 4497, Loss 0.3856259882450104\n","[Training Epoch 3] Batch 4498, Loss 0.3987300992012024\n","[Training Epoch 3] Batch 4499, Loss 0.3856653571128845\n","[Training Epoch 3] Batch 4500, Loss 0.35715043544769287\n","[Training Epoch 3] Batch 4501, Loss 0.4083348512649536\n","[Training Epoch 3] Batch 4502, Loss 0.3877395689487457\n","[Training Epoch 3] Batch 4503, Loss 0.37657153606414795\n","[Training Epoch 3] Batch 4504, Loss 0.3789936900138855\n","[Training Epoch 3] Batch 4505, Loss 0.37757396697998047\n","[Training Epoch 3] Batch 4506, Loss 0.3833025097846985\n","[Training Epoch 3] Batch 4507, Loss 0.3657761514186859\n","[Training Epoch 3] Batch 4508, Loss 0.3518233001232147\n","[Training Epoch 3] Batch 4509, Loss 0.3453660011291504\n","[Training Epoch 3] Batch 4510, Loss 0.3709537386894226\n","[Training Epoch 3] Batch 4511, Loss 0.36504286527633667\n","[Training Epoch 3] Batch 4512, Loss 0.35532015562057495\n","[Training Epoch 3] Batch 4513, Loss 0.37716060876846313\n","[Training Epoch 3] Batch 4514, Loss 0.38916900753974915\n","[Training Epoch 3] Batch 4515, Loss 0.3507901132106781\n","[Training Epoch 3] Batch 4516, Loss 0.36998116970062256\n","[Training Epoch 3] Batch 4517, Loss 0.38601380586624146\n","[Training Epoch 3] Batch 4518, Loss 0.39992329478263855\n","[Training Epoch 3] Batch 4519, Loss 0.38351356983184814\n","[Training Epoch 3] Batch 4520, Loss 0.37780874967575073\n","[Training Epoch 3] Batch 4521, Loss 0.3768664002418518\n","[Training Epoch 3] Batch 4522, Loss 0.3624856173992157\n","[Training Epoch 3] Batch 4523, Loss 0.3602959215641022\n","[Training Epoch 3] Batch 4524, Loss 0.36387741565704346\n","[Training Epoch 3] Batch 4525, Loss 0.3619726896286011\n","[Training Epoch 3] Batch 4526, Loss 0.3636849522590637\n","[Training Epoch 3] Batch 4527, Loss 0.3917328715324402\n","[Training Epoch 3] Batch 4528, Loss 0.3574231266975403\n","[Training Epoch 3] Batch 4529, Loss 0.3953348696231842\n","[Training Epoch 3] Batch 4530, Loss 0.37735816836357117\n","[Training Epoch 3] Batch 4531, Loss 0.3513886332511902\n","[Training Epoch 3] Batch 4532, Loss 0.37331622838974\n","[Training Epoch 3] Batch 4533, Loss 0.35690632462501526\n","[Training Epoch 3] Batch 4534, Loss 0.3744533956050873\n","[Training Epoch 3] Batch 4535, Loss 0.349261075258255\n","[Training Epoch 3] Batch 4536, Loss 0.3339943587779999\n","[Training Epoch 3] Batch 4537, Loss 0.3861789107322693\n","[Training Epoch 3] Batch 4538, Loss 0.371814101934433\n","[Training Epoch 3] Batch 4539, Loss 0.36666327714920044\n","[Training Epoch 3] Batch 4540, Loss 0.3757059872150421\n","[Training Epoch 3] Batch 4541, Loss 0.3752840757369995\n","[Training Epoch 3] Batch 4542, Loss 0.41808000206947327\n","[Training Epoch 3] Batch 4543, Loss 0.3587883412837982\n","[Training Epoch 3] Batch 4544, Loss 0.39805248379707336\n","[Training Epoch 3] Batch 4545, Loss 0.3979288339614868\n","[Training Epoch 3] Batch 4546, Loss 0.37786561250686646\n","[Training Epoch 3] Batch 4547, Loss 0.3758680820465088\n","[Training Epoch 3] Batch 4548, Loss 0.38469192385673523\n","[Training Epoch 3] Batch 4549, Loss 0.3790185749530792\n","[Training Epoch 3] Batch 4550, Loss 0.40541672706604004\n","[Training Epoch 3] Batch 4551, Loss 0.3752554655075073\n","[Training Epoch 3] Batch 4552, Loss 0.388791024684906\n","[Training Epoch 3] Batch 4553, Loss 0.3936460018157959\n","[Training Epoch 3] Batch 4554, Loss 0.4007394313812256\n","[Training Epoch 3] Batch 4555, Loss 0.3708330988883972\n","[Training Epoch 3] Batch 4556, Loss 0.3789677619934082\n","[Training Epoch 3] Batch 4557, Loss 0.33909860253334045\n","[Training Epoch 3] Batch 4558, Loss 0.36604875326156616\n","[Training Epoch 3] Batch 4559, Loss 0.37272509932518005\n","[Training Epoch 3] Batch 4560, Loss 0.39178746938705444\n","[Training Epoch 3] Batch 4561, Loss 0.37023359537124634\n","[Training Epoch 3] Batch 4562, Loss 0.3775346577167511\n","[Training Epoch 3] Batch 4563, Loss 0.40924233198165894\n","[Training Epoch 3] Batch 4564, Loss 0.40737688541412354\n","[Training Epoch 3] Batch 4565, Loss 0.35204774141311646\n","[Training Epoch 3] Batch 4566, Loss 0.3922322690486908\n","[Training Epoch 3] Batch 4567, Loss 0.38076066970825195\n","[Training Epoch 3] Batch 4568, Loss 0.38580769300460815\n","[Training Epoch 3] Batch 4569, Loss 0.3987792730331421\n","[Training Epoch 3] Batch 4570, Loss 0.3855136036872864\n","[Training Epoch 3] Batch 4571, Loss 0.3942137360572815\n","[Training Epoch 3] Batch 4572, Loss 0.3757905960083008\n","[Training Epoch 3] Batch 4573, Loss 0.3776102066040039\n","[Training Epoch 3] Batch 4574, Loss 0.32769909501075745\n","[Training Epoch 3] Batch 4575, Loss 0.38969457149505615\n","[Training Epoch 3] Batch 4576, Loss 0.36935916543006897\n","[Training Epoch 3] Batch 4577, Loss 0.37191522121429443\n","[Training Epoch 3] Batch 4578, Loss 0.4208572208881378\n","[Training Epoch 3] Batch 4579, Loss 0.40123873949050903\n","[Training Epoch 3] Batch 4580, Loss 0.3861464262008667\n","[Training Epoch 3] Batch 4581, Loss 0.38482773303985596\n","[Training Epoch 3] Batch 4582, Loss 0.38914725184440613\n","[Training Epoch 3] Batch 4583, Loss 0.36766982078552246\n","[Training Epoch 3] Batch 4584, Loss 0.3694351613521576\n","[Training Epoch 3] Batch 4585, Loss 0.39051398634910583\n","[Training Epoch 3] Batch 4586, Loss 0.38486480712890625\n","[Training Epoch 3] Batch 4587, Loss 0.40088438987731934\n","[Training Epoch 3] Batch 4588, Loss 0.398480087518692\n","[Training Epoch 3] Batch 4589, Loss 0.3682136833667755\n","[Training Epoch 3] Batch 4590, Loss 0.36576467752456665\n","[Training Epoch 3] Batch 4591, Loss 0.3459821343421936\n","[Training Epoch 3] Batch 4592, Loss 0.3630679249763489\n","[Training Epoch 3] Batch 4593, Loss 0.3615034520626068\n","[Training Epoch 3] Batch 4594, Loss 0.3825927972793579\n","[Training Epoch 3] Batch 4595, Loss 0.40987783670425415\n","[Training Epoch 3] Batch 4596, Loss 0.3556411862373352\n","[Training Epoch 3] Batch 4597, Loss 0.402587890625\n","[Training Epoch 3] Batch 4598, Loss 0.40698161721229553\n","[Training Epoch 3] Batch 4599, Loss 0.3626127243041992\n","[Training Epoch 3] Batch 4600, Loss 0.40306562185287476\n","[Training Epoch 3] Batch 4601, Loss 0.3847598433494568\n","[Training Epoch 3] Batch 4602, Loss 0.38804417848587036\n","[Training Epoch 3] Batch 4603, Loss 0.41708335280418396\n","[Training Epoch 3] Batch 4604, Loss 0.35909855365753174\n","[Training Epoch 3] Batch 4605, Loss 0.38907063007354736\n","[Training Epoch 3] Batch 4606, Loss 0.41395822167396545\n","[Training Epoch 3] Batch 4607, Loss 0.3839215934276581\n","[Training Epoch 3] Batch 4608, Loss 0.38640516996383667\n","[Training Epoch 3] Batch 4609, Loss 0.39987680315971375\n","[Training Epoch 3] Batch 4610, Loss 0.3820752203464508\n","[Training Epoch 3] Batch 4611, Loss 0.37922918796539307\n","[Training Epoch 3] Batch 4612, Loss 0.39332300424575806\n","[Training Epoch 3] Batch 4613, Loss 0.3766189217567444\n","[Training Epoch 3] Batch 4614, Loss 0.3831360936164856\n","[Training Epoch 3] Batch 4615, Loss 0.40748780965805054\n","[Training Epoch 3] Batch 4616, Loss 0.427368700504303\n","[Training Epoch 3] Batch 4617, Loss 0.3768860697746277\n","[Training Epoch 3] Batch 4618, Loss 0.3577256202697754\n","[Training Epoch 3] Batch 4619, Loss 0.3773380219936371\n","[Training Epoch 3] Batch 4620, Loss 0.38796234130859375\n","[Training Epoch 3] Batch 4621, Loss 0.3570559024810791\n","[Training Epoch 3] Batch 4622, Loss 0.3497895896434784\n","[Training Epoch 3] Batch 4623, Loss 0.3580685257911682\n","[Training Epoch 3] Batch 4624, Loss 0.3808378577232361\n","[Training Epoch 3] Batch 4625, Loss 0.4033850431442261\n","[Training Epoch 3] Batch 4626, Loss 0.3615700304508209\n","[Training Epoch 3] Batch 4627, Loss 0.39198243618011475\n","[Training Epoch 3] Batch 4628, Loss 0.38444334268569946\n","[Training Epoch 3] Batch 4629, Loss 0.3452383577823639\n","[Training Epoch 3] Batch 4630, Loss 0.37224239110946655\n","[Training Epoch 3] Batch 4631, Loss 0.365886926651001\n","[Training Epoch 3] Batch 4632, Loss 0.341698557138443\n","[Training Epoch 3] Batch 4633, Loss 0.3785821497440338\n","[Training Epoch 3] Batch 4634, Loss 0.3560885787010193\n","[Training Epoch 3] Batch 4635, Loss 0.3973364233970642\n","[Training Epoch 3] Batch 4636, Loss 0.38751426339149475\n","[Training Epoch 3] Batch 4637, Loss 0.3529966473579407\n","[Training Epoch 3] Batch 4638, Loss 0.40752387046813965\n","[Training Epoch 3] Batch 4639, Loss 0.36782705783843994\n","[Training Epoch 3] Batch 4640, Loss 0.3765677213668823\n","[Training Epoch 3] Batch 4641, Loss 0.38028693199157715\n","[Training Epoch 3] Batch 4642, Loss 0.37787747383117676\n","[Training Epoch 3] Batch 4643, Loss 0.35877692699432373\n","[Training Epoch 3] Batch 4644, Loss 0.3920031189918518\n","[Training Epoch 3] Batch 4645, Loss 0.38447415828704834\n","[Training Epoch 3] Batch 4646, Loss 0.36867988109588623\n","[Training Epoch 3] Batch 4647, Loss 0.3640097975730896\n","[Training Epoch 3] Batch 4648, Loss 0.3762696087360382\n","[Training Epoch 3] Batch 4649, Loss 0.3550755977630615\n","[Training Epoch 3] Batch 4650, Loss 0.37522098422050476\n","[Training Epoch 3] Batch 4651, Loss 0.3515886068344116\n","[Training Epoch 3] Batch 4652, Loss 0.3822058439254761\n","[Training Epoch 3] Batch 4653, Loss 0.37183356285095215\n","[Training Epoch 3] Batch 4654, Loss 0.37802305817604065\n","[Training Epoch 3] Batch 4655, Loss 0.3955451250076294\n","[Training Epoch 3] Batch 4656, Loss 0.3858683407306671\n","[Training Epoch 3] Batch 4657, Loss 0.3772770166397095\n","[Training Epoch 3] Batch 4658, Loss 0.3834468424320221\n","[Training Epoch 3] Batch 4659, Loss 0.39337047934532166\n","[Training Epoch 3] Batch 4660, Loss 0.39734306931495667\n","[Training Epoch 3] Batch 4661, Loss 0.36108335852622986\n","[Training Epoch 3] Batch 4662, Loss 0.36546504497528076\n","[Training Epoch 3] Batch 4663, Loss 0.38476940989494324\n","[Training Epoch 3] Batch 4664, Loss 0.3599008619785309\n","[Training Epoch 3] Batch 4665, Loss 0.42703282833099365\n","[Training Epoch 3] Batch 4666, Loss 0.3937342166900635\n","[Training Epoch 3] Batch 4667, Loss 0.3568499684333801\n","[Training Epoch 3] Batch 4668, Loss 0.39085012674331665\n","[Training Epoch 3] Batch 4669, Loss 0.3911418318748474\n","[Training Epoch 3] Batch 4670, Loss 0.38510647416114807\n","[Training Epoch 3] Batch 4671, Loss 0.3482391834259033\n","[Training Epoch 3] Batch 4672, Loss 0.3542124032974243\n","[Training Epoch 3] Batch 4673, Loss 0.34936922788619995\n","[Training Epoch 3] Batch 4674, Loss 0.3601473569869995\n","[Training Epoch 3] Batch 4675, Loss 0.36058518290519714\n","[Training Epoch 3] Batch 4676, Loss 0.4000532329082489\n","[Training Epoch 3] Batch 4677, Loss 0.3949926197528839\n","[Training Epoch 3] Batch 4678, Loss 0.3768342137336731\n","[Training Epoch 3] Batch 4679, Loss 0.3994506001472473\n","[Training Epoch 3] Batch 4680, Loss 0.35875874757766724\n","[Training Epoch 3] Batch 4681, Loss 0.3460189700126648\n","[Training Epoch 3] Batch 4682, Loss 0.37824541330337524\n","[Training Epoch 3] Batch 4683, Loss 0.3903336822986603\n","[Training Epoch 3] Batch 4684, Loss 0.3653107285499573\n","[Training Epoch 3] Batch 4685, Loss 0.357505738735199\n","[Training Epoch 3] Batch 4686, Loss 0.37876060605049133\n","[Training Epoch 3] Batch 4687, Loss 0.38587355613708496\n","[Training Epoch 3] Batch 4688, Loss 0.38033032417297363\n","[Training Epoch 3] Batch 4689, Loss 0.3898913264274597\n","[Training Epoch 3] Batch 4690, Loss 0.3966611921787262\n","[Training Epoch 3] Batch 4691, Loss 0.36998093128204346\n","[Training Epoch 3] Batch 4692, Loss 0.40756672620773315\n","[Training Epoch 3] Batch 4693, Loss 0.37523192167282104\n","[Training Epoch 3] Batch 4694, Loss 0.36725807189941406\n","[Training Epoch 3] Batch 4695, Loss 0.3798812925815582\n","[Training Epoch 3] Batch 4696, Loss 0.3694942593574524\n","[Training Epoch 3] Batch 4697, Loss 0.35102784633636475\n","[Training Epoch 3] Batch 4698, Loss 0.3805277943611145\n","[Training Epoch 3] Batch 4699, Loss 0.39777451753616333\n","[Training Epoch 3] Batch 4700, Loss 0.3911114037036896\n","[Training Epoch 3] Batch 4701, Loss 0.360965371131897\n","[Training Epoch 3] Batch 4702, Loss 0.38411790132522583\n","[Training Epoch 3] Batch 4703, Loss 0.3833357095718384\n","[Training Epoch 3] Batch 4704, Loss 0.3693917989730835\n","[Training Epoch 3] Batch 4705, Loss 0.3882151246070862\n","[Training Epoch 3] Batch 4706, Loss 0.35480499267578125\n","[Training Epoch 3] Batch 4707, Loss 0.36083856225013733\n","[Training Epoch 3] Batch 4708, Loss 0.37868043780326843\n","[Training Epoch 3] Batch 4709, Loss 0.4107663035392761\n","[Training Epoch 3] Batch 4710, Loss 0.3827195167541504\n","[Training Epoch 3] Batch 4711, Loss 0.41061699390411377\n","[Training Epoch 3] Batch 4712, Loss 0.3935222327709198\n","[Training Epoch 3] Batch 4713, Loss 0.33744385838508606\n","[Training Epoch 3] Batch 4714, Loss 0.3418363332748413\n","[Training Epoch 3] Batch 4715, Loss 0.37240663170814514\n","[Training Epoch 3] Batch 4716, Loss 0.3995060324668884\n","[Training Epoch 3] Batch 4717, Loss 0.3870846629142761\n","[Training Epoch 3] Batch 4718, Loss 0.3741128444671631\n","[Training Epoch 3] Batch 4719, Loss 0.3990710377693176\n","[Training Epoch 3] Batch 4720, Loss 0.3652527332305908\n","[Training Epoch 3] Batch 4721, Loss 0.37172210216522217\n","[Training Epoch 3] Batch 4722, Loss 0.3912679851055145\n","[Training Epoch 3] Batch 4723, Loss 0.38329097628593445\n","[Training Epoch 3] Batch 4724, Loss 0.3841814398765564\n","[Training Epoch 3] Batch 4725, Loss 0.36009281873703003\n","[Training Epoch 3] Batch 4726, Loss 0.3723580837249756\n","[Training Epoch 3] Batch 4727, Loss 0.3872582018375397\n","[Training Epoch 3] Batch 4728, Loss 0.37878063321113586\n","[Training Epoch 3] Batch 4729, Loss 0.3391175866127014\n","[Training Epoch 3] Batch 4730, Loss 0.372353196144104\n","[Training Epoch 3] Batch 4731, Loss 0.3725070357322693\n","[Training Epoch 3] Batch 4732, Loss 0.40186741948127747\n","[Training Epoch 3] Batch 4733, Loss 0.36538684368133545\n","[Training Epoch 3] Batch 4734, Loss 0.4009213149547577\n","[Training Epoch 3] Batch 4735, Loss 0.3817884624004364\n","[Training Epoch 3] Batch 4736, Loss 0.3590589761734009\n","[Training Epoch 3] Batch 4737, Loss 0.3958083987236023\n","[Training Epoch 3] Batch 4738, Loss 0.3677271008491516\n","[Training Epoch 3] Batch 4739, Loss 0.42486950755119324\n","[Training Epoch 3] Batch 4740, Loss 0.4114115834236145\n","[Training Epoch 3] Batch 4741, Loss 0.3975004553794861\n","[Training Epoch 3] Batch 4742, Loss 0.3862001299858093\n","[Training Epoch 3] Batch 4743, Loss 0.3230859935283661\n","[Training Epoch 3] Batch 4744, Loss 0.3855399191379547\n","[Training Epoch 3] Batch 4745, Loss 0.39117830991744995\n","[Training Epoch 3] Batch 4746, Loss 0.37947067618370056\n","[Training Epoch 3] Batch 4747, Loss 0.3458755612373352\n","[Training Epoch 3] Batch 4748, Loss 0.40594080090522766\n","[Training Epoch 3] Batch 4749, Loss 0.3640627861022949\n","[Training Epoch 3] Batch 4750, Loss 0.3696416914463043\n","[Training Epoch 3] Batch 4751, Loss 0.3800490200519562\n","[Training Epoch 3] Batch 4752, Loss 0.3700331449508667\n","[Training Epoch 3] Batch 4753, Loss 0.3880581259727478\n","[Training Epoch 3] Batch 4754, Loss 0.3985782861709595\n","[Training Epoch 3] Batch 4755, Loss 0.36109429597854614\n","[Training Epoch 3] Batch 4756, Loss 0.39342325925827026\n","[Training Epoch 3] Batch 4757, Loss 0.3613058030605316\n","[Training Epoch 3] Batch 4758, Loss 0.3873146176338196\n","[Training Epoch 3] Batch 4759, Loss 0.3873368799686432\n","[Training Epoch 3] Batch 4760, Loss 0.35582542419433594\n","[Training Epoch 3] Batch 4761, Loss 0.39146339893341064\n","[Training Epoch 3] Batch 4762, Loss 0.36498352885246277\n","[Training Epoch 3] Batch 4763, Loss 0.3741106390953064\n","[Training Epoch 3] Batch 4764, Loss 0.39995482563972473\n","[Training Epoch 3] Batch 4765, Loss 0.3887985348701477\n","[Training Epoch 3] Batch 4766, Loss 0.34457212686538696\n","[Training Epoch 3] Batch 4767, Loss 0.39093655347824097\n","[Training Epoch 3] Batch 4768, Loss 0.34959661960601807\n","[Training Epoch 3] Batch 4769, Loss 0.37917178869247437\n","[Training Epoch 3] Batch 4770, Loss 0.3643191456794739\n","[Training Epoch 3] Batch 4771, Loss 0.39239346981048584\n","[Training Epoch 3] Batch 4772, Loss 0.3941173553466797\n","[Training Epoch 3] Batch 4773, Loss 0.36985090374946594\n","[Training Epoch 3] Batch 4774, Loss 0.3891475200653076\n","[Training Epoch 3] Batch 4775, Loss 0.39377540349960327\n","[Training Epoch 3] Batch 4776, Loss 0.3863446116447449\n","[Training Epoch 3] Batch 4777, Loss 0.39196836948394775\n","[Training Epoch 3] Batch 4778, Loss 0.3958607316017151\n","[Training Epoch 3] Batch 4779, Loss 0.3854157626628876\n","[Training Epoch 3] Batch 4780, Loss 0.36981481313705444\n","[Training Epoch 3] Batch 4781, Loss 0.3792268633842468\n","[Training Epoch 3] Batch 4782, Loss 0.3672848641872406\n","[Training Epoch 3] Batch 4783, Loss 0.3598102629184723\n","[Training Epoch 3] Batch 4784, Loss 0.3621803820133209\n","[Training Epoch 3] Batch 4785, Loss 0.39250004291534424\n","[Training Epoch 3] Batch 4786, Loss 0.3420290946960449\n","[Training Epoch 3] Batch 4787, Loss 0.3611605167388916\n","[Training Epoch 3] Batch 4788, Loss 0.35357123613357544\n","[Training Epoch 3] Batch 4789, Loss 0.3510033190250397\n","[Training Epoch 3] Batch 4790, Loss 0.3933992087841034\n","[Training Epoch 3] Batch 4791, Loss 0.3920198678970337\n","[Training Epoch 3] Batch 4792, Loss 0.3700062036514282\n","[Training Epoch 3] Batch 4793, Loss 0.3754097819328308\n","[Training Epoch 3] Batch 4794, Loss 0.3867477774620056\n","[Training Epoch 3] Batch 4795, Loss 0.3874480128288269\n","[Training Epoch 3] Batch 4796, Loss 0.37201380729675293\n","[Training Epoch 3] Batch 4797, Loss 0.3681466579437256\n","[Training Epoch 3] Batch 4798, Loss 0.39344140887260437\n","[Training Epoch 3] Batch 4799, Loss 0.37320148944854736\n","[Training Epoch 3] Batch 4800, Loss 0.3668283224105835\n","[Training Epoch 3] Batch 4801, Loss 0.3672306537628174\n","[Training Epoch 3] Batch 4802, Loss 0.39365124702453613\n","[Training Epoch 3] Batch 4803, Loss 0.33840659260749817\n","[Training Epoch 3] Batch 4804, Loss 0.39409324526786804\n","[Training Epoch 3] Batch 4805, Loss 0.38538235425949097\n","[Training Epoch 3] Batch 4806, Loss 0.35545194149017334\n","[Training Epoch 3] Batch 4807, Loss 0.3788754343986511\n","[Training Epoch 3] Batch 4808, Loss 0.37044817209243774\n","[Training Epoch 3] Batch 4809, Loss 0.3949446678161621\n","[Training Epoch 3] Batch 4810, Loss 0.3801116943359375\n","[Training Epoch 3] Batch 4811, Loss 0.3749164342880249\n","[Training Epoch 3] Batch 4812, Loss 0.3676219880580902\n","[Training Epoch 3] Batch 4813, Loss 0.35740476846694946\n","[Training Epoch 3] Batch 4814, Loss 0.3914415240287781\n","[Training Epoch 3] Batch 4815, Loss 0.35839927196502686\n","[Training Epoch 3] Batch 4816, Loss 0.3885000944137573\n","[Training Epoch 3] Batch 4817, Loss 0.3599011301994324\n","[Training Epoch 3] Batch 4818, Loss 0.4176156222820282\n","[Training Epoch 3] Batch 4819, Loss 0.37688007950782776\n","[Training Epoch 3] Batch 4820, Loss 0.3812243938446045\n","[Training Epoch 3] Batch 4821, Loss 0.3522469997406006\n","[Training Epoch 3] Batch 4822, Loss 0.4014030396938324\n","[Training Epoch 3] Batch 4823, Loss 0.3757677376270294\n","[Training Epoch 3] Batch 4824, Loss 0.3659247159957886\n","[Training Epoch 3] Batch 4825, Loss 0.373940110206604\n","[Training Epoch 3] Batch 4826, Loss 0.35304009914398193\n","[Training Epoch 3] Batch 4827, Loss 0.40526682138442993\n","[Training Epoch 3] Batch 4828, Loss 0.3877237141132355\n","[Training Epoch 3] Batch 4829, Loss 0.3732949197292328\n","[Training Epoch 3] Batch 4830, Loss 0.3850095272064209\n","[Training Epoch 3] Batch 4831, Loss 0.3915737271308899\n","[Training Epoch 3] Batch 4832, Loss 0.35174816846847534\n","[Training Epoch 3] Batch 4833, Loss 0.3936058580875397\n","[Training Epoch 3] Batch 4834, Loss 0.3826100528240204\n","[Training Epoch 3] Batch 4835, Loss 0.38964205980300903\n","[Training Epoch 3] Batch 4836, Loss 0.38019508123397827\n","[Training Epoch 3] Batch 4837, Loss 0.37715157866477966\n","[Training Epoch 3] Batch 4838, Loss 0.38290756940841675\n","[Training Epoch 3] Batch 4839, Loss 0.36206331849098206\n","[Training Epoch 3] Batch 4840, Loss 0.39591044187545776\n","[Training Epoch 3] Batch 4841, Loss 0.352799654006958\n","[Training Epoch 3] Batch 4842, Loss 0.3815435767173767\n","[Training Epoch 3] Batch 4843, Loss 0.3623144030570984\n","[Training Epoch 3] Batch 4844, Loss 0.40365365147590637\n","[Training Epoch 3] Batch 4845, Loss 0.36845219135284424\n","[Training Epoch 3] Batch 4846, Loss 0.39819207787513733\n","[Training Epoch 3] Batch 4847, Loss 0.37195345759391785\n","[Training Epoch 3] Batch 4848, Loss 0.4001023471355438\n","[Training Epoch 3] Batch 4849, Loss 0.34286630153656006\n","[Training Epoch 3] Batch 4850, Loss 0.34382131695747375\n","[Training Epoch 3] Batch 4851, Loss 0.39552146196365356\n","[Training Epoch 3] Batch 4852, Loss 0.395639032125473\n","[Training Epoch 3] Batch 4853, Loss 0.37087902426719666\n","[Training Epoch 3] Batch 4854, Loss 0.412592351436615\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:52: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame.\n","Try using .loc[row_indexer,col_indexer] = value instead\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"]},{"name":"stdout","output_type":"stream","text":["[Evluating Epoch 3] HR = 0.3914, NDCG = 0.2182\n","Epoch 4 starts !\n","--------------------------------------------------------------------------------\n","[Training Epoch 4] Batch 0, Loss 0.36953601241111755\n","[Training Epoch 4] Batch 1, Loss 0.36633774638175964\n","[Training Epoch 4] Batch 2, Loss 0.3917433023452759\n","[Training Epoch 4] Batch 3, Loss 0.3812703490257263\n","[Training Epoch 4] Batch 4, Loss 0.36630260944366455\n","[Training Epoch 4] Batch 5, Loss 0.3642473816871643\n","[Training Epoch 4] Batch 6, Loss 0.36719125509262085\n","[Training Epoch 4] Batch 7, Loss 0.3666333556175232\n","[Training Epoch 4] Batch 8, Loss 0.3870319724082947\n","[Training Epoch 4] Batch 9, Loss 0.3790396451950073\n","[Training Epoch 4] Batch 10, Loss 0.35653233528137207\n","[Training Epoch 4] Batch 11, Loss 0.39046400785446167\n","[Training Epoch 4] Batch 12, Loss 0.3869267702102661\n","[Training Epoch 4] Batch 13, Loss 0.35957443714141846\n","[Training Epoch 4] Batch 14, Loss 0.36320433020591736\n","[Training Epoch 4] Batch 15, Loss 0.3968693017959595\n","[Training Epoch 4] Batch 16, Loss 0.3826918601989746\n","[Training Epoch 4] Batch 17, Loss 0.3596329987049103\n","[Training Epoch 4] Batch 18, Loss 0.3909366726875305\n","[Training Epoch 4] Batch 19, Loss 0.38653358817100525\n","[Training Epoch 4] Batch 20, Loss 0.39233726263046265\n","[Training Epoch 4] Batch 21, Loss 0.3885791301727295\n","[Training Epoch 4] Batch 22, Loss 0.3765717148780823\n","[Training Epoch 4] Batch 23, Loss 0.3519686162471771\n","[Training Epoch 4] Batch 24, Loss 0.36530357599258423\n","[Training Epoch 4] Batch 25, Loss 0.3884276747703552\n","[Training Epoch 4] Batch 26, Loss 0.3558148145675659\n","[Training Epoch 4] Batch 27, Loss 0.3632950782775879\n","[Training Epoch 4] Batch 28, Loss 0.39406949281692505\n","[Training Epoch 4] Batch 29, Loss 0.36697083711624146\n","[Training Epoch 4] Batch 30, Loss 0.38457584381103516\n","[Training Epoch 4] Batch 31, Loss 0.37303590774536133\n","[Training Epoch 4] Batch 32, Loss 0.3938088119029999\n","[Training Epoch 4] Batch 33, Loss 0.39659738540649414\n","[Training Epoch 4] Batch 34, Loss 0.3817979395389557\n","[Training Epoch 4] Batch 35, Loss 0.3653867840766907\n","[Training Epoch 4] Batch 36, Loss 0.40206730365753174\n","[Training Epoch 4] Batch 37, Loss 0.3996480703353882\n","[Training Epoch 4] Batch 38, Loss 0.3922756314277649\n","[Training Epoch 4] Batch 39, Loss 0.3948194682598114\n","[Training Epoch 4] Batch 40, Loss 0.35232770442962646\n","[Training Epoch 4] Batch 41, Loss 0.38276663422584534\n","[Training Epoch 4] Batch 42, Loss 0.3912014961242676\n","[Training Epoch 4] Batch 43, Loss 0.3470516502857208\n","[Training Epoch 4] Batch 44, Loss 0.3857651352882385\n","[Training Epoch 4] Batch 45, Loss 0.3785009980201721\n","[Training Epoch 4] Batch 46, Loss 0.3881496489048004\n","[Training Epoch 4] Batch 47, Loss 0.35297632217407227\n","[Training Epoch 4] Batch 48, Loss 0.39407879114151\n","[Training Epoch 4] Batch 49, Loss 0.3839167356491089\n","[Training Epoch 4] Batch 50, Loss 0.3659617304801941\n","[Training Epoch 4] Batch 51, Loss 0.36354726552963257\n","[Training Epoch 4] Batch 52, Loss 0.37015020847320557\n","[Training Epoch 4] Batch 53, Loss 0.3877641260623932\n","[Training Epoch 4] Batch 54, Loss 0.37429118156433105\n","[Training Epoch 4] Batch 55, Loss 0.3495441675186157\n","[Training Epoch 4] Batch 56, Loss 0.3840446174144745\n","[Training Epoch 4] Batch 57, Loss 0.3518086075782776\n","[Training Epoch 4] Batch 58, Loss 0.36584585905075073\n","[Training Epoch 4] Batch 59, Loss 0.3708682656288147\n","[Training Epoch 4] Batch 60, Loss 0.3832409381866455\n","[Training Epoch 4] Batch 61, Loss 0.3812469244003296\n","[Training Epoch 4] Batch 62, Loss 0.40093615651130676\n","[Training Epoch 4] Batch 63, Loss 0.3970784842967987\n","[Training Epoch 4] Batch 64, Loss 0.3783157467842102\n","[Training Epoch 4] Batch 65, Loss 0.3912138342857361\n","[Training Epoch 4] Batch 66, Loss 0.3627948462963104\n","[Training Epoch 4] Batch 67, Loss 0.37246614694595337\n","[Training Epoch 4] Batch 68, Loss 0.3758539855480194\n","[Training Epoch 4] Batch 69, Loss 0.3742598295211792\n","[Training Epoch 4] Batch 70, Loss 0.36679807305336\n","[Training Epoch 4] Batch 71, Loss 0.3917197585105896\n","[Training Epoch 4] Batch 72, Loss 0.36672234535217285\n","[Training Epoch 4] Batch 73, Loss 0.3716118335723877\n","[Training Epoch 4] Batch 74, Loss 0.3806428909301758\n","[Training Epoch 4] Batch 75, Loss 0.37520402669906616\n","[Training Epoch 4] Batch 76, Loss 0.36773693561553955\n","[Training Epoch 4] Batch 77, Loss 0.3868260085582733\n","[Training Epoch 4] Batch 78, Loss 0.38198065757751465\n","[Training Epoch 4] Batch 79, Loss 0.3848826587200165\n","[Training Epoch 4] Batch 80, Loss 0.3696051836013794\n","[Training Epoch 4] Batch 81, Loss 0.37948864698410034\n","[Training Epoch 4] Batch 82, Loss 0.38129326701164246\n","[Training Epoch 4] Batch 83, Loss 0.36252135038375854\n","[Training Epoch 4] Batch 84, Loss 0.37757614254951477\n","[Training Epoch 4] Batch 85, Loss 0.3655622899532318\n","[Training Epoch 4] Batch 86, Loss 0.3783467710018158\n","[Training Epoch 4] Batch 87, Loss 0.36899346113204956\n","[Training Epoch 4] Batch 88, Loss 0.393412321805954\n","[Training Epoch 4] Batch 89, Loss 0.3451074957847595\n","[Training Epoch 4] Batch 90, Loss 0.39113327860832214\n","[Training Epoch 4] Batch 91, Loss 0.3905356526374817\n","[Training Epoch 4] Batch 92, Loss 0.38943755626678467\n","[Training Epoch 4] Batch 93, Loss 0.4211546778678894\n","[Training Epoch 4] Batch 94, Loss 0.3934353291988373\n","[Training Epoch 4] Batch 95, Loss 0.3912501633167267\n","[Training Epoch 4] Batch 96, Loss 0.38043510913848877\n","[Training Epoch 4] Batch 97, Loss 0.36153167486190796\n","[Training Epoch 4] Batch 98, Loss 0.40748387575149536\n","[Training Epoch 4] Batch 99, Loss 0.38200628757476807\n","[Training Epoch 4] Batch 100, Loss 0.35302606225013733\n","[Training Epoch 4] Batch 101, Loss 0.3702104091644287\n","[Training Epoch 4] Batch 102, Loss 0.3856687545776367\n","[Training Epoch 4] Batch 103, Loss 0.3834663927555084\n","[Training Epoch 4] Batch 104, Loss 0.3810426592826843\n","[Training Epoch 4] Batch 105, Loss 0.37332451343536377\n","[Training Epoch 4] Batch 106, Loss 0.37287259101867676\n","[Training Epoch 4] Batch 107, Loss 0.3870236277580261\n","[Training Epoch 4] Batch 108, Loss 0.3642188012599945\n","[Training Epoch 4] Batch 109, Loss 0.3436369001865387\n","[Training Epoch 4] Batch 110, Loss 0.36092114448547363\n","[Training Epoch 4] Batch 111, Loss 0.37188178300857544\n","[Training Epoch 4] Batch 112, Loss 0.3601505160331726\n","[Training Epoch 4] Batch 113, Loss 0.36792540550231934\n","[Training Epoch 4] Batch 114, Loss 0.38254597783088684\n","[Training Epoch 4] Batch 115, Loss 0.3863251805305481\n","[Training Epoch 4] Batch 116, Loss 0.38717594742774963\n","[Training Epoch 4] Batch 117, Loss 0.3697971999645233\n","[Training Epoch 4] Batch 118, Loss 0.3692203760147095\n","[Training Epoch 4] Batch 119, Loss 0.36041587591171265\n","[Training Epoch 4] Batch 120, Loss 0.4014261066913605\n","[Training Epoch 4] Batch 121, Loss 0.3768921494483948\n","[Training Epoch 4] Batch 122, Loss 0.3807796537876129\n","[Training Epoch 4] Batch 123, Loss 0.3535628020763397\n","[Training Epoch 4] Batch 124, Loss 0.35853224992752075\n","[Training Epoch 4] Batch 125, Loss 0.36079302430152893\n","[Training Epoch 4] Batch 126, Loss 0.3550734221935272\n","[Training Epoch 4] Batch 127, Loss 0.37204211950302124\n","[Training Epoch 4] Batch 128, Loss 0.3615504503250122\n","[Training Epoch 4] Batch 129, Loss 0.3950817584991455\n","[Training Epoch 4] Batch 130, Loss 0.411405473947525\n","[Training Epoch 4] Batch 131, Loss 0.37546300888061523\n","[Training Epoch 4] Batch 132, Loss 0.3575649857521057\n","[Training Epoch 4] Batch 133, Loss 0.37809520959854126\n","[Training Epoch 4] Batch 134, Loss 0.39895960688591003\n","[Training Epoch 4] Batch 135, Loss 0.38266175985336304\n","[Training Epoch 4] Batch 136, Loss 0.38302791118621826\n","[Training Epoch 4] Batch 137, Loss 0.41528308391571045\n","[Training Epoch 4] Batch 138, Loss 0.3808528780937195\n","[Training Epoch 4] Batch 139, Loss 0.38601750135421753\n","[Training Epoch 4] Batch 140, Loss 0.36919960379600525\n","[Training Epoch 4] Batch 141, Loss 0.3496537208557129\n","[Training Epoch 4] Batch 142, Loss 0.3578488230705261\n","[Training Epoch 4] Batch 143, Loss 0.3900158405303955\n","[Training Epoch 4] Batch 144, Loss 0.3512915372848511\n","[Training Epoch 4] Batch 145, Loss 0.3746953010559082\n","[Training Epoch 4] Batch 146, Loss 0.3747088313102722\n","[Training Epoch 4] Batch 147, Loss 0.3916662931442261\n","[Training Epoch 4] Batch 148, Loss 0.37056586146354675\n","[Training Epoch 4] Batch 149, Loss 0.37535279989242554\n","[Training Epoch 4] Batch 150, Loss 0.4131675064563751\n","[Training Epoch 4] Batch 151, Loss 0.3555624485015869\n","[Training Epoch 4] Batch 152, Loss 0.3843548893928528\n","[Training Epoch 4] Batch 153, Loss 0.384596586227417\n","[Training Epoch 4] Batch 154, Loss 0.3520607352256775\n","[Training Epoch 4] Batch 155, Loss 0.3893711566925049\n","[Training Epoch 4] Batch 156, Loss 0.3545733690261841\n","[Training Epoch 4] Batch 157, Loss 0.35152047872543335\n","[Training Epoch 4] Batch 158, Loss 0.39854276180267334\n","[Training Epoch 4] Batch 159, Loss 0.3771424889564514\n","[Training Epoch 4] Batch 160, Loss 0.36515969038009644\n","[Training Epoch 4] Batch 161, Loss 0.34867802262306213\n","[Training Epoch 4] Batch 162, Loss 0.37054431438446045\n","[Training Epoch 4] Batch 163, Loss 0.38089990615844727\n","[Training Epoch 4] Batch 164, Loss 0.38453733921051025\n","[Training Epoch 4] Batch 165, Loss 0.363336980342865\n","[Training Epoch 4] Batch 166, Loss 0.393405556678772\n","[Training Epoch 4] Batch 167, Loss 0.38662007451057434\n","[Training Epoch 4] Batch 168, Loss 0.4086691737174988\n","[Training Epoch 4] Batch 169, Loss 0.36797773838043213\n","[Training Epoch 4] Batch 170, Loss 0.3889167606830597\n","[Training Epoch 4] Batch 171, Loss 0.3768309950828552\n","[Training Epoch 4] Batch 172, Loss 0.36584004759788513\n","[Training Epoch 4] Batch 173, Loss 0.38509219884872437\n","[Training Epoch 4] Batch 174, Loss 0.386447936296463\n","[Training Epoch 4] Batch 175, Loss 0.36986297369003296\n","[Training Epoch 4] Batch 176, Loss 0.3717215061187744\n","[Training Epoch 4] Batch 177, Loss 0.3477294147014618\n","[Training Epoch 4] Batch 178, Loss 0.34920644760131836\n","[Training Epoch 4] Batch 179, Loss 0.3567642569541931\n","[Training Epoch 4] Batch 180, Loss 0.3956218361854553\n","[Training Epoch 4] Batch 181, Loss 0.3783397078514099\n","[Training Epoch 4] Batch 182, Loss 0.37380510568618774\n","[Training Epoch 4] Batch 183, Loss 0.3632904291152954\n","[Training Epoch 4] Batch 184, Loss 0.35154521465301514\n","[Training Epoch 4] Batch 185, Loss 0.3334289789199829\n","[Training Epoch 4] Batch 186, Loss 0.3891027271747589\n","[Training Epoch 4] Batch 187, Loss 0.3789198398590088\n","[Training Epoch 4] Batch 188, Loss 0.38820183277130127\n","[Training Epoch 4] Batch 189, Loss 0.382527232170105\n","[Training Epoch 4] Batch 190, Loss 0.37668460607528687\n","[Training Epoch 4] Batch 191, Loss 0.3716067373752594\n","[Training Epoch 4] Batch 192, Loss 0.3730928301811218\n","[Training Epoch 4] Batch 193, Loss 0.39501819014549255\n","[Training Epoch 4] Batch 194, Loss 0.36174291372299194\n","[Training Epoch 4] Batch 195, Loss 0.36905550956726074\n","[Training Epoch 4] Batch 196, Loss 0.34545814990997314\n","[Training Epoch 4] Batch 197, Loss 0.37310367822647095\n","[Training Epoch 4] Batch 198, Loss 0.36311841011047363\n","[Training Epoch 4] Batch 199, Loss 0.39446431398391724\n","[Training Epoch 4] Batch 200, Loss 0.3705146014690399\n","[Training Epoch 4] Batch 201, Loss 0.37393897771835327\n","[Training Epoch 4] Batch 202, Loss 0.3492395281791687\n","[Training Epoch 4] Batch 203, Loss 0.39016902446746826\n","[Training Epoch 4] Batch 204, Loss 0.3963058590888977\n","[Training Epoch 4] Batch 205, Loss 0.36789000034332275\n","[Training Epoch 4] Batch 206, Loss 0.36595529317855835\n","[Training Epoch 4] Batch 207, Loss 0.38912302255630493\n","[Training Epoch 4] Batch 208, Loss 0.4079297184944153\n","[Training Epoch 4] Batch 209, Loss 0.38175204396247864\n","[Training Epoch 4] Batch 210, Loss 0.3751431405544281\n","[Training Epoch 4] Batch 211, Loss 0.3554879426956177\n","[Training Epoch 4] Batch 212, Loss 0.38085585832595825\n","[Training Epoch 4] Batch 213, Loss 0.3775123953819275\n","[Training Epoch 4] Batch 214, Loss 0.38812965154647827\n","[Training Epoch 4] Batch 215, Loss 0.380217581987381\n","[Training Epoch 4] Batch 216, Loss 0.362257719039917\n","[Training Epoch 4] Batch 217, Loss 0.3866453766822815\n","[Training Epoch 4] Batch 218, Loss 0.357562780380249\n","[Training Epoch 4] Batch 219, Loss 0.40460240840911865\n","[Training Epoch 4] Batch 220, Loss 0.39636191725730896\n","[Training Epoch 4] Batch 221, Loss 0.3809052109718323\n","[Training Epoch 4] Batch 222, Loss 0.36104899644851685\n","[Training Epoch 4] Batch 223, Loss 0.37254273891448975\n","[Training Epoch 4] Batch 224, Loss 0.3811311721801758\n","[Training Epoch 4] Batch 225, Loss 0.3761131167411804\n","[Training Epoch 4] Batch 226, Loss 0.35295212268829346\n","[Training Epoch 4] Batch 227, Loss 0.3820210099220276\n","[Training Epoch 4] Batch 228, Loss 0.39118683338165283\n","[Training Epoch 4] Batch 229, Loss 0.36624854803085327\n","[Training Epoch 4] Batch 230, Loss 0.38614416122436523\n","[Training Epoch 4] Batch 231, Loss 0.37428808212280273\n","[Training Epoch 4] Batch 232, Loss 0.4193495810031891\n","[Training Epoch 4] Batch 233, Loss 0.36715394258499146\n","[Training Epoch 4] Batch 234, Loss 0.40160542726516724\n","[Training Epoch 4] Batch 235, Loss 0.3692609667778015\n","[Training Epoch 4] Batch 236, Loss 0.36914074420928955\n","[Training Epoch 4] Batch 237, Loss 0.399089515209198\n","[Training Epoch 4] Batch 238, Loss 0.35844141244888306\n","[Training Epoch 4] Batch 239, Loss 0.3585323989391327\n","[Training Epoch 4] Batch 240, Loss 0.3636029362678528\n","[Training Epoch 4] Batch 241, Loss 0.34234678745269775\n","[Training Epoch 4] Batch 242, Loss 0.378583699464798\n","[Training Epoch 4] Batch 243, Loss 0.3694978356361389\n","[Training Epoch 4] Batch 244, Loss 0.3662109971046448\n","[Training Epoch 4] Batch 245, Loss 0.3605724573135376\n","[Training Epoch 4] Batch 246, Loss 0.38533779978752136\n","[Training Epoch 4] Batch 247, Loss 0.35309433937072754\n","[Training Epoch 4] Batch 248, Loss 0.38240954279899597\n","[Training Epoch 4] Batch 249, Loss 0.3663235902786255\n","[Training Epoch 4] Batch 250, Loss 0.3453426659107208\n","[Training Epoch 4] Batch 251, Loss 0.3741322457790375\n","[Training Epoch 4] Batch 252, Loss 0.3761258125305176\n","[Training Epoch 4] Batch 253, Loss 0.3675422668457031\n","[Training Epoch 4] Batch 254, Loss 0.3839589059352875\n","[Training Epoch 4] Batch 255, Loss 0.35872939229011536\n","[Training Epoch 4] Batch 256, Loss 0.39168715476989746\n","[Training Epoch 4] Batch 257, Loss 0.37034693360328674\n","[Training Epoch 4] Batch 258, Loss 0.3780096769332886\n","[Training Epoch 4] Batch 259, Loss 0.400674432516098\n","[Training Epoch 4] Batch 260, Loss 0.36945396661758423\n","[Training Epoch 4] Batch 261, Loss 0.37710124254226685\n","[Training Epoch 4] Batch 262, Loss 0.38534265756607056\n","[Training Epoch 4] Batch 263, Loss 0.3725475072860718\n","[Training Epoch 4] Batch 264, Loss 0.37099218368530273\n","[Training Epoch 4] Batch 265, Loss 0.3649751543998718\n","[Training Epoch 4] Batch 266, Loss 0.35255029797554016\n","[Training Epoch 4] Batch 267, Loss 0.39159244298934937\n","[Training Epoch 4] Batch 268, Loss 0.3959321975708008\n","[Training Epoch 4] Batch 269, Loss 0.3654240667819977\n","[Training Epoch 4] Batch 270, Loss 0.35929572582244873\n","[Training Epoch 4] Batch 271, Loss 0.35992246866226196\n","[Training Epoch 4] Batch 272, Loss 0.3627418875694275\n","[Training Epoch 4] Batch 273, Loss 0.36279115080833435\n","[Training Epoch 4] Batch 274, Loss 0.384493350982666\n","[Training Epoch 4] Batch 275, Loss 0.3967515826225281\n","[Training Epoch 4] Batch 276, Loss 0.3795245885848999\n","[Training Epoch 4] Batch 277, Loss 0.3616843819618225\n","[Training Epoch 4] Batch 278, Loss 0.3776075541973114\n","[Training Epoch 4] Batch 279, Loss 0.40315401554107666\n","[Training Epoch 4] Batch 280, Loss 0.39298027753829956\n","[Training Epoch 4] Batch 281, Loss 0.3781546950340271\n","[Training Epoch 4] Batch 282, Loss 0.4057764410972595\n","[Training Epoch 4] Batch 283, Loss 0.38412654399871826\n","[Training Epoch 4] Batch 284, Loss 0.3774719834327698\n","[Training Epoch 4] Batch 285, Loss 0.36053621768951416\n","[Training Epoch 4] Batch 286, Loss 0.3689558207988739\n","[Training Epoch 4] Batch 287, Loss 0.37829864025115967\n","[Training Epoch 4] Batch 288, Loss 0.3583187162876129\n","[Training Epoch 4] Batch 289, Loss 0.3614567518234253\n","[Training Epoch 4] Batch 290, Loss 0.3506464660167694\n","[Training Epoch 4] Batch 291, Loss 0.3747929632663727\n","[Training Epoch 4] Batch 292, Loss 0.3766908347606659\n","[Training Epoch 4] Batch 293, Loss 0.3796927332878113\n","[Training Epoch 4] Batch 294, Loss 0.37746402621269226\n","[Training Epoch 4] Batch 295, Loss 0.3844350576400757\n","[Training Epoch 4] Batch 296, Loss 0.35325682163238525\n","[Training Epoch 4] Batch 297, Loss 0.36942416429519653\n","[Training Epoch 4] Batch 298, Loss 0.37113308906555176\n","[Training Epoch 4] Batch 299, Loss 0.3466731905937195\n","[Training Epoch 4] Batch 300, Loss 0.37122732400894165\n","[Training Epoch 4] Batch 301, Loss 0.398812472820282\n","[Training Epoch 4] Batch 302, Loss 0.373456209897995\n","[Training Epoch 4] Batch 303, Loss 0.3739575147628784\n","[Training Epoch 4] Batch 304, Loss 0.364589124917984\n","[Training Epoch 4] Batch 305, Loss 0.41066285967826843\n","[Training Epoch 4] Batch 306, Loss 0.40640565752983093\n","[Training Epoch 4] Batch 307, Loss 0.36664095520973206\n","[Training Epoch 4] Batch 308, Loss 0.3646400570869446\n","[Training Epoch 4] Batch 309, Loss 0.414838045835495\n","[Training Epoch 4] Batch 310, Loss 0.39513978362083435\n","[Training Epoch 4] Batch 311, Loss 0.3744300603866577\n","[Training Epoch 4] Batch 312, Loss 0.36928021907806396\n","[Training Epoch 4] Batch 313, Loss 0.3565887212753296\n","[Training Epoch 4] Batch 314, Loss 0.39001381397247314\n","[Training Epoch 4] Batch 315, Loss 0.3815842270851135\n","[Training Epoch 4] Batch 316, Loss 0.38419467210769653\n","[Training Epoch 4] Batch 317, Loss 0.3670249283313751\n","[Training Epoch 4] Batch 318, Loss 0.37711286544799805\n","[Training Epoch 4] Batch 319, Loss 0.374249666929245\n","[Training Epoch 4] Batch 320, Loss 0.3720535933971405\n","[Training Epoch 4] Batch 321, Loss 0.39542776346206665\n","[Training Epoch 4] Batch 322, Loss 0.38205796480178833\n","[Training Epoch 4] Batch 323, Loss 0.371089905500412\n","[Training Epoch 4] Batch 324, Loss 0.40376293659210205\n","[Training Epoch 4] Batch 325, Loss 0.40488001704216003\n","[Training Epoch 4] Batch 326, Loss 0.3947910964488983\n","[Training Epoch 4] Batch 327, Loss 0.3525199294090271\n","[Training Epoch 4] Batch 328, Loss 0.37320372462272644\n","[Training Epoch 4] Batch 329, Loss 0.3993809223175049\n","[Training Epoch 4] Batch 330, Loss 0.38779324293136597\n","[Training Epoch 4] Batch 331, Loss 0.3750627040863037\n","[Training Epoch 4] Batch 332, Loss 0.38978099822998047\n","[Training Epoch 4] Batch 333, Loss 0.3572671115398407\n","[Training Epoch 4] Batch 334, Loss 0.354272723197937\n","[Training Epoch 4] Batch 335, Loss 0.3690686821937561\n","[Training Epoch 4] Batch 336, Loss 0.3739646077156067\n","[Training Epoch 4] Batch 337, Loss 0.3952137529850006\n","[Training Epoch 4] Batch 338, Loss 0.3844083547592163\n","[Training Epoch 4] Batch 339, Loss 0.3341798484325409\n","[Training Epoch 4] Batch 340, Loss 0.355390340089798\n","[Training Epoch 4] Batch 341, Loss 0.381435364484787\n","[Training Epoch 4] Batch 342, Loss 0.39536261558532715\n","[Training Epoch 4] Batch 343, Loss 0.3768150210380554\n","[Training Epoch 4] Batch 344, Loss 0.3768266439437866\n","[Training Epoch 4] Batch 345, Loss 0.3513769507408142\n","[Training Epoch 4] Batch 346, Loss 0.39141911268234253\n","[Training Epoch 4] Batch 347, Loss 0.37412339448928833\n","[Training Epoch 4] Batch 348, Loss 0.3606833815574646\n","[Training Epoch 4] Batch 349, Loss 0.3764418959617615\n","[Training Epoch 4] Batch 350, Loss 0.3921913504600525\n","[Training Epoch 4] Batch 351, Loss 0.3976494073867798\n","[Training Epoch 4] Batch 352, Loss 0.3954574465751648\n","[Training Epoch 4] Batch 353, Loss 0.3801211714744568\n","[Training Epoch 4] Batch 354, Loss 0.3783111870288849\n","[Training Epoch 4] Batch 355, Loss 0.3532930612564087\n","[Training Epoch 4] Batch 356, Loss 0.36632680892944336\n","[Training Epoch 4] Batch 357, Loss 0.4080950915813446\n","[Training Epoch 4] Batch 358, Loss 0.39212602376937866\n","[Training Epoch 4] Batch 359, Loss 0.3654244542121887\n","[Training Epoch 4] Batch 360, Loss 0.35078170895576477\n","[Training Epoch 4] Batch 361, Loss 0.35326772928237915\n","[Training Epoch 4] Batch 362, Loss 0.37884393334388733\n","[Training Epoch 4] Batch 363, Loss 0.38198935985565186\n","[Training Epoch 4] Batch 364, Loss 0.3744456171989441\n","[Training Epoch 4] Batch 365, Loss 0.3707917332649231\n","[Training Epoch 4] Batch 366, Loss 0.3298279643058777\n","[Training Epoch 4] Batch 367, Loss 0.3786812722682953\n","[Training Epoch 4] Batch 368, Loss 0.37156838178634644\n","[Training Epoch 4] Batch 369, Loss 0.38708800077438354\n","[Training Epoch 4] Batch 370, Loss 0.3825896382331848\n","[Training Epoch 4] Batch 371, Loss 0.377411425113678\n","[Training Epoch 4] Batch 372, Loss 0.3988095819950104\n","[Training Epoch 4] Batch 373, Loss 0.3503061532974243\n","[Training Epoch 4] Batch 374, Loss 0.33926624059677124\n","[Training Epoch 4] Batch 375, Loss 0.36209964752197266\n","[Training Epoch 4] Batch 376, Loss 0.34142231941223145\n","[Training Epoch 4] Batch 377, Loss 0.3614957928657532\n","[Training Epoch 4] Batch 378, Loss 0.36397773027420044\n","[Training Epoch 4] Batch 379, Loss 0.4110056757926941\n","[Training Epoch 4] Batch 380, Loss 0.3627663850784302\n","[Training Epoch 4] Batch 381, Loss 0.4069121181964874\n","[Training Epoch 4] Batch 382, Loss 0.3830852508544922\n","[Training Epoch 4] Batch 383, Loss 0.39272916316986084\n","[Training Epoch 4] Batch 384, Loss 0.3651469945907593\n","[Training Epoch 4] Batch 385, Loss 0.39017021656036377\n","[Training Epoch 4] Batch 386, Loss 0.3933289051055908\n","[Training Epoch 4] Batch 387, Loss 0.36530178785324097\n","[Training Epoch 4] Batch 388, Loss 0.3899300992488861\n","[Training Epoch 4] Batch 389, Loss 0.3585682511329651\n","[Training Epoch 4] Batch 390, Loss 0.36037135124206543\n","[Training Epoch 4] Batch 391, Loss 0.3410167396068573\n","[Training Epoch 4] Batch 392, Loss 0.3734569549560547\n","[Training Epoch 4] Batch 393, Loss 0.386584997177124\n","[Training Epoch 4] Batch 394, Loss 0.3557432293891907\n","[Training Epoch 4] Batch 395, Loss 0.3723769783973694\n","[Training Epoch 4] Batch 396, Loss 0.39308953285217285\n","[Training Epoch 4] Batch 397, Loss 0.37188607454299927\n","[Training Epoch 4] Batch 398, Loss 0.3762984275817871\n","[Training Epoch 4] Batch 399, Loss 0.3460979759693146\n","[Training Epoch 4] Batch 400, Loss 0.3955692648887634\n","[Training Epoch 4] Batch 401, Loss 0.37694692611694336\n","[Training Epoch 4] Batch 402, Loss 0.3545568585395813\n","[Training Epoch 4] Batch 403, Loss 0.3671182096004486\n","[Training Epoch 4] Batch 404, Loss 0.33387818932533264\n","[Training Epoch 4] Batch 405, Loss 0.3774167001247406\n","[Training Epoch 4] Batch 406, Loss 0.38424086570739746\n","[Training Epoch 4] Batch 407, Loss 0.38610684871673584\n","[Training Epoch 4] Batch 408, Loss 0.3591359853744507\n","[Training Epoch 4] Batch 409, Loss 0.3811992406845093\n","[Training Epoch 4] Batch 410, Loss 0.3938778340816498\n","[Training Epoch 4] Batch 411, Loss 0.36279362440109253\n","[Training Epoch 4] Batch 412, Loss 0.34469032287597656\n","[Training Epoch 4] Batch 413, Loss 0.3782162666320801\n","[Training Epoch 4] Batch 414, Loss 0.39090192317962646\n","[Training Epoch 4] Batch 415, Loss 0.43152540922164917\n","[Training Epoch 4] Batch 416, Loss 0.36632856726646423\n","[Training Epoch 4] Batch 417, Loss 0.34067660570144653\n","[Training Epoch 4] Batch 418, Loss 0.39711302518844604\n","[Training Epoch 4] Batch 419, Loss 0.3753008246421814\n","[Training Epoch 4] Batch 420, Loss 0.3750647008419037\n","[Training Epoch 4] Batch 421, Loss 0.3588223457336426\n","[Training Epoch 4] Batch 422, Loss 0.3747291564941406\n","[Training Epoch 4] Batch 423, Loss 0.3875117599964142\n","[Training Epoch 4] Batch 424, Loss 0.38677409291267395\n","[Training Epoch 4] Batch 425, Loss 0.3839907944202423\n","[Training Epoch 4] Batch 426, Loss 0.3954398036003113\n","[Training Epoch 4] Batch 427, Loss 0.3898756802082062\n","[Training Epoch 4] Batch 428, Loss 0.37213844060897827\n","[Training Epoch 4] Batch 429, Loss 0.3874177038669586\n","[Training Epoch 4] Batch 430, Loss 0.37261664867401123\n","[Training Epoch 4] Batch 431, Loss 0.3555026948451996\n","[Training Epoch 4] Batch 432, Loss 0.35411110520362854\n","[Training Epoch 4] Batch 433, Loss 0.35569486021995544\n","[Training Epoch 4] Batch 434, Loss 0.3872571587562561\n","[Training Epoch 4] Batch 435, Loss 0.38976922631263733\n","[Training Epoch 4] Batch 436, Loss 0.35098063945770264\n","[Training Epoch 4] Batch 437, Loss 0.3873633146286011\n","[Training Epoch 4] Batch 438, Loss 0.3678453266620636\n","[Training Epoch 4] Batch 439, Loss 0.38242828845977783\n","[Training Epoch 4] Batch 440, Loss 0.3805127739906311\n","[Training Epoch 4] Batch 441, Loss 0.3731365501880646\n","[Training Epoch 4] Batch 442, Loss 0.3760532736778259\n","[Training Epoch 4] Batch 443, Loss 0.3976350426673889\n","[Training Epoch 4] Batch 444, Loss 0.3932497799396515\n","[Training Epoch 4] Batch 445, Loss 0.3645252585411072\n","[Training Epoch 4] Batch 446, Loss 0.3686235547065735\n","[Training Epoch 4] Batch 447, Loss 0.3876075744628906\n","[Training Epoch 4] Batch 448, Loss 0.36729615926742554\n","[Training Epoch 4] Batch 449, Loss 0.38152894377708435\n","[Training Epoch 4] Batch 450, Loss 0.366912841796875\n","[Training Epoch 4] Batch 451, Loss 0.38597267866134644\n","[Training Epoch 4] Batch 452, Loss 0.3839655816555023\n","[Training Epoch 4] Batch 453, Loss 0.3548761308193207\n","[Training Epoch 4] Batch 454, Loss 0.38586434721946716\n","[Training Epoch 4] Batch 455, Loss 0.3744330108165741\n","[Training Epoch 4] Batch 456, Loss 0.38116925954818726\n","[Training Epoch 4] Batch 457, Loss 0.4192861318588257\n","[Training Epoch 4] Batch 458, Loss 0.3835645616054535\n","[Training Epoch 4] Batch 459, Loss 0.36307722330093384\n","[Training Epoch 4] Batch 460, Loss 0.3616222143173218\n","[Training Epoch 4] Batch 461, Loss 0.39080315828323364\n","[Training Epoch 4] Batch 462, Loss 0.39442959427833557\n","[Training Epoch 4] Batch 463, Loss 0.3859420120716095\n","[Training Epoch 4] Batch 464, Loss 0.38287192583084106\n","[Training Epoch 4] Batch 465, Loss 0.34426403045654297\n","[Training Epoch 4] Batch 466, Loss 0.37527984380722046\n","[Training Epoch 4] Batch 467, Loss 0.3610673248767853\n","[Training Epoch 4] Batch 468, Loss 0.37605953216552734\n","[Training Epoch 4] Batch 469, Loss 0.38277745246887207\n","[Training Epoch 4] Batch 470, Loss 0.37259697914123535\n","[Training Epoch 4] Batch 471, Loss 0.37131619453430176\n","[Training Epoch 4] Batch 472, Loss 0.34212684631347656\n","[Training Epoch 4] Batch 473, Loss 0.3432815372943878\n","[Training Epoch 4] Batch 474, Loss 0.3841768503189087\n","[Training Epoch 4] Batch 475, Loss 0.37798553705215454\n","[Training Epoch 4] Batch 476, Loss 0.3565329313278198\n","[Training Epoch 4] Batch 477, Loss 0.3654072880744934\n","[Training Epoch 4] Batch 478, Loss 0.363552063703537\n","[Training Epoch 4] Batch 479, Loss 0.3974829912185669\n","[Training Epoch 4] Batch 480, Loss 0.3732920289039612\n","[Training Epoch 4] Batch 481, Loss 0.3949767053127289\n","[Training Epoch 4] Batch 482, Loss 0.3709014654159546\n","[Training Epoch 4] Batch 483, Loss 0.36346185207366943\n","[Training Epoch 4] Batch 484, Loss 0.39058971405029297\n","[Training Epoch 4] Batch 485, Loss 0.3752150237560272\n","[Training Epoch 4] Batch 486, Loss 0.3911672830581665\n","[Training Epoch 4] Batch 487, Loss 0.36965394020080566\n","[Training Epoch 4] Batch 488, Loss 0.35534602403640747\n","[Training Epoch 4] Batch 489, Loss 0.3939514756202698\n","[Training Epoch 4] Batch 490, Loss 0.38333576917648315\n","[Training Epoch 4] Batch 491, Loss 0.39925867319107056\n","[Training Epoch 4] Batch 492, Loss 0.3679102957248688\n","[Training Epoch 4] Batch 493, Loss 0.3595600724220276\n","[Training Epoch 4] Batch 494, Loss 0.3682088851928711\n","[Training Epoch 4] Batch 495, Loss 0.3408505618572235\n","[Training Epoch 4] Batch 496, Loss 0.3876405358314514\n","[Training Epoch 4] Batch 497, Loss 0.36337995529174805\n","[Training Epoch 4] Batch 498, Loss 0.3899260461330414\n","[Training Epoch 4] Batch 499, Loss 0.39356449246406555\n","[Training Epoch 4] Batch 500, Loss 0.39823096990585327\n","[Training Epoch 4] Batch 501, Loss 0.36347949504852295\n","[Training Epoch 4] Batch 502, Loss 0.3711855411529541\n","[Training Epoch 4] Batch 503, Loss 0.357092946767807\n","[Training Epoch 4] Batch 504, Loss 0.36174994707107544\n","[Training Epoch 4] Batch 505, Loss 0.37178727984428406\n","[Training Epoch 4] Batch 506, Loss 0.37007811665534973\n","[Training Epoch 4] Batch 507, Loss 0.3928662836551666\n","[Training Epoch 4] Batch 508, Loss 0.3940729796886444\n","[Training Epoch 4] Batch 509, Loss 0.38409021496772766\n","[Training Epoch 4] Batch 510, Loss 0.39068281650543213\n","[Training Epoch 4] Batch 511, Loss 0.3643045723438263\n","[Training Epoch 4] Batch 512, Loss 0.36634719371795654\n","[Training Epoch 4] Batch 513, Loss 0.36828556656837463\n","[Training Epoch 4] Batch 514, Loss 0.3789452314376831\n","[Training Epoch 4] Batch 515, Loss 0.34353309869766235\n","[Training Epoch 4] Batch 516, Loss 0.37074899673461914\n","[Training Epoch 4] Batch 517, Loss 0.3780609667301178\n","[Training Epoch 4] Batch 518, Loss 0.3490247130393982\n","[Training Epoch 4] Batch 519, Loss 0.34618324041366577\n","[Training Epoch 4] Batch 520, Loss 0.37444764375686646\n","[Training Epoch 4] Batch 521, Loss 0.38071775436401367\n","[Training Epoch 4] Batch 522, Loss 0.36385875940322876\n","[Training Epoch 4] Batch 523, Loss 0.36791902780532837\n","[Training Epoch 4] Batch 524, Loss 0.3956936001777649\n","[Training Epoch 4] Batch 525, Loss 0.33812445402145386\n","[Training Epoch 4] Batch 526, Loss 0.37734246253967285\n","[Training Epoch 4] Batch 527, Loss 0.3863503634929657\n","[Training Epoch 4] Batch 528, Loss 0.3758825659751892\n","[Training Epoch 4] Batch 529, Loss 0.3420372009277344\n","[Training Epoch 4] Batch 530, Loss 0.3871158957481384\n","[Training Epoch 4] Batch 531, Loss 0.39107242226600647\n","[Training Epoch 4] Batch 532, Loss 0.3728911578655243\n","[Training Epoch 4] Batch 533, Loss 0.3695893883705139\n","[Training Epoch 4] Batch 534, Loss 0.37143397331237793\n","[Training Epoch 4] Batch 535, Loss 0.3927616477012634\n","[Training Epoch 4] Batch 536, Loss 0.3739706873893738\n","[Training Epoch 4] Batch 537, Loss 0.3971191346645355\n","[Training Epoch 4] Batch 538, Loss 0.32021665573120117\n","[Training Epoch 4] Batch 539, Loss 0.35802245140075684\n","[Training Epoch 4] Batch 540, Loss 0.39200741052627563\n","[Training Epoch 4] Batch 541, Loss 0.3962828516960144\n","[Training Epoch 4] Batch 542, Loss 0.36733025312423706\n","[Training Epoch 4] Batch 543, Loss 0.36742669343948364\n","[Training Epoch 4] Batch 544, Loss 0.3902236819267273\n","[Training Epoch 4] Batch 545, Loss 0.37577396631240845\n","[Training Epoch 4] Batch 546, Loss 0.3811548948287964\n","[Training Epoch 4] Batch 547, Loss 0.37162676453590393\n","[Training Epoch 4] Batch 548, Loss 0.34818190336227417\n","[Training Epoch 4] Batch 549, Loss 0.38758304715156555\n","[Training Epoch 4] Batch 550, Loss 0.3820897340774536\n","[Training Epoch 4] Batch 551, Loss 0.3852077126502991\n","[Training Epoch 4] Batch 552, Loss 0.38306474685668945\n","[Training Epoch 4] Batch 553, Loss 0.35331636667251587\n","[Training Epoch 4] Batch 554, Loss 0.37904131412506104\n","[Training Epoch 4] Batch 555, Loss 0.36654120683670044\n","[Training Epoch 4] Batch 556, Loss 0.38863182067871094\n","[Training Epoch 4] Batch 557, Loss 0.3687155544757843\n","[Training Epoch 4] Batch 558, Loss 0.3748970925807953\n","[Training Epoch 4] Batch 559, Loss 0.40508556365966797\n","[Training Epoch 4] Batch 560, Loss 0.36687272787094116\n","[Training Epoch 4] Batch 561, Loss 0.404297411441803\n","[Training Epoch 4] Batch 562, Loss 0.3469768762588501\n","[Training Epoch 4] Batch 563, Loss 0.408082515001297\n","[Training Epoch 4] Batch 564, Loss 0.37371331453323364\n","[Training Epoch 4] Batch 565, Loss 0.4007032513618469\n","[Training Epoch 4] Batch 566, Loss 0.38657456636428833\n","[Training Epoch 4] Batch 567, Loss 0.38876742124557495\n","[Training Epoch 4] Batch 568, Loss 0.37565362453460693\n","[Training Epoch 4] Batch 569, Loss 0.39835116267204285\n","[Training Epoch 4] Batch 570, Loss 0.3531774878501892\n","[Training Epoch 4] Batch 571, Loss 0.3658018112182617\n","[Training Epoch 4] Batch 572, Loss 0.40531134605407715\n","[Training Epoch 4] Batch 573, Loss 0.33791816234588623\n","[Training Epoch 4] Batch 574, Loss 0.3740576505661011\n","[Training Epoch 4] Batch 575, Loss 0.39950117468833923\n","[Training Epoch 4] Batch 576, Loss 0.3434426784515381\n","[Training Epoch 4] Batch 577, Loss 0.3444674015045166\n","[Training Epoch 4] Batch 578, Loss 0.37032806873321533\n","[Training Epoch 4] Batch 579, Loss 0.3795372545719147\n","[Training Epoch 4] Batch 580, Loss 0.3756445646286011\n","[Training Epoch 4] Batch 581, Loss 0.39094287157058716\n","[Training Epoch 4] Batch 582, Loss 0.3661508560180664\n","[Training Epoch 4] Batch 583, Loss 0.38059699535369873\n","[Training Epoch 4] Batch 584, Loss 0.3746362626552582\n","[Training Epoch 4] Batch 585, Loss 0.37321871519088745\n","[Training Epoch 4] Batch 586, Loss 0.3678743243217468\n","[Training Epoch 4] Batch 587, Loss 0.37464067339897156\n","[Training Epoch 4] Batch 588, Loss 0.37229296565055847\n","[Training Epoch 4] Batch 589, Loss 0.3630288541316986\n","[Training Epoch 4] Batch 590, Loss 0.38193070888519287\n","[Training Epoch 4] Batch 591, Loss 0.41261276602745056\n","[Training Epoch 4] Batch 592, Loss 0.3759080469608307\n","[Training Epoch 4] Batch 593, Loss 0.37999415397644043\n","[Training Epoch 4] Batch 594, Loss 0.38214004039764404\n","[Training Epoch 4] Batch 595, Loss 0.37465012073516846\n","[Training Epoch 4] Batch 596, Loss 0.35498684644699097\n","[Training Epoch 4] Batch 597, Loss 0.37868523597717285\n","[Training Epoch 4] Batch 598, Loss 0.3884933888912201\n","[Training Epoch 4] Batch 599, Loss 0.387933611869812\n","[Training Epoch 4] Batch 600, Loss 0.3680667281150818\n","[Training Epoch 4] Batch 601, Loss 0.35835573077201843\n","[Training Epoch 4] Batch 602, Loss 0.3736196458339691\n","[Training Epoch 4] Batch 603, Loss 0.38724446296691895\n","[Training Epoch 4] Batch 604, Loss 0.37937867641448975\n","[Training Epoch 4] Batch 605, Loss 0.3661518394947052\n","[Training Epoch 4] Batch 606, Loss 0.36275792121887207\n","[Training Epoch 4] Batch 607, Loss 0.3669353127479553\n","[Training Epoch 4] Batch 608, Loss 0.3946681022644043\n","[Training Epoch 4] Batch 609, Loss 0.3392210006713867\n","[Training Epoch 4] Batch 610, Loss 0.366335928440094\n","[Training Epoch 4] Batch 611, Loss 0.36173689365386963\n","[Training Epoch 4] Batch 612, Loss 0.3739381432533264\n","[Training Epoch 4] Batch 613, Loss 0.355829119682312\n","[Training Epoch 4] Batch 614, Loss 0.36531656980514526\n","[Training Epoch 4] Batch 615, Loss 0.34576308727264404\n","[Training Epoch 4] Batch 616, Loss 0.3965804874897003\n","[Training Epoch 4] Batch 617, Loss 0.3636544942855835\n","[Training Epoch 4] Batch 618, Loss 0.38660353422164917\n","[Training Epoch 4] Batch 619, Loss 0.3530223071575165\n","[Training Epoch 4] Batch 620, Loss 0.3817483186721802\n","[Training Epoch 4] Batch 621, Loss 0.3891683518886566\n","[Training Epoch 4] Batch 622, Loss 0.3511672019958496\n","[Training Epoch 4] Batch 623, Loss 0.3456559181213379\n","[Training Epoch 4] Batch 624, Loss 0.3914814591407776\n","[Training Epoch 4] Batch 625, Loss 0.38617652654647827\n","[Training Epoch 4] Batch 626, Loss 0.36064839363098145\n","[Training Epoch 4] Batch 627, Loss 0.3666144609451294\n","[Training Epoch 4] Batch 628, Loss 0.3665502667427063\n","[Training Epoch 4] Batch 629, Loss 0.3576458692550659\n","[Training Epoch 4] Batch 630, Loss 0.3748294711112976\n","[Training Epoch 4] Batch 631, Loss 0.3680315911769867\n","[Training Epoch 4] Batch 632, Loss 0.36669760942459106\n","[Training Epoch 4] Batch 633, Loss 0.36657842993736267\n","[Training Epoch 4] Batch 634, Loss 0.3856585621833801\n","[Training Epoch 4] Batch 635, Loss 0.36645251512527466\n","[Training Epoch 4] Batch 636, Loss 0.3632693886756897\n","[Training Epoch 4] Batch 637, Loss 0.39577221870422363\n","[Training Epoch 4] Batch 638, Loss 0.34007906913757324\n","[Training Epoch 4] Batch 639, Loss 0.3747726082801819\n","[Training Epoch 4] Batch 640, Loss 0.3668701648712158\n","[Training Epoch 4] Batch 641, Loss 0.3800041079521179\n","[Training Epoch 4] Batch 642, Loss 0.35760587453842163\n","[Training Epoch 4] Batch 643, Loss 0.35585612058639526\n","[Training Epoch 4] Batch 644, Loss 0.3658790588378906\n","[Training Epoch 4] Batch 645, Loss 0.37715116143226624\n","[Training Epoch 4] Batch 646, Loss 0.38455116748809814\n","[Training Epoch 4] Batch 647, Loss 0.3776807487010956\n","[Training Epoch 4] Batch 648, Loss 0.407178670167923\n","[Training Epoch 4] Batch 649, Loss 0.3567715883255005\n","[Training Epoch 4] Batch 650, Loss 0.37874218821525574\n","[Training Epoch 4] Batch 651, Loss 0.3704916834831238\n","[Training Epoch 4] Batch 652, Loss 0.3756152391433716\n","[Training Epoch 4] Batch 653, Loss 0.38507115840911865\n","[Training Epoch 4] Batch 654, Loss 0.35856956243515015\n","[Training Epoch 4] Batch 655, Loss 0.3711129426956177\n","[Training Epoch 4] Batch 656, Loss 0.3944425880908966\n","[Training Epoch 4] Batch 657, Loss 0.36671537160873413\n","[Training Epoch 4] Batch 658, Loss 0.4077609181404114\n","[Training Epoch 4] Batch 659, Loss 0.3817581832408905\n","[Training Epoch 4] Batch 660, Loss 0.34498846530914307\n","[Training Epoch 4] Batch 661, Loss 0.36868786811828613\n","[Training Epoch 4] Batch 662, Loss 0.3772323429584503\n","[Training Epoch 4] Batch 663, Loss 0.37702876329421997\n","[Training Epoch 4] Batch 664, Loss 0.3670763671398163\n","[Training Epoch 4] Batch 665, Loss 0.39819085597991943\n","[Training Epoch 4] Batch 666, Loss 0.35634511709213257\n","[Training Epoch 4] Batch 667, Loss 0.38243257999420166\n","[Training Epoch 4] Batch 668, Loss 0.36355575919151306\n","[Training Epoch 4] Batch 669, Loss 0.3638576865196228\n","[Training Epoch 4] Batch 670, Loss 0.3756571114063263\n","[Training Epoch 4] Batch 671, Loss 0.38563960790634155\n","[Training Epoch 4] Batch 672, Loss 0.37798553705215454\n","[Training Epoch 4] Batch 673, Loss 0.37518152594566345\n","[Training Epoch 4] Batch 674, Loss 0.39606308937072754\n","[Training Epoch 4] Batch 675, Loss 0.34451040625572205\n","[Training Epoch 4] Batch 676, Loss 0.3476661443710327\n","[Training Epoch 4] Batch 677, Loss 0.38565295934677124\n","[Training Epoch 4] Batch 678, Loss 0.3591526448726654\n","[Training Epoch 4] Batch 679, Loss 0.3833892345428467\n","[Training Epoch 4] Batch 680, Loss 0.35114985704421997\n","[Training Epoch 4] Batch 681, Loss 0.4096699357032776\n","[Training Epoch 4] Batch 682, Loss 0.40033769607543945\n","[Training Epoch 4] Batch 683, Loss 0.35483622550964355\n","[Training Epoch 4] Batch 684, Loss 0.38537833094596863\n","[Training Epoch 4] Batch 685, Loss 0.3650570511817932\n","[Training Epoch 4] Batch 686, Loss 0.3983660638332367\n","[Training Epoch 4] Batch 687, Loss 0.3583654761314392\n","[Training Epoch 4] Batch 688, Loss 0.3864758014678955\n","[Training Epoch 4] Batch 689, Loss 0.34026145935058594\n","[Training Epoch 4] Batch 690, Loss 0.39543551206588745\n","[Training Epoch 4] Batch 691, Loss 0.4175782799720764\n","[Training Epoch 4] Batch 692, Loss 0.35466986894607544\n","[Training Epoch 4] Batch 693, Loss 0.3487238585948944\n","[Training Epoch 4] Batch 694, Loss 0.3412541151046753\n","[Training Epoch 4] Batch 695, Loss 0.3659602403640747\n","[Training Epoch 4] Batch 696, Loss 0.39886921644210815\n","[Training Epoch 4] Batch 697, Loss 0.34737706184387207\n","[Training Epoch 4] Batch 698, Loss 0.3551141917705536\n","[Training Epoch 4] Batch 699, Loss 0.3781901001930237\n","[Training Epoch 4] Batch 700, Loss 0.3348805606365204\n","[Training Epoch 4] Batch 701, Loss 0.3865167796611786\n","[Training Epoch 4] Batch 702, Loss 0.35013896226882935\n","[Training Epoch 4] Batch 703, Loss 0.3750794231891632\n","[Training Epoch 4] Batch 704, Loss 0.34802156686782837\n","[Training Epoch 4] Batch 705, Loss 0.3734568953514099\n","[Training Epoch 4] Batch 706, Loss 0.4021747410297394\n","[Training Epoch 4] Batch 707, Loss 0.35795295238494873\n","[Training Epoch 4] Batch 708, Loss 0.3713236451148987\n","[Training Epoch 4] Batch 709, Loss 0.37730950117111206\n","[Training Epoch 4] Batch 710, Loss 0.3934936821460724\n","[Training Epoch 4] Batch 711, Loss 0.359332799911499\n","[Training Epoch 4] Batch 712, Loss 0.3891901969909668\n","[Training Epoch 4] Batch 713, Loss 0.35811886191368103\n","[Training Epoch 4] Batch 714, Loss 0.3651825189590454\n","[Training Epoch 4] Batch 715, Loss 0.37613433599472046\n","[Training Epoch 4] Batch 716, Loss 0.37532755732536316\n","[Training Epoch 4] Batch 717, Loss 0.3928552269935608\n","[Training Epoch 4] Batch 718, Loss 0.383565217256546\n","[Training Epoch 4] Batch 719, Loss 0.36864906549453735\n","[Training Epoch 4] Batch 720, Loss 0.37583354115486145\n","[Training Epoch 4] Batch 721, Loss 0.3524998426437378\n","[Training Epoch 4] Batch 722, Loss 0.3865751624107361\n","[Training Epoch 4] Batch 723, Loss 0.3775898218154907\n","[Training Epoch 4] Batch 724, Loss 0.3893737196922302\n","[Training Epoch 4] Batch 725, Loss 0.3426189422607422\n","[Training Epoch 4] Batch 726, Loss 0.3996371328830719\n","[Training Epoch 4] Batch 727, Loss 0.3773520588874817\n","[Training Epoch 4] Batch 728, Loss 0.3673757016658783\n","[Training Epoch 4] Batch 729, Loss 0.3672587275505066\n","[Training Epoch 4] Batch 730, Loss 0.4124657213687897\n","[Training Epoch 4] Batch 731, Loss 0.37074044346809387\n","[Training Epoch 4] Batch 732, Loss 0.37118205428123474\n","[Training Epoch 4] Batch 733, Loss 0.34831732511520386\n","[Training Epoch 4] Batch 734, Loss 0.34926334023475647\n","[Training Epoch 4] Batch 735, Loss 0.377133846282959\n","[Training Epoch 4] Batch 736, Loss 0.3551866412162781\n","[Training Epoch 4] Batch 737, Loss 0.3919592499732971\n","[Training Epoch 4] Batch 738, Loss 0.3874651789665222\n","[Training Epoch 4] Batch 739, Loss 0.3488089442253113\n","[Training Epoch 4] Batch 740, Loss 0.3402743935585022\n","[Training Epoch 4] Batch 741, Loss 0.37622570991516113\n","[Training Epoch 4] Batch 742, Loss 0.3415176272392273\n","[Training Epoch 4] Batch 743, Loss 0.4157029390335083\n","[Training Epoch 4] Batch 744, Loss 0.37007206678390503\n","[Training Epoch 4] Batch 745, Loss 0.3594639003276825\n","[Training Epoch 4] Batch 746, Loss 0.40205222368240356\n","[Training Epoch 4] Batch 747, Loss 0.3647337853908539\n","[Training Epoch 4] Batch 748, Loss 0.3630916476249695\n","[Training Epoch 4] Batch 749, Loss 0.3574177026748657\n","[Training Epoch 4] Batch 750, Loss 0.37912261486053467\n","[Training Epoch 4] Batch 751, Loss 0.3899862468242645\n","[Training Epoch 4] Batch 752, Loss 0.3402671813964844\n","[Training Epoch 4] Batch 753, Loss 0.3636569380760193\n","[Training Epoch 4] Batch 754, Loss 0.3684452474117279\n","[Training Epoch 4] Batch 755, Loss 0.36091160774230957\n","[Training Epoch 4] Batch 756, Loss 0.3882567286491394\n","[Training Epoch 4] Batch 757, Loss 0.3631812334060669\n","[Training Epoch 4] Batch 758, Loss 0.3545793294906616\n","[Training Epoch 4] Batch 759, Loss 0.3768690228462219\n","[Training Epoch 4] Batch 760, Loss 0.39521515369415283\n","[Training Epoch 4] Batch 761, Loss 0.3704354763031006\n","[Training Epoch 4] Batch 762, Loss 0.3390740156173706\n","[Training Epoch 4] Batch 763, Loss 0.37706732749938965\n","[Training Epoch 4] Batch 764, Loss 0.36124008893966675\n","[Training Epoch 4] Batch 765, Loss 0.3612247407436371\n","[Training Epoch 4] Batch 766, Loss 0.37682509422302246\n","[Training Epoch 4] Batch 767, Loss 0.3436025381088257\n","[Training Epoch 4] Batch 768, Loss 0.3692975640296936\n","[Training Epoch 4] Batch 769, Loss 0.34899842739105225\n","[Training Epoch 4] Batch 770, Loss 0.36775264143943787\n","[Training Epoch 4] Batch 771, Loss 0.39116448163986206\n","[Training Epoch 4] Batch 772, Loss 0.3599638044834137\n","[Training Epoch 4] Batch 773, Loss 0.3743552565574646\n","[Training Epoch 4] Batch 774, Loss 0.3606297969818115\n","[Training Epoch 4] Batch 775, Loss 0.36677342653274536\n","[Training Epoch 4] Batch 776, Loss 0.37316641211509705\n","[Training Epoch 4] Batch 777, Loss 0.3743633031845093\n","[Training Epoch 4] Batch 778, Loss 0.4021468162536621\n","[Training Epoch 4] Batch 779, Loss 0.35481518507003784\n","[Training Epoch 4] Batch 780, Loss 0.36433255672454834\n","[Training Epoch 4] Batch 781, Loss 0.39170223474502563\n","[Training Epoch 4] Batch 782, Loss 0.359413743019104\n","[Training Epoch 4] Batch 783, Loss 0.3998231291770935\n","[Training Epoch 4] Batch 784, Loss 0.40082481503486633\n","[Training Epoch 4] Batch 785, Loss 0.38735878467559814\n","[Training Epoch 4] Batch 786, Loss 0.35948848724365234\n","[Training Epoch 4] Batch 787, Loss 0.3841037154197693\n","[Training Epoch 4] Batch 788, Loss 0.35300320386886597\n","[Training Epoch 4] Batch 789, Loss 0.338130384683609\n","[Training Epoch 4] Batch 790, Loss 0.3441632390022278\n","[Training Epoch 4] Batch 791, Loss 0.3805067539215088\n","[Training Epoch 4] Batch 792, Loss 0.36859241127967834\n","[Training Epoch 4] Batch 793, Loss 0.36080852150917053\n","[Training Epoch 4] Batch 794, Loss 0.38711366057395935\n","[Training Epoch 4] Batch 795, Loss 0.34664222598075867\n","[Training Epoch 4] Batch 796, Loss 0.38323974609375\n","[Training Epoch 4] Batch 797, Loss 0.37503981590270996\n","[Training Epoch 4] Batch 798, Loss 0.3500429391860962\n","[Training Epoch 4] Batch 799, Loss 0.3644128739833832\n","[Training Epoch 4] Batch 800, Loss 0.37944191694259644\n","[Training Epoch 4] Batch 801, Loss 0.3771292567253113\n","[Training Epoch 4] Batch 802, Loss 0.3875802159309387\n","[Training Epoch 4] Batch 803, Loss 0.3590948283672333\n","[Training Epoch 4] Batch 804, Loss 0.3523106873035431\n","[Training Epoch 4] Batch 805, Loss 0.38073617219924927\n","[Training Epoch 4] Batch 806, Loss 0.4154694974422455\n","[Training Epoch 4] Batch 807, Loss 0.36229175329208374\n","[Training Epoch 4] Batch 808, Loss 0.3878526985645294\n","[Training Epoch 4] Batch 809, Loss 0.38358649611473083\n","[Training Epoch 4] Batch 810, Loss 0.3553764522075653\n","[Training Epoch 4] Batch 811, Loss 0.4108603000640869\n","[Training Epoch 4] Batch 812, Loss 0.3616403341293335\n","[Training Epoch 4] Batch 813, Loss 0.3886408805847168\n","[Training Epoch 4] Batch 814, Loss 0.3742278218269348\n","[Training Epoch 4] Batch 815, Loss 0.3482499420642853\n","[Training Epoch 4] Batch 816, Loss 0.33586385846138\n","[Training Epoch 4] Batch 817, Loss 0.3574937880039215\n","[Training Epoch 4] Batch 818, Loss 0.31449246406555176\n","[Training Epoch 4] Batch 819, Loss 0.3704874813556671\n","[Training Epoch 4] Batch 820, Loss 0.36486583948135376\n","[Training Epoch 4] Batch 821, Loss 0.38163459300994873\n","[Training Epoch 4] Batch 822, Loss 0.38140207529067993\n","[Training Epoch 4] Batch 823, Loss 0.3967719078063965\n","[Training Epoch 4] Batch 824, Loss 0.3773540258407593\n","[Training Epoch 4] Batch 825, Loss 0.37365037202835083\n","[Training Epoch 4] Batch 826, Loss 0.39420050382614136\n","[Training Epoch 4] Batch 827, Loss 0.379332572221756\n","[Training Epoch 4] Batch 828, Loss 0.36111950874328613\n","[Training Epoch 4] Batch 829, Loss 0.35898905992507935\n","[Training Epoch 4] Batch 830, Loss 0.37554657459259033\n","[Training Epoch 4] Batch 831, Loss 0.3860701620578766\n","[Training Epoch 4] Batch 832, Loss 0.363048255443573\n","[Training Epoch 4] Batch 833, Loss 0.3923146426677704\n","[Training Epoch 4] Batch 834, Loss 0.35675686597824097\n","[Training Epoch 4] Batch 835, Loss 0.32779037952423096\n","[Training Epoch 4] Batch 836, Loss 0.3607434630393982\n","[Training Epoch 4] Batch 837, Loss 0.33219194412231445\n","[Training Epoch 4] Batch 838, Loss 0.3927401006221771\n","[Training Epoch 4] Batch 839, Loss 0.3849451541900635\n","[Training Epoch 4] Batch 840, Loss 0.3589494824409485\n","[Training Epoch 4] Batch 841, Loss 0.39048948884010315\n","[Training Epoch 4] Batch 842, Loss 0.3837513327598572\n","[Training Epoch 4] Batch 843, Loss 0.37723642587661743\n","[Training Epoch 4] Batch 844, Loss 0.3840274214744568\n","[Training Epoch 4] Batch 845, Loss 0.37484151124954224\n","[Training Epoch 4] Batch 846, Loss 0.39017969369888306\n","[Training Epoch 4] Batch 847, Loss 0.37914085388183594\n","[Training Epoch 4] Batch 848, Loss 0.374266654253006\n","[Training Epoch 4] Batch 849, Loss 0.3663567900657654\n","[Training Epoch 4] Batch 850, Loss 0.386898934841156\n","[Training Epoch 4] Batch 851, Loss 0.3907712399959564\n","[Training Epoch 4] Batch 852, Loss 0.38098064064979553\n","[Training Epoch 4] Batch 853, Loss 0.3713875412940979\n","[Training Epoch 4] Batch 854, Loss 0.34246233105659485\n","[Training Epoch 4] Batch 855, Loss 0.3529813289642334\n","[Training Epoch 4] Batch 856, Loss 0.3703668713569641\n","[Training Epoch 4] Batch 857, Loss 0.3667420446872711\n","[Training Epoch 4] Batch 858, Loss 0.3624870181083679\n","[Training Epoch 4] Batch 859, Loss 0.35282763838768005\n","[Training Epoch 4] Batch 860, Loss 0.373363733291626\n","[Training Epoch 4] Batch 861, Loss 0.358068585395813\n","[Training Epoch 4] Batch 862, Loss 0.34496331214904785\n","[Training Epoch 4] Batch 863, Loss 0.384991317987442\n","[Training Epoch 4] Batch 864, Loss 0.35335254669189453\n","[Training Epoch 4] Batch 865, Loss 0.3375293016433716\n","[Training Epoch 4] Batch 866, Loss 0.3803598880767822\n","[Training Epoch 4] Batch 867, Loss 0.362495481967926\n","[Training Epoch 4] Batch 868, Loss 0.4020525813102722\n","[Training Epoch 4] Batch 869, Loss 0.36636900901794434\n","[Training Epoch 4] Batch 870, Loss 0.4165758490562439\n","[Training Epoch 4] Batch 871, Loss 0.4156433343887329\n","[Training Epoch 4] Batch 872, Loss 0.389762282371521\n","[Training Epoch 4] Batch 873, Loss 0.39471855759620667\n","[Training Epoch 4] Batch 874, Loss 0.3905741572380066\n","[Training Epoch 4] Batch 875, Loss 0.39437347650527954\n","[Training Epoch 4] Batch 876, Loss 0.37298154830932617\n","[Training Epoch 4] Batch 877, Loss 0.4065975844860077\n","[Training Epoch 4] Batch 878, Loss 0.3974927067756653\n","[Training Epoch 4] Batch 879, Loss 0.3694944381713867\n","[Training Epoch 4] Batch 880, Loss 0.3803706765174866\n","[Training Epoch 4] Batch 881, Loss 0.32365256547927856\n","[Training Epoch 4] Batch 882, Loss 0.36981987953186035\n","[Training Epoch 4] Batch 883, Loss 0.3702331781387329\n","[Training Epoch 4] Batch 884, Loss 0.3667493164539337\n","[Training Epoch 4] Batch 885, Loss 0.3523658514022827\n","[Training Epoch 4] Batch 886, Loss 0.38324761390686035\n","[Training Epoch 4] Batch 887, Loss 0.32261034846305847\n","[Training Epoch 4] Batch 888, Loss 0.38793036341667175\n","[Training Epoch 4] Batch 889, Loss 0.3848120868206024\n","[Training Epoch 4] Batch 890, Loss 0.3752627670764923\n","[Training Epoch 4] Batch 891, Loss 0.3694886565208435\n","[Training Epoch 4] Batch 892, Loss 0.35419392585754395\n","[Training Epoch 4] Batch 893, Loss 0.3666369318962097\n","[Training Epoch 4] Batch 894, Loss 0.39423811435699463\n","[Training Epoch 4] Batch 895, Loss 0.33773180842399597\n","[Training Epoch 4] Batch 896, Loss 0.35277578234672546\n","[Training Epoch 4] Batch 897, Loss 0.348086416721344\n","[Training Epoch 4] Batch 898, Loss 0.37547293305397034\n","[Training Epoch 4] Batch 899, Loss 0.3473905026912689\n","[Training Epoch 4] Batch 900, Loss 0.3485633134841919\n","[Training Epoch 4] Batch 901, Loss 0.3791230320930481\n","[Training Epoch 4] Batch 902, Loss 0.38796478509902954\n","[Training Epoch 4] Batch 903, Loss 0.36264950037002563\n","[Training Epoch 4] Batch 904, Loss 0.39587461948394775\n","[Training Epoch 4] Batch 905, Loss 0.37241363525390625\n","[Training Epoch 4] Batch 906, Loss 0.3785848319530487\n","[Training Epoch 4] Batch 907, Loss 0.3385453522205353\n","[Training Epoch 4] Batch 908, Loss 0.38812845945358276\n","[Training Epoch 4] Batch 909, Loss 0.3686053156852722\n","[Training Epoch 4] Batch 910, Loss 0.3472297489643097\n","[Training Epoch 4] Batch 911, Loss 0.3520292341709137\n","[Training Epoch 4] Batch 912, Loss 0.3637023866176605\n","[Training Epoch 4] Batch 913, Loss 0.35436350107192993\n","[Training Epoch 4] Batch 914, Loss 0.3821237087249756\n","[Training Epoch 4] Batch 915, Loss 0.36580079793930054\n","[Training Epoch 4] Batch 916, Loss 0.38461434841156006\n","[Training Epoch 4] Batch 917, Loss 0.35593345761299133\n","[Training Epoch 4] Batch 918, Loss 0.3580965995788574\n","[Training Epoch 4] Batch 919, Loss 0.4013085961341858\n","[Training Epoch 4] Batch 920, Loss 0.3502510190010071\n","[Training Epoch 4] Batch 921, Loss 0.342275470495224\n","[Training Epoch 4] Batch 922, Loss 0.39494049549102783\n","[Training Epoch 4] Batch 923, Loss 0.38075900077819824\n","[Training Epoch 4] Batch 924, Loss 0.3535071015357971\n","[Training Epoch 4] Batch 925, Loss 0.3788262605667114\n","[Training Epoch 4] Batch 926, Loss 0.38311970233917236\n","[Training Epoch 4] Batch 927, Loss 0.3546513319015503\n","[Training Epoch 4] Batch 928, Loss 0.34663230180740356\n","[Training Epoch 4] Batch 929, Loss 0.3996949791908264\n","[Training Epoch 4] Batch 930, Loss 0.36036258935928345\n","[Training Epoch 4] Batch 931, Loss 0.35697460174560547\n","[Training Epoch 4] Batch 932, Loss 0.3863484859466553\n","[Training Epoch 4] Batch 933, Loss 0.3719644248485565\n","[Training Epoch 4] Batch 934, Loss 0.3626140058040619\n","[Training Epoch 4] Batch 935, Loss 0.3407430350780487\n","[Training Epoch 4] Batch 936, Loss 0.3974500298500061\n","[Training Epoch 4] Batch 937, Loss 0.36993399262428284\n","[Training Epoch 4] Batch 938, Loss 0.34746411442756653\n","[Training Epoch 4] Batch 939, Loss 0.3440432846546173\n","[Training Epoch 4] Batch 940, Loss 0.3803935647010803\n","[Training Epoch 4] Batch 941, Loss 0.3627154231071472\n","[Training Epoch 4] Batch 942, Loss 0.3814675211906433\n","[Training Epoch 4] Batch 943, Loss 0.3812147378921509\n","[Training Epoch 4] Batch 944, Loss 0.3308762311935425\n","[Training Epoch 4] Batch 945, Loss 0.36452504992485046\n","[Training Epoch 4] Batch 946, Loss 0.3368529677391052\n","[Training Epoch 4] Batch 947, Loss 0.38195109367370605\n","[Training Epoch 4] Batch 948, Loss 0.3884020149707794\n","[Training Epoch 4] Batch 949, Loss 0.38022708892822266\n","[Training Epoch 4] Batch 950, Loss 0.35184621810913086\n","[Training Epoch 4] Batch 951, Loss 0.37618234753608704\n","[Training Epoch 4] Batch 952, Loss 0.3682481348514557\n","[Training Epoch 4] Batch 953, Loss 0.37128642201423645\n","[Training Epoch 4] Batch 954, Loss 0.3864593505859375\n","[Training Epoch 4] Batch 955, Loss 0.3481006622314453\n","[Training Epoch 4] Batch 956, Loss 0.38040870428085327\n","[Training Epoch 4] Batch 957, Loss 0.3912625014781952\n","[Training Epoch 4] Batch 958, Loss 0.3696984052658081\n","[Training Epoch 4] Batch 959, Loss 0.3667865991592407\n","[Training Epoch 4] Batch 960, Loss 0.35783880949020386\n","[Training Epoch 4] Batch 961, Loss 0.38416531682014465\n","[Training Epoch 4] Batch 962, Loss 0.38500022888183594\n","[Training Epoch 4] Batch 963, Loss 0.39962732791900635\n","[Training Epoch 4] Batch 964, Loss 0.3782733082771301\n","[Training Epoch 4] Batch 965, Loss 0.394929438829422\n","[Training Epoch 4] Batch 966, Loss 0.38358616828918457\n","[Training Epoch 4] Batch 967, Loss 0.36003467440605164\n","[Training Epoch 4] Batch 968, Loss 0.35552388429641724\n","[Training Epoch 4] Batch 969, Loss 0.3813124895095825\n","[Training Epoch 4] Batch 970, Loss 0.3873952627182007\n","[Training Epoch 4] Batch 971, Loss 0.41527968645095825\n","[Training Epoch 4] Batch 972, Loss 0.36442476511001587\n","[Training Epoch 4] Batch 973, Loss 0.3728041648864746\n","[Training Epoch 4] Batch 974, Loss 0.3814240097999573\n","[Training Epoch 4] Batch 975, Loss 0.399541974067688\n","[Training Epoch 4] Batch 976, Loss 0.4141978621482849\n","[Training Epoch 4] Batch 977, Loss 0.38738521933555603\n","[Training Epoch 4] Batch 978, Loss 0.36998826265335083\n","[Training Epoch 4] Batch 979, Loss 0.38659176230430603\n","[Training Epoch 4] Batch 980, Loss 0.3675105571746826\n","[Training Epoch 4] Batch 981, Loss 0.3792457580566406\n","[Training Epoch 4] Batch 982, Loss 0.3819246292114258\n","[Training Epoch 4] Batch 983, Loss 0.38577741384506226\n","[Training Epoch 4] Batch 984, Loss 0.3566151261329651\n","[Training Epoch 4] Batch 985, Loss 0.39323359727859497\n","[Training Epoch 4] Batch 986, Loss 0.36090415716171265\n","[Training Epoch 4] Batch 987, Loss 0.39911115169525146\n","[Training Epoch 4] Batch 988, Loss 0.3979712724685669\n","[Training Epoch 4] Batch 989, Loss 0.39257967472076416\n","[Training Epoch 4] Batch 990, Loss 0.37616094946861267\n","[Training Epoch 4] Batch 991, Loss 0.40926823019981384\n","[Training Epoch 4] Batch 992, Loss 0.358530193567276\n","[Training Epoch 4] Batch 993, Loss 0.3787934184074402\n","[Training Epoch 4] Batch 994, Loss 0.38215965032577515\n","[Training Epoch 4] Batch 995, Loss 0.3810264468193054\n","[Training Epoch 4] Batch 996, Loss 0.3765706419944763\n","[Training Epoch 4] Batch 997, Loss 0.3706001043319702\n","[Training Epoch 4] Batch 998, Loss 0.3698282241821289\n","[Training Epoch 4] Batch 999, Loss 0.3697968125343323\n","[Training Epoch 4] Batch 1000, Loss 0.3785104751586914\n","[Training Epoch 4] Batch 1001, Loss 0.3785322904586792\n","[Training Epoch 4] Batch 1002, Loss 0.3635144829750061\n","[Training Epoch 4] Batch 1003, Loss 0.3589816093444824\n","[Training Epoch 4] Batch 1004, Loss 0.3582829236984253\n","[Training Epoch 4] Batch 1005, Loss 0.3873589336872101\n","[Training Epoch 4] Batch 1006, Loss 0.38058918714523315\n","[Training Epoch 4] Batch 1007, Loss 0.3772592842578888\n","[Training Epoch 4] Batch 1008, Loss 0.35556671023368835\n","[Training Epoch 4] Batch 1009, Loss 0.40155279636383057\n","[Training Epoch 4] Batch 1010, Loss 0.37338122725486755\n","[Training Epoch 4] Batch 1011, Loss 0.392585426568985\n","[Training Epoch 4] Batch 1012, Loss 0.3758769631385803\n","[Training Epoch 4] Batch 1013, Loss 0.37030985951423645\n","[Training Epoch 4] Batch 1014, Loss 0.3708761930465698\n","[Training Epoch 4] Batch 1015, Loss 0.3680858016014099\n","[Training Epoch 4] Batch 1016, Loss 0.3752093017101288\n","[Training Epoch 4] Batch 1017, Loss 0.3656620383262634\n","[Training Epoch 4] Batch 1018, Loss 0.3945612907409668\n","[Training Epoch 4] Batch 1019, Loss 0.36201387643814087\n","[Training Epoch 4] Batch 1020, Loss 0.3850462734699249\n","[Training Epoch 4] Batch 1021, Loss 0.3749040961265564\n","[Training Epoch 4] Batch 1022, Loss 0.39211487770080566\n","[Training Epoch 4] Batch 1023, Loss 0.3697894215583801\n","[Training Epoch 4] Batch 1024, Loss 0.3655930757522583\n","[Training Epoch 4] Batch 1025, Loss 0.3517746031284332\n","[Training Epoch 4] Batch 1026, Loss 0.3610016703605652\n","[Training Epoch 4] Batch 1027, Loss 0.3455851972103119\n","[Training Epoch 4] Batch 1028, Loss 0.37793242931365967\n","[Training Epoch 4] Batch 1029, Loss 0.38124382495880127\n","[Training Epoch 4] Batch 1030, Loss 0.37592995166778564\n","[Training Epoch 4] Batch 1031, Loss 0.3503945469856262\n","[Training Epoch 4] Batch 1032, Loss 0.40955960750579834\n","[Training Epoch 4] Batch 1033, Loss 0.3526792526245117\n","[Training Epoch 4] Batch 1034, Loss 0.37144872546195984\n","[Training Epoch 4] Batch 1035, Loss 0.3525847792625427\n","[Training Epoch 4] Batch 1036, Loss 0.36641889810562134\n","[Training Epoch 4] Batch 1037, Loss 0.3477775454521179\n","[Training Epoch 4] Batch 1038, Loss 0.37460070848464966\n","[Training Epoch 4] Batch 1039, Loss 0.39078235626220703\n","[Training Epoch 4] Batch 1040, Loss 0.3837885558605194\n","[Training Epoch 4] Batch 1041, Loss 0.37156856060028076\n","[Training Epoch 4] Batch 1042, Loss 0.3551374673843384\n","[Training Epoch 4] Batch 1043, Loss 0.38071751594543457\n","[Training Epoch 4] Batch 1044, Loss 0.3609512150287628\n","[Training Epoch 4] Batch 1045, Loss 0.36942169070243835\n","[Training Epoch 4] Batch 1046, Loss 0.36617571115493774\n","[Training Epoch 4] Batch 1047, Loss 0.36084339022636414\n","[Training Epoch 4] Batch 1048, Loss 0.39710837602615356\n","[Training Epoch 4] Batch 1049, Loss 0.37994474172592163\n","[Training Epoch 4] Batch 1050, Loss 0.3684138357639313\n","[Training Epoch 4] Batch 1051, Loss 0.36386266350746155\n","[Training Epoch 4] Batch 1052, Loss 0.3634974956512451\n","[Training Epoch 4] Batch 1053, Loss 0.37852829694747925\n","[Training Epoch 4] Batch 1054, Loss 0.38480985164642334\n","[Training Epoch 4] Batch 1055, Loss 0.3895680904388428\n","[Training Epoch 4] Batch 1056, Loss 0.3928327262401581\n","[Training Epoch 4] Batch 1057, Loss 0.3793093264102936\n","[Training Epoch 4] Batch 1058, Loss 0.3500245213508606\n","[Training Epoch 4] Batch 1059, Loss 0.38236725330352783\n","[Training Epoch 4] Batch 1060, Loss 0.381641149520874\n","[Training Epoch 4] Batch 1061, Loss 0.35656315088272095\n","[Training Epoch 4] Batch 1062, Loss 0.3414788842201233\n","[Training Epoch 4] Batch 1063, Loss 0.35013124346733093\n","[Training Epoch 4] Batch 1064, Loss 0.368325412273407\n","[Training Epoch 4] Batch 1065, Loss 0.33375149965286255\n","[Training Epoch 4] Batch 1066, Loss 0.359247624874115\n","[Training Epoch 4] Batch 1067, Loss 0.394052654504776\n","[Training Epoch 4] Batch 1068, Loss 0.3625636696815491\n","[Training Epoch 4] Batch 1069, Loss 0.3470332622528076\n","[Training Epoch 4] Batch 1070, Loss 0.37607383728027344\n","[Training Epoch 4] Batch 1071, Loss 0.3773040771484375\n","[Training Epoch 4] Batch 1072, Loss 0.40175318717956543\n","[Training Epoch 4] Batch 1073, Loss 0.3652717173099518\n","[Training Epoch 4] Batch 1074, Loss 0.41207727789878845\n","[Training Epoch 4] Batch 1075, Loss 0.39033910632133484\n","[Training Epoch 4] Batch 1076, Loss 0.3573569059371948\n","[Training Epoch 4] Batch 1077, Loss 0.37629303336143494\n","[Training Epoch 4] Batch 1078, Loss 0.3423464596271515\n","[Training Epoch 4] Batch 1079, Loss 0.3576512336730957\n","[Training Epoch 4] Batch 1080, Loss 0.35115376114845276\n","[Training Epoch 4] Batch 1081, Loss 0.3736001253128052\n","[Training Epoch 4] Batch 1082, Loss 0.38273850083351135\n","[Training Epoch 4] Batch 1083, Loss 0.4023529291152954\n","[Training Epoch 4] Batch 1084, Loss 0.384521484375\n","[Training Epoch 4] Batch 1085, Loss 0.3892759680747986\n","[Training Epoch 4] Batch 1086, Loss 0.3529931306838989\n","[Training Epoch 4] Batch 1087, Loss 0.34970641136169434\n","[Training Epoch 4] Batch 1088, Loss 0.3623928129673004\n","[Training Epoch 4] Batch 1089, Loss 0.356265127658844\n","[Training Epoch 4] Batch 1090, Loss 0.3722638487815857\n","[Training Epoch 4] Batch 1091, Loss 0.34830227494239807\n","[Training Epoch 4] Batch 1092, Loss 0.3509661555290222\n","[Training Epoch 4] Batch 1093, Loss 0.3542773127555847\n","[Training Epoch 4] Batch 1094, Loss 0.3604104518890381\n","[Training Epoch 4] Batch 1095, Loss 0.3834466338157654\n","[Training Epoch 4] Batch 1096, Loss 0.3582350015640259\n","[Training Epoch 4] Batch 1097, Loss 0.38622885942459106\n","[Training Epoch 4] Batch 1098, Loss 0.3468063473701477\n","[Training Epoch 4] Batch 1099, Loss 0.33511531352996826\n","[Training Epoch 4] Batch 1100, Loss 0.34856271743774414\n","[Training Epoch 4] Batch 1101, Loss 0.40299850702285767\n","[Training Epoch 4] Batch 1102, Loss 0.3862263560295105\n","[Training Epoch 4] Batch 1103, Loss 0.3536609709262848\n","[Training Epoch 4] Batch 1104, Loss 0.3663094639778137\n","[Training Epoch 4] Batch 1105, Loss 0.3621164560317993\n","[Training Epoch 4] Batch 1106, Loss 0.3749680817127228\n","[Training Epoch 4] Batch 1107, Loss 0.3700993061065674\n","[Training Epoch 4] Batch 1108, Loss 0.3874530792236328\n","[Training Epoch 4] Batch 1109, Loss 0.36195093393325806\n","[Training Epoch 4] Batch 1110, Loss 0.3700534999370575\n","[Training Epoch 4] Batch 1111, Loss 0.3667929172515869\n","[Training Epoch 4] Batch 1112, Loss 0.368535578250885\n","[Training Epoch 4] Batch 1113, Loss 0.3848266303539276\n","[Training Epoch 4] Batch 1114, Loss 0.34310707449913025\n","[Training Epoch 4] Batch 1115, Loss 0.39005374908447266\n","[Training Epoch 4] Batch 1116, Loss 0.3489345610141754\n","[Training Epoch 4] Batch 1117, Loss 0.350290983915329\n","[Training Epoch 4] Batch 1118, Loss 0.36244046688079834\n","[Training Epoch 4] Batch 1119, Loss 0.36427128314971924\n","[Training Epoch 4] Batch 1120, Loss 0.36157888174057007\n","[Training Epoch 4] Batch 1121, Loss 0.35863277316093445\n","[Training Epoch 4] Batch 1122, Loss 0.3695720434188843\n","[Training Epoch 4] Batch 1123, Loss 0.3941008746623993\n","[Training Epoch 4] Batch 1124, Loss 0.37822145223617554\n","[Training Epoch 4] Batch 1125, Loss 0.35601842403411865\n","[Training Epoch 4] Batch 1126, Loss 0.3765029311180115\n","[Training Epoch 4] Batch 1127, Loss 0.3978935480117798\n","[Training Epoch 4] Batch 1128, Loss 0.39733195304870605\n","[Training Epoch 4] Batch 1129, Loss 0.3797065019607544\n","[Training Epoch 4] Batch 1130, Loss 0.35788142681121826\n","[Training Epoch 4] Batch 1131, Loss 0.36433953046798706\n","[Training Epoch 4] Batch 1132, Loss 0.3854660391807556\n","[Training Epoch 4] Batch 1133, Loss 0.36224499344825745\n","[Training Epoch 4] Batch 1134, Loss 0.3659362196922302\n","[Training Epoch 4] Batch 1135, Loss 0.36674964427948\n","[Training Epoch 4] Batch 1136, Loss 0.38100388646125793\n","[Training Epoch 4] Batch 1137, Loss 0.38188937306404114\n","[Training Epoch 4] Batch 1138, Loss 0.3701414465904236\n","[Training Epoch 4] Batch 1139, Loss 0.3756192922592163\n","[Training Epoch 4] Batch 1140, Loss 0.40160703659057617\n","[Training Epoch 4] Batch 1141, Loss 0.3598937690258026\n","[Training Epoch 4] Batch 1142, Loss 0.3699016869068146\n","[Training Epoch 4] Batch 1143, Loss 0.3670681118965149\n","[Training Epoch 4] Batch 1144, Loss 0.3766685724258423\n","[Training Epoch 4] Batch 1145, Loss 0.37078067660331726\n","[Training Epoch 4] Batch 1146, Loss 0.3843730688095093\n","[Training Epoch 4] Batch 1147, Loss 0.39478567242622375\n","[Training Epoch 4] Batch 1148, Loss 0.36212775111198425\n","[Training Epoch 4] Batch 1149, Loss 0.37265029549598694\n","[Training Epoch 4] Batch 1150, Loss 0.37346547842025757\n","[Training Epoch 4] Batch 1151, Loss 0.37821781635284424\n","[Training Epoch 4] Batch 1152, Loss 0.362907350063324\n","[Training Epoch 4] Batch 1153, Loss 0.3664261996746063\n","[Training Epoch 4] Batch 1154, Loss 0.365253746509552\n","[Training Epoch 4] Batch 1155, Loss 0.40167734026908875\n","[Training Epoch 4] Batch 1156, Loss 0.35249704122543335\n","[Training Epoch 4] Batch 1157, Loss 0.36601197719573975\n","[Training Epoch 4] Batch 1158, Loss 0.3729242980480194\n","[Training Epoch 4] Batch 1159, Loss 0.36458998918533325\n","[Training Epoch 4] Batch 1160, Loss 0.3956175148487091\n","[Training Epoch 4] Batch 1161, Loss 0.4018535912036896\n","[Training Epoch 4] Batch 1162, Loss 0.36319369077682495\n","[Training Epoch 4] Batch 1163, Loss 0.3474957346916199\n","[Training Epoch 4] Batch 1164, Loss 0.36827152967453003\n","[Training Epoch 4] Batch 1165, Loss 0.3560374975204468\n","[Training Epoch 4] Batch 1166, Loss 0.38471513986587524\n","[Training Epoch 4] Batch 1167, Loss 0.3740471303462982\n","[Training Epoch 4] Batch 1168, Loss 0.38004469871520996\n","[Training Epoch 4] Batch 1169, Loss 0.3687137961387634\n","[Training Epoch 4] Batch 1170, Loss 0.3923119306564331\n","[Training Epoch 4] Batch 1171, Loss 0.35194069147109985\n","[Training Epoch 4] Batch 1172, Loss 0.40130338072776794\n","[Training Epoch 4] Batch 1173, Loss 0.3508163094520569\n","[Training Epoch 4] Batch 1174, Loss 0.3626101613044739\n","[Training Epoch 4] Batch 1175, Loss 0.34737837314605713\n","[Training Epoch 4] Batch 1176, Loss 0.3562875986099243\n","[Training Epoch 4] Batch 1177, Loss 0.3736141324043274\n","[Training Epoch 4] Batch 1178, Loss 0.40384840965270996\n","[Training Epoch 4] Batch 1179, Loss 0.3704296946525574\n","[Training Epoch 4] Batch 1180, Loss 0.3581485152244568\n","[Training Epoch 4] Batch 1181, Loss 0.3840330243110657\n","[Training Epoch 4] Batch 1182, Loss 0.3476984202861786\n","[Training Epoch 4] Batch 1183, Loss 0.38231173157691956\n","[Training Epoch 4] Batch 1184, Loss 0.373593807220459\n","[Training Epoch 4] Batch 1185, Loss 0.3580251932144165\n","[Training Epoch 4] Batch 1186, Loss 0.37311628460884094\n","[Training Epoch 4] Batch 1187, Loss 0.3648248612880707\n","[Training Epoch 4] Batch 1188, Loss 0.3450380265712738\n","[Training Epoch 4] Batch 1189, Loss 0.34172481298446655\n","[Training Epoch 4] Batch 1190, Loss 0.34480032324790955\n","[Training Epoch 4] Batch 1191, Loss 0.39575421810150146\n","[Training Epoch 4] Batch 1192, Loss 0.37223222851753235\n","[Training Epoch 4] Batch 1193, Loss 0.36163440346717834\n","[Training Epoch 4] Batch 1194, Loss 0.3581618070602417\n","[Training Epoch 4] Batch 1195, Loss 0.3379684090614319\n","[Training Epoch 4] Batch 1196, Loss 0.37692511081695557\n","[Training Epoch 4] Batch 1197, Loss 0.3866997957229614\n","[Training Epoch 4] Batch 1198, Loss 0.3682957887649536\n","[Training Epoch 4] Batch 1199, Loss 0.3524068295955658\n","[Training Epoch 4] Batch 1200, Loss 0.3641709089279175\n","[Training Epoch 4] Batch 1201, Loss 0.35147014260292053\n","[Training Epoch 4] Batch 1202, Loss 0.35831618309020996\n","[Training Epoch 4] Batch 1203, Loss 0.360099732875824\n","[Training Epoch 4] Batch 1204, Loss 0.38185834884643555\n","[Training Epoch 4] Batch 1205, Loss 0.34364354610443115\n","[Training Epoch 4] Batch 1206, Loss 0.3922029435634613\n","[Training Epoch 4] Batch 1207, Loss 0.3679637312889099\n","[Training Epoch 4] Batch 1208, Loss 0.358737587928772\n","[Training Epoch 4] Batch 1209, Loss 0.381380558013916\n","[Training Epoch 4] Batch 1210, Loss 0.3657257556915283\n","[Training Epoch 4] Batch 1211, Loss 0.36760932207107544\n","[Training Epoch 4] Batch 1212, Loss 0.3784253001213074\n","[Training Epoch 4] Batch 1213, Loss 0.3584340810775757\n","[Training Epoch 4] Batch 1214, Loss 0.392004132270813\n","[Training Epoch 4] Batch 1215, Loss 0.36940521001815796\n","[Training Epoch 4] Batch 1216, Loss 0.3620002269744873\n","[Training Epoch 4] Batch 1217, Loss 0.3813219368457794\n","[Training Epoch 4] Batch 1218, Loss 0.37171536684036255\n","[Training Epoch 4] Batch 1219, Loss 0.36032673716545105\n","[Training Epoch 4] Batch 1220, Loss 0.35682016611099243\n","[Training Epoch 4] Batch 1221, Loss 0.3775201737880707\n","[Training Epoch 4] Batch 1222, Loss 0.3765884041786194\n","[Training Epoch 4] Batch 1223, Loss 0.3858726918697357\n","[Training Epoch 4] Batch 1224, Loss 0.35898488759994507\n","[Training Epoch 4] Batch 1225, Loss 0.3913818597793579\n","[Training Epoch 4] Batch 1226, Loss 0.3807908594608307\n","[Training Epoch 4] Batch 1227, Loss 0.36802369356155396\n","[Training Epoch 4] Batch 1228, Loss 0.36668068170547485\n","[Training Epoch 4] Batch 1229, Loss 0.35471516847610474\n","[Training Epoch 4] Batch 1230, Loss 0.3826116621494293\n","[Training Epoch 4] Batch 1231, Loss 0.37681448459625244\n","[Training Epoch 4] Batch 1232, Loss 0.3573024272918701\n","[Training Epoch 4] Batch 1233, Loss 0.3827730417251587\n","[Training Epoch 4] Batch 1234, Loss 0.3948294222354889\n","[Training Epoch 4] Batch 1235, Loss 0.37815502285957336\n","[Training Epoch 4] Batch 1236, Loss 0.35664403438568115\n","[Training Epoch 4] Batch 1237, Loss 0.4070044755935669\n","[Training Epoch 4] Batch 1238, Loss 0.33767202496528625\n","[Training Epoch 4] Batch 1239, Loss 0.3640783727169037\n","[Training Epoch 4] Batch 1240, Loss 0.3572312295436859\n","[Training Epoch 4] Batch 1241, Loss 0.34875115752220154\n","[Training Epoch 4] Batch 1242, Loss 0.37535572052001953\n","[Training Epoch 4] Batch 1243, Loss 0.3686780333518982\n","[Training Epoch 4] Batch 1244, Loss 0.3563992977142334\n","[Training Epoch 4] Batch 1245, Loss 0.3727874457836151\n","[Training Epoch 4] Batch 1246, Loss 0.34560978412628174\n","[Training Epoch 4] Batch 1247, Loss 0.3777155876159668\n","[Training Epoch 4] Batch 1248, Loss 0.36927521228790283\n","[Training Epoch 4] Batch 1249, Loss 0.37752383947372437\n","[Training Epoch 4] Batch 1250, Loss 0.3697032332420349\n","[Training Epoch 4] Batch 1251, Loss 0.36498600244522095\n","[Training Epoch 4] Batch 1252, Loss 0.38534802198410034\n","[Training Epoch 4] Batch 1253, Loss 0.3864152431488037\n","[Training Epoch 4] Batch 1254, Loss 0.39748871326446533\n","[Training Epoch 4] Batch 1255, Loss 0.3736310601234436\n","[Training Epoch 4] Batch 1256, Loss 0.38448530435562134\n","[Training Epoch 4] Batch 1257, Loss 0.3530147671699524\n","[Training Epoch 4] Batch 1258, Loss 0.387183278799057\n","[Training Epoch 4] Batch 1259, Loss 0.37941664457321167\n","[Training Epoch 4] Batch 1260, Loss 0.38620632886886597\n","[Training Epoch 4] Batch 1261, Loss 0.38620495796203613\n","[Training Epoch 4] Batch 1262, Loss 0.3839857876300812\n","[Training Epoch 4] Batch 1263, Loss 0.37539488077163696\n","[Training Epoch 4] Batch 1264, Loss 0.35493844747543335\n","[Training Epoch 4] Batch 1265, Loss 0.38173848390579224\n","[Training Epoch 4] Batch 1266, Loss 0.37413710355758667\n","[Training Epoch 4] Batch 1267, Loss 0.371459424495697\n","[Training Epoch 4] Batch 1268, Loss 0.360435426235199\n","[Training Epoch 4] Batch 1269, Loss 0.39508330821990967\n","[Training Epoch 4] Batch 1270, Loss 0.37988877296447754\n","[Training Epoch 4] Batch 1271, Loss 0.3579603135585785\n","[Training Epoch 4] Batch 1272, Loss 0.34382641315460205\n","[Training Epoch 4] Batch 1273, Loss 0.38116320967674255\n","[Training Epoch 4] Batch 1274, Loss 0.37674885988235474\n","[Training Epoch 4] Batch 1275, Loss 0.3770480751991272\n","[Training Epoch 4] Batch 1276, Loss 0.3729872703552246\n","[Training Epoch 4] Batch 1277, Loss 0.37299346923828125\n","[Training Epoch 4] Batch 1278, Loss 0.3596370816230774\n","[Training Epoch 4] Batch 1279, Loss 0.37083446979522705\n","[Training Epoch 4] Batch 1280, Loss 0.37557584047317505\n","[Training Epoch 4] Batch 1281, Loss 0.37384316325187683\n","[Training Epoch 4] Batch 1282, Loss 0.39536523818969727\n","[Training Epoch 4] Batch 1283, Loss 0.3804938793182373\n","[Training Epoch 4] Batch 1284, Loss 0.40984368324279785\n","[Training Epoch 4] Batch 1285, Loss 0.3575323224067688\n","[Training Epoch 4] Batch 1286, Loss 0.3564450740814209\n","[Training Epoch 4] Batch 1287, Loss 0.38075774908065796\n","[Training Epoch 4] Batch 1288, Loss 0.35719701647758484\n","[Training Epoch 4] Batch 1289, Loss 0.37231016159057617\n","[Training Epoch 4] Batch 1290, Loss 0.3534649610519409\n","[Training Epoch 4] Batch 1291, Loss 0.4019845128059387\n","[Training Epoch 4] Batch 1292, Loss 0.3590468168258667\n","[Training Epoch 4] Batch 1293, Loss 0.3916994035243988\n","[Training Epoch 4] Batch 1294, Loss 0.366849422454834\n","[Training Epoch 4] Batch 1295, Loss 0.38818657398223877\n","[Training Epoch 4] Batch 1296, Loss 0.3708348572254181\n","[Training Epoch 4] Batch 1297, Loss 0.36317405104637146\n","[Training Epoch 4] Batch 1298, Loss 0.37025579810142517\n","[Training Epoch 4] Batch 1299, Loss 0.3683464527130127\n","[Training Epoch 4] Batch 1300, Loss 0.3796280026435852\n","[Training Epoch 4] Batch 1301, Loss 0.34146785736083984\n","[Training Epoch 4] Batch 1302, Loss 0.34635382890701294\n","[Training Epoch 4] Batch 1303, Loss 0.38955920934677124\n","[Training Epoch 4] Batch 1304, Loss 0.3823533058166504\n","[Training Epoch 4] Batch 1305, Loss 0.33931976556777954\n","[Training Epoch 4] Batch 1306, Loss 0.410286545753479\n","[Training Epoch 4] Batch 1307, Loss 0.3807671368122101\n","[Training Epoch 4] Batch 1308, Loss 0.36578065156936646\n","[Training Epoch 4] Batch 1309, Loss 0.35577982664108276\n","[Training Epoch 4] Batch 1310, Loss 0.36895233392715454\n","[Training Epoch 4] Batch 1311, Loss 0.36501044034957886\n","[Training Epoch 4] Batch 1312, Loss 0.37950873374938965\n","[Training Epoch 4] Batch 1313, Loss 0.35926011204719543\n","[Training Epoch 4] Batch 1314, Loss 0.3697893023490906\n","[Training Epoch 4] Batch 1315, Loss 0.35809969902038574\n","[Training Epoch 4] Batch 1316, Loss 0.35697388648986816\n","[Training Epoch 4] Batch 1317, Loss 0.38109222054481506\n","[Training Epoch 4] Batch 1318, Loss 0.3890538811683655\n","[Training Epoch 4] Batch 1319, Loss 0.3567735552787781\n","[Training Epoch 4] Batch 1320, Loss 0.35195672512054443\n","[Training Epoch 4] Batch 1321, Loss 0.3676280379295349\n","[Training Epoch 4] Batch 1322, Loss 0.37911903858184814\n","[Training Epoch 4] Batch 1323, Loss 0.38201403617858887\n","[Training Epoch 4] Batch 1324, Loss 0.3637588620185852\n","[Training Epoch 4] Batch 1325, Loss 0.3597530126571655\n","[Training Epoch 4] Batch 1326, Loss 0.3527376651763916\n","[Training Epoch 4] Batch 1327, Loss 0.3565441966056824\n","[Training Epoch 4] Batch 1328, Loss 0.36879226565361023\n","[Training Epoch 4] Batch 1329, Loss 0.3692512512207031\n","[Training Epoch 4] Batch 1330, Loss 0.3800891041755676\n","[Training Epoch 4] Batch 1331, Loss 0.38882309198379517\n","[Training Epoch 4] Batch 1332, Loss 0.3796407878398895\n","[Training Epoch 4] Batch 1333, Loss 0.38926076889038086\n","[Training Epoch 4] Batch 1334, Loss 0.379067987203598\n","[Training Epoch 4] Batch 1335, Loss 0.363559365272522\n","[Training Epoch 4] Batch 1336, Loss 0.3801056742668152\n","[Training Epoch 4] Batch 1337, Loss 0.37044936418533325\n","[Training Epoch 4] Batch 1338, Loss 0.3754080533981323\n","[Training Epoch 4] Batch 1339, Loss 0.36130285263061523\n","[Training Epoch 4] Batch 1340, Loss 0.33804798126220703\n","[Training Epoch 4] Batch 1341, Loss 0.34839290380477905\n","[Training Epoch 4] Batch 1342, Loss 0.3825319707393646\n","[Training Epoch 4] Batch 1343, Loss 0.36337345838546753\n","[Training Epoch 4] Batch 1344, Loss 0.36413222551345825\n","[Training Epoch 4] Batch 1345, Loss 0.3612190783023834\n","[Training Epoch 4] Batch 1346, Loss 0.35753366351127625\n","[Training Epoch 4] Batch 1347, Loss 0.36773017048835754\n","[Training Epoch 4] Batch 1348, Loss 0.353763222694397\n","[Training Epoch 4] Batch 1349, Loss 0.37194764614105225\n","[Training Epoch 4] Batch 1350, Loss 0.3192042410373688\n","[Training Epoch 4] Batch 1351, Loss 0.37099891901016235\n","[Training Epoch 4] Batch 1352, Loss 0.37108951807022095\n","[Training Epoch 4] Batch 1353, Loss 0.3871948719024658\n","[Training Epoch 4] Batch 1354, Loss 0.35867175459861755\n","[Training Epoch 4] Batch 1355, Loss 0.38102924823760986\n","[Training Epoch 4] Batch 1356, Loss 0.3886662721633911\n","[Training Epoch 4] Batch 1357, Loss 0.36163896322250366\n","[Training Epoch 4] Batch 1358, Loss 0.3636990785598755\n","[Training Epoch 4] Batch 1359, Loss 0.3552393913269043\n","[Training Epoch 4] Batch 1360, Loss 0.38882994651794434\n","[Training Epoch 4] Batch 1361, Loss 0.3920915722846985\n","[Training Epoch 4] Batch 1362, Loss 0.3729683756828308\n","[Training Epoch 4] Batch 1363, Loss 0.377069354057312\n","[Training Epoch 4] Batch 1364, Loss 0.38348129391670227\n","[Training Epoch 4] Batch 1365, Loss 0.3650205731391907\n","[Training Epoch 4] Batch 1366, Loss 0.34781402349472046\n","[Training Epoch 4] Batch 1367, Loss 0.38270866870880127\n","[Training Epoch 4] Batch 1368, Loss 0.381902277469635\n","[Training Epoch 4] Batch 1369, Loss 0.38470709323883057\n","[Training Epoch 4] Batch 1370, Loss 0.3817434310913086\n","[Training Epoch 4] Batch 1371, Loss 0.3746483623981476\n","[Training Epoch 4] Batch 1372, Loss 0.3714195787906647\n","[Training Epoch 4] Batch 1373, Loss 0.38108015060424805\n","[Training Epoch 4] Batch 1374, Loss 0.38597196340560913\n","[Training Epoch 4] Batch 1375, Loss 0.37693119049072266\n","[Training Epoch 4] Batch 1376, Loss 0.4044376611709595\n","[Training Epoch 4] Batch 1377, Loss 0.3672809600830078\n","[Training Epoch 4] Batch 1378, Loss 0.37612465023994446\n","[Training Epoch 4] Batch 1379, Loss 0.3565483093261719\n","[Training Epoch 4] Batch 1380, Loss 0.3745589256286621\n","[Training Epoch 4] Batch 1381, Loss 0.3585808277130127\n","[Training Epoch 4] Batch 1382, Loss 0.363481342792511\n","[Training Epoch 4] Batch 1383, Loss 0.380911648273468\n","[Training Epoch 4] Batch 1384, Loss 0.37683460116386414\n","[Training Epoch 4] Batch 1385, Loss 0.37450936436653137\n","[Training Epoch 4] Batch 1386, Loss 0.3532017171382904\n","[Training Epoch 4] Batch 1387, Loss 0.34916943311691284\n","[Training Epoch 4] Batch 1388, Loss 0.34637776017189026\n","[Training Epoch 4] Batch 1389, Loss 0.36611121892929077\n","[Training Epoch 4] Batch 1390, Loss 0.3939332067966461\n","[Training Epoch 4] Batch 1391, Loss 0.36493709683418274\n","[Training Epoch 4] Batch 1392, Loss 0.3619675040245056\n","[Training Epoch 4] Batch 1393, Loss 0.3461412191390991\n","[Training Epoch 4] Batch 1394, Loss 0.3766941428184509\n","[Training Epoch 4] Batch 1395, Loss 0.3692597448825836\n","[Training Epoch 4] Batch 1396, Loss 0.36016932129859924\n","[Training Epoch 4] Batch 1397, Loss 0.36882197856903076\n","[Training Epoch 4] Batch 1398, Loss 0.393340528011322\n","[Training Epoch 4] Batch 1399, Loss 0.3919369578361511\n","[Training Epoch 4] Batch 1400, Loss 0.3322824537754059\n","[Training Epoch 4] Batch 1401, Loss 0.34967464208602905\n","[Training Epoch 4] Batch 1402, Loss 0.3722735643386841\n","[Training Epoch 4] Batch 1403, Loss 0.3657163977622986\n","[Training Epoch 4] Batch 1404, Loss 0.35692015290260315\n","[Training Epoch 4] Batch 1405, Loss 0.35725077986717224\n","[Training Epoch 4] Batch 1406, Loss 0.37003064155578613\n","[Training Epoch 4] Batch 1407, Loss 0.35545217990875244\n","[Training Epoch 4] Batch 1408, Loss 0.3889123797416687\n","[Training Epoch 4] Batch 1409, Loss 0.3367328345775604\n","[Training Epoch 4] Batch 1410, Loss 0.39909517765045166\n","[Training Epoch 4] Batch 1411, Loss 0.3589599132537842\n","[Training Epoch 4] Batch 1412, Loss 0.3826445937156677\n","[Training Epoch 4] Batch 1413, Loss 0.3616641163825989\n","[Training Epoch 4] Batch 1414, Loss 0.38865047693252563\n","[Training Epoch 4] Batch 1415, Loss 0.35231903195381165\n","[Training Epoch 4] Batch 1416, Loss 0.37356242537498474\n","[Training Epoch 4] Batch 1417, Loss 0.3577585816383362\n","[Training Epoch 4] Batch 1418, Loss 0.34558600187301636\n","[Training Epoch 4] Batch 1419, Loss 0.36683404445648193\n","[Training Epoch 4] Batch 1420, Loss 0.35417795181274414\n","[Training Epoch 4] Batch 1421, Loss 0.33793455362319946\n","[Training Epoch 4] Batch 1422, Loss 0.3825419843196869\n","[Training Epoch 4] Batch 1423, Loss 0.38945943117141724\n","[Training Epoch 4] Batch 1424, Loss 0.3745948374271393\n","[Training Epoch 4] Batch 1425, Loss 0.3668258488178253\n","[Training Epoch 4] Batch 1426, Loss 0.3876674473285675\n","[Training Epoch 4] Batch 1427, Loss 0.3554269075393677\n","[Training Epoch 4] Batch 1428, Loss 0.32077300548553467\n","[Training Epoch 4] Batch 1429, Loss 0.37264132499694824\n","[Training Epoch 4] Batch 1430, Loss 0.3616396486759186\n","[Training Epoch 4] Batch 1431, Loss 0.34942811727523804\n","[Training Epoch 4] Batch 1432, Loss 0.3983302712440491\n","[Training Epoch 4] Batch 1433, Loss 0.3649497628211975\n","[Training Epoch 4] Batch 1434, Loss 0.35207176208496094\n","[Training Epoch 4] Batch 1435, Loss 0.36160677671432495\n","[Training Epoch 4] Batch 1436, Loss 0.3728163242340088\n","[Training Epoch 4] Batch 1437, Loss 0.3646657168865204\n","[Training Epoch 4] Batch 1438, Loss 0.3550299406051636\n","[Training Epoch 4] Batch 1439, Loss 0.3782406151294708\n","[Training Epoch 4] Batch 1440, Loss 0.37599509954452515\n","[Training Epoch 4] Batch 1441, Loss 0.36395183205604553\n","[Training Epoch 4] Batch 1442, Loss 0.35065019130706787\n","[Training Epoch 4] Batch 1443, Loss 0.37824928760528564\n","[Training Epoch 4] Batch 1444, Loss 0.3691517114639282\n","[Training Epoch 4] Batch 1445, Loss 0.4042803645133972\n","[Training Epoch 4] Batch 1446, Loss 0.400787353515625\n","[Training Epoch 4] Batch 1447, Loss 0.3701581656932831\n","[Training Epoch 4] Batch 1448, Loss 0.37338292598724365\n","[Training Epoch 4] Batch 1449, Loss 0.35461124777793884\n","[Training Epoch 4] Batch 1450, Loss 0.39749544858932495\n","[Training Epoch 4] Batch 1451, Loss 0.3533428907394409\n","[Training Epoch 4] Batch 1452, Loss 0.4169568717479706\n","[Training Epoch 4] Batch 1453, Loss 0.3934791088104248\n","[Training Epoch 4] Batch 1454, Loss 0.36668938398361206\n","[Training Epoch 4] Batch 1455, Loss 0.37326574325561523\n","[Training Epoch 4] Batch 1456, Loss 0.3605012595653534\n","[Training Epoch 4] Batch 1457, Loss 0.35796523094177246\n","[Training Epoch 4] Batch 1458, Loss 0.4203783869743347\n","[Training Epoch 4] Batch 1459, Loss 0.34765884280204773\n","[Training Epoch 4] Batch 1460, Loss 0.3772223889827728\n","[Training Epoch 4] Batch 1461, Loss 0.4139106571674347\n","[Training Epoch 4] Batch 1462, Loss 0.34983235597610474\n","[Training Epoch 4] Batch 1463, Loss 0.37560778856277466\n","[Training Epoch 4] Batch 1464, Loss 0.35812997817993164\n","[Training Epoch 4] Batch 1465, Loss 0.3623170554637909\n","[Training Epoch 4] Batch 1466, Loss 0.35941416025161743\n","[Training Epoch 4] Batch 1467, Loss 0.3831469416618347\n","[Training Epoch 4] Batch 1468, Loss 0.33773672580718994\n","[Training Epoch 4] Batch 1469, Loss 0.3660740554332733\n","[Training Epoch 4] Batch 1470, Loss 0.3860754370689392\n","[Training Epoch 4] Batch 1471, Loss 0.39198917150497437\n","[Training Epoch 4] Batch 1472, Loss 0.3664770722389221\n","[Training Epoch 4] Batch 1473, Loss 0.36092713475227356\n","[Training Epoch 4] Batch 1474, Loss 0.3652884066104889\n","[Training Epoch 4] Batch 1475, Loss 0.34501156210899353\n","[Training Epoch 4] Batch 1476, Loss 0.3740866780281067\n","[Training Epoch 4] Batch 1477, Loss 0.37830379605293274\n","[Training Epoch 4] Batch 1478, Loss 0.31947970390319824\n","[Training Epoch 4] Batch 1479, Loss 0.3672296106815338\n","[Training Epoch 4] Batch 1480, Loss 0.38621383905410767\n","[Training Epoch 4] Batch 1481, Loss 0.35311007499694824\n","[Training Epoch 4] Batch 1482, Loss 0.3787337839603424\n","[Training Epoch 4] Batch 1483, Loss 0.37573084235191345\n","[Training Epoch 4] Batch 1484, Loss 0.3443973958492279\n","[Training Epoch 4] Batch 1485, Loss 0.3673800826072693\n","[Training Epoch 4] Batch 1486, Loss 0.3835650682449341\n","[Training Epoch 4] Batch 1487, Loss 0.37063461542129517\n","[Training Epoch 4] Batch 1488, Loss 0.38332441449165344\n","[Training Epoch 4] Batch 1489, Loss 0.33015692234039307\n","[Training Epoch 4] Batch 1490, Loss 0.36856216192245483\n","[Training Epoch 4] Batch 1491, Loss 0.3653123676776886\n","[Training Epoch 4] Batch 1492, Loss 0.3456360995769501\n","[Training Epoch 4] Batch 1493, Loss 0.34750840067863464\n","[Training Epoch 4] Batch 1494, Loss 0.37473148107528687\n","[Training Epoch 4] Batch 1495, Loss 0.3643341660499573\n","[Training Epoch 4] Batch 1496, Loss 0.3633054196834564\n","[Training Epoch 4] Batch 1497, Loss 0.37485238909721375\n","[Training Epoch 4] Batch 1498, Loss 0.35490450263023376\n","[Training Epoch 4] Batch 1499, Loss 0.3878992199897766\n","[Training Epoch 4] Batch 1500, Loss 0.3601025640964508\n","[Training Epoch 4] Batch 1501, Loss 0.3855586647987366\n","[Training Epoch 4] Batch 1502, Loss 0.35598546266555786\n","[Training Epoch 4] Batch 1503, Loss 0.35116758942604065\n","[Training Epoch 4] Batch 1504, Loss 0.38193315267562866\n","[Training Epoch 4] Batch 1505, Loss 0.374469131231308\n","[Training Epoch 4] Batch 1506, Loss 0.38688695430755615\n","[Training Epoch 4] Batch 1507, Loss 0.37587907910346985\n","[Training Epoch 4] Batch 1508, Loss 0.40053054690361023\n","[Training Epoch 4] Batch 1509, Loss 0.3536348342895508\n","[Training Epoch 4] Batch 1510, Loss 0.34405750036239624\n","[Training Epoch 4] Batch 1511, Loss 0.344207763671875\n","[Training Epoch 4] Batch 1512, Loss 0.37327805161476135\n","[Training Epoch 4] Batch 1513, Loss 0.40917065739631653\n","[Training Epoch 4] Batch 1514, Loss 0.3781973719596863\n","[Training Epoch 4] Batch 1515, Loss 0.34003180265426636\n","[Training Epoch 4] Batch 1516, Loss 0.3729294240474701\n","[Training Epoch 4] Batch 1517, Loss 0.3597036600112915\n","[Training Epoch 4] Batch 1518, Loss 0.34899118542671204\n","[Training Epoch 4] Batch 1519, Loss 0.3807086646556854\n","[Training Epoch 4] Batch 1520, Loss 0.3717122972011566\n","[Training Epoch 4] Batch 1521, Loss 0.352988064289093\n","[Training Epoch 4] Batch 1522, Loss 0.3882887363433838\n","[Training Epoch 4] Batch 1523, Loss 0.3535808324813843\n","[Training Epoch 4] Batch 1524, Loss 0.3521270155906677\n","[Training Epoch 4] Batch 1525, Loss 0.35378825664520264\n","[Training Epoch 4] Batch 1526, Loss 0.3871206045150757\n","[Training Epoch 4] Batch 1527, Loss 0.3830755650997162\n","[Training Epoch 4] Batch 1528, Loss 0.4022561311721802\n","[Training Epoch 4] Batch 1529, Loss 0.3745788335800171\n","[Training Epoch 4] Batch 1530, Loss 0.3745696544647217\n","[Training Epoch 4] Batch 1531, Loss 0.3951985538005829\n","[Training Epoch 4] Batch 1532, Loss 0.4059498906135559\n","[Training Epoch 4] Batch 1533, Loss 0.36483389139175415\n","[Training Epoch 4] Batch 1534, Loss 0.34949755668640137\n","[Training Epoch 4] Batch 1535, Loss 0.3726503849029541\n","[Training Epoch 4] Batch 1536, Loss 0.36979228258132935\n","[Training Epoch 4] Batch 1537, Loss 0.37237945199012756\n","[Training Epoch 4] Batch 1538, Loss 0.37850672006607056\n","[Training Epoch 4] Batch 1539, Loss 0.39870020747184753\n","[Training Epoch 4] Batch 1540, Loss 0.386918842792511\n","[Training Epoch 4] Batch 1541, Loss 0.3547893166542053\n","[Training Epoch 4] Batch 1542, Loss 0.3801124393939972\n","[Training Epoch 4] Batch 1543, Loss 0.3582269847393036\n","[Training Epoch 4] Batch 1544, Loss 0.3626564145088196\n","[Training Epoch 4] Batch 1545, Loss 0.3434024155139923\n","[Training Epoch 4] Batch 1546, Loss 0.3485134541988373\n","[Training Epoch 4] Batch 1547, Loss 0.3892393708229065\n","[Training Epoch 4] Batch 1548, Loss 0.3514091968536377\n","[Training Epoch 4] Batch 1549, Loss 0.3587200939655304\n","[Training Epoch 4] Batch 1550, Loss 0.3285072445869446\n","[Training Epoch 4] Batch 1551, Loss 0.36214613914489746\n","[Training Epoch 4] Batch 1552, Loss 0.3547450006008148\n","[Training Epoch 4] Batch 1553, Loss 0.35880255699157715\n","[Training Epoch 4] Batch 1554, Loss 0.3481919467449188\n","[Training Epoch 4] Batch 1555, Loss 0.36894822120666504\n","[Training Epoch 4] Batch 1556, Loss 0.4235996603965759\n","[Training Epoch 4] Batch 1557, Loss 0.3859633803367615\n","[Training Epoch 4] Batch 1558, Loss 0.38838404417037964\n","[Training Epoch 4] Batch 1559, Loss 0.34446895122528076\n","[Training Epoch 4] Batch 1560, Loss 0.3562633991241455\n","[Training Epoch 4] Batch 1561, Loss 0.35107865929603577\n","[Training Epoch 4] Batch 1562, Loss 0.40645748376846313\n","[Training Epoch 4] Batch 1563, Loss 0.37181538343429565\n","[Training Epoch 4] Batch 1564, Loss 0.3735644519329071\n","[Training Epoch 4] Batch 1565, Loss 0.34802600741386414\n","[Training Epoch 4] Batch 1566, Loss 0.3836040794849396\n","[Training Epoch 4] Batch 1567, Loss 0.37502521276474\n","[Training Epoch 4] Batch 1568, Loss 0.3901328444480896\n","[Training Epoch 4] Batch 1569, Loss 0.3468695878982544\n","[Training Epoch 4] Batch 1570, Loss 0.36623385548591614\n","[Training Epoch 4] Batch 1571, Loss 0.38478294014930725\n","[Training Epoch 4] Batch 1572, Loss 0.38390207290649414\n","[Training Epoch 4] Batch 1573, Loss 0.38138318061828613\n","[Training Epoch 4] Batch 1574, Loss 0.37784865498542786\n","[Training Epoch 4] Batch 1575, Loss 0.4133974313735962\n","[Training Epoch 4] Batch 1576, Loss 0.3654095232486725\n","[Training Epoch 4] Batch 1577, Loss 0.3826577663421631\n","[Training Epoch 4] Batch 1578, Loss 0.3944498896598816\n","[Training Epoch 4] Batch 1579, Loss 0.35161644220352173\n","[Training Epoch 4] Batch 1580, Loss 0.37885648012161255\n","[Training Epoch 4] Batch 1581, Loss 0.3650261163711548\n","[Training Epoch 4] Batch 1582, Loss 0.3619847893714905\n","[Training Epoch 4] Batch 1583, Loss 0.35625460743904114\n","[Training Epoch 4] Batch 1584, Loss 0.3487170934677124\n","[Training Epoch 4] Batch 1585, Loss 0.34718936681747437\n","[Training Epoch 4] Batch 1586, Loss 0.35878509283065796\n","[Training Epoch 4] Batch 1587, Loss 0.3551192283630371\n","[Training Epoch 4] Batch 1588, Loss 0.37298357486724854\n","[Training Epoch 4] Batch 1589, Loss 0.3912253975868225\n","[Training Epoch 4] Batch 1590, Loss 0.36312055587768555\n","[Training Epoch 4] Batch 1591, Loss 0.357246458530426\n","[Training Epoch 4] Batch 1592, Loss 0.3913230001926422\n","[Training Epoch 4] Batch 1593, Loss 0.3509184718132019\n","[Training Epoch 4] Batch 1594, Loss 0.3455284833908081\n","[Training Epoch 4] Batch 1595, Loss 0.38484281301498413\n","[Training Epoch 4] Batch 1596, Loss 0.34619253873825073\n","[Training Epoch 4] Batch 1597, Loss 0.3748589754104614\n","[Training Epoch 4] Batch 1598, Loss 0.373221755027771\n","[Training Epoch 4] Batch 1599, Loss 0.3876439034938812\n","[Training Epoch 4] Batch 1600, Loss 0.35532689094543457\n","[Training Epoch 4] Batch 1601, Loss 0.34849685430526733\n","[Training Epoch 4] Batch 1602, Loss 0.36201804876327515\n","[Training Epoch 4] Batch 1603, Loss 0.37630972266197205\n","[Training Epoch 4] Batch 1604, Loss 0.37230396270751953\n","[Training Epoch 4] Batch 1605, Loss 0.3578701615333557\n","[Training Epoch 4] Batch 1606, Loss 0.37523582577705383\n","[Training Epoch 4] Batch 1607, Loss 0.3740484416484833\n","[Training Epoch 4] Batch 1608, Loss 0.3965590000152588\n","[Training Epoch 4] Batch 1609, Loss 0.35634925961494446\n","[Training Epoch 4] Batch 1610, Loss 0.37788671255111694\n","[Training Epoch 4] Batch 1611, Loss 0.35601985454559326\n","[Training Epoch 4] Batch 1612, Loss 0.3795723021030426\n","[Training Epoch 4] Batch 1613, Loss 0.373701810836792\n","[Training Epoch 4] Batch 1614, Loss 0.3552663326263428\n","[Training Epoch 4] Batch 1615, Loss 0.35501065850257874\n","[Training Epoch 4] Batch 1616, Loss 0.38234299421310425\n","[Training Epoch 4] Batch 1617, Loss 0.36494365334510803\n","[Training Epoch 4] Batch 1618, Loss 0.37947991490364075\n","[Training Epoch 4] Batch 1619, Loss 0.3840435743331909\n","[Training Epoch 4] Batch 1620, Loss 0.3577997088432312\n","[Training Epoch 4] Batch 1621, Loss 0.37626147270202637\n","[Training Epoch 4] Batch 1622, Loss 0.36050862073898315\n","[Training Epoch 4] Batch 1623, Loss 0.34543564915657043\n","[Training Epoch 4] Batch 1624, Loss 0.3449748754501343\n","[Training Epoch 4] Batch 1625, Loss 0.3592349886894226\n","[Training Epoch 4] Batch 1626, Loss 0.39313530921936035\n","[Training Epoch 4] Batch 1627, Loss 0.384149432182312\n","[Training Epoch 4] Batch 1628, Loss 0.35252416133880615\n","[Training Epoch 4] Batch 1629, Loss 0.3517987132072449\n","[Training Epoch 4] Batch 1630, Loss 0.36292198300361633\n","[Training Epoch 4] Batch 1631, Loss 0.363841712474823\n","[Training Epoch 4] Batch 1632, Loss 0.32819169759750366\n","[Training Epoch 4] Batch 1633, Loss 0.3549724817276001\n","[Training Epoch 4] Batch 1634, Loss 0.3657481074333191\n","[Training Epoch 4] Batch 1635, Loss 0.377658873796463\n","[Training Epoch 4] Batch 1636, Loss 0.3406328558921814\n","[Training Epoch 4] Batch 1637, Loss 0.3803867995738983\n","[Training Epoch 4] Batch 1638, Loss 0.32675206661224365\n","[Training Epoch 4] Batch 1639, Loss 0.3225674629211426\n","[Training Epoch 4] Batch 1640, Loss 0.37286609411239624\n","[Training Epoch 4] Batch 1641, Loss 0.3955264091491699\n","[Training Epoch 4] Batch 1642, Loss 0.3780299723148346\n","[Training Epoch 4] Batch 1643, Loss 0.35581061244010925\n","[Training Epoch 4] Batch 1644, Loss 0.385922908782959\n","[Training Epoch 4] Batch 1645, Loss 0.35846805572509766\n","[Training Epoch 4] Batch 1646, Loss 0.37382617592811584\n","[Training Epoch 4] Batch 1647, Loss 0.37523627281188965\n","[Training Epoch 4] Batch 1648, Loss 0.3523300290107727\n","[Training Epoch 4] Batch 1649, Loss 0.3944721221923828\n","[Training Epoch 4] Batch 1650, Loss 0.3550969958305359\n","[Training Epoch 4] Batch 1651, Loss 0.39177602529525757\n","[Training Epoch 4] Batch 1652, Loss 0.35987693071365356\n","[Training Epoch 4] Batch 1653, Loss 0.37990790605545044\n","[Training Epoch 4] Batch 1654, Loss 0.39290791749954224\n","[Training Epoch 4] Batch 1655, Loss 0.3568613529205322\n","[Training Epoch 4] Batch 1656, Loss 0.3707955479621887\n","[Training Epoch 4] Batch 1657, Loss 0.3651818037033081\n","[Training Epoch 4] Batch 1658, Loss 0.3738700747489929\n","[Training Epoch 4] Batch 1659, Loss 0.3919721841812134\n","[Training Epoch 4] Batch 1660, Loss 0.3947913646697998\n","[Training Epoch 4] Batch 1661, Loss 0.37390458583831787\n","[Training Epoch 4] Batch 1662, Loss 0.3575208783149719\n","[Training Epoch 4] Batch 1663, Loss 0.40421220660209656\n","[Training Epoch 4] Batch 1664, Loss 0.3804183602333069\n","[Training Epoch 4] Batch 1665, Loss 0.3815552294254303\n","[Training Epoch 4] Batch 1666, Loss 0.39400866627693176\n","[Training Epoch 4] Batch 1667, Loss 0.3618577718734741\n","[Training Epoch 4] Batch 1668, Loss 0.3640415370464325\n","[Training Epoch 4] Batch 1669, Loss 0.40184468030929565\n","[Training Epoch 4] Batch 1670, Loss 0.4157520532608032\n","[Training Epoch 4] Batch 1671, Loss 0.33704718947410583\n","[Training Epoch 4] Batch 1672, Loss 0.34209734201431274\n","[Training Epoch 4] Batch 1673, Loss 0.3668438196182251\n","[Training Epoch 4] Batch 1674, Loss 0.3602496385574341\n","[Training Epoch 4] Batch 1675, Loss 0.37912437319755554\n","[Training Epoch 4] Batch 1676, Loss 0.3898782730102539\n","[Training Epoch 4] Batch 1677, Loss 0.37877386808395386\n","[Training Epoch 4] Batch 1678, Loss 0.3675730228424072\n","[Training Epoch 4] Batch 1679, Loss 0.3475834131240845\n","[Training Epoch 4] Batch 1680, Loss 0.36658889055252075\n","[Training Epoch 4] Batch 1681, Loss 0.3403596878051758\n","[Training Epoch 4] Batch 1682, Loss 0.35354551672935486\n","[Training Epoch 4] Batch 1683, Loss 0.3649185597896576\n","[Training Epoch 4] Batch 1684, Loss 0.37274324893951416\n","[Training Epoch 4] Batch 1685, Loss 0.37969908118247986\n","[Training Epoch 4] Batch 1686, Loss 0.33103322982788086\n","[Training Epoch 4] Batch 1687, Loss 0.3623054027557373\n","[Training Epoch 4] Batch 1688, Loss 0.38550907373428345\n","[Training Epoch 4] Batch 1689, Loss 0.38888147473335266\n","[Training Epoch 4] Batch 1690, Loss 0.3503180742263794\n","[Training Epoch 4] Batch 1691, Loss 0.3697921931743622\n","[Training Epoch 4] Batch 1692, Loss 0.33749109506607056\n","[Training Epoch 4] Batch 1693, Loss 0.3922428786754608\n","[Training Epoch 4] Batch 1694, Loss 0.37509408593177795\n","[Training Epoch 4] Batch 1695, Loss 0.36715152859687805\n","[Training Epoch 4] Batch 1696, Loss 0.3398479223251343\n","[Training Epoch 4] Batch 1697, Loss 0.34544605016708374\n","[Training Epoch 4] Batch 1698, Loss 0.38473135232925415\n","[Training Epoch 4] Batch 1699, Loss 0.38228505849838257\n","[Training Epoch 4] Batch 1700, Loss 0.35362836718559265\n","[Training Epoch 4] Batch 1701, Loss 0.35044780373573303\n","[Training Epoch 4] Batch 1702, Loss 0.3783567547798157\n","[Training Epoch 4] Batch 1703, Loss 0.36198800802230835\n","[Training Epoch 4] Batch 1704, Loss 0.36882126331329346\n","[Training Epoch 4] Batch 1705, Loss 0.35180044174194336\n","[Training Epoch 4] Batch 1706, Loss 0.3673045337200165\n","[Training Epoch 4] Batch 1707, Loss 0.3911069631576538\n","[Training Epoch 4] Batch 1708, Loss 0.3498694896697998\n","[Training Epoch 4] Batch 1709, Loss 0.3465545177459717\n","[Training Epoch 4] Batch 1710, Loss 0.3953406810760498\n","[Training Epoch 4] Batch 1711, Loss 0.3555144667625427\n","[Training Epoch 4] Batch 1712, Loss 0.3708312213420868\n","[Training Epoch 4] Batch 1713, Loss 0.40267521142959595\n","[Training Epoch 4] Batch 1714, Loss 0.3483259975910187\n","[Training Epoch 4] Batch 1715, Loss 0.3603612780570984\n","[Training Epoch 4] Batch 1716, Loss 0.3853538930416107\n","[Training Epoch 4] Batch 1717, Loss 0.35430675745010376\n","[Training Epoch 4] Batch 1718, Loss 0.34317153692245483\n","[Training Epoch 4] Batch 1719, Loss 0.3466860353946686\n","[Training Epoch 4] Batch 1720, Loss 0.3774922490119934\n","[Training Epoch 4] Batch 1721, Loss 0.36433911323547363\n","[Training Epoch 4] Batch 1722, Loss 0.352457195520401\n","[Training Epoch 4] Batch 1723, Loss 0.4031751751899719\n","[Training Epoch 4] Batch 1724, Loss 0.38229483366012573\n","[Training Epoch 4] Batch 1725, Loss 0.36710065603256226\n","[Training Epoch 4] Batch 1726, Loss 0.3705922067165375\n","[Training Epoch 4] Batch 1727, Loss 0.38804471492767334\n","[Training Epoch 4] Batch 1728, Loss 0.3666551113128662\n","[Training Epoch 4] Batch 1729, Loss 0.401375412940979\n","[Training Epoch 4] Batch 1730, Loss 0.3591674566268921\n","[Training Epoch 4] Batch 1731, Loss 0.38664549589157104\n","[Training Epoch 4] Batch 1732, Loss 0.3731643557548523\n","[Training Epoch 4] Batch 1733, Loss 0.36282244324684143\n","[Training Epoch 4] Batch 1734, Loss 0.38681840896606445\n","[Training Epoch 4] Batch 1735, Loss 0.3793545961380005\n","[Training Epoch 4] Batch 1736, Loss 0.361833393573761\n","[Training Epoch 4] Batch 1737, Loss 0.3812829852104187\n","[Training Epoch 4] Batch 1738, Loss 0.35842639207839966\n","[Training Epoch 4] Batch 1739, Loss 0.35591310262680054\n","[Training Epoch 4] Batch 1740, Loss 0.3688405752182007\n","[Training Epoch 4] Batch 1741, Loss 0.3805931508541107\n","[Training Epoch 4] Batch 1742, Loss 0.3492163419723511\n","[Training Epoch 4] Batch 1743, Loss 0.3957923650741577\n","[Training Epoch 4] Batch 1744, Loss 0.388912558555603\n","[Training Epoch 4] Batch 1745, Loss 0.38736921548843384\n","[Training Epoch 4] Batch 1746, Loss 0.35782188177108765\n","[Training Epoch 4] Batch 1747, Loss 0.3517244756221771\n","[Training Epoch 4] Batch 1748, Loss 0.34856027364730835\n","[Training Epoch 4] Batch 1749, Loss 0.36112454533576965\n","[Training Epoch 4] Batch 1750, Loss 0.3740319013595581\n","[Training Epoch 4] Batch 1751, Loss 0.39355555176734924\n","[Training Epoch 4] Batch 1752, Loss 0.40607354044914246\n","[Training Epoch 4] Batch 1753, Loss 0.34059053659439087\n","[Training Epoch 4] Batch 1754, Loss 0.38381290435791016\n","[Training Epoch 4] Batch 1755, Loss 0.37090277671813965\n","[Training Epoch 4] Batch 1756, Loss 0.3714398741722107\n","[Training Epoch 4] Batch 1757, Loss 0.3809911012649536\n","[Training Epoch 4] Batch 1758, Loss 0.3857470154762268\n","[Training Epoch 4] Batch 1759, Loss 0.34111487865448\n","[Training Epoch 4] Batch 1760, Loss 0.3725036680698395\n","[Training Epoch 4] Batch 1761, Loss 0.36250901222229004\n","[Training Epoch 4] Batch 1762, Loss 0.37464046478271484\n","[Training Epoch 4] Batch 1763, Loss 0.34222936630249023\n","[Training Epoch 4] Batch 1764, Loss 0.37798643112182617\n","[Training Epoch 4] Batch 1765, Loss 0.3584609031677246\n","[Training Epoch 4] Batch 1766, Loss 0.3952610492706299\n","[Training Epoch 4] Batch 1767, Loss 0.35586094856262207\n","[Training Epoch 4] Batch 1768, Loss 0.3727407157421112\n","[Training Epoch 4] Batch 1769, Loss 0.3672249913215637\n","[Training Epoch 4] Batch 1770, Loss 0.38369354605674744\n","[Training Epoch 4] Batch 1771, Loss 0.40699827671051025\n","[Training Epoch 4] Batch 1772, Loss 0.3571844696998596\n","[Training Epoch 4] Batch 1773, Loss 0.345559298992157\n","[Training Epoch 4] Batch 1774, Loss 0.36351093649864197\n","[Training Epoch 4] Batch 1775, Loss 0.3688983619213104\n","[Training Epoch 4] Batch 1776, Loss 0.39235907793045044\n","[Training Epoch 4] Batch 1777, Loss 0.3709411025047302\n","[Training Epoch 4] Batch 1778, Loss 0.35134392976760864\n","[Training Epoch 4] Batch 1779, Loss 0.3514467775821686\n","[Training Epoch 4] Batch 1780, Loss 0.3853396475315094\n","[Training Epoch 4] Batch 1781, Loss 0.3656918406486511\n","[Training Epoch 4] Batch 1782, Loss 0.3604300022125244\n","[Training Epoch 4] Batch 1783, Loss 0.36234134435653687\n","[Training Epoch 4] Batch 1784, Loss 0.3742583394050598\n","[Training Epoch 4] Batch 1785, Loss 0.3553616404533386\n","[Training Epoch 4] Batch 1786, Loss 0.37111803889274597\n","[Training Epoch 4] Batch 1787, Loss 0.37296855449676514\n","[Training Epoch 4] Batch 1788, Loss 0.3553110957145691\n","[Training Epoch 4] Batch 1789, Loss 0.3908807933330536\n","[Training Epoch 4] Batch 1790, Loss 0.395826518535614\n","[Training Epoch 4] Batch 1791, Loss 0.36585915088653564\n","[Training Epoch 4] Batch 1792, Loss 0.3585754930973053\n","[Training Epoch 4] Batch 1793, Loss 0.33958351612091064\n","[Training Epoch 4] Batch 1794, Loss 0.38812774419784546\n","[Training Epoch 4] Batch 1795, Loss 0.3851345181465149\n","[Training Epoch 4] Batch 1796, Loss 0.3853902220726013\n","[Training Epoch 4] Batch 1797, Loss 0.34944260120391846\n","[Training Epoch 4] Batch 1798, Loss 0.3596697747707367\n","[Training Epoch 4] Batch 1799, Loss 0.409392386674881\n","[Training Epoch 4] Batch 1800, Loss 0.34803134202957153\n","[Training Epoch 4] Batch 1801, Loss 0.3525436818599701\n","[Training Epoch 4] Batch 1802, Loss 0.3564683794975281\n","[Training Epoch 4] Batch 1803, Loss 0.37331080436706543\n","[Training Epoch 4] Batch 1804, Loss 0.3789486885070801\n","[Training Epoch 4] Batch 1805, Loss 0.34109848737716675\n","[Training Epoch 4] Batch 1806, Loss 0.36296799778938293\n","[Training Epoch 4] Batch 1807, Loss 0.3609004616737366\n","[Training Epoch 4] Batch 1808, Loss 0.3709963262081146\n","[Training Epoch 4] Batch 1809, Loss 0.37439462542533875\n","[Training Epoch 4] Batch 1810, Loss 0.36297643184661865\n","[Training Epoch 4] Batch 1811, Loss 0.35395100712776184\n","[Training Epoch 4] Batch 1812, Loss 0.38489454984664917\n","[Training Epoch 4] Batch 1813, Loss 0.35758358240127563\n","[Training Epoch 4] Batch 1814, Loss 0.38830408453941345\n","[Training Epoch 4] Batch 1815, Loss 0.3466523587703705\n","[Training Epoch 4] Batch 1816, Loss 0.36209142208099365\n","[Training Epoch 4] Batch 1817, Loss 0.3759244680404663\n","[Training Epoch 4] Batch 1818, Loss 0.36975353956222534\n","[Training Epoch 4] Batch 1819, Loss 0.3553856313228607\n","[Training Epoch 4] Batch 1820, Loss 0.3641013503074646\n","[Training Epoch 4] Batch 1821, Loss 0.33603453636169434\n","[Training Epoch 4] Batch 1822, Loss 0.34470146894454956\n","[Training Epoch 4] Batch 1823, Loss 0.3571874499320984\n","[Training Epoch 4] Batch 1824, Loss 0.3788207471370697\n","[Training Epoch 4] Batch 1825, Loss 0.3593636155128479\n","[Training Epoch 4] Batch 1826, Loss 0.3759393095970154\n","[Training Epoch 4] Batch 1827, Loss 0.37632066011428833\n","[Training Epoch 4] Batch 1828, Loss 0.3569297790527344\n","[Training Epoch 4] Batch 1829, Loss 0.3866943120956421\n","[Training Epoch 4] Batch 1830, Loss 0.371798574924469\n","[Training Epoch 4] Batch 1831, Loss 0.3422246277332306\n","[Training Epoch 4] Batch 1832, Loss 0.38680434226989746\n","[Training Epoch 4] Batch 1833, Loss 0.35071301460266113\n","[Training Epoch 4] Batch 1834, Loss 0.3591122031211853\n","[Training Epoch 4] Batch 1835, Loss 0.39952802658081055\n","[Training Epoch 4] Batch 1836, Loss 0.37605637311935425\n","[Training Epoch 4] Batch 1837, Loss 0.3658093810081482\n","[Training Epoch 4] Batch 1838, Loss 0.3652797341346741\n","[Training Epoch 4] Batch 1839, Loss 0.35441696643829346\n","[Training Epoch 4] Batch 1840, Loss 0.3723221719264984\n","[Training Epoch 4] Batch 1841, Loss 0.3681625425815582\n","[Training Epoch 4] Batch 1842, Loss 0.36723053455352783\n","[Training Epoch 4] Batch 1843, Loss 0.38864028453826904\n","[Training Epoch 4] Batch 1844, Loss 0.39216849207878113\n","[Training Epoch 4] Batch 1845, Loss 0.3497006297111511\n","[Training Epoch 4] Batch 1846, Loss 0.3380807936191559\n","[Training Epoch 4] Batch 1847, Loss 0.37822726368904114\n","[Training Epoch 4] Batch 1848, Loss 0.37965357303619385\n","[Training Epoch 4] Batch 1849, Loss 0.36080697178840637\n","[Training Epoch 4] Batch 1850, Loss 0.3537293076515198\n","[Training Epoch 4] Batch 1851, Loss 0.3730494976043701\n","[Training Epoch 4] Batch 1852, Loss 0.39284825325012207\n","[Training Epoch 4] Batch 1853, Loss 0.3522416353225708\n","[Training Epoch 4] Batch 1854, Loss 0.3698018193244934\n","[Training Epoch 4] Batch 1855, Loss 0.33600103855133057\n","[Training Epoch 4] Batch 1856, Loss 0.37756386399269104\n","[Training Epoch 4] Batch 1857, Loss 0.32943791151046753\n","[Training Epoch 4] Batch 1858, Loss 0.3592378497123718\n","[Training Epoch 4] Batch 1859, Loss 0.38759756088256836\n","[Training Epoch 4] Batch 1860, Loss 0.3514610528945923\n","[Training Epoch 4] Batch 1861, Loss 0.35502055287361145\n","[Training Epoch 4] Batch 1862, Loss 0.35688161849975586\n","[Training Epoch 4] Batch 1863, Loss 0.3784179091453552\n","[Training Epoch 4] Batch 1864, Loss 0.3572629392147064\n","[Training Epoch 4] Batch 1865, Loss 0.393363356590271\n","[Training Epoch 4] Batch 1866, Loss 0.3599894642829895\n","[Training Epoch 4] Batch 1867, Loss 0.3689866065979004\n","[Training Epoch 4] Batch 1868, Loss 0.3606812357902527\n","[Training Epoch 4] Batch 1869, Loss 0.3755715787410736\n","[Training Epoch 4] Batch 1870, Loss 0.37733039259910583\n","[Training Epoch 4] Batch 1871, Loss 0.3472324013710022\n","[Training Epoch 4] Batch 1872, Loss 0.3895289897918701\n","[Training Epoch 4] Batch 1873, Loss 0.37612423300743103\n","[Training Epoch 4] Batch 1874, Loss 0.3659510016441345\n","[Training Epoch 4] Batch 1875, Loss 0.3976329267024994\n","[Training Epoch 4] Batch 1876, Loss 0.3469310402870178\n","[Training Epoch 4] Batch 1877, Loss 0.3664431571960449\n","[Training Epoch 4] Batch 1878, Loss 0.3758874535560608\n","[Training Epoch 4] Batch 1879, Loss 0.3499895930290222\n","[Training Epoch 4] Batch 1880, Loss 0.38442352414131165\n","[Training Epoch 4] Batch 1881, Loss 0.39406830072402954\n","[Training Epoch 4] Batch 1882, Loss 0.3783091902732849\n","[Training Epoch 4] Batch 1883, Loss 0.3539808392524719\n","[Training Epoch 4] Batch 1884, Loss 0.3846126198768616\n","[Training Epoch 4] Batch 1885, Loss 0.355056494474411\n","[Training Epoch 4] Batch 1886, Loss 0.35839635133743286\n","[Training Epoch 4] Batch 1887, Loss 0.3629271984100342\n","[Training Epoch 4] Batch 1888, Loss 0.3616707921028137\n","[Training Epoch 4] Batch 1889, Loss 0.3525073230266571\n","[Training Epoch 4] Batch 1890, Loss 0.3434159755706787\n","[Training Epoch 4] Batch 1891, Loss 0.3510848879814148\n","[Training Epoch 4] Batch 1892, Loss 0.36121487617492676\n","[Training Epoch 4] Batch 1893, Loss 0.3662489354610443\n","[Training Epoch 4] Batch 1894, Loss 0.3752756118774414\n","[Training Epoch 4] Batch 1895, Loss 0.35610121488571167\n","[Training Epoch 4] Batch 1896, Loss 0.37073221802711487\n","[Training Epoch 4] Batch 1897, Loss 0.35382360219955444\n","[Training Epoch 4] Batch 1898, Loss 0.3425131142139435\n","[Training Epoch 4] Batch 1899, Loss 0.38492104411125183\n","[Training Epoch 4] Batch 1900, Loss 0.3347563147544861\n","[Training Epoch 4] Batch 1901, Loss 0.3467411398887634\n","[Training Epoch 4] Batch 1902, Loss 0.35077646374702454\n","[Training Epoch 4] Batch 1903, Loss 0.3437175154685974\n","[Training Epoch 4] Batch 1904, Loss 0.3833200931549072\n","[Training Epoch 4] Batch 1905, Loss 0.37269502878189087\n","[Training Epoch 4] Batch 1906, Loss 0.3442418575286865\n","[Training Epoch 4] Batch 1907, Loss 0.38343513011932373\n","[Training Epoch 4] Batch 1908, Loss 0.3601790964603424\n","[Training Epoch 4] Batch 1909, Loss 0.3693002462387085\n","[Training Epoch 4] Batch 1910, Loss 0.338910847902298\n","[Training Epoch 4] Batch 1911, Loss 0.35534054040908813\n","[Training Epoch 4] Batch 1912, Loss 0.3884614109992981\n","[Training Epoch 4] Batch 1913, Loss 0.3324822783470154\n","[Training Epoch 4] Batch 1914, Loss 0.3633689880371094\n","[Training Epoch 4] Batch 1915, Loss 0.37128955125808716\n","[Training Epoch 4] Batch 1916, Loss 0.39396271109580994\n","[Training Epoch 4] Batch 1917, Loss 0.37998253107070923\n","[Training Epoch 4] Batch 1918, Loss 0.35474756360054016\n","[Training Epoch 4] Batch 1919, Loss 0.3487999141216278\n","[Training Epoch 4] Batch 1920, Loss 0.37001654505729675\n","[Training Epoch 4] Batch 1921, Loss 0.35430020093917847\n","[Training Epoch 4] Batch 1922, Loss 0.3446108102798462\n","[Training Epoch 4] Batch 1923, Loss 0.3733290433883667\n","[Training Epoch 4] Batch 1924, Loss 0.34246110916137695\n","[Training Epoch 4] Batch 1925, Loss 0.38050273060798645\n","[Training Epoch 4] Batch 1926, Loss 0.3716392517089844\n","[Training Epoch 4] Batch 1927, Loss 0.36788374185562134\n","[Training Epoch 4] Batch 1928, Loss 0.3586697578430176\n","[Training Epoch 4] Batch 1929, Loss 0.33553367853164673\n","[Training Epoch 4] Batch 1930, Loss 0.3810163140296936\n","[Training Epoch 4] Batch 1931, Loss 0.35355111956596375\n","[Training Epoch 4] Batch 1932, Loss 0.3885568380355835\n","[Training Epoch 4] Batch 1933, Loss 0.3563089668750763\n","[Training Epoch 4] Batch 1934, Loss 0.3781525492668152\n","[Training Epoch 4] Batch 1935, Loss 0.37005311250686646\n","[Training Epoch 4] Batch 1936, Loss 0.3620138168334961\n","[Training Epoch 4] Batch 1937, Loss 0.3689105212688446\n","[Training Epoch 4] Batch 1938, Loss 0.3766980767250061\n","[Training Epoch 4] Batch 1939, Loss 0.3719593286514282\n","[Training Epoch 4] Batch 1940, Loss 0.3770126402378082\n","[Training Epoch 4] Batch 1941, Loss 0.36374905705451965\n","[Training Epoch 4] Batch 1942, Loss 0.3642147183418274\n","[Training Epoch 4] Batch 1943, Loss 0.32776135206222534\n","[Training Epoch 4] Batch 1944, Loss 0.3690476715564728\n","[Training Epoch 4] Batch 1945, Loss 0.3814840316772461\n","[Training Epoch 4] Batch 1946, Loss 0.38795986771583557\n","[Training Epoch 4] Batch 1947, Loss 0.37186628580093384\n","[Training Epoch 4] Batch 1948, Loss 0.33970868587493896\n","[Training Epoch 4] Batch 1949, Loss 0.36575600504875183\n","[Training Epoch 4] Batch 1950, Loss 0.3580968379974365\n","[Training Epoch 4] Batch 1951, Loss 0.38289952278137207\n","[Training Epoch 4] Batch 1952, Loss 0.3544527590274811\n","[Training Epoch 4] Batch 1953, Loss 0.38270431756973267\n","[Training Epoch 4] Batch 1954, Loss 0.3418159782886505\n","[Training Epoch 4] Batch 1955, Loss 0.37107110023498535\n","[Training Epoch 4] Batch 1956, Loss 0.3427993059158325\n","[Training Epoch 4] Batch 1957, Loss 0.3488236963748932\n","[Training Epoch 4] Batch 1958, Loss 0.37500691413879395\n","[Training Epoch 4] Batch 1959, Loss 0.37534913420677185\n","[Training Epoch 4] Batch 1960, Loss 0.3859051764011383\n","[Training Epoch 4] Batch 1961, Loss 0.36336690187454224\n","[Training Epoch 4] Batch 1962, Loss 0.3810136318206787\n","[Training Epoch 4] Batch 1963, Loss 0.3497397005558014\n","[Training Epoch 4] Batch 1964, Loss 0.3706260919570923\n","[Training Epoch 4] Batch 1965, Loss 0.39090684056282043\n","[Training Epoch 4] Batch 1966, Loss 0.3901793956756592\n","[Training Epoch 4] Batch 1967, Loss 0.3655948042869568\n","[Training Epoch 4] Batch 1968, Loss 0.3773030638694763\n","[Training Epoch 4] Batch 1969, Loss 0.3962985575199127\n","[Training Epoch 4] Batch 1970, Loss 0.36879247426986694\n","[Training Epoch 4] Batch 1971, Loss 0.36977648735046387\n","[Training Epoch 4] Batch 1972, Loss 0.3517398238182068\n","[Training Epoch 4] Batch 1973, Loss 0.38501355051994324\n","[Training Epoch 4] Batch 1974, Loss 0.3816946744918823\n","[Training Epoch 4] Batch 1975, Loss 0.3757888078689575\n","[Training Epoch 4] Batch 1976, Loss 0.3535791039466858\n","[Training Epoch 4] Batch 1977, Loss 0.398412823677063\n","[Training Epoch 4] Batch 1978, Loss 0.35790807008743286\n","[Training Epoch 4] Batch 1979, Loss 0.3633655309677124\n","[Training Epoch 4] Batch 1980, Loss 0.3622684180736542\n","[Training Epoch 4] Batch 1981, Loss 0.35666537284851074\n","[Training Epoch 4] Batch 1982, Loss 0.3846333920955658\n","[Training Epoch 4] Batch 1983, Loss 0.3856089115142822\n","[Training Epoch 4] Batch 1984, Loss 0.38507652282714844\n","[Training Epoch 4] Batch 1985, Loss 0.3611169457435608\n","[Training Epoch 4] Batch 1986, Loss 0.38631004095077515\n","[Training Epoch 4] Batch 1987, Loss 0.36817604303359985\n","[Training Epoch 4] Batch 1988, Loss 0.3236485719680786\n","[Training Epoch 4] Batch 1989, Loss 0.3738568425178528\n","[Training Epoch 4] Batch 1990, Loss 0.36904096603393555\n","[Training Epoch 4] Batch 1991, Loss 0.3611789047718048\n","[Training Epoch 4] Batch 1992, Loss 0.3623252213001251\n","[Training Epoch 4] Batch 1993, Loss 0.35197189450263977\n","[Training Epoch 4] Batch 1994, Loss 0.345506489276886\n","[Training Epoch 4] Batch 1995, Loss 0.37484651803970337\n","[Training Epoch 4] Batch 1996, Loss 0.3972873389720917\n","[Training Epoch 4] Batch 1997, Loss 0.35844188928604126\n","[Training Epoch 4] Batch 1998, Loss 0.3961043357849121\n","[Training Epoch 4] Batch 1999, Loss 0.3488385081291199\n","[Training Epoch 4] Batch 2000, Loss 0.3830922842025757\n","[Training Epoch 4] Batch 2001, Loss 0.4058961570262909\n","[Training Epoch 4] Batch 2002, Loss 0.38514444231987\n","[Training Epoch 4] Batch 2003, Loss 0.395525187253952\n","[Training Epoch 4] Batch 2004, Loss 0.36066538095474243\n","[Training Epoch 4] Batch 2005, Loss 0.3670847415924072\n","[Training Epoch 4] Batch 2006, Loss 0.372641921043396\n","[Training Epoch 4] Batch 2007, Loss 0.3870808482170105\n","[Training Epoch 4] Batch 2008, Loss 0.34118831157684326\n","[Training Epoch 4] Batch 2009, Loss 0.35764187574386597\n","[Training Epoch 4] Batch 2010, Loss 0.350877583026886\n","[Training Epoch 4] Batch 2011, Loss 0.37045818567276\n","[Training Epoch 4] Batch 2012, Loss 0.3780350983142853\n","[Training Epoch 4] Batch 2013, Loss 0.3766906261444092\n","[Training Epoch 4] Batch 2014, Loss 0.3507798910140991\n","[Training Epoch 4] Batch 2015, Loss 0.36201852560043335\n","[Training Epoch 4] Batch 2016, Loss 0.3799029588699341\n","[Training Epoch 4] Batch 2017, Loss 0.38934922218322754\n","[Training Epoch 4] Batch 2018, Loss 0.3685862720012665\n","[Training Epoch 4] Batch 2019, Loss 0.34416869282722473\n","[Training Epoch 4] Batch 2020, Loss 0.3812686502933502\n","[Training Epoch 4] Batch 2021, Loss 0.3411220908164978\n","[Training Epoch 4] Batch 2022, Loss 0.37343913316726685\n","[Training Epoch 4] Batch 2023, Loss 0.3806133270263672\n","[Training Epoch 4] Batch 2024, Loss 0.37342527508735657\n","[Training Epoch 4] Batch 2025, Loss 0.35957759618759155\n","[Training Epoch 4] Batch 2026, Loss 0.38119226694107056\n","[Training Epoch 4] Batch 2027, Loss 0.32052576541900635\n","[Training Epoch 4] Batch 2028, Loss 0.36782407760620117\n","[Training Epoch 4] Batch 2029, Loss 0.3769061863422394\n","[Training Epoch 4] Batch 2030, Loss 0.37370064854621887\n","[Training Epoch 4] Batch 2031, Loss 0.3593360185623169\n","[Training Epoch 4] Batch 2032, Loss 0.33892786502838135\n","[Training Epoch 4] Batch 2033, Loss 0.34969866275787354\n","[Training Epoch 4] Batch 2034, Loss 0.37687304615974426\n","[Training Epoch 4] Batch 2035, Loss 0.3645586371421814\n","[Training Epoch 4] Batch 2036, Loss 0.35534733533859253\n","[Training Epoch 4] Batch 2037, Loss 0.3706912696361542\n","[Training Epoch 4] Batch 2038, Loss 0.3657657206058502\n","[Training Epoch 4] Batch 2039, Loss 0.41488000750541687\n","[Training Epoch 4] Batch 2040, Loss 0.34783852100372314\n","[Training Epoch 4] Batch 2041, Loss 0.3716753423213959\n","[Training Epoch 4] Batch 2042, Loss 0.3743273615837097\n","[Training Epoch 4] Batch 2043, Loss 0.33826249837875366\n","[Training Epoch 4] Batch 2044, Loss 0.3900260925292969\n","[Training Epoch 4] Batch 2045, Loss 0.3730332851409912\n","[Training Epoch 4] Batch 2046, Loss 0.35301774740219116\n","[Training Epoch 4] Batch 2047, Loss 0.37362873554229736\n","[Training Epoch 4] Batch 2048, Loss 0.3829687833786011\n","[Training Epoch 4] Batch 2049, Loss 0.36553439497947693\n","[Training Epoch 4] Batch 2050, Loss 0.35925889015197754\n","[Training Epoch 4] Batch 2051, Loss 0.38672351837158203\n","[Training Epoch 4] Batch 2052, Loss 0.36485475301742554\n","[Training Epoch 4] Batch 2053, Loss 0.3407161235809326\n","[Training Epoch 4] Batch 2054, Loss 0.36197084188461304\n","[Training Epoch 4] Batch 2055, Loss 0.372835636138916\n","[Training Epoch 4] Batch 2056, Loss 0.36685439944267273\n","[Training Epoch 4] Batch 2057, Loss 0.36051732301712036\n","[Training Epoch 4] Batch 2058, Loss 0.3521227240562439\n","[Training Epoch 4] Batch 2059, Loss 0.36714833974838257\n","[Training Epoch 4] Batch 2060, Loss 0.36420077085494995\n","[Training Epoch 4] Batch 2061, Loss 0.3666054606437683\n","[Training Epoch 4] Batch 2062, Loss 0.3468230962753296\n","[Training Epoch 4] Batch 2063, Loss 0.3626841902732849\n","[Training Epoch 4] Batch 2064, Loss 0.38132911920547485\n","[Training Epoch 4] Batch 2065, Loss 0.3564866781234741\n","[Training Epoch 4] Batch 2066, Loss 0.3557398319244385\n","[Training Epoch 4] Batch 2067, Loss 0.3653552532196045\n","[Training Epoch 4] Batch 2068, Loss 0.37013936042785645\n","[Training Epoch 4] Batch 2069, Loss 0.36320775747299194\n","[Training Epoch 4] Batch 2070, Loss 0.36965224146842957\n","[Training Epoch 4] Batch 2071, Loss 0.37980473041534424\n","[Training Epoch 4] Batch 2072, Loss 0.34772735834121704\n","[Training Epoch 4] Batch 2073, Loss 0.3441672623157501\n","[Training Epoch 4] Batch 2074, Loss 0.35113662481307983\n","[Training Epoch 4] Batch 2075, Loss 0.35095542669296265\n","[Training Epoch 4] Batch 2076, Loss 0.3531482219696045\n","[Training Epoch 4] Batch 2077, Loss 0.36807116866111755\n","[Training Epoch 4] Batch 2078, Loss 0.382580041885376\n","[Training Epoch 4] Batch 2079, Loss 0.3431750535964966\n","[Training Epoch 4] Batch 2080, Loss 0.3702654242515564\n","[Training Epoch 4] Batch 2081, Loss 0.35046833753585815\n","[Training Epoch 4] Batch 2082, Loss 0.382210910320282\n","[Training Epoch 4] Batch 2083, Loss 0.36460667848587036\n","[Training Epoch 4] Batch 2084, Loss 0.35677778720855713\n","[Training Epoch 4] Batch 2085, Loss 0.3588305711746216\n","[Training Epoch 4] Batch 2086, Loss 0.36977243423461914\n","[Training Epoch 4] Batch 2087, Loss 0.36482128500938416\n","[Training Epoch 4] Batch 2088, Loss 0.32573050260543823\n","[Training Epoch 4] Batch 2089, Loss 0.3769589066505432\n","[Training Epoch 4] Batch 2090, Loss 0.3778269290924072\n","[Training Epoch 4] Batch 2091, Loss 0.357648640871048\n","[Training Epoch 4] Batch 2092, Loss 0.37279024720191956\n","[Training Epoch 4] Batch 2093, Loss 0.3584398031234741\n","[Training Epoch 4] Batch 2094, Loss 0.3627212643623352\n","[Training Epoch 4] Batch 2095, Loss 0.3724900484085083\n","[Training Epoch 4] Batch 2096, Loss 0.3726152777671814\n","[Training Epoch 4] Batch 2097, Loss 0.36060631275177\n","[Training Epoch 4] Batch 2098, Loss 0.37434500455856323\n","[Training Epoch 4] Batch 2099, Loss 0.38148704171180725\n","[Training Epoch 4] Batch 2100, Loss 0.3694743514060974\n","[Training Epoch 4] Batch 2101, Loss 0.34983494877815247\n","[Training Epoch 4] Batch 2102, Loss 0.3621733486652374\n","[Training Epoch 4] Batch 2103, Loss 0.38752037286758423\n","[Training Epoch 4] Batch 2104, Loss 0.3543238341808319\n","[Training Epoch 4] Batch 2105, Loss 0.3799171447753906\n","[Training Epoch 4] Batch 2106, Loss 0.361024409532547\n","[Training Epoch 4] Batch 2107, Loss 0.3590857982635498\n","[Training Epoch 4] Batch 2108, Loss 0.3849179148674011\n","[Training Epoch 4] Batch 2109, Loss 0.38635244965553284\n","[Training Epoch 4] Batch 2110, Loss 0.37945717573165894\n","[Training Epoch 4] Batch 2111, Loss 0.390659362077713\n","[Training Epoch 4] Batch 2112, Loss 0.37085476517677307\n","[Training Epoch 4] Batch 2113, Loss 0.37359634041786194\n","[Training Epoch 4] Batch 2114, Loss 0.3572981059551239\n","[Training Epoch 4] Batch 2115, Loss 0.35403159260749817\n","[Training Epoch 4] Batch 2116, Loss 0.3683483898639679\n","[Training Epoch 4] Batch 2117, Loss 0.3473581075668335\n","[Training Epoch 4] Batch 2118, Loss 0.36199283599853516\n","[Training Epoch 4] Batch 2119, Loss 0.3777981698513031\n","[Training Epoch 4] Batch 2120, Loss 0.37168431282043457\n","[Training Epoch 4] Batch 2121, Loss 0.35692811012268066\n","[Training Epoch 4] Batch 2122, Loss 0.3560023307800293\n","[Training Epoch 4] Batch 2123, Loss 0.3600338101387024\n","[Training Epoch 4] Batch 2124, Loss 0.36771321296691895\n","[Training Epoch 4] Batch 2125, Loss 0.39287447929382324\n","[Training Epoch 4] Batch 2126, Loss 0.35492706298828125\n","[Training Epoch 4] Batch 2127, Loss 0.3717969059944153\n","[Training Epoch 4] Batch 2128, Loss 0.39545947313308716\n","[Training Epoch 4] Batch 2129, Loss 0.3609715402126312\n","[Training Epoch 4] Batch 2130, Loss 0.3748299479484558\n","[Training Epoch 4] Batch 2131, Loss 0.3476250469684601\n","[Training Epoch 4] Batch 2132, Loss 0.38931968808174133\n","[Training Epoch 4] Batch 2133, Loss 0.3658912181854248\n","[Training Epoch 4] Batch 2134, Loss 0.4007453918457031\n","[Training Epoch 4] Batch 2135, Loss 0.3847885727882385\n","[Training Epoch 4] Batch 2136, Loss 0.3615015745162964\n","[Training Epoch 4] Batch 2137, Loss 0.3567811846733093\n","[Training Epoch 4] Batch 2138, Loss 0.3600451946258545\n","[Training Epoch 4] Batch 2139, Loss 0.37997955083847046\n","[Training Epoch 4] Batch 2140, Loss 0.3533945381641388\n","[Training Epoch 4] Batch 2141, Loss 0.37480998039245605\n","[Training Epoch 4] Batch 2142, Loss 0.3476279377937317\n","[Training Epoch 4] Batch 2143, Loss 0.3707923889160156\n","[Training Epoch 4] Batch 2144, Loss 0.3941836357116699\n","[Training Epoch 4] Batch 2145, Loss 0.3757556974887848\n","[Training Epoch 4] Batch 2146, Loss 0.3645405173301697\n","[Training Epoch 4] Batch 2147, Loss 0.36703765392303467\n","[Training Epoch 4] Batch 2148, Loss 0.36919349431991577\n","[Training Epoch 4] Batch 2149, Loss 0.36939525604248047\n","[Training Epoch 4] Batch 2150, Loss 0.3481009006500244\n","[Training Epoch 4] Batch 2151, Loss 0.35769492387771606\n","[Training Epoch 4] Batch 2152, Loss 0.34752941131591797\n","[Training Epoch 4] Batch 2153, Loss 0.39321020245552063\n","[Training Epoch 4] Batch 2154, Loss 0.36647871136665344\n","[Training Epoch 4] Batch 2155, Loss 0.38321277499198914\n","[Training Epoch 4] Batch 2156, Loss 0.3717527985572815\n","[Training Epoch 4] Batch 2157, Loss 0.3535405993461609\n","[Training Epoch 4] Batch 2158, Loss 0.3540893793106079\n","[Training Epoch 4] Batch 2159, Loss 0.3690682351589203\n","[Training Epoch 4] Batch 2160, Loss 0.3740932047367096\n","[Training Epoch 4] Batch 2161, Loss 0.40315741300582886\n","[Training Epoch 4] Batch 2162, Loss 0.3546808362007141\n","[Training Epoch 4] Batch 2163, Loss 0.3882749676704407\n","[Training Epoch 4] Batch 2164, Loss 0.38300567865371704\n","[Training Epoch 4] Batch 2165, Loss 0.37445586919784546\n","[Training Epoch 4] Batch 2166, Loss 0.3552710711956024\n","[Training Epoch 4] Batch 2167, Loss 0.35553866624832153\n","[Training Epoch 4] Batch 2168, Loss 0.39991495013237\n","[Training Epoch 4] Batch 2169, Loss 0.36993610858917236\n","[Training Epoch 4] Batch 2170, Loss 0.3593931198120117\n","[Training Epoch 4] Batch 2171, Loss 0.35635244846343994\n","[Training Epoch 4] Batch 2172, Loss 0.3655557632446289\n","[Training Epoch 4] Batch 2173, Loss 0.392067015171051\n","[Training Epoch 4] Batch 2174, Loss 0.38161179423332214\n","[Training Epoch 4] Batch 2175, Loss 0.37507545948028564\n","[Training Epoch 4] Batch 2176, Loss 0.3435584306716919\n","[Training Epoch 4] Batch 2177, Loss 0.3854234516620636\n","[Training Epoch 4] Batch 2178, Loss 0.35805240273475647\n","[Training Epoch 4] Batch 2179, Loss 0.36024653911590576\n","[Training Epoch 4] Batch 2180, Loss 0.35771188139915466\n","[Training Epoch 4] Batch 2181, Loss 0.38754910230636597\n","[Training Epoch 4] Batch 2182, Loss 0.36463820934295654\n","[Training Epoch 4] Batch 2183, Loss 0.3233144283294678\n","[Training Epoch 4] Batch 2184, Loss 0.3665825128555298\n","[Training Epoch 4] Batch 2185, Loss 0.3892594873905182\n","[Training Epoch 4] Batch 2186, Loss 0.3811328709125519\n","[Training Epoch 4] Batch 2187, Loss 0.36651772260665894\n","[Training Epoch 4] Batch 2188, Loss 0.3606131076812744\n","[Training Epoch 4] Batch 2189, Loss 0.36363857984542847\n","[Training Epoch 4] Batch 2190, Loss 0.36174118518829346\n","[Training Epoch 4] Batch 2191, Loss 0.3583466112613678\n","[Training Epoch 4] Batch 2192, Loss 0.38288748264312744\n","[Training Epoch 4] Batch 2193, Loss 0.35958510637283325\n","[Training Epoch 4] Batch 2194, Loss 0.3869152367115021\n","[Training Epoch 4] Batch 2195, Loss 0.33949628472328186\n","[Training Epoch 4] Batch 2196, Loss 0.37431055307388306\n","[Training Epoch 4] Batch 2197, Loss 0.38513097167015076\n","[Training Epoch 4] Batch 2198, Loss 0.3490384817123413\n","[Training Epoch 4] Batch 2199, Loss 0.3598097860813141\n","[Training Epoch 4] Batch 2200, Loss 0.36170926690101624\n","[Training Epoch 4] Batch 2201, Loss 0.3817044496536255\n","[Training Epoch 4] Batch 2202, Loss 0.3333697021007538\n","[Training Epoch 4] Batch 2203, Loss 0.36117124557495117\n","[Training Epoch 4] Batch 2204, Loss 0.369543194770813\n","[Training Epoch 4] Batch 2205, Loss 0.33972105383872986\n","[Training Epoch 4] Batch 2206, Loss 0.3501799702644348\n","[Training Epoch 4] Batch 2207, Loss 0.40198448300361633\n","[Training Epoch 4] Batch 2208, Loss 0.41071879863739014\n","[Training Epoch 4] Batch 2209, Loss 0.3472287952899933\n","[Training Epoch 4] Batch 2210, Loss 0.3876592218875885\n","[Training Epoch 4] Batch 2211, Loss 0.34454917907714844\n","[Training Epoch 4] Batch 2212, Loss 0.3704473376274109\n","[Training Epoch 4] Batch 2213, Loss 0.36755695939064026\n","[Training Epoch 4] Batch 2214, Loss 0.34285470843315125\n","[Training Epoch 4] Batch 2215, Loss 0.36376118659973145\n","[Training Epoch 4] Batch 2216, Loss 0.37554848194122314\n","[Training Epoch 4] Batch 2217, Loss 0.3391822576522827\n","[Training Epoch 4] Batch 2218, Loss 0.3825565278530121\n","[Training Epoch 4] Batch 2219, Loss 0.35048145055770874\n","[Training Epoch 4] Batch 2220, Loss 0.3490557074546814\n","[Training Epoch 4] Batch 2221, Loss 0.3523215055465698\n","[Training Epoch 4] Batch 2222, Loss 0.40315377712249756\n","[Training Epoch 4] Batch 2223, Loss 0.3479791283607483\n","[Training Epoch 4] Batch 2224, Loss 0.3611280620098114\n","[Training Epoch 4] Batch 2225, Loss 0.3539518415927887\n","[Training Epoch 4] Batch 2226, Loss 0.35510826110839844\n","[Training Epoch 4] Batch 2227, Loss 0.34710991382598877\n","[Training Epoch 4] Batch 2228, Loss 0.3796612620353699\n","[Training Epoch 4] Batch 2229, Loss 0.3639153838157654\n","[Training Epoch 4] Batch 2230, Loss 0.3862196207046509\n","[Training Epoch 4] Batch 2231, Loss 0.36641019582748413\n","[Training Epoch 4] Batch 2232, Loss 0.36266469955444336\n","[Training Epoch 4] Batch 2233, Loss 0.38956376910209656\n","[Training Epoch 4] Batch 2234, Loss 0.37912416458129883\n","[Training Epoch 4] Batch 2235, Loss 0.37560176849365234\n","[Training Epoch 4] Batch 2236, Loss 0.358028382062912\n","[Training Epoch 4] Batch 2237, Loss 0.3818283975124359\n","[Training Epoch 4] Batch 2238, Loss 0.34765952825546265\n","[Training Epoch 4] Batch 2239, Loss 0.34462904930114746\n","[Training Epoch 4] Batch 2240, Loss 0.3465791344642639\n","[Training Epoch 4] Batch 2241, Loss 0.34483855962753296\n","[Training Epoch 4] Batch 2242, Loss 0.3767950236797333\n","[Training Epoch 4] Batch 2243, Loss 0.35748183727264404\n","[Training Epoch 4] Batch 2244, Loss 0.3554705083370209\n","[Training Epoch 4] Batch 2245, Loss 0.3674236536026001\n","[Training Epoch 4] Batch 2246, Loss 0.3671957552433014\n","[Training Epoch 4] Batch 2247, Loss 0.36866194009780884\n","[Training Epoch 4] Batch 2248, Loss 0.37601417303085327\n","[Training Epoch 4] Batch 2249, Loss 0.36607301235198975\n","[Training Epoch 4] Batch 2250, Loss 0.36689525842666626\n","[Training Epoch 4] Batch 2251, Loss 0.34982556104660034\n","[Training Epoch 4] Batch 2252, Loss 0.359996497631073\n","[Training Epoch 4] Batch 2253, Loss 0.34864693880081177\n","[Training Epoch 4] Batch 2254, Loss 0.41856709122657776\n","[Training Epoch 4] Batch 2255, Loss 0.37461602687835693\n","[Training Epoch 4] Batch 2256, Loss 0.38103610277175903\n","[Training Epoch 4] Batch 2257, Loss 0.3684427738189697\n","[Training Epoch 4] Batch 2258, Loss 0.3501368761062622\n","[Training Epoch 4] Batch 2259, Loss 0.38606691360473633\n","[Training Epoch 4] Batch 2260, Loss 0.37892264127731323\n","[Training Epoch 4] Batch 2261, Loss 0.3775044083595276\n","[Training Epoch 4] Batch 2262, Loss 0.4244363307952881\n","[Training Epoch 4] Batch 2263, Loss 0.4043651223182678\n","[Training Epoch 4] Batch 2264, Loss 0.3519217073917389\n","[Training Epoch 4] Batch 2265, Loss 0.3655277490615845\n","[Training Epoch 4] Batch 2266, Loss 0.37039148807525635\n","[Training Epoch 4] Batch 2267, Loss 0.34619975090026855\n","[Training Epoch 4] Batch 2268, Loss 0.37874293327331543\n","[Training Epoch 4] Batch 2269, Loss 0.36124205589294434\n","[Training Epoch 4] Batch 2270, Loss 0.36463481187820435\n","[Training Epoch 4] Batch 2271, Loss 0.3911992311477661\n","[Training Epoch 4] Batch 2272, Loss 0.3618957996368408\n","[Training Epoch 4] Batch 2273, Loss 0.3851481080055237\n","[Training Epoch 4] Batch 2274, Loss 0.3738616108894348\n","[Training Epoch 4] Batch 2275, Loss 0.373884379863739\n","[Training Epoch 4] Batch 2276, Loss 0.39188021421432495\n","[Training Epoch 4] Batch 2277, Loss 0.3650940954685211\n","[Training Epoch 4] Batch 2278, Loss 0.3787968158721924\n","[Training Epoch 4] Batch 2279, Loss 0.35180750489234924\n","[Training Epoch 4] Batch 2280, Loss 0.36492371559143066\n","[Training Epoch 4] Batch 2281, Loss 0.3668202757835388\n","[Training Epoch 4] Batch 2282, Loss 0.36065441370010376\n","[Training Epoch 4] Batch 2283, Loss 0.3594846725463867\n","[Training Epoch 4] Batch 2284, Loss 0.3754592835903168\n","[Training Epoch 4] Batch 2285, Loss 0.33680978417396545\n","[Training Epoch 4] Batch 2286, Loss 0.38984715938568115\n","[Training Epoch 4] Batch 2287, Loss 0.3701028823852539\n","[Training Epoch 4] Batch 2288, Loss 0.35423171520233154\n","[Training Epoch 4] Batch 2289, Loss 0.37058359384536743\n","[Training Epoch 4] Batch 2290, Loss 0.38224294781684875\n","[Training Epoch 4] Batch 2291, Loss 0.37040185928344727\n","[Training Epoch 4] Batch 2292, Loss 0.38233238458633423\n","[Training Epoch 4] Batch 2293, Loss 0.3566015958786011\n","[Training Epoch 4] Batch 2294, Loss 0.35964393615722656\n","[Training Epoch 4] Batch 2295, Loss 0.3719138503074646\n","[Training Epoch 4] Batch 2296, Loss 0.35001254081726074\n","[Training Epoch 4] Batch 2297, Loss 0.362561970949173\n","[Training Epoch 4] Batch 2298, Loss 0.34472358226776123\n","[Training Epoch 4] Batch 2299, Loss 0.3951452970504761\n","[Training Epoch 4] Batch 2300, Loss 0.3957078158855438\n","[Training Epoch 4] Batch 2301, Loss 0.3879820704460144\n","[Training Epoch 4] Batch 2302, Loss 0.3453529477119446\n","[Training Epoch 4] Batch 2303, Loss 0.37150460481643677\n","[Training Epoch 4] Batch 2304, Loss 0.3384913206100464\n","[Training Epoch 4] Batch 2305, Loss 0.3816755414009094\n","[Training Epoch 4] Batch 2306, Loss 0.3680097460746765\n","[Training Epoch 4] Batch 2307, Loss 0.34077125787734985\n","[Training Epoch 4] Batch 2308, Loss 0.3687092065811157\n","[Training Epoch 4] Batch 2309, Loss 0.3464162349700928\n","[Training Epoch 4] Batch 2310, Loss 0.35479068756103516\n","[Training Epoch 4] Batch 2311, Loss 0.359829843044281\n","[Training Epoch 4] Batch 2312, Loss 0.3639634847640991\n","[Training Epoch 4] Batch 2313, Loss 0.35934385657310486\n","[Training Epoch 4] Batch 2314, Loss 0.36623066663742065\n","[Training Epoch 4] Batch 2315, Loss 0.372803658246994\n","[Training Epoch 4] Batch 2316, Loss 0.3365315794944763\n","[Training Epoch 4] Batch 2317, Loss 0.35554683208465576\n","[Training Epoch 4] Batch 2318, Loss 0.38334280252456665\n","[Training Epoch 4] Batch 2319, Loss 0.36389604210853577\n","[Training Epoch 4] Batch 2320, Loss 0.38477998971939087\n","[Training Epoch 4] Batch 2321, Loss 0.3702302873134613\n","[Training Epoch 4] Batch 2322, Loss 0.355516254901886\n","[Training Epoch 4] Batch 2323, Loss 0.3783205449581146\n","[Training Epoch 4] Batch 2324, Loss 0.39458444714546204\n","[Training Epoch 4] Batch 2325, Loss 0.34975317120552063\n","[Training Epoch 4] Batch 2326, Loss 0.36780938506126404\n","[Training Epoch 4] Batch 2327, Loss 0.3564342260360718\n","[Training Epoch 4] Batch 2328, Loss 0.3667287826538086\n","[Training Epoch 4] Batch 2329, Loss 0.38723745942115784\n","[Training Epoch 4] Batch 2330, Loss 0.35135534405708313\n","[Training Epoch 4] Batch 2331, Loss 0.33252230286598206\n","[Training Epoch 4] Batch 2332, Loss 0.35769540071487427\n","[Training Epoch 4] Batch 2333, Loss 0.37374067306518555\n","[Training Epoch 4] Batch 2334, Loss 0.3750748038291931\n","[Training Epoch 4] Batch 2335, Loss 0.3415464162826538\n","[Training Epoch 4] Batch 2336, Loss 0.3451288342475891\n","[Training Epoch 4] Batch 2337, Loss 0.36901038885116577\n","[Training Epoch 4] Batch 2338, Loss 0.32504403591156006\n","[Training Epoch 4] Batch 2339, Loss 0.3818040192127228\n","[Training Epoch 4] Batch 2340, Loss 0.40117621421813965\n","[Training Epoch 4] Batch 2341, Loss 0.3516711890697479\n","[Training Epoch 4] Batch 2342, Loss 0.37936505675315857\n","[Training Epoch 4] Batch 2343, Loss 0.3605238199234009\n","[Training Epoch 4] Batch 2344, Loss 0.37819385528564453\n","[Training Epoch 4] Batch 2345, Loss 0.37511855363845825\n","[Training Epoch 4] Batch 2346, Loss 0.3535383939743042\n","[Training Epoch 4] Batch 2347, Loss 0.34642404317855835\n","[Training Epoch 4] Batch 2348, Loss 0.34187018871307373\n","[Training Epoch 4] Batch 2349, Loss 0.34779879450798035\n","[Training Epoch 4] Batch 2350, Loss 0.3620451092720032\n","[Training Epoch 4] Batch 2351, Loss 0.38134506344795227\n","[Training Epoch 4] Batch 2352, Loss 0.36065393686294556\n","[Training Epoch 4] Batch 2353, Loss 0.38192862272262573\n","[Training Epoch 4] Batch 2354, Loss 0.3939456641674042\n","[Training Epoch 4] Batch 2355, Loss 0.40020620822906494\n","[Training Epoch 4] Batch 2356, Loss 0.3834185004234314\n","[Training Epoch 4] Batch 2357, Loss 0.3499695360660553\n","[Training Epoch 4] Batch 2358, Loss 0.3833833336830139\n","[Training Epoch 4] Batch 2359, Loss 0.38094618916511536\n","[Training Epoch 4] Batch 2360, Loss 0.3741373121738434\n","[Training Epoch 4] Batch 2361, Loss 0.384281724691391\n","[Training Epoch 4] Batch 2362, Loss 0.37309783697128296\n","[Training Epoch 4] Batch 2363, Loss 0.3752743601799011\n","[Training Epoch 4] Batch 2364, Loss 0.3359832167625427\n","[Training Epoch 4] Batch 2365, Loss 0.3533468246459961\n","[Training Epoch 4] Batch 2366, Loss 0.3616602420806885\n","[Training Epoch 4] Batch 2367, Loss 0.37712597846984863\n","[Training Epoch 4] Batch 2368, Loss 0.36945217847824097\n","[Training Epoch 4] Batch 2369, Loss 0.36446788907051086\n","[Training Epoch 4] Batch 2370, Loss 0.36678922176361084\n","[Training Epoch 4] Batch 2371, Loss 0.39004093408584595\n","[Training Epoch 4] Batch 2372, Loss 0.39385396242141724\n","[Training Epoch 4] Batch 2373, Loss 0.399377703666687\n","[Training Epoch 4] Batch 2374, Loss 0.383139967918396\n","[Training Epoch 4] Batch 2375, Loss 0.36821985244750977\n","[Training Epoch 4] Batch 2376, Loss 0.3591969609260559\n","[Training Epoch 4] Batch 2377, Loss 0.35293304920196533\n","[Training Epoch 4] Batch 2378, Loss 0.365575909614563\n","[Training Epoch 4] Batch 2379, Loss 0.3556252717971802\n","[Training Epoch 4] Batch 2380, Loss 0.34101536870002747\n","[Training Epoch 4] Batch 2381, Loss 0.3751174211502075\n","[Training Epoch 4] Batch 2382, Loss 0.350497305393219\n","[Training Epoch 4] Batch 2383, Loss 0.376094251871109\n","[Training Epoch 4] Batch 2384, Loss 0.37953275442123413\n","[Training Epoch 4] Batch 2385, Loss 0.36187663674354553\n","[Training Epoch 4] Batch 2386, Loss 0.38388651609420776\n","[Training Epoch 4] Batch 2387, Loss 0.3948093056678772\n","[Training Epoch 4] Batch 2388, Loss 0.3479839563369751\n","[Training Epoch 4] Batch 2389, Loss 0.3534820079803467\n","[Training Epoch 4] Batch 2390, Loss 0.3559165596961975\n","[Training Epoch 4] Batch 2391, Loss 0.35824093222618103\n","[Training Epoch 4] Batch 2392, Loss 0.34166961908340454\n","[Training Epoch 4] Batch 2393, Loss 0.3783060908317566\n","[Training Epoch 4] Batch 2394, Loss 0.3565554916858673\n","[Training Epoch 4] Batch 2395, Loss 0.3664931058883667\n","[Training Epoch 4] Batch 2396, Loss 0.3742464780807495\n","[Training Epoch 4] Batch 2397, Loss 0.32430407404899597\n","[Training Epoch 4] Batch 2398, Loss 0.39063483476638794\n","[Training Epoch 4] Batch 2399, Loss 0.3655474781990051\n","[Training Epoch 4] Batch 2400, Loss 0.32608461380004883\n","[Training Epoch 4] Batch 2401, Loss 0.3459600806236267\n","[Training Epoch 4] Batch 2402, Loss 0.35071271657943726\n","[Training Epoch 4] Batch 2403, Loss 0.35329338908195496\n","[Training Epoch 4] Batch 2404, Loss 0.36999091506004333\n","[Training Epoch 4] Batch 2405, Loss 0.3664199113845825\n","[Training Epoch 4] Batch 2406, Loss 0.3595179617404938\n","[Training Epoch 4] Batch 2407, Loss 0.3703140914440155\n","[Training Epoch 4] Batch 2408, Loss 0.4027506709098816\n","[Training Epoch 4] Batch 2409, Loss 0.3701034188270569\n","[Training Epoch 4] Batch 2410, Loss 0.36492806673049927\n","[Training Epoch 4] Batch 2411, Loss 0.3559863865375519\n","[Training Epoch 4] Batch 2412, Loss 0.3877614140510559\n","[Training Epoch 4] Batch 2413, Loss 0.3435823321342468\n","[Training Epoch 4] Batch 2414, Loss 0.373573899269104\n","[Training Epoch 4] Batch 2415, Loss 0.36219584941864014\n","[Training Epoch 4] Batch 2416, Loss 0.36670637130737305\n","[Training Epoch 4] Batch 2417, Loss 0.35399091243743896\n","[Training Epoch 4] Batch 2418, Loss 0.3780510425567627\n","[Training Epoch 4] Batch 2419, Loss 0.3877484202384949\n","[Training Epoch 4] Batch 2420, Loss 0.399655818939209\n","[Training Epoch 4] Batch 2421, Loss 0.3886769115924835\n","[Training Epoch 4] Batch 2422, Loss 0.4066523611545563\n","[Training Epoch 4] Batch 2423, Loss 0.37318938970565796\n","[Training Epoch 4] Batch 2424, Loss 0.35760635137557983\n","[Training Epoch 4] Batch 2425, Loss 0.34509193897247314\n","[Training Epoch 4] Batch 2426, Loss 0.3784022927284241\n","[Training Epoch 4] Batch 2427, Loss 0.3979281485080719\n","[Training Epoch 4] Batch 2428, Loss 0.3591756820678711\n","[Training Epoch 4] Batch 2429, Loss 0.36865055561065674\n","[Training Epoch 4] Batch 2430, Loss 0.35487258434295654\n","[Training Epoch 4] Batch 2431, Loss 0.3671901822090149\n","[Training Epoch 4] Batch 2432, Loss 0.3896543085575104\n","[Training Epoch 4] Batch 2433, Loss 0.34693610668182373\n","[Training Epoch 4] Batch 2434, Loss 0.3685716688632965\n","[Training Epoch 4] Batch 2435, Loss 0.3520820140838623\n","[Training Epoch 4] Batch 2436, Loss 0.3704892694950104\n","[Training Epoch 4] Batch 2437, Loss 0.35265669226646423\n","[Training Epoch 4] Batch 2438, Loss 0.37526190280914307\n","[Training Epoch 4] Batch 2439, Loss 0.3588005304336548\n","[Training Epoch 4] Batch 2440, Loss 0.3870859444141388\n","[Training Epoch 4] Batch 2441, Loss 0.3693842589855194\n","[Training Epoch 4] Batch 2442, Loss 0.36812904477119446\n","[Training Epoch 4] Batch 2443, Loss 0.36355119943618774\n","[Training Epoch 4] Batch 2444, Loss 0.3794443607330322\n","[Training Epoch 4] Batch 2445, Loss 0.35110530257225037\n","[Training Epoch 4] Batch 2446, Loss 0.37198615074157715\n","[Training Epoch 4] Batch 2447, Loss 0.3614060878753662\n","[Training Epoch 4] Batch 2448, Loss 0.35237646102905273\n","[Training Epoch 4] Batch 2449, Loss 0.38028985261917114\n","[Training Epoch 4] Batch 2450, Loss 0.34543561935424805\n","[Training Epoch 4] Batch 2451, Loss 0.3558773696422577\n","[Training Epoch 4] Batch 2452, Loss 0.38931456208229065\n","[Training Epoch 4] Batch 2453, Loss 0.3588656783103943\n","[Training Epoch 4] Batch 2454, Loss 0.35716304183006287\n","[Training Epoch 4] Batch 2455, Loss 0.38833701610565186\n","[Training Epoch 4] Batch 2456, Loss 0.34296154975891113\n","[Training Epoch 4] Batch 2457, Loss 0.3752884268760681\n","[Training Epoch 4] Batch 2458, Loss 0.3622215986251831\n","[Training Epoch 4] Batch 2459, Loss 0.344113826751709\n","[Training Epoch 4] Batch 2460, Loss 0.3592402935028076\n","[Training Epoch 4] Batch 2461, Loss 0.38494569063186646\n","[Training Epoch 4] Batch 2462, Loss 0.3704889416694641\n","[Training Epoch 4] Batch 2463, Loss 0.3731248378753662\n","[Training Epoch 4] Batch 2464, Loss 0.3945608139038086\n","[Training Epoch 4] Batch 2465, Loss 0.3549291491508484\n","[Training Epoch 4] Batch 2466, Loss 0.351182222366333\n","[Training Epoch 4] Batch 2467, Loss 0.3746849596500397\n","[Training Epoch 4] Batch 2468, Loss 0.3557564616203308\n","[Training Epoch 4] Batch 2469, Loss 0.35426926612854004\n","[Training Epoch 4] Batch 2470, Loss 0.3585880994796753\n","[Training Epoch 4] Batch 2471, Loss 0.3537822961807251\n","[Training Epoch 4] Batch 2472, Loss 0.35581958293914795\n","[Training Epoch 4] Batch 2473, Loss 0.35517942905426025\n","[Training Epoch 4] Batch 2474, Loss 0.3846017122268677\n","[Training Epoch 4] Batch 2475, Loss 0.3713943362236023\n","[Training Epoch 4] Batch 2476, Loss 0.3501623272895813\n","[Training Epoch 4] Batch 2477, Loss 0.380047470331192\n","[Training Epoch 4] Batch 2478, Loss 0.33526670932769775\n","[Training Epoch 4] Batch 2479, Loss 0.3591037094593048\n","[Training Epoch 4] Batch 2480, Loss 0.40352195501327515\n","[Training Epoch 4] Batch 2481, Loss 0.36137068271636963\n","[Training Epoch 4] Batch 2482, Loss 0.3705109655857086\n","[Training Epoch 4] Batch 2483, Loss 0.3394280672073364\n","[Training Epoch 4] Batch 2484, Loss 0.3754166066646576\n","[Training Epoch 4] Batch 2485, Loss 0.37581124901771545\n","[Training Epoch 4] Batch 2486, Loss 0.33906030654907227\n","[Training Epoch 4] Batch 2487, Loss 0.3665908873081207\n","[Training Epoch 4] Batch 2488, Loss 0.37281322479248047\n","[Training Epoch 4] Batch 2489, Loss 0.3361203074455261\n","[Training Epoch 4] Batch 2490, Loss 0.3779447078704834\n","[Training Epoch 4] Batch 2491, Loss 0.37889564037323\n","[Training Epoch 4] Batch 2492, Loss 0.37494850158691406\n","[Training Epoch 4] Batch 2493, Loss 0.34631699323654175\n","[Training Epoch 4] Batch 2494, Loss 0.3835272192955017\n","[Training Epoch 4] Batch 2495, Loss 0.3682999610900879\n","[Training Epoch 4] Batch 2496, Loss 0.3596012592315674\n","[Training Epoch 4] Batch 2497, Loss 0.3747779428958893\n","[Training Epoch 4] Batch 2498, Loss 0.3775271475315094\n","[Training Epoch 4] Batch 2499, Loss 0.37492549419403076\n","[Training Epoch 4] Batch 2500, Loss 0.4046289026737213\n","[Training Epoch 4] Batch 2501, Loss 0.3372080326080322\n","[Training Epoch 4] Batch 2502, Loss 0.3477281332015991\n","[Training Epoch 4] Batch 2503, Loss 0.3849064111709595\n","[Training Epoch 4] Batch 2504, Loss 0.3439202904701233\n","[Training Epoch 4] Batch 2505, Loss 0.3794030547142029\n","[Training Epoch 4] Batch 2506, Loss 0.3568640351295471\n","[Training Epoch 4] Batch 2507, Loss 0.34157243371009827\n","[Training Epoch 4] Batch 2508, Loss 0.3249119520187378\n","[Training Epoch 4] Batch 2509, Loss 0.3958002030849457\n","[Training Epoch 4] Batch 2510, Loss 0.354458749294281\n","[Training Epoch 4] Batch 2511, Loss 0.3495214581489563\n","[Training Epoch 4] Batch 2512, Loss 0.38437312841415405\n","[Training Epoch 4] Batch 2513, Loss 0.3674866557121277\n","[Training Epoch 4] Batch 2514, Loss 0.37618163228034973\n","[Training Epoch 4] Batch 2515, Loss 0.33614230155944824\n","[Training Epoch 4] Batch 2516, Loss 0.35676443576812744\n","[Training Epoch 4] Batch 2517, Loss 0.35530203580856323\n","[Training Epoch 4] Batch 2518, Loss 0.406433641910553\n","[Training Epoch 4] Batch 2519, Loss 0.37235593795776367\n","[Training Epoch 4] Batch 2520, Loss 0.37678879499435425\n","[Training Epoch 4] Batch 2521, Loss 0.36230093240737915\n","[Training Epoch 4] Batch 2522, Loss 0.353898823261261\n","[Training Epoch 4] Batch 2523, Loss 0.36817318201065063\n","[Training Epoch 4] Batch 2524, Loss 0.3707195520401001\n","[Training Epoch 4] Batch 2525, Loss 0.36378398537635803\n","[Training Epoch 4] Batch 2526, Loss 0.38928335905075073\n","[Training Epoch 4] Batch 2527, Loss 0.3730425238609314\n","[Training Epoch 4] Batch 2528, Loss 0.36803752183914185\n","[Training Epoch 4] Batch 2529, Loss 0.3622613549232483\n","[Training Epoch 4] Batch 2530, Loss 0.37941673398017883\n","[Training Epoch 4] Batch 2531, Loss 0.38555508852005005\n","[Training Epoch 4] Batch 2532, Loss 0.3774421811103821\n","[Training Epoch 4] Batch 2533, Loss 0.3773280382156372\n","[Training Epoch 4] Batch 2534, Loss 0.35230061411857605\n","[Training Epoch 4] Batch 2535, Loss 0.36763375997543335\n","[Training Epoch 4] Batch 2536, Loss 0.3708226680755615\n","[Training Epoch 4] Batch 2537, Loss 0.3490193486213684\n","[Training Epoch 4] Batch 2538, Loss 0.3439987897872925\n","[Training Epoch 4] Batch 2539, Loss 0.3850677013397217\n","[Training Epoch 4] Batch 2540, Loss 0.3615367114543915\n","[Training Epoch 4] Batch 2541, Loss 0.36345791816711426\n","[Training Epoch 4] Batch 2542, Loss 0.37068939208984375\n","[Training Epoch 4] Batch 2543, Loss 0.385297030210495\n","[Training Epoch 4] Batch 2544, Loss 0.36162200570106506\n","[Training Epoch 4] Batch 2545, Loss 0.36958783864974976\n","[Training Epoch 4] Batch 2546, Loss 0.36334118247032166\n","[Training Epoch 4] Batch 2547, Loss 0.3414720296859741\n","[Training Epoch 4] Batch 2548, Loss 0.36698031425476074\n","[Training Epoch 4] Batch 2549, Loss 0.36471444368362427\n","[Training Epoch 4] Batch 2550, Loss 0.3488484025001526\n","[Training Epoch 4] Batch 2551, Loss 0.34652167558670044\n","[Training Epoch 4] Batch 2552, Loss 0.3609365224838257\n","[Training Epoch 4] Batch 2553, Loss 0.3831578493118286\n","[Training Epoch 4] Batch 2554, Loss 0.37196069955825806\n","[Training Epoch 4] Batch 2555, Loss 0.34273460507392883\n","[Training Epoch 4] Batch 2556, Loss 0.3557451367378235\n","[Training Epoch 4] Batch 2557, Loss 0.3473958969116211\n","[Training Epoch 4] Batch 2558, Loss 0.37132444977760315\n","[Training Epoch 4] Batch 2559, Loss 0.38185030221939087\n","[Training Epoch 4] Batch 2560, Loss 0.353160560131073\n","[Training Epoch 4] Batch 2561, Loss 0.3638128340244293\n","[Training Epoch 4] Batch 2562, Loss 0.37753474712371826\n","[Training Epoch 4] Batch 2563, Loss 0.3554162085056305\n","[Training Epoch 4] Batch 2564, Loss 0.38352343440055847\n","[Training Epoch 4] Batch 2565, Loss 0.3586193323135376\n","[Training Epoch 4] Batch 2566, Loss 0.3425103425979614\n","[Training Epoch 4] Batch 2567, Loss 0.37646666169166565\n","[Training Epoch 4] Batch 2568, Loss 0.36761265993118286\n","[Training Epoch 4] Batch 2569, Loss 0.3831222653388977\n","[Training Epoch 4] Batch 2570, Loss 0.35714417695999146\n","[Training Epoch 4] Batch 2571, Loss 0.3775935471057892\n","[Training Epoch 4] Batch 2572, Loss 0.3688412308692932\n","[Training Epoch 4] Batch 2573, Loss 0.38071727752685547\n","[Training Epoch 4] Batch 2574, Loss 0.4021211266517639\n","[Training Epoch 4] Batch 2575, Loss 0.3495272397994995\n","[Training Epoch 4] Batch 2576, Loss 0.3749208450317383\n","[Training Epoch 4] Batch 2577, Loss 0.39736151695251465\n","[Training Epoch 4] Batch 2578, Loss 0.38561636209487915\n","[Training Epoch 4] Batch 2579, Loss 0.3298739194869995\n","[Training Epoch 4] Batch 2580, Loss 0.3616068661212921\n","[Training Epoch 4] Batch 2581, Loss 0.3716508746147156\n","[Training Epoch 4] Batch 2582, Loss 0.3634093701839447\n","[Training Epoch 4] Batch 2583, Loss 0.370580792427063\n","[Training Epoch 4] Batch 2584, Loss 0.39183127880096436\n","[Training Epoch 4] Batch 2585, Loss 0.3920149803161621\n","[Training Epoch 4] Batch 2586, Loss 0.3742840886116028\n","[Training Epoch 4] Batch 2587, Loss 0.37727558612823486\n","[Training Epoch 4] Batch 2588, Loss 0.3750535249710083\n","[Training Epoch 4] Batch 2589, Loss 0.35243332386016846\n","[Training Epoch 4] Batch 2590, Loss 0.37250766158103943\n","[Training Epoch 4] Batch 2591, Loss 0.35619911551475525\n","[Training Epoch 4] Batch 2592, Loss 0.38642823696136475\n","[Training Epoch 4] Batch 2593, Loss 0.3537256717681885\n","[Training Epoch 4] Batch 2594, Loss 0.33922260999679565\n","[Training Epoch 4] Batch 2595, Loss 0.3664332628250122\n","[Training Epoch 4] Batch 2596, Loss 0.36944103240966797\n","[Training Epoch 4] Batch 2597, Loss 0.37020695209503174\n","[Training Epoch 4] Batch 2598, Loss 0.3360721170902252\n","[Training Epoch 4] Batch 2599, Loss 0.3523375988006592\n","[Training Epoch 4] Batch 2600, Loss 0.3856394290924072\n","[Training Epoch 4] Batch 2601, Loss 0.37827134132385254\n","[Training Epoch 4] Batch 2602, Loss 0.34676796197891235\n","[Training Epoch 4] Batch 2603, Loss 0.3929517865180969\n","[Training Epoch 4] Batch 2604, Loss 0.3762527108192444\n","[Training Epoch 4] Batch 2605, Loss 0.344612717628479\n","[Training Epoch 4] Batch 2606, Loss 0.3628863990306854\n","[Training Epoch 4] Batch 2607, Loss 0.34321045875549316\n","[Training Epoch 4] Batch 2608, Loss 0.34823015332221985\n","[Training Epoch 4] Batch 2609, Loss 0.3728305399417877\n","[Training Epoch 4] Batch 2610, Loss 0.3481709957122803\n","[Training Epoch 4] Batch 2611, Loss 0.3788243532180786\n","[Training Epoch 4] Batch 2612, Loss 0.3585153818130493\n","[Training Epoch 4] Batch 2613, Loss 0.37110814452171326\n","[Training Epoch 4] Batch 2614, Loss 0.376678466796875\n","[Training Epoch 4] Batch 2615, Loss 0.3527274429798126\n","[Training Epoch 4] Batch 2616, Loss 0.39858952164649963\n","[Training Epoch 4] Batch 2617, Loss 0.37968215346336365\n","[Training Epoch 4] Batch 2618, Loss 0.3563903868198395\n","[Training Epoch 4] Batch 2619, Loss 0.35133910179138184\n","[Training Epoch 4] Batch 2620, Loss 0.35294029116630554\n","[Training Epoch 4] Batch 2621, Loss 0.372708797454834\n","[Training Epoch 4] Batch 2622, Loss 0.3643018305301666\n","[Training Epoch 4] Batch 2623, Loss 0.38665056228637695\n","[Training Epoch 4] Batch 2624, Loss 0.3267917037010193\n","[Training Epoch 4] Batch 2625, Loss 0.376941055059433\n","[Training Epoch 4] Batch 2626, Loss 0.3432491421699524\n","[Training Epoch 4] Batch 2627, Loss 0.3694528639316559\n","[Training Epoch 4] Batch 2628, Loss 0.378823459148407\n","[Training Epoch 4] Batch 2629, Loss 0.3712356686592102\n","[Training Epoch 4] Batch 2630, Loss 0.3735279440879822\n","[Training Epoch 4] Batch 2631, Loss 0.37875860929489136\n","[Training Epoch 4] Batch 2632, Loss 0.33311745524406433\n","[Training Epoch 4] Batch 2633, Loss 0.38681116700172424\n","[Training Epoch 4] Batch 2634, Loss 0.36411845684051514\n","[Training Epoch 4] Batch 2635, Loss 0.3745555877685547\n","[Training Epoch 4] Batch 2636, Loss 0.3516562283039093\n","[Training Epoch 4] Batch 2637, Loss 0.3692060708999634\n","[Training Epoch 4] Batch 2638, Loss 0.3844678997993469\n","[Training Epoch 4] Batch 2639, Loss 0.3463609218597412\n","[Training Epoch 4] Batch 2640, Loss 0.3812258839607239\n","[Training Epoch 4] Batch 2641, Loss 0.3565715551376343\n","[Training Epoch 4] Batch 2642, Loss 0.35930752754211426\n","[Training Epoch 4] Batch 2643, Loss 0.3708358108997345\n","[Training Epoch 4] Batch 2644, Loss 0.3893260955810547\n","[Training Epoch 4] Batch 2645, Loss 0.35502275824546814\n","[Training Epoch 4] Batch 2646, Loss 0.37518247961997986\n","[Training Epoch 4] Batch 2647, Loss 0.3444114327430725\n","[Training Epoch 4] Batch 2648, Loss 0.37618228793144226\n","[Training Epoch 4] Batch 2649, Loss 0.3863879442214966\n","[Training Epoch 4] Batch 2650, Loss 0.3926275968551636\n","[Training Epoch 4] Batch 2651, Loss 0.34775519371032715\n","[Training Epoch 4] Batch 2652, Loss 0.37425506114959717\n","[Training Epoch 4] Batch 2653, Loss 0.3679809868335724\n","[Training Epoch 4] Batch 2654, Loss 0.3671322762966156\n","[Training Epoch 4] Batch 2655, Loss 0.37969040870666504\n","[Training Epoch 4] Batch 2656, Loss 0.3805432617664337\n","[Training Epoch 4] Batch 2657, Loss 0.3733769655227661\n","[Training Epoch 4] Batch 2658, Loss 0.3774853050708771\n","[Training Epoch 4] Batch 2659, Loss 0.38285398483276367\n","[Training Epoch 4] Batch 2660, Loss 0.3509701192378998\n","[Training Epoch 4] Batch 2661, Loss 0.3523152470588684\n","[Training Epoch 4] Batch 2662, Loss 0.38655343651771545\n","[Training Epoch 4] Batch 2663, Loss 0.3583650290966034\n","[Training Epoch 4] Batch 2664, Loss 0.354911208152771\n","[Training Epoch 4] Batch 2665, Loss 0.37045514583587646\n","[Training Epoch 4] Batch 2666, Loss 0.3660890758037567\n","[Training Epoch 4] Batch 2667, Loss 0.3808257281780243\n","[Training Epoch 4] Batch 2668, Loss 0.389523983001709\n","[Training Epoch 4] Batch 2669, Loss 0.37811100482940674\n","[Training Epoch 4] Batch 2670, Loss 0.34627485275268555\n","[Training Epoch 4] Batch 2671, Loss 0.35972172021865845\n","[Training Epoch 4] Batch 2672, Loss 0.3612160086631775\n","[Training Epoch 4] Batch 2673, Loss 0.3711642026901245\n","[Training Epoch 4] Batch 2674, Loss 0.4057510495185852\n","[Training Epoch 4] Batch 2675, Loss 0.35454249382019043\n","[Training Epoch 4] Batch 2676, Loss 0.3758068084716797\n","[Training Epoch 4] Batch 2677, Loss 0.3759775757789612\n","[Training Epoch 4] Batch 2678, Loss 0.33774247765541077\n","[Training Epoch 4] Batch 2679, Loss 0.3637986183166504\n","[Training Epoch 4] Batch 2680, Loss 0.35444724559783936\n","[Training Epoch 4] Batch 2681, Loss 0.3592478334903717\n","[Training Epoch 4] Batch 2682, Loss 0.3775634169578552\n","[Training Epoch 4] Batch 2683, Loss 0.38969698548316956\n","[Training Epoch 4] Batch 2684, Loss 0.39387163519859314\n","[Training Epoch 4] Batch 2685, Loss 0.37420594692230225\n","[Training Epoch 4] Batch 2686, Loss 0.37688666582107544\n","[Training Epoch 4] Batch 2687, Loss 0.41422945261001587\n","[Training Epoch 4] Batch 2688, Loss 0.36925727128982544\n","[Training Epoch 4] Batch 2689, Loss 0.34840530157089233\n","[Training Epoch 4] Batch 2690, Loss 0.34323322772979736\n","[Training Epoch 4] Batch 2691, Loss 0.3675292134284973\n","[Training Epoch 4] Batch 2692, Loss 0.35687345266342163\n","[Training Epoch 4] Batch 2693, Loss 0.38875067234039307\n","[Training Epoch 4] Batch 2694, Loss 0.3559843599796295\n","[Training Epoch 4] Batch 2695, Loss 0.38018688559532166\n","[Training Epoch 4] Batch 2696, Loss 0.3377988934516907\n","[Training Epoch 4] Batch 2697, Loss 0.3725605010986328\n","[Training Epoch 4] Batch 2698, Loss 0.35770338773727417\n","[Training Epoch 4] Batch 2699, Loss 0.3660450875759125\n","[Training Epoch 4] Batch 2700, Loss 0.36875754594802856\n","[Training Epoch 4] Batch 2701, Loss 0.3480719327926636\n","[Training Epoch 4] Batch 2702, Loss 0.37635719776153564\n","[Training Epoch 4] Batch 2703, Loss 0.38032853603363037\n","[Training Epoch 4] Batch 2704, Loss 0.3681398034095764\n","[Training Epoch 4] Batch 2705, Loss 0.3605974316596985\n","[Training Epoch 4] Batch 2706, Loss 0.3686896562576294\n","[Training Epoch 4] Batch 2707, Loss 0.4136234521865845\n","[Training Epoch 4] Batch 2708, Loss 0.35022783279418945\n","[Training Epoch 4] Batch 2709, Loss 0.35577714443206787\n","[Training Epoch 4] Batch 2710, Loss 0.37340909242630005\n","[Training Epoch 4] Batch 2711, Loss 0.3692488968372345\n","[Training Epoch 4] Batch 2712, Loss 0.3412543535232544\n","[Training Epoch 4] Batch 2713, Loss 0.3563927710056305\n","[Training Epoch 4] Batch 2714, Loss 0.3438315987586975\n","[Training Epoch 4] Batch 2715, Loss 0.34884175658226013\n","[Training Epoch 4] Batch 2716, Loss 0.41229432821273804\n","[Training Epoch 4] Batch 2717, Loss 0.33546751737594604\n","[Training Epoch 4] Batch 2718, Loss 0.3715827167034149\n","[Training Epoch 4] Batch 2719, Loss 0.3548559546470642\n","[Training Epoch 4] Batch 2720, Loss 0.3741339445114136\n","[Training Epoch 4] Batch 2721, Loss 0.36613038182258606\n","[Training Epoch 4] Batch 2722, Loss 0.37251564860343933\n","[Training Epoch 4] Batch 2723, Loss 0.3778720796108246\n","[Training Epoch 4] Batch 2724, Loss 0.36246487498283386\n","[Training Epoch 4] Batch 2725, Loss 0.35786134004592896\n","[Training Epoch 4] Batch 2726, Loss 0.37052351236343384\n","[Training Epoch 4] Batch 2727, Loss 0.3594783544540405\n","[Training Epoch 4] Batch 2728, Loss 0.39778196811676025\n","[Training Epoch 4] Batch 2729, Loss 0.3530109226703644\n","[Training Epoch 4] Batch 2730, Loss 0.3595154285430908\n","[Training Epoch 4] Batch 2731, Loss 0.3857467770576477\n","[Training Epoch 4] Batch 2732, Loss 0.3545045852661133\n","[Training Epoch 4] Batch 2733, Loss 0.35128599405288696\n","[Training Epoch 4] Batch 2734, Loss 0.4021424651145935\n","[Training Epoch 4] Batch 2735, Loss 0.3711952567100525\n","[Training Epoch 4] Batch 2736, Loss 0.35748305916786194\n","[Training Epoch 4] Batch 2737, Loss 0.3926239609718323\n","[Training Epoch 4] Batch 2738, Loss 0.38539040088653564\n","[Training Epoch 4] Batch 2739, Loss 0.3530412018299103\n","[Training Epoch 4] Batch 2740, Loss 0.3664032220840454\n","[Training Epoch 4] Batch 2741, Loss 0.3865184783935547\n","[Training Epoch 4] Batch 2742, Loss 0.3977223336696625\n","[Training Epoch 4] Batch 2743, Loss 0.3433140516281128\n","[Training Epoch 4] Batch 2744, Loss 0.350297212600708\n","[Training Epoch 4] Batch 2745, Loss 0.3825157880783081\n","[Training Epoch 4] Batch 2746, Loss 0.38678497076034546\n","[Training Epoch 4] Batch 2747, Loss 0.368246853351593\n","[Training Epoch 4] Batch 2748, Loss 0.3738343119621277\n","[Training Epoch 4] Batch 2749, Loss 0.37683290243148804\n","[Training Epoch 4] Batch 2750, Loss 0.348737508058548\n","[Training Epoch 4] Batch 2751, Loss 0.37608104944229126\n","[Training Epoch 4] Batch 2752, Loss 0.37499916553497314\n","[Training Epoch 4] Batch 2753, Loss 0.3469769358634949\n","[Training Epoch 4] Batch 2754, Loss 0.37510302662849426\n","[Training Epoch 4] Batch 2755, Loss 0.3614688813686371\n","[Training Epoch 4] Batch 2756, Loss 0.37902629375457764\n","[Training Epoch 4] Batch 2757, Loss 0.356101393699646\n","[Training Epoch 4] Batch 2758, Loss 0.3689645528793335\n","[Training Epoch 4] Batch 2759, Loss 0.3227209150791168\n","[Training Epoch 4] Batch 2760, Loss 0.3626227378845215\n","[Training Epoch 4] Batch 2761, Loss 0.3837510943412781\n","[Training Epoch 4] Batch 2762, Loss 0.36290305852890015\n","[Training Epoch 4] Batch 2763, Loss 0.37772783637046814\n","[Training Epoch 4] Batch 2764, Loss 0.3811378479003906\n","[Training Epoch 4] Batch 2765, Loss 0.35323581099510193\n","[Training Epoch 4] Batch 2766, Loss 0.38017547130584717\n","[Training Epoch 4] Batch 2767, Loss 0.33464717864990234\n","[Training Epoch 4] Batch 2768, Loss 0.37979263067245483\n","[Training Epoch 4] Batch 2769, Loss 0.34842345118522644\n","[Training Epoch 4] Batch 2770, Loss 0.3945610523223877\n","[Training Epoch 4] Batch 2771, Loss 0.32362228631973267\n","[Training Epoch 4] Batch 2772, Loss 0.38355469703674316\n","[Training Epoch 4] Batch 2773, Loss 0.3479902446269989\n","[Training Epoch 4] Batch 2774, Loss 0.38711220026016235\n","[Training Epoch 4] Batch 2775, Loss 0.38103169202804565\n","[Training Epoch 4] Batch 2776, Loss 0.3649302124977112\n","[Training Epoch 4] Batch 2777, Loss 0.3719654977321625\n","[Training Epoch 4] Batch 2778, Loss 0.35514378547668457\n","[Training Epoch 4] Batch 2779, Loss 0.37653857469558716\n","[Training Epoch 4] Batch 2780, Loss 0.361191987991333\n","[Training Epoch 4] Batch 2781, Loss 0.4184998869895935\n","[Training Epoch 4] Batch 2782, Loss 0.3758406341075897\n","[Training Epoch 4] Batch 2783, Loss 0.3677608370780945\n","[Training Epoch 4] Batch 2784, Loss 0.37371790409088135\n","[Training Epoch 4] Batch 2785, Loss 0.3548889458179474\n","[Training Epoch 4] Batch 2786, Loss 0.3680296540260315\n","[Training Epoch 4] Batch 2787, Loss 0.37465402483940125\n","[Training Epoch 4] Batch 2788, Loss 0.3467159867286682\n","[Training Epoch 4] Batch 2789, Loss 0.35306185483932495\n","[Training Epoch 4] Batch 2790, Loss 0.34514984488487244\n","[Training Epoch 4] Batch 2791, Loss 0.35392093658447266\n","[Training Epoch 4] Batch 2792, Loss 0.3545781373977661\n","[Training Epoch 4] Batch 2793, Loss 0.37784674763679504\n","[Training Epoch 4] Batch 2794, Loss 0.3841054439544678\n","[Training Epoch 4] Batch 2795, Loss 0.3386106789112091\n","[Training Epoch 4] Batch 2796, Loss 0.3912241458892822\n","[Training Epoch 4] Batch 2797, Loss 0.3983154892921448\n","[Training Epoch 4] Batch 2798, Loss 0.35985058546066284\n","[Training Epoch 4] Batch 2799, Loss 0.3810694217681885\n","[Training Epoch 4] Batch 2800, Loss 0.3474932610988617\n","[Training Epoch 4] Batch 2801, Loss 0.3535771071910858\n","[Training Epoch 4] Batch 2802, Loss 0.355036199092865\n","[Training Epoch 4] Batch 2803, Loss 0.392708420753479\n","[Training Epoch 4] Batch 2804, Loss 0.33512622117996216\n","[Training Epoch 4] Batch 2805, Loss 0.3451826274394989\n","[Training Epoch 4] Batch 2806, Loss 0.358384907245636\n","[Training Epoch 4] Batch 2807, Loss 0.36476531624794006\n","[Training Epoch 4] Batch 2808, Loss 0.37928295135498047\n","[Training Epoch 4] Batch 2809, Loss 0.36636093258857727\n","[Training Epoch 4] Batch 2810, Loss 0.3607032299041748\n","[Training Epoch 4] Batch 2811, Loss 0.3914148509502411\n","[Training Epoch 4] Batch 2812, Loss 0.35840216279029846\n","[Training Epoch 4] Batch 2813, Loss 0.36137887835502625\n","[Training Epoch 4] Batch 2814, Loss 0.37684255838394165\n","[Training Epoch 4] Batch 2815, Loss 0.35839328169822693\n","[Training Epoch 4] Batch 2816, Loss 0.3533027172088623\n","[Training Epoch 4] Batch 2817, Loss 0.34476035833358765\n","[Training Epoch 4] Batch 2818, Loss 0.3673802614212036\n","[Training Epoch 4] Batch 2819, Loss 0.37017902731895447\n","[Training Epoch 4] Batch 2820, Loss 0.35905352234840393\n","[Training Epoch 4] Batch 2821, Loss 0.37370821833610535\n","[Training Epoch 4] Batch 2822, Loss 0.40879735350608826\n","[Training Epoch 4] Batch 2823, Loss 0.3711506128311157\n","[Training Epoch 4] Batch 2824, Loss 0.390128493309021\n","[Training Epoch 4] Batch 2825, Loss 0.3160848915576935\n","[Training Epoch 4] Batch 2826, Loss 0.36889195442199707\n","[Training Epoch 4] Batch 2827, Loss 0.34772825241088867\n","[Training Epoch 4] Batch 2828, Loss 0.3683810830116272\n","[Training Epoch 4] Batch 2829, Loss 0.3529587388038635\n","[Training Epoch 4] Batch 2830, Loss 0.34904253482818604\n","[Training Epoch 4] Batch 2831, Loss 0.36514055728912354\n","[Training Epoch 4] Batch 2832, Loss 0.36916235089302063\n","[Training Epoch 4] Batch 2833, Loss 0.3492780327796936\n","[Training Epoch 4] Batch 2834, Loss 0.36138349771499634\n","[Training Epoch 4] Batch 2835, Loss 0.3311063051223755\n","[Training Epoch 4] Batch 2836, Loss 0.3895733952522278\n","[Training Epoch 4] Batch 2837, Loss 0.38313230872154236\n","[Training Epoch 4] Batch 2838, Loss 0.33814969658851624\n","[Training Epoch 4] Batch 2839, Loss 0.36027592420578003\n","[Training Epoch 4] Batch 2840, Loss 0.39696717262268066\n","[Training Epoch 4] Batch 2841, Loss 0.36079055070877075\n","[Training Epoch 4] Batch 2842, Loss 0.38531386852264404\n","[Training Epoch 4] Batch 2843, Loss 0.3779488205909729\n","[Training Epoch 4] Batch 2844, Loss 0.3618023097515106\n","[Training Epoch 4] Batch 2845, Loss 0.3688693344593048\n","[Training Epoch 4] Batch 2846, Loss 0.34695324301719666\n","[Training Epoch 4] Batch 2847, Loss 0.336871862411499\n","[Training Epoch 4] Batch 2848, Loss 0.35839807987213135\n","[Training Epoch 4] Batch 2849, Loss 0.3826131820678711\n","[Training Epoch 4] Batch 2850, Loss 0.36734655499458313\n","[Training Epoch 4] Batch 2851, Loss 0.363910973072052\n","[Training Epoch 4] Batch 2852, Loss 0.3564926087856293\n","[Training Epoch 4] Batch 2853, Loss 0.38467925786972046\n","[Training Epoch 4] Batch 2854, Loss 0.3589858412742615\n","[Training Epoch 4] Batch 2855, Loss 0.3586810827255249\n","[Training Epoch 4] Batch 2856, Loss 0.3662077784538269\n","[Training Epoch 4] Batch 2857, Loss 0.35895368456840515\n","[Training Epoch 4] Batch 2858, Loss 0.3537006378173828\n","[Training Epoch 4] Batch 2859, Loss 0.39109349250793457\n","[Training Epoch 4] Batch 2860, Loss 0.39172327518463135\n","[Training Epoch 4] Batch 2861, Loss 0.37857070565223694\n","[Training Epoch 4] Batch 2862, Loss 0.3428593575954437\n","[Training Epoch 4] Batch 2863, Loss 0.36840352416038513\n","[Training Epoch 4] Batch 2864, Loss 0.34517642855644226\n","[Training Epoch 4] Batch 2865, Loss 0.361025333404541\n","[Training Epoch 4] Batch 2866, Loss 0.4213807284832001\n","[Training Epoch 4] Batch 2867, Loss 0.3610134720802307\n","[Training Epoch 4] Batch 2868, Loss 0.35917341709136963\n","[Training Epoch 4] Batch 2869, Loss 0.373643159866333\n","[Training Epoch 4] Batch 2870, Loss 0.3580241799354553\n","[Training Epoch 4] Batch 2871, Loss 0.36801576614379883\n","[Training Epoch 4] Batch 2872, Loss 0.34424662590026855\n","[Training Epoch 4] Batch 2873, Loss 0.3893163800239563\n","[Training Epoch 4] Batch 2874, Loss 0.350580096244812\n","[Training Epoch 4] Batch 2875, Loss 0.34089595079421997\n","[Training Epoch 4] Batch 2876, Loss 0.34594982862472534\n","[Training Epoch 4] Batch 2877, Loss 0.3448571562767029\n","[Training Epoch 4] Batch 2878, Loss 0.37170159816741943\n","[Training Epoch 4] Batch 2879, Loss 0.358254611492157\n","[Training Epoch 4] Batch 2880, Loss 0.36708033084869385\n","[Training Epoch 4] Batch 2881, Loss 0.3268060088157654\n","[Training Epoch 4] Batch 2882, Loss 0.35006171464920044\n","[Training Epoch 4] Batch 2883, Loss 0.3465307652950287\n","[Training Epoch 4] Batch 2884, Loss 0.350677490234375\n","[Training Epoch 4] Batch 2885, Loss 0.33992698788642883\n","[Training Epoch 4] Batch 2886, Loss 0.3712080717086792\n","[Training Epoch 4] Batch 2887, Loss 0.36469757556915283\n","[Training Epoch 4] Batch 2888, Loss 0.3764258325099945\n","[Training Epoch 4] Batch 2889, Loss 0.325190007686615\n","[Training Epoch 4] Batch 2890, Loss 0.37198489904403687\n","[Training Epoch 4] Batch 2891, Loss 0.366629421710968\n","[Training Epoch 4] Batch 2892, Loss 0.37414321303367615\n","[Training Epoch 4] Batch 2893, Loss 0.3403697609901428\n","[Training Epoch 4] Batch 2894, Loss 0.3540199398994446\n","[Training Epoch 4] Batch 2895, Loss 0.36022597551345825\n","[Training Epoch 4] Batch 2896, Loss 0.3464202582836151\n","[Training Epoch 4] Batch 2897, Loss 0.37795037031173706\n","[Training Epoch 4] Batch 2898, Loss 0.366699755191803\n","[Training Epoch 4] Batch 2899, Loss 0.3565991520881653\n","[Training Epoch 4] Batch 2900, Loss 0.379509836435318\n","[Training Epoch 4] Batch 2901, Loss 0.39991843700408936\n","[Training Epoch 4] Batch 2902, Loss 0.349149227142334\n","[Training Epoch 4] Batch 2903, Loss 0.34783393144607544\n","[Training Epoch 4] Batch 2904, Loss 0.39170828461647034\n","[Training Epoch 4] Batch 2905, Loss 0.37640538811683655\n","[Training Epoch 4] Batch 2906, Loss 0.38054466247558594\n","[Training Epoch 4] Batch 2907, Loss 0.37953221797943115\n","[Training Epoch 4] Batch 2908, Loss 0.3753068447113037\n","[Training Epoch 4] Batch 2909, Loss 0.3812904953956604\n","[Training Epoch 4] Batch 2910, Loss 0.3854854106903076\n","[Training Epoch 4] Batch 2911, Loss 0.3768288493156433\n","[Training Epoch 4] Batch 2912, Loss 0.37760815024375916\n","[Training Epoch 4] Batch 2913, Loss 0.3573012948036194\n","[Training Epoch 4] Batch 2914, Loss 0.38623571395874023\n","[Training Epoch 4] Batch 2915, Loss 0.3533611297607422\n","[Training Epoch 4] Batch 2916, Loss 0.3900512456893921\n","[Training Epoch 4] Batch 2917, Loss 0.3586019277572632\n","[Training Epoch 4] Batch 2918, Loss 0.35634681582450867\n","[Training Epoch 4] Batch 2919, Loss 0.42448481917381287\n","[Training Epoch 4] Batch 2920, Loss 0.3446585237979889\n","[Training Epoch 4] Batch 2921, Loss 0.38767826557159424\n","[Training Epoch 4] Batch 2922, Loss 0.40433427691459656\n","[Training Epoch 4] Batch 2923, Loss 0.35540884733200073\n","[Training Epoch 4] Batch 2924, Loss 0.3656497001647949\n","[Training Epoch 4] Batch 2925, Loss 0.3502586781978607\n","[Training Epoch 4] Batch 2926, Loss 0.3763585388660431\n","[Training Epoch 4] Batch 2927, Loss 0.3582383394241333\n","[Training Epoch 4] Batch 2928, Loss 0.3643091022968292\n","[Training Epoch 4] Batch 2929, Loss 0.37754207849502563\n","[Training Epoch 4] Batch 2930, Loss 0.37593820691108704\n","[Training Epoch 4] Batch 2931, Loss 0.37360966205596924\n","[Training Epoch 4] Batch 2932, Loss 0.37080878019332886\n","[Training Epoch 4] Batch 2933, Loss 0.37361806631088257\n","[Training Epoch 4] Batch 2934, Loss 0.36439985036849976\n","[Training Epoch 4] Batch 2935, Loss 0.3641887307167053\n","[Training Epoch 4] Batch 2936, Loss 0.37028056383132935\n","[Training Epoch 4] Batch 2937, Loss 0.3335043489933014\n","[Training Epoch 4] Batch 2938, Loss 0.35627859830856323\n","[Training Epoch 4] Batch 2939, Loss 0.3886841833591461\n","[Training Epoch 4] Batch 2940, Loss 0.3700310289859772\n","[Training Epoch 4] Batch 2941, Loss 0.3807831406593323\n","[Training Epoch 4] Batch 2942, Loss 0.3657798171043396\n","[Training Epoch 4] Batch 2943, Loss 0.34666669368743896\n","[Training Epoch 4] Batch 2944, Loss 0.3538309335708618\n","[Training Epoch 4] Batch 2945, Loss 0.3469851016998291\n","[Training Epoch 4] Batch 2946, Loss 0.3992725908756256\n","[Training Epoch 4] Batch 2947, Loss 0.3939703106880188\n","[Training Epoch 4] Batch 2948, Loss 0.3461194336414337\n","[Training Epoch 4] Batch 2949, Loss 0.3580400347709656\n","[Training Epoch 4] Batch 2950, Loss 0.3655230700969696\n","[Training Epoch 4] Batch 2951, Loss 0.35990777611732483\n","[Training Epoch 4] Batch 2952, Loss 0.34673357009887695\n","[Training Epoch 4] Batch 2953, Loss 0.33842021226882935\n","[Training Epoch 4] Batch 2954, Loss 0.3833179473876953\n","[Training Epoch 4] Batch 2955, Loss 0.364803284406662\n","[Training Epoch 4] Batch 2956, Loss 0.3409379720687866\n","[Training Epoch 4] Batch 2957, Loss 0.36505815386772156\n","[Training Epoch 4] Batch 2958, Loss 0.3923775851726532\n","[Training Epoch 4] Batch 2959, Loss 0.3853585124015808\n","[Training Epoch 4] Batch 2960, Loss 0.3647713363170624\n","[Training Epoch 4] Batch 2961, Loss 0.37467825412750244\n","[Training Epoch 4] Batch 2962, Loss 0.35938364267349243\n","[Training Epoch 4] Batch 2963, Loss 0.3580275774002075\n","[Training Epoch 4] Batch 2964, Loss 0.3708401322364807\n","[Training Epoch 4] Batch 2965, Loss 0.3742000162601471\n","[Training Epoch 4] Batch 2966, Loss 0.37594276666641235\n","[Training Epoch 4] Batch 2967, Loss 0.36035728454589844\n","[Training Epoch 4] Batch 2968, Loss 0.3450029492378235\n","[Training Epoch 4] Batch 2969, Loss 0.35899102687835693\n","[Training Epoch 4] Batch 2970, Loss 0.35302913188934326\n","[Training Epoch 4] Batch 2971, Loss 0.3595068156719208\n","[Training Epoch 4] Batch 2972, Loss 0.3547515273094177\n","[Training Epoch 4] Batch 2973, Loss 0.3423217535018921\n","[Training Epoch 4] Batch 2974, Loss 0.3584296703338623\n","[Training Epoch 4] Batch 2975, Loss 0.34220466017723083\n","[Training Epoch 4] Batch 2976, Loss 0.3669125735759735\n","[Training Epoch 4] Batch 2977, Loss 0.360940158367157\n","[Training Epoch 4] Batch 2978, Loss 0.3778190016746521\n","[Training Epoch 4] Batch 2979, Loss 0.3562855124473572\n","[Training Epoch 4] Batch 2980, Loss 0.3529670834541321\n","[Training Epoch 4] Batch 2981, Loss 0.36633485555648804\n","[Training Epoch 4] Batch 2982, Loss 0.3605154752731323\n","[Training Epoch 4] Batch 2983, Loss 0.3675249218940735\n","[Training Epoch 4] Batch 2984, Loss 0.3614499866962433\n","[Training Epoch 4] Batch 2985, Loss 0.3633388578891754\n","[Training Epoch 4] Batch 2986, Loss 0.40071454644203186\n","[Training Epoch 4] Batch 2987, Loss 0.3580637276172638\n","[Training Epoch 4] Batch 2988, Loss 0.3524148464202881\n","[Training Epoch 4] Batch 2989, Loss 0.3827770948410034\n","[Training Epoch 4] Batch 2990, Loss 0.3616081476211548\n","[Training Epoch 4] Batch 2991, Loss 0.3703613877296448\n","[Training Epoch 4] Batch 2992, Loss 0.4022645652294159\n","[Training Epoch 4] Batch 2993, Loss 0.3598933219909668\n","[Training Epoch 4] Batch 2994, Loss 0.3621506094932556\n","[Training Epoch 4] Batch 2995, Loss 0.3531496524810791\n","[Training Epoch 4] Batch 2996, Loss 0.3722573518753052\n","[Training Epoch 4] Batch 2997, Loss 0.3514396846294403\n","[Training Epoch 4] Batch 2998, Loss 0.363631010055542\n","[Training Epoch 4] Batch 2999, Loss 0.40912342071533203\n","[Training Epoch 4] Batch 3000, Loss 0.3836434483528137\n","[Training Epoch 4] Batch 3001, Loss 0.3808584213256836\n","[Training Epoch 4] Batch 3002, Loss 0.36808258295059204\n","[Training Epoch 4] Batch 3003, Loss 0.3594592213630676\n","[Training Epoch 4] Batch 3004, Loss 0.3582296669483185\n","[Training Epoch 4] Batch 3005, Loss 0.3610350489616394\n","[Training Epoch 4] Batch 3006, Loss 0.35998037457466125\n","[Training Epoch 4] Batch 3007, Loss 0.37212246656417847\n","[Training Epoch 4] Batch 3008, Loss 0.34391337633132935\n","[Training Epoch 4] Batch 3009, Loss 0.37526872754096985\n","[Training Epoch 4] Batch 3010, Loss 0.3625132441520691\n","[Training Epoch 4] Batch 3011, Loss 0.3629615604877472\n","[Training Epoch 4] Batch 3012, Loss 0.3369726836681366\n","[Training Epoch 4] Batch 3013, Loss 0.3734612464904785\n","[Training Epoch 4] Batch 3014, Loss 0.34491199254989624\n","[Training Epoch 4] Batch 3015, Loss 0.3527674078941345\n","[Training Epoch 4] Batch 3016, Loss 0.39656564593315125\n","[Training Epoch 4] Batch 3017, Loss 0.36584362387657166\n","[Training Epoch 4] Batch 3018, Loss 0.33912894129753113\n","[Training Epoch 4] Batch 3019, Loss 0.3595195710659027\n","[Training Epoch 4] Batch 3020, Loss 0.4085064232349396\n","[Training Epoch 4] Batch 3021, Loss 0.3785449266433716\n","[Training Epoch 4] Batch 3022, Loss 0.3911655843257904\n","[Training Epoch 4] Batch 3023, Loss 0.3825366497039795\n","[Training Epoch 4] Batch 3024, Loss 0.358470618724823\n","[Training Epoch 4] Batch 3025, Loss 0.3876416087150574\n","[Training Epoch 4] Batch 3026, Loss 0.38275426626205444\n","[Training Epoch 4] Batch 3027, Loss 0.3674565851688385\n","[Training Epoch 4] Batch 3028, Loss 0.3560293912887573\n","[Training Epoch 4] Batch 3029, Loss 0.3563690185546875\n","[Training Epoch 4] Batch 3030, Loss 0.37813249230384827\n","[Training Epoch 4] Batch 3031, Loss 0.36167576909065247\n","[Training Epoch 4] Batch 3032, Loss 0.34528934955596924\n","[Training Epoch 4] Batch 3033, Loss 0.3712308406829834\n","[Training Epoch 4] Batch 3034, Loss 0.34631091356277466\n","[Training Epoch 4] Batch 3035, Loss 0.36323755979537964\n","[Training Epoch 4] Batch 3036, Loss 0.334170401096344\n","[Training Epoch 4] Batch 3037, Loss 0.3467133939266205\n","[Training Epoch 4] Batch 3038, Loss 0.40526777505874634\n","[Training Epoch 4] Batch 3039, Loss 0.3525606393814087\n","[Training Epoch 4] Batch 3040, Loss 0.37211117148399353\n","[Training Epoch 4] Batch 3041, Loss 0.3734227418899536\n","[Training Epoch 4] Batch 3042, Loss 0.3953857123851776\n","[Training Epoch 4] Batch 3043, Loss 0.37129533290863037\n","[Training Epoch 4] Batch 3044, Loss 0.3646174669265747\n","[Training Epoch 4] Batch 3045, Loss 0.350170761346817\n","[Training Epoch 4] Batch 3046, Loss 0.3503538966178894\n","[Training Epoch 4] Batch 3047, Loss 0.3628842830657959\n","[Training Epoch 4] Batch 3048, Loss 0.3388044238090515\n","[Training Epoch 4] Batch 3049, Loss 0.37231844663619995\n","[Training Epoch 4] Batch 3050, Loss 0.32368016242980957\n","[Training Epoch 4] Batch 3051, Loss 0.36626487970352173\n","[Training Epoch 4] Batch 3052, Loss 0.34792596101760864\n","[Training Epoch 4] Batch 3053, Loss 0.3481486439704895\n","[Training Epoch 4] Batch 3054, Loss 0.35962802171707153\n","[Training Epoch 4] Batch 3055, Loss 0.37256985902786255\n","[Training Epoch 4] Batch 3056, Loss 0.3685559034347534\n","[Training Epoch 4] Batch 3057, Loss 0.3516401946544647\n","[Training Epoch 4] Batch 3058, Loss 0.3421051502227783\n","[Training Epoch 4] Batch 3059, Loss 0.36877697706222534\n","[Training Epoch 4] Batch 3060, Loss 0.36420953273773193\n","[Training Epoch 4] Batch 3061, Loss 0.34299391508102417\n","[Training Epoch 4] Batch 3062, Loss 0.35704338550567627\n","[Training Epoch 4] Batch 3063, Loss 0.384334534406662\n","[Training Epoch 4] Batch 3064, Loss 0.3635799288749695\n","[Training Epoch 4] Batch 3065, Loss 0.3675761818885803\n","[Training Epoch 4] Batch 3066, Loss 0.38409730792045593\n","[Training Epoch 4] Batch 3067, Loss 0.34853440523147583\n","[Training Epoch 4] Batch 3068, Loss 0.3573344051837921\n","[Training Epoch 4] Batch 3069, Loss 0.3856498599052429\n","[Training Epoch 4] Batch 3070, Loss 0.36983785033226013\n","[Training Epoch 4] Batch 3071, Loss 0.34800001978874207\n","[Training Epoch 4] Batch 3072, Loss 0.36095088720321655\n","[Training Epoch 4] Batch 3073, Loss 0.38220739364624023\n","[Training Epoch 4] Batch 3074, Loss 0.37936145067214966\n","[Training Epoch 4] Batch 3075, Loss 0.34924453496932983\n","[Training Epoch 4] Batch 3076, Loss 0.37676218152046204\n","[Training Epoch 4] Batch 3077, Loss 0.36487969756126404\n","[Training Epoch 4] Batch 3078, Loss 0.38343873620033264\n","[Training Epoch 4] Batch 3079, Loss 0.3626614809036255\n","[Training Epoch 4] Batch 3080, Loss 0.3504258692264557\n","[Training Epoch 4] Batch 3081, Loss 0.3482811152935028\n","[Training Epoch 4] Batch 3082, Loss 0.3701387047767639\n","[Training Epoch 4] Batch 3083, Loss 0.35623008012771606\n","[Training Epoch 4] Batch 3084, Loss 0.36268389225006104\n","[Training Epoch 4] Batch 3085, Loss 0.3638473153114319\n","[Training Epoch 4] Batch 3086, Loss 0.3936728537082672\n","[Training Epoch 4] Batch 3087, Loss 0.35890090465545654\n","[Training Epoch 4] Batch 3088, Loss 0.3707951307296753\n","[Training Epoch 4] Batch 3089, Loss 0.3532010316848755\n","[Training Epoch 4] Batch 3090, Loss 0.3753635883331299\n","[Training Epoch 4] Batch 3091, Loss 0.3585538864135742\n","[Training Epoch 4] Batch 3092, Loss 0.3707032799720764\n","[Training Epoch 4] Batch 3093, Loss 0.32395946979522705\n","[Training Epoch 4] Batch 3094, Loss 0.3983680307865143\n","[Training Epoch 4] Batch 3095, Loss 0.35446497797966003\n","[Training Epoch 4] Batch 3096, Loss 0.38154926896095276\n","[Training Epoch 4] Batch 3097, Loss 0.3711738586425781\n","[Training Epoch 4] Batch 3098, Loss 0.3854506313800812\n","[Training Epoch 4] Batch 3099, Loss 0.38082146644592285\n","[Training Epoch 4] Batch 3100, Loss 0.36872541904449463\n","[Training Epoch 4] Batch 3101, Loss 0.3524813652038574\n","[Training Epoch 4] Batch 3102, Loss 0.3632725179195404\n","[Training Epoch 4] Batch 3103, Loss 0.3631191551685333\n","[Training Epoch 4] Batch 3104, Loss 0.38033944368362427\n","[Training Epoch 4] Batch 3105, Loss 0.35801905393600464\n","[Training Epoch 4] Batch 3106, Loss 0.35465022921562195\n","[Training Epoch 4] Batch 3107, Loss 0.3489537239074707\n","[Training Epoch 4] Batch 3108, Loss 0.3477797210216522\n","[Training Epoch 4] Batch 3109, Loss 0.36261609196662903\n","[Training Epoch 4] Batch 3110, Loss 0.3340136408805847\n","[Training Epoch 4] Batch 3111, Loss 0.3453463315963745\n","[Training Epoch 4] Batch 3112, Loss 0.3629087507724762\n","[Training Epoch 4] Batch 3113, Loss 0.38856884837150574\n","[Training Epoch 4] Batch 3114, Loss 0.37028419971466064\n","[Training Epoch 4] Batch 3115, Loss 0.3581533432006836\n","[Training Epoch 4] Batch 3116, Loss 0.35921287536621094\n","[Training Epoch 4] Batch 3117, Loss 0.35093122720718384\n","[Training Epoch 4] Batch 3118, Loss 0.3611152470111847\n","[Training Epoch 4] Batch 3119, Loss 0.3403704762458801\n","[Training Epoch 4] Batch 3120, Loss 0.3706766963005066\n","[Training Epoch 4] Batch 3121, Loss 0.37476420402526855\n","[Training Epoch 4] Batch 3122, Loss 0.39099472761154175\n","[Training Epoch 4] Batch 3123, Loss 0.3445066511631012\n","[Training Epoch 4] Batch 3124, Loss 0.3427754044532776\n","[Training Epoch 4] Batch 3125, Loss 0.32638609409332275\n","[Training Epoch 4] Batch 3126, Loss 0.36000582575798035\n","[Training Epoch 4] Batch 3127, Loss 0.36521223187446594\n","[Training Epoch 4] Batch 3128, Loss 0.3405477702617645\n","[Training Epoch 4] Batch 3129, Loss 0.36821305751800537\n","[Training Epoch 4] Batch 3130, Loss 0.363906592130661\n","[Training Epoch 4] Batch 3131, Loss 0.3928050696849823\n","[Training Epoch 4] Batch 3132, Loss 0.36232003569602966\n","[Training Epoch 4] Batch 3133, Loss 0.3474756181240082\n","[Training Epoch 4] Batch 3134, Loss 0.3859664797782898\n","[Training Epoch 4] Batch 3135, Loss 0.4286695420742035\n","[Training Epoch 4] Batch 3136, Loss 0.37251001596450806\n","[Training Epoch 4] Batch 3137, Loss 0.3893360495567322\n","[Training Epoch 4] Batch 3138, Loss 0.3183833360671997\n","[Training Epoch 4] Batch 3139, Loss 0.3920646905899048\n","[Training Epoch 4] Batch 3140, Loss 0.33464479446411133\n","[Training Epoch 4] Batch 3141, Loss 0.3629838824272156\n","[Training Epoch 4] Batch 3142, Loss 0.36594685912132263\n","[Training Epoch 4] Batch 3143, Loss 0.3764260709285736\n","[Training Epoch 4] Batch 3144, Loss 0.3525780439376831\n","[Training Epoch 4] Batch 3145, Loss 0.36548715829849243\n","[Training Epoch 4] Batch 3146, Loss 0.3670525550842285\n","[Training Epoch 4] Batch 3147, Loss 0.3845852315425873\n","[Training Epoch 4] Batch 3148, Loss 0.35910922288894653\n","[Training Epoch 4] Batch 3149, Loss 0.34986981749534607\n","[Training Epoch 4] Batch 3150, Loss 0.3528253734111786\n","[Training Epoch 4] Batch 3151, Loss 0.3888375163078308\n","[Training Epoch 4] Batch 3152, Loss 0.3862763047218323\n","[Training Epoch 4] Batch 3153, Loss 0.33624154329299927\n","[Training Epoch 4] Batch 3154, Loss 0.35099858045578003\n","[Training Epoch 4] Batch 3155, Loss 0.3726292550563812\n","[Training Epoch 4] Batch 3156, Loss 0.37294459342956543\n","[Training Epoch 4] Batch 3157, Loss 0.36410003900527954\n","[Training Epoch 4] Batch 3158, Loss 0.37950772047042847\n","[Training Epoch 4] Batch 3159, Loss 0.3388001620769501\n","[Training Epoch 4] Batch 3160, Loss 0.3647734820842743\n","[Training Epoch 4] Batch 3161, Loss 0.3934858441352844\n","[Training Epoch 4] Batch 3162, Loss 0.40045803785324097\n","[Training Epoch 4] Batch 3163, Loss 0.37217527627944946\n","[Training Epoch 4] Batch 3164, Loss 0.3284766674041748\n","[Training Epoch 4] Batch 3165, Loss 0.3717607259750366\n","[Training Epoch 4] Batch 3166, Loss 0.37906551361083984\n","[Training Epoch 4] Batch 3167, Loss 0.3822057843208313\n","[Training Epoch 4] Batch 3168, Loss 0.3719315528869629\n","[Training Epoch 4] Batch 3169, Loss 0.374871164560318\n","[Training Epoch 4] Batch 3170, Loss 0.40470045804977417\n","[Training Epoch 4] Batch 3171, Loss 0.3463878333568573\n","[Training Epoch 4] Batch 3172, Loss 0.3549814224243164\n","[Training Epoch 4] Batch 3173, Loss 0.3758281469345093\n","[Training Epoch 4] Batch 3174, Loss 0.38135814666748047\n","[Training Epoch 4] Batch 3175, Loss 0.37445423007011414\n","[Training Epoch 4] Batch 3176, Loss 0.3426642417907715\n","[Training Epoch 4] Batch 3177, Loss 0.4173361361026764\n","[Training Epoch 4] Batch 3178, Loss 0.37142130732536316\n","[Training Epoch 4] Batch 3179, Loss 0.3451957702636719\n","[Training Epoch 4] Batch 3180, Loss 0.37331345677375793\n","[Training Epoch 4] Batch 3181, Loss 0.34976571798324585\n","[Training Epoch 4] Batch 3182, Loss 0.3659284710884094\n","[Training Epoch 4] Batch 3183, Loss 0.3767664432525635\n","[Training Epoch 4] Batch 3184, Loss 0.37502190470695496\n","[Training Epoch 4] Batch 3185, Loss 0.3510741591453552\n","[Training Epoch 4] Batch 3186, Loss 0.3649357557296753\n","[Training Epoch 4] Batch 3187, Loss 0.37310951948165894\n","[Training Epoch 4] Batch 3188, Loss 0.36482614278793335\n","[Training Epoch 4] Batch 3189, Loss 0.3725627064704895\n","[Training Epoch 4] Batch 3190, Loss 0.35872170329093933\n","[Training Epoch 4] Batch 3191, Loss 0.3586350679397583\n","[Training Epoch 4] Batch 3192, Loss 0.3717285394668579\n","[Training Epoch 4] Batch 3193, Loss 0.38040798902511597\n","[Training Epoch 4] Batch 3194, Loss 0.3634151220321655\n","[Training Epoch 4] Batch 3195, Loss 0.3527236580848694\n","[Training Epoch 4] Batch 3196, Loss 0.3664618730545044\n","[Training Epoch 4] Batch 3197, Loss 0.3784072995185852\n","[Training Epoch 4] Batch 3198, Loss 0.365886926651001\n","[Training Epoch 4] Batch 3199, Loss 0.3808879256248474\n","[Training Epoch 4] Batch 3200, Loss 0.37205785512924194\n","[Training Epoch 4] Batch 3201, Loss 0.3927733302116394\n","[Training Epoch 4] Batch 3202, Loss 0.3548511266708374\n","[Training Epoch 4] Batch 3203, Loss 0.36456066370010376\n","[Training Epoch 4] Batch 3204, Loss 0.39391380548477173\n","[Training Epoch 4] Batch 3205, Loss 0.33648020029067993\n","[Training Epoch 4] Batch 3206, Loss 0.38400977849960327\n","[Training Epoch 4] Batch 3207, Loss 0.37261635065078735\n","[Training Epoch 4] Batch 3208, Loss 0.4047345519065857\n","[Training Epoch 4] Batch 3209, Loss 0.3686380088329315\n","[Training Epoch 4] Batch 3210, Loss 0.35259705781936646\n","[Training Epoch 4] Batch 3211, Loss 0.3639433681964874\n","[Training Epoch 4] Batch 3212, Loss 0.3563936948776245\n","[Training Epoch 4] Batch 3213, Loss 0.3575957417488098\n","[Training Epoch 4] Batch 3214, Loss 0.3659716546535492\n","[Training Epoch 4] Batch 3215, Loss 0.3566073179244995\n","[Training Epoch 4] Batch 3216, Loss 0.4044903516769409\n","[Training Epoch 4] Batch 3217, Loss 0.34438076615333557\n","[Training Epoch 4] Batch 3218, Loss 0.3755227327346802\n","[Training Epoch 4] Batch 3219, Loss 0.3480997085571289\n","[Training Epoch 4] Batch 3220, Loss 0.3091462254524231\n","[Training Epoch 4] Batch 3221, Loss 0.34202679991722107\n","[Training Epoch 4] Batch 3222, Loss 0.39081117510795593\n","[Training Epoch 4] Batch 3223, Loss 0.3221544325351715\n","[Training Epoch 4] Batch 3224, Loss 0.352699875831604\n","[Training Epoch 4] Batch 3225, Loss 0.370442271232605\n","[Training Epoch 4] Batch 3226, Loss 0.3743082582950592\n","[Training Epoch 4] Batch 3227, Loss 0.3541660010814667\n","[Training Epoch 4] Batch 3228, Loss 0.34841427206993103\n","[Training Epoch 4] Batch 3229, Loss 0.35494664311408997\n","[Training Epoch 4] Batch 3230, Loss 0.3781241178512573\n","[Training Epoch 4] Batch 3231, Loss 0.36461275815963745\n","[Training Epoch 4] Batch 3232, Loss 0.3852119445800781\n","[Training Epoch 4] Batch 3233, Loss 0.33664846420288086\n","[Training Epoch 4] Batch 3234, Loss 0.3341379165649414\n","[Training Epoch 4] Batch 3235, Loss 0.35571181774139404\n","[Training Epoch 4] Batch 3236, Loss 0.38010990619659424\n","[Training Epoch 4] Batch 3237, Loss 0.363528311252594\n","[Training Epoch 4] Batch 3238, Loss 0.3354755640029907\n","[Training Epoch 4] Batch 3239, Loss 0.39001113176345825\n","[Training Epoch 4] Batch 3240, Loss 0.38578295707702637\n","[Training Epoch 4] Batch 3241, Loss 0.37437117099761963\n","[Training Epoch 4] Batch 3242, Loss 0.3581593334674835\n","[Training Epoch 4] Batch 3243, Loss 0.35421425104141235\n","[Training Epoch 4] Batch 3244, Loss 0.35580408573150635\n","[Training Epoch 4] Batch 3245, Loss 0.37535539269447327\n","[Training Epoch 4] Batch 3246, Loss 0.37084510922431946\n","[Training Epoch 4] Batch 3247, Loss 0.38265740871429443\n","[Training Epoch 4] Batch 3248, Loss 0.359825074672699\n","[Training Epoch 4] Batch 3249, Loss 0.3617589473724365\n","[Training Epoch 4] Batch 3250, Loss 0.36887210607528687\n","[Training Epoch 4] Batch 3251, Loss 0.34973689913749695\n","[Training Epoch 4] Batch 3252, Loss 0.37966811656951904\n","[Training Epoch 4] Batch 3253, Loss 0.3580951690673828\n","[Training Epoch 4] Batch 3254, Loss 0.3407992720603943\n","[Training Epoch 4] Batch 3255, Loss 0.3836660087108612\n","[Training Epoch 4] Batch 3256, Loss 0.37659674882888794\n","[Training Epoch 4] Batch 3257, Loss 0.3652716279029846\n","[Training Epoch 4] Batch 3258, Loss 0.34786200523376465\n","[Training Epoch 4] Batch 3259, Loss 0.3706805109977722\n","[Training Epoch 4] Batch 3260, Loss 0.34373849630355835\n","[Training Epoch 4] Batch 3261, Loss 0.35760068893432617\n","[Training Epoch 4] Batch 3262, Loss 0.40080082416534424\n","[Training Epoch 4] Batch 3263, Loss 0.37396240234375\n","[Training Epoch 4] Batch 3264, Loss 0.3953089118003845\n","[Training Epoch 4] Batch 3265, Loss 0.3436998128890991\n","[Training Epoch 4] Batch 3266, Loss 0.37875649333000183\n","[Training Epoch 4] Batch 3267, Loss 0.37157368659973145\n","[Training Epoch 4] Batch 3268, Loss 0.3442615866661072\n","[Training Epoch 4] Batch 3269, Loss 0.36689049005508423\n","[Training Epoch 4] Batch 3270, Loss 0.3729863464832306\n","[Training Epoch 4] Batch 3271, Loss 0.3705074191093445\n","[Training Epoch 4] Batch 3272, Loss 0.3545563220977783\n","[Training Epoch 4] Batch 3273, Loss 0.3810168504714966\n","[Training Epoch 4] Batch 3274, Loss 0.39105379581451416\n","[Training Epoch 4] Batch 3275, Loss 0.39848336577415466\n","[Training Epoch 4] Batch 3276, Loss 0.3748254179954529\n","[Training Epoch 4] Batch 3277, Loss 0.3552025556564331\n","[Training Epoch 4] Batch 3278, Loss 0.3974568247795105\n","[Training Epoch 4] Batch 3279, Loss 0.35504859685897827\n","[Training Epoch 4] Batch 3280, Loss 0.35434603691101074\n","[Training Epoch 4] Batch 3281, Loss 0.39322376251220703\n","[Training Epoch 4] Batch 3282, Loss 0.36410754919052124\n","[Training Epoch 4] Batch 3283, Loss 0.3662094473838806\n","[Training Epoch 4] Batch 3284, Loss 0.3569396138191223\n","[Training Epoch 4] Batch 3285, Loss 0.3623185157775879\n","[Training Epoch 4] Batch 3286, Loss 0.3377184271812439\n","[Training Epoch 4] Batch 3287, Loss 0.37295976281166077\n","[Training Epoch 4] Batch 3288, Loss 0.3356155455112457\n","[Training Epoch 4] Batch 3289, Loss 0.35837292671203613\n","[Training Epoch 4] Batch 3290, Loss 0.36250239610671997\n","[Training Epoch 4] Batch 3291, Loss 0.3691157400608063\n","[Training Epoch 4] Batch 3292, Loss 0.3457207679748535\n","[Training Epoch 4] Batch 3293, Loss 0.3886308968067169\n","[Training Epoch 4] Batch 3294, Loss 0.3728289008140564\n","[Training Epoch 4] Batch 3295, Loss 0.36282846331596375\n","[Training Epoch 4] Batch 3296, Loss 0.3395642340183258\n","[Training Epoch 4] Batch 3297, Loss 0.3661903738975525\n","[Training Epoch 4] Batch 3298, Loss 0.3769417405128479\n","[Training Epoch 4] Batch 3299, Loss 0.38856714963912964\n","[Training Epoch 4] Batch 3300, Loss 0.3645821213722229\n","[Training Epoch 4] Batch 3301, Loss 0.40166762471199036\n","[Training Epoch 4] Batch 3302, Loss 0.3418281674385071\n","[Training Epoch 4] Batch 3303, Loss 0.3762880861759186\n","[Training Epoch 4] Batch 3304, Loss 0.37296801805496216\n","[Training Epoch 4] Batch 3305, Loss 0.33314821124076843\n","[Training Epoch 4] Batch 3306, Loss 0.3752983808517456\n","[Training Epoch 4] Batch 3307, Loss 0.3733672797679901\n","[Training Epoch 4] Batch 3308, Loss 0.34050917625427246\n","[Training Epoch 4] Batch 3309, Loss 0.3735062777996063\n","[Training Epoch 4] Batch 3310, Loss 0.3607383966445923\n","[Training Epoch 4] Batch 3311, Loss 0.36511439085006714\n","[Training Epoch 4] Batch 3312, Loss 0.39841529726982117\n","[Training Epoch 4] Batch 3313, Loss 0.3751643896102905\n","[Training Epoch 4] Batch 3314, Loss 0.3538236916065216\n","[Training Epoch 4] Batch 3315, Loss 0.3494810461997986\n","[Training Epoch 4] Batch 3316, Loss 0.3694077134132385\n","[Training Epoch 4] Batch 3317, Loss 0.37435853481292725\n","[Training Epoch 4] Batch 3318, Loss 0.36145827174186707\n","[Training Epoch 4] Batch 3319, Loss 0.3807212710380554\n","[Training Epoch 4] Batch 3320, Loss 0.3484543561935425\n","[Training Epoch 4] Batch 3321, Loss 0.3457641303539276\n","[Training Epoch 4] Batch 3322, Loss 0.34594088792800903\n","[Training Epoch 4] Batch 3323, Loss 0.4139477610588074\n","[Training Epoch 4] Batch 3324, Loss 0.4012029767036438\n","[Training Epoch 4] Batch 3325, Loss 0.37005552649497986\n","[Training Epoch 4] Batch 3326, Loss 0.38306358456611633\n","[Training Epoch 4] Batch 3327, Loss 0.35699981451034546\n","[Training Epoch 4] Batch 3328, Loss 0.37856751680374146\n","[Training Epoch 4] Batch 3329, Loss 0.34566935896873474\n","[Training Epoch 4] Batch 3330, Loss 0.34104812145233154\n","[Training Epoch 4] Batch 3331, Loss 0.3548654019832611\n","[Training Epoch 4] Batch 3332, Loss 0.3599942922592163\n","[Training Epoch 4] Batch 3333, Loss 0.3855997323989868\n","[Training Epoch 4] Batch 3334, Loss 0.36715245246887207\n","[Training Epoch 4] Batch 3335, Loss 0.3593350648880005\n","[Training Epoch 4] Batch 3336, Loss 0.3760696351528168\n","[Training Epoch 4] Batch 3337, Loss 0.32459038496017456\n","[Training Epoch 4] Batch 3338, Loss 0.3937489986419678\n","[Training Epoch 4] Batch 3339, Loss 0.3631645739078522\n","[Training Epoch 4] Batch 3340, Loss 0.3991166055202484\n","[Training Epoch 4] Batch 3341, Loss 0.3763503432273865\n","[Training Epoch 4] Batch 3342, Loss 0.353634774684906\n","[Training Epoch 4] Batch 3343, Loss 0.3610546290874481\n","[Training Epoch 4] Batch 3344, Loss 0.3706800043582916\n","[Training Epoch 4] Batch 3345, Loss 0.35673394799232483\n","[Training Epoch 4] Batch 3346, Loss 0.3791002631187439\n","[Training Epoch 4] Batch 3347, Loss 0.3841172456741333\n","[Training Epoch 4] Batch 3348, Loss 0.34407907724380493\n","[Training Epoch 4] Batch 3349, Loss 0.345060795545578\n","[Training Epoch 4] Batch 3350, Loss 0.34726405143737793\n","[Training Epoch 4] Batch 3351, Loss 0.3526875376701355\n","[Training Epoch 4] Batch 3352, Loss 0.3669917583465576\n","[Training Epoch 4] Batch 3353, Loss 0.3629454970359802\n","[Training Epoch 4] Batch 3354, Loss 0.3635762929916382\n","[Training Epoch 4] Batch 3355, Loss 0.3444622755050659\n","[Training Epoch 4] Batch 3356, Loss 0.34507113695144653\n","[Training Epoch 4] Batch 3357, Loss 0.36700013279914856\n","[Training Epoch 4] Batch 3358, Loss 0.37058162689208984\n","[Training Epoch 4] Batch 3359, Loss 0.3977701961994171\n","[Training Epoch 4] Batch 3360, Loss 0.35669124126434326\n","[Training Epoch 4] Batch 3361, Loss 0.3565945029258728\n","[Training Epoch 4] Batch 3362, Loss 0.36433178186416626\n","[Training Epoch 4] Batch 3363, Loss 0.34368568658828735\n","[Training Epoch 4] Batch 3364, Loss 0.3580471873283386\n","[Training Epoch 4] Batch 3365, Loss 0.32290661334991455\n","[Training Epoch 4] Batch 3366, Loss 0.38254314661026\n","[Training Epoch 4] Batch 3367, Loss 0.40436089038848877\n","[Training Epoch 4] Batch 3368, Loss 0.38155731558799744\n","[Training Epoch 4] Batch 3369, Loss 0.3713769018650055\n","[Training Epoch 4] Batch 3370, Loss 0.36943548917770386\n","[Training Epoch 4] Batch 3371, Loss 0.36808183789253235\n","[Training Epoch 4] Batch 3372, Loss 0.3650321066379547\n","[Training Epoch 4] Batch 3373, Loss 0.38261592388153076\n","[Training Epoch 4] Batch 3374, Loss 0.3445333242416382\n","[Training Epoch 4] Batch 3375, Loss 0.36194777488708496\n","[Training Epoch 4] Batch 3376, Loss 0.3748532235622406\n","[Training Epoch 4] Batch 3377, Loss 0.3584737479686737\n","[Training Epoch 4] Batch 3378, Loss 0.358455628156662\n","[Training Epoch 4] Batch 3379, Loss 0.347762793302536\n","[Training Epoch 4] Batch 3380, Loss 0.39435821771621704\n","[Training Epoch 4] Batch 3381, Loss 0.36866605281829834\n","[Training Epoch 4] Batch 3382, Loss 0.3585917353630066\n","[Training Epoch 4] Batch 3383, Loss 0.3621274530887604\n","[Training Epoch 4] Batch 3384, Loss 0.35292163491249084\n","[Training Epoch 4] Batch 3385, Loss 0.3930584192276001\n","[Training Epoch 4] Batch 3386, Loss 0.3505193889141083\n","[Training Epoch 4] Batch 3387, Loss 0.3914877772331238\n","[Training Epoch 4] Batch 3388, Loss 0.37044757604599\n","[Training Epoch 4] Batch 3389, Loss 0.38037556409835815\n","[Training Epoch 4] Batch 3390, Loss 0.35075920820236206\n","[Training Epoch 4] Batch 3391, Loss 0.3688322901725769\n","[Training Epoch 4] Batch 3392, Loss 0.361764132976532\n","[Training Epoch 4] Batch 3393, Loss 0.34124553203582764\n","[Training Epoch 4] Batch 3394, Loss 0.39212122559547424\n","[Training Epoch 4] Batch 3395, Loss 0.3506079316139221\n","[Training Epoch 4] Batch 3396, Loss 0.3647070527076721\n","[Training Epoch 4] Batch 3397, Loss 0.3634769916534424\n","[Training Epoch 4] Batch 3398, Loss 0.33614301681518555\n","[Training Epoch 4] Batch 3399, Loss 0.36695003509521484\n","[Training Epoch 4] Batch 3400, Loss 0.35292333364486694\n","[Training Epoch 4] Batch 3401, Loss 0.3902266025543213\n","[Training Epoch 4] Batch 3402, Loss 0.3475761413574219\n","[Training Epoch 4] Batch 3403, Loss 0.3678979277610779\n","[Training Epoch 4] Batch 3404, Loss 0.3603649139404297\n","[Training Epoch 4] Batch 3405, Loss 0.3324365019798279\n","[Training Epoch 4] Batch 3406, Loss 0.369375616312027\n","[Training Epoch 4] Batch 3407, Loss 0.3941965103149414\n","[Training Epoch 4] Batch 3408, Loss 0.37041565775871277\n","[Training Epoch 4] Batch 3409, Loss 0.36621469259262085\n","[Training Epoch 4] Batch 3410, Loss 0.3849871754646301\n","[Training Epoch 4] Batch 3411, Loss 0.3525949716567993\n","[Training Epoch 4] Batch 3412, Loss 0.3832182288169861\n","[Training Epoch 4] Batch 3413, Loss 0.35700082778930664\n","[Training Epoch 4] Batch 3414, Loss 0.3590008020401001\n","[Training Epoch 4] Batch 3415, Loss 0.36019229888916016\n","[Training Epoch 4] Batch 3416, Loss 0.3576239347457886\n","[Training Epoch 4] Batch 3417, Loss 0.3831249475479126\n","[Training Epoch 4] Batch 3418, Loss 0.36513978242874146\n","[Training Epoch 4] Batch 3419, Loss 0.36215320229530334\n","[Training Epoch 4] Batch 3420, Loss 0.3445299565792084\n","[Training Epoch 4] Batch 3421, Loss 0.382277250289917\n","[Training Epoch 4] Batch 3422, Loss 0.3781919777393341\n","[Training Epoch 4] Batch 3423, Loss 0.3624725937843323\n","[Training Epoch 4] Batch 3424, Loss 0.33715009689331055\n","[Training Epoch 4] Batch 3425, Loss 0.35264915227890015\n","[Training Epoch 4] Batch 3426, Loss 0.36777085065841675\n","[Training Epoch 4] Batch 3427, Loss 0.3888058364391327\n","[Training Epoch 4] Batch 3428, Loss 0.3498407304286957\n","[Training Epoch 4] Batch 3429, Loss 0.3809182941913605\n","[Training Epoch 4] Batch 3430, Loss 0.36736416816711426\n","[Training Epoch 4] Batch 3431, Loss 0.3626534342765808\n","[Training Epoch 4] Batch 3432, Loss 0.3512035608291626\n","[Training Epoch 4] Batch 3433, Loss 0.3591299057006836\n","[Training Epoch 4] Batch 3434, Loss 0.34974074363708496\n","[Training Epoch 4] Batch 3435, Loss 0.35743677616119385\n","[Training Epoch 4] Batch 3436, Loss 0.3627285361289978\n","[Training Epoch 4] Batch 3437, Loss 0.3413616120815277\n","[Training Epoch 4] Batch 3438, Loss 0.3539111614227295\n","[Training Epoch 4] Batch 3439, Loss 0.38389068841934204\n","[Training Epoch 4] Batch 3440, Loss 0.3475147485733032\n","[Training Epoch 4] Batch 3441, Loss 0.36732059717178345\n","[Training Epoch 4] Batch 3442, Loss 0.40626275539398193\n","[Training Epoch 4] Batch 3443, Loss 0.3671683669090271\n","[Training Epoch 4] Batch 3444, Loss 0.3346845507621765\n","[Training Epoch 4] Batch 3445, Loss 0.35925814509391785\n","[Training Epoch 4] Batch 3446, Loss 0.3695710301399231\n","[Training Epoch 4] Batch 3447, Loss 0.36533474922180176\n","[Training Epoch 4] Batch 3448, Loss 0.3786264657974243\n","[Training Epoch 4] Batch 3449, Loss 0.34368568658828735\n","[Training Epoch 4] Batch 3450, Loss 0.374330997467041\n","[Training Epoch 4] Batch 3451, Loss 0.3468172252178192\n","[Training Epoch 4] Batch 3452, Loss 0.35776758193969727\n","[Training Epoch 4] Batch 3453, Loss 0.3806798458099365\n","[Training Epoch 4] Batch 3454, Loss 0.36273443698883057\n","[Training Epoch 4] Batch 3455, Loss 0.3641435503959656\n","[Training Epoch 4] Batch 3456, Loss 0.3499348759651184\n","[Training Epoch 4] Batch 3457, Loss 0.34227097034454346\n","[Training Epoch 4] Batch 3458, Loss 0.37034982442855835\n","[Training Epoch 4] Batch 3459, Loss 0.37752965092658997\n","[Training Epoch 4] Batch 3460, Loss 0.3440212607383728\n","[Training Epoch 4] Batch 3461, Loss 0.3734353482723236\n","[Training Epoch 4] Batch 3462, Loss 0.36130744218826294\n","[Training Epoch 4] Batch 3463, Loss 0.3743723928928375\n","[Training Epoch 4] Batch 3464, Loss 0.386832058429718\n","[Training Epoch 4] Batch 3465, Loss 0.3613772988319397\n","[Training Epoch 4] Batch 3466, Loss 0.33509206771850586\n","[Training Epoch 4] Batch 3467, Loss 0.35308903455734253\n","[Training Epoch 4] Batch 3468, Loss 0.357932984828949\n","[Training Epoch 4] Batch 3469, Loss 0.360729843378067\n","[Training Epoch 4] Batch 3470, Loss 0.37675949931144714\n","[Training Epoch 4] Batch 3471, Loss 0.3378640413284302\n","[Training Epoch 4] Batch 3472, Loss 0.3567918539047241\n","[Training Epoch 4] Batch 3473, Loss 0.37589508295059204\n","[Training Epoch 4] Batch 3474, Loss 0.359807550907135\n","[Training Epoch 4] Batch 3475, Loss 0.3224112391471863\n","[Training Epoch 4] Batch 3476, Loss 0.38857316970825195\n","[Training Epoch 4] Batch 3477, Loss 0.35788607597351074\n","[Training Epoch 4] Batch 3478, Loss 0.35640448331832886\n","[Training Epoch 4] Batch 3479, Loss 0.3461414575576782\n","[Training Epoch 4] Batch 3480, Loss 0.3503667116165161\n","[Training Epoch 4] Batch 3481, Loss 0.3742765784263611\n","[Training Epoch 4] Batch 3482, Loss 0.37549203634262085\n","[Training Epoch 4] Batch 3483, Loss 0.3604162037372589\n","[Training Epoch 4] Batch 3484, Loss 0.3659384846687317\n","[Training Epoch 4] Batch 3485, Loss 0.3506869077682495\n","[Training Epoch 4] Batch 3486, Loss 0.34970441460609436\n","[Training Epoch 4] Batch 3487, Loss 0.3365517556667328\n","[Training Epoch 4] Batch 3488, Loss 0.35636478662490845\n","[Training Epoch 4] Batch 3489, Loss 0.3611450791358948\n","[Training Epoch 4] Batch 3490, Loss 0.3738667964935303\n","[Training Epoch 4] Batch 3491, Loss 0.3682001531124115\n","[Training Epoch 4] Batch 3492, Loss 0.35411375761032104\n","[Training Epoch 4] Batch 3493, Loss 0.3711426556110382\n","[Training Epoch 4] Batch 3494, Loss 0.3739688992500305\n","[Training Epoch 4] Batch 3495, Loss 0.34743741154670715\n","[Training Epoch 4] Batch 3496, Loss 0.39062777161598206\n","[Training Epoch 4] Batch 3497, Loss 0.36558085680007935\n","[Training Epoch 4] Batch 3498, Loss 0.3516646921634674\n","[Training Epoch 4] Batch 3499, Loss 0.35419148206710815\n","[Training Epoch 4] Batch 3500, Loss 0.3705542981624603\n","[Training Epoch 4] Batch 3501, Loss 0.3842645287513733\n","[Training Epoch 4] Batch 3502, Loss 0.358611524105072\n","[Training Epoch 4] Batch 3503, Loss 0.3507848381996155\n","[Training Epoch 4] Batch 3504, Loss 0.3732668161392212\n","[Training Epoch 4] Batch 3505, Loss 0.39071595668792725\n","[Training Epoch 4] Batch 3506, Loss 0.32668691873550415\n","[Training Epoch 4] Batch 3507, Loss 0.35664382576942444\n","[Training Epoch 4] Batch 3508, Loss 0.3779434561729431\n","[Training Epoch 4] Batch 3509, Loss 0.36102157831192017\n","[Training Epoch 4] Batch 3510, Loss 0.37834632396698\n","[Training Epoch 4] Batch 3511, Loss 0.33612972497940063\n","[Training Epoch 4] Batch 3512, Loss 0.37462425231933594\n","[Training Epoch 4] Batch 3513, Loss 0.36255550384521484\n","[Training Epoch 4] Batch 3514, Loss 0.3862125277519226\n","[Training Epoch 4] Batch 3515, Loss 0.3570452928543091\n","[Training Epoch 4] Batch 3516, Loss 0.3775102496147156\n","[Training Epoch 4] Batch 3517, Loss 0.3487437963485718\n","[Training Epoch 4] Batch 3518, Loss 0.3693881630897522\n","[Training Epoch 4] Batch 3519, Loss 0.3863334059715271\n","[Training Epoch 4] Batch 3520, Loss 0.3374782204627991\n","[Training Epoch 4] Batch 3521, Loss 0.3670530319213867\n","[Training Epoch 4] Batch 3522, Loss 0.381557822227478\n","[Training Epoch 4] Batch 3523, Loss 0.3493163287639618\n","[Training Epoch 4] Batch 3524, Loss 0.3886909484863281\n","[Training Epoch 4] Batch 3525, Loss 0.31821244955062866\n","[Training Epoch 4] Batch 3526, Loss 0.40170061588287354\n","[Training Epoch 4] Batch 3527, Loss 0.34881189465522766\n","[Training Epoch 4] Batch 3528, Loss 0.3264656960964203\n","[Training Epoch 4] Batch 3529, Loss 0.3712717294692993\n","[Training Epoch 4] Batch 3530, Loss 0.37448519468307495\n","[Training Epoch 4] Batch 3531, Loss 0.3539108633995056\n","[Training Epoch 4] Batch 3532, Loss 0.3372659683227539\n","[Training Epoch 4] Batch 3533, Loss 0.3558284640312195\n","[Training Epoch 4] Batch 3534, Loss 0.38923585414886475\n","[Training Epoch 4] Batch 3535, Loss 0.3516826033592224\n","[Training Epoch 4] Batch 3536, Loss 0.3553871512413025\n","[Training Epoch 4] Batch 3537, Loss 0.35345199704170227\n","[Training Epoch 4] Batch 3538, Loss 0.38623860478401184\n","[Training Epoch 4] Batch 3539, Loss 0.3314400017261505\n","[Training Epoch 4] Batch 3540, Loss 0.34265196323394775\n","[Training Epoch 4] Batch 3541, Loss 0.35286980867385864\n","[Training Epoch 4] Batch 3542, Loss 0.3824564516544342\n","[Training Epoch 4] Batch 3543, Loss 0.3351025879383087\n","[Training Epoch 4] Batch 3544, Loss 0.3557456135749817\n","[Training Epoch 4] Batch 3545, Loss 0.3520071804523468\n","[Training Epoch 4] Batch 3546, Loss 0.3598182499408722\n","[Training Epoch 4] Batch 3547, Loss 0.3619232177734375\n","[Training Epoch 4] Batch 3548, Loss 0.397988498210907\n","[Training Epoch 4] Batch 3549, Loss 0.34075939655303955\n","[Training Epoch 4] Batch 3550, Loss 0.37297794222831726\n","[Training Epoch 4] Batch 3551, Loss 0.36274927854537964\n","[Training Epoch 4] Batch 3552, Loss 0.35200196504592896\n","[Training Epoch 4] Batch 3553, Loss 0.34858274459838867\n","[Training Epoch 4] Batch 3554, Loss 0.36629006266593933\n","[Training Epoch 4] Batch 3555, Loss 0.36157387495040894\n","[Training Epoch 4] Batch 3556, Loss 0.3613799512386322\n","[Training Epoch 4] Batch 3557, Loss 0.3621811866760254\n","[Training Epoch 4] Batch 3558, Loss 0.35463064908981323\n","[Training Epoch 4] Batch 3559, Loss 0.3612009882926941\n","[Training Epoch 4] Batch 3560, Loss 0.35308030247688293\n","[Training Epoch 4] Batch 3561, Loss 0.3543544411659241\n","[Training Epoch 4] Batch 3562, Loss 0.36699575185775757\n","[Training Epoch 4] Batch 3563, Loss 0.3785492181777954\n","[Training Epoch 4] Batch 3564, Loss 0.3770748972892761\n","[Training Epoch 4] Batch 3565, Loss 0.34677156805992126\n","[Training Epoch 4] Batch 3566, Loss 0.3767157793045044\n","[Training Epoch 4] Batch 3567, Loss 0.36142241954803467\n","[Training Epoch 4] Batch 3568, Loss 0.3694332242012024\n","[Training Epoch 4] Batch 3569, Loss 0.3365628719329834\n","[Training Epoch 4] Batch 3570, Loss 0.36917245388031006\n","[Training Epoch 4] Batch 3571, Loss 0.36797595024108887\n","[Training Epoch 4] Batch 3572, Loss 0.34577444195747375\n","[Training Epoch 4] Batch 3573, Loss 0.3528210520744324\n","[Training Epoch 4] Batch 3574, Loss 0.3596521019935608\n","[Training Epoch 4] Batch 3575, Loss 0.34488949179649353\n","[Training Epoch 4] Batch 3576, Loss 0.39336416125297546\n","[Training Epoch 4] Batch 3577, Loss 0.3922356367111206\n","[Training Epoch 4] Batch 3578, Loss 0.3755403161048889\n","[Training Epoch 4] Batch 3579, Loss 0.3444805145263672\n","[Training Epoch 4] Batch 3580, Loss 0.3847336769104004\n","[Training Epoch 4] Batch 3581, Loss 0.3719138503074646\n","[Training Epoch 4] Batch 3582, Loss 0.3559153378009796\n","[Training Epoch 4] Batch 3583, Loss 0.38095417618751526\n","[Training Epoch 4] Batch 3584, Loss 0.3807554543018341\n","[Training Epoch 4] Batch 3585, Loss 0.3809407353401184\n","[Training Epoch 4] Batch 3586, Loss 0.35594746470451355\n","[Training Epoch 4] Batch 3587, Loss 0.36788541078567505\n","[Training Epoch 4] Batch 3588, Loss 0.3780885636806488\n","[Training Epoch 4] Batch 3589, Loss 0.3674471974372864\n","[Training Epoch 4] Batch 3590, Loss 0.37141209840774536\n","[Training Epoch 4] Batch 3591, Loss 0.36681267619132996\n","[Training Epoch 4] Batch 3592, Loss 0.34709104895591736\n","[Training Epoch 4] Batch 3593, Loss 0.3417391777038574\n","[Training Epoch 4] Batch 3594, Loss 0.3862585723400116\n","[Training Epoch 4] Batch 3595, Loss 0.3396261930465698\n","[Training Epoch 4] Batch 3596, Loss 0.3390476405620575\n","[Training Epoch 4] Batch 3597, Loss 0.3875012993812561\n","[Training Epoch 4] Batch 3598, Loss 0.3734503984451294\n","[Training Epoch 4] Batch 3599, Loss 0.34471699595451355\n","[Training Epoch 4] Batch 3600, Loss 0.3657045066356659\n","[Training Epoch 4] Batch 3601, Loss 0.35197287797927856\n","[Training Epoch 4] Batch 3602, Loss 0.3831307590007782\n","[Training Epoch 4] Batch 3603, Loss 0.40077006816864014\n","[Training Epoch 4] Batch 3604, Loss 0.3529345393180847\n","[Training Epoch 4] Batch 3605, Loss 0.4057636559009552\n","[Training Epoch 4] Batch 3606, Loss 0.3599361181259155\n","[Training Epoch 4] Batch 3607, Loss 0.36991292238235474\n","[Training Epoch 4] Batch 3608, Loss 0.4106070101261139\n","[Training Epoch 4] Batch 3609, Loss 0.38995426893234253\n","[Training Epoch 4] Batch 3610, Loss 0.36825114488601685\n","[Training Epoch 4] Batch 3611, Loss 0.37212520837783813\n","[Training Epoch 4] Batch 3612, Loss 0.32069915533065796\n","[Training Epoch 4] Batch 3613, Loss 0.36642420291900635\n","[Training Epoch 4] Batch 3614, Loss 0.3580576777458191\n","[Training Epoch 4] Batch 3615, Loss 0.38548290729522705\n","[Training Epoch 4] Batch 3616, Loss 0.3485022783279419\n","[Training Epoch 4] Batch 3617, Loss 0.3491963744163513\n","[Training Epoch 4] Batch 3618, Loss 0.3851643204689026\n","[Training Epoch 4] Batch 3619, Loss 0.3795708417892456\n","[Training Epoch 4] Batch 3620, Loss 0.35917744040489197\n","[Training Epoch 4] Batch 3621, Loss 0.37235140800476074\n","[Training Epoch 4] Batch 3622, Loss 0.35736656188964844\n","[Training Epoch 4] Batch 3623, Loss 0.35464945435523987\n","[Training Epoch 4] Batch 3624, Loss 0.33556485176086426\n","[Training Epoch 4] Batch 3625, Loss 0.352449506521225\n","[Training Epoch 4] Batch 3626, Loss 0.3686439096927643\n","[Training Epoch 4] Batch 3627, Loss 0.3360491096973419\n","[Training Epoch 4] Batch 3628, Loss 0.3574221730232239\n","[Training Epoch 4] Batch 3629, Loss 0.357917845249176\n","[Training Epoch 4] Batch 3630, Loss 0.3537992835044861\n","[Training Epoch 4] Batch 3631, Loss 0.37009066343307495\n","[Training Epoch 4] Batch 3632, Loss 0.3792649805545807\n","[Training Epoch 4] Batch 3633, Loss 0.3515565097332001\n","[Training Epoch 4] Batch 3634, Loss 0.39118731021881104\n","[Training Epoch 4] Batch 3635, Loss 0.3753592371940613\n","[Training Epoch 4] Batch 3636, Loss 0.39166128635406494\n","[Training Epoch 4] Batch 3637, Loss 0.3557263910770416\n","[Training Epoch 4] Batch 3638, Loss 0.37507832050323486\n","[Training Epoch 4] Batch 3639, Loss 0.3406484127044678\n","[Training Epoch 4] Batch 3640, Loss 0.36733466386795044\n","[Training Epoch 4] Batch 3641, Loss 0.3652404248714447\n","[Training Epoch 4] Batch 3642, Loss 0.3598055839538574\n","[Training Epoch 4] Batch 3643, Loss 0.33144915103912354\n","[Training Epoch 4] Batch 3644, Loss 0.3498592972755432\n","[Training Epoch 4] Batch 3645, Loss 0.3746887147426605\n","[Training Epoch 4] Batch 3646, Loss 0.34501218795776367\n","[Training Epoch 4] Batch 3647, Loss 0.3649618327617645\n","[Training Epoch 4] Batch 3648, Loss 0.36809423565864563\n","[Training Epoch 4] Batch 3649, Loss 0.3583671450614929\n","[Training Epoch 4] Batch 3650, Loss 0.33762940764427185\n","[Training Epoch 4] Batch 3651, Loss 0.3500679135322571\n","[Training Epoch 4] Batch 3652, Loss 0.35526084899902344\n","[Training Epoch 4] Batch 3653, Loss 0.3570147156715393\n","[Training Epoch 4] Batch 3654, Loss 0.35516178607940674\n","[Training Epoch 4] Batch 3655, Loss 0.3431652784347534\n","[Training Epoch 4] Batch 3656, Loss 0.352409303188324\n","[Training Epoch 4] Batch 3657, Loss 0.3723570704460144\n","[Training Epoch 4] Batch 3658, Loss 0.3799602687358856\n","[Training Epoch 4] Batch 3659, Loss 0.369104266166687\n","[Training Epoch 4] Batch 3660, Loss 0.34628432989120483\n","[Training Epoch 4] Batch 3661, Loss 0.36079326272010803\n","[Training Epoch 4] Batch 3662, Loss 0.385079026222229\n","[Training Epoch 4] Batch 3663, Loss 0.356747567653656\n","[Training Epoch 4] Batch 3664, Loss 0.36285290122032166\n","[Training Epoch 4] Batch 3665, Loss 0.3924890458583832\n","[Training Epoch 4] Batch 3666, Loss 0.360363632440567\n","[Training Epoch 4] Batch 3667, Loss 0.3921656012535095\n","[Training Epoch 4] Batch 3668, Loss 0.34875553846359253\n","[Training Epoch 4] Batch 3669, Loss 0.3520473539829254\n","[Training Epoch 4] Batch 3670, Loss 0.3584649860858917\n","[Training Epoch 4] Batch 3671, Loss 0.3668857216835022\n","[Training Epoch 4] Batch 3672, Loss 0.3735092282295227\n","[Training Epoch 4] Batch 3673, Loss 0.37039291858673096\n","[Training Epoch 4] Batch 3674, Loss 0.330791711807251\n","[Training Epoch 4] Batch 3675, Loss 0.3669770359992981\n","[Training Epoch 4] Batch 3676, Loss 0.3326044976711273\n","[Training Epoch 4] Batch 3677, Loss 0.3706631660461426\n","[Training Epoch 4] Batch 3678, Loss 0.36060282588005066\n","[Training Epoch 4] Batch 3679, Loss 0.35839587450027466\n","[Training Epoch 4] Batch 3680, Loss 0.3691110610961914\n","[Training Epoch 4] Batch 3681, Loss 0.39122575521469116\n","[Training Epoch 4] Batch 3682, Loss 0.390799343585968\n","[Training Epoch 4] Batch 3683, Loss 0.37552493810653687\n","[Training Epoch 4] Batch 3684, Loss 0.3518565893173218\n","[Training Epoch 4] Batch 3685, Loss 0.3653236925601959\n","[Training Epoch 4] Batch 3686, Loss 0.3855592608451843\n","[Training Epoch 4] Batch 3687, Loss 0.36465954780578613\n","[Training Epoch 4] Batch 3688, Loss 0.3502120077610016\n","[Training Epoch 4] Batch 3689, Loss 0.36027103662490845\n","[Training Epoch 4] Batch 3690, Loss 0.3979639410972595\n","[Training Epoch 4] Batch 3691, Loss 0.37556904554367065\n","[Training Epoch 4] Batch 3692, Loss 0.3649730086326599\n","[Training Epoch 4] Batch 3693, Loss 0.36570852994918823\n","[Training Epoch 4] Batch 3694, Loss 0.38955259323120117\n","[Training Epoch 4] Batch 3695, Loss 0.37352198362350464\n","[Training Epoch 4] Batch 3696, Loss 0.3526012897491455\n","[Training Epoch 4] Batch 3697, Loss 0.37208282947540283\n","[Training Epoch 4] Batch 3698, Loss 0.3837403655052185\n","[Training Epoch 4] Batch 3699, Loss 0.35352981090545654\n","[Training Epoch 4] Batch 3700, Loss 0.3536803722381592\n","[Training Epoch 4] Batch 3701, Loss 0.3820437788963318\n","[Training Epoch 4] Batch 3702, Loss 0.39622053503990173\n","[Training Epoch 4] Batch 3703, Loss 0.3677729368209839\n","[Training Epoch 4] Batch 3704, Loss 0.3843283951282501\n","[Training Epoch 4] Batch 3705, Loss 0.37543001770973206\n","[Training Epoch 4] Batch 3706, Loss 0.38107264041900635\n","[Training Epoch 4] Batch 3707, Loss 0.3969912528991699\n","[Training Epoch 4] Batch 3708, Loss 0.33428555727005005\n","[Training Epoch 4] Batch 3709, Loss 0.36328259110450745\n","[Training Epoch 4] Batch 3710, Loss 0.37119272351264954\n","[Training Epoch 4] Batch 3711, Loss 0.3633278012275696\n","[Training Epoch 4] Batch 3712, Loss 0.332825243473053\n","[Training Epoch 4] Batch 3713, Loss 0.39227810502052307\n","[Training Epoch 4] Batch 3714, Loss 0.37842199206352234\n","[Training Epoch 4] Batch 3715, Loss 0.3631931245326996\n","[Training Epoch 4] Batch 3716, Loss 0.3951723575592041\n","[Training Epoch 4] Batch 3717, Loss 0.3491630554199219\n","[Training Epoch 4] Batch 3718, Loss 0.34519898891448975\n","[Training Epoch 4] Batch 3719, Loss 0.37023839354515076\n","[Training Epoch 4] Batch 3720, Loss 0.34508419036865234\n","[Training Epoch 4] Batch 3721, Loss 0.36495521664619446\n","[Training Epoch 4] Batch 3722, Loss 0.3465234041213989\n","[Training Epoch 4] Batch 3723, Loss 0.3559733033180237\n","[Training Epoch 4] Batch 3724, Loss 0.3540686368942261\n","[Training Epoch 4] Batch 3725, Loss 0.3786787986755371\n","[Training Epoch 4] Batch 3726, Loss 0.41587430238723755\n","[Training Epoch 4] Batch 3727, Loss 0.36185652017593384\n","[Training Epoch 4] Batch 3728, Loss 0.3355967104434967\n","[Training Epoch 4] Batch 3729, Loss 0.37008658051490784\n","[Training Epoch 4] Batch 3730, Loss 0.3685265779495239\n","[Training Epoch 4] Batch 3731, Loss 0.3580329418182373\n","[Training Epoch 4] Batch 3732, Loss 0.39874857664108276\n","[Training Epoch 4] Batch 3733, Loss 0.35695818066596985\n","[Training Epoch 4] Batch 3734, Loss 0.38690322637557983\n","[Training Epoch 4] Batch 3735, Loss 0.3317010700702667\n","[Training Epoch 4] Batch 3736, Loss 0.3689011335372925\n","[Training Epoch 4] Batch 3737, Loss 0.3240492343902588\n","[Training Epoch 4] Batch 3738, Loss 0.35800397396087646\n","[Training Epoch 4] Batch 3739, Loss 0.3510527014732361\n","[Training Epoch 4] Batch 3740, Loss 0.38344502449035645\n","[Training Epoch 4] Batch 3741, Loss 0.36283180117607117\n","[Training Epoch 4] Batch 3742, Loss 0.3656735420227051\n","[Training Epoch 4] Batch 3743, Loss 0.3501221537590027\n","[Training Epoch 4] Batch 3744, Loss 0.35273391008377075\n","[Training Epoch 4] Batch 3745, Loss 0.3799997568130493\n","[Training Epoch 4] Batch 3746, Loss 0.36781325936317444\n","[Training Epoch 4] Batch 3747, Loss 0.3670315146446228\n","[Training Epoch 4] Batch 3748, Loss 0.36658525466918945\n","[Training Epoch 4] Batch 3749, Loss 0.34149283170700073\n","[Training Epoch 4] Batch 3750, Loss 0.4000234603881836\n","[Training Epoch 4] Batch 3751, Loss 0.36311161518096924\n","[Training Epoch 4] Batch 3752, Loss 0.35889166593551636\n","[Training Epoch 4] Batch 3753, Loss 0.36597907543182373\n","[Training Epoch 4] Batch 3754, Loss 0.37016090750694275\n","[Training Epoch 4] Batch 3755, Loss 0.34814172983169556\n","[Training Epoch 4] Batch 3756, Loss 0.37954095005989075\n","[Training Epoch 4] Batch 3757, Loss 0.3628893494606018\n","[Training Epoch 4] Batch 3758, Loss 0.3727402985095978\n","[Training Epoch 4] Batch 3759, Loss 0.367400586605072\n","[Training Epoch 4] Batch 3760, Loss 0.34295350313186646\n","[Training Epoch 4] Batch 3761, Loss 0.366504967212677\n","[Training Epoch 4] Batch 3762, Loss 0.3612833321094513\n","[Training Epoch 4] Batch 3763, Loss 0.3739223778247833\n","[Training Epoch 4] Batch 3764, Loss 0.3559020757675171\n","[Training Epoch 4] Batch 3765, Loss 0.36122697591781616\n","[Training Epoch 4] Batch 3766, Loss 0.3267708420753479\n","[Training Epoch 4] Batch 3767, Loss 0.3874359130859375\n","[Training Epoch 4] Batch 3768, Loss 0.35484588146209717\n","[Training Epoch 4] Batch 3769, Loss 0.37752580642700195\n","[Training Epoch 4] Batch 3770, Loss 0.33824753761291504\n","[Training Epoch 4] Batch 3771, Loss 0.3675089478492737\n","[Training Epoch 4] Batch 3772, Loss 0.35154253244400024\n","[Training Epoch 4] Batch 3773, Loss 0.35565856099128723\n","[Training Epoch 4] Batch 3774, Loss 0.3515062928199768\n","[Training Epoch 4] Batch 3775, Loss 0.34825509786605835\n","[Training Epoch 4] Batch 3776, Loss 0.3537118434906006\n","[Training Epoch 4] Batch 3777, Loss 0.3662991523742676\n","[Training Epoch 4] Batch 3778, Loss 0.3273516595363617\n","[Training Epoch 4] Batch 3779, Loss 0.3390881419181824\n","[Training Epoch 4] Batch 3780, Loss 0.39457130432128906\n","[Training Epoch 4] Batch 3781, Loss 0.3885045051574707\n","[Training Epoch 4] Batch 3782, Loss 0.36485347151756287\n","[Training Epoch 4] Batch 3783, Loss 0.3685046434402466\n","[Training Epoch 4] Batch 3784, Loss 0.3624556064605713\n","[Training Epoch 4] Batch 3785, Loss 0.3643572926521301\n","[Training Epoch 4] Batch 3786, Loss 0.364834189414978\n","[Training Epoch 4] Batch 3787, Loss 0.33961552381515503\n","[Training Epoch 4] Batch 3788, Loss 0.36253148317337036\n","[Training Epoch 4] Batch 3789, Loss 0.3375258147716522\n","[Training Epoch 4] Batch 3790, Loss 0.3511226177215576\n","[Training Epoch 4] Batch 3791, Loss 0.3753099739551544\n","[Training Epoch 4] Batch 3792, Loss 0.3897271752357483\n","[Training Epoch 4] Batch 3793, Loss 0.3717460632324219\n","[Training Epoch 4] Batch 3794, Loss 0.35488519072532654\n","[Training Epoch 4] Batch 3795, Loss 0.35792937874794006\n","[Training Epoch 4] Batch 3796, Loss 0.36691153049468994\n","[Training Epoch 4] Batch 3797, Loss 0.3605722188949585\n","[Training Epoch 4] Batch 3798, Loss 0.37888363003730774\n","[Training Epoch 4] Batch 3799, Loss 0.35216885805130005\n","[Training Epoch 4] Batch 3800, Loss 0.3616710603237152\n","[Training Epoch 4] Batch 3801, Loss 0.36172449588775635\n","[Training Epoch 4] Batch 3802, Loss 0.3641628623008728\n","[Training Epoch 4] Batch 3803, Loss 0.37563565373420715\n","[Training Epoch 4] Batch 3804, Loss 0.3557206988334656\n","[Training Epoch 4] Batch 3805, Loss 0.33566370606422424\n","[Training Epoch 4] Batch 3806, Loss 0.38467419147491455\n","[Training Epoch 4] Batch 3807, Loss 0.34221774339675903\n","[Training Epoch 4] Batch 3808, Loss 0.35621392726898193\n","[Training Epoch 4] Batch 3809, Loss 0.39142921566963196\n","[Training Epoch 4] Batch 3810, Loss 0.361514687538147\n","[Training Epoch 4] Batch 3811, Loss 0.37784847617149353\n","[Training Epoch 4] Batch 3812, Loss 0.35727250576019287\n","[Training Epoch 4] Batch 3813, Loss 0.32714366912841797\n","[Training Epoch 4] Batch 3814, Loss 0.3821641206741333\n","[Training Epoch 4] Batch 3815, Loss 0.35017603635787964\n","[Training Epoch 4] Batch 3816, Loss 0.3645457625389099\n","[Training Epoch 4] Batch 3817, Loss 0.38260573148727417\n","[Training Epoch 4] Batch 3818, Loss 0.3697598874568939\n","[Training Epoch 4] Batch 3819, Loss 0.3833397626876831\n","[Training Epoch 4] Batch 3820, Loss 0.35109513998031616\n","[Training Epoch 4] Batch 3821, Loss 0.3704604208469391\n","[Training Epoch 4] Batch 3822, Loss 0.3535264730453491\n","[Training Epoch 4] Batch 3823, Loss 0.37078356742858887\n","[Training Epoch 4] Batch 3824, Loss 0.35334324836730957\n","[Training Epoch 4] Batch 3825, Loss 0.3700293302536011\n","[Training Epoch 4] Batch 3826, Loss 0.3492336869239807\n","[Training Epoch 4] Batch 3827, Loss 0.34190401434898376\n","[Training Epoch 4] Batch 3828, Loss 0.3595433831214905\n","[Training Epoch 4] Batch 3829, Loss 0.35644108057022095\n","[Training Epoch 4] Batch 3830, Loss 0.4045250415802002\n","[Training Epoch 4] Batch 3831, Loss 0.36172497272491455\n","[Training Epoch 4] Batch 3832, Loss 0.39168447256088257\n","[Training Epoch 4] Batch 3833, Loss 0.3407754898071289\n","[Training Epoch 4] Batch 3834, Loss 0.37400609254837036\n","[Training Epoch 4] Batch 3835, Loss 0.34610921144485474\n","[Training Epoch 4] Batch 3836, Loss 0.39009130001068115\n","[Training Epoch 4] Batch 3837, Loss 0.3575543165206909\n","[Training Epoch 4] Batch 3838, Loss 0.33722519874572754\n","[Training Epoch 4] Batch 3839, Loss 0.36423105001449585\n","[Training Epoch 4] Batch 3840, Loss 0.35422366857528687\n","[Training Epoch 4] Batch 3841, Loss 0.39621448516845703\n","[Training Epoch 4] Batch 3842, Loss 0.36159175634384155\n","[Training Epoch 4] Batch 3843, Loss 0.33096230030059814\n","[Training Epoch 4] Batch 3844, Loss 0.4012936055660248\n","[Training Epoch 4] Batch 3845, Loss 0.37757444381713867\n","[Training Epoch 4] Batch 3846, Loss 0.36780738830566406\n","[Training Epoch 4] Batch 3847, Loss 0.3414459228515625\n","[Training Epoch 4] Batch 3848, Loss 0.3438735902309418\n","[Training Epoch 4] Batch 3849, Loss 0.38595762848854065\n","[Training Epoch 4] Batch 3850, Loss 0.3374844193458557\n","[Training Epoch 4] Batch 3851, Loss 0.3553028106689453\n","[Training Epoch 4] Batch 3852, Loss 0.3600953221321106\n","[Training Epoch 4] Batch 3853, Loss 0.36363786458969116\n","[Training Epoch 4] Batch 3854, Loss 0.32334402203559875\n","[Training Epoch 4] Batch 3855, Loss 0.3560713827610016\n","[Training Epoch 4] Batch 3856, Loss 0.40402454137802124\n","[Training Epoch 4] Batch 3857, Loss 0.36612457036972046\n","[Training Epoch 4] Batch 3858, Loss 0.3285728693008423\n","[Training Epoch 4] Batch 3859, Loss 0.37855762243270874\n","[Training Epoch 4] Batch 3860, Loss 0.3505532741546631\n","[Training Epoch 4] Batch 3861, Loss 0.361928790807724\n","[Training Epoch 4] Batch 3862, Loss 0.34673529863357544\n","[Training Epoch 4] Batch 3863, Loss 0.3683389723300934\n","[Training Epoch 4] Batch 3864, Loss 0.38459306955337524\n","[Training Epoch 4] Batch 3865, Loss 0.3693296015262604\n","[Training Epoch 4] Batch 3866, Loss 0.37963002920150757\n","[Training Epoch 4] Batch 3867, Loss 0.3716282844543457\n","[Training Epoch 4] Batch 3868, Loss 0.3449214696884155\n","[Training Epoch 4] Batch 3869, Loss 0.3528549075126648\n","[Training Epoch 4] Batch 3870, Loss 0.3785286843776703\n","[Training Epoch 4] Batch 3871, Loss 0.35020917654037476\n","[Training Epoch 4] Batch 3872, Loss 0.35405635833740234\n","[Training Epoch 4] Batch 3873, Loss 0.3793333172798157\n","[Training Epoch 4] Batch 3874, Loss 0.33303260803222656\n","[Training Epoch 4] Batch 3875, Loss 0.33626705408096313\n","[Training Epoch 4] Batch 3876, Loss 0.3777172863483429\n","[Training Epoch 4] Batch 3877, Loss 0.37357059121131897\n","[Training Epoch 4] Batch 3878, Loss 0.34347301721572876\n","[Training Epoch 4] Batch 3879, Loss 0.3766416311264038\n","[Training Epoch 4] Batch 3880, Loss 0.36537930369377136\n","[Training Epoch 4] Batch 3881, Loss 0.3656044006347656\n","[Training Epoch 4] Batch 3882, Loss 0.35817036032676697\n","[Training Epoch 4] Batch 3883, Loss 0.3782604932785034\n","[Training Epoch 4] Batch 3884, Loss 0.35012286901474\n","[Training Epoch 4] Batch 3885, Loss 0.3531632423400879\n","[Training Epoch 4] Batch 3886, Loss 0.36956384778022766\n","[Training Epoch 4] Batch 3887, Loss 0.36634886264801025\n","[Training Epoch 4] Batch 3888, Loss 0.3700143098831177\n","[Training Epoch 4] Batch 3889, Loss 0.38617637753486633\n","[Training Epoch 4] Batch 3890, Loss 0.3669449985027313\n","[Training Epoch 4] Batch 3891, Loss 0.3822208344936371\n","[Training Epoch 4] Batch 3892, Loss 0.3534058928489685\n","[Training Epoch 4] Batch 3893, Loss 0.3770325779914856\n","[Training Epoch 4] Batch 3894, Loss 0.3576053977012634\n","[Training Epoch 4] Batch 3895, Loss 0.38009437918663025\n","[Training Epoch 4] Batch 3896, Loss 0.37199604511260986\n","[Training Epoch 4] Batch 3897, Loss 0.35737571120262146\n","[Training Epoch 4] Batch 3898, Loss 0.34705889225006104\n","[Training Epoch 4] Batch 3899, Loss 0.31020990014076233\n","[Training Epoch 4] Batch 3900, Loss 0.4023994505405426\n","[Training Epoch 4] Batch 3901, Loss 0.36742284893989563\n","[Training Epoch 4] Batch 3902, Loss 0.3414114713668823\n","[Training Epoch 4] Batch 3903, Loss 0.3356257677078247\n","[Training Epoch 4] Batch 3904, Loss 0.37287381291389465\n","[Training Epoch 4] Batch 3905, Loss 0.3973056674003601\n","[Training Epoch 4] Batch 3906, Loss 0.3916589617729187\n","[Training Epoch 4] Batch 3907, Loss 0.33793509006500244\n","[Training Epoch 4] Batch 3908, Loss 0.33000242710113525\n","[Training Epoch 4] Batch 3909, Loss 0.3251025080680847\n","[Training Epoch 4] Batch 3910, Loss 0.3558965027332306\n","[Training Epoch 4] Batch 3911, Loss 0.3295714855194092\n","[Training Epoch 4] Batch 3912, Loss 0.35874906182289124\n","[Training Epoch 4] Batch 3913, Loss 0.3829319477081299\n","[Training Epoch 4] Batch 3914, Loss 0.355685293674469\n","[Training Epoch 4] Batch 3915, Loss 0.35172805190086365\n","[Training Epoch 4] Batch 3916, Loss 0.3818414509296417\n","[Training Epoch 4] Batch 3917, Loss 0.37938356399536133\n","[Training Epoch 4] Batch 3918, Loss 0.37701416015625\n","[Training Epoch 4] Batch 3919, Loss 0.3362153172492981\n","[Training Epoch 4] Batch 3920, Loss 0.38272157311439514\n","[Training Epoch 4] Batch 3921, Loss 0.3392488658428192\n","[Training Epoch 4] Batch 3922, Loss 0.3425626754760742\n","[Training Epoch 4] Batch 3923, Loss 0.36428141593933105\n","[Training Epoch 4] Batch 3924, Loss 0.3542020320892334\n","[Training Epoch 4] Batch 3925, Loss 0.3872888684272766\n","[Training Epoch 4] Batch 3926, Loss 0.34468305110931396\n","[Training Epoch 4] Batch 3927, Loss 0.35850030183792114\n","[Training Epoch 4] Batch 3928, Loss 0.38390105962753296\n","[Training Epoch 4] Batch 3929, Loss 0.34657323360443115\n","[Training Epoch 4] Batch 3930, Loss 0.3627047538757324\n","[Training Epoch 4] Batch 3931, Loss 0.3820383548736572\n","[Training Epoch 4] Batch 3932, Loss 0.35944774746894836\n","[Training Epoch 4] Batch 3933, Loss 0.3676658272743225\n","[Training Epoch 4] Batch 3934, Loss 0.3674412965774536\n","[Training Epoch 4] Batch 3935, Loss 0.33104804158210754\n","[Training Epoch 4] Batch 3936, Loss 0.3295690715312958\n","[Training Epoch 4] Batch 3937, Loss 0.3885929584503174\n","[Training Epoch 4] Batch 3938, Loss 0.3774096667766571\n","[Training Epoch 4] Batch 3939, Loss 0.3638342022895813\n","[Training Epoch 4] Batch 3940, Loss 0.3793681561946869\n","[Training Epoch 4] Batch 3941, Loss 0.3829237222671509\n","[Training Epoch 4] Batch 3942, Loss 0.3666548728942871\n","[Training Epoch 4] Batch 3943, Loss 0.3689269721508026\n","[Training Epoch 4] Batch 3944, Loss 0.36281818151474\n","[Training Epoch 4] Batch 3945, Loss 0.3470373749732971\n","[Training Epoch 4] Batch 3946, Loss 0.3584762513637543\n","[Training Epoch 4] Batch 3947, Loss 0.3587430417537689\n","[Training Epoch 4] Batch 3948, Loss 0.3642374277114868\n","[Training Epoch 4] Batch 3949, Loss 0.3929896056652069\n","[Training Epoch 4] Batch 3950, Loss 0.3810305595397949\n","[Training Epoch 4] Batch 3951, Loss 0.33380192518234253\n","[Training Epoch 4] Batch 3952, Loss 0.36041104793548584\n","[Training Epoch 4] Batch 3953, Loss 0.3685341477394104\n","[Training Epoch 4] Batch 3954, Loss 0.3389844596385956\n","[Training Epoch 4] Batch 3955, Loss 0.3523171544075012\n","[Training Epoch 4] Batch 3956, Loss 0.36193832755088806\n","[Training Epoch 4] Batch 3957, Loss 0.3754284977912903\n","[Training Epoch 4] Batch 3958, Loss 0.3604997396469116\n","[Training Epoch 4] Batch 3959, Loss 0.36679506301879883\n","[Training Epoch 4] Batch 3960, Loss 0.34333452582359314\n","[Training Epoch 4] Batch 3961, Loss 0.3614097833633423\n","[Training Epoch 4] Batch 3962, Loss 0.37842026352882385\n","[Training Epoch 4] Batch 3963, Loss 0.36987489461898804\n","[Training Epoch 4] Batch 3964, Loss 0.37329113483428955\n","[Training Epoch 4] Batch 3965, Loss 0.3674715757369995\n","[Training Epoch 4] Batch 3966, Loss 0.3320576548576355\n","[Training Epoch 4] Batch 3967, Loss 0.37503573298454285\n","[Training Epoch 4] Batch 3968, Loss 0.3595319092273712\n","[Training Epoch 4] Batch 3969, Loss 0.382549524307251\n","[Training Epoch 4] Batch 3970, Loss 0.34017857909202576\n","[Training Epoch 4] Batch 3971, Loss 0.35254085063934326\n","[Training Epoch 4] Batch 3972, Loss 0.3520643413066864\n","[Training Epoch 4] Batch 3973, Loss 0.35041773319244385\n","[Training Epoch 4] Batch 3974, Loss 0.3249650299549103\n","[Training Epoch 4] Batch 3975, Loss 0.35705241560935974\n","[Training Epoch 4] Batch 3976, Loss 0.408349871635437\n","[Training Epoch 4] Batch 3977, Loss 0.3598867356777191\n","[Training Epoch 4] Batch 3978, Loss 0.3587605357170105\n","[Training Epoch 4] Batch 3979, Loss 0.33719193935394287\n","[Training Epoch 4] Batch 3980, Loss 0.3701285123825073\n","[Training Epoch 4] Batch 3981, Loss 0.35547465085983276\n","[Training Epoch 4] Batch 3982, Loss 0.336534321308136\n","[Training Epoch 4] Batch 3983, Loss 0.35662952065467834\n","[Training Epoch 4] Batch 3984, Loss 0.3562096655368805\n","[Training Epoch 4] Batch 3985, Loss 0.37849780917167664\n","[Training Epoch 4] Batch 3986, Loss 0.36835184693336487\n","[Training Epoch 4] Batch 3987, Loss 0.33986473083496094\n","[Training Epoch 4] Batch 3988, Loss 0.37216871976852417\n","[Training Epoch 4] Batch 3989, Loss 0.3667774200439453\n","[Training Epoch 4] Batch 3990, Loss 0.35721248388290405\n","[Training Epoch 4] Batch 3991, Loss 0.38315045833587646\n","[Training Epoch 4] Batch 3992, Loss 0.37130528688430786\n","[Training Epoch 4] Batch 3993, Loss 0.38230621814727783\n","[Training Epoch 4] Batch 3994, Loss 0.3352174162864685\n","[Training Epoch 4] Batch 3995, Loss 0.3448632061481476\n","[Training Epoch 4] Batch 3996, Loss 0.35593652725219727\n","[Training Epoch 4] Batch 3997, Loss 0.3549344539642334\n","[Training Epoch 4] Batch 3998, Loss 0.3712167739868164\n","[Training Epoch 4] Batch 3999, Loss 0.35631632804870605\n","[Training Epoch 4] Batch 4000, Loss 0.3476641774177551\n","[Training Epoch 4] Batch 4001, Loss 0.35970914363861084\n","[Training Epoch 4] Batch 4002, Loss 0.38647323846817017\n","[Training Epoch 4] Batch 4003, Loss 0.38084515929222107\n","[Training Epoch 4] Batch 4004, Loss 0.3697461783885956\n","[Training Epoch 4] Batch 4005, Loss 0.3956729769706726\n","[Training Epoch 4] Batch 4006, Loss 0.3743444085121155\n","[Training Epoch 4] Batch 4007, Loss 0.3636336028575897\n","[Training Epoch 4] Batch 4008, Loss 0.3438538610935211\n","[Training Epoch 4] Batch 4009, Loss 0.3578876852989197\n","[Training Epoch 4] Batch 4010, Loss 0.3683336675167084\n","[Training Epoch 4] Batch 4011, Loss 0.3591216802597046\n","[Training Epoch 4] Batch 4012, Loss 0.34516412019729614\n","[Training Epoch 4] Batch 4013, Loss 0.36417198181152344\n","[Training Epoch 4] Batch 4014, Loss 0.35417866706848145\n","[Training Epoch 4] Batch 4015, Loss 0.3762242794036865\n","[Training Epoch 4] Batch 4016, Loss 0.3702353239059448\n","[Training Epoch 4] Batch 4017, Loss 0.3823694586753845\n","[Training Epoch 4] Batch 4018, Loss 0.3657343089580536\n","[Training Epoch 4] Batch 4019, Loss 0.3691754937171936\n","[Training Epoch 4] Batch 4020, Loss 0.34328338503837585\n","[Training Epoch 4] Batch 4021, Loss 0.37514540553092957\n","[Training Epoch 4] Batch 4022, Loss 0.33460813760757446\n","[Training Epoch 4] Batch 4023, Loss 0.38207462430000305\n","[Training Epoch 4] Batch 4024, Loss 0.34728097915649414\n","[Training Epoch 4] Batch 4025, Loss 0.35938912630081177\n","[Training Epoch 4] Batch 4026, Loss 0.34723222255706787\n","[Training Epoch 4] Batch 4027, Loss 0.3782467246055603\n","[Training Epoch 4] Batch 4028, Loss 0.34144172072410583\n","[Training Epoch 4] Batch 4029, Loss 0.37621039152145386\n","[Training Epoch 4] Batch 4030, Loss 0.3846275806427002\n","[Training Epoch 4] Batch 4031, Loss 0.35100477933883667\n","[Training Epoch 4] Batch 4032, Loss 0.37865495681762695\n","[Training Epoch 4] Batch 4033, Loss 0.3694263994693756\n","[Training Epoch 4] Batch 4034, Loss 0.34792959690093994\n","[Training Epoch 4] Batch 4035, Loss 0.34498172998428345\n","[Training Epoch 4] Batch 4036, Loss 0.35290879011154175\n","[Training Epoch 4] Batch 4037, Loss 0.33419913053512573\n","[Training Epoch 4] Batch 4038, Loss 0.32237493991851807\n","[Training Epoch 4] Batch 4039, Loss 0.3855961561203003\n","[Training Epoch 4] Batch 4040, Loss 0.3502470552921295\n","[Training Epoch 4] Batch 4041, Loss 0.36405259370803833\n","[Training Epoch 4] Batch 4042, Loss 0.33938854932785034\n","[Training Epoch 4] Batch 4043, Loss 0.3557313084602356\n","[Training Epoch 4] Batch 4044, Loss 0.3467967212200165\n","[Training Epoch 4] Batch 4045, Loss 0.3534266948699951\n","[Training Epoch 4] Batch 4046, Loss 0.36506280303001404\n","[Training Epoch 4] Batch 4047, Loss 0.40656036138534546\n","[Training Epoch 4] Batch 4048, Loss 0.32157278060913086\n","[Training Epoch 4] Batch 4049, Loss 0.36280110478401184\n","[Training Epoch 4] Batch 4050, Loss 0.36346137523651123\n","[Training Epoch 4] Batch 4051, Loss 0.35718196630477905\n","[Training Epoch 4] Batch 4052, Loss 0.3540310859680176\n","[Training Epoch 4] Batch 4053, Loss 0.34798452258110046\n","[Training Epoch 4] Batch 4054, Loss 0.34756147861480713\n","[Training Epoch 4] Batch 4055, Loss 0.33802223205566406\n","[Training Epoch 4] Batch 4056, Loss 0.35232192277908325\n","[Training Epoch 4] Batch 4057, Loss 0.3214147090911865\n","[Training Epoch 4] Batch 4058, Loss 0.35927921533584595\n","[Training Epoch 4] Batch 4059, Loss 0.36504286527633667\n","[Training Epoch 4] Batch 4060, Loss 0.37399452924728394\n","[Training Epoch 4] Batch 4061, Loss 0.36128947138786316\n","[Training Epoch 4] Batch 4062, Loss 0.36134418845176697\n","[Training Epoch 4] Batch 4063, Loss 0.3736029863357544\n","[Training Epoch 4] Batch 4064, Loss 0.38611581921577454\n","[Training Epoch 4] Batch 4065, Loss 0.34510111808776855\n","[Training Epoch 4] Batch 4066, Loss 0.3601537346839905\n","[Training Epoch 4] Batch 4067, Loss 0.35852938890457153\n","[Training Epoch 4] Batch 4068, Loss 0.39129766821861267\n","[Training Epoch 4] Batch 4069, Loss 0.3660012483596802\n","[Training Epoch 4] Batch 4070, Loss 0.3632311522960663\n","[Training Epoch 4] Batch 4071, Loss 0.3631245493888855\n","[Training Epoch 4] Batch 4072, Loss 0.366189181804657\n","[Training Epoch 4] Batch 4073, Loss 0.35759294033050537\n","[Training Epoch 4] Batch 4074, Loss 0.373945415019989\n","[Training Epoch 4] Batch 4075, Loss 0.3557935357093811\n","[Training Epoch 4] Batch 4076, Loss 0.3484916388988495\n","[Training Epoch 4] Batch 4077, Loss 0.3438020348548889\n","[Training Epoch 4] Batch 4078, Loss 0.33721262216567993\n","[Training Epoch 4] Batch 4079, Loss 0.3702579438686371\n","[Training Epoch 4] Batch 4080, Loss 0.3858628273010254\n","[Training Epoch 4] Batch 4081, Loss 0.36531758308410645\n","[Training Epoch 4] Batch 4082, Loss 0.3519914746284485\n","[Training Epoch 4] Batch 4083, Loss 0.3680420517921448\n","[Training Epoch 4] Batch 4084, Loss 0.3603932857513428\n","[Training Epoch 4] Batch 4085, Loss 0.3504488468170166\n","[Training Epoch 4] Batch 4086, Loss 0.3459743857383728\n","[Training Epoch 4] Batch 4087, Loss 0.39388686418533325\n","[Training Epoch 4] Batch 4088, Loss 0.37919357419013977\n","[Training Epoch 4] Batch 4089, Loss 0.3527664840221405\n","[Training Epoch 4] Batch 4090, Loss 0.35140517354011536\n","[Training Epoch 4] Batch 4091, Loss 0.34452876448631287\n","[Training Epoch 4] Batch 4092, Loss 0.3589760959148407\n","[Training Epoch 4] Batch 4093, Loss 0.3537832200527191\n","[Training Epoch 4] Batch 4094, Loss 0.348335325717926\n","[Training Epoch 4] Batch 4095, Loss 0.3723292946815491\n","[Training Epoch 4] Batch 4096, Loss 0.37898892164230347\n","[Training Epoch 4] Batch 4097, Loss 0.3683294653892517\n","[Training Epoch 4] Batch 4098, Loss 0.34182509779930115\n","[Training Epoch 4] Batch 4099, Loss 0.36983388662338257\n","[Training Epoch 4] Batch 4100, Loss 0.4047057032585144\n","[Training Epoch 4] Batch 4101, Loss 0.3669106960296631\n","[Training Epoch 4] Batch 4102, Loss 0.37375015020370483\n","[Training Epoch 4] Batch 4103, Loss 0.3625299632549286\n","[Training Epoch 4] Batch 4104, Loss 0.38993895053863525\n","[Training Epoch 4] Batch 4105, Loss 0.3807646930217743\n","[Training Epoch 4] Batch 4106, Loss 0.33004021644592285\n","[Training Epoch 4] Batch 4107, Loss 0.3449864983558655\n","[Training Epoch 4] Batch 4108, Loss 0.3692631423473358\n","[Training Epoch 4] Batch 4109, Loss 0.3891896605491638\n","[Training Epoch 4] Batch 4110, Loss 0.36714833974838257\n","[Training Epoch 4] Batch 4111, Loss 0.3790881931781769\n","[Training Epoch 4] Batch 4112, Loss 0.3720467686653137\n","[Training Epoch 4] Batch 4113, Loss 0.3616458475589752\n","[Training Epoch 4] Batch 4114, Loss 0.34844771027565\n","[Training Epoch 4] Batch 4115, Loss 0.3556250035762787\n","[Training Epoch 4] Batch 4116, Loss 0.33133333921432495\n","[Training Epoch 4] Batch 4117, Loss 0.35106223821640015\n","[Training Epoch 4] Batch 4118, Loss 0.37902265787124634\n","[Training Epoch 4] Batch 4119, Loss 0.3477966785430908\n","[Training Epoch 4] Batch 4120, Loss 0.38347160816192627\n","[Training Epoch 4] Batch 4121, Loss 0.3185928165912628\n","[Training Epoch 4] Batch 4122, Loss 0.36109739542007446\n","[Training Epoch 4] Batch 4123, Loss 0.3460293710231781\n","[Training Epoch 4] Batch 4124, Loss 0.35092616081237793\n","[Training Epoch 4] Batch 4125, Loss 0.37672001123428345\n","[Training Epoch 4] Batch 4126, Loss 0.35657986998558044\n","[Training Epoch 4] Batch 4127, Loss 0.3708797097206116\n","[Training Epoch 4] Batch 4128, Loss 0.36170512437820435\n","[Training Epoch 4] Batch 4129, Loss 0.3619827926158905\n","[Training Epoch 4] Batch 4130, Loss 0.36336296796798706\n","[Training Epoch 4] Batch 4131, Loss 0.388423889875412\n","[Training Epoch 4] Batch 4132, Loss 0.34944286942481995\n","[Training Epoch 4] Batch 4133, Loss 0.37483781576156616\n","[Training Epoch 4] Batch 4134, Loss 0.3571612238883972\n","[Training Epoch 4] Batch 4135, Loss 0.36119550466537476\n","[Training Epoch 4] Batch 4136, Loss 0.34098976850509644\n","[Training Epoch 4] Batch 4137, Loss 0.3655255436897278\n","[Training Epoch 4] Batch 4138, Loss 0.33183836936950684\n","[Training Epoch 4] Batch 4139, Loss 0.3541649281978607\n","[Training Epoch 4] Batch 4140, Loss 0.3541291356086731\n","[Training Epoch 4] Batch 4141, Loss 0.34325462579727173\n","[Training Epoch 4] Batch 4142, Loss 0.37193864583969116\n","[Training Epoch 4] Batch 4143, Loss 0.3670671582221985\n","[Training Epoch 4] Batch 4144, Loss 0.3529052734375\n","[Training Epoch 4] Batch 4145, Loss 0.3599255084991455\n","[Training Epoch 4] Batch 4146, Loss 0.36180099844932556\n","[Training Epoch 4] Batch 4147, Loss 0.3918178677558899\n","[Training Epoch 4] Batch 4148, Loss 0.40288078784942627\n","[Training Epoch 4] Batch 4149, Loss 0.3685714304447174\n","[Training Epoch 4] Batch 4150, Loss 0.38182783126831055\n","[Training Epoch 4] Batch 4151, Loss 0.3520171344280243\n","[Training Epoch 4] Batch 4152, Loss 0.3571258783340454\n","[Training Epoch 4] Batch 4153, Loss 0.40262699127197266\n","[Training Epoch 4] Batch 4154, Loss 0.35602664947509766\n","[Training Epoch 4] Batch 4155, Loss 0.35927820205688477\n","[Training Epoch 4] Batch 4156, Loss 0.3872610628604889\n","[Training Epoch 4] Batch 4157, Loss 0.40738046169281006\n","[Training Epoch 4] Batch 4158, Loss 0.37686687707901\n","[Training Epoch 4] Batch 4159, Loss 0.3673631548881531\n","[Training Epoch 4] Batch 4160, Loss 0.36598217487335205\n","[Training Epoch 4] Batch 4161, Loss 0.36261141300201416\n","[Training Epoch 4] Batch 4162, Loss 0.36660489439964294\n","[Training Epoch 4] Batch 4163, Loss 0.34807658195495605\n","[Training Epoch 4] Batch 4164, Loss 0.33609575033187866\n","[Training Epoch 4] Batch 4165, Loss 0.3726036846637726\n","[Training Epoch 4] Batch 4166, Loss 0.3649817407131195\n","[Training Epoch 4] Batch 4167, Loss 0.34997251629829407\n","[Training Epoch 4] Batch 4168, Loss 0.36113741993904114\n","[Training Epoch 4] Batch 4169, Loss 0.33412036299705505\n","[Training Epoch 4] Batch 4170, Loss 0.35043609142303467\n","[Training Epoch 4] Batch 4171, Loss 0.3701392412185669\n","[Training Epoch 4] Batch 4172, Loss 0.3601081371307373\n","[Training Epoch 4] Batch 4173, Loss 0.36035940051078796\n","[Training Epoch 4] Batch 4174, Loss 0.3703017830848694\n","[Training Epoch 4] Batch 4175, Loss 0.3587667644023895\n","[Training Epoch 4] Batch 4176, Loss 0.39143478870391846\n","[Training Epoch 4] Batch 4177, Loss 0.34829792380332947\n","[Training Epoch 4] Batch 4178, Loss 0.36512309312820435\n","[Training Epoch 4] Batch 4179, Loss 0.3842790722846985\n","[Training Epoch 4] Batch 4180, Loss 0.31227490305900574\n","[Training Epoch 4] Batch 4181, Loss 0.3332613706588745\n","[Training Epoch 4] Batch 4182, Loss 0.3590199947357178\n","[Training Epoch 4] Batch 4183, Loss 0.3427579402923584\n","[Training Epoch 4] Batch 4184, Loss 0.3512778878211975\n","[Training Epoch 4] Batch 4185, Loss 0.3833945095539093\n","[Training Epoch 4] Batch 4186, Loss 0.36312395334243774\n","[Training Epoch 4] Batch 4187, Loss 0.38291195034980774\n","[Training Epoch 4] Batch 4188, Loss 0.39127016067504883\n","[Training Epoch 4] Batch 4189, Loss 0.3804478049278259\n","[Training Epoch 4] Batch 4190, Loss 0.3804922103881836\n","[Training Epoch 4] Batch 4191, Loss 0.37923744320869446\n","[Training Epoch 4] Batch 4192, Loss 0.3349483013153076\n","[Training Epoch 4] Batch 4193, Loss 0.357164204120636\n","[Training Epoch 4] Batch 4194, Loss 0.39301788806915283\n","[Training Epoch 4] Batch 4195, Loss 0.34564459323883057\n","[Training Epoch 4] Batch 4196, Loss 0.3749822974205017\n","[Training Epoch 4] Batch 4197, Loss 0.3849332630634308\n","[Training Epoch 4] Batch 4198, Loss 0.3548692464828491\n","[Training Epoch 4] Batch 4199, Loss 0.3672826886177063\n","[Training Epoch 4] Batch 4200, Loss 0.3482741713523865\n","[Training Epoch 4] Batch 4201, Loss 0.3684864640235901\n","[Training Epoch 4] Batch 4202, Loss 0.3691890239715576\n","[Training Epoch 4] Batch 4203, Loss 0.3606410622596741\n","[Training Epoch 4] Batch 4204, Loss 0.36055514216423035\n","[Training Epoch 4] Batch 4205, Loss 0.36408787965774536\n","[Training Epoch 4] Batch 4206, Loss 0.35780757665634155\n","[Training Epoch 4] Batch 4207, Loss 0.349954754114151\n","[Training Epoch 4] Batch 4208, Loss 0.3299652934074402\n","[Training Epoch 4] Batch 4209, Loss 0.3644809424877167\n","[Training Epoch 4] Batch 4210, Loss 0.34092819690704346\n","[Training Epoch 4] Batch 4211, Loss 0.33747440576553345\n","[Training Epoch 4] Batch 4212, Loss 0.3331214189529419\n","[Training Epoch 4] Batch 4213, Loss 0.3749934434890747\n","[Training Epoch 4] Batch 4214, Loss 0.3646693229675293\n","[Training Epoch 4] Batch 4215, Loss 0.37683191895484924\n","[Training Epoch 4] Batch 4216, Loss 0.3496244251728058\n","[Training Epoch 4] Batch 4217, Loss 0.367489755153656\n","[Training Epoch 4] Batch 4218, Loss 0.3374701142311096\n","[Training Epoch 4] Batch 4219, Loss 0.3716827929019928\n","[Training Epoch 4] Batch 4220, Loss 0.37847158312797546\n","[Training Epoch 4] Batch 4221, Loss 0.36206355690956116\n","[Training Epoch 4] Batch 4222, Loss 0.3495410084724426\n","[Training Epoch 4] Batch 4223, Loss 0.3905804455280304\n","[Training Epoch 4] Batch 4224, Loss 0.3671264052391052\n","[Training Epoch 4] Batch 4225, Loss 0.36328446865081787\n","[Training Epoch 4] Batch 4226, Loss 0.36673349142074585\n","[Training Epoch 4] Batch 4227, Loss 0.3692532181739807\n","[Training Epoch 4] Batch 4228, Loss 0.34776395559310913\n","[Training Epoch 4] Batch 4229, Loss 0.373863160610199\n","[Training Epoch 4] Batch 4230, Loss 0.36093786358833313\n","[Training Epoch 4] Batch 4231, Loss 0.34997254610061646\n","[Training Epoch 4] Batch 4232, Loss 0.35271546244621277\n","[Training Epoch 4] Batch 4233, Loss 0.3546751141548157\n","[Training Epoch 4] Batch 4234, Loss 0.3754076063632965\n","[Training Epoch 4] Batch 4235, Loss 0.3667924404144287\n","[Training Epoch 4] Batch 4236, Loss 0.37055087089538574\n","[Training Epoch 4] Batch 4237, Loss 0.37781840562820435\n","[Training Epoch 4] Batch 4238, Loss 0.3471028208732605\n","[Training Epoch 4] Batch 4239, Loss 0.35027560591697693\n","[Training Epoch 4] Batch 4240, Loss 0.36675921082496643\n","[Training Epoch 4] Batch 4241, Loss 0.3705187439918518\n","[Training Epoch 4] Batch 4242, Loss 0.36727744340896606\n","[Training Epoch 4] Batch 4243, Loss 0.35544803738594055\n","[Training Epoch 4] Batch 4244, Loss 0.3489754796028137\n","[Training Epoch 4] Batch 4245, Loss 0.3308027982711792\n","[Training Epoch 4] Batch 4246, Loss 0.36393606662750244\n","[Training Epoch 4] Batch 4247, Loss 0.3524097800254822\n","[Training Epoch 4] Batch 4248, Loss 0.36469966173171997\n","[Training Epoch 4] Batch 4249, Loss 0.3775017559528351\n","[Training Epoch 4] Batch 4250, Loss 0.35799741744995117\n","[Training Epoch 4] Batch 4251, Loss 0.35269397497177124\n","[Training Epoch 4] Batch 4252, Loss 0.39397627115249634\n","[Training Epoch 4] Batch 4253, Loss 0.3478699028491974\n","[Training Epoch 4] Batch 4254, Loss 0.3922213315963745\n","[Training Epoch 4] Batch 4255, Loss 0.35238197445869446\n","[Training Epoch 4] Batch 4256, Loss 0.34496816992759705\n","[Training Epoch 4] Batch 4257, Loss 0.34259316325187683\n","[Training Epoch 4] Batch 4258, Loss 0.3811753988265991\n","[Training Epoch 4] Batch 4259, Loss 0.3507271707057953\n","[Training Epoch 4] Batch 4260, Loss 0.3485429286956787\n","[Training Epoch 4] Batch 4261, Loss 0.33313772082328796\n","[Training Epoch 4] Batch 4262, Loss 0.40094679594039917\n","[Training Epoch 4] Batch 4263, Loss 0.3352483808994293\n","[Training Epoch 4] Batch 4264, Loss 0.419567346572876\n","[Training Epoch 4] Batch 4265, Loss 0.33037641644477844\n","[Training Epoch 4] Batch 4266, Loss 0.341632604598999\n","[Training Epoch 4] Batch 4267, Loss 0.3641584515571594\n","[Training Epoch 4] Batch 4268, Loss 0.3253762722015381\n","[Training Epoch 4] Batch 4269, Loss 0.36673808097839355\n","[Training Epoch 4] Batch 4270, Loss 0.358672559261322\n","[Training Epoch 4] Batch 4271, Loss 0.3848462700843811\n","[Training Epoch 4] Batch 4272, Loss 0.3700976073741913\n","[Training Epoch 4] Batch 4273, Loss 0.36579352617263794\n","[Training Epoch 4] Batch 4274, Loss 0.35043248534202576\n","[Training Epoch 4] Batch 4275, Loss 0.3625016212463379\n","[Training Epoch 4] Batch 4276, Loss 0.36061879992485046\n","[Training Epoch 4] Batch 4277, Loss 0.36793580651283264\n","[Training Epoch 4] Batch 4278, Loss 0.3451569974422455\n","[Training Epoch 4] Batch 4279, Loss 0.3534933030605316\n","[Training Epoch 4] Batch 4280, Loss 0.37098008394241333\n","[Training Epoch 4] Batch 4281, Loss 0.36948466300964355\n","[Training Epoch 4] Batch 4282, Loss 0.38820651173591614\n","[Training Epoch 4] Batch 4283, Loss 0.3749067783355713\n","[Training Epoch 4] Batch 4284, Loss 0.35454314947128296\n","[Training Epoch 4] Batch 4285, Loss 0.31750941276550293\n","[Training Epoch 4] Batch 4286, Loss 0.3906160891056061\n","[Training Epoch 4] Batch 4287, Loss 0.3636605143547058\n","[Training Epoch 4] Batch 4288, Loss 0.3575810194015503\n","[Training Epoch 4] Batch 4289, Loss 0.3810579478740692\n","[Training Epoch 4] Batch 4290, Loss 0.3619312047958374\n","[Training Epoch 4] Batch 4291, Loss 0.3596680760383606\n","[Training Epoch 4] Batch 4292, Loss 0.35275405645370483\n","[Training Epoch 4] Batch 4293, Loss 0.3355197310447693\n","[Training Epoch 4] Batch 4294, Loss 0.3749707341194153\n","[Training Epoch 4] Batch 4295, Loss 0.3724404573440552\n","[Training Epoch 4] Batch 4296, Loss 0.34217190742492676\n","[Training Epoch 4] Batch 4297, Loss 0.3785921335220337\n","[Training Epoch 4] Batch 4298, Loss 0.3801494836807251\n","[Training Epoch 4] Batch 4299, Loss 0.34632161259651184\n","[Training Epoch 4] Batch 4300, Loss 0.383023738861084\n","[Training Epoch 4] Batch 4301, Loss 0.3851899802684784\n","[Training Epoch 4] Batch 4302, Loss 0.39341938495635986\n","[Training Epoch 4] Batch 4303, Loss 0.356410413980484\n","[Training Epoch 4] Batch 4304, Loss 0.3652872145175934\n","[Training Epoch 4] Batch 4305, Loss 0.3702392578125\n","[Training Epoch 4] Batch 4306, Loss 0.3495666980743408\n","[Training Epoch 4] Batch 4307, Loss 0.3519565463066101\n","[Training Epoch 4] Batch 4308, Loss 0.36751222610473633\n","[Training Epoch 4] Batch 4309, Loss 0.34962332248687744\n","[Training Epoch 4] Batch 4310, Loss 0.35725581645965576\n","[Training Epoch 4] Batch 4311, Loss 0.38770580291748047\n","[Training Epoch 4] Batch 4312, Loss 0.35542240738868713\n","[Training Epoch 4] Batch 4313, Loss 0.3598480820655823\n","[Training Epoch 4] Batch 4314, Loss 0.38860493898391724\n","[Training Epoch 4] Batch 4315, Loss 0.3314575254917145\n","[Training Epoch 4] Batch 4316, Loss 0.36414042115211487\n","[Training Epoch 4] Batch 4317, Loss 0.36352139711380005\n","[Training Epoch 4] Batch 4318, Loss 0.34923088550567627\n","[Training Epoch 4] Batch 4319, Loss 0.35970038175582886\n","[Training Epoch 4] Batch 4320, Loss 0.36512672901153564\n","[Training Epoch 4] Batch 4321, Loss 0.3621119260787964\n","[Training Epoch 4] Batch 4322, Loss 0.3561342656612396\n","[Training Epoch 4] Batch 4323, Loss 0.3620222806930542\n","[Training Epoch 4] Batch 4324, Loss 0.36519286036491394\n","[Training Epoch 4] Batch 4325, Loss 0.34837841987609863\n","[Training Epoch 4] Batch 4326, Loss 0.3126011788845062\n","[Training Epoch 4] Batch 4327, Loss 0.34380513429641724\n","[Training Epoch 4] Batch 4328, Loss 0.3719167113304138\n","[Training Epoch 4] Batch 4329, Loss 0.3323841691017151\n","[Training Epoch 4] Batch 4330, Loss 0.31919968128204346\n","[Training Epoch 4] Batch 4331, Loss 0.350179523229599\n","[Training Epoch 4] Batch 4332, Loss 0.36440619826316833\n","[Training Epoch 4] Batch 4333, Loss 0.3648563623428345\n","[Training Epoch 4] Batch 4334, Loss 0.3651566505432129\n","[Training Epoch 4] Batch 4335, Loss 0.31221166253089905\n","[Training Epoch 4] Batch 4336, Loss 0.34724313020706177\n","[Training Epoch 4] Batch 4337, Loss 0.37360209226608276\n","[Training Epoch 4] Batch 4338, Loss 0.37087249755859375\n","[Training Epoch 4] Batch 4339, Loss 0.3533904552459717\n","[Training Epoch 4] Batch 4340, Loss 0.39007568359375\n","[Training Epoch 4] Batch 4341, Loss 0.3601653277873993\n","[Training Epoch 4] Batch 4342, Loss 0.3741821050643921\n","[Training Epoch 4] Batch 4343, Loss 0.37880200147628784\n","[Training Epoch 4] Batch 4344, Loss 0.32971253991127014\n","[Training Epoch 4] Batch 4345, Loss 0.36179664731025696\n","[Training Epoch 4] Batch 4346, Loss 0.3547491133213043\n","[Training Epoch 4] Batch 4347, Loss 0.37753748893737793\n","[Training Epoch 4] Batch 4348, Loss 0.3505316376686096\n","[Training Epoch 4] Batch 4349, Loss 0.3806215524673462\n","[Training Epoch 4] Batch 4350, Loss 0.3850240111351013\n","[Training Epoch 4] Batch 4351, Loss 0.3799961805343628\n","[Training Epoch 4] Batch 4352, Loss 0.3634297847747803\n","[Training Epoch 4] Batch 4353, Loss 0.3523517847061157\n","[Training Epoch 4] Batch 4354, Loss 0.375309020280838\n","[Training Epoch 4] Batch 4355, Loss 0.38292044401168823\n","[Training Epoch 4] Batch 4356, Loss 0.3698905110359192\n","[Training Epoch 4] Batch 4357, Loss 0.3788623511791229\n","[Training Epoch 4] Batch 4358, Loss 0.3680107593536377\n","[Training Epoch 4] Batch 4359, Loss 0.37056827545166016\n","[Training Epoch 4] Batch 4360, Loss 0.3722255229949951\n","[Training Epoch 4] Batch 4361, Loss 0.36808210611343384\n","[Training Epoch 4] Batch 4362, Loss 0.36010605096817017\n","[Training Epoch 4] Batch 4363, Loss 0.3478046655654907\n","[Training Epoch 4] Batch 4364, Loss 0.3991934657096863\n","[Training Epoch 4] Batch 4365, Loss 0.3622317314147949\n","[Training Epoch 4] Batch 4366, Loss 0.38266029953956604\n","[Training Epoch 4] Batch 4367, Loss 0.3814629316329956\n","[Training Epoch 4] Batch 4368, Loss 0.38225388526916504\n","[Training Epoch 4] Batch 4369, Loss 0.3423446714878082\n","[Training Epoch 4] Batch 4370, Loss 0.37064939737319946\n","[Training Epoch 4] Batch 4371, Loss 0.3461586833000183\n","[Training Epoch 4] Batch 4372, Loss 0.3491886258125305\n","[Training Epoch 4] Batch 4373, Loss 0.33461129665374756\n","[Training Epoch 4] Batch 4374, Loss 0.3417089581489563\n","[Training Epoch 4] Batch 4375, Loss 0.3509392738342285\n","[Training Epoch 4] Batch 4376, Loss 0.3229805827140808\n","[Training Epoch 4] Batch 4377, Loss 0.3380656838417053\n","[Training Epoch 4] Batch 4378, Loss 0.37664830684661865\n","[Training Epoch 4] Batch 4379, Loss 0.37393248081207275\n","[Training Epoch 4] Batch 4380, Loss 0.3775162398815155\n","[Training Epoch 4] Batch 4381, Loss 0.384182870388031\n","[Training Epoch 4] Batch 4382, Loss 0.3374224901199341\n","[Training Epoch 4] Batch 4383, Loss 0.3568474054336548\n","[Training Epoch 4] Batch 4384, Loss 0.35589170455932617\n","[Training Epoch 4] Batch 4385, Loss 0.3638221025466919\n","[Training Epoch 4] Batch 4386, Loss 0.38246816396713257\n","[Training Epoch 4] Batch 4387, Loss 0.3875812292098999\n","[Training Epoch 4] Batch 4388, Loss 0.3494985103607178\n","[Training Epoch 4] Batch 4389, Loss 0.32610470056533813\n","[Training Epoch 4] Batch 4390, Loss 0.3903593420982361\n","[Training Epoch 4] Batch 4391, Loss 0.34803903102874756\n","[Training Epoch 4] Batch 4392, Loss 0.38708728551864624\n","[Training Epoch 4] Batch 4393, Loss 0.3484283685684204\n","[Training Epoch 4] Batch 4394, Loss 0.3685900568962097\n","[Training Epoch 4] Batch 4395, Loss 0.34936094284057617\n","[Training Epoch 4] Batch 4396, Loss 0.4039834141731262\n","[Training Epoch 4] Batch 4397, Loss 0.36977219581604004\n","[Training Epoch 4] Batch 4398, Loss 0.3606933653354645\n","[Training Epoch 4] Batch 4399, Loss 0.41284263134002686\n","[Training Epoch 4] Batch 4400, Loss 0.3566185235977173\n","[Training Epoch 4] Batch 4401, Loss 0.3491821885108948\n","[Training Epoch 4] Batch 4402, Loss 0.36617299914360046\n","[Training Epoch 4] Batch 4403, Loss 0.3691195845603943\n","[Training Epoch 4] Batch 4404, Loss 0.3268209993839264\n","[Training Epoch 4] Batch 4405, Loss 0.3955347537994385\n","[Training Epoch 4] Batch 4406, Loss 0.3775915503501892\n","[Training Epoch 4] Batch 4407, Loss 0.32801806926727295\n","[Training Epoch 4] Batch 4408, Loss 0.33413511514663696\n","[Training Epoch 4] Batch 4409, Loss 0.3472789525985718\n","[Training Epoch 4] Batch 4410, Loss 0.36188948154449463\n","[Training Epoch 4] Batch 4411, Loss 0.32110682129859924\n","[Training Epoch 4] Batch 4412, Loss 0.3418359160423279\n","[Training Epoch 4] Batch 4413, Loss 0.3384106755256653\n","[Training Epoch 4] Batch 4414, Loss 0.3730482757091522\n","[Training Epoch 4] Batch 4415, Loss 0.39211446046829224\n","[Training Epoch 4] Batch 4416, Loss 0.3409813940525055\n","[Training Epoch 4] Batch 4417, Loss 0.3592178523540497\n","[Training Epoch 4] Batch 4418, Loss 0.3761371672153473\n","[Training Epoch 4] Batch 4419, Loss 0.3735501766204834\n","[Training Epoch 4] Batch 4420, Loss 0.4071974456310272\n","[Training Epoch 4] Batch 4421, Loss 0.35618796944618225\n","[Training Epoch 4] Batch 4422, Loss 0.3573058843612671\n","[Training Epoch 4] Batch 4423, Loss 0.4007246494293213\n","[Training Epoch 4] Batch 4424, Loss 0.37140199542045593\n","[Training Epoch 4] Batch 4425, Loss 0.3434004783630371\n","[Training Epoch 4] Batch 4426, Loss 0.33832648396492004\n","[Training Epoch 4] Batch 4427, Loss 0.3594541847705841\n","[Training Epoch 4] Batch 4428, Loss 0.3308093547821045\n","[Training Epoch 4] Batch 4429, Loss 0.36243435740470886\n","[Training Epoch 4] Batch 4430, Loss 0.35357797145843506\n","[Training Epoch 4] Batch 4431, Loss 0.3435787558555603\n","[Training Epoch 4] Batch 4432, Loss 0.356383353471756\n","[Training Epoch 4] Batch 4433, Loss 0.3550131916999817\n","[Training Epoch 4] Batch 4434, Loss 0.3897831439971924\n","[Training Epoch 4] Batch 4435, Loss 0.3684002459049225\n","[Training Epoch 4] Batch 4436, Loss 0.3629348874092102\n","[Training Epoch 4] Batch 4437, Loss 0.35251861810684204\n","[Training Epoch 4] Batch 4438, Loss 0.37032997608184814\n","[Training Epoch 4] Batch 4439, Loss 0.3576223850250244\n","[Training Epoch 4] Batch 4440, Loss 0.35155239701271057\n","[Training Epoch 4] Batch 4441, Loss 0.3214424252510071\n","[Training Epoch 4] Batch 4442, Loss 0.3646274209022522\n","[Training Epoch 4] Batch 4443, Loss 0.36881235241889954\n","[Training Epoch 4] Batch 4444, Loss 0.3510425090789795\n","[Training Epoch 4] Batch 4445, Loss 0.33428919315338135\n","[Training Epoch 4] Batch 4446, Loss 0.3445066213607788\n","[Training Epoch 4] Batch 4447, Loss 0.3725470304489136\n","[Training Epoch 4] Batch 4448, Loss 0.3595070242881775\n","[Training Epoch 4] Batch 4449, Loss 0.36715906858444214\n","[Training Epoch 4] Batch 4450, Loss 0.35588276386260986\n","[Training Epoch 4] Batch 4451, Loss 0.3602645993232727\n","[Training Epoch 4] Batch 4452, Loss 0.34946078062057495\n","[Training Epoch 4] Batch 4453, Loss 0.3587722182273865\n","[Training Epoch 4] Batch 4454, Loss 0.3560327887535095\n","[Training Epoch 4] Batch 4455, Loss 0.3534143269062042\n","[Training Epoch 4] Batch 4456, Loss 0.389311283826828\n","[Training Epoch 4] Batch 4457, Loss 0.3743235766887665\n","[Training Epoch 4] Batch 4458, Loss 0.3878009021282196\n","[Training Epoch 4] Batch 4459, Loss 0.3990199565887451\n","[Training Epoch 4] Batch 4460, Loss 0.35406774282455444\n","[Training Epoch 4] Batch 4461, Loss 0.37096089124679565\n","[Training Epoch 4] Batch 4462, Loss 0.36699485778808594\n","[Training Epoch 4] Batch 4463, Loss 0.30618423223495483\n","[Training Epoch 4] Batch 4464, Loss 0.3590465784072876\n","[Training Epoch 4] Batch 4465, Loss 0.34739941358566284\n","[Training Epoch 4] Batch 4466, Loss 0.37737545371055603\n","[Training Epoch 4] Batch 4467, Loss 0.37359869480133057\n","[Training Epoch 4] Batch 4468, Loss 0.3550369143486023\n","[Training Epoch 4] Batch 4469, Loss 0.3422519266605377\n","[Training Epoch 4] Batch 4470, Loss 0.3486074209213257\n","[Training Epoch 4] Batch 4471, Loss 0.35913434624671936\n","[Training Epoch 4] Batch 4472, Loss 0.38001906871795654\n","[Training Epoch 4] Batch 4473, Loss 0.3379266858100891\n","[Training Epoch 4] Batch 4474, Loss 0.36453571915626526\n","[Training Epoch 4] Batch 4475, Loss 0.382320761680603\n","[Training Epoch 4] Batch 4476, Loss 0.33415645360946655\n","[Training Epoch 4] Batch 4477, Loss 0.3881155252456665\n","[Training Epoch 4] Batch 4478, Loss 0.3269764482975006\n","[Training Epoch 4] Batch 4479, Loss 0.4099090099334717\n","[Training Epoch 4] Batch 4480, Loss 0.4196455478668213\n","[Training Epoch 4] Batch 4481, Loss 0.3540513217449188\n","[Training Epoch 4] Batch 4482, Loss 0.3562062978744507\n","[Training Epoch 4] Batch 4483, Loss 0.33096200227737427\n","[Training Epoch 4] Batch 4484, Loss 0.3587583303451538\n","[Training Epoch 4] Batch 4485, Loss 0.3648756146430969\n","[Training Epoch 4] Batch 4486, Loss 0.3558960258960724\n","[Training Epoch 4] Batch 4487, Loss 0.33172374963760376\n","[Training Epoch 4] Batch 4488, Loss 0.36348652839660645\n","[Training Epoch 4] Batch 4489, Loss 0.34877312183380127\n","[Training Epoch 4] Batch 4490, Loss 0.3940620422363281\n","[Training Epoch 4] Batch 4491, Loss 0.383891761302948\n","[Training Epoch 4] Batch 4492, Loss 0.38157910108566284\n","[Training Epoch 4] Batch 4493, Loss 0.3863797187805176\n","[Training Epoch 4] Batch 4494, Loss 0.35710352659225464\n","[Training Epoch 4] Batch 4495, Loss 0.3968305289745331\n","[Training Epoch 4] Batch 4496, Loss 0.36807724833488464\n","[Training Epoch 4] Batch 4497, Loss 0.3646482229232788\n","[Training Epoch 4] Batch 4498, Loss 0.3739197850227356\n","[Training Epoch 4] Batch 4499, Loss 0.3867083787918091\n","[Training Epoch 4] Batch 4500, Loss 0.3747369647026062\n","[Training Epoch 4] Batch 4501, Loss 0.3569546341896057\n","[Training Epoch 4] Batch 4502, Loss 0.3793054521083832\n","[Training Epoch 4] Batch 4503, Loss 0.37409764528274536\n","[Training Epoch 4] Batch 4504, Loss 0.36419230699539185\n","[Training Epoch 4] Batch 4505, Loss 0.37140291929244995\n","[Training Epoch 4] Batch 4506, Loss 0.3592875003814697\n","[Training Epoch 4] Batch 4507, Loss 0.3150368928909302\n","[Training Epoch 4] Batch 4508, Loss 0.3709181547164917\n","[Training Epoch 4] Batch 4509, Loss 0.3982679843902588\n","[Training Epoch 4] Batch 4510, Loss 0.3928033709526062\n","[Training Epoch 4] Batch 4511, Loss 0.359771192073822\n","[Training Epoch 4] Batch 4512, Loss 0.357845664024353\n","[Training Epoch 4] Batch 4513, Loss 0.41156166791915894\n","[Training Epoch 4] Batch 4514, Loss 0.3780530095100403\n","[Training Epoch 4] Batch 4515, Loss 0.34666043519973755\n","[Training Epoch 4] Batch 4516, Loss 0.3595297336578369\n","[Training Epoch 4] Batch 4517, Loss 0.39585447311401367\n","[Training Epoch 4] Batch 4518, Loss 0.3697386384010315\n","[Training Epoch 4] Batch 4519, Loss 0.34995222091674805\n","[Training Epoch 4] Batch 4520, Loss 0.34676873683929443\n","[Training Epoch 4] Batch 4521, Loss 0.3534542918205261\n","[Training Epoch 4] Batch 4522, Loss 0.38763049244880676\n","[Training Epoch 4] Batch 4523, Loss 0.38023442029953003\n","[Training Epoch 4] Batch 4524, Loss 0.34354329109191895\n","[Training Epoch 4] Batch 4525, Loss 0.36528319120407104\n","[Training Epoch 4] Batch 4526, Loss 0.4043259918689728\n","[Training Epoch 4] Batch 4527, Loss 0.3691162168979645\n","[Training Epoch 4] Batch 4528, Loss 0.3609187602996826\n","[Training Epoch 4] Batch 4529, Loss 0.33742594718933105\n","[Training Epoch 4] Batch 4530, Loss 0.35870692133903503\n","[Training Epoch 4] Batch 4531, Loss 0.37156784534454346\n","[Training Epoch 4] Batch 4532, Loss 0.37254464626312256\n","[Training Epoch 4] Batch 4533, Loss 0.368153840303421\n","[Training Epoch 4] Batch 4534, Loss 0.3467326760292053\n","[Training Epoch 4] Batch 4535, Loss 0.3548646867275238\n","[Training Epoch 4] Batch 4536, Loss 0.3993125557899475\n","[Training Epoch 4] Batch 4537, Loss 0.35093605518341064\n","[Training Epoch 4] Batch 4538, Loss 0.3845469653606415\n","[Training Epoch 4] Batch 4539, Loss 0.3848001956939697\n","[Training Epoch 4] Batch 4540, Loss 0.3388165831565857\n","[Training Epoch 4] Batch 4541, Loss 0.3678704798221588\n","[Training Epoch 4] Batch 4542, Loss 0.33791959285736084\n","[Training Epoch 4] Batch 4543, Loss 0.40985292196273804\n","[Training Epoch 4] Batch 4544, Loss 0.3288532495498657\n","[Training Epoch 4] Batch 4545, Loss 0.3755872845649719\n","[Training Epoch 4] Batch 4546, Loss 0.35731178522109985\n","[Training Epoch 4] Batch 4547, Loss 0.3513137698173523\n","[Training Epoch 4] Batch 4548, Loss 0.3662675619125366\n","[Training Epoch 4] Batch 4549, Loss 0.3686840236186981\n","[Training Epoch 4] Batch 4550, Loss 0.3815471827983856\n","[Training Epoch 4] Batch 4551, Loss 0.371996134519577\n","[Training Epoch 4] Batch 4552, Loss 0.37469232082366943\n","[Training Epoch 4] Batch 4553, Loss 0.3824175000190735\n","[Training Epoch 4] Batch 4554, Loss 0.3663238286972046\n","[Training Epoch 4] Batch 4555, Loss 0.3955160975456238\n","[Training Epoch 4] Batch 4556, Loss 0.373688668012619\n","[Training Epoch 4] Batch 4557, Loss 0.34898629784584045\n","[Training Epoch 4] Batch 4558, Loss 0.34511029720306396\n","[Training Epoch 4] Batch 4559, Loss 0.3429999351501465\n","[Training Epoch 4] Batch 4560, Loss 0.34746989607810974\n","[Training Epoch 4] Batch 4561, Loss 0.3822490870952606\n","[Training Epoch 4] Batch 4562, Loss 0.3742433786392212\n","[Training Epoch 4] Batch 4563, Loss 0.34039509296417236\n","[Training Epoch 4] Batch 4564, Loss 0.366948664188385\n","[Training Epoch 4] Batch 4565, Loss 0.35579177737236023\n","[Training Epoch 4] Batch 4566, Loss 0.36818167567253113\n","[Training Epoch 4] Batch 4567, Loss 0.32418256998062134\n","[Training Epoch 4] Batch 4568, Loss 0.345458447933197\n","[Training Epoch 4] Batch 4569, Loss 0.36959946155548096\n","[Training Epoch 4] Batch 4570, Loss 0.35663387179374695\n","[Training Epoch 4] Batch 4571, Loss 0.36039236187934875\n","[Training Epoch 4] Batch 4572, Loss 0.3869435787200928\n","[Training Epoch 4] Batch 4573, Loss 0.3670154809951782\n","[Training Epoch 4] Batch 4574, Loss 0.3509591817855835\n","[Training Epoch 4] Batch 4575, Loss 0.36058351397514343\n","[Training Epoch 4] Batch 4576, Loss 0.3567526340484619\n","[Training Epoch 4] Batch 4577, Loss 0.3414947986602783\n","[Training Epoch 4] Batch 4578, Loss 0.3432731628417969\n","[Training Epoch 4] Batch 4579, Loss 0.3613028824329376\n","[Training Epoch 4] Batch 4580, Loss 0.3535723090171814\n","[Training Epoch 4] Batch 4581, Loss 0.3516533374786377\n","[Training Epoch 4] Batch 4582, Loss 0.3878535330295563\n","[Training Epoch 4] Batch 4583, Loss 0.36813437938690186\n","[Training Epoch 4] Batch 4584, Loss 0.39181411266326904\n","[Training Epoch 4] Batch 4585, Loss 0.3735198676586151\n","[Training Epoch 4] Batch 4586, Loss 0.3957578241825104\n","[Training Epoch 4] Batch 4587, Loss 0.37052106857299805\n","[Training Epoch 4] Batch 4588, Loss 0.37669575214385986\n","[Training Epoch 4] Batch 4589, Loss 0.39520734548568726\n","[Training Epoch 4] Batch 4590, Loss 0.35444968938827515\n","[Training Epoch 4] Batch 4591, Loss 0.35523921251296997\n","[Training Epoch 4] Batch 4592, Loss 0.350211501121521\n","[Training Epoch 4] Batch 4593, Loss 0.3657568097114563\n","[Training Epoch 4] Batch 4594, Loss 0.33239611983299255\n","[Training Epoch 4] Batch 4595, Loss 0.37783509492874146\n","[Training Epoch 4] Batch 4596, Loss 0.3449473977088928\n","[Training Epoch 4] Batch 4597, Loss 0.36779505014419556\n","[Training Epoch 4] Batch 4598, Loss 0.35577744245529175\n","[Training Epoch 4] Batch 4599, Loss 0.37116336822509766\n","[Training Epoch 4] Batch 4600, Loss 0.36668598651885986\n","[Training Epoch 4] Batch 4601, Loss 0.3807864487171173\n","[Training Epoch 4] Batch 4602, Loss 0.3488820493221283\n","[Training Epoch 4] Batch 4603, Loss 0.37951844930648804\n","[Training Epoch 4] Batch 4604, Loss 0.3516979217529297\n","[Training Epoch 4] Batch 4605, Loss 0.3591019809246063\n","[Training Epoch 4] Batch 4606, Loss 0.33534184098243713\n","[Training Epoch 4] Batch 4607, Loss 0.3975972533226013\n","[Training Epoch 4] Batch 4608, Loss 0.32475969195365906\n","[Training Epoch 4] Batch 4609, Loss 0.3654567003250122\n","[Training Epoch 4] Batch 4610, Loss 0.3488084673881531\n","[Training Epoch 4] Batch 4611, Loss 0.3390330672264099\n","[Training Epoch 4] Batch 4612, Loss 0.37577587366104126\n","[Training Epoch 4] Batch 4613, Loss 0.36304426193237305\n","[Training Epoch 4] Batch 4614, Loss 0.36365729570388794\n","[Training Epoch 4] Batch 4615, Loss 0.34365224838256836\n","[Training Epoch 4] Batch 4616, Loss 0.3738672733306885\n","[Training Epoch 4] Batch 4617, Loss 0.33180105686187744\n","[Training Epoch 4] Batch 4618, Loss 0.32197755575180054\n","[Training Epoch 4] Batch 4619, Loss 0.35815680027008057\n","[Training Epoch 4] Batch 4620, Loss 0.35850265622138977\n","[Training Epoch 4] Batch 4621, Loss 0.37345001101493835\n","[Training Epoch 4] Batch 4622, Loss 0.3511888086795807\n","[Training Epoch 4] Batch 4623, Loss 0.34131771326065063\n","[Training Epoch 4] Batch 4624, Loss 0.34321826696395874\n","[Training Epoch 4] Batch 4625, Loss 0.38736432790756226\n","[Training Epoch 4] Batch 4626, Loss 0.3390546441078186\n","[Training Epoch 4] Batch 4627, Loss 0.40398842096328735\n","[Training Epoch 4] Batch 4628, Loss 0.37039023637771606\n","[Training Epoch 4] Batch 4629, Loss 0.3558207154273987\n","[Training Epoch 4] Batch 4630, Loss 0.34757301211357117\n","[Training Epoch 4] Batch 4631, Loss 0.3801451027393341\n","[Training Epoch 4] Batch 4632, Loss 0.38951870799064636\n","[Training Epoch 4] Batch 4633, Loss 0.3281579911708832\n","[Training Epoch 4] Batch 4634, Loss 0.37590211629867554\n","[Training Epoch 4] Batch 4635, Loss 0.33823004364967346\n","[Training Epoch 4] Batch 4636, Loss 0.3578171133995056\n","[Training Epoch 4] Batch 4637, Loss 0.36158594489097595\n","[Training Epoch 4] Batch 4638, Loss 0.3805670738220215\n","[Training Epoch 4] Batch 4639, Loss 0.36554238200187683\n","[Training Epoch 4] Batch 4640, Loss 0.3529818654060364\n","[Training Epoch 4] Batch 4641, Loss 0.3608661890029907\n","[Training Epoch 4] Batch 4642, Loss 0.32875797152519226\n","[Training Epoch 4] Batch 4643, Loss 0.3695560097694397\n","[Training Epoch 4] Batch 4644, Loss 0.3463382124900818\n","[Training Epoch 4] Batch 4645, Loss 0.3530685007572174\n","[Training Epoch 4] Batch 4646, Loss 0.36790651082992554\n","[Training Epoch 4] Batch 4647, Loss 0.37864750623703003\n","[Training Epoch 4] Batch 4648, Loss 0.37108737230300903\n","[Training Epoch 4] Batch 4649, Loss 0.3438466489315033\n","[Training Epoch 4] Batch 4650, Loss 0.3914863169193268\n","[Training Epoch 4] Batch 4651, Loss 0.36115291714668274\n","[Training Epoch 4] Batch 4652, Loss 0.3744823932647705\n","[Training Epoch 4] Batch 4653, Loss 0.3672119379043579\n","[Training Epoch 4] Batch 4654, Loss 0.3643677234649658\n","[Training Epoch 4] Batch 4655, Loss 0.3634405732154846\n","[Training Epoch 4] Batch 4656, Loss 0.3742072880268097\n","[Training Epoch 4] Batch 4657, Loss 0.3413909673690796\n","[Training Epoch 4] Batch 4658, Loss 0.3580299913883209\n","[Training Epoch 4] Batch 4659, Loss 0.37824565172195435\n","[Training Epoch 4] Batch 4660, Loss 0.3472197353839874\n","[Training Epoch 4] Batch 4661, Loss 0.37578123807907104\n","[Training Epoch 4] Batch 4662, Loss 0.34799206256866455\n","[Training Epoch 4] Batch 4663, Loss 0.37647366523742676\n","[Training Epoch 4] Batch 4664, Loss 0.3824828863143921\n","[Training Epoch 4] Batch 4665, Loss 0.36350899934768677\n","[Training Epoch 4] Batch 4666, Loss 0.3526936173439026\n","[Training Epoch 4] Batch 4667, Loss 0.3667212426662445\n","[Training Epoch 4] Batch 4668, Loss 0.34679660201072693\n","[Training Epoch 4] Batch 4669, Loss 0.3653096854686737\n","[Training Epoch 4] Batch 4670, Loss 0.3549596071243286\n","[Training Epoch 4] Batch 4671, Loss 0.3760755956172943\n","[Training Epoch 4] Batch 4672, Loss 0.3867272734642029\n","[Training Epoch 4] Batch 4673, Loss 0.35195374488830566\n","[Training Epoch 4] Batch 4674, Loss 0.3782094120979309\n","[Training Epoch 4] Batch 4675, Loss 0.3915581703186035\n","[Training Epoch 4] Batch 4676, Loss 0.39238154888153076\n","[Training Epoch 4] Batch 4677, Loss 0.3516305983066559\n","[Training Epoch 4] Batch 4678, Loss 0.3301352858543396\n","[Training Epoch 4] Batch 4679, Loss 0.3748723268508911\n","[Training Epoch 4] Batch 4680, Loss 0.3797348141670227\n","[Training Epoch 4] Batch 4681, Loss 0.38101810216903687\n","[Training Epoch 4] Batch 4682, Loss 0.3457278609275818\n","[Training Epoch 4] Batch 4683, Loss 0.35208213329315186\n","[Training Epoch 4] Batch 4684, Loss 0.35360223054885864\n","[Training Epoch 4] Batch 4685, Loss 0.353432834148407\n","[Training Epoch 4] Batch 4686, Loss 0.33664536476135254\n","[Training Epoch 4] Batch 4687, Loss 0.3611190915107727\n","[Training Epoch 4] Batch 4688, Loss 0.3579316735267639\n","[Training Epoch 4] Batch 4689, Loss 0.365251362323761\n","[Training Epoch 4] Batch 4690, Loss 0.3341172933578491\n","[Training Epoch 4] Batch 4691, Loss 0.37603873014450073\n","[Training Epoch 4] Batch 4692, Loss 0.3869338929653168\n","[Training Epoch 4] Batch 4693, Loss 0.3857468068599701\n","[Training Epoch 4] Batch 4694, Loss 0.3603582978248596\n","[Training Epoch 4] Batch 4695, Loss 0.3527098298072815\n","[Training Epoch 4] Batch 4696, Loss 0.38875263929367065\n","[Training Epoch 4] Batch 4697, Loss 0.34869980812072754\n","[Training Epoch 4] Batch 4698, Loss 0.37406083941459656\n","[Training Epoch 4] Batch 4699, Loss 0.35975760221481323\n","[Training Epoch 4] Batch 4700, Loss 0.3601953089237213\n","[Training Epoch 4] Batch 4701, Loss 0.3711085319519043\n","[Training Epoch 4] Batch 4702, Loss 0.35662952065467834\n","[Training Epoch 4] Batch 4703, Loss 0.3630845248699188\n","[Training Epoch 4] Batch 4704, Loss 0.35703563690185547\n","[Training Epoch 4] Batch 4705, Loss 0.35979750752449036\n","[Training Epoch 4] Batch 4706, Loss 0.36488014459609985\n","[Training Epoch 4] Batch 4707, Loss 0.334455668926239\n","[Training Epoch 4] Batch 4708, Loss 0.3403511047363281\n","[Training Epoch 4] Batch 4709, Loss 0.3610638976097107\n","[Training Epoch 4] Batch 4710, Loss 0.3696393370628357\n","[Training Epoch 4] Batch 4711, Loss 0.33199968934059143\n","[Training Epoch 4] Batch 4712, Loss 0.33604276180267334\n","[Training Epoch 4] Batch 4713, Loss 0.4014418125152588\n","[Training Epoch 4] Batch 4714, Loss 0.35331037640571594\n","[Training Epoch 4] Batch 4715, Loss 0.3550589680671692\n","[Training Epoch 4] Batch 4716, Loss 0.34632083773612976\n","[Training Epoch 4] Batch 4717, Loss 0.34572622179985046\n","[Training Epoch 4] Batch 4718, Loss 0.3603013753890991\n","[Training Epoch 4] Batch 4719, Loss 0.36619919538497925\n","[Training Epoch 4] Batch 4720, Loss 0.35412198305130005\n","[Training Epoch 4] Batch 4721, Loss 0.36664384603500366\n","[Training Epoch 4] Batch 4722, Loss 0.35441920161247253\n","[Training Epoch 4] Batch 4723, Loss 0.3645741939544678\n","[Training Epoch 4] Batch 4724, Loss 0.349460631608963\n","[Training Epoch 4] Batch 4725, Loss 0.3451082706451416\n","[Training Epoch 4] Batch 4726, Loss 0.3513440191745758\n","[Training Epoch 4] Batch 4727, Loss 0.3298613429069519\n","[Training Epoch 4] Batch 4728, Loss 0.3676767349243164\n","[Training Epoch 4] Batch 4729, Loss 0.3346814513206482\n","[Training Epoch 4] Batch 4730, Loss 0.3835830092430115\n","[Training Epoch 4] Batch 4731, Loss 0.3354441523551941\n","[Training Epoch 4] Batch 4732, Loss 0.3675876259803772\n","[Training Epoch 4] Batch 4733, Loss 0.37005436420440674\n","[Training Epoch 4] Batch 4734, Loss 0.3504614531993866\n","[Training Epoch 4] Batch 4735, Loss 0.3396799564361572\n","[Training Epoch 4] Batch 4736, Loss 0.3683757781982422\n","[Training Epoch 4] Batch 4737, Loss 0.37224531173706055\n","[Training Epoch 4] Batch 4738, Loss 0.34352242946624756\n","[Training Epoch 4] Batch 4739, Loss 0.3379550576210022\n","[Training Epoch 4] Batch 4740, Loss 0.3635897636413574\n","[Training Epoch 4] Batch 4741, Loss 0.3691255450248718\n","[Training Epoch 4] Batch 4742, Loss 0.34136754274368286\n","[Training Epoch 4] Batch 4743, Loss 0.3710564970970154\n","[Training Epoch 4] Batch 4744, Loss 0.33592498302459717\n","[Training Epoch 4] Batch 4745, Loss 0.33846738934516907\n","[Training Epoch 4] Batch 4746, Loss 0.3588396906852722\n","[Training Epoch 4] Batch 4747, Loss 0.36631083488464355\n","[Training Epoch 4] Batch 4748, Loss 0.3682546615600586\n","[Training Epoch 4] Batch 4749, Loss 0.3771406412124634\n","[Training Epoch 4] Batch 4750, Loss 0.3616766929626465\n","[Training Epoch 4] Batch 4751, Loss 0.3667076826095581\n","[Training Epoch 4] Batch 4752, Loss 0.3759610652923584\n","[Training Epoch 4] Batch 4753, Loss 0.36680352687835693\n","[Training Epoch 4] Batch 4754, Loss 0.3520447015762329\n","[Training Epoch 4] Batch 4755, Loss 0.3664756715297699\n","[Training Epoch 4] Batch 4756, Loss 0.3409541845321655\n","[Training Epoch 4] Batch 4757, Loss 0.3559373617172241\n","[Training Epoch 4] Batch 4758, Loss 0.4116401970386505\n","[Training Epoch 4] Batch 4759, Loss 0.39865854382514954\n","[Training Epoch 4] Batch 4760, Loss 0.37347859144210815\n","[Training Epoch 4] Batch 4761, Loss 0.37671732902526855\n","[Training Epoch 4] Batch 4762, Loss 0.34584081172943115\n","[Training Epoch 4] Batch 4763, Loss 0.35214751958847046\n","[Training Epoch 4] Batch 4764, Loss 0.36128687858581543\n","[Training Epoch 4] Batch 4765, Loss 0.3613557815551758\n","[Training Epoch 4] Batch 4766, Loss 0.3452834188938141\n","[Training Epoch 4] Batch 4767, Loss 0.3940025568008423\n","[Training Epoch 4] Batch 4768, Loss 0.3268873691558838\n","[Training Epoch 4] Batch 4769, Loss 0.3567279875278473\n","[Training Epoch 4] Batch 4770, Loss 0.3787330687046051\n","[Training Epoch 4] Batch 4771, Loss 0.39939892292022705\n","[Training Epoch 4] Batch 4772, Loss 0.35000258684158325\n","[Training Epoch 4] Batch 4773, Loss 0.32341212034225464\n","[Training Epoch 4] Batch 4774, Loss 0.3825356066226959\n","[Training Epoch 4] Batch 4775, Loss 0.35561248660087585\n","[Training Epoch 4] Batch 4776, Loss 0.36779385805130005\n","[Training Epoch 4] Batch 4777, Loss 0.3612159192562103\n","[Training Epoch 4] Batch 4778, Loss 0.351484477519989\n","[Training Epoch 4] Batch 4779, Loss 0.3582635521888733\n","[Training Epoch 4] Batch 4780, Loss 0.3512333631515503\n","[Training Epoch 4] Batch 4781, Loss 0.3577989339828491\n","[Training Epoch 4] Batch 4782, Loss 0.3509555160999298\n","[Training Epoch 4] Batch 4783, Loss 0.3963024318218231\n","[Training Epoch 4] Batch 4784, Loss 0.36278557777404785\n","[Training Epoch 4] Batch 4785, Loss 0.3587532639503479\n","[Training Epoch 4] Batch 4786, Loss 0.383573055267334\n","[Training Epoch 4] Batch 4787, Loss 0.34670841693878174\n","[Training Epoch 4] Batch 4788, Loss 0.35662373900413513\n","[Training Epoch 4] Batch 4789, Loss 0.37804725766181946\n","[Training Epoch 4] Batch 4790, Loss 0.363569438457489\n","[Training Epoch 4] Batch 4791, Loss 0.34576913714408875\n","[Training Epoch 4] Batch 4792, Loss 0.32518434524536133\n","[Training Epoch 4] Batch 4793, Loss 0.35242772102355957\n","[Training Epoch 4] Batch 4794, Loss 0.34553956985473633\n","[Training Epoch 4] Batch 4795, Loss 0.34807664155960083\n","[Training Epoch 4] Batch 4796, Loss 0.3552778363227844\n","[Training Epoch 4] Batch 4797, Loss 0.3665284514427185\n","[Training Epoch 4] Batch 4798, Loss 0.35449570417404175\n","[Training Epoch 4] Batch 4799, Loss 0.36677736043930054\n","[Training Epoch 4] Batch 4800, Loss 0.363686203956604\n","[Training Epoch 4] Batch 4801, Loss 0.3625072240829468\n","[Training Epoch 4] Batch 4802, Loss 0.36722517013549805\n","[Training Epoch 4] Batch 4803, Loss 0.37963443994522095\n","[Training Epoch 4] Batch 4804, Loss 0.3861827850341797\n","[Training Epoch 4] Batch 4805, Loss 0.38280385732650757\n","[Training Epoch 4] Batch 4806, Loss 0.3253900408744812\n","[Training Epoch 4] Batch 4807, Loss 0.3537244200706482\n","[Training Epoch 4] Batch 4808, Loss 0.3782561421394348\n","[Training Epoch 4] Batch 4809, Loss 0.36548498272895813\n","[Training Epoch 4] Batch 4810, Loss 0.3818154036998749\n","[Training Epoch 4] Batch 4811, Loss 0.34299856424331665\n","[Training Epoch 4] Batch 4812, Loss 0.37998130917549133\n","[Training Epoch 4] Batch 4813, Loss 0.3745507597923279\n","[Training Epoch 4] Batch 4814, Loss 0.37185072898864746\n","[Training Epoch 4] Batch 4815, Loss 0.3670627772808075\n","[Training Epoch 4] Batch 4816, Loss 0.322262704372406\n","[Training Epoch 4] Batch 4817, Loss 0.3967703580856323\n","[Training Epoch 4] Batch 4818, Loss 0.3492298126220703\n","[Training Epoch 4] Batch 4819, Loss 0.35100066661834717\n","[Training Epoch 4] Batch 4820, Loss 0.3213804364204407\n","[Training Epoch 4] Batch 4821, Loss 0.3665940761566162\n","[Training Epoch 4] Batch 4822, Loss 0.35940083861351013\n","[Training Epoch 4] Batch 4823, Loss 0.35245388746261597\n","[Training Epoch 4] Batch 4824, Loss 0.36410823464393616\n","[Training Epoch 4] Batch 4825, Loss 0.3747537136077881\n","[Training Epoch 4] Batch 4826, Loss 0.337063193321228\n","[Training Epoch 4] Batch 4827, Loss 0.39369410276412964\n","[Training Epoch 4] Batch 4828, Loss 0.36674976348876953\n","[Training Epoch 4] Batch 4829, Loss 0.3556213974952698\n","[Training Epoch 4] Batch 4830, Loss 0.35132932662963867\n","[Training Epoch 4] Batch 4831, Loss 0.39355742931365967\n","[Training Epoch 4] Batch 4832, Loss 0.37907731533050537\n","[Training Epoch 4] Batch 4833, Loss 0.34467095136642456\n","[Training Epoch 4] Batch 4834, Loss 0.3565904498100281\n","[Training Epoch 4] Batch 4835, Loss 0.34328925609588623\n","[Training Epoch 4] Batch 4836, Loss 0.341796338558197\n","[Training Epoch 4] Batch 4837, Loss 0.3561728000640869\n","[Training Epoch 4] Batch 4838, Loss 0.3350965678691864\n","[Training Epoch 4] Batch 4839, Loss 0.41184765100479126\n","[Training Epoch 4] Batch 4840, Loss 0.3571063280105591\n","[Training Epoch 4] Batch 4841, Loss 0.3574279844760895\n","[Training Epoch 4] Batch 4842, Loss 0.3268316090106964\n","[Training Epoch 4] Batch 4843, Loss 0.34853893518447876\n","[Training Epoch 4] Batch 4844, Loss 0.36693286895751953\n","[Training Epoch 4] Batch 4845, Loss 0.3467456102371216\n","[Training Epoch 4] Batch 4846, Loss 0.3755817413330078\n","[Training Epoch 4] Batch 4847, Loss 0.36446982622146606\n","[Training Epoch 4] Batch 4848, Loss 0.34951913356781006\n","[Training Epoch 4] Batch 4849, Loss 0.34548240900039673\n","[Training Epoch 4] Batch 4850, Loss 0.3459133207798004\n","[Training Epoch 4] Batch 4851, Loss 0.3780086040496826\n","[Training Epoch 4] Batch 4852, Loss 0.36214035749435425\n","[Training Epoch 4] Batch 4853, Loss 0.36113467812538147\n","[Training Epoch 4] Batch 4854, Loss 0.37215662002563477\n","[Evluating Epoch 4] HR = 0.4233, NDCG = 0.2347\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:52: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame.\n","Try using .loc[row_indexer,col_indexer] = value instead\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"]}],"source":["gmf_config = {'alias': 'gmf_factor8neg4-implict',\n","              'num_epoch': 5,\n","              'batch_size': 1024,\n","              # 'optimizer': 'sgd',\n","              # 'sgd_lr': 1e-3,\n","              # 'sgd_momentum': 0.9,\n","              # 'optimizer': 'rmsprop',\n","              # 'rmsprop_lr': 1e-3,\n","              # 'rmsprop_alpha': 0.99,\n","              # 'rmsprop_momentum': 0,\n","              'optimizer': 'adam',\n","              'adam_lr': 1e-3,\n","              'num_users': 6040,\n","              'num_items': 3706,\n","              'latent_dim': 8,\n","              'num_negative': 4,\n","              'l2_regularization': 0, # 0.01\n","              'use_cuda': True,\n","              'device_id': 0,\n","              'model_dir':'checkpoints/{}_Epoch{}_HR{:.4f}_NDCG{:.4f}.model'}\n","\n","mlp_config = {'alias': 'mlp_factor8neg4_bz256_166432168_pretrain_reg_0.0000001',\n","              'num_epoch': 200,\n","              'batch_size': 256,  # 1024,\n","              'optimizer': 'adam',\n","              'adam_lr': 1e-3,\n","              'num_users': 6040,\n","              'num_items': 3706,\n","              'latent_dim': 8,\n","              'num_negative': 4,\n","              'layers': [16,64,32,16,8],  # layers[0] is the concat of latent user vector & latent item vector\n","              'l2_regularization': 0.0000001,  # MLP model is sensitive to hyper params\n","              'use_cuda': True,\n","              'device_id': 7,\n","              'pretrain': True,\n","              'pretrain_mf': 'checkpoints/{}'.format('gmf_factor8neg4_Epoch100_HR0.6391_NDCG0.2852.model'),\n","              'model_dir':'checkpoints/{}_Epoch{}_HR{:.4f}_NDCG{:.4f}.model'}\n","\n","neumf_config = {'alias': 'pretrain_neumf_factor8neg4',\n","                'num_epoch': 200,\n","                'batch_size': 1024,\n","                'optimizer': 'adam',\n","                'adam_lr': 1e-3,\n","                'num_users': 6040,\n","                'num_items': 3706,\n","                'latent_dim_mf': 8,\n","                'latent_dim_mlp': 8,\n","                'num_negative': 4,\n","                'layers': [16,32,16,8],  # layers[0] is the concat of latent user vector & latent item vector\n","                'l2_regularization': 0.01,\n","                'use_cuda': True,\n","                'device_id': 7,\n","                'pretrain': True,\n","                'pretrain_mf': 'checkpoints/{}'.format('gmf_factor8neg4_Epoch100_HR0.6391_NDCG0.2852.model'),\n","                'pretrain_mlp': 'checkpoints/{}'.format('mlp_factor8neg4_Epoch100_HR0.5606_NDCG0.2463.model'),\n","                'model_dir':'checkpoints/{}_Epoch{}_HR{:.4f}_NDCG{:.4f}.model'\n","                }\n","\n","# Load Data\n","!wget -nc https://raw.githubusercontent.com/sparsh-ai/rec-data-public/master/ml-1m-dat/ratings.dat\n","ml1m_dir = 'ratings.dat'\n","ml1m_rating = pd.read_csv(ml1m_dir, sep='::', header=None, names=['uid', 'mid', 'rating', 'timestamp'],  engine='python')\n","# Reindex\n","user_id = ml1m_rating[['uid']].drop_duplicates().reindex()\n","user_id['userId'] = np.arange(len(user_id))\n","ml1m_rating = pd.merge(ml1m_rating, user_id, on=['uid'], how='left')\n","item_id = ml1m_rating[['mid']].drop_duplicates()\n","item_id['itemId'] = np.arange(len(item_id))\n","ml1m_rating = pd.merge(ml1m_rating, item_id, on=['mid'], how='left')\n","ml1m_rating = ml1m_rating[['userId', 'itemId', 'rating', 'timestamp']]\n","print('Range of userId is [{}, {}]'.format(ml1m_rating.userId.min(), ml1m_rating.userId.max()))\n","print('Range of itemId is [{}, {}]'.format(ml1m_rating.itemId.min(), ml1m_rating.itemId.max()))\n","# DataLoader for training\n","sample_generator = SampleGenerator(ratings=ml1m_rating)\n","evaluate_data = sample_generator.evaluate_data\n","# Specify the exact model\n","config = gmf_config\n","engine = GMFEngine(config)\n","# config = mlp_config\n","# engine = MLPEngine(config)\n","# config = neumf_config\n","# engine = NeuMFEngine(config)\n","for epoch in range(config['num_epoch']):\n","    print('Epoch {} starts !'.format(epoch))\n","    print('-' * 80)\n","    train_loader = sample_generator.instance_a_train_loader(config['num_negative'], config['batch_size'])\n","    engine.train_an_epoch(train_loader, epoch_id=epoch)\n","    hit_ratio, ndcg = engine.evaluate(evaluate_data, epoch_id=epoch)\n","    engine.save(config['alias'], epoch, hit_ratio, ndcg)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1B5VVq__uiOK"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"authorship_tag":"ABX9TyPI46CqzLEQ44OK2crg+eo5","collapsed_sections":[],"mount_file_id":"1ZzSlzzryVmNfaOTTt5VgRthYotwK-_Su","name":"rec-algo-ncf-pytorch-yihongchen.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"}},"nbformat":4,"nbformat_minor":0}
