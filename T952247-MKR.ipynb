{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"T952247 | MKR","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyOvjwZ3bDaJNSY4CGfLUsJZ"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"P8_N8CXrApV9"},"source":["# MKR"]},{"cell_type":"markdown","metadata":{"id":"1_H4_cJqA7Y_"},"source":["## Imports"]},{"cell_type":"code","metadata":{"id":"rjFG6wmlA7Vs"},"source":["import numpy as np\n","import os\n","from tqdm import tqdm\n","from abc import abstractmethod\n","\n","import sys\n","from tqdm import tqdm\n","from sklearn.metrics import roc_auc_score\n","# from trace_grad import plot_grad_flow\n","\n","import torch\n","from torch import nn\n","from torch.nn.parameter import Parameter\n","from torch.utils.data import DataLoader\n","from torch.utils.data.sampler import SubsetRandomSampler\n","from torch.utils.tensorboard import SummaryWriter"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NorXOGuyBKOM"},"source":["## Preprocessing"]},{"cell_type":"code","metadata":{"id":"5eApT8YjChHH"},"source":["# !git clone https://github.com/hsientzucheng/MKR.PyTorch.git\n","# !mv MKR.PyTorch/data/* /content"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"esqzN_l2f0io"},"source":["!git clone https://github.com/sparsh-ai/multiobjective-optimizations.git\n","!mv multiobjective-optimizations/data/book .\n","!mv multiobjective-optimizations/data/movie .\n","!mv multiobjective-optimizations/data/music ."],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ppNWjSvKBWH4"},"source":["class Args:\n","    datapath = '/content/'\n","    DATASET = 'book'\n","    RATING_FILE_NAME = dict({'movie': 'ratings.dat',\n","                         'book': 'BX-Book-Ratings.csv',\n","                         'music': 'user_artists.dat',\n","                         'news': 'ratings.txt'})\n","    SEP = dict({'movie': '::', 'book': ';', 'music': '\\t', 'news': '\\t'})\n","    THRESHOLD = dict({'movie': 4, 'book': 0, 'music': 0, 'news': 0})\n","\n","\n","args = Args()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4vSyQUDTBKKH","executionInfo":{"status":"ok","timestamp":1634925327270,"user_tz":-330,"elapsed":31841,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}},"outputId":"3ab638f0-6800-43d3-ad14-89614b0e64e2"},"source":["def read_item_index_to_entity_id_file():\n","    file = args.datapath + args.DATASET + '/item_index2entity_id.txt'\n","    print('reading item index to entity id file: ' + file + ' ...')\n","    i = 0\n","    for line in open(file, encoding='utf-8').readlines():\n","        item_index = line.strip().split('\\t')[0]\n","        satori_id = line.strip().split('\\t')[1]\n","        item_index_old2new[item_index] = i\n","        entity_id2index[satori_id] = i\n","        i += 1\n","\n","\n","def convert_rating():\n","    file = args.datapath + args.DATASET + '/' + args.RATING_FILE_NAME[args.DATASET]\n","\n","    print('reading rating file ...')\n","    item_set = set(item_index_old2new.values())\n","    user_pos_ratings = dict()\n","    user_neg_ratings = dict()\n","\n","    for line in open(file, encoding='utf-8').readlines()[1:]:\n","        array = line.strip().split(args.SEP[args.DATASET])\n","\n","        # remove prefix and suffix quotation marks for BX dataset\n","        if args.DATASET == 'book':\n","            array = list(map(lambda x: x[1:-1], array))\n","\n","        item_index_old = array[1]\n","        if item_index_old not in item_index_old2new:  # the item is not in the final item set\n","            continue\n","        item_index = item_index_old2new[item_index_old]\n","\n","        user_index_old = int(array[0])\n","\n","        rating = float(array[2])\n","        if rating >= args.THRESHOLD[args.DATASET]:\n","            if user_index_old not in user_pos_ratings:\n","                user_pos_ratings[user_index_old] = set()\n","            user_pos_ratings[user_index_old].add(item_index)\n","        else:\n","            if user_index_old not in user_neg_ratings:\n","                user_neg_ratings[user_index_old] = set()\n","            user_neg_ratings[user_index_old].add(item_index)\n","\n","    print('converting rating file ...')\n","    writer = open(args.datapath + args.DATASET + '/ratings_final.txt', 'w', encoding='utf-8')\n","    user_cnt = 0\n","    user_index_old2new = dict()\n","    for user_index_old, pos_item_set in user_pos_ratings.items():\n","        if user_index_old not in user_index_old2new:\n","            user_index_old2new[user_index_old] = user_cnt\n","            user_cnt += 1\n","        user_index = user_index_old2new[user_index_old]\n","\n","        for item in pos_item_set:\n","            writer.write('%d\\t%d\\t1\\n' % (user_index, item))\n","        unwatched_set = item_set - pos_item_set\n","        if user_index_old in user_neg_ratings:\n","            unwatched_set -= user_neg_ratings[user_index_old]\n","        for item in np.random.choice(list(unwatched_set), size=len(pos_item_set), replace=False):\n","            writer.write('%d\\t%d\\t0\\n' % (user_index, item))\n","    writer.close()\n","    print('number of users: %d' % user_cnt)\n","    print('number of items: %d' % len(item_set))\n","\n","\n","def convert_kg():\n","    print('converting kg.txt file ...')\n","    entity_cnt = len(entity_id2index)\n","    relation_cnt = 0\n","\n","    writer = open(args.datapath + args.DATASET + '/kg_final.txt', 'w', encoding='utf-8')\n","    file = open(args.datapath + args.DATASET + '/kg.txt', encoding='utf-8')\n","\n","    for line in file:\n","        array = line.strip().split('\\t')\n","        head_old = array[0]\n","        relation_old = array[1]\n","        tail_old = array[2]\n","\n","        if head_old not in entity_id2index:\n","            continue\n","        head = entity_id2index[head_old]\n","\n","        if tail_old not in entity_id2index:\n","            entity_id2index[tail_old] = entity_cnt\n","            entity_cnt += 1\n","        tail = entity_id2index[tail_old]\n","\n","        if relation_old not in relation_id2index:\n","            relation_id2index[relation_old] = relation_cnt\n","            relation_cnt += 1\n","        relation = relation_id2index[relation_old]\n","\n","        writer.write('%d\\t%d\\t%d\\n' % (head, relation, tail))\n","\n","    writer.close()\n","    print('number of entities (containing items): %d' % entity_cnt)\n","    print('number of relations: %d' % relation_cnt)\n","\n","\n","if __name__ == '__main__':\n","    np.random.seed(555)\n","\n","    entity_id2index = dict()\n","    relation_id2index = dict()\n","    item_index_old2new = dict()\n","\n","    read_item_index_to_entity_id_file()\n","    convert_rating()\n","    convert_kg()\n","\n","    print('done')"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["reading item index to entity id file: /content/book/item_index2entity_id.txt ...\n","reading rating file ...\n","converting rating file ...\n","number of users: 17860\n","number of items: 14910\n","converting kg.txt file ...\n","number of entities (containing items): 24039\n","number of relations: 10\n","done\n"]}]},{"cell_type":"markdown","metadata":{"id":"paBHi79_GtyH"},"source":["## Dataloader"]},{"cell_type":"code","metadata":{"id":"r9hyrJaQGwAU"},"source":["class RSDataset:\n","    def __init__(self, args):\n","        self.args = args\n","        self.n_user, self.n_item, self.raw_data, self.data, self.indices = self._load_rating()\n","\n","    def __getitem__(self, index):\n","        return self.raw_data[index]\n","\n","    def _load_rating(self):\n","        print('Reading rating file')\n","\n","        rating_file = os.path.join(self.args.dataset, 'ratings_final')\n","        if os.path.exists(rating_file + '.npy'):\n","            rating_np = np.load(rating_file + '.npy')\n","        else:\n","            rating_np = np.loadtxt(rating_file + '.txt', dtype=np.int32)\n","            np.save(rating_file + '.npy', rating_np)\n","\n","        n_user = len(set(rating_np[:, 0]))\n","        n_item = len(set(rating_np[:, 1]))\n","        raw_data, data, indices = self._dataset_split(rating_np)\n","\n","        return n_user, n_item, raw_data, data, indices\n","\n","\n","    def _dataset_split(self, rating_np):\n","        print('Splitting dataset')\n","\n","        # train:eval:test = 6:2:2\n","        eval_ratio = 0.2\n","        test_ratio = 0.2\n","        n_ratings = rating_np.shape[0]\n","\n","        eval_indices = np.random.choice(list(range(n_ratings)), size=int(n_ratings * eval_ratio), replace=False)\n","        left = set(range(n_ratings)) - set(eval_indices)\n","        test_indices = np.random.choice(list(left), size=int(n_ratings * test_ratio), replace=False)\n","        train_indices = list(left - set(test_indices))\n","\n","        train_data = rating_np[train_indices]\n","        eval_data = rating_np[eval_indices]\n","        test_data = rating_np[test_indices]\n","\n","        return rating_np, [train_data, eval_data, test_data], [train_indices, eval_indices, test_indices]\n","\n","class KGDataset:\n","    def __init__(self, args):\n","        self.args = args\n","        self.n_entity, self.n_relation, self.kg = self._load_kg()\n","\n","    def __getitem__(self, index):\n","        return self.kg[index]\n","\n","    def __len__(self):\n","        return len(self.kg)\n","\n","    def _load_kg(self):\n","        print('Reading KG file')\n","\n","        kg_file = os.path.join(self.args.dataset, 'kg_final')\n","        if os.path.exists(kg_file + '.npy'):\n","            kg = np.load(kg_file + '.npy')\n","        else:\n","            kg = np.loadtxt(kg_file + '.txt', dtype=np.int32)\n","            np.save(kg_file + '.npy', kg)\n","\n","        n_entity = len(set(kg[:, 0]) | set(kg[:, 2]))\n","        n_relation = len(set(kg[:, 1]))\n","\n","        return n_entity, n_relation, kg"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SrsWQL25GiWj"},"source":["## Layers"]},{"cell_type":"code","metadata":{"id":"o-4MmZLXGjrW"},"source":["LAYER_IDS = {}\n","\n","class Dense(nn.Module):\n","    def __init__(self, input_dim, output_dim, dropout=0.0, chnl=8):\n","        super(Dense, self).__init__()\n","        self.input_dim = input_dim\n","        self.output_dim = output_dim\n","        self.dropout = dropout\n","        self.act = nn.ReLU()\n","        self.drop_layer = nn.Dropout(p=self.dropout) # Pytorch drop: ratio to zeroed\n","        self.fc = nn.Linear(self.input_dim, self.output_dim)\n","\n","    def forward(self, inputs):\n","        x = self.drop_layer(inputs)\n","        output = self.fc(x)\n","        return self.act(output)\n","\n","class Bias(nn.Module):\n","    def __init__(self, dim):\n","        super(Bias, self).__init__()\n","        self.bias = nn.Parameter(torch.zeros(dim))\n","    def forward(self, x):\n","        x = x + self.bias\n","        return x\n","\n","\n","class CrossCompressUnit(nn.Module):\n","    def __init__(self, dim):\n","        super(CrossCompressUnit, self).__init__()\n","        self.dim = dim\n","        self.fc_vv = nn.Linear(dim, 1, bias=False)\n","        self.fc_ev = nn.Linear(dim, 1, bias=False)\n","        self.fc_ve = nn.Linear(dim, 1, bias=False)\n","        self.fc_ee = nn.Linear(dim, 1, bias=False)\n","\n","        self.bias_v = Bias(dim)\n","        self.bias_e = Bias(dim)\n","\n","        # self.fc_v = nn.Linear(dim, dim)\n","        # self.fc_e = nn.Linear(dim, dim)\n","\n","    def forward(self, inputs):\n","        v, e = inputs\n","\n","        # [batch_size, dim, 1], [batch_size, 1, dim]\n","        v = torch.unsqueeze(v, 2)\n","        e = torch.unsqueeze(e, 1)\n","\n","        # [batch_size, dim, dim]\n","        c_matrix = torch.matmul(v, e)\n","        c_matrix_transpose = c_matrix.permute(0,2,1)\n","\n","        # [batch_size * dim, dim]\n","        c_matrix = c_matrix.view(-1, self.dim)\n","        c_matrix_transpose = c_matrix_transpose.contiguous().view(-1, self.dim)\n","\n","        # [batch_size, dim]\n","        v_intermediate = self.fc_vv(c_matrix) + self.fc_ev(c_matrix_transpose)\n","        e_intermediate = self.fc_ve(c_matrix) + self.fc_ee(c_matrix_transpose)\n","        v_intermediate = v_intermediate.view(-1, self.dim)\n","        e_intermediate = e_intermediate.view(-1, self.dim)\n","\n","        v_output = self.bias_v(v_intermediate)\n","        e_output = self.bias_e(e_intermediate)\n","\n","        # v_output = self.fc_v(v_intermediate)\n","        # e_output = self.fc_e(e_intermediate)\n","\n","\n","        return v_output, e_output"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Ai3xfyXnA7S8"},"source":["## Model"]},{"cell_type":"code","metadata":{"id":"wBl8B-3PA7QI"},"source":["class MKR_model(nn.Module):\n","    def __init__(self, args, n_user, n_item, n_entity, n_relation, use_inner_product=True):\n","        super(MKR_model, self).__init__()\n","\n","        # <Lower Model>\n","        self.args = args\n","        self.n_user = n_user\n","        self.n_item = n_item\n","        self.n_entity = n_entity\n","        self.n_relation = n_relation\n","        self.use_inner_product = use_inner_product\n","\n","        # Init embeddings\n","        self.user_embeddings_lookup = nn.Embedding(self.n_user, self.args.dim)\n","        self.item_embeddings_lookup = nn.Embedding(self.n_item, self.args.dim)\n","        self.entity_embeddings_lookup = nn.Embedding(self.n_entity, self.args.dim)\n","        self.relation_embeddings_lookup = nn.Embedding(self.n_relation, self.args.dim)\n","\n","        self.user_mlp = nn.Sequential()\n","        self.tail_mlp = nn.Sequential()\n","        self.cc_unit = nn.Sequential()\n","        for i_cnt in range(self.args.L):\n","            self.user_mlp.add_module('user_mlp{}'.format(i_cnt),\n","                                     Dense(self.args.dim, self.args.dim))\n","            self.tail_mlp.add_module('tail_mlp{}'.format(i_cnt),\n","                                     Dense(self.args.dim, self.args.dim))\n","            self.cc_unit.add_module('cc_unit{}'.format(i_cnt),\n","                                     CrossCompressUnit(self.args.dim))\n","        # <Higher Model>\n","        self.kge_pred_mlp = Dense(self.args.dim * 2, self.args.dim)\n","        self.kge_mlp = nn.Sequential()\n","        for i_cnt in range(self.args.H - 1):\n","            self.kge_mlp.add_module('kge_mlp{}'.format(i_cnt),\n","                                    Dense(self.args.dim * 2, self.args.dim * 2))\n","        if self.use_inner_product==False:\n","            self.rs_pred_mlp = Dense(self.args.dim * 2, 1)\n","            self.rs_mlp = nn.Sequential()\n","            for i_cnt in range(self.args.H - 1):\n","                self.rs_mlp.add_module('rs_mlp{}'.format(i_cnt),\n","                                       Dense(self.args.dim * 2, self.args.dim * 2))\n","\n","    def forward(self, user_indices=None, item_indices=None, head_indices=None,\n","            relation_indices=None, tail_indices=None):\n","\n","        # <Lower Model>\n","\n","        if user_indices is not None:\n","            self.user_indices = user_indices\n","        if item_indices is not None:\n","            self.item_indices = item_indices\n","        if head_indices is not None:\n","            self.head_indices = head_indices\n","        if relation_indices is not None:\n","            self.relation_indices = relation_indices\n","        if tail_indices is not None:\n","            self.tail_indices = tail_indices\n","\n","        # Embeddings\n","        self.item_embeddings = self.item_embeddings_lookup(self.item_indices)\n","        self.head_embeddings = self.entity_embeddings_lookup(self.head_indices)\n","        self.item_embeddings, self.head_embeddings = self.cc_unit([self.item_embeddings, self.head_embeddings])\n","\n","        # <Higher Model>\n","        if user_indices is not None:\n","            # RS\n","            self.user_embeddings = self.user_embeddings_lookup(self.user_indices)\n","            self.user_embeddings = self.user_mlp(self.user_embeddings)\n","            if self.use_inner_product:\n","                # [batch_size]\n","                self.scores = torch.sum(self.user_embeddings * self.item_embeddings, 1)\n","            else:\n","                # [batch_size, dim * 2]\n","                self.user_item_concat = torch.cat([self.user_embeddings, self.item_embeddings], 1)\n","                self.user_item_concat = self.rs_mlp(self.user_item_concat)\n","                # [batch_size]\n","                self.scores = torch.squeeze(self.rs_pred_mlp(self.user_item_concat))\n","            self.scores_normalized = torch.sigmoid(self.scores)\n","            outputs = [self.user_embeddings, self.item_embeddings, self.scores, self.scores_normalized]\n","        if relation_indices is not None:\n","            # KGE\n","            self.tail_embeddings = self.entity_embeddings_lookup(self.tail_indices)\n","            self.relation_embeddings = self.relation_embeddings_lookup(self.relation_indices)\n","            self.tail_embeddings = self.tail_mlp(self.tail_embeddings)\n","            # [batch_size, dim * 2]\n","            self.head_relation_concat = torch.cat([self.head_embeddings, self.relation_embeddings], 1)\n","            self.head_relation_concat = self.kge_mlp(self.head_relation_concat)\n","            # [batch_size, 1]\n","            self.tail_pred = self.kge_pred_mlp(self.head_relation_concat)\n","            self.tail_pred = torch.sigmoid(self.tail_pred)\n","            self.scores_kge = torch.sigmoid(torch.sum(self.tail_embeddings * self.tail_pred, 1))\n","            self.rmse = torch.mean(\n","                torch.sqrt(torch.sum(torch.pow(self.tail_embeddings -\n","                           self.tail_pred, 2), 1) / self.args.dim))\n","            outputs = [self.head_embeddings, self.tail_embeddings, self.scores_kge, self.rmse]\n","\n","        return outputs\n","\n","\n","class MKR(object):\n","    def __init__(self, args, n_users, n_items, n_entities,\n","                 n_relations):\n","        self.args = args\n","        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","        self._parse_args(n_users, n_items, n_entities, n_relations)\n","        self._build_model()\n","        self._build_loss()\n","        self._build_ops()\n","\n","    def _parse_args(self, n_users, n_items, n_entities, n_relations):\n","        self.n_user = n_users\n","        self.n_item = n_items\n","        self.n_entity = n_entities\n","        self.n_relation = n_relations\n","\n","    def _build_model(self):\n","        print(\"Build models\")\n","        self.MKR_model = MKR_model(self.args, self.n_user, self.n_item, self.n_entity, self.n_relation)\n","        self.MKR_model = self.MKR_model.to(self.device, non_blocking=True)\n","        for m in self.MKR_model.modules():\n","            if isinstance(m, nn.Linear):\n","                nn.init.xavier_uniform_(m.weight)\n","                if m.bias is not None:\n","                    nn.init.zeros_(m.bias)\n","            if isinstance(m, nn.Embedding):\n","                torch.nn.init.xavier_uniform_(m.weight)\n","        # for param in self.MKR_model.parameters():\n","        #     param.requires_grad = True\n","\n","    def _build_loss(self):\n","        self.sigmoid_BCE = nn.BCEWithLogitsLoss()\n","        self.sigmoid = nn.Sigmoid()\n","\n","    def _build_ops(self):\n","        self.optimizer_rs = torch.optim.Adam(self.MKR_model.parameters(),\n","                                             lr=self.args.lr_rs)\n","        self.optimizer_kge = torch.optim.Adam(self.MKR_model.parameters(),\n","                                              lr=self.args.lr_kge)\n","\n","    def _inference_rs(self, inputs):\n","        # Inputs\n","        self.user_indices = inputs[:, 0].long().to(self.device,\n","                non_blocking=True)\n","        self.item_indices = inputs[:, 1].long().to(self.device,\n","                non_blocking=True)\n","        labels = inputs[:, 2].float().to(self.device)\n","        self.head_indices = inputs[:, 1].long().to(self.device,\n","                non_blocking=True)\n","\n","        # Inference\n","        outputs = self.MKR_model(user_indices=self.user_indices,\n","                                 item_indices=self.item_indices,\n","                                 head_indices=self.head_indices,\n","                                 relation_indices=None,\n","                                 tail_indices=None)\n","\n","        user_embeddings, item_embeddings, scores, scores_normalized = outputs\n","        return user_embeddings, item_embeddings, scores, scores_normalized, labels\n","\n","    def _inference_kge(self, inputs):\n","        # Inputs\n","        self.item_indices = inputs[:, 0].long().to(self.device,\n","                non_blocking=True)\n","        self.head_indices = inputs[:, 0].long().to(self.device,\n","                non_blocking=True)\n","        self.relation_indices = inputs[:, 1].long().to(self.device,\n","                non_blocking=True)\n","        self.tail_indices = inputs[:, 2].long().to(self.device,\n","                non_blocking=True)\n","\n","        # Inference\n","        outputs = self.MKR_model(user_indices=None,\n","                                 item_indices=self.item_indices,\n","                                 head_indices=self.head_indices,\n","                                 relation_indices=self.relation_indices,\n","                                 tail_indices=self.tail_indices)\n","\n","        head_embeddings, tail_embeddings, scores_kge, rmse = outputs\n","        return head_embeddings, tail_embeddings, scores_kge, rmse\n","\n","    def l2_loss(self, inputs):\n","        return torch.sum(inputs ** 2) / 2\n","\n","    def loss_rs(self, user_embeddings, item_embeddings, scores, labels):\n","        # scores_for_signll = torch.cat([1-self.sigmoid(scores).unsqueeze(1),\n","        #                                self.sigmoid(scores).unsqueeze(1)], 1)\n","        # base_loss_rs = torch.mean(self.nll_loss(scores_for_signll, labels))\n","        base_loss_rs = torch.mean(self.sigmoid_BCE(scores, labels))\n","        l2_loss_rs = self.l2_loss(user_embeddings) + self.l2_loss(item_embeddings)\n","        for name, param in self.MKR_model.named_parameters():\n","            if param.requires_grad and ('embeddings_lookup' not in name) \\\n","                    and (('rs' in name) or ('cc_unit' in name) or ('user' in name)) \\\n","                    and ('weight' in name):\n","                l2_loss_rs = l2_loss_rs + self.l2_loss(param)\n","        loss_rs = base_loss_rs + l2_loss_rs * self.args.l2_weight\n","        return loss_rs, base_loss_rs, l2_loss_rs\n","\n","    def loss_kge(self, scores_kge, head_embeddings, tail_embeddings):\n","        base_loss_kge = -scores_kge\n","        l2_loss_kge = self.l2_loss(head_embeddings) + self.l2_loss(tail_embeddings)\n","        for name, param in self.MKR_model.named_parameters():\n","            if param.requires_grad and ('embeddings_lookup' not in name) \\\n","                    and (('kge' in name) or ('tail' in name) or ('cc_unit' in name)) \\\n","                    and ('weight' in name):\n","                l2_loss_kge = l2_loss_kge + self.l2_loss(param)\n","        # Note: L2 regularization will be done by weight_decay of pytorch optimizer\n","        loss_kge = base_loss_kge + l2_loss_kge * self.args.l2_weight\n","        return loss_kge, base_loss_kge, l2_loss_kge\n","\n","    def train_rs(self, inputs, show_grad=False, glob_step=None):\n","        self.MKR_model.train()\n","        user_embeddings, item_embeddings, scores, _, labels= self._inference_rs(inputs)\n","        loss_rs, base_loss_rs, l2_loss_rs = self.loss_rs(user_embeddings, item_embeddings, scores, labels)\n","\n","        self.optimizer_rs.zero_grad()\n","        loss_rs.backward()\n","        # if show_grad:\n","        #     plot_grad_flow(self.MKR_model.named_parameters(),\n","        #                    \"grad_plot/rs_grad_step{}\".format(glob_step))\n","\n","        self.optimizer_rs.step()\n","        loss_rs.detach()\n","        user_embeddings.detach()\n","        item_embeddings.detach()\n","        scores.detach()\n","        labels.detach()\n","\n","        return loss_rs, base_loss_rs, l2_loss_rs\n","\n","    def train_kge(self, inputs, show_grad=False, glob_step=None):\n","        self.MKR_model.train()\n","        head_embeddings, tail_embeddings, scores_kge, rmse = self._inference_kge(inputs)\n","        loss_kge, base_loss_kge, l2_loss_kge = self.loss_kge(scores_kge, head_embeddings, tail_embeddings)\n","\n","        self.optimizer_kge.zero_grad()\n","        loss_kge.sum().backward()\n","        # if show_grad:\n","        #     plot_grad_flow(self.MKR_model.named_parameters(),\n","        #                    \"grad_plot/kge_grad_step{}\".format(glob_step))\n","\n","        self.optimizer_kge.step()\n","        loss_kge.detach()\n","        head_embeddings.detach()\n","        tail_embeddings.detach()\n","        scores_kge.detach()\n","        rmse.detach()\n","        return rmse, loss_kge.sum(), base_loss_kge.sum(), l2_loss_kge\n","\n","    def eval(self, inputs):\n","        self.MKR_model.eval()\n","        inputs = torch.from_numpy(inputs)\n","        user_embeddings, item_embeddings, _, scores, labels = self._inference_rs(inputs)\n","        labels = labels.to(\"cpu\").detach().numpy()\n","        scores = scores.to(\"cpu\").detach().numpy()\n","        auc = roc_auc_score(y_true=labels, y_score=scores)\n","        predictions = [1 if i >= 0.5 else 0 for i in scores]\n","        acc = np.mean(np.equal(predictions, labels))\n","\n","        return auc, acc\n","\n","    def topk_eval(self, user_list, train_record, test_record, item_set, k_list):\n","        print(\"Eval TopK\")\n","        precision_list = {k: [] for k in k_list}\n","        recall_list = {k: [] for k in k_list}\n","        for user in tqdm(user_list):\n","            test_item_list = list(item_set - train_record[user])\n","            item_score_map = dict()\n","            scores = self._get_scores(np.array([user]*len(test_item_list)),\n","                                      np.array(test_item_list),\n","                                      np.array(test_item_list))\n","            items = np.array(test_item_list)\n","            for item, score in zip(items, scores):\n","                item_score_map[item] = score\n","            item_score_pair_sorted = sorted(item_score_map.items(), key=lambda x: x[1], reverse=True)\n","            item_sorted = [i[0] for i in item_score_pair_sorted]\n","            for k in k_list:\n","                hit_num = len(set(item_sorted[:k]) & test_record[user])\n","                precision_list[k].append(hit_num / k)\n","                recall_list[k].append(hit_num / len(test_record[user]))\n","        precision = [np.mean(precision_list[k]) for k in k_list]\n","        recall = [np.mean(recall_list[k]) for k in k_list]\n","        f1 = [2 / (1 / precision[i] + 1 / recall[i]) for i in range(len(k_list))]\n","\n","        return precision, recall, f1\n","\n","    def _get_scores(self, user, item_list, head_list):\n","        # Inputs\n","        user = torch.from_numpy(user)\n","        item_list = torch.from_numpy(item_list)\n","        head_list = torch.from_numpy(head_list)\n","        self.user_indices = user.long().to(self.device)\n","        self.item_indices = item_list.long().to(self.device)\n","        self.head_indices = head_list.long().to(self.device)\n","\n","        self.MKR_model.eval()\n","        outputs = self.MKR_model(self.user_indices, self.item_indices,\n","                                 self.head_indices, self.relation_indices,\n","                                 self.tail_indices)\n","        user_embeddings, item_embeddings, _, scores = outputs\n","        return scores"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"h5LBo3T6BDo_"},"source":["## Training"]},{"cell_type":"code","metadata":{"id":"WqVtFhtPA7Mi"},"source":["def train(args, rs_dataset, kg_dataset):\n","\n","    show_loss = args.show_loss\n","    show_topk = args.show_topk\n","\n","    # Get RS data\n","    n_user = rs_dataset.n_user\n","    n_item = rs_dataset.n_item\n","    train_data, eval_data, test_data = rs_dataset.data\n","    train_indices, eval_indices, test_indices = rs_dataset.indices\n","\n","    # Get KG data\n","    n_entity = kg_dataset.n_entity\n","    n_relation = kg_dataset.n_relation\n","    kg = kg_dataset.kg\n","\n","    # Init train sampler\n","    train_sampler = SubsetRandomSampler(train_indices)\n","\n","    # Init MKR model\n","    model = MKR(args, n_user, n_item, n_entity, n_relation)\n","\n","    # Init Sumwriter\n","    writer = SummaryWriter(args.summary_path)\n","\n","    # Top-K evaluation settings\n","    user_num = 100\n","    k_list = [1, 2, 5, 10, 20, 50, 100]\n","    train_record = get_user_record(train_data, True)\n","    test_record = get_user_record(test_data, False)\n","    user_list = list(set(train_record.keys()) & set(test_record.keys()))\n","    if len(user_list) > user_num:\n","        user_list = np.random.choice(user_list, size=user_num, replace=False)\n","    item_set = set(list(range(n_item)))\n","    step = 0\n","    for epoch in range(args.n_epochs):\n","        print(\"Train RS\")\n","        train_loader = DataLoader(rs_dataset, batch_size=args.batch_size,\n","                                  num_workers=args.workers, sampler=train_sampler)\n","        for i, rs_batch_data in enumerate(train_loader):\n","            loss, base_loss_rs, l2_loss_rs = model.train_rs(rs_batch_data)\n","            writer.add_scalar(\"rs_loss\", loss.cpu().detach().numpy(), global_step=step)\n","            writer.add_scalar(\"rs_base_loss\", base_loss_rs.cpu().detach().numpy(), global_step=step)\n","            writer.add_scalar(\"rs_l2_loss\", l2_loss_rs.cpu().detach().numpy(), global_step=step)\n","            step += 1\n","            if show_loss:\n","                print(loss)\n","\n","        if epoch % args.kge_interval == 0:\n","            print(\"Train KGE\")\n","            kg_train_loader = DataLoader(kg_dataset, batch_size=args.batch_size,\n","                                         num_workers=args.workers, shuffle=True)\n","            for i, kg_batch_data in enumerate(kg_train_loader):\n","                rmse, loss_kge, base_loss_kge, l2_loss_kge = model.train_kge(kg_batch_data)\n","                writer.add_scalar(\"kge_rmse_loss\", rmse.cpu().detach().numpy(), global_step=step)\n","                writer.add_scalar(\"kge_loss\", loss_kge.cpu().detach().numpy(), global_step=step)\n","                writer.add_scalar(\"kge_base_loss\", base_loss_kge.cpu().detach().numpy(), global_step=step)\n","                writer.add_scalar(\"kge_l2_loss\", l2_loss_kge.cpu().detach().numpy(), global_step=step)\n","                step += 1\n","                if show_loss:\n","                    print(rmse)\n","\n","\n","        # CTR evaluation\n","        train_auc, train_acc = model.eval(train_data)\n","        eval_auc, eval_acc = model.eval(eval_data)\n","        test_auc, test_acc = model.eval(test_data)\n","\n","        print('epoch %d    train auc: %.4f  acc: %.4f    eval auc: %.4f  acc: %.4f    test auc: %.4f  acc: %.4f'\n","              % (epoch, train_auc, train_acc, eval_auc, eval_acc, test_auc, test_acc))\n","\n","        # top-K evaluation\n","        if show_topk:\n","            precision, recall, f1 = model.topk_eval(user_list, train_record, test_record, item_set, k_list)\n","            print('precision: ', end='')\n","            for i in precision:\n","                print('%.4f\\t' % i, end='')\n","            print()\n","            print('recall: ', end='')\n","            for i in recall:\n","                print('%.4f\\t' % i, end='')\n","            print()\n","            print('f1: ', end='')\n","            for i in f1:\n","                print('%.4f\\t' % i, end='')\n","            print('\\n')\n","\n","def get_user_record(data, is_train):\n","    user_history_dict = dict()\n","    for interaction in data:\n","        user = interaction[0]\n","        item = interaction[1]\n","        label = interaction[2]\n","        if is_train or label == 1:\n","            if user not in user_history_dict:\n","                user_history_dict[user] = set()\n","            user_history_dict[user].add(item)\n","    return user_history_dict"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UbZIyHrtA7IK","executionInfo":{"status":"ok","timestamp":1634925619317,"user_tz":-330,"elapsed":291346,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}},"outputId":"7c45643d-2c23-480d-a63f-b6ad64206f79"},"source":["import argparse\n","\n","parser = argparse.ArgumentParser()\n","\n","'''\n","# movie\n","parser.add_argument('--dataset', type=str, default='movie', help='which dataset to use')\n","parser.add_argument('--n_epochs', type=int, default=20, help='the number of epochs')\n","parser.add_argument('--dim', type=int, default=8, help='dimension of user and entity embeddings')\n","parser.add_argument('--L', type=int, default=1, help='number of low layers')\n","parser.add_argument('--H', type=int, default=1, help='number of high layers')\n","parser.add_argument('--batch_size', type=int, default=4096, help='batch size')\n","parser.add_argument('--l2_weight', type=float, default=1e-6, help='weight of l2 regularization')\n","parser.add_argument('--lr_rs', type=float, default=0.02, help='learning rate of RS task')\n","parser.add_argument('--lr_kge', type=float, default=0.01, help='learning rate of KGE task')\n","parser.add_argument('--kge_interval', type=int, default=3, help='training interval of KGE task')\n","'''\n","\n","# '''\n","# book\n","parser.add_argument('--dataset', type=str, default='book', help='which dataset to use')\n","parser.add_argument('--n_epochs', type=int, default=10, help='the number of epochs')\n","parser.add_argument('--dim', type=int, default=8, help='dimension of user and entity embeddings')\n","parser.add_argument('--L', type=int, default=1, help='number of low layers')\n","parser.add_argument('--H', type=int, default=1, help='number of high layers')\n","parser.add_argument('--batch_size', type=int, default=32, help='batch size')\n","parser.add_argument('--l2_weight', type=float, default=1e-6, help='weight of l2 regularization')\n","parser.add_argument('--lr_rs', type=float, default=2e-4, help='learning rate of RS task')\n","parser.add_argument('--lr_kge', type=float, default=2e-5, help='learning rate of KGE task')\n","parser.add_argument('--kge_interval', type=int, default=2, help='training interval of KGE task')\n","# '''\n","\n","'''\n","# music\n","parser.add_argument('--dataset', type=str, default='music', help='which dataset to use')\n","parser.add_argument('--n_epochs', type=int, default=10, help='the number of epochs')\n","parser.add_argument('--dim', type=int, default=4, help='dimension of user and entity embeddings')\n","parser.add_argument('--L', type=int, default=2, help='number of low layers')\n","parser.add_argument('--H', type=int, default=1, help='number of high layers')\n","parser.add_argument('--batch_size', type=int, default=256, help='batch size')\n","parser.add_argument('--l2_weight', type=float, default=1e-6, help='weight of l2 regularization')\n","parser.add_argument('--lr_rs', type=float, default=1e-3, help='learning rate of RS task')\n","parser.add_argument('--lr_kge', type=float, default=2e-4, help='learning rate of KGE task')\n","parser.add_argument('--kge_interval', type=int, default=2, help='training interval of KGE task')\n","'''\n","\n","parser.add_argument('--workers', type=int, default=2,\n","                    help='number of data loading workers')\n","parser.add_argument('-sl', '--show_loss', action='store_true',\n","                    help='show loss or not')\n","parser.add_argument('-st', '--show_topk', action='store_true',\n","                    help='show topK or not')\n","parser.add_argument('-sum', '--summary_path', type=str, default='./summary',\n","                    help='path to store training summary')\n","args = parser.parse_args(args={})\n","\n","\n","rs_dataset = RSDataset(args)\n","kg_dataset = KGDataset(args)\n","train(args, rs_dataset, kg_dataset)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Reading rating file\n","Splitting dataset\n","Reading KG file\n","Build models\n","Train RS\n","Train KGE\n","epoch 0    train auc: 0.8203  acc: 0.6389    eval auc: 0.7148  acc: 0.6267    test auc: 0.7138  acc: 0.6308\n","Train RS\n","epoch 1    train auc: 0.8390  acc: 0.6982    eval auc: 0.7180  acc: 0.6778    test auc: 0.7189  acc: 0.6772\n","Train RS\n","Train KGE\n","epoch 2    train auc: 0.8541  acc: 0.7280    eval auc: 0.7256  acc: 0.6962    test auc: 0.7267  acc: 0.6972\n","Train RS\n","epoch 3    train auc: 0.8636  acc: 0.7424    eval auc: 0.7281  acc: 0.7035    test auc: 0.7295  acc: 0.7025\n","Train RS\n","Train KGE\n","epoch 4    train auc: 0.8716  acc: 0.7567    eval auc: 0.7281  acc: 0.7029    test auc: 0.7296  acc: 0.7018\n","Train RS\n","epoch 5    train auc: 0.8783  acc: 0.7725    eval auc: 0.7259  acc: 0.6883    test auc: 0.7278  acc: 0.6912\n","Train RS\n","Train KGE\n","epoch 6    train auc: 0.8840  acc: 0.7823    eval auc: 0.7234  acc: 0.6762    test auc: 0.7255  acc: 0.6800\n","Train RS\n","epoch 7    train auc: 0.8889  acc: 0.7896    eval auc: 0.7216  acc: 0.6637    test auc: 0.7239  acc: 0.6667\n","Train RS\n","Train KGE\n","epoch 8    train auc: 0.8931  acc: 0.7935    eval auc: 0.7195  acc: 0.6556    test auc: 0.7222  acc: 0.6607\n","Train RS\n","epoch 9    train auc: 0.8968  acc: 0.7951    eval auc: 0.7176  acc: 0.6528    test auc: 0.7205  acc: 0.6571\n"]}]}]}